I found this very timely - the authors do well to point out the mis-match between the FDA's willingness
to approve the marketing of AI methods and the quality and quantity of evidence supporting them. One
expects this sort of unfounded enthusiasm in the popular press (where one also finds, too often, its
opposite in the form of, for example, scare-mongering associated with well-proven vaccination
programmes), but to find unjustified encouragement at an authoritative level is worrying. I think this
point alone makes publication obligatory, but there were a few other points that occurred to me while
reading the paper that are mentioned below.
On p 8, the use of the word "cohort" was confusing since in RCTs it usually applies to subjects, but here
it seems to mean a medical reference group of which, however, only one member needs to be "expert" all very puzzling and I think it needs a bit more description.
The comments on patient satisfaction on p 11 were opaque to me because it was not clear what the
patients had been asked, nor whether they were blinded as to the AI/human variable. This point, and
some others, might have been improved had there been lay involvement in the study management,
even if limited to the reporting stage (see p 10).
I do not think that the use of "deep learning" software is of great significance here. We are still aiming to
compare machine performance with human, no matter how sophisticated the automated method is by its
own lights. There will certainly be future developments - "even deeper learning" - but the question
remains the same: is it better than having humans do it?
