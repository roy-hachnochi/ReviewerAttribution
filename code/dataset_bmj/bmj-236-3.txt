This is an interesting review of the quality of and claims made by studies that compare deep learning
with clinicians for the evaluation of medical images. Deep learning has become a buzzword that leads to
overly optimistic expectations. I have some main issues, and then minor issues and details that mainly
relate to further information or clarification of matters that were unclear to me

MAIN COMMENTS
1. The paper summarizes claims made by the paper, and describe that most papers make positive
claims in favour of deep learning. In that respect, please add information on the reported performance
for the observational studies. Basically, an overview table of the 81 papers, including their design
(retrospective, prospective, real world), development and/or validation nature, sample size,
performance, and claims made would be useful. In contrast to this, reported performance for the two
RCTs was described in the text.
2. I did not see much overlap between the bullet points about ‘what this study adds’ and the 5 key
findings described in the discussion. Anyway, consistent with the ‘what this study adds’ section, the high

risk of bias and poor reporting of many studies (mainly non-randomized) is a key finding in itself (now
hidden in the 5th finding in the discussion).
3. On p14, the risk of bias in non-randomized studies is very shortly discussed. It deserves more
information, because this is a very important issue. E.g. what does ‘analysis domain’ refer to? What did
the studies at high risk of bias do? In general, the detailed information regarding the content of and
results regarding the PROBAST items is too limited/abstract in my view. Please expand.
4. Cf. Appendix 3: The review excluded studies in which clinicians were also involved in determination of
the ground truth. However: these studies are also published, and claims are made in these studies as
well. I do not understand why these are excluded. These studies should be included and listed as at high
risk of bias.
5. The impact of expertise is very relevant (cf fourth key finding in discussion):
- The definition of expert vs non-expert may be elaborated on more.
- Did any study try to assess the effect of clinician expertise?

MINOR ISSUES
- P8, the definition of a ‘clinical problem’ is vague. Isn’t it mostly about diagnosis of some condition?
- Table 1 is hard to read, please try to rearrange. Also, it is often not easy to know what kind of medical
images are being used, e.g. when the intervention is described as ‘AI assisted clinic’. It is unclear what is
done.
- P11, what does ‘accuracy for treatment recommendation’ refer to? What is the ground truth here?
- P11-12: please add sample size when describing the RCTs in text.
- P13: the 7 studies that claimed that the algorithm could now be used in clinical practice, where these
mainly the prospective real world validations?
- P13: what kind of sample size calculations did these 14 studies do? Otherwise, this claim means little
to me.
- P13: did studies using split sample always split into training, validation, and test parts? This seems to
be implied, but is not clear. And when providing median sample sizes, how were studies with CV or
bootstrapping (instead of split sample) handled? (see also overview in Appendix 5)
- Data augmentation: it would be interesting to add more information on how often different techniques
were used. (see also overview in Appendix 5)
- P14: how many studies are ‘the vast majority’? And what happened in other studies? Were all images
then rated by 1 of the clinicians?
- Nearly two thirds of studies failed to recommend further studies: is this related to whether the study
was retrospective, prospective, or a real world study?
- Limitations in discussion: did you consider to add ad hoc items to TRIPOD or PROBAST that would be
relevant for this review?

- Limitations in discussion: What did you mean when saying that generalizing to other types of AI is not
appropriate? When modelling electronic health record data, the aim is rarely to compare with clinicians
(as you mention in appendix 3)?
- Figure 1: the first reason for excluding full texts (34 excluded because of it) was unclear to me. It
seems like a mixture of things?
- Appendix 3: The issue on overfitting is unclear to me. I do not understand what you are referring to
here.
- Appendix 3, on expertise level: do you mean that detailed level of expertise was not recorded?
Confusing, because you do make a distinction between experts and non-experts.
- Appendix 5: 9 studies were ‘development only’ studies? This deserves more attention, it seems very
problematic. But then later, when TRIPOD study types are presented, 0 studies were ‘development only’.
This was confusing, please clarify.
- Appendix 5: the external dataset testing bullet is unclear to me.
- Appendix 5: Training set events: give an overview of outcomes. If outcome was non binary, was it
always nominal? How many studies had a binary outcome?
- Appendix 5 on code availability was too cryptical for me. Does ‘modelling’ mean that the model was
made available somehow?

DETAILS FOR CONSIDERATION BY THE AUTHORS
- P6 ‘raw data’: perhaps be more specific here, the rest of the sentence seems to refer to medical
images.
- P6, the second step of the path to implementation involves RCTs to evaluate real world impact and
usefulness. True, but it is a general problem of prediction models that these studies rarely happen. This
deserves a comment, I feel. Detail: at this point, was unclear to me how these two steps related to the
aim of the current paper. Perhaps in the last paragraph you may mention that you aim to search for
randomized and non-randomized studies on the evaluation of medical images.
- P9, about study selection and extraction of data:
i. the fourth person (DR) for abstract screening was not mentioned yet (only later, in
acknowledgements).
ii. Full text assessment: was this done by the same people who screened the abstract?
- P10, data synthesis: perhaps mention the pre-specified and post hoc features for descriptive analysis?
- P11-12, only risk of bias for blinding: this is not easy to avoid?
- P14, lines 8-10: sentence (about volume and granularity etc) was unclear to me.
- P16: internal review refers to review by FDA staff?
- Search strategy in appendix: typo in ‘deuplicate’
- Appendix 3 is very nice, thank you. Is this referred to in the text?

- Appendix 3: ‘we may have missed studies in non-imaging areas’: so what was the initial aim? Any
study comparing any AI/machine learning method with clinician judgment?
- Appendix 5: What is meant with non-random split? Chronological or geographical split?
- Appendix 5: Validation set size: available in 37 of X studies
- Appendix 5: ‘requirement for prospective +/- trials’: what does +/- mean?
- Appendix 5: write GPU in full