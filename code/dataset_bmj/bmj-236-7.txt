This is an interesting and timely article. I have a few minor comments.
Study selection - how were the "clearly irrelevant" records removed? That is, how many reviewers
screened them?
I'm not sure why predictor variables are irrelevant here. They are not applicable if the algorithm does
not use any data other than images as the input, but we very well can build algorithms that take both
images and other variables as predictors. It seems to me that the eligibility criteria described here are
clear that this review is limited to the former where the sole input to algorithms is images. I wouldn't
ask you to revise your assessment of studies but I'd ask you to state that you did not use this item in
TRIPOD. This applies to items 1, 5c, 7a, 7b, 9, 10a, 10b, 11, 12, 13b.
I am not certain of the relevance of reporting hardware configurations. It is pertinent in the context of
anticipated run-time delays, so information specific to run-time performance is more informative than
the hardware configuration. From a peer reviewer's perspective, I would like to know what computing
power was necessary to run the algorithms because it can give me a sense of whether it is realistic as
opposed to having been made up. I understand that hardware information is sometimes reported, more
so in the engineering literature, but what matters for a clinical application is what to expect in run-time

delay. I also think that investigators developing and validating the algorithm are not required to evaluate
and report information on expected run-time performance. This relates to the comment on studies that
show non-inferior but quicker performance than other methods.
The fifth item in the discussion section regarding "descriptive phrases" - did the authors consider
assessing the manuscripts for spin?
I suggest the authors discuss the US FDA guidance on software as a device (reference #28). They are
pertinent in the context of which version of an algorithm is evaluated in a randomized controlled trial,
and how it is documented in a published report of the trial.
Figure 3 - labels on x-axis are too small to read. Also, is "completeness" of reporting defined in the
Methods section?
I'm afraid it is not clear to me how the authors evaluated "real world testing". What criteria would make
a study deserve the label of "real world testing"?