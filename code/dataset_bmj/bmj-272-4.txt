
I have moderate to high enthusiasm for this article. Its major strength is that it will be a
point of reference for the rapidly developing topic of how to assess scientists. It’s major
weakness, I feel, is the scanty presentation of what appears to be a developing movement
to improve the assessment of scientists using criteria that include research practices that
may improve the science and reduce waste.
About lack of context: the manuscript describes the background literature very briefly, so
that it’s hard to assess the context to which it contribute. The manuscript references a
2018 PLOS Biology article by some of the same authors. The article described the results
of a one-day expert panel workshop to discuss ways to assess scientists. The panel was
preceded by a non-systematic review of the literature; the present article would benefit
from a concise review of the main findings of that review, which the authors characterize
as showing that research will have more value when it is performed according to principles
known to improve research. This article shows that the present manuscript fits into a line
of articles of increasing specificity including a report by the US National Academy of
Sciences, which advocated for assessing scientists by measuring the impact of their work.
The main output of the expert panel was 6 principles for assessing scientists. Reproducing
these principles in this manuscript would help to put the present study and its proposals in
context, since the present study’s focus is on one of the six principles: assessing scientists
by indicators that reflect the best publication practices (including data sharing, study
registration, alternative metrics of impact, adherence to reporting standards, publishing
negative or inconclusive studies). Without seeing the other five principles, the reader
won’t understand that the present manuscript addresses part of the problem but that a
survey of current practices for evaluating researchers is a key part of developing this
growing field. As such, I favor publication after major revision.
The authors pre-specified “traditional” and “progressive” promotion criteria. The latter
include rates of citation of the candidate’s articles, willingness to share their research data
sets, adherence to research reporting standards like CONSORT, publishing in open access
journals, registering their research, accommodating extenuating circumstances). They
assembled the promotion criteria at a large random sample of university-affiliated
health-related schools. They found that many organizations used the traditional criteria
and almost none used the progressive criteria (except for counting citations and
accommodating extenuating circumstances). They argue that schools would promote
better research if they adopted the progressive criteria because authors would be
motivated to share data, publish in open access journals, and adhere to reporting
standards because their chances of academic promotion rested in part on adherence to the
progressive criteria.
The article falls in the middle between a) a short opinion piece advocating that universities
adopt criteria reflecting best publication practices and b) reporting a study comparing
adherence to the progressive criteria after instituting them as criteria for promotion. It is
more than an opinion piece and less than a test of the hypothesis that progressive criteria
would change faculty authors adherence to the progressive criteria. The article poses the
hypothesis but doesn’t test it except to show that there are many universities that would
be eligible for a controlled study. Nonetheless, as a documentation of present practice, it is
a point of reference for the field.
Where did the progressive criteria come from? The authors didn’t describe the process of
selecting criteria, but it sounds as if they were the source. In an era of involving
stakeholders in the design of research, it would have been more convincing to engage
stakeholders in a survey/focus group to generate ideas for criteria that would promote the
public good. The 2018 expert panel used stakeholders in the academic and science worlds.
Both studies would be better grounded if they listened to stakeholders who are funders,
implementers, and consumers of research.
The sample of institutional guidelines was 63% of the institutions with faculties of
biomedical science. This leaves open the possibility that the institutions that did not
respond had a high rate of adherence to the progressive criteria. This seems unlikely, but
the high non-response rate does raise questions about the representativeness of the study
sample.

The most interesting aspect of the report is the wide variety of implementing the traditional
criteria. Perhaps the study will stimulate academic institutions to add criteria, both
traditional and, especially, the progressive criteria. The latter is the authors’ main point.
A recurring terminology inconsistency is what appears to be the use of “criteria” and
“incentives” as interchangeable terms for the same object. While they are both applicable,
it’s confusing to switch back and forth between them.
The regression analyses were described thoroughly, but I didn’t find a clear explanation of
their purpose, the covariates that they wanted to adjust for and why, or a description of
the independent and dependent variables.
The process of extracting promotion criteria from university documents was exemplary,
with duplicate data extraction and resolution of disagreement by a third reviewer.
