General comment - The topic is certainly of interest to a broad clinical audience. The methods are
sound, and the presentation is generally clear. I do, however, have significant concerns about some
aspects of the presentation.
Major points:
1)
The presentation of “results” regarding PPV is problematic, as reported PPV is actually a
combination of an observed result (specificity) and a somewhat arbitrary supplied parameter (expected
prevalence). PPV will, of course, vary widely with presumed prevalence. Two options are possible. The
authors could revise this section of the results to clarify what is observed and what is presumed (e.g.
“When sensitivity and specificity were used to calculate expected positive predictive value at varying
levels of depression prevalence, PPV ranged from * at a prevalence of * to * at a prevalence of *”). OR,
the authors could move this presentation to the discussion and acknowledge that PPV calculation reflects
observed results and expectations/assumptions based on other data sources. Major depression

prevalence in this dataset is not the appropriate anchor for estimating PPV as it is an artifact of these
specific studies.
2)
The mentions of PPV in the discussion should be similarly tempered. Whether a PPV of
approximately 50% is “low” depends on the purpose of the assessment. And the discussion of “false
positives” should clearly acknowledge that these are almost entirely people with symptoms of depression
that lie below the (arbitrary) diagnostic threshold. A “false positive” screen for depression is not at all
the same as a “false positive” screen for cancer of HIV. Subthreshold depression is often still deserving
of clinical attention.
3)
The discussion should give significant attention to imperfect accuracy of the “gold standard”
clinical interviews used to assess PHQ9 performance. Data regarding test-retest reliability of these
clinical assessments should certainly be cited. Given imperfect test-retest reliability of these “gold
standards”, I suspect that performance (sensitivity/specificity or AUC) of the PHQ9 actually approaches
the maximum that could possibly be observed.
4)
The section on accuracy among subgroups (page 24) seems confusing (at least to me). The
authors mention that there are really no substantive differences across subgroups and then mention
several “statistically significant” differences that are probably not meaningful. I think it would be clearer
to simply summarize and refer readers to an appendix. And a detailed presentation in an appendix
should probably emphasize estimated differences in performance with confidence limits rather than
“statistically significant” differences. In such a large sample, p-values have less true value.
Minor points:
1)
The wording of the first sentence on page 23 could be confusing. Better to say “Sensitivity was
greater when compared to semi-structured interviews than when compared to fully structured
interviews…”
2)
I believe there is a word missing in line 15 on page 23 (3rd sentence of paragraph)
