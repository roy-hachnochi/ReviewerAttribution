The authors present an IPMA of diagnostic data. Unfortunately, I found the manuscript was hard to
follow with no clear message appearing due to the multiple separate analyses that did not always seem
to relate to addressing an overall research question. Some specific points to address are given below:
1.
The authors cite appropriate methodology for their meta-analyses but should state the number
of quadrature points used within their models and the means they used to ensure that this provided
sufficient accuracy.
2.

Tables are wrongly referenced.

3.
The authors first compare statistics across cut-points (table 3a) and select that which maximises
the sensitivity + specificity. Thus equal weighting is given to false negatives and positives, but there is
no justification for this choice. Clinically I would imagine one mistake was more important than the
other. The authors must justify this approach if used. Furthermore, whilst 10 might be the derived
cut-point (using this potentially flawed approach) from table 3a measures, it would not select this value
using the MINI reference standard (table 3b) nor for all the subâ€”categories in etables 3.
4.
Tables 3a and 3b would be best combined to avoid replicating the Semi-structured reference
values. Figure 1 does not add to the information in these tables.
5.
eTables3 which refer to factors with more than 2 categories similarly repeat information. Tables
would be better formed with the 3 categories shown on the same table, which will require some
re-organisation of the data given, but should be perfectly possible. (For example, having estimate (ci) in
same columns rather than separating out.)
6.
It is unclear how any weighting from the studies might impact on results. The individual study
prevalences are not incorporated since sensitivities and specificities are estimated and then combined to
give PPV and NPV estimates based on these and not individual datasets with their inherent prevalences.
7.
Comparisons are made between those not currently diagnosed and all participants, yet this
latter group contains the former. Those not currently diagnosed should be compared to those diagnosed,
so that each patient belongs in only one of the comparison groups (etables 3b, 3q and 3j).
8.
The authors present forest plots showing the study results between studies and then make
statements about the differences in sensitivity and specificity using bootstrap approaches. The numerous
figures and tables with separate analyses within categories are not easy to interpret and do not give an
overall picture of the relationship between sensitivities, specificities and other patient features. Given the
authors have IPD, a random effects regression model could be used to properly investigate the

relationships and to determine what are the important features that might impact sensitivity and
specificity. Clustering would be by study and interaction terms could also be investigated. This approach
negates the need for an arbitrary dichotomy of age and allows for a better characterisation of this too.
The results from the model should be presented in the body of the paper and will inform the conclusions.
9.
The authors make statements that sensitivity and specificity differ significantly between some
subgroups and yet this information is not related to estimates of the differences with confidence
intervals. Precision estimates should be given for all such comparisons made. This would be a natural
by-product of a regression approach as outlined above. (Perhaps giving univariable differences firstly,
followed by multivariable model results.)
10.
Furthermore, the authors state that no comparisons found to be significantly different in one
reference category were statistically significant in either of the other two, yet no formal comparison is
made of these. If such findings are of importance, then there must be formal comparison and confidence
intervals given for the differences. The regression approach could also incorporate terms for reference
standard and make direct investigation of this in relation to patient features too.
11.
The authors give evidence in the introduction that the results of the 3 reference standards are
expected to differ, with some being more accurate or over-inclusive than others. I do not agree
therefore with the statement made in the first paragraph of the discussion that non-replication of
differences between subgroups across reference standards infers that the PHQ-9 performs similarly for
patient sub-populations. (It might be surprising to find the same results across difference reference
standards and furthermore, no formal comparison is made of whether the results are actually different
over and above some cross the p<0.05 and others do not.)
12.
There should be some discussion about the validity of using a single cut-off value. Given the
currently available technology for Apps etc. which allow a clinician to quickly assimilate larger amounts
of data, is such a binary approach warranted? The approach mooted does not distinguish between an
individual who scores 1 or one who scores 9, yet the latter is probably far more likely to have major
depression. It is a simple matter to convert the scores to the probability of depression, which will
increase the higher the score. Adding more patient features into this model would give greater accuracy
of diagnosis. (For example, a logistic regression model can be used to give the odds of depression in a
male, aged 72, with a PHQ-9 score of 8, which may differ to the odds of a 40 year old male with the
same score.)
