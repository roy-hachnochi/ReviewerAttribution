This is a well-written, methodologically rigorous and timely paper on the quality of available clinical
practice guidelines for the treatment of COVID-19. The study is important as guidance should be
evidence-based as much as possible and that is difficult in the midst of a pandemic that is tying up the
health workforce and expertise used to inform clinical practice guidelines. Guidelines need to be
developed rapidly and in-step with the emerging evidence. "Living" guidelines are important in this
context and work is underway in Australia to produce these using the MAGIC app platform (see
https://covid19evidence.net.au/).
The abstract and introduction of the paper are clearly and cogently laid out and the purpose of the paper
is well justified. The only thing that was presented in the abstract which was not clearly explained in the
main body of the paper was the assertion that COVID-19 guidelines were compared to MERS and SARS
guidelines. I did not see that done in the main body of the paper.

The methods were generally appropriate for a rapid review with an acceptable trade-off between rapidity
and rigour. It was pleasing to see the attention paid to grey literature searches to support the lack of
information in bibliographic databases at this point. Some specific points about the methods:
- the Guidelines International Network (G-I-N) library was not searched. G-I-N also has a COVID-19
resource page that would have been helpful to search.
- the searches appear to have been done before the NICE rapid guidelines became publicly available (9
to date) and before the living guidelines produced in Australia (by Cochrane) were also available. It
would be interesting to see whether these newer guidelines would be assessed as having higher rigour
than the ones identified in the paper even though they were only released 1-3 weeks later than the
evidence collated.
- the literature search in the appendix indicates the searches were conducted on 6/2/20, although the
methods section indicates the searches were conducted on 14/3/20. Which is correct?
- the literature search in the appendix included a line for identifying diagnostic and testing evidence,
although the scope of the rapid review excluded diagnostic/testing clinical practice guidelines. Did the
scope of the full systematic review differ in terms of the nested rapid review in terms of types of eligible
clinical practice guidelines?
- Was the target audience of the guideline extracted at any point?
- The use of the AGREE II tool for appraising the quality of the guidelines was appropriate. Two
appraisers were used in the study (presumably because this was a rapid review), although 4 is
preferred. How much agreement was there between the appraisers and what was the level of experience
of those doing the appraisal? (ie novice or experienced in evidence synthesis? This is particularly
important for the assessment of rigour of the guidelines.
- As the review was done using DistillSR, could you use the function in this software to produce
inter-rater reliability assessments (kappa scores) for screening and inclusion of guidelines in the
evidence base? Did you use the artificial-intelligence based second screener or error check functions of
DistillSR as a quality control measure for the screening and inclusion of guidelines into the evidence
base?
The results were generally well presented. I particularly liked the tables and figures and how they
synthesised the information extracted. Specific comments:
- Table 3. It would be helpful if the table indicated which guideline related to COVID-19, MERS or SARS.
It would also be helpful if abbreviations were defined at the bottom of the table. In the row for the
France guideline in column 3 suggest re-wording to "First line treatment chosen because readily
available". I also suggest there should be better alignment between the recommendation and the
treatment in the rows ie in the Netherlands is it only oseltamivir that is not recommended? In Spain is it
only remdosivir that is recommended for compassionate use?
- Table 4. Define abbreviations at the bottom of the table. I would also be interested to know whether
the sparse evidence used to support recommendations in some of the guidelines was actually empirical
evidence or just opinion.
- Figure 2. Good presentation. Would have been helpful, however, if it was clearer as to the scale for a
"perfect" guideline (600%) instead of the 300% presented. It tends to give a misleading impression that
the guidelines are of good quality, unless you read the text which makes the limitations much clearer.
- Figure 3. There was a lot of variability in the key domain of rigour of development. Are you able to
explain what were the key deficiencies in the subdomains comprising that domain? Can you also explain
the meaning of the vertical width of the box in the graph? The vertical lines appear to indicate the range,
and the horizontal the mean (I am assuming the combined score between appraisers was averaged?).
It would help with interpreting the results.
- Table 7. Is it possible to put the AGREE II quality assessment scores in this table? The text indicates
these guidelines are of high quality but that is not quantified and cannot be compared with respect to
the guideline quality given in Figure 3.
The discussion included many excellent points about the importance of clear links between the evidence
base (if there is any) and recommendations, and of explicating how decisions are made to recommend
specific practices. Audit and update of guidelines are critically important in the context of a rapidly
evolving evidence base so I would suggest you mention the possibility of using living guidelines in
emergency/pandemic situations.

The lack of mention in the guidelines about care of the elderly was very surprising given this is the
demographic most severely affected by COVID-19. On page 21, line 43 there is a suggestion that GRADE
should be used to adapt guidelines. I would suggest that you mention a range of methods - namely, the
pioneering guideline adaptation method produced by the ADAPTE Collaboration (Darzi et al 2017;
ADAPTE Collaboration, 2009), the modified version of this produced by CAN-IMPLEMENT (Harrison et al
2013), as well as the GRADE-ADOLOPMENT framework (Schunemann et al 2017) - noting that the
efficiency of the latter relies on the availability of GRADE Evidence to Decision (EtD) tables for each
recommendation (Neumann et al 2016) and these might not be available.
In general, the suggestions I have made are minor and I would recommend the publication of this
manuscript. It is important for clinicians to understand the credibility of the guidance they are receiving
on the treatment of COVID-19.
