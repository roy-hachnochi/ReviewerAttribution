Dear Editor and Authors,
Thank you for the opportunity to comment on this excellent article, and I commend the author group for
completing such a good review in the very limited timeframe they had available.
This systematic review and meta–analysis of diagnostic test accuracy is an important contribution to our
knowledge base for the performance of tests to diagnose COVID19, and will be of great interest and
importance to all healthcare professionals, teachers, the public and (hopefully) to policymakers. The
ability of the BMJ to reach all of these audiences makes it the right destination for the authors’ important
findings to be disseminated to all these groups.
I am an experienced test evaluation methodologist, though with limited statistical expertise and so I
have not commented on the adequacy of the meta–analytic techniques employed.
***Please note: where I refer to page numbers, I am using the page in lower right of manuscript text
pages (doesn’t agree with the ‘Page x of 51’ at top left).
The authors have done a great job to present these many results in a succinct manner, pulling out the
most important findings in their discussion and abstract. They have defined their research question well,
and have used good methods for searching, sifting papers, and data extraction.
All key results are present in the abstract, results and discussion with one exception – the finding that
tests are more sensitive at 3weeks+ is absent in the abstract and should be added. In my opinion the
authors do very well not to report the pooled estimates in the summary box, as the key takeaway
message should rightly focus on the absence of evidence of a high enough quality to conclude on the
performance of these tests that is generalisable to the intended care pathway.
The review addresses a complicated topic, and provides lots of results which are difficult to take in even
for a specialist. The authors have written an excellent, clear and succinct technical interpretation of their
results in the discussion; however considering the BMJ’s target audience the paper would benefit highly
from providing simple explanations of what the pooled results would mean to practicing clinicians, as
well as to other professionals and the the public. For example, a ‘pooled sensitivity of LFIA’s of 68%’
(primary outcome) means we can expect that on average 32% of people who have had COVID19 would
be falsely identified as not having had the infection, while the test’s pooled specificity of 97% means 3%
of those who have not had the infection would be given false 'reassurance' that they have had COVID19.
This would fit in well in the discussion p.10 line 15 after the current statement “we observed an
important false negative rate for ELISA and LFIA even at this time point”. One approach could be to
consider the Cochrane DTA methods for transmitting this understanding to a non–technical audience
(summary of findings tables), since they allow an estimation of numbers of people to whom these
scenarios would apply given the median prevalence of COVID19 across all studies included in each
meta–analysis. Another approach would be to summarise your findings in 1 or more summary ROC
plots, including the pooled estimate (with confidence region).
I am surprised that so many studies appear to have used adequate reference standard tests (75% of
your includes, p9 line 6–7), and ask if you could report which criteria you have used to judge a ref st as
low risk of bias? This is also a critical detail for interpreting the performance of the index tests (and so is
important to report somewhere in the paper/supplementary materials), since RT–PCR is suspected to
have a high false–negative proportion and so a more reliable ‘disease negative’ determination is made
from two consecutively negative PCR results.

Double–counting in the post–hoc analysis (Methods p.6 lines 6–13; Results p.8 lines 32–41): I was a
little confused as to the meaning of this additional sensitivity analysis. My understanding is that more
than one 2x2 per study was included in this analysis for some studies (those that did not report 2x2s for
the threshold IgM and/or IgG positive), which results in double–counting patient test results from some
studies (those likely to be more poorly reported). Methodologically this is problematic (since the IgM and
IgG results are dependent in the same individuals) and in terms of interpretation I am not sure they add
valuable (clinically interpretable) meaning. Furthermore while the results are reported in the text, no
interpretation is given; I would recommend either adding an interpretation or removing this set of
analyses altogether.
Use of Tau statistic: Reporting the Tau did not offer me any additional interpretation of the clear
heterogeneity between included studies (probably as I am not a statistician so the statistic has no
intuitive meaning to me I’m afraid!). Perhaps a ROC plot of estimates for the 3 test types (primary
analysis only, IgM or IgG threshold) including a 95% prediction region would display between study
heterogeneity well to a non–statistical audience.
Finally, I wonder whether the authors would be prepared to make a stronger conclusion in the main text,
reflecting the strong conclusion they rightly make in their abstract? This last comment is a ponderance
and not a recommendation!
Minor points / typos:
The authors report Ig class analyses as IgM, IgG, or both. For the latter (‘both’), it would add clarity to
the reader if this were consistently described as ‘positive by IgM or IgG’, or similar (abstract p.28 line
28; Methods p.6 line 5; Results p.8 line 26).
Methods > Data analysis – please consider re–stating your primary analysis
Methods > Data analysis p.6 line 16–17 – please consider revising ‘stratum–specific pooled sensitivity
and specificity’ to something less technical, e.g. ‘we compared pooled se and sp across subgroups for
the following study characteristics:…’
Results p.7 lines 43–44: % evaluating commercial kits appears to be incorrect (61%), should be 56%?
Results p.8 lines 32–41: if results of the post–hoc analysis are not removed, please also report the
number of arms analysed here (currently only reported in table S6).
Discussion > review limitations p.10: since Embase does not appear to have been used in your search
strategy, please consider listing this as a study limitation.
Patient vs samples analysis: Need a sentence in main text somewhere highlighting that n studies did not
provide the number of patients analysed (n samples only), so if there are more than 1 sample per
patient this would affect accuracy estimate in those studies.
