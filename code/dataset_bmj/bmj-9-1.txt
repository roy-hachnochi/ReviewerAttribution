Assuming the authors' arguments are correct (EWS have a big impact on patients, are used
extensively, and have not recently been comprehensively reviewed), this paper addresses
a relevant issue. This review illustrates that methodology and reporting are poor for EWS.
Several similar reviews already exist for other application areas of predictive modelling.
Given that, in my view, the situation is not improving dramatically, studies like this one are
still very welcome.
I found the paper interesting, and enjoyed reading it. I have many comments for
clarifications and additional items to report, which I hope are helpful for the authors to
further improve this work.
- Perhaps mention in abstract that EWS are prediction models?
- P2, line 31-33: consider to disentangle 34 and 84 studies into 11 studies with
development only, 23 with development + external validation, and 61 external validation
studies.
- There are more validation studies than development studies, and Fig 3 shows no EWS
with 0 validations: is that merely a result of the study eligibility criteria, or is validation
more common for EWS than for other prediction models?
- P2, line 52-3, “internal validation was carried out in 19 studies”: that is 19/34, right? 15
did not perform internal validation, but these perhaps these included external validation
and therefore thought it was not necessary to also perform internal validation? See also
p15.
- Introduction: Smith et al is a fairly recent SR (2014), so please indicate the specific
added value of the current review over the one from Smith.
- Introduction: “the available analysis datasets may include multiple measurements per
patient”, is this what is later referred to as >1 observation per patient?
- Introduction: The authors write that the most appropriate way to analyse data with
multiple measurements is not clear, here the authors may refer to work from Goldstein and
Pencina (JAMIA 2017, Stat Med 2017)?
- Introduction: I would think that EWS would typically be developed and validated using
EHR, and hence be retrospective. Later, the authors mention several prospective studies.
That may deserve a comment.
- Eligibility criteria: did the authors explicitly focus on models predicting risk (irrespective
of whether they were simplified at the end)?
- P6, line 36-37, “(with the exception of the original EWS)”: this was confusing to me, not
sure what the authors are trying to say.
- Study selection and data extraction: how were conflicts in selection/extraction resolved?
- Assessment of bias: I’d prefer to have the 23 PROBAST items listed in appendix, perhaps
with detailed results for them (cf comment below).
- P9, line 14: 23 described development and ‘external’ validation
- P9, line 20-21: 93 used a patient dataset, what did the two other studies do?
- 3 development studies were based on clinical consensus: I would like to have more
detailed information on what that meant for each of these studies.
- Analogous comment for the 2 studies that present a modification of an existing score.
What happened? Was some kind of model updating used?
- The authors state on p9 that the modelling approach was given in table 1, but it seems to
be in Table C in appendix?
- P13: it would be interesting to describe whether and how papers justified the choice of
outcome and time horizon.
- P13, section ‘predictors’: the authors report a median of 12 candidate predictors and a
median of 7 predictors in the final model. As a reader, I wanted to know how many studies
did variable selection, however this information only came later.
- P13, ‘it was not always clear whether…’: please give numbers?
- I am confused about the issue of multiple observations. Do the authors refer to repeated
measurements of the predictors, or multiple episodes (e.g. stays leading to the event not).
If it refers to episodes, should the median number of events at the observation level (284)

not be higher than the median number of events at the patient level (396)? Or is this
discrepancy caused by the fact that both numbers are not based on the same set of
studies?
- P13, ‘studies using only one observation set per patient usually used the first recorded
observation’. This relates to 9/15 studies, please report what the other 6 studies did. Same
comment for external validation studies (p23). See also Tables D and I, where ‘other’ is
written.
- P14, missing data: why do the authors focus on the 29 studies that used statistical
methods, and not on all 34 development studies?
- P14, model building: I am confused by the number on the various modelling approaches.
On p9, the authors write that 23/29 used a regression modelling approach, and the
remaining six used a variety of methods. On p14, the authors write that 15 of 23 that used
a regression approach used logistic regression; that 4 used machine learning, and 2 used
Cox regression. In table C, I count this: 6 NA, 15 logistic, 3 tree, 1 Cox, 1 multinomial LR,
1 NB/logistic, 1 discrete time LR, 1 ANN. I cannot reconcile p9, p14, and Table C. What do
the authors mean with 6 NA? Did 6 studies not state their approach? If 15 of 23 that used
a regression approach used logistic regression, which are the other 8/23? The authors
mention 2 Cox regressions in text, but I only see 1 in Table C? What is NB/logistic? Do the
authors count machine learning as a regression modelling approach, so you have 23
‘regression modeling’ vs 6 NA? Please clarify.
- P14, model building: apparently (table C), for 6 studies it is unclear what modelling
approach was used and whether variable selection was done. That’s 21% of 29 studies,
isn’t that worth mentioning? Appalling!
- P14, model building: in Table C I only notice 3 with ‘all significant in univariable’, whereas
4 is reported in the text. Perhaps that one study used univariable selection followed by
backward? Also, no study used stepwise selection?
- P14, model building: only 2/11 studies used a statistical adjustment for using multiple
observations per patient. What adjustment?
- P14, model building: please refer to supplementary material for detailed information. This
comment also applies to other results sections.
- P14, handling of continuous predictors. One study has ‘other, forest’ in Table C. Does this
study then use random forest (modelling approach says ‘tree’)?
- P15, model presentation: again, I am confused with what the authors refer to as
regression models. It seems that trees and ANN are included, but I am not sure. Anyway,
where trees presented in full detail? Was the ANN model in some way accessible?
- P15, model presentation: I’d like to see more information on what kind of simplified
methods/models were created. In the end, that is probably what will be used in practice,
right? What was the range of points-based systems, and did any study try to estimate
outcome incidence conditional on the number of points?
- P15, apparent performance: what calibration method was used in that one study that did
not use the HL test?
- It would be interesting to have some information about how risk thresholds were defined
to calculate sensitivity etc.
- P15, internal validation: the authors report 13 studies with split sample, 2 with
bootstrapping, and 2 with CV, which sums to 17. What did the two other studies do for
internal validation? In Table G, this is only referred to as ‘other’, but this needs to be
explained.
- Studies from Jones, Kyriacos, and Tarassenko do not have apparent, internal, or external
validation. So what did these studies do? Are these perhaps studies using ‘clinical
consensus’, or modification of an existing score?
- P22: the term ‘event patients’ was unclear to me. Is this defined anywhere? (It is also
used elsewhere in the text.)
- P23, statistical methods: 35 studies used CCA, 2 had no missing values, and 1 used MI.
What did the others do?
- P23, predictive performance: all but one study used the c statistic for discrimination, I’m
curious what that one other study used (could be added to Table K, for example). Then, for

the 11 studies using the HL test, the denominator was not 15 (studies addressing
calibration)?
- Overall, I missed information on how many studies used multicentre (or otherwise
clustered) data.
- P23, PROBAST: I always feel that summaries of PROBAST assessments are quite general.
Is it not possible to add a detailed assessment (e.g. by PROBAST item, and by whether the
study did development, dev + external validation, or external validation alone)? The
discussion reports that all studies were at high risk of bias, but that is not clear from p23.
Also, where do the authors define when a study is considered as high risk of bias overall?
Is the definition that the classification is not ‘low risk’ for all 4 PROBAST domains?
- P24: I prefer to keep R2 and clinical utility separate, as they have very different
objectives. To me, it is more relevant to state how often utility was assessed, but I am not
so worried about R2 (or Brier).
- Box 1: to me, it would make sense to explicitly mention TRIPOD here. It could even be a
separate recommendation? Also, the authors recommend reporting R2 or Brier, but how
essential is this (cf also bottom of p30)?
- P27, ‘use a sufficiently large sample size’: isn’t it worth mentioning the fact that using
data driven variable selection impacts on the required sample size? To me, a
recommendation would be that data driven selection should be avoided where reasonably
possible (depending on domain knowledge and available sample size), not sure what the
authors’ position is regarding this issue? Further, the authors state that the number of
events is critical, but perhaps that is too strong given the new guidance that the authors
mention (they refer to Riley et al, but van Smeden et al in Stat Meth Med Res is also very
instructive on this topic).
- P28, account for missing data: it may be of interest to stress that MI, although widely
regarded as the best approach, should be used mindlessly. I’d mention that there should
always be an honest interpretation of likely missingness mechanism (with honest I refer to
my impression that some people are keen to assume that it is mostly MCAR), and a
thoughtful choice for and setup of an MI procedure. And the procedure should be reported
in detail.
- P28, time horizons: long-term horizons are criticized by the authors. Is there any relevant
literature they could cite to back-up their claim?
- P28, best practice statistics: the authors give the impression that interactions should be
liberally addressed. I’m tempted to stress that this should be restricted to what makes
most sense a priori.
- P29 at the bottom: first the authors write that bootstrapping and CV are two preferred
approaches, but then they recommend bootstrapping. Perhaps they can explain their
preference of bootstrapping over CV. Also, not sure whether they intended to cite ref 145
(Steyerberg Stat Med 2014). I’d expect Steyerberg (JCE 2001).
- P30, typo: prediction models should be prediction model (line13-14).
- P30, when citing 146-148, they may add 18 as well.
- When discussing calibration, I take the freedom to mention that my paper in JCE (2016)
is relevant here.
- P47, figure 1: I did not understand what ‘validation of non-review EWSs’ referred to.
- Supplementary Tables C and J: ‘single imputation’ can mean so much, could that be
specified? Also, could ‘other’ be specified? If MI, was it always clear how this was done? My
experience is that explanation of the MI procedure is poor, if not absent.
