This paper is related to a previously reviewed MID inventory paper. Also with this current
paper I have quite a number of questions, primarily on the methods and the results.
Title:
• Title says development and reliability of an instrument but the development is not at all
well described.

Abstract:
• Under heading design, it is said that the results are on the basis of a literature review. A
search strategy for the inventory paper is presented in an appendix but it is unclear how the
authors developed initial criteria from this search?
Methods:
• In methods it is said that from the search for the inventory paper, the authors “identified
and reviewed methods articles addressing MID estimation using anchor-based approaches”
and then thematic analysis technique was used. How this actually was done is not described,
there is no figure over papers and no description of inclusion and exclusion criteria for the
identified papers.
• Nowell 2017 is used a methods reference but nothing more is said about the thematic
analysis, how it was performed etc.
• The authors used own experience, ref 3, 9-15, when they developed initial criteria – still
this must be described!
• Under heading Face and content validity, these initial criteria become an “instrument” –
unclear how.
• This instrument was reviewed for clarity by a group of experts but we have no information
about numbers and professions of this group. No patients seem to have been involved,
though. This must be described!
• Then they “achieved consensus” – but unclear who were in this group, the authors?
• An early version of the instrument is published earlier (Ebrahim 2017) but the search by
Ebrahim was directed against instruments in pediatric care, did they use the same search?
Also in the paper by Ebrahim et al 2017 it is very unclear how the authors developed
credibility criteria (“Based on this initial survey of the literature and our group’s
experience”).
Results:
• In results it is said that 41 relevant MID method articles were found but it is not described
how they were used in the study. If these 41 papers were identified through the literature
search, this should be described with inclusion and exclusion criteria and a flow chart. I am
very unsure that all relevant papers in this topic have been assessed. For ex. Cosby et al
2003 wrote a methods paper discussing how to define and measure clinically meaningful
change including use of anchors, and there could be more paper that should have been
included.
• These 41 resulting articles are not described and it is very unclear how data were extracted
from them, only saying that they “informed the item generation stage of instrument
development.” How criteria were actually developed is not described.
The core criteria:
• Item 2: mortality is easy to understand but maybe not relevant for MID?
• Item 3: why is it better that the anchor has a correlation of ≥ 0.7 than of ≥ 0.5?
• Item 4: how have these thresholds been decided on, why ≤ 0.20%? And why combine the
assessment of variability with sample size? An unprecise MID I will have a good credibility
score in a have a sample size of 200?
• Item 5 is a little special, because reflecting on a small but important difference is part of
the MID definition. I don’t think the authors should use a part of the definition in their
instrument and require researchers to ask for permission to use it. This is common sense.
Examples of high or low credibility:
• In example 2 the MID is calculated as the mean change in those “improved”. That example
could include both patients with small and large improvement but still have high credibility?
Instead, example 3 is showed as having low credibility due to the fact that all being “worse”
were included.
Discussion:

• The authors argue they have developed an instrument to evaluate the design, conduct and
analysis of studies measuring anchor-based MIDs but I cannot see that this is underpinned
in their methods and results. It is not described how the instrument could be used. How can
the scores (if the answers would lead to scoring) show the quality of the design or conduct of
a study? Have all items equal impact on the assessment or not? Would items with different
response options give different scores?
• I question the “detailed examples” because they don’t give me a good picture of an actual
use of this instrument. I can respond to the questions but how should I interpret the
answers, how can I compare quality between studies?
• Other problems with assessing change are not discussed, for ex. that patients may change
their own internal standards over time.
References:
• Ref 17 seems to be identical with ref 82.
Table 1:
• In table 1, the threshold for a definitely precise MID is 20%, in the appendix it is 10%.
Appendix gives examples on how to answer the items but still nothing is explained about
interpretations of answers. You could give high scores (definitely yes) on all items and a no if
the change score is reflecting a MID!