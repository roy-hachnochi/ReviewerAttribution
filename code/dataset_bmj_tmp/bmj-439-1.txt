This manuscript presents the opinions of the GRADE Working Group regarding how to incorporate expert
opinion and expert evidence into clinical practice guideline development. As such, "it is what it is", by
which I mean it is the output of a group that has many smart people in it and has made many important

contributions to guideline development. Thus, on this basis alone it will be of interest to the audience of
guideline developers. Any comments I make are from the perspective of someone who might try and
operationalize these recommendations. I anticipate two problems I would have using this advice. First,
in my experience the difference between expert evidence and expert opinion is not the bright line that
this manuscript posits can be drawn. So, consider these statements, paraphrased from ones I have
heard over the past 30 years:
"In my last 100 cases, I've observed outcomes are better for patients treated with X as compared to Y'
"In my experience, outcomes are better for patients treated with X as compared to Y"
"I think outcomes are better for patients treated with X as compared to Y".
Which of these statements are expert evidence? Which are expert opinion? Does it matter?
Second, I have seen the forms the authors propose for use in collecting expert evidence, and they are
daunting, to say the least. I cannot imagine guideline panel members that I have worked with viewing
the completion of these forms with any enthusiasm. "A disadvantage is the additional time and
resources required to formally collect and appraise expert evidence" understates the potential downside
to trying to use the authors' methods. Above the sentence I just quoted they list 10 bullet points of
advantages. Can the authors provide any evidence that these advantages outweigh what is certain to
be viewed by guideline panelists as a disadvantage of the first order?
Lastly, I am a bit surprised by what is not included in this manuscript: namely, any discussion or even
mention of the best studied method for synthesizing expert judgment on medical interventions: the
RAND / UCLA Appropriateness Method. This method, which is explicitly about the "the effects of
interventions, the balance of benefits and harms" (which is what these authors state as the requirement
to formulate recommendations) has been the subject of numerous studies of its methodologic
properties, such that we know quite a bit about its test-retest reliability, its reproducibility, its sensitivity
to panel composition, face and construct validity, and even the predictive validity of the expert panel's
judgments, by which I mean that patients who are treated consistent with the group's judgments about
where the balance of benefits and harms lies for specific clinical situations have better outcomes than do
patients treated inconsistent with those judgments. The application of this method does not require the
detailed, parsed out data collection and evaluation that is required to operationalize the authors'
proposed methods (although it most definitely requires more effort than most guideline panelists are
used to having to put out from their prior experience on guideline panels conducted with informal
methods). If the authors are going to propose their method for collecting and analyzing expert opinion
and expert evidence, and particularly if that method involves filling out forms that might be viewed as
onerous by the panel members, don't they need to provide some evidence that their proposition has
evidence of better reproducibility and validity than does an existing method, or produces equivalent
results but can be accomplished with fewer resources?
