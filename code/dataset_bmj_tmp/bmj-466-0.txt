This is a clear and very well written manuscript that uses appropriate methods to evaluate the success of past IPDMAs in obtaining data. These findings are complemented and contextualised by a more qualitative and personal
reflection of 20 years experience of conducting IPD-MA in epilepsy. I have no substantive comments and believe
that, with current interest in IPD-MA and data sharing, that the paper will be of interest to BMJ readers.
Although the paper is able to comment on the data yield (where it is possible to calculate this), what it has
understandably not been able to do is to assess whether there are differences between the available and unavailable
data. Thus whether the data obtained and analysed are representative or whether they are unrepresentative and
consequently biased. The authors mention representativeness in the introduction, but it may be worth mentioning in
the strength and weakness section.
The discussion would be enhanced by the authors briefly reflecting on the correlations between IPD-MA attributes
and percentage of data retrieved (table 2). The reasons for yield being greater in IPD-MAs with an authorship policy
may warrant further consideration - because it is something that those carrying out IPD-MA can change. It would be
interesting to know what proportions of those IPD-MAs with such policies had group authorship. I suspect that
having an up-front policy that grants some form of group authorship may serve as an incentive to participate. The
presence of such a policy could also be a proxy for a well-designed project where project plans are described
clearly, thereby engendering confidence in entrusting data for analysis. A project that gives due consideration to
considering/rewarding others at the outset may also reflect a more collaborative ethos, which in my experience is
vital to IPD-MA success.
Minor specific comments:
In the ‘what is known’ section I would qualify the first bullet point by adding and the data obtained are
unrepresentative.
Page 6: I am curious about the IPD-MA with 16 participants ?!
Page 6: In the numbers reported it would be helpful to state (here or in the methods section) how projects that
carried out multiple IPD-MAs (series of related questions/comparisons reported in a single communication) were
counted – as one IPD-MA or as the number of individual comparisons.
Page 12/13 Data sharing platforms will only provide a clear communication between data requestors and providers
if facilitating mechanisms are included in the platform and if data providers use the platforms – something that is
not currently the case for academic trials. I suspect that for a variety of reasons academics may prefer to share data
on an individual basis for specific projects.