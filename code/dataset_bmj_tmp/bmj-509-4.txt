This study evaluated the impact of the introduction of the Hospital Value-Based Purchasing (HVBP) programme, i.e. a U.S.
hospital pay-for-performance programme introduced by Medicare, on patients’ 30-day-risk-adjusted mortality for three target
conditions. The results suggest that the difference in mortality trends between HVBP-participating hospitals and non-HVBP
participating hospitals was small and non-significant. Furthermore, the study found no subgroups of hospitals where HVBP was
associated with better outcomes, including poor performers at baseline.
The study tried to provide empirical evidence for an important and topical research question, i.e. whether and to what extend
the introduction of hospital pay-for-performance incentives have impact on patients’ health outcomes. This study uses a unique
national scope patient level data set to evaluate a particular hospital incentive policy. The research design is clear and well
thought. The arguments are clearly stated and the article is well written. Furthermore, I have no doubt about the great
importance of its impacts and policy implications.
I have a number of questions, mainly if not all, about the methodological part of this study for the editor and authors to
consider.
1. The study used patients’ 30-day risk adjusted mortality as the only measure for patients’ outcomes in the three targeted
conditions to evaluate the impact of the HVBP programme on patients’ outcomes. The ground for the choice of outcome
measure is not clear for me. It is therefore not clear that whether the non-responsive effect from the incentive programme
might be explained by the choice of patients’ outcome measures.
2. What aspects of the hospital quality that the HVBP programme particularly incentivise for? I am not familiar with the scoring
system in the HVBP programme. By reading the text of the paper only, it is unclear for me that whether there is a direct link
between (some aspects of the) HVBP incentives and 30 day risk adjusted mortality, and therefore whether one might expect a
significant reduction of the hospital mortality as the result of the introduction of the HVBP incentive programme.
3. In the first difference-in-differences analysis, the authors controlled for the changes of mortality from non-HVBP hospitals on
targeted conditions over time. Can you subtract further to control for the difference of mortality changes over time on nontargeted conditions between HVBP and non-HVBP hospitals? It is another trend that you might to consider. This is effectively a
triple difference analysis.
4. The authors self-critiqued their choice of control group in the Difference-in-Differences analysis as the first bullet point in the
limitation section. It is appreciated. Also it is recognised that the chosen hospitals for control groups might be the only
available hospitals that could be used as non-incentivised group. The authors might have no other choice for a better control
group but possibly have not exhausted yet the choice of methods. For example, it is clear that a number of characteristics
between the non HVBP incentive hospitals and HVBP incentivised hospitals are significantly different. It is appreciated that the
authors don’t want to try other more sophisticated modelling approaches like mapping to better control the underlining

differences between the control group and intervention group than a hospital fixed effect. However, given the importance of
the control group in any Difference-in-Differences analysis, I, as a reader, would appreciate an explicit full explanations about
the impacts your choice of control group on the results.
Minor comments
1. In the first paragraph of page 10, you mentioned that “… matching 733 hospitals exposed to HVBP one-to-one with hospitals
in the non-HVBP group.” How did you do the one-to-one matching, and matching by what?
2. I expect the last block of table 2 with sub-title of ”non-target conditions” report the same numbers as the first block of
Appendix table 6 with titled of “non-target conditions”. They are not. Did I misunderstand the structure of the results?
3. Contents list on page 22 is quite confusing.