This paper provides a descriptive analysis of the early clinical testing reported in the literature
for novel medical devices. This is an interesting and well-designed study, although the authors
could better showcase the findings of greatest clinical and policy import and better
contextualize these findings. Below are a few major and specific comments that the authors
could consider in revising their manuscript.
MAJOR COMMENTS
1. The authors report that the (limited) clinical testing reported in the literature is often not
accessible before new devices are cleared or approved by the US FDA. This is a novel finding
that merits more attention in the text and that extends the work by Zuckerman et al. (JAMA
Intern Med 2014;174:1781), which found that scientific data to support 510(k) device
clearances is frequently unavailable in the mandatory FDA summaries, and Chang et al. (BMJ
2015;350:h2613), which found that the key study results for many of the highest-risk
cardiovascular devices remain unpublished post-approval.
2. The finding mentioned in point 1 would be made stronger if the authors could characterize
the type of studies they identified for each of the approved devices. How many patients did
these studies enroll on average? What was the study design (randomization, masking,
controls)? Were larger, higher-quality studies conducted after approval, or were the initial
studies the best that clinicians got? This would provide some sense for the quality of the
evidence available when these devices enter the market. Some relevant cites: Dhruva et al.
(JAMA 2009;302:2679); Hwang et al. (BMJ 2014;348:g217); Rathi et al. (JAMA

2015;314:604)
3. The number of studies collected for PMA devices (n=17) seems very low. Chang et al. report
a publication rate for cardiovascular PMA devices of 57% (60/106). During the study period
(2000-2004), the FDA approved 216 new PMA devices, which suggests up to 100+ trials should
have appeared in the literature. Even if some of these are excluded due to studies published
before 2000 or after 2004, and even if there are varying rates of publication in noncardiovascular specialties, there is still quite a big gap between the authors’ figure and the
back-of-the-envelope calculation above. To validate their search strategy, the authors could
cross-check their results against 1 or 2 years-worth of PMA approvals (i.e., take the ~50 PMA
devices approved in 2002, search each for publications, and determine whether any were
missed)
MINOR COMMENTS
4. I would suggest revising the title, which currently does not convey the key aim or results of
the study (in particular making clear the focus was early / first human clinical testing)
5. Page 7, line 112 – suggest rephrasing to emphasize that the US has a central regulatory
body for premarket device review, whereas the EU and other jurisdictions lack such authority
6. Page 7, line 121 – an important caveat is that other studies have pointed to the importance
of academic and public-sector research in transformative drug therapies. See Stevens et al.
(NEJM 2011;364:535) and Kesselheim et al. (Health Aff 2015;34:286)
7. Page 8, line 142 – there needs to be more information on how the academic / industry tag or
“relationship” was determined. Was it on the basis of affiliation only, any financial disclosure,
provision of the device/intervention, or some combination of the above? (repeated for page 9,
line 179; page 13, line 290)
8. Page 10, line 192 – I presume the authors also checked search engines and other public
sources since some devices may have been discontinued, withdrawn, or recalled and would not
currently appear in the FDA database
9. The Results section was somewhat difficult to follow, and the presentation of the findings in
this section did not seem to mirror the findings highlighted in the abstract or Discussion. I’d
recommend reorganizing around the key points
10. Page 10, line 207 – the most frequent journals of publication seemed much less relevant
(and could be moved to an appendix) than the most frequent therapeutic areas (cardiovascular,
orthopedic, general / plastic surgery, as shown in Table 1)
11. Page 11, line 217 – should the denominators here be 99 instead of 218?
12. Page 11, line 217 – what was the range of approval years, or when was the latest approval
for one of the studied devices?
13. Page 11, line 219 – what was the lag between approval and publication specifically for the
43 devices that were approved before any clinical studies?
14. Page 11, line 222 – do you mean “approved / cleared by FDA”? the use of “translation” and
“approval” interchangeably here, and elsewhere in the manuscript, is confusing, and if this
terminology is retained, it should be better defined upfront
15. Page 12, line 245 – the IDEAL model could be explained for readers unfamiliar with this
concept
16. Page 12, line 258 – I am not convinced that this paper could be read-across to the
Contopoulos-Ioannidis paper, which appears primarily to include preclinical research (vs. the
exclusively in-human studies included here). This would bias the current results to a higher %
resulting in approved products relative to the C_I paper
17. References – I would recommend strengthening the references list. The BMJ has published
quite a bit recently on devices and transparency, for example, and most of those studies are
not cited. Other suggestions are listed above
18. Table 2 – percentages / p-values for the tests reported in Results would be helpful here