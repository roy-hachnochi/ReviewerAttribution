In their response to my previous comments, the authors have clarified that only the data from the Experimental phase were used to calculate prediction accuracy. However, if I now understand the analysis procedure correctly, there are serious concerns with the approach adopted. First, let me state what I now understand the analysis procedure to be: For each subject the PD values across the 20 trials were converted to z-scores. For each stimulus, the mean z-score was calculated. The sign of the mean z-score for each stimulus was used to make predictions. For each of the 20 trials, if the sign of the z-score on that trial was the same as for the mean z-score for that stimulus, a hit (correct prediction) was assigned. In contrast, if the sign of the z-score on that trial was the opposite as for the mean z-score for that stimulus, a miss (incorrect prediction) was assigned. For each stimulus the total hits and misses were calculated. Average hits (correct prediction) for each stimulus was calculated across subjects. If this is a correct description of the procedure, the problem is that the same data were used to determine the sign of the z-score that would be associated with a correct prediction and to determine the actual correct predictions. This will effectively guarantee a correct prediction rate above chance. To check if this is true, I quickly generated random data and used the analysis procedure as laid out above (see MATLAB code below). Across 10,000 iterations of 100 random subjects, the average accuracy was ~57% for each stimulus (standard deviation, 1.1%), remarkably similar to the values reported by the authors in their two studies. In this simulation, I assumed that all subjects contributed 20 trials, but in the actual data analyzed in the study, some subjects contributed fewer than 20 trials due to artifacts in the pupil measurements. If the above description of the analysis procedure is correct, then I think the authors have provided no evidence to support pupil dilation prediction of random events, with the results reflecting circularity in the analysis procedure. However, if the above description of the procedure is incorrect, the authors need to clarify exactly what the analysis procedure was, perhaps by providing their analysis scripts. MATLAB code: nTrials = 10; % 10 trials for each stimulus/condition for boot = 1:10000 % 10,000 iterations for bootstrapping for i = 1:100 % 100 subjects data = randn(nTrials,2); % generate random values for each trial meandata = squeeze(mean(data(:))); % calculate mean stddata = std(data(:)); % calculate standard deviation zdata = (data - meandata)/(stddata); % convert to z-scores meancond1 = mean(zdata(:,1)); % calculate mean for each stimulus/condition meancond2 = mean(zdata(:,2)); if meancond1 0 % evaluate sign of the mean values conscore = 1; % conscore indicates for which condition, positive z-values will indicate correctness conscoreB = 2; % conscoreB indicates for which condition, negative z-values will indicate correctness elseif meancond2 0 conscore = 2; conscoreB = 1; else error = They are equal % if mean z-values are equal, arbitrarily assign correctness conscore = 1; conscoreB = 2; end accScores(i) = sum(squeeze(zdata(:,conscore)) %calculate average correct for each condition for each subject accScoresB(i) = sum(squeeze(zdata(:,conscoreB)) end mAcc(boot) = mean(accScores); % calculate average correct for each condition across subjects for each iteration mAccB(boot) = mean(accScoresB); end meanBoot = mean(mAcc)% calculate mean correct for each condition across iterations meanBootB = mean(mAccB) stdBoot = std(mAcc)% calculate standard deviation for each condition across iterations stdBootB = std(mAccB)