At first glance, this appears to be a thorough demonstration that pupil diameter can be predictive of upcoming events. The study is a conceptual replication of an earlier study by the same group ( Tressoldi et al , 2011 ) and the manuscript contains two separate experiments, each with 100 participants. Further, the second experiment was registered at http://www.openscienceframework.org before data collection (although at the time of writing this, I was unable to access any public record of the registration on the web site). Finally, the authors analyzed their results using both frequentist and Bayesian approaches, producing similar results. However, the claim being made - that somehow pupil responses can predict upcoming random events - would be incredibly remarkable (if true). Given such a bold, and some would say impossible claim, it is critical to carefully examine the experimental design and the veracity of the results. To their credit, the authors have made much of the raw data available as well as the source code for the experiments. Here, Im going to just focus on evaluating the specific methods and analyses in this particular study, ignoring any evidence from prior work. Briefly, each experiment was composed of two phases: Preliminary Phase - The experimenters first measured individual participants pupil responses to two visual stimuli, a smiley face and a pointed gun (accompanied by the sound of a shot), which appeared following the presentation of a picture of a door. There were 10 trials of each condition presented in a random order (without replacement) and pupil diameter was measured during the presentation of the second stimulus (smiley or gun). Experimental Phase In two blocks of ten trials, participants were again presented with a picture of a door followed by a smiley or a gun, but the trials were fully randomized (no replacement) and pupil diameter was now measured during the period when the door was on the screen. The authors claim is that in the Experimental Phase, the pupil diameter measured during the presentation of the door is predictive of the upcoming (random) stimulus. Unfortunately, the method for generating and testing predictions is not entirely clear. My understanding is that for each phase of the experiment, they separately z-scored the data across all (both smiley and gun) trials. Then for the Preliminary Phase data, they determined whether the average z-score for smileys and guns was above or below zero. This was necessary because it appears there were individual differences in the relative pupil size for the two stimuli. The prediction for each Experimental Phase trial was then generated by determining whether the z-score for that trial was above or below zero, and assigning the corresponding stimulus that had a positive or negative mean in the Preliminary Phase as the predicted stimulus. This method does not generate trial-by-trial predictions since it simply assesses whether the pupil diameter for a given stimulus tended to be in the same half of the data in both the Preliminary and Experimental Phase. Using this method the authors then generated percentage hits, comparing the predicted and actual stimuli, and determined that they were significantly above chance (for both stimuli considered separately and combined) using both frequentist and Bayesian approaches. All results were replicated in the second (pre-registered) experiment. Concerns: There are many issues that could be raised about this study, questioning whether the result should be taken to reflect unconscious prediction of random events (many, including myself, would be unwilling to accept this interpretation) or whether it is an artifact of some experimental procedure. I will admit that I dont have a ready explanation for what sort of artifact can have produced these results (across two separate experiments), but its worth making a few points: Replication, in and of itself, does not prove this is a real effect. If there is a methodological or statistical artifact, this would readily replicate too. Replication is much more valuable when conducted by independent research groups. The authors could do much more to convince a skeptic as to the quality of their underlying data. As someone who has recently started to use pupil diameter as an experimental measure, Ive become aware of how noisy this measure can be from trial-to-trial and I find it extremely hard to believe that the signals will be sufficiently robust across such a small number of trials for any kind of subtle prediction, whether it be of a physical stimulus or an upcoming random event (as is claimed here). The authors have provided no information to attest to the quality of their pupil recordings. What is particularly surprising to me is that the pupil diameter data is from completely different time periods of a trial between the Preliminary and Experimental Phases, when there are great differences in the luminance of the stimuli that are on the screen at that time. What I would like to see is much more careful presentation and analysis of the raw pupil data. For example, time course plots of individual trials for both experimental phases, assessments of trail-to-trial variance etc. Aside from the lack of clarity on the method for generating predictions, there are a couple of other details that need to be more fully specified. Its not clear exactly what luminance measurements were taken of the stimuli and how they were calibrated for luminance. How were the center and periphery measurements taken? Were subjects asked to fixate, or were they free to move their eyes? Over what time period was the pupil diameter measured in each phase? Across the whole 5-second periods? What accounts for the individual differences in relative pupil diameter to the same exact stimuli with the same luminance properties? Providing this sort of information is critical if any other group is to try and replicate the current findings. Ultimately, these results do not convince me that it is possible for participants to unconsciously predict future random events. I view the results as a curiosity and suspect that there is some underlying methodological or analytical artifact that can explain the results but this is something that may be very hard to pick up from the manuscript, even though the authors have provided much of the raw data. I remember a story a former postdoc of mine told me about a behavioral experiment he ran, in which participants had to indicate whether two sequential stimuli were the same or different. He found that some subjects were incredibly fast and accurate at the task and couldnt work out how it was possible until one of his participants told him that he was listening to the sound of the computer disk spinning on different trials the computer would have to spin and load up a second image!