Thank you for the opportunity to review this article. It makes an important argument in a critical area of inquiry, and deserves publication. I have some specific suggestions for improvement below: " ...punitive measures against individuals committing research misconduct are neither sufficient nor useful because this is a systemic issue stemming from a lack of positive incentive. " Id agree that such measures are not sufficient, but what is the evidence that they are not useful? "From 19942003, 259 cases of misconduct were formally investigated by the Office of Research Integrity 5 . In contrast, ~480,000 papers funded by the NIH were published 6 . It would be impractical and ineffective to investigate why 70% of published findings are irreproducible, even though ultimately the ability to repeat and build upon prior work is the key component of research integrity that we should care about." While it is useful to discuss ORIs limited resources, there are more recent data on their investigations, for example Figure 4 of this paper: http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.1001563 . Id also make it clear that the ORI only has jurisdiction over fabrication, falsification, and plagiarism (FFP) e.g. scientific misconduct -- and there is no evidence that FFP is responsible for most irreproducibility. So I wouldnt rely on ORI stats for why its impractical and ineffective to investigate irreproducibility. "...peer review still relies on two or three peers who are unlikely to be qualified to assess every experimental technique in the study." I agree, but can the authors say more about how standardization of methodology design and required controls will solve this problem? "The recent ascent of crowd-sourced post publication peer reviews have identified manuscripts with problematic content, but they remain most active for articles on new techniques that other researchers are eager to replicate for their own experiments (e.g. http://www.ipscell.com/stap-new-data/ date accessed: 2014-04-28 and http://f1000research.com/articles/3-102/v1 date accessed: 2014-05-20 ). " While these two examples demonstrate cases in which post-publication peer reviews are " on new techniques that other researchers are eager to replicate for their own experiments, " Im not sure thats really where post-publication peer review is most active. I would mention PubPeer here, at the very least for context. The authors make a few comments about costs, which are welcome: ". ..it remains to be proven whether it will be a cost-effective mechanism to conduct direct replications ." and " Not identifying robust and reproducible research is very costly and impairs our ability to make effective progress against diseases like cancer in which we have already invested billions of dollars." It would be useful to try to estimate how much replication efforts will cost, and where this funding will come from. 