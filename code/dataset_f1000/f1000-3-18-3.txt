The authors have done the formidable job of running five programs that search for mutations in English text on 10 annotated full text articles (they call this "intrinsic") and also on articles that have been annotated by curators of two databases ("extrinsic"). The results make sense, they analyze them well, the article is very readable and the methods are suitable. The conclusions are relevant for everyone who is developing mutation mining tools, probably most users of text mining algorithms, and they are well justified. Why I have reservations: The authors were able to conduct this study thanks to a great practice in bioinformatics, namely to provide the source code of the program used in a scientific article. They were able to download the source code of at least three programs from other websites or supplemental data and were even able to benchmark a program which has not been published yet in an article (SETH), and found that it led the field. This shows how the state of the art progresses by sharing code (even without publications) - the authors did not have to write their own mutation finding tool, they could run existing programs on a new corpus. However, it is hard to understand why the authors do not do the same. Anyone who would like to inspect or extend the results of this article would have to redo the same work again: write parsers for five programs, convert both corpora, put the main corpus into a suitable format, including downloading some files from EuropePMC, others from PMC, and write a program to evaluate the differences. I fully agree with Referee 1 ( Philippe Thomas ) that when claims about performance of open source programs are made that have been written by various teams, then at least the corpus + annotations have to be added as a supplemental file. Otherwise it is very hard for a field to improve their algorithms and validate claims made in the benchmark. In this case, the code for running the mutation detection programs should also be provided. Philippe Thomas (the author of SETH) comment that the results of SETH should have been better for a particular case can only be answered by looking at the code, the version (or the github commit ID in case of pre-publication code) the authors used and the converter, to find out where the differences come from. For a reader, it is currently impossible to validate the observation about SETH. This could be really easy to change, if the authors just attached their corpus+program as a supplemental file or on github, like most other authors in this field today.