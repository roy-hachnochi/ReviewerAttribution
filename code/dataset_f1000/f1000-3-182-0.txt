I thank the authors for this newly revised version of their manuscript. Unfortunately, while they carried out some of the analyses we discussed, the evidence still does not support their claims, even though the claims have now been toned down. However, before I elaborate on this point I feel I must first discuss a more general point. It is not a secret that I am an outspoken critic of research, that is, experiments seeking to demonstrate the existence of purported extra sensory perception, telepathy, psychokinesis, precognition etc. My prior belief in such phenomena is presumably very different from that of the authors of this manuscript. But this raises an important question: What level of evidence should suffice to accept their findings and approve this manuscript? The way I see it, the purpose of this review is not to evaluate the claim that there is a telepathic link (the authors dont use this term but this is what is implied) between brains at any distance. The authors would need far more compelling evidence to convince me that this is actually a possibility but this is not really the claim put forth by this manuscript. Rather the claim is that there is an as yet unexplained phenomenon that appears like a telepathic link. The authors have however been very clear from the beginning that future research needs to strengthen this evidence. They write in the conclusions: Only multiple independent replications and further controls on further potential methodological and statistical artifacts can support this hypothesis But of course this statement applies to any scientific result. Only replication and appropriate controls can really strengthen our confidence in a hypothesis. However, this does not imply that we should accept every small result as tentative or preliminary result. Even an infinite number of replications would be completely worthless if the basic premise of the experiment is not sound. My review is therefore not about the evidence for the telepathy hypothesis but about the claim that something as-yet unexplained has been observed at all. I do not think the evidence supports this statement for the following reasons: 1. Main analysis is meaningless Most of my previous reviews focused on a major conceptual flaw in the main analysis used by the authors. They decode the presence of a signal using data within each individual and randomly splitting the data samples into training and test sets. My reanalysis with two different non-linear classifiers (a SVM similar to that used by the authors as well as a k-nearest-neighbour algorithm) suggests that training and testing on completely arbitrary stimulus labels produces almost perfect decoding performance, provided that the epochs are not too short (a few seconds seem to suffice). Presumably this is because temporal correlations between adjacent time samples (which I previously demonstrated exist in the raw data) will break down when labels are defined completely at random. The authors have not addressed this issue at all. They do not report any attempt to repeat my analysis using their classifier but only state that they repeat their original analysis five times to show it is robust. This does not address the issue, however, because it does not matter how the training and test data sets are randomised. It will produce similar findings every time because of temporal correlations between adjacent samples. In essence, this means that the main analysis, which remains the primary result in the new version of the manuscript, is completely confounded. Unless the authors can provide evidence that this does not happen for their own classifier, which to my understanding is based on a publicly available implementation of SVM (LibSVM) that in my experience does not differ dramatically from the one I used in my reanalyses, and, if so, a compelling explanation why that might be the case, I do not see any way this analysis can be accepted. It simply shows that decoding within participant works due to temporal correlations. Therefore this entire analysis should be removed. 2. Cross-participant analysis is confounded The authors performed an analysis we discussed in the comments on the previous version of this manuscript. They used data from the Sender in each pair to train the classifier and then decode using the data recorded from the Receiver. This would be a better test because it shouldnt be confounded by temporal correlations in each participants time series and it also directly tests the question whether there is actually any similarity in the EEG signals between the two brains. In my previous attempts to perform such an analysis, I only did this on a handful of pairs and found no evidence of above chance decoding. This is consistent with the authors new results that this Sender-to-Receiver decoding only works for 5 of the pairs. As I said in my previous review, considering that there are some correlations between the time series for different participants it isnt terribly surprising that there should also be some cross-participant decoding. The authors therefore also performed a control analysis by training on Senders from these 5 pairs showing cross-participant decoding but testing on unpaired Receivers. They report that training on Sender 13 results in similar accuracy when testing the classifier on data from Receiver 7, 8, 10, 14, 16, 17, and 18. What they do not mention is that these are all Receivers for whom the first stimulus period was perfectly correlated with that of Sender 13. For Sender 8 they report only good correspondence with Receiver 10 and for the remaining three pairs they reported no correspondences. This in itself appears questionable because inspection of the NewCoincidences files suggest that there appear to be some rather obvious in the decoding for training on Sender 8 with Receivers 2, 4, 5, 7, 9, 14, 17, 18, and 20. There are also many traces for which the decoded neither classified the trace as signal nor as silence. It still is unclear to me what this means. Typically a SVM classifier will assign either one or the other label so it is puzzling why these data are missing. By and large this also again raises the question why the authors chose to classify events based on these instead of actually quantifying the classification performance directly. Regardless of these issues, what these data show is that cross-participant decoding is not very impressive. Even if we accept that cross-participant decoding works only for 4 of the 20 pairs this still does not really suggest that something very unusual is happening. This only actually reflects a small number of events being classified correctly by my count 5 out of 34 signal events, if we follow the authors way to quantify . Importantly, the authors also perform a similar cross-pair analysis to quantify the strength of correlations between time series from different participants. This clearly suggests that the correlations they observed for the alpha and gamma bands within pairs are actually extremely close (and with overlapping confidence intervals) as those observed between pairs. This suggests that there are simply strong correlations in the EEG time series between different people regardless of whether they were paired up or not. These correlations are particularly strong for the alpha band, which makes sense considering the relaxing experimental conditions under which the recordings were conducted. It may also reflect anticipatory behaviour or imagery the participants could have engaged in a notion the authors reject as impossible although they didnt actually test this possibility. Or perhaps it could also relate to slow fluctuations in gamma power that this experimental design didnt control for because the stimulus protocol was always 30 s of signal followed by 60 s of silence. In any case, considering these correlations across pairs it seems not overly surprising that there should be some decoding when training on Senders and testing on Receivers in some pairs. 3. Other outstanding issues The authors also made no effort to address some of the perhaps more minor concerns. The binomial test on the decoding when both signal and silence events are combined is still statistically incorrect. It simply is not true that the chance level is 50% because there were fewer signal events than silence events. For unbalanced designs a classifier will frequently not assign events/trials with a probability of 50%. The appropriate way to test for significant decoding would be to use a permutation test. The fact that the Bayes Factor in this test was 360,625 should raise suspicion. Rather than being interpreted as overwhelming evidence such a result is usually either completely trivial or a hint of an artifact. The authors also still claim that the initial silence period was randomised to be 1, 2, or 3 minutes, which according to the traces is clearly not correct. The initial silence was either 1 or 2 minutes which means there was less randomisation than the authors claim. The authors also do not even begin to discuss the high colinearity between the stimulus protocols used in the 20 pairs and how this could have influenced the predictability of the design or even the correlation between whether any given epoch was a signal or silence period and the state of the EEG time series. The authors previously suggested that it would apparently be impossible for participants to predict the sequence. I am not convinced by this argument but even if we take it at face value this does not preclude order effects. Conclusion Even if some of these issues could be addressed by a further revision, as I discussed here and in my previous reviews, the numerous issues with inadequate randomisation and lack of control conditions lead me to think that only a complete overhaul of this experiment could begin to address these problems properly. More importantly, as this is now the third revision of the manuscript I feel confident in saying that the discussion is going in circles (two points are insufficient evidence to confirm circularity but three are just about enough). I am happy I was given the chance to review this manuscript. Looking into these raw data has been educational for me and this has been my first experience with a totally transparent review process. I have long advocated making review comments public. However, I believe a reviewers job is to provide an experts opinion about manuscript. It is not to make a judgment about whether a study should or should not have been published. With the F1000Research publishing model this is the decision of the readers, and I also do not believe that such a decision should be made solely based on one reviewers comments. In a situation like this, where two reviewers clearly disagree, ideally a third expert opinion should be sought to adjudicate. Alternatively, the readers may decide to weight the two reviews based on what they perceived is fair and accurate. In any case, I do not think I can contribute any more useful information to this discussion. I believe I have evaluated this manuscript as thoroughly as I can. Perhaps I missed or misunderstood something fundamental. The beauty of the transparent review process is that anyone can identify these shortcomings and use them to reach their final decision.