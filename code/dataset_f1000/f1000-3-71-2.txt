 Caveat: I lead a group that does scientific software development. I am interested in software development both from the conceptual and the technical side. However, I would not be a serious contender for a software engineering chair. As a consequence, I cannot claim full knowledge about the state of the art of software engineering processes. I agree to the following takeaways that I pull out of the paper: If you know you are building an application which is for other people than yourself, it makes sense to follow a software engineering process. Think about maintainability and sustainability of your software. Think about your users and write usable software. Operate using cyclic refinement The Butterfly model is an assembly of current software practices, emphasizing the needs of the scientific software developer. I see the strength of the article in giving a concise overview of what a scientific software developer should do. However, I disagree with some statements made in the paper. Some of these statements relate rather to anecdotal experience. This points to one of the weaknesses in the paper: There are statements made with respect to the scientific software domain that are not marked as anecdotal, but which are not backed by either empirical facts (e.g. questionnaires) or pointers to such facts in the literature. In more detail: Generally I agree, science is a fast-paced environment with rapidly changing requirements asking for suitable software engineering processes. Current software engineering and development The paper comprises a large number of citations. However, I do not agree with the insights taken by the authors from that literature. To my knowledge, the field of Software Engineering has realized that one key factors for success is if software matches the needs of users. Agile Development follows the view that often-times, users dont even know what they need until they are closely involved in development and see where their own ideas of their work (i.e. using the program) could fail. This is why there is less of an initial requirements analysis, but there is a series of sprints that each seek the implementation of features. Each feature implemented also influences the next stage of the requirements analysis. The authors of this paper see the lack of quality software production as one of the weaknesses of agile software engineering practices. However, Agile software engineering practices like XP and Scrum emphasize the importance of testing and other practices intended to increase the software quality. Such processes definitely do not disregard the production of quality production. So, to cut a long story short, I do not agree with the impression created in the paper that agile processes lead to muddle-through software that is unstable and short term due to the process. You criticize the "ripples" through the program that come from changes. Ripples is the word describing the needs for software changes due to one change elsewhere in the program e.g. the change in the view necessitated by changing a model. The paper defining ripples is from 1978, and you cite another paper about measuring ripples (2001). However to my knowledge, in agile development, refactoring (improving code quality without changing its semantic meaning) is seen as an effort that accompanies development, with a view on minimizing ripples. Unit, integration, acceptance tests are automated as far as possible so they can be run after almost every change. Ongoing refactoring without being able to test easily if the program still works is admittedly very hard. But tests well done can help with this. I think it would be useful for the reader if you would outline how ripples are relevant in XP or Scrum. The fact that user interaction is not written into the Scrum process does not mean that its not on the map. It is implicit in the requirements given by the users and users being part of the Agile team. Typically, users that are part of the Agile team will accept or not the software created. User tests can be part of the acceptance procedure, usability improvements can be features in a Scrum process. While user experience is put to the forefront, the paper is quite sparse on the processes and on citations regarding HCI. You state: " Unfortunately HCI is the most ignored and unattended phase of scientific software solution development ". To my impression, this is not the state of the art any more, and the EBI even have specialized UX personnel who can be called into projects. The authors then describe what they see as reasons for poor HCI properties of scientific software and state that no-one uses software with bad HCI and state that software with bad HCI wont be used. In my (anecdotal) experience, many software products start out as concrete problem solvers for their authors, with bad HCI (for others, as the original author writes the software tuned to his/her own needs) which then are successively refined to serve a larger public. Of course this process bears its problems for outside users. However, such software gets adopted, because of word of mouth. In the article, HCI "design patterns" are not laid out in sufficient detail. The normal forms (1NF) etc. could be either treated more shortly or in more detail. The enumerating of 1NF to 5NF and not saying what they signify is in my view not the right middle ground. --- " Real time examples " should be changed to "Real life examples" I think these examples are an interesting overview, but to my feeling they are not addressing what is needed to support the claims made about the Butterfly process. Rather than giving a list of software artifacts that were created using the process I would prefer more information about the outcome of the different phases. What were iterative improvements etc.? If you are using a ticketing systems and a versioning system, it is highly likely that you can generate from logs a compelling story that lets the readers know more about the Butterfly process in everyday life. In particular it would be interesting to learn more about how many people performed the processes, and how the roles were distributed. Such an approach would also benefit the software system comparison of systems that did not use Butterfly . Following the Butterfly paradigm will take time and effort. Was a sufficient amount of time available for the systems used for the overview? --- All in all, I find this paper is a mixture between an opinion piece, a methods piece, and an overview piece. Even if I do not agree with many claims made in the paper, I do think the question of whether scientific software engineering (or even some scientific domains) need special software engineering processes is an interesting one (albeit maybe not a new one). 