The paper How difficult is the validation of clinical biomarkers? by Jan Voskuil is timely in that as the author points out the burgeoning growth of biomarker assays particularly in chronic diseases and notably in cancer has led to the misinterpretation of both research and clinical data for all of the reasons pointed out by the author. As correctly noted by the author, there are huge variations in sample collection, storage, preparation, assay used, antibodies involved, and analysisall of which can lead to wide variability in results and confound the end user of such data. The author correctly points out steps that can be taken early on in the process that can minimize or preclude the accuracy of the results so obtained. There is a need for standardization. Furthermore, the statistical analyses of biomarker data need to be stipulated before the data are collected and not afterwards, using statistical software in a black-box manner to see what results might show statistical significance, even though the significance may be serendipitous or not clinically relevant ( Zapf et al. , 2015 ). The slavish adherence to standard software packages by many physicians and biomedical scientists would be amusing if the wider implications of their innumeracy were not so dire ( Partin et al ., 2013 ; Tuppin et al., 2012 ). In order for a biomarker to be useful, it must reflect a change in concentration in the media sampled with a change in disease status. It is frequently assumed that serum or blood are the best media for the study of biomarkers but because of the number of potentially confounding variables in serum or blood, tears and saliva, because they reflect intracellular fluids, might serve as better indicators of intracellular events long before these are reflected in the blood ( Pieragostino et al . 2015 ; Salvisberg et al ., 2014 ). I recommend acceptance of this very interesting paper for indexation.