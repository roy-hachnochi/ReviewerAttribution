This manuscript produced by the MARC consortium is an impressively comprehensive study of run-level performance of the Oxford Nanopore MinION. The consortium have decided to focus on tightly defined parameters by sequencing a single bacterial strain, Escherichia coli K-12 MG1655, using a specific library preparation protocol and identical run parameters. Although some minor variability following the standard protocol between labs was observed, this is unlikely to affect the results. It therefore provides the most extensive view of platform performance variability to date, focusing on a specific combination of library preparation chemistry (SQK-MAP-005) and flow cell type (R7.3). Its strengths are therefore in the emphasis on platform reproducibility. There are a few extra things that could have been done to make the results more akin to user experiences, such as not pre-filtering flow cells for those with 400 group 1 pores, but this is not a significant issue, but it would be nice to see a spread of performance of all tested flow cells. The performance reported is in line with our own experiences. For me, the most interesting/useful elements of the paper were: the generally consistent results in terms of data quality achieved by different labs the extensive variability in throughput between flow cells which all give good QC (g1400) values, suggesting that the QC stage is not a particularly reliable estimate of how well a flow cell will run Figure 1, which provides a useful summary figure (although please note caveats below) the close attention to detail to alignment methodologies the relationship between experiment run time, throughput and read quality, which suggest that a 48 hour workflow is not optimal, and pores should be remixed more frequently A frustration is that the underlying reasons for the variability in performance, with the exception of operational issues, are not really explained and therefore these results do not really help users plan how to mitigate the variability. This is not the authors fault but remains an issue for those planning experiments that require a particular yield. My major criticism is that the paper is over long and would have benefited from a good editor to try and reduce excessive verbiage. Sentences are often laborious and there is significant repetition throughout the manuscript. To pick on the first sentence of the manuscript: “the advent of a miniaturized DNA sequencing device with a high-throughput contextual sequencing capacity embodies the next generation of large scale sequencing tools”. This is fairly garbled. What is “contextual sequencing capacity”. And why does it “embody the next generation of large scale sequencing tools” ? A few hours with a proof reader would do wonders. Greater use of active voice would improve readability. But it is up to the authors to decide whether they want to spend more time revising the manuscript in this manner. Generally I am happy with the manuscript to be approved as it is but I would suggest addressing a few technical points: Figure 1. I could not figure out panels G or H easily. I could not figure out the relationship between the k-mers relating to each strand, they did not obviously seem to match up or be reverse complements of each other. Panel H I also cannot figure out how the 2D consensus sequence relates to the 1D reads. For example, why is there an insertion in the 2D sequence which does not have a corresponding alignment in the 1D? In the section relating to the consensus sequences, the method seems to suggest that nanopolish was used to create a consensus sequence, using the reference sequence as the input alignment? I am not sure this is a particularly meaningful process. Accuracy measures like this (and any reference based alignment) are likely to be skewed by ‘reference attraction’, particularly given the alignment settings used - a pure de novo assembly may have been a more robust measurement. If that is too much work, the data could have been used (separately, or in combination) to polish the assembly from the nanopolish paper. I wish the analysis had not made so much of the (ad hoc) separation of 2D pass and fail reads. In reality reads are in a continuum of quality and the pass filter is simply defined by the Metrichor workflow, which is presumably version dependent (and can be turned off). At the least a definition of the pass filter would be useful, from figure 12 it looks like it relates to a Q value of around 10. A detailed treatment of the “usability” of 2D pass versus 2D fail reads is missing, but this is probably out of scope in the paper. One laboratory, Lab 3 reported high levels of Pseudomonas contamination. Although reagent contamination is a possibility, these very high levels are very unlikely to be due to vendor reagent contamination. Had this laboratory specifically handled P. putida in the recent past? Minor nitpicking points: Inconsistent use of trademarks e.g. MinION(TM), MinION in two contiguous sentences. AGBT 2012 presentation has been posted in the F1000 channel and can be referenced. “beta-testing” - is this formally defined or vernacular? It is not clear how much of the description of the chemistry is informed guessing, based on company materials or from unpublished communications, it would be nice to clarify what is a definitive statement and what is speculative. Along that line, I did not realise that tethers are on both strand, is that definitely true? 512 channels - this nomenclature is confusing, especially when compared to ‘wells’. Could this be clarified as to what a channel is?