This article gives an insightful and valuable overview of the challenges and opportunities of adopting new practices of digital scholarship within AHSS research processes. Building on the work of the Wilsdon review and earlier studies into practices within arts, humanities and social sciences research (e.g. the Crossick review), this provides a timely description of the difficulties we face in implementing broad solutions to tricky problems within a diverse research base. The diversity of research is often name-checked by those looking at the whole system and seeking to improve the way it works, but perhaps not fully understood. The specific process-oriented examples and case studies revealed here provide important contextual information to inform the sensible and sensitive roll-out of modern research management tools and approaches it may be desirable, from a management and assessment perspective, to see universal adoption of ORCIDs, DOIs and so on, but this isnt as easy as it sounds, and this article helps to explain why this might be the case while providing helpful examples of where it has worked and suggestions for ways forward. A particular problem is the complex and finely balanced nature of the relationship between different facets of the research process. While ethics, IP, copyright, digitisation, licensing, identification, citation, metrics and credit are often thought of as somewhat bounded issues that can be solved by fixing the plumbing (e.g. by introducing ORCIDs), this article reminds us how complex their linkages are within the research process and how upsetting just one part of the balance can introduce vulnerabilities into the whole system. The examples given here about data management within arts disciplines are rich and informative, and justify a bottom-up approach to managing this agenda (as called for in the conclusion). It is already clear that means different things to different disciplines; even across (largely STEM) disciplines that generate numerical data as a primary output, one finds large variations in definitions, standards, practices and expectations that tend to muddle us. Extending the meaning of to include all inputs and outputs that inform and support the insights generated from the research process is a laudable aim of those seeking to increase the transparency, robustness, replicability, dissemination and impact of research; doing so in a way that take sufficient account of the complex dependencies between anonymity, confidentiality, intellectual property, ethical propriety and so on is a particular challenge within AHSS research and one that is perhaps not given sufficient attention by those operating at the level of research administration, assessment and policy development. Beyond data, the particular problems of contributor anonymity, delineating roles within collaborations with non-academic colleagues, the invalidity of digital simulacra of real-world artistic artefacts, the complexity of documentation of data drawn from a wide range of often privately-owned sources these are problems that are not felt by colleagues in STEM (the group of disciplines from which it is often felt that moves to research are flowing). The assignment of DOIs, ORCIDs, OA licences and so on to the outputs of research operating in this environment is tricky and fraught with real dangers that will require careful further investigation. There is a clear need to need to tease out the limitations of these new aspects of the research within disciplines, explore novel solutions, find what works and what doesnt, and seek a sensible way forward. At the heart of this is the question of ethics. The close dependency between more open and transparent scholarly communication practices and more effective research integrity are not disputed, but this is often used to justify a conclusion that is in all cases. The examples above, particularly of ethnographic research, reveal that ethical limitations within disciplinary practice often inform models of communication in a way that might hinder openness, and that this is entirely appropriate in the disciplinary context. This at first appears to fly in the face of the very idea of open science, but in practice it only underlines the need for context-specific approaches to openness that take sufficient accounts of the ethical practices within disciplines. Clear delineation is needed, though, between genuine ethical considerations and those simply borne of more affected academic-cultural norms or resistant to practical change we need to head off any unfair accusations of special treatment being granted to these disciplines purely on political grounds. We need to better understand this problem, so that we can more effectively and sensitively tailor our approaches to achieve open research communication in a way that respects good research practice in all disciplines. Finally, the question of metrics. Central to the arguments made above, and elsewhere, is a concern that the of DOIs, ORCIDs, Web of Science coverage etc. is insufficient to enable the accurate capture of research outputs within AHSS, and therefore the metrics systems that depend on counting research outputs will unfairly discriminate against these disciplines. As the article states, the upshot of this is that those disciplines that are less digitally oriented, are likely to obtain unhelpful metric ratings. In my view, this masks a more pressing issue, which is that metrics are most applicable to those disciplines that their outputs into easily quantifiable forms, with quantifiable relationships to one another, with quantifiable citation practices, quantifiable(ish) contributions of academics to the research, and so on. Its clear from the above, and from my own discussions with AHSS researchers, that the problems of quantifying AHSS research are not only related the coverage of DOIs and ORCIDs, and we should be careful not to assume that we entirely fix the issue of metrics by fixing the plumbing (even though we might get a few quick wins in a few areas). 