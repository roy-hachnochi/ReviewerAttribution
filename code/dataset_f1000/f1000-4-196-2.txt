 Overview This papers sets out to exploit some of the massive amount of data collected during the Italian 2012-2014 National Scientific Habilitation (Abilitazione Scientifica Nazionale; ASN). The database created for this evaluation exercise listed the academic output of all Italian researchers applying to become associate or full professors within the Italian university of research institutes. Data was classified by output type academic, non-academic, journal articles, books, monographs - and also by the disciplinary section of the candidate. Interestingly, the collected data for article publications went through two independent review processes with a panel ranking journals as academic and non-academic, and giving different weighting to the academic series, and another panel carrying out peer review to look at the quality of the articles submitted. This aim of the study described in this paper was to see if there is agreement between the rankings produced by the two panels that would allow journal rank to be a reasonable proxy for quality. It looks at a selected number of very different fields from the HSS, namely architecture, arts and humanities, history and philosophy, law and sociology and political science. The study proceeds by a statistical analysis of journals ranking and the classification of individual articles given by reviewers on a scale from A (excellent) to D (limited). It also takes into account a series of variables such as the language of production and field size as well as the discipline and age of the writer. The results are clearly of great interest in developing new bibliometric tools for handling data in large evaluation exercises in that they show a clear correlation between journal ranking and the outcome of peer review appraisal. The outcomes indicate that those articles with the best evaluation appeared in class A journals and that thus, although peer review remains important, a journal ranking may be a good proxy for quality. Assessment As this is a short article, much background knowledge would be needed to fully apprehend the criteria behind the original exercise. We have little information as to either the peer review panels and the reality of the criteria they applied, or how the panel classifying journals work, and the extent that it simply followed existing classifications as that of ERIH. These are both key elements in evaluating the output as they represent sources of potential bias and the influence of normative approaches to quality. The following comments thus concern more potential underlying biases in data collection and evaluation policy that the methodology applied here. The first question applies to the ranking of the journals. Who are the experts and what is the danger of field bias? ERIH has come in for enormous criticism as the degree to which the experts represent a field is far from clear. In the case of this assessment, we do not know how the panels were constituted and to what extent the inclusion of a journal in an A list was free of domain bias. In France, the AERES agency had to abandon lists for the HSS as being too hotly disputed, and the field of law had simply ranked its own journals as A anyway. It is relatively easy to highlight a group of high profile, high impact journals in any field, but much more difficult to obtain clear criteria for ranking other journals as reputation measures can be fearfully biased. The same problems arise with peer review, namely representativity of the panels and relevance of criteria to individual fields. We do not know how many reviewers read any individual paper and the extent they actually read the whole paper and were more competent to judge than the field specific peer reviewers who initially reviewed the article. The outcomes from the UK REF showed that reviewers only skimmed publications due to lack of time, given the volume of data to be treated, did these reviewers read more closely. Another issue arises from the allocation of experts, notably international experts who were chosen because the article was already deemed as having a quality potential. Another important factor is the variety within the broad field of SSH, and even variety within disciplines. Architecture might be expected to have a more engineering dissemination profile, whereas political science and sociology can be at the end of the HSS spectrum that is closer to the sciences. The area 11, History, philosophy, pedagogy and psychology, is particularly wide as psychology can have a very different dissemination pattern to history, and is often not included or treated as borderline, within the HSS sphere. Within language, there will be great variation between areas. The oft-cited maxim that humanities researchers write books has been shown to be far too simplistic and is just an example of how broad brush strokes can hide the diversity within fields. The article itself does point out that there can be perverse effects of an evaluation process, especially if it is normative. This may be happening here with internationalisation. Internationalisation is obviously an added value in research, but it should not be seen as a necessary prerequisite of quality. A relative lack of internationalisation is inherent in many humanities disciplines, and is particularly common in law. This is not a lack of quality, but simply due to the national orientation in the field of study. Penalising by too rigid evaluation criteria would be a bad thing as it is for evaluators to understand a field and adapt, not to try and change the field to suit their criteria. These factors do not change the interest of the methodology adopted, but do need to be considered before making policy decisions based on outcomes. The methodology itself is through and opens vistas for analysing what is happening in assessment exercises and how it affects dissemination practice. I have only one minor gripe in what is otherwise a very clear and stimulating paper, and that is with data presentation. For the two datasets shown in tables 1 and 2, it would have been nice to have had averages shown so that we have an instantly visible means of comparison. This would have revealed that law beats all disciplines with a very high level of A class journals, 68% as opposed to only 39% in architecture. These differences merit comment. The article uses sound methodology and opens perspectives for more detailed work, and hopefully a close grain analysis within disciplines that will take into account researcher motivations. This is vital as correlation at this level does not necessarily justify the use of bibliometric criteria over peer review as there are inevitably built in biases in both quantitative and qualitative approaches. Until in-depth studies of how and why researchers disseminate are carried out, the picture will always be falsified. Thus, this research opens interesting channels, and always calls for much more sociological analysis before confirming outcomes.