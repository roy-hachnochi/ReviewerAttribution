The modifications that the authors have made in the current version are mostly superficial and improvements are minor. The main weaknesses remain, which is why I do not approve the indexation of this article. To provide support for my decision, the major shortcomings are outlined below: Research questions The issue with the research questions remains and it is central to the paper. â€‹What is the rational for the questions and how do they inform us regarding the reading/saving practices of Mendeley users? From my perspective the questions reflect what could very easily be derived from the dataset -- the absolute number of readers per Mendeley discipline -- instead of what would be interesting to analyze -- the Mendeley readers discipline in comparison to an expected value. To be more specific: RQ1a/b: Are F1000Prime papers only read by people from biomedicine or are people from other disciplines also interested? If so, which other disciplines show interest in F1000Prime papers? As noted in my previous report, it is not clear what the authors expect to find. How many readers from each of the fields are ? How much would be expected? Do as few as 1 reader from a discipline other than biology or medicine already indicate interest from other disciplines? Mohammadi Thelwall (2014) created an expected value based on the discipline of the paper in the Web of Science. An alternative would be the discipline of the citing papers, which would actually be more suitable, as it is comparable to readers. Without such a comparison, I do not see the value of the analysis. What does it mean that two-thirds of Mendeley reader counts of F1000Prime recommended papers came from biology readers and 15% from medicine? RQ2: Which disciplines read F1000Prime papers frequently or seldom together? How can a discipline read? Rather: To what extent do Mendeley readers of different disciplines save the same papers? Again, this lacks a reference value without which it is impossible to assess whether certain values are . In my previous review I had suggested to use the PubMed/Medline subject classification as an expected value regarding disciplines and to analyze the knowledge flow from authors to readers on the level of disciplines. The fact that the authors of the study already use the PMID to match datasets, would it make quite easy to retrieve this information and establish a benchmark and help to answer the research questions in a more meaningful way. This has not been addressed by the authors. Reference list The authors did include some of the suggested references but mention them briefly rather than integrating findings and methods into their study. One example of such a passing mention is Mohammadi Thelwall (2014) , which, in my opinion, is fundamental regarding the subject as well as methods of the study, because they analyzed the disciplines of the Mendeley readers and compare them to the discipline of the papers - an important reference value that is missing in this study. Unfortunately the authors only cite the paper with regards to correlations between readers and citations, which is not relevant to their study: Mohammadi Thelwall (2014) found significant correlations between Mendeley reader counts and citation counts for the social sciences and humanities. As the authors noted in their reply to the review, it is true that that Mohammadi Thelwall (2014)s paper is based on a previous restrictions of the Mendeley API (to the top 3 disciplines per paper), but this does not make their methods of comparing reader and paper disciplines less relevant to the present study. Dataset The authors added a paragraph about the initial F1000Prime set (114,582 papers) and extended the retrieval method a bit. It is still not known how many of the queried papers were found on Mendeley and how many of them did not have any readers. Publication years, document types and particularly disciplines of these papers are not known. As the other reviewer noted, it is mandatory to select a discipline when signing up to Mendeley but the authors write: "Each user can select a discipline and sub-discipline from a drop-down menu. This piece of information is not mandatory, like the users location." In their recent preprint they write: "It is optional for the users of Mendeley to provide their disciplinary affiliations (selecting from predefined sub-disciplines) and location." This is contradictory. The fact that readers are provided by 99.9% of readers but only 17.6% provide the country (according to their ISSI paper) also implies that the discipline is mandatory. More information should be provided about this. Network analysis The additional information provided about the construction of the network is helpful do understand the methods but also raises more questions: "Two reader counts from discipline A and at least two reader counts from discipline B constitute two links between both disciplines. From a matrix point of view, we have a symmetric readership coupling matrix, which has a two in row A and column B and vice versa for the aforementioned example." Does this mean that the smallest common denominator between two disciplines was chosen? That would mean: If paper x has 10 readers from discipline A and 12 readers from discipline B, and paper y has 10 readers from A and 1000 readers from B, the connection between A and B for paper x and y is 10 each, which would add up to 20 in the matrix? This methods favors papers with large number of readers as well as underestimates large disciplines. Why was it chosen over others such as a binary counts based on papers, where the connection between discipline A and B would be the number of papers that had readers from A and B? Both methods (and other weighted approaches) have their advantages and disadvantages, which affect results. The authors should discuss this. The new network graph positioned the nodes of the same cluster together but all other shortcomings remain: the most important advantage of a network visualization, namely the network structure is still missing, as almost all nodes are connected with each other. In fact, the authors note that the density is 1: " With a density of 1, the network is rather dense." At a density of 1 it is not rather dense but as dense as it gets. A density of 1 would mean that each discipline has appeared together with each and every other at least once. Is that really the case even for the less frequent disciplines? Eliminating weak links would provide structure. Connections should be normalized as the strongest connections in terms of absolute counts are always between the most occuring disciplines. This will also influence the community detection. The authors note that they do not normalize the connections as different normalization methods lead to different results. This is of course true, but discussing similarity between entities of different size, they are still superior to the absolute number. I still think that self-loops for the number of readers from the same discipline would provide very important information in the network graph. Currently papers with readers from one discipline only are excluded entirely. The authors note that they did not include self-loops due to limitations in Pajek. If Pajek is not able to handle self-loops, they can be easily visualized with UCInet or Gephi. These tools also have more visualization options than VOSviewer such as changing edge widths and colors. Discussion and conclusion The authors have extended the discussions and conclusions but due to the above-mentioned weaknesses they are not convincing. I do not see how this study shows that Mendeley data can be used meaningfully beyond the fact that Mendeley provides discipline data for its users.