The study combines data from F1000 and Mendeley to analyze the (self-reported) disciplines of Mendeley users saving articles recommended in F1000. Research questions lack focus and clarity, analysis and discussions are weak, conclusions are absent. This is mainly due to the fact that the analysis focuses on a very small component of the data (sum of reader count per discipline), despite the fact that the two datasets contain many more interesting aspects that are worth analyzing. Many relevant previous studies are ignored. Due to the many shortcomings and weaknesses I do not approve the indexation of this article in its current state. Major revisions: The research questions lack clarity and the motivation of the study should not solely be based on the availability of datasets (Mendeley and F1000). It should be emphasized in how far this study is different from previous work, in particular Mohammadi Thelwall , who analyzed very similar aspects on Mendeley and, in addition, compared the discipline of users to that of the citing papers. For the present study, it is not clear what the authors expect to find (how much biology readers are normal?) and what the data is able to show: Do papers recommended on F1000 have Mendeley users from more diverse disciplines than expected? It would be much more interesting and valuable to observe the effect of being recommended on F1000 by comparing Mendeley readership counts and disciplines of users of the dataset used in this study with a control set of papers that were not recommended. This could be achieved by analyzing and comparing the data for the population of PubMed articles for a certain set of recent years: Does the F1000 recommendation provide visibility to papers that increases the number of readers on Mendeley as well as the diversity of the audience in terms of disciplines and academic status? PubMed/Medline could also provide a meaningful subject classification for papers to measure interdisciplinary knowledge flows from authors to readers. The authors also need to clarify in how far the present study differs and distinguishes itself from their other publications on similar topics and the same datasets: 1) Who reads F1000Prime publications? 2) Who publishes, reads, and cites papers? An analysis of country information 3) Usefulness of altmetrics for measuring the broader impact of research: A case study using data from PLOS and F1000Prime 4) Validity of altmetrics data for measuring societal impact: A study using data from Altmetric and F1000Prime 5) Overlay maps based on Mendeley data: The use of altmetrics for readership networks 6) Usefulness of altmetrics for measuring the broader impact of research: A case study using data from PLOS (altmetrics) and F1000Prime (paper tags) 7) The authors also mention Haunschild, Stefaner Bornmann (in preparation), which seems to focus on the geographic location of Mendeley users of the same dataset. Could this aspect not be integrated in the present study? The reference list is particularly poor and not acceptable in its current form. There have been plenty of studies by Thelwall, Mohammadi, Costas, Zahedi, Kraker, Haustein and others that have evaluated Mendeley reader counts - these are completely ignored. The introduction should be additionally supported by core altmetrics publications by Priem (particularly his overview of altmetrics in Beyond Bibliometrics which includes a definition of altmetrics), Piwowar , regarding reference managers Taraborelli and Haustein Siebenlist , as well as the above mentioned authors for research on altmetrics. Peter Krakers work on readership networks based on Mendeley users needs to be considered as well. The parallels between early bibliometrics research and current altmetrics lack references either to the particular bibliometrics studies or detailed discussions of this parallel, for example in Haustein, Bowman Costas . In addition, some of the references cited in the text are also not listed in the reference list. This needs proper revision. The dataset is not clearly defined . What is the F1000 Prime publication dataset? How and when was it retrieved? Are those all recommendations ever made in the database? What publications do the 114,582 papers refer to (journals, publications years, document types, discipline, research field, etc.)? What is the metadata quality of these entries; in particular, how many of these have a correct DOI, PMID or both? In how far does the availability of identifiers as well as the characteristics of papers (publication year, journals) influence and bias the matching with and availability in Mendeley? Regarding the matching of results: how many unique documents do the 6,263,913 reader counts refer to? What is the percentage of documents that could not be found in Mendeley? Readers/users should be distinguished from reader counts - to avoid the implication that there are 6.2 million readers. Mendeley has around 3 million users (2.8 million as of February 2014; Haustein Larivire ), who create reader(ship) counts by adding documents to their libraries. The description of the methods for the network analysis is too brief: How were co-occurrences calculated? How were they normalized? Due to the density of 1 (i.e., all nodes are connected), the network layout is not very meaningful and not easy to interpret, often counterintuitive. The informativeness of the network could be improved by removing weak links to obtain a more meaningful network structure, where central (i.e., well-connected) nodes are positioned in the center of the network and less important ones in the periphery. Moreover, similar nodes as detected by the clustering algorithm (yellow and green in Figure 1) should be placed close together. In addition, it would make sense to include self-loops for papers saved by users of the same discipline to highlight homogeneous user groups. As the authors use the VOSviewer clustering method, why was VOSviewer not chosen for the mapping? In my opinion, it provides much more meaningful robust networks and better visualizations than Pajek. Other alternatives are Gephi , GUESS , UCInet , etc. Regarding the interpretation of the network, the authors state that "[t]he thicker and darker the edges between two disciplines, the more frequently [the users] have read a F1000Prime paper jointly". Should it not rather be that "[t]he thicker and darker the edges between two disciplines, the more frequently papers were saved by users of these two particular disciplines"? The grouping into clusters seems to a certain extent counterintuitive: Why is environmental science grouped together with psychology instead of biology? Could this be introduced by (the lack of) normalization? These counterintuitive results need to be discussed! The discussion needs to be extended and a conclusion is missing . It is not clear what the study actually shows/proofs and in how far the few results (sum of readers per discipline) warrant a separate publication. The dataset of F1000 recommendations and Mendeley include many other pieces of interesting information such as the recommendation scores, F1000 tags (from F1000) and the geographic location and academic status of users (from Mendeley), which could be included to make the study much stronger and contribute to the understanding of readership counts and the effect of F1000 recommendations. In addition, subject classifications for the papers and locations of authors could be included to show if readers come from the same or different disciplines and countries as the paper and authors. I would also recommend the above mentioned extension of the study to include papers that were not recommended in F1000 to measure the effect of recommendations on readership counts. Combining these different aspects, one could investigate whether recommendations on F1000 lead to more diverse user groups on Mendeley in terms of discipline, country and academic status. For example, is a biology paper recommended and tagged as "good for teaching" on F1000 read by more Bachelor students from biology than a biology paper that was not recommended and tagged as such? Minor revisions: The first sentence "Interest in the broad impact of research (Bornmann, 2012, 2013) has resulted in new forms of impact measurements." simplifies the situation too much: there is also the technological push and publishers interest who resulted in the availability of new metrics, plus these metrics have not been validated as measuring impact yet. Also, the references to support interest in broad impact measures should refer to sources that show these interests such as REF etc. instead of papers by Bornmann, which claim that these interests exist. Regarding the use of altmetrics: apart from Snowball Metrics, they are also applied in the sense that various journals now show them to indicate the "impact" and use of articles (for example, PLOS journals, Nature, Wiley journals etc.). Funders have also declared interest in using these metrics (for example, see Dinsmore, Allen Dolby ). "Since data from Mendeley can be received by an Application Programming Interface (API) without any problems" - this is not completely true, there are a lot of issues with data quality and reliability for Mendeley, see for example: Bar-Ilan and Zahedi, Haustein Bowman . These limitations need to be acknowledged in particular because the study is based on matching DOIs and PMIDs - Mendeley entries without these or incorrect IDs will be lost. What is the error rate introduced by using these identifiers only? In the methods, authors should specify what was done when problems with the API connection occurred. How was it insured that data was not lost due to these problems? It would be helpful to add the number of unique papers and mean (+ std. dev.) number of reader counts per paper per discipline to Table 1 and include also the other disciplines with less than 1% of reader counts. 