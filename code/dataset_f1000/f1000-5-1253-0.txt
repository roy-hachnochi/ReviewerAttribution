The author tackles the problem of determining the mutational processes that were active on a tumor, and specifically in a single sample setting by leveraging already available signatures. The relevance of this approach is thus clear and was established in previous work: it allows working with signatures in a more general setting such as the clinic, and reusing already available signatures helps interpretation by the community as these become more familiar to all. The author's contribution is limited to a technological advance, but in that is seems to surpass the previous approach in speed and accuracy (though I present some reservations below) by casting the problem into the more sophisticated framework of quadratic programming. I think this approach has benefits and I'm convinced that at no expense, and as such, I'm strongly favorable. I have however a few concerns that I'd like to raise. Biologically I understand that mutational processes have signatures that are non-orthogonal, so a particular footprint of activity (the mutations on a sample) could in general be explained by different activation patterns of these signatures. How do these methods account for prior probabilities? e.g. mutational patterns related to smoking can be far more prevalent that exposure to a rare carcinogenic that could resemble the smoking signature in whole or in part. I can imagine the methods that extract this signature leveraging cohort data to untangle these prior probabilities, but then I think the methods presented in this paper in the deconstructSig cannot make use of this priors. In any case, I don't think current cohort methods predicting de-novo signatures are accounting for these priors since I would imagine they should be reporting these in addition to the signatures, which I believe they are not. Coming back to the article at hand, the second paragraph in the result section seems to relate to this question in part. I find this paragraph confusing, possibly due to my own shortcomings so perhaps the author can clarify it for me, or even make it more clear on the text if need be. Let me explain. The author claims that the signature matrix is full rank. Correct me if I'm wrong, but in general it need not be, making the problem of approximating the result with different combinations is not just a result of noise and actually not specific to ILM, but to both methods. In fact the following phrase: 'an ILM approach is not guaranteed to give consideration to the correct combination of signatures' seems unfair, does the QP approach offer such guarantees? If so, perhaps this could be explained. This paragraph was one of the main arguments for the improvement on accuracy, and I've presented my reservations. The other argument are some experiments presented on synthetic data involving the mixture of two signatures. These experiments seem too simplistic and I believe they do not address the problems presented in the previous paragraph either. However I do find that they suffice for the purposes of this article. In conclusion, I concur with Mohamed that its mostly the performance that drives the message home at this point. Though I would not like to discourage indexing of this article, I feel that the author could improve his arguments regarding accuracy.