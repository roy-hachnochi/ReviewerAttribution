Thank you for giving us the opportunity to review the revised article and explanations from the authors. In general they have responded and given clarifications either in the revised article, or the covering letter, and sometimes both. However, there are a few outstanding points that have not been addressed and we would like to see responses to these before we would be ready to amend the current status of approved with reservations. Our outstanding three comments are detailed below. The authors have added a para about exporting data from the red zone, but it would still be good to have some simple text explaining exactly what staff needed to do to transfer the data, to help those readers who find ‘servers’ a technical term. We are still a bit puzzled about the reference to USB sticks for backup and updating and whether that refers only to outside the red zone back up, or internally. As mentioned in our previous referee report: “The results are generally clearly described, and suggest that the tablet may be at least as good as paper records, though the analysis currently does not appear to take into account the role of chance, and a proportion of records (17% ?) had to be excluded. The qualitative study is quite weak. It appears that different data were collected systematically using the two different systems, so only some of the data were collected using both methods. Further comments on the quantitative study are: If we understand correctly the agreement between the two methods was calculated by simple percentage agreement: would it be possible to perform a Kappa test to take into account chance? What was the range of interaction pairs per patient? Was any difference seen in the recording by either method related to the severity of the patient’s illness, or the number of people admitted at the time? Is it possible to state the percentage of missing data overall and/or by recording method and comment on the implications for the agreement results? On rough calculation overall it seems to be 17% (204-(85x2) = 34, 34/204) missing data in one or other system. The agreement between data sources ranged between 69 and 95%. This is rated as ‘high consistency’ but I was not sure on what basis. It was not clear to me, which collection method was felt to be the gold standard, or even if this was determined before the comparison. I sense from the discussion, the tablet was thought to perform better than the paper method, but I am not convinced about this from the results.” 1 These points are not addressed either in response or text – and the reference to high consistency remains (1 st para discussion) Similarly, this point has also not been addressed: “The authors claim that this is the first such device to be developed. How did they search for other evidence of similar projects? I heard anecdotally of other projects, but am not aware whether they reached fruition, or have ever been published in any form. It would be good to have a thorough systematic search to establish what else has been developed.” 1 References 1. Bower H, Whitworth J: Referee Report For:Electronic medical records in humanitarian emergencies – the development of an Ebola clinical information and patient management system [version 2; referees: 1 approved, 1 approved with reservations]. F1000Research . 2016; 5 (1477). Publisher Full Text Competing Interests: A colleague from LSHTM is acknowledged for his input into the development of this information system. We have had no direct interaction about this project.   HB worked for MSF in Sierra Leone in the same period as the trial but had no involvement in it. She has worked with some of the authors in other locations and contexts. I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above. Close READ LESS CITE CITE HOW TO CITE THIS REPORT Whitworth J. Reviewer Report For: Electronic medical records in humanitarian emergencies – the development of an Ebola clinical information and patient management system [version 3; peer review: 2 approved] . F1000Research 2017, 5 :1477 ( https://doi.org/10.5256/f1000research.10435.r17187 ) The direct URL for this report is: https://f1000research.com/articles/5-1477/v2#referee-response-17187 NOTE: it is important to ensure the information in square brackets after the title is included in all citations of this article. COPY CITATION DETAILS Report a concern Author Response 23 Feb 2017 kiran jobanputra , Médecins sans Frontières (MSF), London, UK 23 Feb 2017 Author Response The authors would like to thank the reviewer for the comments in their response letter, and have endeavoured to address the outstanding issues raised: Data transfer: The authors have ... Continue reading The authors would like to thank the reviewer for the comments in their response letter, and have endeavoured to address the outstanding issues raised: Data transfer: The authors have added a para about exporting data from the red zone, but it would still be good to have some simple text explaining exactly what staff needed to do to transfer the data, to help those readers who find ‘servers’ a technical term. We are still a bit puzzled about the reference to USB sticks for backup and updating and whether that refers only to outside the red zone back up, or internally. We have addressed this in the text of the article (v3) Data accuracy: If we understand correctly the agreement between the two methods was calculated by simple percentage agreement: would it be possible to perform a Kappa test to take into account chance? What was the range of interaction pairs per patient? Was any difference seen in the recording by either method related to the severity of the patient’s illness, or the number of people admitted at the time? Is it possible to state the percentage of missing data overall and/or by recording method and comment on the implications for the agreement results? On rough calculation overall it seems to be 17% (204-(85x2) = 34, 34/204) missing data in one or other system. The agreement between data sources ranged between 69 and 95%. This is rated as ‘high consistency’ but I was not sure on what basis. It was not clear to me, which collection method was felt to be the gold standard, or even if this was determined before the comparison. I sense from the discussion, the tablet was thought to perform better than the paper method, but I am not convinced about this from the results.” The methods have been edited and percentage agreement results have been replaced with Kappa coefficients (v3). Data on missing results and the number of records per patient has been added. Relatively few patients were admitted during the period, and bleeding symptoms were rare, so attempting to account for these issues is unlikely to be valid with this dataset. Neither method was a gold standard. Other comparable products: “The authors claim that this is the first such device to be developed. How did they search for other evidence of similar projects? I heard anecdotally of other projects, but am not aware whether they reached fruition, or have ever been published in any form. It would be good to have a thorough systematic search to establish what else has been developed.” 1 At the time of writing, we searched PubMed and Google using various search terms including “EMR”, “data collection” and “Ebola”, and found no description of similar products. Through our informal networks, we found out about two data collection devices that were developed for the ebola epidemic, but neither had EMR functionality. After submission of this article, we learnt about an ebola EMR developed by ThoughtWorks, but without the server infrastructure. Although a systematic review is beyond the scope of this article, we have add a reference to this other product in the text (v3). The authors would like to thank the reviewer for the comments in their response letter, and have endeavoured to address the outstanding issues raised: Data transfer: The authors have added a para about exporting data from the red zone, but it would still be good to have some simple text explaining exactly what staff needed to do to transfer the data, to help those readers who find ‘servers’ a technical term. We are still a bit puzzled about the reference to USB sticks for backup and updating and whether that refers only to outside the red zone back up, or internally. We have addressed this in the text of the article (v3) Data accuracy: If we understand correctly the agreement between the two methods was calculated by simple percentage agreement: would it be possible to perform a Kappa test to take into account chance? What was the range of interaction pairs per patient? Was any difference seen in the recording by either method related to the severity of the patient’s illness, or the number of people admitted at the time? Is it possible to state the percentage of missing data overall and/or by recording method and comment on the implications for the agreement results? On rough calculation overall it seems to be 17% (204-(85x2) = 34, 34/204) missing data in one or other system. The agreement between data sources ranged between 69 and 95%. This is rated as ‘high consistency’ but I was not sure on what basis. It was not clear to me, which collection method was felt to be the gold standard, or even if this was determined before the comparison. I sense from the discussion, the tablet was thought to perform better than the paper method, but I am not convinced about this from the results.” The methods have been edited and percentage agreement results have been replaced with Kappa coefficients (v3). Data on missing results and the number of records per patient has been added. Relatively few patients were admitted during the period, and bleeding symptoms were rare, so attempting to account for these issues is unlikely to be valid with this dataset. Neither method was a gold standard. Other comparable products: “The authors claim that this is the first such device to be developed. How did they search for other evidence of similar projects? I heard anecdotally of other projects, but am not aware whether they reached fruition, or have ever been published in any form. It would be good to have a thorough systematic search to establish what else has been developed.” 1 At the time of writing, we searched PubMed and Google using various search terms including “EMR”, “data collection” and “Ebola”, and found no description of similar products. Through our informal networks, we found out about two data collection devices that were developed for the ebola epidemic, but neither had EMR functionality. After submission of this article, we learnt about an ebola EMR developed by ThoughtWorks, but without the server infrastructure. Although a systematic review is beyond the scope of this article, we have add a reference to this other product in the text (v3). Competing Interests: No competing interests Close Report a concern Respond or Comment COMMENTS ON THIS REPORT Author Response 23 Feb 2017 kiran jobanputra , Médecins sans Frontières (MSF), London, UK 23 Feb 2017 Author Response The authors would like to thank the reviewer for the comments in their response letter, and have endeavoured to address the outstanding issues raised: Data transfer: The authors have ... Continue reading The authors would like to thank the reviewer for the comments in their response letter, and have endeavoured to address the outstanding issues raised: Data transfer: The authors have added a para about exporting data from the red zone, but it would still be good to have some simple text explaining exactly what staff needed to do to transfer the data, to help those readers who find ‘servers’ a technical term. We are still a bit puzzled about the reference to USB sticks for backup and updating and whether that refers only to outside the red zone back up, or internally. We have addressed this in the text of the article (v3) Data accuracy: If we understand correctly the agreement between the two methods was calculated by simple percentage agreement: would it be possible to perform a Kappa test to take into account chance? What was the range of interaction pairs per patient? Was any difference seen in the recording by either method related to the severity of the patient’s illness, or the number of people admitted at the time? Is it possible to state the percentage of missing data overall and/or by recording method and comment on the implications for the agreement results? On rough calculation overall it seems to be 17% (204-(85x2) = 34, 34/204) missing data in one or other system. The agreement between data sources ranged between 69 and 95%. This is rated as ‘high consistency’ but I was not sure on what basis. It was not clear to me, which collection method was felt to be the gold standard, or even if this was determined before the comparison. I sense from the discussion, the tablet was thought to perform better than the paper method, but I am not convinced about this from the results.” The methods have been edited and percentage agreement results have been replaced with Kappa coefficients (v3). Data on missing results and the number of records per patient has been added. Relatively few patients were admitted during the period, and bleeding symptoms were rare, so attempting to account for these issues is unlikely to be valid with this dataset. Neither method was a gold standard. Other comparable products: “The authors claim that this is the first such device to be developed. How did they search for other evidence of similar projects? I heard anecdotally of other projects, but am not aware whether they reached fruition, or have ever been published in any form. It would be good to have a thorough systematic search to establish what else has been developed.” 1 At the time of writing, we searched PubMed and Google using various search terms including “EMR”, “data collection” and “Ebola”, and found no description of similar products. Through our informal networks, we found out about two data collection devices that were developed for the ebola epidemic, but neither had EMR functionality. After submission of this article, we learnt about an ebola EMR developed by ThoughtWorks, but without the server infrastructure. Although a systematic review is beyond the scope of this article, we have add a reference to this other product in the text (v3). The authors would like to thank the reviewer for the comments in their response letter, and have endeavoured to address the outstanding issues raised: Data transfer: The authors have added a para about exporting data from the red zone, but it would still be good to have some simple text explaining exactly what staff needed to do to transfer the data, to help those readers who find ‘servers’ a technical term. We are still a bit puzzled about the reference to USB sticks for backup and updating and whether that refers only to outside the red zone back up, or internally. We have addressed this in the text of the article (v3) Data accuracy: If we understand correctly the agreement between the two methods was calculated by simple percentage agreement: would it be possible to perform a Kappa test to take into account chance? What was the range of interaction pairs per patient? Was any difference seen in the recording by either method related to the severity of the patient’s illness, or the number of people admitted at the time? Is it possible to state the percentage of missing data overall and/or by recording method and comment on the implications for the agreement results? On rough calculation overall it seems to be 17% (204-(85x2) = 34, 34/204) missing data in one or other system. The agreement between data sources ranged between 69 and 95%. This is rated as ‘high consistency’ but I was not sure on what basis. It was not clear to me, which collection method was felt to be the gold standard, or even if this was determined before the comparison. I sense from the discussion, the tablet was thought to perform better than the paper method, but I am not convinced about this from the results.” The methods have been edited and percentage agreement results have been replaced with Kappa coefficients (v3). Data on missing results and the number of records per patient has been added. Relatively few patients were admitted during the period, and bleeding symptoms were rare, so attempting to account for these issues is unlikely to be valid with this dataset. Neither method was a gold standard. Other comparable products: “The authors claim that this is the first such device to be developed. How did they search for other evidence of similar projects? I heard anecdotally of other projects, but am not aware whether they reached fruition, or have ever been published in any form. It would be good to have a thorough systematic search to establish what else has been developed.” 1 At the time of writing, we searched PubMed and Google using various search terms including “EMR”, “data collection” and “Ebola”, and found no description of similar products. Through our informal networks, we found out about two data collection devices that were developed for the ebola epidemic, but neither had EMR functionality. After submission of this article, we learnt about an ebola EMR developed by ThoughtWorks, but without the server infrastructure. Although a systematic review is beyond the scope of this article, we have add a reference to this other product in the text (v3). Competing Interests: No competing interests Close Report a concern COMMENT ON THIS REPORT Version 1 VERSION 1 PUBLISHED 23 Jun 2016 Views 0 Cite How to cite this report: Black BO. Reviewer Report For: Electronic medical records in humanitarian emergencies – the development of an Ebola clinical information and patient management system [version 3; peer review: 2 approved] . F1000Research 2017, 5 :1477 ( https://doi.org/10.5256/f1000research.8913.r15182 ) The direct URL for this report is: https://f1000research.com/articles/5-1477/v1#referee-response-15182 NOTE: it is important to ensure the information in square brackets after the title is included in this citation. Close Copy Citation Details Reviewer Report 23 Aug 2016 Benjamin O. Black , The Whittington Hospital, London, UK Approved VIEWS 0 https://doi.org/10.5256/f1000research.8913.r15182 Research and development of technology in the humanitarian arena is a rapidly growing field that encompasses many specialities. There are international working groups, peer reviewed journals and conferences dedicated to this field. This paper is a valuable addition to the ... Continue reading READ ALL 