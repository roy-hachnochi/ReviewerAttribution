The authors summarize the work they have done to improve computational methods for predicting which prostate cancer patients will discontinue the cancer drug docetaxel. The methods were tested using clinical data given in one of the DREAM competitions, a series of competitions in which computational biologists are challenged to submit predictions for quantitative biological and biomedical questions for which answers have been withheld from the competitors during the competition phase. I am marking this as “Approved with Reservations” because of oversimplification in the abstract and page 7 concerning the performance of Hill Climbing (HC) and Random Forest Importance (RFI). (The journal specifically asks us to comment on whether the “abstract represents a suitable summary of the work”). Although RFI arguably did better than HC on the ENTHUSE-33 dataset, HC did substantially better in the hold-out experiments in Table 1. On page 7 they write: “Compared to the hill-climbing method, [RFI] produced better and more consistent AUCs across clinical trials.” Having higher AUC in 2/4 ENTHUSE-33 tests doesn’t strike me as consistently better. To get me to “Approve” this, I would like full attention to my main point above (this shouldn’t be too hard) and some subset of the comments below (some of which are optional). With respect to my main comment above, it would be nice if the authors could give some measure (e.g., 95% CIs) of the variation in their AUC estimates. Is 0.146 (AUC for RFI on the ENTHUSE-33 dataset) significantly higher than 0.129 (AUC for HC)? A bit more discussion about the lessons that you learned from this exercise would be helpful. Based on your research, what advice, if any, would you give to someone entering a similar competition next year? If you feel uncomfortable providing more advice, please explain what it is that makes you uncomfortable. With regard to comment #2, one of the key points I derived from this paper was the importance of properly cleaning and augmenting the data. However, the quantitative data that support this conclusion was tucked into a parenthetical remark in the discussion. These data belong in the results. Also, in the discussion, please provide whatever thoughts you may have as to why HC didn’t do as well on the ENTHUSE-33 dataset as it did in the other experiments. The data in Figure 3 are highly erratic. As far as I can tell, 3, 5, 10, 20, or 24 features would be indistinguishable from 4. Please comment. I concur with one of previous reviewers about the value of adding the AUC score for BL on the ENTHUSE-33 dataset. Also, consider adding a row (first three entries of which would be blank) giving AUCs for ENTHUSE-33 for models trained on the full dataset. I wonder if the statement in the abstract “We also empirically studied the performance of many classification algorithms, including support vector machines and neural networks” could be converted into some kind of result (which would need to supported in the results section with AUC values). Maybe cut back on the background to make room from this. Minor: “random” came out as “andom” on page 5. “cutoff” should be under the x-axis in Figure 3. Bold entry in Table 1 isn’t explained.