In general, the paper is understandable and well organized. The use of boosting to improve the Cox proportional hazard and regression models is interesting. Despite the issues the authors address regarding missing data, there appears to be adequate information in this dataset to test this approach. Abstract “as well as an improvement of the model developed after the challenge” This is vague statement that adds little to the abstract. It is irrelevant when the improvement was made. The abstract should describe the clinical features and any improvements in an organized manner so that the reader can determine whether the method proposed differs from previous approaches. “TYTDreamChallenge model performed similarly as the gold-standard Halabi model” What was the performance accuracy or misclassification rate? The “Halabi” model terminology is jargonistic and should be described briefly in conventional oncology criteria (ie. an abbreviated form of the statement in the introduction would be sufficient). Was the increase in the ROC by 3% significant? If so, please explain why. Introduction The authors term the paper by Halabi et al. to be the “gold standard” prognostic model. While it performs well and is a reasonable comparator, it does not meet the criteria for a gold standard. It is a conventionally trained and tested and validated with a single external dataset. Gold standards, on the other hand, have been reproduced by other investigators using other patient cohorts multiple times with similar findings, may have fulfilled ISO or other standards and have been recommended by internationally recognized authorities (for example, the dicentric chromosome assay for radiation dosimetry). Methods “some features were removed from the study due to missing values, high correlations with other features or being unimportant for such survival analysis as determined by clinical experts. “ Please indicate which features were removed. What proportions were attributable to missing values, etc. “based on consultation with oncologists, rows with measurements of 13 important lab tests” Define important vs unimportant lab tests, and reasons for selecting them. Did the authors evaluate whether their methods were very sensitive to assumptions made about the missingness mechanism or about the distributions of the variables with missing data? If so, please state. Results The authors don’t clearly distinguish which methods were used in their submission of results to the DREAM challenge vs. how or why the “improvements” were made after the submission to the challenge. Furthermore, the overall risk score vs separate scores at different time points can simply be reported, without making this artificial distinction. Thus, the distinction made in the abstract between these lacks context, and I would recommend removing it. There are many other measures for evaluating the models that the authors could report besides AUC, including Matthews Correlation Coefficient, F-measure, Precision, and Accuracy. They may consider computing and reporting these. Figure 3 is unacceptable quality. Even at 200% magnification, the labels on each of the graphs are barely readable. Panel C requires some further explanation in order to interpret it. The text indicates “This suggested intuitive interpretations for the different features,” which does not explain the results or whether the partial dependence can be used for feature selection or interpretation. “These findings are well in line with the general hypothesis that these factors are basic values representing the volume of the disease.” By volume, are the authors referring to the extent of the disease? The extent of the disease is not the same as the survival risk, which is what the authors state they are modeling in the introduction. In cancer, volume refers to the size of the tumor and quantitative distribution (Castro-Mesta et al. 2016 1 ). In fact, the authors use the numbers of lesions to infer survival risk, so it would appear that this is a circular argument. 