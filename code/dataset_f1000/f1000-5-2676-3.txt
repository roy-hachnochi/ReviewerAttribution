This paper is written by CAMP, a winning team of the 2015 Prostate Cancer DREAM Challenge (“the PCDC”, or “the challenge”), to introduce their winning method. The authors built heterogeneous ensembles with the training data (Trials ASCENT-2, MAINSAIL, VENICE) and the unlabeled part of the validation data (Trial ENTHUSE-33). The high performance of their method, especially in predicting patients’ days to death, was confirmed by the challenge organizers on the validation data. This manuscript contains sufficient details about the actual method they used in the PCDC. Achilles heels for this paper include: 1) To show the necessity of using ensembles; 2) To establish the generalizability of the proposed ensemble models to new data sets. See the major issues below for more detailed comments. Major issues : The power of averaging over the base learners was taken for granted in the paper without experimental evidence. Training an ensemble costs much more effort than training a single model, and therefore it has to be shown that such effort is worth it. Direct comparison in performance between the ensemble and base learners is needed to make this point clear. The training of the ensembles, in particular the ensemble pruning step, used information from the validation set. Although only the features, but not the outcomes, of the validation data were seen by the model, this practice is still not encouraged. A generalizable model should not use the validation data in any way during training. Therefore, whether the proposed method is generalizable to new data sets is in doubt. I would suggest the authors to prune the ensemble on the training set and check the performance on the validation set. There is no instruction in the code documentation about how to apply the code to new data sets. Adding such information can greatly increase the chance that the code will be used by other researchers. Can the authors mine some knowledge from the trained model? For example, what are the most important features? Where are the baseline (i.e. Halabi’s model 1 ) features in the ranked list? Such analysis of the model can be helpful to biomedical researchers and doctors. Minor issues : In Algorithms 1 2, how did the authors choose the minimum desired performance c min and the desired set of ensemble S? Page 6, paragraph 2, line 3: “Median” should be changed to “standard deviation” or some other measures of variance, because in a within-trial validation the “median” is not directly related to “the difference between observed time points in the training and test data” (lines 5-6). Page 8, paragraph 2, the last 8 lines: This example is not very convincing. A model considering all features trained on the first dataset will assign a very small (if not zero) weight to feature 3, which will compensate little for the fact that feature 3 is important in the second dataset. Page 8, paragraph 5: What numerical difficulties did the authors encounter so that they could not include the Cox regression in the ensembles? Is there anything special about Cox model that makes it harder to train than other base learners? It is not explicitly stated in the paper that the authors are from Team CAMP. Grammar : Page 4, last paragraph: “within-in trial validation” should be “within-trial validation”; “between trials validation” should be “between-trial validation”. References 1. Halabi S, Lin CY, Kelly WK, Fizazi KS, et al.: Updated prognostic model for predicting overall survival in first-line chemotherapy for patients with metastatic castration-resistant prostate cancer. J Clin Oncol . 2014; 32 (7): 671-7 PubMed Abstract | Publisher Full Text Competing Interests: No competing interests were disclosed. I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above. Close READ LESS CITE CITE HOW TO CITE THIS REPORT Xiao J. Reviewer Report For: Heterogeneous ensembles for predicting survival of metastatic, castrate-resistant prostate cancer patients [version 3; peer review: 2 approved, 1 approved with reservations] . F1000Research 2017, 5 :2676 ( https://doi.org/10.5256/f1000research.8853.r20214 ) The direct URL for this report is: https://f1000research.com/articles/5-2676/v1#referee-response-20214 NOTE: it is important to ensure the information in square brackets after the title is included in all citations of this article. COPY CITATION DETAILS Report a concern Author Response 27 Jun 2017 Sebastian Plsterl , Technical University of Munich, Munich, Germany 27 Jun 2017 Author Response This paper is written by CAMP, a winning team of the 2015 Prostate Cancer DREAM Challenge (“the PCDC”, or “the challenge”), to introduce their winning method. The authors built heterogeneous ... Continue reading This paper is written by CAMP, a winning team of the 2015 Prostate Cancer DREAM Challenge (“the PCDC”, or “the challenge”), to introduce their winning method. The authors built heterogeneous ensembles with the training data (Trials ASCENT-2, MAINSAIL, VENICE) and the unlabeled part of the validation data (Trial ENTHUSE-33). The high performance of their method, especially in predicting patients’ days to death, was confirmed by the challenge organizers on the validation data. This manuscript contains sufficient details about the actual method they used in the PCDC. Achilles heels for this paper include: 1) To show the necessity of using ensembles; 2) To establish the generalizability of the proposed ensemble models to new data sets. See the major issues below for more detailed comments. Major issues : The power of averaging over the base learners was taken for granted in the paper without experimental evidence. Training an ensemble costs much more effort than training a single model, and therefore it has to be shown that such effort is worth it. Direct comparison in performance between the ensemble and base learners is needed to make this point clear. Response : We included heterogeneous ensembles in the between trials validation (see figures 3 and 5) and in our discussion of the results. The training of the ensembles, in particular the ensemble pruning step, used information from the validation set. Although only the features, but not the outcomes, of the validation data were seen by the model, this practice is still not encouraged. A generalizable model should not use the validation data in any way during training. Therefore, whether the proposed method is generalizable to new data sets is in doubt. I would suggest the authors to prune the ensemble on the training set and check the performance on the validation set. Response : For survival data, the pruning step is delayed until prediction is performed, because predictions are risk scores on an arbitrary scale for which a per-sample error measure is not readily available. This is in contrast to ensemble pruning for regression problems, where a per-sample error can be easily computed and models having highly correlated errors are pruned. As referee 2 suggests, the pruning step could be performed via cross-validation on the training data. However, our pruning step does not take survival times or censoring status into account, therefore we prefer to delay the pruning step as long as possible as to avoid overfitting on the training data. If the additional costs associated with storing the ensemble before pruning are prohibitive, we recommend that pruning should be performed via cross-validation on the training data. There is no instruction in the code documentation about how to apply the code to new data sets. Adding such information can greatly increase the chance that the code will be used by other researchers. Response: Our source code is accompanied by a README file explaining all steps necessary to reproduce our results. The source code can be downloaded from Synapse ( http://dx.doi.org/10.7303/syn3647478 ) as well as GitHub ( https://github.com/tum-camp/dream-prostate-cancer-challenge ). Can the authors mine some knowledge from the trained model? For example, what are the most important features? Where are the baseline (i.e. Halabi’s model 1 ) features in the ranked list? Such analysis of the model can be helpful to biomedical researchers and doctors. Response : Please see our response to question 8 of referee 1. Minor issues : In Algorithms 1 2, how did the authors choose the minimum desired performance c min and the desired set of ensemble S? Response : We chose c min = 0.66 based on results of the with-in trial validation (figure 2): approximately 30% of the experiments performed worse. The final ensemble consists of all base learners in the top 5% according the combined accuracy and diversity score (see table 2 and algorithm 2). Both c min and S remained fixed throughout our experiments and were not optimised. Page 6, paragraph 2, line 3: “Median” should be changed to “standard deviation” or some other measures of variance, because in a within-trial validation the “median” is not directly related to “the difference between observed time points in the training and test data” (lines 5-6). Response : Thank you for the suggestion, we replaced median by standard deviation in the manuscript. Page 8, paragraph 2, the last 8 lines: This example is not very convincing. A model considering all features trained on the first dataset will assign a very small (if not zero) weight to feature 3, which will compensate little for the fact that feature 3 is important in the second dataset. Response : We agree that the example was inadequate to explain this observation. We replaced it by referencing the work by Meinshausen and Bhlmann, who showed that models with embedded feature selection suffer from false positive selections in high dimensions. Page 8, paragraph 5: What numerical difficulties did the authors encounter so that they could not include the Cox regression in the ensembles? Is there anything special about Cox model that makes it harder to train than other base learners? Response : Please see our response to question 7 of referee 1. It is not explicitly stated in the paper that the authors are from Team CAMP. Response : We mentioned that we participated under the name “Team CAMP” at the end of the introduction and in the section “Challenge hold-out data”. Grammar : Page 4, last paragraph: “within-in trial validation” should be “within-trial validation”; “between trials validation” should be “between-trial validation”. Response : Thank you, we corrected these errors in the manuscript. This paper is written by CAMP, a winning team of the 2015 Prostate Cancer DREAM Challenge (“the PCDC”, or “the challenge”), to introduce their winning method. The authors built heterogeneous ensembles with the training data (Trials ASCENT-2, MAINSAIL, VENICE) and the unlabeled part of the validation data (Trial ENTHUSE-33). The high performance of their method, especially in predicting patients’ days to death, was confirmed by the challenge organizers on the validation data. This manuscript contains sufficient details about the actual method they used in the PCDC. Achilles heels for this paper include: 1) To show the necessity of using ensembles; 2) To establish the generalizability of the proposed ensemble models to new data sets. See the major issues below for more detailed comments. Major issues : The power of averaging over the base learners was taken for granted in the paper without experimental evidence. Training an ensemble costs much more effort than training a single model, and therefore it has to be shown that such effort is worth it. Direct comparison in performance between the ensemble and base learners is needed to make this point clear. Response : We included heterogeneous ensembles in the between trials validation (see figures 3 and 5) and in our discussion of the results. The training of the ensembles, in particular the ensemble pruning step, used information from the validation set. Although only the features, but not the outcomes, of the validation data were seen by the model, this practice is still not encouraged. A generalizable model should not use the validation data in any way during training. Therefore, whether the proposed method is generalizable to new data sets is in doubt. I would suggest the authors to prune the ensemble on the training set and check the performance on the validation set. Response : For survival data, the pruning step is delayed until prediction is performed, because predictions are risk scores on an arbitrary scale for which a per-sample error measure is not readily available. This is in contrast to ensemble pruning for regression problems, where a per-sample error can be easily computed and models having highly correlated errors are pruned. As referee 2 suggests, the pruning step could be performed via cross-validation on the training data. However, our pruning step does not take survival times or censoring status into account, therefore we prefer to delay the pruning step as long as possible as to avoid overfitting on the training data. If the additional costs associated with storing the ensemble before pruning are prohibitive, we recommend that pruning should be performed via cross-validation on the training data. There is no instruction in the code documentation about how to apply the code to new data sets. Adding such information can greatly increase the chance that the code will be used by other researchers. Response: Our source code is accompanied by a README file explaining all steps necessary to reproduce our results. The source code can be downloaded from Synapse ( http://dx.doi.org/10.7303/syn3647478 ) as well as GitHub ( https://github.com/tum-camp/dream-prostate-cancer-challenge ). Can the authors mine some knowledge from the trained model? For example, what are the most important features? Where are the baseline (i.e. Halabi’s model 1 ) features in the ranked list? Such analysis of the model can be helpful to biomedical researchers and doctors. Response : Please see our response to question 8 of referee 1. Minor issues : In Algorithms 1 2, how did the authors choose the minimum desired performance c min and the desired set of ensemble S? Response : We chose c min = 0.66 based on results of the with-in trial validation (figure 2): approximately 30% of the experiments performed worse. The final ensemble consists of all base learners in the top 5% according the combined accuracy and diversity score (see table 2 and algorithm 2). Both c min and S remained fixed throughout our experiments and were not optimised. Page 6, paragraph 2, line 3: “Median” should be changed to “standard deviation” or some other measures of variance, because in a within-trial validation the “median” is not directly related to “the difference between observed time points in the training and test data” (lines 5-6). Response : Thank you for the suggestion, we replaced median by standard deviation in the manuscript. Page 8, paragraph 2, the last 8 lines: This example is not very convincing. A model considering all features trained on the first dataset will assign a very small (if not zero) weight to feature 3, which will compensate little for the fact that feature 3 is important in the second dataset. Response : We agree that the example was inadequate to explain this observation. We replaced it by referencing the work by Meinshausen and Bhlmann, who showed that models with embedded feature selection suffer from false positive selections in high dimensions. Page 8, paragraph 5: What numerical difficulties did the authors encounter so that they could not include the Cox regression in the ensembles? Is there anything special about Cox model that makes it harder to train than other base learners? Response : Please see our response to question 7 of referee 1. It is not explicitly stated in the paper that the authors are from Team CAMP. Response : We mentioned that we participated under the name “Team CAMP” at the end of the introduction and in the section “Challenge hold-out data”. Grammar : Page 4, last paragraph: “within-in trial validation” should be “within-trial validation”; “between trials validation” should be “between-trial validation”. Response : Thank you, we corrected these errors in the manuscript. Competing Interests: No competing interests were disclosed. Close Report a concern Respond or Comment COMMENTS ON THIS REPORT Author Response 27 Jun 2017 Sebastian Plsterl , Technical University of Munich, Munich, Germany 27 Jun 2017 Author Response This paper is written by CAMP, a winning team of the 2015 Prostate Cancer DREAM Challenge (“the PCDC”, or “the challenge”), to introduce their winning method. The authors built heterogeneous ... Continue reading This paper is written by CAMP, a winning team of the 2015 Prostate Cancer DREAM Challenge (“the PCDC”, or “the challenge”), to introduce their winning method. The authors built heterogeneous ensembles with the training data (Trials ASCENT-2, MAINSAIL, VENICE) and the unlabeled part of the validation data (Trial ENTHUSE-33). The high performance of their method, especially in predicting patients’ days to death, was confirmed by the challenge organizers on the validation data. This manuscript contains sufficient details about the actual method they used in the PCDC. Achilles heels for this paper include: 1) To show the necessity of using ensembles; 2) To establish the generalizability of the proposed ensemble models to new data sets. See the major issues below for more detailed comments. Major issues : The power of averaging over the base learners was taken for granted in the paper without experimental evidence. Training an ensemble costs much more effort than training a single model, and therefore it has to be shown that such effort is worth it. Direct comparison in performance between the ensemble and base learners is needed to make this point clear. Response : We included heterogeneous ensembles in the between trials validation (see figures 3 and 5) and in our discussion of the results. The training of the ensembles, in particular the ensemble pruning step, used information from the validation set. Although only the features, but not the outcomes, of the validation data were seen by the model, this practice is still not encouraged. A generalizable model should not use the validation data in any way during training. Therefore, whether the proposed method is generalizable to new data sets is in doubt. I would suggest the authors to prune the ensemble on the training set and check the performance on the validation set. Response : For survival data, the pruning step is delayed until prediction is performed, because predictions are risk scores on an arbitrary scale for which a per-sample error measure is not readily available. This is in contrast to ensemble pruning for regression problems, where a per-sample error can be easily computed and models having highly correlated errors are pruned. As referee 2 suggests, the pruning step could be performed via cross-validation on the training data. However, our pruning step does not take survival times or censoring status into account, therefore we prefer to delay the pruning step as long as possible as to avoid overfitting on the training data. If the additional costs associated with storing the ensemble before pruning are prohibitive, we recommend that pruning should be performed via cross-validation on the training data. There is no instruction in the code documentation about how to apply the code to new data sets. Adding such information can greatly increase the chance that the code will be used by other researchers. Response: Our source code is accompanied by a README file explaining all steps necessary to reproduce our results. The source code can be downloaded from Synapse ( http://dx.doi.org/10.7303/syn3647478 ) as well as GitHub ( https://github.com/tum-camp/dream-prostate-cancer-challenge ). Can the authors mine some knowledge from the trained model? For example, what are the most important features? Where are the baseline (i.e. Halabi’s model 1 ) features in the ranked list? Such analysis of the model can be helpful to biomedical researchers and doctors. Response : Please see our response to question 8 of referee 1. Minor issues : In Algorithms 1 2, how did the authors choose the minimum desired performance c min and the desired set of ensemble S? Response : We chose c min = 0.66 based on results of the with-in trial validation (figure 2): approximately 30% of the experiments performed worse. The final ensemble consists of all base learners in the top 5% according the combined accuracy and diversity score (see table 2 and algorithm 2). Both c min and S remained fixed throughout our experiments and were not optimised. Page 6, paragraph 2, line 3: “Median” should be changed to “standard deviation” or some other measures of variance, because in a within-trial validation the “median” is not directly related to “the difference between observed time points in the training and test data” (lines 5-6). Response : Thank you for the suggestion, we replaced median by standard deviation in the manuscript. Page 8, paragraph 2, the last 8 lines: This example is not very convincing. A model considering all features trained on the first dataset will assign a very small (if not zero) weight to feature 3, which will compensate little for the fact that feature 3 is important in the second dataset. Response : We agree that the example was inadequate to explain this observation. We replaced it by referencing the work by Meinshausen and Bhlmann, who showed that models with embedded feature selection suffer from false positive selections in high dimensions. Page 8, paragraph 5: What numerical difficulties did the authors encounter so that they could not include the Cox regression in the ensembles? Is there anything special about Cox model that makes it harder to train than other base learners? Response : Please see our response to question 7 of referee 1. It is not explicitly stated in the paper that the authors are from Team CAMP. Response : We mentioned that we participated under the name “Team CAMP” at the end of the introduction and in the section “Challenge hold-out data”. Grammar : Page 4, last paragraph: “within-in trial validation” should be “within-trial validation”; “between trials validation” should be “between-trial validation”. Response : Thank you, we corrected these errors in the manuscript. This paper is written by CAMP, a winning team of the 2015 Prostate Cancer DREAM Challenge (“the PCDC”, or “the challenge”), to introduce their winning method. The authors built heterogeneous ensembles with the training data (Trials ASCENT-2, MAINSAIL, VENICE) and the unlabeled part of the validation data (Trial ENTHUSE-33). The high performance of their method, especially in predicting patients’ days to death, was confirmed by the challenge organizers on the validation data. This manuscript contains sufficient details about the actual method they used in the PCDC. Achilles heels for this paper include: 1) To show the necessity of using ensembles; 2) To establish the generalizability of the proposed ensemble models to new data sets. See the major issues below for more detailed comments. Major issues : The power of averaging over the base learners was taken for granted in the paper without experimental evidence. Training an ensemble costs much more effort than training a single model, and therefore it has to be shown that such effort is worth it. Direct comparison in performance between the ensemble and base learners is needed to make this point clear. Response : We included heterogeneous ensembles in the between trials validation (see figures 3 and 5) and in our discussion of the results. The training of the ensembles, in particular the ensemble pruning step, used information from the validation set. Although only the features, but not the outcomes, of the validation data were seen by the model, this practice is still not encouraged. A generalizable model should not use the validation data in any way during training. Therefore, whether the proposed method is generalizable to new data sets is in doubt. I would suggest the authors to prune the ensemble on the training set and check the performance on the validation set. Response : For survival data, the pruning step is delayed until prediction is performed, because predictions are risk scores on an arbitrary scale for which a per-sample error measure is not readily available. This is in contrast to ensemble pruning for regression problems, where a per-sample error can be easily computed and models having highly correlated errors are pruned. As referee 2 suggests, the pruning step could be performed via cross-validation on the training data. However, our pruning step does not take survival times or censoring status into account, therefore we prefer to delay the pruning step as long as possible as to avoid overfitting on the training data. If the additional costs associated with storing the ensemble before pruning are prohibitive, we recommend that pruning should be performed via cross-validation on the training data. There is no instruction in the code documentation about how to apply the code to new data sets. Adding such information can greatly increase the chance that the code will be used by other researchers. Response: Our source code is accompanied by a README file explaining all steps necessary to reproduce our results. The source code can be downloaded from Synapse ( http://dx.doi.org/10.7303/syn3647478 ) as well as GitHub ( https://github.com/tum-camp/dream-prostate-cancer-challenge ). Can the authors mine some knowledge from the trained model? For example, what are the most important features? Where are the baseline (i.e. Halabi’s model 1 ) features in the ranked list? Such analysis of the model can be helpful to biomedical researchers and doctors. Response : Please see our response to question 8 of referee 1. Minor issues : In Algorithms 1 2, how did the authors choose the minimum desired performance c min and the desired set of ensemble S? Response : We chose c min = 0.66 based on results of the with-in trial validation (figure 2): approximately 30% of the experiments performed worse. The final ensemble consists of all base learners in the top 5% according the combined accuracy and diversity score (see table 2 and algorithm 2). Both c min and S remained fixed throughout our experiments and were not optimised. Page 6, paragraph 2, line 3: “Median” should be changed to “standard deviation” or some other measures of variance, because in a within-trial validation the “median” is not directly related to “the difference between observed time points in the training and test data” (lines 5-6). Response : Thank you for the suggestion, we replaced median by standard deviation in the manuscript. Page 8, paragraph 2, the last 8 lines: This example is not very convincing. A model considering all features trained on the first dataset will assign a very small (if not zero) weight to feature 3, which will compensate little for the fact that feature 3 is important in the second dataset. Response : We agree that the example was inadequate to explain this observation. We replaced it by referencing the work by Meinshausen and Bhlmann, who showed that models with embedded feature selection suffer from false positive selections in high dimensions. Page 8, paragraph 5: What numerical difficulties did the authors encounter so that they could not include the Cox regression in the ensembles? Is there anything special about Cox model that makes it harder to train than other base learners? Response : Please see our response to question 7 of referee 1. It is not explicitly stated in the paper that the authors are from Team CAMP. Response : We mentioned that we participated under the name “Team CAMP” at the end of the introduction and in the section “Challenge hold-out data”. Grammar : Page 4, last paragraph: “within-in trial validation” should be “within-trial validation”; “between trials validation” should be “between-trial validation”. Response : Thank you, we corrected these errors in the manuscript. Competing Interests: No competing interests were disclosed. Close Report a concern COMMENT ON THIS REPORT Views 0 Cite How to cite this report: Ankerst DP. Reviewer Report For: Heterogeneous ensembles for predicting survival of metastatic, castrate-resistant prostate cancer patients [version 3; peer review: 2 approved, 1 approved with reservations] . F1000Research 2017, 5 :2676 ( https://doi.org/10.5256/f1000research.8853.r18609 ) The direct URL for this report is: https://f1000research.com/articles/5-2676/v1#referee-response-18609 NOTE: it is important to ensure the information in square brackets after the title is included in this citation. Close Copy Citation Details Reviewer Report 19 Dec 2016 Donna P. Ankerst , Department of Mathematics, Technical University of Munich, Garching, Germany Approved with Reservations VIEWS 0 https://doi.org/10.5256/f1000research.8853.r18609 The authors are to be congratulated for landing among the circle of winners of the Prostate Cancer DREAM Challenge and for clearly describing their innovative methods in this paper. An informative discussion critically appraises their approach, providing suggestions for advancing ... Continue reading READ ALL 