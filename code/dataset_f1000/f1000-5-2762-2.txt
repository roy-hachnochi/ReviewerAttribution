The author presents an interesting solution to the problem of selecting the best threshold for a binary classifier which defines a bioinformatics algorithm, when the proportion of positive and negative samples in nature is unknown. His approach consists on using the minimax approach, considering that the goal is to maximize the gain/accuracy (in the specific the case where the yields are 1 for TP and TN, and 0 for FP and FN) in a worst case scenario. For that, the author describes the utility as a function of both the threshold and the nature proportion of negatives q_n. Based on the analysis of page 3, given any action of the player (a point in the ROC curve, defined by a threshold), its worst performance (lower utility) will be obtained for some q_n value. The idea is to maximize this minimal, across all possible actions (pairs of FPR,TPR across the ROC curve). This is obtained when the point in the ROC complies with TPR+FPR=1. The article is original in the sense that it proposes a valid requirement for the design, to be robust against the unknown state of nature (proportion of positives and negatives), deriving a threshold selection that attains that condition (FPR+TPR=1). Despite the originality of the approach, the article needs minor revisions. In particular it seems the manuscript deals with aspects of pattern recognition from the perspective of Bioinformatics. Although understandable, this is a pattern recognition problem and the work would improve significantly when the role of ROC curves on classifier design, and the mathematics leading to the main result are explained with more detail and improved formalization. Below follows a more detailed list of issues that need to be addressed: The author presents this article in a bioinformatics context that is never specified. For example, in the introduction the authors says "Many bioinformatics algorithms can be understood as binary classifiers, ...", and then refers to "score based binary classifiers". At no part of the article the author provides a list, or some examples, of such algorithms in bioinformatics. Since this is presented as a bioinformatics article, it would be good to include some of such algorithms, which surely were the inspiration for this work. Alternatively, this might be presented in the more mathematical setting of pattern recognition, although this might require more profound changes. Also, it is clear that the problem studied in this article is the one where the classifier model is already defined, producing one ROC curve, and the only adjusting parameter the user has is the threshold. Although this is a rather typical solution in computational biology, it should be mentioned that if the user were able to adjust the classifier, better results might be obtained. There are many references in literature of robust classifier design, some are indicated below. The author should review the statement that ROC curves are often being used for the training of classifiers. The training of classifiers is usually done by minimization or maximization of quality metrics, via the adjustment of the classifier parameters, once the model is defined. Few works are related to training classifiers based on ROC (Some examples are Blockeel, 2002 1 and Srinivasan, 2001 2 ). On the other hand, classifier evaluation (after training) and model selection are often done via ROC curves. For example, in the second paragraph of the introduction, the author comments that "Binary classifiers are often trained and compared under ... the ROC curve", presenting a reference on the ROC curve, but not to classifier training. Here the term "training" may be confusing, since it is used, usually, for the process of selecting the parameters of the classifiers, from the data, once the model is already selected 3 . Usually the ROC is used for model selection, where different classifier models are trained (without considering the ROC), and then compared based on their ROC, as described in the reference 2 of the paper. Based on the previous comment, the author should also review the sentence, in the same paragraph, where he states "Classifier training often aims at maximizing the area under the ROC". Actually, the training aims, usually, to minimize error, or cost, or other measures of accuracy. Reference 4 of the article, in the first paragraph of the introduction, also makes clear the difference between model building (training), and model evaluation, where the ROC is used. In general, the assumption on a=d=1, c=b=0 could start earlier, simplifying the concepts and equations 1 and 2. The mathematical proof that the minimax threshold is obtained by FPR+TPR=1 requires some elaboration. In one hand, given a classifier trained from data, with a variable threshold t, both FPR and TPR are functions of the threshold, so they would be FPR(t) and TPR(t). From this, the utility is dependent of two variables, q_n and t, so it would be Utility(q_n,t). Now, the problem of finding a minimax value of t would be a bit more complex than the solution described in the article. Still more, one should impose the restriction on the curve (TPR(t),FPR(t)) to be increasing, so that if t2=t1, then TPR(t2)=TPR(t1) and FPR(t2)=TPR(t1). If such condition is held (which is expected on a properly designed ROC curve), then the proof that FPR(t)+FNR(t)=1 is the minimax solution is almost straightforward. Without that condition defined above (which is not defined in the paper), one could have a pair (0.8,0.2) which complies with FPR+TPR=1 and (0.81,0.18) that does not comply with that rule, and where the minimum utility (as function of q_n) for the first pair is 0.8, and for the second pair is 0.81, which disagree with the mathematical result presented in the paper. The key here is the increasing nature of the ROC curve. An increasing ROC curve (where TPR increases when FPR increases) could not contain these two points, since an increase in TPR (0.8 to 0.81) should be accompanied by an increase on the FPR (but it goes from 0.2 to 0.18). Equation 1 uses q_n, which is only defined later, after the equation. The author presents the work in the general context of zero-sum game, with general yield values, but for the main theoretical result, the problem is reduced to the specific situation of a=d=1 and b=c=0. Therefore, the result is not general, only specific to that situation, so it is a little superfluous to maintain all these generic yield values across the paper. For example, Equation 1, when using the costs a=d=1, c=b=0, studied in the theory section, reduces to skew ratio = q_n/q_p, which is more informative for the rest of the paper. In the discussion the author talks about maximizing robustness, but actually the goal is to design "a robust classifier" 4 . The discussion section also states that the robustness is obtained against a) skew ratio and b) yield skew. This last part (b) is not proved in the paper, since the theoretical results show a minimax results (robustness) against skew ratio, for the specific case of a=d=1 and b=c=0. It seems true that no other methods for choosing the operating point present a good rationale beyond mathematical considerations. To strengthen the impact I suggest the author should at least name some of such approaches, like the three ones described by Song, 2014. In a situation of continuous ROC curve, this point should always exist, since it is an increasing curve from (0,0) to (1,1). In the discrete ROC case, which is the usual one when the classifier and the ROC are based on a finite amount of data, this intersection may not occur, and the thresholds may have to be selected using an appropriate interpolation technique. This needs to be clarified. Finally there is a question about the approach. Despite the theoretical attractiveness of the proposed approach, it is not clear how the maximization of expected utility is a desirable quality for a classifier being used in bioinformatics. There is a lot of study and discussion about whether is it better to optimize accuracy, precision, recall, etc, and how each application may require a different goal, but there is not so much discussion about why a classifier should maximize utility in a zero-sum game. The author could address this issue, with a brief statement on the advantages of this measure (the utility) in the context of bioinformatics. References 1. Blockeel H, Struyf J: Deriving Biased Classifiers for Better ROC Performance. Slovene Society Informatika . 2002; 26 (1). 2. Srinivasan A Machine Learning . 2001; 44 (3): 301-324 Publisher Full Text 3. Jain AK, Duin RPW, Mao J: Statistical Pattern Recognition: A Review. IEE Transactions on Pattern Analysis and Machine Intelligence . 2000; 22 (1). 4. Dougherty E, Hua J, Xiong Z, Chen Y: Optimal robust classifiers. Pattern Recognition . 2005; 38 (10): 1520-1532 Publisher Full Text Competing Interests: No competing interests were disclosed. We confirm that we have read this submission and believe that we have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however we have significant reservations, as outlined above. Close READ LESS CITE CITE HOW TO CITE THIS REPORT Brun M and ten Have A. Reviewer Report For: Optimal threshold estimation for binary classifiers using game theory [version 3; peer review: 3 approved] . F1000Research 2017, 5 (ISCB Comm J):2762 ( https://doi.org/10.5256/f1000research.11243.r18290 ) The direct URL for this report is: https://f1000research.com/articles/5-2762/v2#referee-response-18290 NOTE: it is important to ensure the information in square brackets after the title is included in all citations of this article. COPY CITATION DETAILS Report a concern Respond or Comment COMMENT ON THIS REPORT Version 1 VERSION 1 PUBLISHED 25 Nov 2016 Views 0 Cite How to cite this report: Diambra L. Reviewer Report For: Optimal threshold estimation for binary classifiers using game theory [version 3; peer review: 3 approved] . F1000Research 2017, 5 (ISCB Comm J):2762 ( https://doi.org/10.5256/f1000research.10895.r17994 ) The direct URL for this report is: https://f1000research.com/articles/5-2762/v1#referee-response-17994 NOTE: it is important to ensure the information in square brackets after the title is included in this citation. Close Copy Citation Details Reviewer Report 07 Dec 2016 Luis Diambra , Centro Regional de Estudios Gen√≥micos, Universidad Nacional de La Plata (UNLP-CONICET), La Plata, Argentina Approved VIEWS 0 https://doi.org/10.5256/f1000research.10895.r17994 The author presents a criterion to choose the operating point for a binary classifier. This criterion is analyzed in term of the game theory. By using the mininax principle author proposes to use as classifier threshold the intersection between the ROC curve and ... Continue reading READ ALL 