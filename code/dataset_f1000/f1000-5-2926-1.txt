 Breckels et al. have written a very nice piece on analysing appropriate proteomics data for subcellular localisation. I particularly like the " workshop characteristics " of the text. Which allows a novice, but interested reader to work through the analysis stepwise and reproduce the results described therein. The authors took great care in keeping this ideal up during their text and this is also where I have put my greatest reservation to the manuscript in its present form - since a reader cannot work through the code presented in the manuscript, since there at at least two situations where a readily available HPC and quite some time is required. This kind of leaves a dent in my impression - however, given this can be resolved as well as some typos - the workflow report is superb. Major comments: Next to reducing the dimensions of data for visualisation, PCA also offers a way to understand how the variability is distributed across the multidimensional data by providing linear combinations of the variables which then constitute the actual PCs. On that note it would be nice to mention this in Visualising markers section on page 16, where PC7 explains not much variability but due to the correct weighing of the variables we do get a separation between mitochondrial and peroxisome. This then can be further motivated with Figure 9 - where we probably can see that the weights for the fractions where the two localisations differ are larger than otherwise. I was unable to reproduce Figure 13 comparing the two MSnSets. While I was able to look at each set separately using pRolocVis(hllst@x[[I]]), where i is 1 or 2, I only got an error using the code from the manuscript: pRolocVis(hllst, app=“compare”) Subsetting MSnSetList to their common feature names 5032 features in common Remapping data to the same PC space Error in (function (od, vd) : object and replacement value dimnames differ Error in pRolocVis_compare(object, ...) : object 'idDT' not found When using ‘remap=FALSE’ it actually works, but since this makes barely sense it is of no use - but just as a hint at debugging it. You really need to make the results from the phenoDisco classification available too. It is super disappointing that one cannot continue reproducing the code from page 23 on, because it takes 24 hours to compute it using 40 cores… The above comment is of course also true for the KNN TL Optimisation on page 33 - this needs to be downloadable, since not everyone has access to Cambridge’s HPC and probably even less have 76 hours to spare. Your comment on the increase suitability of classification instead of clustering (when additional information on classes is available) at the bottom of page 35 could be more pronounced - for educational reasons. Minor comments: I was not able to navely reproduce the workflow from the R commands in the article due to an error installing pRolocdata on a Windows machine. On OS X it was smooth. On page 10 line 2 there is a ‘to’ missing. I never came across the verb imputate in the context of missing values, I guess the proper term is impute. On page 11 the image2 function is called after the filterNA function a couple of lines above. This however would result in an only black heat map (since there are no more missing). The image2 function should be called before the filterNA function. Since the reader does not see the chunk options, it could be puzzling. For completeness sake there should also be an install.packages(c(“hexbin”, “rgl”)) somewhere to generate the second PCA-plot and the 3D plot. Moreover, Mac users will need to install xquartz to use rgl properly. On page 14 the plotting code chunk is off track - in the middle of the marker sets output. On page 18: …wanted to highlight a proteins with the… - lose the a and later in the sentence there is a ‘ create a ’ too many. Direct comparisons of individual channels in replicated experiments do not provide… You may want to consider adding a layout(1) or similar, after changing the mfrow argument of the parameters to accommodate 2 panels, such that the uncanny reader does not get confused. I would prefer links to referred sections of the text, but that may be personal taste… Page 23: One should note that the decreasing the GS, and increasing the … at least one the too many, probably two. On page 25: We find the general tendancy to be that it is not the choice … tendency? On page 28 you refer to ‘…the code chunk below…’ for Figure 17, however, the following code chunk is generating Figure 16 (which is above and btw not referenced in the text). Maybe force your figures a little to float where you want them/refer to them. On page 28: …by extracting the median or 3rd quantile score per organelle… do you mean quartile? Otherwise I do not follow. On page 32: …package to query the relevent database … relevant? On page 32 - there is something wrong with this sentence: To remove the 4 classes and create a new column of markers in the feature data called tlmarkers to use for the analysis : On page 34: From examining the parameter seach plots as described in section Optimisation… search! On page 36: …and later reload the object using save. - that would be ‘load’ then! On page 38 - I fully agree with the following sentence, but right after the updating comment it kind of seems ‘ misplaced ’? Maybe add a title like ‘ Getting help ’? It is always important to include session information details along with a short reproducible example highlighting the problem or question at hand. 