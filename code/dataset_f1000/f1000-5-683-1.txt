Thank you for the opportunity to review this manuscript. This is a well-done study, and the conclusions follow from the results. We would recommend accepting the article once all clarifications and revisions have been made, or the lack of doing so adequately justified. A. While brevity is generally to be admired, we would recommend a bit more detail about the statistical analyses. These are critical, but are reduced to 3 sentences and a referral to the programming language through an external link. We would suggest that the main text include the (brief) discussion of the analyses done, and rationale for them, rather than have those relegated to the external link. B. The interpretation of the findings seems to be attributing causal factors - an A leads to B consideration - for which the control of variables is too limited. We believe that interpreting these as associations would be more consistent with the findings. Consider the statement: "Journals and editors should also think carefully about the optimum number of peer reviewers per paper. With each extra reviewer, we found that an extra 7.4 days are added to the review process." Given that there appeared to be no inclusion of either article quality or complexity in the evaluation, is it not possible that issues within the article itself required the use of additional reviewers (i.e. a B leads to A perspective)? Perhaps extra reviewers with specific expertise was required, or concerns with potential problems in the manuscript led to consultations with other reviewers. It does not seem safe to assume that it was the addition of the reviewer that added extra days. Similarly, the study centers around the role of the editor in the reviewing process, and the discussion suggested that the involvement of the reviewing editor as a peer reviewer expedited the process. There was little discussion of other factors that could have accounted for the statistical results. For example, perhaps the reviewing editor selected articles that piqued his or her interest, or were more clearly presented. Perhaps the reviewing editor selected to review at times more convenient to his or her workload, while other reviewers did not have such an option. The reviewing editor might select to review articles perceived to be of greater or timelier value to the journal itself, which may increase the speed of the review. Specific questions: A. According to their method section, the authors state that they began with an initial N=9,589. After purging other articles they had an N=8,905. They then isolated a total of 2,750 articles subjected to the peer review process for the study: "For the rest of the paper, we focus our analysis on this subset of 2,750 papers, of which 1,405 had been accepted, 1,099 had been rejected, and the rest [which would equal 246] were still under consideration." Looking at the Excel spreadsheet for citation counts, there are 1407 lines with entry numbers. For peer-reviewed papers, excel spreadsheet has 2747 (after removing duplicate entries based on the MS NO column) entries for manuscripts numbered up to 12621. The excel spreadsheet for unique reviewers has 2747 entries, with a final MS NO of 12621. The numbers do not appear to match, and there is no explanation for that in methods. Exactly how many manuscripts were reviewed, how many rejected and why, and how many were tracked? B. In the Excel spreadsheet for citations, the second column was titled "Citations," but these figures do not appear to have any relation to the Scopus citation numbers. What numbers were used for the actual citation counts? We also note that we find the suggestions by other reviewers compelling, and would be happy to review a revision of this manuscript should that be considered useful.