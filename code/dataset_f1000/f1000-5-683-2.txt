Giordan et al analyzed 2,750 manuscripts sent out at the journal eLife for peer review (of which 1,405 ended up published). The authors compare papers in which the editors functions as ‘reviewing editor’ that is as one of three referees. Globally, and at almost every decision stage the process is accelerate significantly if the reviewing editor functions as one of the referees, with no or very small impact in author revision time and citation rates, respectively. The authors calculate that every additional external referee adds 7.4 days to the process and suggest that journals strive to balance the need for covering all the required expertises carefully with the negative effect on the speed of evaluation. The quality and speed of the peer review process are topics of active debate. Despite widespread criticism, the publication in certain peer reviewed journals continues to directly impact research assessment by both funders and institutions. The quality and fairness of the process is therefore paramount not only to assure the reliability of the literature, but also to inform research assessment in a balanced manner. Notwithstanding the slow delivery of this particular referee report, speed matters in particular in fast moving and highly competitive research areas like the biosciences. Quantitative evidence that well defined aspects of an editorial process has positive effect on quality and or speed is therefore of significant importance. The authors have carefully analyzed a decent sized dataset and report a statistically significant effect of a well defined change in the editorial process, while also showing evidence that this change has no detrimental effect on the quality of the editorial assessment, at least as far as the outcome is analyzed (here, in terms of two parameters: revision time and citation rate). While this manuscript makes a significant contribution, I have a number of suggestions I would invite the authors to consider in revision: Textural: Abstract/main text; Background: It is not merely the growth of the number publications that puts the system under pressure (after all, in principle the editorial/peer review process may well be able to scale with increased research output), but rather the increased pressure publish in a small number of high Impact Factor journals in an effort to optimize chances of a positive impact on research assessment. Please introduce the journal eLife, including the scientific scope, as different communities have widely different peer review and citation cultures and this will likely affect the findings reported here. Abstract: Results. As presented, I found it confusing that the first sentence describes an apparently sizeable difference between accepted (10 days faster) and rejected (5 days faster), while the next sentence states ‘there was no effect on whether submissions were accepted or rejected)’. Abstract: the dataset is described as consisting of an analysis of 8,905 submissions, when in reality the 2,750 papers sent for review were analyzed. This could be formulated more clearly. I would suggest for clarity to remove the ‘False’ and ‘True’ nomenclature and change to ‘reviewing editor’ and ‘editor’ assessment or similar. Analysis: It is unclear to me if the authors can exclude any biases in terms of which manuscripts were selected for formal review by an editor vs. outside only refereeing. Have the authors attempted to assess possible bias? For example, maybe the reviewing editors tend to review the manuscripts themselves that strike them as the more interesting ones, or maybe certain subject areas are preferentially subject to one approach. I am missing a clear definition of what editorial peer review is and what is not. It is clear that this is likely not a completely binary situation and the authors do not describe how the decisions were parsed so clearly into the two groups (on p3. ‘more of a supervisory role’ vs. less of such a role sounds quite vague). Why were appealed manuscripts removed form the analysis? This may introduce a bias as possibly erroneous decisions are excluded. How many appeals were excluded? It is unclear to me if all paper invariably had three referees (i.e. 3 outside or 2 outside+reviewing ed.). I assume some papers only had two referees. Were they excluded? If not, how did these score for speed, revision time and citation? The definition of ‘Total Time’ on p3. Is unclear: it is stated to be both ‘first submission-acceptance’ and ‘first submission to publication’: which one is it? Please state that Scopus citations were assessed when fist introducing the topic. I would recommend to use the same time window for all papers (e.g. 12 months after publication) as this renders citation rates more comparable. Why was the eLife website scraped for citation rates, and not the primary Scopus database – that data may in principle be more reliable. The 10 days (accepted) vs. 5 days (rejected) faster: is this simply the additive effect of 2 rounds vs. 1 round of review? Please include basic stats information in the figure legends – in particular fig 2, where the numbers will decrease dramatically for ‘revision 1’ and ‘revision 2’. It is unclear to me if reviewing editors were invariably faster the outside referees. It would be useful to quantify this and assuming there is a striking difference to speculate why – it is the individuals selected by e-Life or due to policing or incentives provided by the journal? After all, similar strategies could be applied to outside referees. On a related point, it would be useful to quantify if the reports by the reviewing editors were qualitatively different (e.g. length). One assumes the ultimate decision on the manuscript was as also much better correlated with the reports by reviewing editors than those of the outside referees. I am confused: in fig 2 ‘Reject full submission’ in revision 0 and revision 1 is slower than ‘accept’. This seems to be the opposite to fig 1 and in fact less intuitive than the results in fig 1. Since manuscripts are rarely re-reviewed (see p1), are all the datapoints displayed in ‘revision 1’ and ‘revision 2’ for re-review processes? For fig 4, I would suggest to plot the manuscript load/editor. Non-essential further reaching analysis (suggestions): it would have been useful to measure and present the acceptance/rejection rates of manuscripts assess by three outside referees compared with two referees + reviewing editor. it would have been useful to quantify the % of agreement between the reviewing editor and the outside referees, compared with agreement between the outside referees. 