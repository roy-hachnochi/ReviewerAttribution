The authors introduce the two winning workflows (WFs) from the Teach-Discover-Treat competition 2014 featuring ligand-based virtual screening pipelines for anti-malaria compounds together with results from an experimental follow-up study. The workflows show hit rates of 57% (52% WF1, 100% WF2) in a prospective study on a rather small sample size due to compound availability and funding reasons (72 WF1/ 9 WF2 novel compounds). The article is well written, easy to understand and the study showed promising results in finding new active compounds. Furthermore, both workflows are available to the community. Minor comments that could be addressed to improve the manuscript: Methods/Workflows: - More detail on the more advanced ML techniques, number of features, and especially feature importances would be helpful for the reader. - The data set is highly unbalanced (~1.5K actives vs. 290K inactives). Could one expect a boost in performance when using under-/oversampling methods? - This may have been addressed in the competition itself, but it would be interesting to see how a simple model performs on the data, for example simply ranking by similarity to known actives? Evaluation: - The authors admit the little flaw in the original WF1 that the top 1000 molecules accidently represent a random selection from the top 10K. It’s hard to compare the results now that the selection and testing phase is over, but it would be nice to see some evidence that the intended selection strategy would actually have been superior. E.g. the positions of the intended ranking could be included in Table 5, the prospectively tested set is small but a trend may become apparent? - Since 31 of the selected 114 compounds (~30%) were present in the HTS, I’m wondering how many of the HTS compounds were present in eMolecules in total? Because they have been used for training, taking them out of the evaluation would be more convincing and would probably also improve the rankings of the tested compounds. - The authors claim that the two methods pick generally similar compounds, which can be somehow expected from the design of the two WFs (similar MLs and fingerprints). Nevertheless, this trend is not obvious to me from the few mentioned values, e.g., 7 compounds selected from WF2 are in top 10K from WF1 (page 8). It would be more meaningful to calculate the overlap of the top 1000 compounds between the methods or the similarity between these compounds (also with respect to different top 1000 selections in WF1, see point above). 