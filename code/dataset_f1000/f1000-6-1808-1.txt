It’s good to read a thoughtful reflection on how peer review panels function given their importance for research. This study looked at a funding scheme that covered a range of fields and had an important aim of funding international groups to work on overseas aid projects. It would be worthwhile citing the recent systematic review in this area (Guthrie et al., 2017 1 ), which includes some additional references that consider the question whether panel meetings are worth the additional costs compared with independent reviews. The panel’s gender split was nowhere near 50:50. This was commented on in the results in that it: “proved difficult given the relative paucity of women and African scientists in these three research areas.” Part of the criteria for funding is “Support for female scientists”, but the group organizing the panel is ironically not adhering to this. Taking part in a panel can benefit researchers as it is a tremendous way to learn how to write a good proposal. As Guthrie et al state, “Members of funding panels may also benefit directly from their membership.” Hence not selecting women on the panel may just perpetuate the “paucity” of female scientists in this area and the funding agencies could be a key group for addressing this problem. Was there any discussion of the funding line and how many proposals might be funded? This often shapes the discussion as if only a handful of proposals can be funded then the panels members know that only the very best will get funding. The authors state: “There is evidence that pooling scores across a panel increases reliability compared to using scores produced by individuals’ reviewers (Fogelholm et al. , 2012)” But the main conclusion of Fogelholm’s paper was: “panel discussions per se did not improve the reliability of the evaluation.” What order were proposals discussed in? The order of the day can matter (e.g., reviewers being more alert in the morning), and ideally the proposals should be assessed in a random order to remove any long-term biases (e.g., from alphabetical ordering). What feedback was given to applicants? This is an important part of the process and given that panels cost time and money to assemble it would be worth giving detailed feedback so that applicants can improve. A funding scheme that I was involved in gave audio transcripts of the panel meeting to applicants (Barnett et al., 2015) 2 , which was greatly appreciated by applicants. This point maybe outside the scope of the paper, but there is a great emphasis on “excellence” in this funding scheme, but “excellence” has little meaning in research as pointed out here: https://www.nature.com/articles/palcomms2016105 3 , and the authors of that paper conclude that “soundness and capacity-building” are far better criteria. Minor comments: Did any of the panel members also have applications being assessed? It would be useful to include a link to the funding scheme or a list of the winning proposals. I was surprised to read the Journal Impact Factors were used, given that they are a poor proxy the individual quality of papers. The authors state: “The peer review process is known to be influenced by the constitution of the panel with more deference between members of multi-disciplinary panels in which their expertise does not substantially overlap (Lamont Huutoniemi, 2012), a finding which was confirmed by this study.” I did not read this as a major finding of this study and it was not highlighted in the results, nor was there any formal analysis that covered this. I think this needs expanding in the results to make such a strong statement in the conclusion. “There was also greater consensus among the panel regarding applications to be recommended or not recommended for an award” This finding was not shown in results. 