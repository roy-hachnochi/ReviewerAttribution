Thank you for providing answers to my specific questions, and for the detailed description of the method. This clears up several matters. The addition of Figure 6 is helpful. You may additionally want to calculate a p-value to highlight how unexpected your result is compared to the distribution generated from bootstrapping. However, I still have some reservations about the evaluation of the results. The method described in the paper returns a set of proteins that are similar in function to a target set. The way I would expect to see this evaluated is to compare the results for the RAS pathway against a gold standard set which would be determined beforehand and would include all and only the proteins that should be annotated the same as the RAS pathway. The paper should justify how this gold standard was determined, whether it was hand annotated or otherwise. I would then expect to see the recall and precision of the method calculated against this gold standard, along with an error analysis to show what types of proteins are false positives and negatives, and to discuss why these proteins are returned/missed. Sorry for not making this clearer in the original report, but this is what I intended by questions 5-7. The discussion of the results shown in Figures 1-4 centres on the true positives, but it is discussion and quantification of the false positives and false negatives that will provide credibility to your method. As it stands now, the evaluation that is provided in the paper is not sufficient for me to feel comfortable about using the results for a biological analysis.