The question of how to predict functions for proteins with unknown function is interesting and highly relevant to biology. Using a machine learning strategy, as is done in this work, is a reasonable way to approach this task. This paper by Jung and Ge presents a method to assign functions to proteins along with code which is available on github and installable via bioconductor. Thank you for making your code open source and easily available to install and use. I was able to run the R code and reproduce the figures in the paper following the given use case and using the comment left by the previous reviewers. However, the methods section in the paper does not provide sufficient details for me to recreate the software. Much of what is written in the methods section is introduction to the methods, and describes in general how they work. It does not provide a detailed description of how these methods are applied to the problem the authors are trying to solve. Even though the code is provided, the application of the methods must be described in detail in the paper. Further, the R package includes the pre-trained (extremely large, 3Gb) model and neither the code nor the paper includes details on how to reproduce the model, or how to train it again on new data. Specifically, the following questions must be clearly answered, and a figure to illustrate the method would be a helpful addition. What are the features provided to the OCSVM, and what is the output? What are the features provided to the subsequent SVM, and what is the output? How was a gold standard set of annotations (for each organism) selected? On what data was the model trained and how can the model be reproduced? How was the data partitioned for training, testing and evaluation of the SVM? What is the performance of the two SVMs on their respective test sets? What evaluation methods do the authors use ensure that their models are not overfit? What parameters were used for each SVM, and how/why were they selected? The use case predicts mouse proteins that have similar function to the Ras signalling pathway as defined by KEGG. Figures 1 through 4 present different ways of visualizing the GO terms and KEGG pathways that the predicted proteins are related to. GO terms such as "kinase activity" are significantly overrepresented in these proteins, which is confusing since the Ras pathway signals via GTPases, not kinases. Figure 5 shows a STRING network of the proteins in the Ras pathway, together with the top 20 predicted proteins. The authors claim that because there are STRING interactions between these two sets of proteins that this is support for them having the same function. To support this, I would at least like to see this number of interactions compared to those between a random set of 20 proteins and the proteins of the Ras pathway, if not additionally some error analysis of proteins that are also connected to the pathway but not picked by the algorithm. It is also not clear to me if the training data for the SVMs make use of the network interactions; if it does, then providing STRING interactions as support for validating the output is circular. Since the paper leaves open some fundamental questions in the design of the machine learning approach, I cannot approve this article without major reservations. I hope that the authors will add these additional details, since this article addresses an interesting problem, and I am curious to see their method described in detail. Some minor comments on the article follow: All acronyms such as ORA should be defined in the text. The Figure 1 generated by the example code is not exactly the same as the plot included in the manuscript, but is very close. The figure caption for Figure 1 should specify what the size of the points means. I cannot replicate Figure 2 with the code provided, but it seems that the underlying data is the same as the published figure. The package PPInfer should depend on limma, KEGG.db and GO.db since these are necessary to run the use case. The text for the interactive figures is too large, making them very difficult to read - although it is not clear to me if this is a bug in the F1000Research website itself or in the underlying figure. 