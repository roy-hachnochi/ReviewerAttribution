I applaud Kemperman and McCall on the work they put forward to make methods comparisons more accessible to researchers interested in microRNA quantification. I generally agree with Dr. Waldron's comments. Specific points where I have reservations include: 1) I am still not sure how users can add their own data to compare various methods on it. It seems like if one selects "custom" for one of the two methods, then one must load a processed version of the same dataset in order to compare a new algorithm to existing algorithms. 2) In general, I think Dr. Waldron's comment on whether users still need to use the Bioconductor package directly is very valuable. It partly depends on what users the authors have in mind for the shiny app in terms of their level of R/bioinformatics expertise. Two specific examples here: a) If an unfamiliar user were to use this app, it seems like they would still need to go to the Bioconductor package to get the dataset. Perhaps the authors could include this dataset in the app and make it easier to download as both a CSV file and an .RData file? It would also be worthwhile to include a citation to the dataset, as opposed to needing to go through Bioconductor. b) More explanations need to accompany the plots and explain exactly what is being plotted and why - each plot should at least include the same type of level of detail used when writing a figure caption in a scientific journal. A more detailed introduction should also be provided, along with links to the Bioconductor package(s) and to this paper, but written so that it is at least somewhat self-contained. For example, the first plot, "Limit of detection" appears to not be a comparison plot at all, but rather to show just the limit of detection for the first method. What should the user expect to see here for a "good" method? What does "Proportion Poor Quality" even mean (is there a threshold that can be changed to indicate this, why is there never a boxplot for the proportion = 1?) For "Accuracy" it is also not clear what "percentage of data to exclude" means (why is it being excluded? quality issues?) For "Accuracy" and "Precision," the Low/Medium/High values on the x-axis should be described, along with stating that within each category, the methods are being compared (maybe some dashed vertical lines between categories would also help here). In general, it should be indicated what one should look for in terms of one method having better performance compared to another. I strongly encourage the authors to make these changes/additions in order to allow a larger number of individuals to use and benefit from their tool. 