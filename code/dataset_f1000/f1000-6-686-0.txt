General comments: In this contribution, the authors present part of a system to infuse models and the data linked with them with semantic content through a high-level programmatic interface, bringing digital content more in line with the FAIR guiding principles. The manuscript presents some encouraging progress and clearly represents a formidable effort, however, does reveal some concerning (but certainly addressable) issues with the system and the ontological framework it depends upon. In my opinion, the strongest component of this contribution is the consolidated and practical approach to adding semantics to the toolbox of modellers. To me, material like code snippet (14) represents a hugely exciting and powerful advancement; I would be much more supportive of this submission if it focused primarily on these capabilities. The transparent and systematic handling of naming authorities and connection to internationally endorsed vocabularies is also promising, as is the care taken to prevent these conventions from dictating more rigorous semantics. As stated in the MS, the potential to use semantics to bridge these resources is immense and the authors’ work in this direction is of high value. I believe the weakest part of this work is the semantic framework described within it. I couldn’t find the ontologies themselves, so evaluation is based only on the descriptions in the manuscript. Frankly, I find the approach to handling "observables" haphazard and something of an uncomfortable shortcut relative to semantics developed in, e.g., the Biocollections Ontology (BCO). Initially, such shortcuts may appear to relieve a burden of using semantic technologies on end users, however, this quickly becomes uncontrolled. Further, in my experience, the commitment to realism in ontology (i.e. avoiding dealing with concepts and focusing on empirical phenomena) development is essential in the scientific realm to prevent rapid semantic drift between initiatives. The treatment of scale here seems to lend itself to this issue, unless the semantics reflect that scales overlap (I couldn’t find any evidence of this). Expecting a coherent "common sense" approach to prevail rarely bears fruit across multiple groups without concentrated effort at parsing out their knowledge into some mature upper level framework. Further, I'm left wondering why there doesn't seem to be a developed attempt at reuse of ontologies such as BFO, DOLCE, or other upper level resources. The authors mention that they use BFO, but the framework they propose seems to counter many of its basic semantics without clarification or logical justifications. I’m afraid I can’t really understand how the reuse of these foundational and well-adopted semantics is actually implemented or if it’s defensible. That could be a paper in itself and it probably should be if the authors want to defend their proposals more completely: in this format, it’s very hard to endorse the proposals. I highlight a few specific issues in my detailed comments below. Much the same can be said for the use of mid-level ontologies like the TO and ENVO in which the authors point out some important flaws/limitations. Generally speaking, the editors of such resources are happy to receive feedback and coordinate with external systems rather than forcing the duplication of work. Were there attempts at engagement via issue trackers or similar? The lack of engagement would be odd given how much the authors emphasise FAIRness. Perhaps a clarification of how, exactly, these resources were reused and what their future role in the ARIES system will be (assuming they are willing to cooperate with its development) would help. Additionally, there are some descriptions and claims (e.g. that k.IM makes things "immediately actionable", more below) which need some form of support or demonstration to be substantiated. This need not be exhaustive: a few illustrative examples as a supplement should suffice. If no such demonstration is provided, I recommend omitting these claims. Overall, much of this contribution is more a specification rather than a report on results and outcomes. I suggest large parts of this (most tables and statements) be moved into a supplement and a few used to illustrate the key points made. Despite these critiques, I am very supportive of this work. This is one of the most streamlined semantics-to-modelling systems I’ve encountered and I’m hopeful that it can iron out the issues outlined above. Further, I echo the authors’ call for a more consolidated semantic landscape to allow such technologies the stability they need to grow efficiently. Parallel efforts should begin converging and complementing one another (e.g. OBO and other such resources providing worldviews for ARIES while ARIES reshapes their content when inconsistencies are found). I’d be very happy to support this moving forward Detailed comments: Below, please find a non-exhaustive set of specific comments to clarify and expand on my positions above. I don’t require a response to each one, but the major points they collectively describe should be addressed in a revision. Worldview: To me, a worldview sounds more or less like a “domain ontology”. Is this the case? If so, how are overlaps between domains handled? I feel uncomfortable with some of the syntax used in the various code snippets: e.g. how is a model a measure? This is not really intuitive and requires commitment to an unusual abstraction. This reflects some of the semantic ambiguity at the foundation of the ARIES/kIM system discussed above. While I certainly can appreciate the effort involved, many years of design and user feedback do not necessarily translate, 1:1, into quality: I would tone down suggestions to this effect. This is especially true as the user/testing group is rather small (which doesn't mean the feedback was poor, but reduces the weight nonetheless). Simply stating the begin date of the work and the number of active testers at the beginning and at the end would be enough. Regarding the statements regarding OWL/OWL2: It seems that authors are proposing that those working with OWL or using other upper level ontologies should either 1) switch to the ARIES system or 2) maintain translators between OWL2 and ARIES-compliant languages. This is quite a bold position for a relatively new and untested system. I would suggest that this should be the other way around for the early phases: ARIES should develop translation layers to the establish technologies. Perhaps I misunderstand, but I find it quite odd that the authors criticise other ontologies for requiring commitment to a given system of operation while simultaneously recommending that users subscribe to the IM system in the first few lines of discussion. As with any technology, users must understand the tools they work with and, indeed, commit to some sort of convention. What is promised in the forthcoming contributions is quite ambitious and exciting, but has little relevance to this submission and, naturally, no evidence to support the claims made. I thus suggest this be toned down considerably. An "operational semantic web" that is able to pull data together into a coherent data set for immediate analysis would indeed be very useful. There are application-specific examples of this in the biomedical domain (e.g. the Monarch Initiative). However, simply being linked in such a network does not make data and models FAIR-compliant. How do you know they are really interoperable and reusable unless the contributors of those datasets have made them so? k.LAB cannot magically make this happen, or can it? I think a careful look at what FAIR specifies is needed before claims of compliance are made. "Faced with a state of the art in which semantic interoperability is still often understood as “matching of terms”" By whom? It is true that many in the community tragically think of semantics as terminological mapping, but there are quite a few others that really do target the meaning behind terms. I think that statements like this don't do the community justice and give the false impression that the SMM/k.LAB solution is the only one addressing true semantics. "In our view, a major need for progress towards this goal is a solid, uncontroversial phenomenological base, i.e. the basic semantics for the types of phenomena and entities that can be understood by human observers." I’m afraid that controversy is not likely to be dispelled: there will always be disagreement as human understanding is continually changing and growing, with researchers and other observers frequently disagreeing. Statements like this presuppose some form of authority which is antithetical to open progress. I would suggest that systems should be able to handle controversy while offering systemic stability. "Formalisms and toolsets must be built to support it, to ease the specification of domains and allow for extension, while enforcing a consistent design discipline. We need clear best practices for specialization and connection of terms, and guidelines on how to integrate always growing, and potentially infinite, domain content from vocabularies without breaking the logical integrity of the resulting annotations." This sounds more or less identical to the objective and actual modus operandi of the OBO Foundry and Library. I would suggest that OBO Principles be discussed here. I take strong exception to how concepts are nearly equated to the objects of empirical observation – in my experience, this quickly becomes toxic to handling the informatics of natural science. This also doesn’t seem to resonate with how the authors treat particulars like real-world entities across the manuscript. This and superficial treatments of ideas like “Platonic realism” concern me. If particulars are concepts, how can they maintain their identity through time unless they are being conceptualized? What is exactly mean when the authors discuss the observation of a concept? I think such claims and arguments should be the put forth in a separate paper reviewed by logicians and developers of upper level ontologies. The value of this paper is more the implementation of the semantics-to-model link. "If a physical object, event, process or relationship can be simply acknowledged to exist in a context of interest, qualities, such as the elevation of a mountain or the temperature of a body, can only be observed indirectly, i.e. by comparison with reference observations." I don't think this works - the same can be said for the physical objects themselves. Every distinction is based on comparison to one or more points of reference. The idea of indirect observation of a quality but direct acknowledgement of a physical object, event, process, or relationship is also occult to me. Further, I see no meaningful distinction between a process and event save an arbitrary fiat boundary. I recognise the importance of handling entities across scales, but the arguments offered don’t really convince me. Formalising scale semantics independently of the "observables" seems off: the observables don't disappear or cease to influence a system simply because the scale changes. Indeed, the scales being handled are anchored to the observables identified in a given application. This is true even if they are not “directly” observable. I'm not sure how the authors’ handling of scale is useful as it sounds like something that would actually amplify bias. Some sort of demonstration of why this approach is more useful is needed. "The ability to accurately characterize semantics along observable, observation and context dimensions addresses the interoperable and reusable FAIR criteria. Semantic specifications can be rewritten into queries that select interoperable counterparts for an observation, addressing the findable and accessible requirements." Looking at the description of the FAIR criteria, I wouldn't say that they've been addressed so easily. Semantic technologies are of course needed to make data FAIR (especially the “I” criterion), but FAIRness is something that can only be evaluated in action, not in theory. I would weaken these claims until a demonstration is provided. "While observation and context semantics are relatively well-understood, the characterization of what things are - observable semantics - remains difficult and uncertain, even with increasing investments in ontologies and vocabularies and an engaged community behind the current state of the art." Such a statement would need more support - how are these levels of understanding assessed by the authors? Some more clear arguments would go a long way here. "Terms describing commonly acknowledged classes of physical entities (such as persons or objects) are complemented through inference, comparison, association and imagination...Such observable entities..." These entities cannot be observable if they are imagined. The argumentation gets somewhat murky here. Or is imagination meant in terms of the creativity of the knowledge engineer? This is slightly less worrying. "It follows that interoperability can exist in a conceptualization, as long as the boundaries of stability of meaning for all concepts with respect to their fundamental phenomenology are stable. Scale, commonly defined as the choice of resolutions and extents through which we make observations of the world, binds the observables of informational artifacts to precise phenomenological categories, establishing boundaries of validity for conceptualizations." Ideas like “fundamental phenomenology” are dubious: many ontology developers have been in search of this Holy Grail, but we can only deal with what empirical studies have predictably and repeatedly confirmed. Knowledge and meaning are maintained around these anchors. Further, scale is typically imposed by observers unless one considers the scale associated with an entity itself. I don’t see how this connects to the validity of a conceptualisation. Is it not valid to assert that roads are present in the European continent even though they can’t be observed due to coarse scale? Indeed, the idea that continuants themselves can be morphed is problematic: shifting perspective doesn’t make a continuant cease to be. This perceiver-based semantic basis seems counterproductive and no results are put forth to support arguments that it is superior to alternate modes of handling scale. I really don’t see how subjects can morph into qualities. Alluding to the cyanobacteria in water, the cyanobacteria haven't disappeared or changed, one is simply talking about the green quality of the chlorophyll in these cells integrated over the expanse of a different entity (a lake, pond, etc). A rigorous semantic solution would track this over multiple scales, rather than ‘fudge’ it and directly claim the water body is green. There needs to be simplication of how "structural" and "functional" relationships are discussed. This is nothing more than differentiating instantaneous (in the SNAP logic) relationships from those that require a temporal window to be realised (e.g. realisations of functions and dispositions in the BFO world). Indeed, why do we need yet another way of doing something that is done by other upper level ontologies? If the authors must re-invent this, they should, at the very least, make the relationships to existing upper level resources clear and argue why their approach is superior (again, I think that should be a different paper). "At the same time, it became clear that no community of modelers, data scientists or other prospective users would consider an investment in OWL or other semantic web-endorsed formalism as the vehicle to express the semantics in data and models" Really? Why not? Why do they even need to know that OWL is being generated in the background? Why abandon a functional system because some unspecified community of users won't interact with it? Should we abandon HTML5 because average web users don't understand how to write it? Obviously not, it's just hidden and tools that generate it created (e.g. CMS tool). Claiming that some unspecified experience the authors have demonstrate the necessity of a completely new semantic standard is not scientific reporting or even convincing. I’m afraid that Figure 1 makes little sense to me. Not all occurrents are finer in scale that all continuants and, as the authors themselves argue in their examples, not all qualities are finer in spatial scale than all subjects. Full logical elucidations and robustness under counter examples must be provided (again, another paper). Regarding the criteria of compatibility, expressiveness, readability, and parsimony: Compatibility: What sort of maintenance do these compilers require? Who will maintain them? How is accuracy evaluated? Are there use cases? I feel as though this needs more clear specification and I would dedicate more room to this than the attempts at upper level semantics in this manuscript. The authors may be considering publishing these in a follow up paper, but they really should be here as these are the elements that are of the highest value. Expressiveness: Intuition is quite a subjective thing and actually the source of many semantic errors in the first place. I would consider it common knowledge that usage of similar terminology even by interacting expert groups often does not correspond to ‘uncontroversial’ usage unless they’re using a standardised guide. So I can’t support the idea that ‘keywords’ are sufficient to satisfy expressiveness, especially from a machine perspective. Further, most expressive (i.e. well-axiomatised) ontologies I'm aware of don't require users to know anything about the underlying axiomatisation in order to use their content, so this isn’t really novel. Sections like this switch the nature of the manuscript into something more like a review, rather than focus on the outcomes of the project at hand, which is confusing. Further, are synonyms of classes handled/matched or do keywords (i.e. primary labels) have to be memorised by the user? Readability: Naturally, very important, however do the examples provided really improve on expressions that are entered into ontology editors such as TopBraid or Prot? Further, the readability outlined here requires some effort by the user to subscribe to and understand the ARIES upper level, which must be learned to be useful, as with any other system. Parsimony: I assume the authors mean that the system should allow post-composition. This is already done with "dead simple design patterns" or TermGenies. These should be acknowledged so as no to give the impression that this well-recognised issue hasn’t been at least partially addressed thusfar. "In k.IM, particulars and universals are combined to specify observables;" This makes very little sense to me - perhaps this just needs rephrasing, but I don't see how or why there would be a need to combine universals and particulars. One always observes the particular, which is linked through instantiation with the universal. "k.IM statements readable and understandable by mimicking English syntax, while specifying much more complex, correct and consistent OWL2 axioms" In principle, I see this as a useful step forward - but as the more friendly syntax is less specific and logically fuzzy (Table 1), what happens when new content is added and the syntax expands? Will this not just result in another layer of ambiguity through the creation of a strange k.IM dialect that silos users into its translation system? This connects to the issues with “expressiveness” outlined above. I would like to see more elucidation of the definitions in Table 1 and also some developed justification as to why existing upper level treatments are not sufficient for the ARIES objectives. There may not be 1:1 matching in all cases, but I don’t see great difficulty in pre-composing the classes noted in this table. Once again, reuse should be preferred to duplication unless there’s a very clear reason not to go this route. Subject: Is this not just the subject of observation? This then presupposes that an observation action has occurred, which suggests that this is not a high-level class, but something lower-level and operational. configuration: This seems superfluous: isn’t this true of any precomposed and well-axiomatised class? Thing: how is this different from entity? Why should this be inanimate? As this is a common term (due to Prot and other resources), would it not be more sensible to call this “inanimate object”? Again, if this has to be the output of an observation process, is this not too low-level to warrant the use of the word “thing”? agent: This suggests all agents must be self-aware, correct? Priority: This is certainly not the normal, intuitive, or common-sensical usage of this word, which runs counter to the authors’ claims and reasoning above. Further, ranking is a numerical operation which can be arbitrarily applied across qualities, making this definition suspect. The same is true for quantity. There is a layer missing in these semantics. Class: Again, a very dubious use of a term that is more or less reserved in the semantics community and not naturally or intuitively associated with qualities. Process: I’m not sure why a process would only include a single subject. This doesn’t seem very realistic. Do qualities need to change during participation in a process? I could go on with each row of this table. The bottom line is that this manuscript does not provide sufficient arguments to show that this treatment is stable: this should be done in a separate publication reviewed by developers of upper level semantic resources. Concerning the geographical elevation: it's very peculiar that the work done in other ontologies handling geology/geography and qualities is not used or their developers engaged to coordinate with this approach: again, it seems that this system is creating a silo, rather than linked, interoperable, and portable products. Given that this work is focused on FAIR thinking, I see this as a significant issue. Statement (1) is quite readable, but I don't really feel that this is so much easier than composing statements in Protege, using a reasonable set of ontologies. What is much easier is that all of the components can be called by just identifying the namespace. I can imagine this as a very approachable working environment, calling in external semantics that are more rigorously developed. I would encourage the authors to make this more prominent relative to the claim that k.IM/k.Lab offers an intuitive experience: it seems that it's just as idiosyncratic as other solutions. A graph/network figure illustrating the semantic model that corresponds to statement (1) - including a few classes not identified in the statement, but linked to those that are identified - would help show the interconnectivity and architecture of the system. This would make statements like "This is done by tying the concept being defined to the core observation ontology" much more meaningful. "The language contains keywords for many fundamental quantities, allowing users easy specification in most situations (Table 1)." Claims like this need some sort of support or explanation of why the authors think the coverage is so high. Which domain scientists have found this to be sufficient in most situations? If that data is not available, a description of what the authors have covered (as a supplement perhaps, as Table 1 cannot go into such depth) would be needed. "along with constraints and relationships for all common scientific observables" Similar issue to the above - I find it hard to believe or parse this statement. What is common here? How can the complete coverage be so confidently claimed? Operators in general: Is this not similar to/the same as what the Relations Ontology and other stores of Object Properties does? Are the properties defined for reasoning (more formally than the list in Table 2)? Is their reflexivity and inversion properties coded for reasoners and query systems to make use of? The language here suggests more novelty than there is and, once again, this doesn't seem reuse well-adopted object properties that already exist, making claims of FAIRness weaker. Table 2: This reveals some confusing patterns - how does an operator produce a quality? I can understand presence being represented as a BFO:'dependent continuant' or PATO:quality, but I don't see how a relation (operator) can "produce" one. - What is the distinction, metaphysically, of a quality and a quantity? - Are "Countables" defined in a class? or are range restrictions placed on the operator? - What is meant by "in a (spatial) context"? Is not everything that concerns this system in a spatial context? - It seems strange to have occurrence as a shorthand for ‘probability of’. There are more points of imprecision/ambiguity that I could list here, but perhaps I'm not appreciating how these are treated internally. This should be more clear in the paper: how are these checked for logical consistency? Statement (2): This seems like lax modelling: the CSV file is not the model, it's a CSV file which, perhaps, pertains to a model. I appreciate that the condensed syntax may expand internally to express things more precisely, but - if this is not the case - claims that this is semantically rigorous need to be softened. "As concrete qualities (those of which observations can be made) can only exist inherently to a direct observable, the observable must be made explicit before the concepts can be used (e.g., earth:Region in the previous elevation example)" This seems like semantics for no clear purpose. What is a non-concrete quality? Who defines the limits of observation? Naturally, a thing must exist (be instantiated) to be observed. Perhaps this is just poorly phrased? "...so that users can easily locate concepts by textual searching." This assumes that users all use the same syntax and/or terminology – which is hardly supportable without training. A useful feature, nonetheless. However, does this mean that users can specify any "concept description" they wish? How is this semantically controlled? If these definitions are done haphazardly, this could defeat the purpose of a semantic resource. "is first established as its fundamental nature" Claims like this seem un-falsifiable, and thus suspect. Does this mean the authors take these as primitives? Is there a list of all these primitives with their logical elucidations present? If not, it's hard to support this as a semantic resource. "it will be correct to annotate elevation within a watershed, as long as a previous statement defines the Watershed concept as a type of earth:Region." I’m not sure I follow. Does elevation inhere in the watershed or does it inhere in some entity that is, itself, somewhere within the watershed? This is not linguistically clear, and thus cannot be logically clear. "The of keyword is used when the quality refers to a second, implicit observable in the context of inherency. For example, the “height of trees” quality in a region is inherent to that region, but implicitly describes tree subjects in it." This is hard to support - this quality does not actually inhere in a region. Like the cholophyll in the lake example, this can be understood as shorthand. While I appreciate the convenience, unless this is expanded in the background, this feels like a very risky route in semantic modelling with too many shortcuts. "In keeping with our readability requirement, we only allow two levels of specification and use two different keywords (within and optionally of)." This makes the language in k.IM very idiosyncratic. It's debatable whether this is useful for readability that results in reasonable semantics. This feels like setting one’s own goalposts without objective criteria. "We found that legitimate chained specifications, such as “x within y within z within …”, were awkward and difficult to understand in usage tests and decided against allowing such statements." True, these are awkward, but why prevent them if they're logically valid? What's the alternative? "Multiple chains of inherency of this kind can be defined using intermediate concepts" This must be explained further. "In knowledge domains (as opposed to physical ones), the implicit inherent subject is often a configuration." This raises a few red flags for me - it's very easy to create semantic resources that have little bearing on physical reality and are thus of questionable use in the natural sciences. I don't see why topology or terrain (3) is treated as not "being directly amenable to providing the observable of an informational artifact". It is the bearer of a quality, which is claimed to be linkable to an information artifact. Following this logic, wouldn't events also be indescribable? It is useful to be able to pre-compose semantics for a group of phenomena (~ a configuration) that are often referenced together, but these are still physical. "One can assume that identifying height of corn would require a further specialization of plant height, and the corn identity would simply be implied syntactically by using a Corn… prefix in the term assigned to the concept." Are we talking about TO here? There is no PTO in OBO. Regarding TO and other OBO ontologies, this wouldn't/shouldn't be limited to a syntactic specialisation, but include an axiom linking the class to a taxonomy. Many classes in TO follow this recommendation. Also, "giant embryo (a gene type)" is not correct. It's not a gene type, it's a quality. Also, these are not "concepts" as OBO resources take a realist stance on knowledge representation. "plant height uniformity (a quality not physically commensurable with height)" One would have to be more clear about this. I see the issue here as one of inherence (this quality inheres in a collection of whole plants, thus conflicting with the assertion in the superclass). Did the authors reach out to the TO developers to correct this? "relative plant height (seemingly adding an observation-related attribute, relativity, out of many possible)." Again, this has to be more clear. I agree that this does not sit well in the TO as this is more an information artifact than a new type of quality and requires cross-axiomatisation with an ontology dealing with measurement processes. In OBO, this is not an uncommon occurrence and numerous "stubs" exist. These ontologies have issue trackers where ambiguities like these can be raised for later revision. "Yet it is clear from this example that no ontology can force users to adopt cogent annotation practices, ensuring that physical and biological identities are preserved along inheritance chains and attributes retain traceable and stable meaning." I'm not sure that the examples provided illustrate this point. They do show that even well-developed ontologies aren't perfect and need some classes to be refactored. However, annotation practices are an order removed from the development of a semantic backbone. It’s of course possible to annotate in many ways, but whether these are going to work with the reference ontology and suit a user’s objectives is a different question. Traceable and stable meaning is handled by the URI scheme and obsolesence best practices. Further, I don't quite see how the k.IM example that follows (11) fixes these issues. The "language itself" is often rife with ambiguity and jargon and not a stable shortcut to develop rigorous semantic models (but can be good for a first pass). In the example, a "measure" is roughly the same as a BFO:quality, so they both require a material entity to inhere in. A quality and a unit do not embody the "how", one would need a process ontology like the "protocol" branch of OBI to do that. A particular quality/measure can only enforce a set of units of such constraints are hard-coded in the ontologies used for k.IM - is this the case? Would it not be sensible and more sustainable to create a working relationship with TO and other ontologies to fix the errors spotted and provide ARIES with better worldviews? "Definitions (11) and (12) intentionally use agriculture:PlantIndividual instead of biology:Individual, as the latter requires a precise species identity (definition 7), while the former references the commonsense taxonomy used in AGROVOC for crop types, reflecting the intended semantics for the data." How do the PlantIndividual and biology:Individual interoperate? This sounds like a point where semantic drift can occur quickly. In the OBO world, one would create union classes of biological species for less specific common terminologies. "Most importantly, the adoption of rigorous phenomenological inheritance and specification syntax requires the realization (and the explicit statement) that height is first of all a quality of a plant subject, and that the data refer to plants within a cropfield subject." I'm not sure where this requirement is coming from - the user? The task? The worldview? Further, expressing this (along with logical constraints on qualities etc) is quite possible using an application ontology dervied from PATO, PO, PCO and ENVO rather than developing an ad hoc semantic. "While such details still need to be learned by a user, the syntax itself serves as a guide for the annotation workflow: the use of inconsistent observables or the lack of proper inherency would yield ungrammatical statements that are reported as errors." I don't really see how this is that better than the reasoning checks used by other ontologies. Further, I'm not sure if grammatical evaluation will always be a good indicator - that largely depends on the labels used, whether the syntax is sensible (I’ve already noted issues with some usage above) , and if the grammar is appropriate for all cases in the scope of this work (not evaluated). This is actually quite a major issue – whatever mechanisms are used to check logic should be absolutely true to that logic, not a grammar. Are there no reasoners used to check axiomatic consistency? "non-abstract quality" The distinction between abstract and non-abstract qualities etc remains very confusing to me. A box or a supplement is needed to help explain the need with clear examples. "The result is readable by a non-expert and compiles to axioms specifying a single OWL concept, which can be transferred to a remote endpoint in axiomatic form and reconstructed for reasoning or database querying; the shared worldview is the only requirement for its interpretation." There are many "ifs" along this path: it may be readable by a non-expert, but so are asserted axioms in Protege - as the authors note above, the various conventions of both the k.IM system and the worldview chosen need to be familiar to the user for this to be not just read but understood. Yes, this can be transferred to an endpoint in an axiomatic form, but that assumes understanding of and stable interaction with the resources on the other end of that endpoint. I don't see evidence that this system does that as a rule: the authors should include examples of this or make this statement more clearly aspirational. The sentences following this one are claims of a similar kind that would need to be substantiated somehow. "Specifications such as (13) are simple enough to be added to metadata data files with the same name - which may be automatically detected and indexed by specifically designed web crawlers, so that indexes of web-accessible, annotated datasets can be built and maintained." This is very interesting - similar approaches (tiny semantic annotation files) have shown promise (see the PhenoPackets project). I think this MS would be greatly improved by focusing on such practical approaches rather than semantics which are present in other resources. Spending more time talking about how the environment can link elements of information artifacts to ontologies (e.g. as in (14)) would target an urgent gap in research and implementation. Unfortunately, most of this is not shown. "Specification (13) is complete and correct, as geography:Elevation is fully characterized in terms of inherency within the worldview, as seen in statement (1)." What is meant by "fully" here? I don’t really see this as complete or correct. The definition of (1) is circular and calls for a model for description, which seems extraneous. Also, "terrain" is defined as pertaining to land surfaces only (3) implying that surfaces permanently covered by water don't have elevation (which I’m assuming means elevation above sea level), which is incorrect (e.g. many lakes such as Lake Tahoe and Khl Nuur). "SMM, a modeling approach where FAIR+ interoperability is an integral requirement, sees data and models as definitions for possible observations: while datasets can produce, possibly through mediation of observation or context semantics, the requested observations in a self-contained way, models do so through computation that may involve the observation of other concepts they depend on, to be resolved through other data or models." Again, defining “FAIR+ interoperability” (which seems internally redundant) as accomplished without any real demonstration of it in action can’t be supported. Further, data and models are not definitions and treating them as such will likely lead to more ambiguity. The rest of this paragraph seems only half-developed and I’m not sure what the authors are trying to convey. Reference 39 isn't really about k.LAB, while the prose suggests it is. "The most important aspect in this sense is the clear focus on observable semantics: no time is wasted seeking semantics to express model-related concepts (“model”, “variable”), observation-related ones (“measurement”) or context-related ones (“spatial resolution”), all of which figure prominently in commonly used ontologies." I take strong exception to this - time is by no means wasted by handling the semantics of informational and procedural entities. The fact that these feature prominently in other ontologies should be an indicator of this. In fact, one could argue that procedural metadata is needed to handling anything FAIRly, otherwise there can be no true reproducibility. The fact that the current approach neglects these is not really a virtue to be celebrated, although I do recognise that an observational focus is a good point of initialisation. "Formalizing a simple phenomenology for observables and universals. The base observables in Figure 1 have proven intuitive enough to be understood and remembered by diverse users, helping them “home in” quickly on observable semantics as described in the previous point. Also, the use of independently defined and flexibly attributed universals to express attributes, identities and roles has effectively and intuitively solved, in our applications, the plaguing issue of excessive and improper specialization." This is purely speculative without some sort of comparative study. As stated above, I see major issues with the phenomenology or base semantics used here and no evidence that they are more accurate or useful than other systems (assuming users have equal training in both). "home in" -- "hone in" "The k.IM language and k.LAB platform make ontologies and annotations immediately actionable, enforcing the logical consistency of each definition both by enforcing syntactical correctness through intelligent editing tools and by employing a machine reasoner to identify and report logical errors to the user. The language guides, simplifies and validates the definition of knowledge; the support software provides feedback and allows users to immediately perform user queries and compute workflows whose results enable at-a-glance validation of the semantic correctness of the concepts employed." This is the most exciting part of this contribution, but its capacities are not shown or discussed at a meaningful depth. Which reasoners are used? Are they valid for any external resources brought in? What is meant by immediately actionable? How, exactly, does enforcement occur? Without more context, I can't see if the system actually simplifies and/or validates semantic work, as claimed. 