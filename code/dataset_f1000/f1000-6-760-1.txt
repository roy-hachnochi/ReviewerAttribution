This study tests version R9.0 of the MinION nanopore sequencer, and describes its error rate, read lengths, throughput, and other characteristics. The article's timing is unfortunate, because I believe R9.0 had already been superceded at the time of publication, but this does not affect the study's soundness. As far as I can tell, this study is basically sound, but there are some careless mistakes: This text does not match Table 2: maybe pass - total? "64–111 K 2D pass reads (compared with 21 K for the typical R7.3 experiment)". Is this really an "increase"? "increase in N50 read length to 7.3–9.1 Kb for all 2D reads in the R9.0 experiments (compared with 7.4 Kb". The abstract says: "The 2-dimensional (“2D”) N50 read length was unchanged". This text does not match Table 3, maybe pass/fail - total/pass? "mean template length for “pass” reads increasing to 7.2–8.6 Kb (from 5.0 for R7.3) and increasing to 7.8–9.4 for “fail” reads (from 6.2": This text also does not match Table 3: "Read length N50 increased to 13.1–15.7 Kb for “pass” reads (compared with 6.9 Kb for the typical R7.3) and 13.6–16.2 Kb" This is true only for 1D reads: "For template reads from both 1D and 2D experiments, 99.9% of “pass” reads were alignable from both 1D and 2D". 88% is not similar to 78%: "The median identity of reads from 1D and 2D experiments (Table 4) was similar... The median identity for 1D template reads was ~88% and ~76%, for “pass” and “fail”, respectively (compared with 78% and 75%". This is not comparing like with like (R9.0 template versus R7.3 2D): "For the 2D experiments, the read identity was ~89% and ~85%, for “pass” and “fail”, respectively (compared with ~92% and ~82%, respectively, for the typical R7.3 experiment)." A few things should be clarified: Why do "Identity %" and "Total error %" not sum to 100? Fig 2: - what is the difference between Q-score and BQ? - what is "(Temp)"? - what is the difference between "GC" and "GC (Temp)"? - what is "throughput": count of what per what? What is "1D consensus follow through"? What does "bridging experiment" mean? What is a "run script"? Other minor comments: The title should be shortened to something like "Analysis of MinION R9.0 chemistry". The rest is not scientifically meaningful, and might be perceived as "appeal to authority". The abstract "methods" section is incorrectly brief. The abstract "conclusions" section should probably not say "new" R9.0 chemistry. Is this really "higher"? "higher at... 6.0–6.5 Kb (compared with 6.0 Kb". Page 4: "were statistics computed" - "statistics were computed". The LAST usage is likely suboptimal (though I guess it matters little here). The currently-recommended usage has been here since 2016-11-22: https://github.com/mcfrith/last-rna/blob/master/last-long-reads.md Data availability: I found it excessively hard to obtain the data. The PRJEB18053 link leads to three "component projects", each of which has numerous files. Which file is which dataset? This should be better organized, or at least described. The best I could do was to mouse-over the links: a name like "Nott_R9_run2_1D_pass_f74a133aa1ac903384a928a51051582db2cc412b_0.fastq" gives me a clue, but a name like "ERR2025969.fastq" is hopeless. For example, what is the difference between these files? Nott_R9_run2_1D.pass.1D.fastq Nott_R9_run2_1D_pass_f74a133aa1ac903384a928a51051582db2cc412b_0.fastq Suggestions for future studies of this type: Characterize the substitution errors further, e.g. is A-G more frequent? Are the insertions and deletions long-and-rare, or short-and-numerous? Are the base quality scores accurate/useful? (And what do they even mean when indels are the main error?) Characterize context-dependence of errors, e.g. homopolymers, CCXGG context (http://www.biorxiv.org/content/early/2017/06/29/157040). Can rearrangement errors be characterized? Long reads are promising for finding rearrangements (e.g. inversions, translocations), but do artifactual rearrangements occur? 