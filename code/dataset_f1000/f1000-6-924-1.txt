The outsides of organisms are most often exquisitely, even ruthlessly, adaptive. Inside the organism’s body, the situation is more heterogeneous. Both physiology and the function of macromolecular complexes are in many instances technically stunning. On the other hand, the wiring diagram of the cell is bedlam. Even based on our current — and likely quite incomplete — state of knowledge, regulatory networks appear to be both more densely and more broadly interconnected than would seem necessary. This is surely a puzzle of modern biology, and Frank has done us a service by cataloging the live hypotheses and pointing us towards the possibility of a resolution wherein this “over-wiring” simply reflects general principles of inductive inference. My main criticism of this article is that it does not engage sufficiently strongly with either the contemporary or historical literature. On the contemporary side, it would seem appropriate to directly address the ongoing efforts by several groups to formally link population genetics to general principles of inference (of course, Frank has contributed substantially in this area himself by clarifying the relationship between natural selection and information geometry, see e.g. Frank 2012 “Natural selection V. How to read the fundamental equations of evolutionary change in terms of information theory”). These efforts have been recently reviewed by Watson and Szathmary 2016 in a TREE piece “How can evolution learn?”, which hits on many of the same themes as the latter half current manuscript. Watson’s work in this area seems particularly relevant, and indeed he calls his theory “Evolutionary connectionism” (Watson et al. 2015). An important insight from this series of papers is a possible relationship between the evolutionary problem of evolvability and the statistical problem of overfitting. In particular, they suggest that pressure for developmental simplicity can improve the ability of evolutionary systems to generalize in a manner similar to how regularization, drop outs, or early stopping can prevent over-fitting in machine learning (e.g. Kouvaris et al. 2017 “How evolution learns to generalize, using the principles of learning theory to understand the evolution of developmental organisation”). The idea that the topology of regulatory networks is a generic consequence of evolution by gene duplication (as in, e.g. the work of Ricard Sol), and more generally by the expansion of gene families, also seems like it deserves a mention as at least a possible proximal cause of over-wiring. On the historical side, I think more could be done to link the current discussion with historical themes in evolutionary thought. For instance, the discussion about many possible genomic changes with small effects smoothing the gradient and allowing evolutionary optimization could be put in the context of Fisher and Wright’s disagreements over the structure of fitness landscapes. Wright thought that the reality of building a functional physiology would produce fitness landscapes with many local maxima, so that the key question in evolution was to identify the population-genetic regimes where progress on such a landscape is possible (Wright 1931, 1932). Fisher thought that in high dimensions, these local maxima would largely turn into saddle points, and that in any case, environments were generally changing fast enough that populations were usually chasing a moving optimum rather than adapting on a fixed fitness landscape (Fisher 1930). Frank’s discussion of “getting stuck” in the current manuscript provides additional nuance to this classical disagreement by emphasizing the possibility of extended, high-dimensional plateaus that while strictly speaking are saddle points function in an evolutionary sense more like local optima. The reader interested in resolving this puzzle should also be directed to some of the stone-cold classics in this area such as Wagner and Altenberg 1996 and Stoltzfus 1999 (“On the possibility of constructive neutral evolution”). 