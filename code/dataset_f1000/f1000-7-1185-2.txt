I reviewed with interest the paper entitled “The Congress Impact Factor: a proposal….”and, as far as my knowledge goes, this is the first time that a metric evaluation of a medical congress is proposed. The authors propose to measure an impact factor based on the mean H-index of invited lecturers normalized for lecture topic (i.e. the H-index of an author limited to the topic of the invited lecture) related to the number of invited lectures. Obviously, this metric has several limitations that come both from the intrinsic original defects of the H-index and from the complexity of evaluating the quality of a conference and of the speakers. In fact, the H-index reflects only the number of papers that have received a certain number of citations and does not include any information about the real contribution of that author to the manuscript nor the number of self citations. Furthermore, it tends to increase with time with increasing number of citations even though that author is no more productive. Because of these limitations several attempts have been made to improve the H-index trying to take into account the contribution of that author to the paper or the period of activity of the researcher adjusting for the number of years since the first publication. Nonetheless, there still is no perfect index to measure the quality and quantity of research which may be affected by so many factors 1 - 4 . In fact, some researchers that have deeply impacted the world of science do not have impressive H-index 2 , 3 . Dealing with the world of medicine there is another point to consider that is practical expertise. The professionalism of a physician is not represented by the H-index. We all know that being scientifically very productive does not always correspond to being an “hands on” expert and to measure the practical expertise is an even more challenging task. The implementation of such an index could significantly impact the choice of speakers and may leave out non productive “hands on” experts. Finally, the metric may be affected by the number of speakers; i.e. a small conference may see its H-index rise if just a few authors with high H-index are invited. In such a case, the median with the range may better reflect the overall composition of invited speakers. Despite all these observations, I believe this paper deserves publication in order to start a serious discussion about scientific conferences. However, I believe the road to develop an acceptable measure of the quality of a conference is still long and rough. Coming specifically to the paper I have the following comments: Page 4, last paragraph before the discussion section: it should not be “between these two congresses…” but “…topics…” The discussion section should be partially rewritten taking into account the comments I made above and the fact that the H-index is not so robust. The authors should acknowledge the limitations of the metric and the possible drawbacks. In the last paragraph of the discussion the authors state that the conference impact factor can become a valid instrument of education to develop a competitive academic curriculum vitae. I disagree with this concept since participating to a conference does not necessarily correspond to an improvement of the professional knowledge. In this view the CME program is more close to this concept than the IF of a conference that does not measure learning. I would erase this sentence limiting the conclusion to the fact that the IFc may represent the first step in developing a simple tool to evaluate scientific conferences. 