 Addressing Major Criticisms This is a controversial topic and careful consideration of objections is needed in a meta-analysis. Presentiment or Predictive Physiological Anticipation Studies (PAA) are typically criticized on these grounds (see, for example, Wagenmakers, Wetzels, Borsboom, Kievit, van der Maas, 2015): Physical impossibility File-drawer effect Biases due to multiple comparisons or p-hacking As for the first criticism, discussion of physical plausibility is beyond the scope of this meta-analysis and is left to the discretion of the authors. Nevertheless, apparent violations of our intuitions of time are found at the quantum level, such as the Wheeler Delayed-Choice experiment. It is not impossible that such effects may scale up to a macroscopic level in a not-yet-understood emergent process. I think the final two sentences of the introduction satisfy considerations of the physical impossibility objection and no changes are needed. Though file-drawer effects are frequently cited as a serious concern, the results section adequately discusses this issue. However, expert review is needed for this area (my response to "Is the statistical analysis and its interpretation appropriate? " should really be a combination of "Partly" and "A qualified statistician is needed".) I agree with the first sentence of the "Publication Bias" subsection that publication bias is not that serious of a concern because of the limited number of researchers and available funding. By far the most serious concern is the third, that of multiple comparisons or p-hacking, which I do not believe is adequately addressed by either the discussion or conclusion sections. Two sentences in the conclusion are not sufficient to address this serious concern. I have included recommendations later in this review. I am aware the authors already know the following but by doing multiple analyses and only reporting a sub-sample of them a believer or supporter of a hypothesis could bias effect sizes up while a skeptic or opponent could bias effect sizes down (and none of these biases are necessarily intentional or even conscious). In the context of PAA, serious sources of p-hacking concern are establishing baselines for electrophysiological data, deciding time regions for analysis, and methodologies for rejecting bad data and artifacts. For some physiological measurements, the problem is even worse. In Electroencephalography (EEG) studies, for example, a researcher could either study event-related potentials (ERPs), the spectral power densities of various oscillations, or the phases of such oscillations, or a host of other possible analyses. Considering oscillations, the frequency range of an analysis can also be freely selected. Additionally, a researcher could select different bandpass filters to use or even which section of the head is included in the analysis. This is in addition to the concerns with artifact rejection, time region, and baselining already discussed. With so many free parameters, a non-preplanned study is practically useless as hard evidence for an effect unless the statistical significance of the effect is high enough that it becomes implausible that the effect in question can be generated by tweaking free parameters. Even if the statistical significance is high, the effect size is still untrustworthy because an analyst could be tweaking parameters in an effort to improve the analysis or fix problems but is only homing in on statistical fluctuations. These concerns are one reason why I refused to include exploratory EEG research from my own lab in this meta-analysis. The solution to the multiple analysis problem is to separate research into exploratory studies where adjustments can be made in analysis and pre-planned confirmatory studies. Some of the studies included in the meta-analysis are pre-planned confirmatory studies, which should be considered the only truly reliable results for estimates of effect size due to the concerns laid out in this review (even for confirmatory studies, mistakes by researchers could distort effect sizes but these mistakes may average out in the long run). My recommended solutions for this paper are: More discussion of the risks of p-hacking in biasing results in the discussion section Separated analyses of pre-registered confirmatory studies and exploratory studies and discussion comparing the two For exploratory studies in the study tables, include the experimenter expectation of whether the hypothesis will be verified (such as in Galak, LeBoeuf, Nelson, Simmons, 2012) Show whether multiple comparison corrections were made for exploratory studies Exploratory studies are necessary for advancing the field. But a meta-analysis should not include them without major caveats due to potential distortions of the effect size. I am aware the extra attention given to p-hacking risks in this research is not precedented by other fields but the small effect sizes and the major implications to our understanding of physics, psychology, and neuroscience PAA research engenders may justify additional caution be used. My colleagues and I discuss this further in Schooler, Baumgart, Franklin, 2018. Other Comments “The presentiment hypothesis calls for a difference between arousing and neural pre-stimulus response and this is calculated across sessions” is not always true. For example, the hypothesis could also cover the difference between two different types of arousing stimulus (for example, auditory versus visual stimulus or two different types of visual stimulus). Further discussion should be included for the observations mentioned of the second-to-last paragraph of the discussion; otherwise, it may be unclear why these studies are interesting as the paper asserts. 