Thank you for sending me this interesting note. A few thoughts on the analysis from a statistician It’s an interesting, though sobering, fact that between 30 and 40 percent of the machine-entered heights are incorrect. Normally the tendency would be for such errors to obscure, rather than generate, associations. This now-known, high error rate makes it less interesting to explore this section of the results. What does a Friedman test measure? It’s a non-parametric version of a repeated measures one-way analysis of variance. Two issues are worth considering.It requires a complete table, so only those subjects with all four years of data may be included. Secondly it produces a three degree of freedom test, which is not very well directed to address the most likely question of interest. We might be most interested in detecting a smooth, linear trend over time. However just as much weight is being given to detect non-linear patterns such as curvature {low, high, high, low} and saw-tooth {low, high, low, high}. I don’t know how centres are compared officially. Comparison of neighbouring years’ data would be unstable. Also, using these non-linear components could be very misleading. I hope that the linear trend is used. Are there any alternative analyses that would address these two issues?Certainly. To begin with, let’s use the original data for FEV1% and not worry about their normality. You could fit a mixed model to these data once you stack them in long format (“varstocases” in SPSS). This would enable you to use all data, not just data for those with a complete set. It would also enable you to extract a one degree of freedom test for trend across the four years.This should be a more powerful approach. I now see that the other referees refer to this as well, though I don’t agree that you need to have at least three observations per subject. If you are worried about the normality (though the published quartiles are not that alarming) then two alternatives would be (1) to find a normalising transformation that would apply to the stacked column of FEV1% values, or (2) to use a rank-based transformation (“Fisher-Yates”) available in SPSS as “rank y /normal into z.” What might be the mathematics underlying any difference in slope obtained by the Knudson and GLI methods? I have tried to abstract the formulae used by Knudson and by GLIin deriving the predicted FEV1 that is used in the calculation of FEV1%. For a specific example I have chosen males (slightly more common in this study) aged 25 to 28 years (somewhere near the median age) with height of 175cm (just below mean UK adult height). The Knudson equation has a functional form FEV1 predicted = 5.1228 – 0.0292.age. FEV1% = FEV1/(FEV1 predicted) can then be differentiated to see how it varies with changes in age and FEV1 However the GLI equivalent is given as point estimates from a Cole-Green LMS fitting procedure. The penalised cubic splines are not given, so no functional form is available. ​The table shows, just for this combination, how the predicted values compare.Those from Knudson are slightly lower and decrease slightly more rapidly with age.Such differences, and those from other combinations, will work together to determine how FEV1% might be expected to change with age and observed FEV. 