This paper reports the results of a factorial experiment evaluating the Drink Less Smart phone app and its behaviour change components, with the benefit of an extended data-set. The aims of the study were both substantive and methodological. In relation to the latter, the paper provides a very useful example of the advantages of Bayesian hypothesis testing and in particular its legitimate provision for extending data collection until a firm conclusion has been reached. The paper also provides a good example, with earlier papers, of the MOST method for developing and evaluating behaviour change interventions. Regarding the substantive aim, I have two major and several minor comments. MAJOR: Analysis of the full data-set after extension confirmed the mainly inconclusive results of tests of the 5 individual components of the app in the original evaluation study and failed to confirm the previously reported evidence in favour of two interaction effects between components. In view of the authors' thorough and painstaking development of the app over the years, their rigorous evaluations of the components of the app and their stated intention to optimise the app for a definitive test in an RCT with extended follow-up, these results must be regarded as very disappointing. Yet this is not commented on and no possible explanation is offered why components whose inclusion was supported strongly by theory and previous research failed to show effects on drinking behaviour. Is it possible, as the authors have previously suggested, that the 'minimal', control interventions were too active to allow an effect to emerge? What other explanations are there for these disappointing results? Overall, where do the authors go from here in the attempt to bring this very promising intervention technology to practical use? In the Abstract the authors state: 'There was weak evidence for a synergistic effect of four components'. I feel even this is too strong. In the text on p.9 we have: ' An unplanned analysis provided weak anecdotal evidence of a synergistic effect of the ‘enhanced’ versions of these four intervention modules together'. Something along these lines, with the inclusion of 'unplanned' and 'anecdotal' would be more appropriate for the Abstract. Another concern here is that the putative effect in question derives from a post hoc hypothesis based on unplanned comparisons arrived at only after the extension of data collection. In a frequentist approach to hypothesis testing, this would not of course be permissible but, even in the Bayesian approach, there must surely be some constraints on the legitimacy of testing post hoc hypotheses derived from exploratory analyses (rather than using such analyses to generate hypotheses not tested in the present data). The authors should comments on this issue and, if necessary, seek expert advice. MINOR: Why was the follow-up rate so much lower in the extended data-set that in the original data (8.5% versus 26.6%)? Can the authors attempt to explain this difference? p.8: '... to provide potential evidence for what effect size we can expect when planning the trial.' This is presumably the definitive trial with extended follow-up but this is not clear. What about the data on secondary outcomes? These are not reported here but it is not stated that they will not be considered in this paper and the reader is not told where they will be found. The primary outcome measure of self-reported change in past week alcohol consumption was presumably based on the AUDIT-C questionnaire, as suggested by Table 2, but this is not made clear in the text. 