I commend the authors on addressing a critical problem in the scientific literature: the poor quality of scientific writing. We need more empirical studies that address this important issue, and I hope that the authors will continue to pursue this line of research. I also appreciate that the authors have written the paper in a clear and concise manner. The data were also easy to access and understand. The main finding of interest is that there is a weak inverse correlation between CiteScore (a measure of a journal’s impact) and prolixity (a measure of wordiness). This may be evidence that journals with better writing get more citations, although the evidence for this conclusion is fairly weak. I believe that the paper would be strengthened by considering additional metrics of readability. Specific comments: I probably would have chosen the word “wordiness” over “prolixity” in keeping with the theme of encouraging authors to write in a straightforward manner. Prolixity is a more academic term, whereas wordiness is simpler and easier to understand. I’m not sure why the authors have looked at the number of paragraphs and number of characters as outcomes in themselves. The length of the introduction doesn’t tell us about writing quality or wordiness. A long piece can be well-written and a short piece can be poorly written. Also, different journals have different constraints (such as constraints on word counts) that may affect the length of the introduction. Thus, I don’t think these outcomes are very informative, and need not be highlighted in graphics. Why were characters, numbers, and citations analyzed by CiteScore quartile but relative prolixity analyzed treating CiteScore as a continuous variable? In Figure 1, I expected to see a Figure 1d that showed the average prolixity per quartile. This paper would be strengthened by considering other metrics of readability beyond just the ratio of characters to citations. There are numerous online tools that allow one to measure readability with validated measures such as the Flesh Reading Ease Scale. See for example, this tool: http://www.checktext.org/ . Examining the correlation between CiteScore and readability scores would add to the impact of this paper. Journals sometimes have limits on the number of references, which would influence the number of citations appearing in the introduction section. The authors should state whether any of the journals examined had such limitations. The aim of this study was to gauge the association between a journal’s CiteScore and a measure of the journal’s prolixity. But only 3 samples per journal were taken. Given that prolixity may vary widely from paper to paper within the same journal, I believe that a larger sample size per journal would have strengthened this study. It’s not clear that the last three papers published are going to be representative of all studies in a journal. The authors should comment on how they arrived at the choice of 3 studies per journal. The authors should give the magnitude of the correlation coefficient between CiteScore and prolixity. I believe it’s about -.20, which would be considered an extremely weak correlation even though it is statistically significant. The authors should comment on the fact that the correlation is weak. Also, there is one study with high prolixity (600) that is making it hard to see the pattern of correlation in Figure 2; consider presenting the graph both with and without this point. Bar graphs are not ideal. I would recommend that the authors replace bar graphs with box plots with individual points overlaid, as these are more informative. The authors should read and reference this study on text readability in the scientific literature (Plavn-Sigray et al. , 2017 1 ). 