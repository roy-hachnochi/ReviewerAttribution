Biomedical research relies increasingly on the use of high-performance computing (HPC), but there is a gap in effective and timely HPC-training for practising biomedical researchers. This article describes the implementation and deployment of a modular HPC training programme. It is useful in two ways: First, the authors show that a thoughtfully designed and delivered course can achieve the goal of providing useful training on HPC that life scientists can apply to their own research. Second, by making all course materials available under a Creative Commons license, the authors have created an invaluable resource for instructors, HPC facility coordinators and self-directed learners worldwide. We also appreciated that the author share the full (de-identified) data from the participant survey. We do feel that the article category "Opinion Article" does not do this work justice, since it describes a rigorously designed educational intervention and collection of resources, as well as presenting evaluation data. We leave it up to the editors to find a better suited category. Minor points: The abstract should include information about the outcomes of the course. The very first sentence ("The era of genomics and DNA sequencing is being rapidly incorporated into life science research fields ...") should be re-phrased for clarity. Figure 1: A reader's interpretation of a pie chart depends on their ability to interpret areas on the pie chart as representing the underlying numbers. Making the pie chart three-dimensional and tilting it does not add any information. Worse yet, it distorts the information available. Consider using "gender" instead of sex, since this is very probably the information that has been collected from participants. In describing how the course works, we would have liked a bit more information about the teaching assistants, in particular what exactly their role was, how many there were, and how they were trained and prepared for the task. The response rate on the post-course survey was not bad for this type of survey, but there is still a sizable proportion of non-respondents. There should be a short discussion on how this may affect survey outcomes. There is some indication (e.g. Bacon et al. , Marketing Education Review, 2016 1 ) that responders are typically at the more "extreme" ends of the student satisfaction spectrum, i.e. students who really liked or really disliked the course are most likely to respond. As a consequence, a higher response rate would lower the scores for classes with high scores and raise the scores for classes with low scores. But on the other hand, see Nowell et al. (International Review of Economics Education, 2014 2 ) who tried to estimate non-response bias in online teaching evaluations and found it negligibly small. It may be useful for the reader to have at least an acknowledgement of the problem in the article. Reference 10 is to an erratum on a paper, not the paper itself. The erratum here is important, because it clarifies the first author's last name, but we feel that the original paper should also be referenced. The github.io site for the course is really nice, but we notice that the links to final recordings are sometimes broken and just take the user back to the main site. 