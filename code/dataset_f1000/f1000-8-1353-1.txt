This paper gives a good definition of what is a research software (RS) and I agree with the authors that an extensive version of what is a RS. I have however various remarks: A career of a researcher has extensively changed in the recent years: more chairs available for a relatively short period of time (typically 5 years) are given so that in this limited amount of time a relatively young researcher cannot afford to stand by the dissemination and maintenance work required by the evaluation scheme provided by the authors The authors still favor a specific type of RS, with availability, documented, licensed and with versioning. This is dangerous as many RS do not enter in this category: software that is produced to check a specific scientific result, software specifically designed to manage a particular system etc.. These software are not intended to be disseminated either as they are used only for checking and benchmarking or because they are very specific (for example a real-time software where the purpose is to save as much as possible computation time so that it cannot be general by essence as any general procedure will require tests that will penalize the performances) and therefore cannot be reproducible. As developing software is time intensive having too strict rule for the evaluation of software (such as documentation and availability) may discourage researchers producing them although they provide some good idea of the efficiency of the underlying theoretical work. Citations of publications as a measure of impact is already questionable and I will say this is even worse for RS. It is quite current in the computer science community to say that the only good software is the one produced by the authors so that citations is limited and usually not very positive. I fully agree that software development may be a major component in the daily activity of a researcher and therefore must be taken into account in our evaluation. However the specificities of the RS and of the domain must also be taken into account in this evaluation. The CDUR procedure proposed by the authors is very fine but can be applied only on a limited number of RS types and his application will be disastrous for other types. For example it is easy to find RS for which the CDU part is not relevant while the R part is the only one of importance: for example a RS that performs the control of a specific robotic system cannot be reproducible (unless the reviewer has at hand an exactly similar robot), its use is limited to a specific prototype (and the RS has been designed for it) and has to bey tailored to manage another system so it cannot be disseminated. Hence the only evaluation criteria is the R part which is illustrated by the performance of the whole system, being given the performance of the hardware. I recognize that the authors are right in term of objectives: our current approach of RS development may lead to a waste of time with researchers programming again and again the same algorithms instead on focusing on their specific objectives. Some kind of mutualization will be beneficial provided that he RS is sufficiently disseminated and open to the community. But a researcher has to find the right balance between his/her research activity and the time devoted to maintenance, documentation and dissemination, a balance that greatly depends upon the domain. Furthermore good software practices for development is also dependent of the domains. In summary although the authors have been very careful in their definition and evaluation rules and the absolute necessity of flexibility in the evaluation rules, they ended up with an evaluation procedure that may lead researcher to avoid going into the business of RS development. But this is nice paper that is worth being indexed for opening a discussion on RS evaluation according to the specific domain and context in which it has been developed. 