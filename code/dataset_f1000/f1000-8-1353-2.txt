Overall, this is a clear, well-written article outlining a novel procedure that could impact, in positive ways, policies and practices regarding scientific software development and impact. However, I believe there are some aspects of the article that could be either improved or clarified. The study seems to be mostly based on secondary research – more specifically, arguments and ideas are developed based on findings from literature. Moreover, it can be assumed, the study is informed by the proximity of the authors to the academic/professional environment it investigates. However, there is no clear indication on how the material that serves as basis for the analysis was selected (indexed databases? specialized resources such as publications and conferences?), in which case it would be interesting to include a justification on why those sources are relevant. Additional material that could, potentially, be referenced are Chue Hong`s position paper on the need for a framework for comparing software 1 ; and also ESRC's plans of developing a software accreditation framework 2 . In section 2, the authors write: "Note that to cite software in science is different to citing scientific (or more precisely, research) software, which is the issue here". Perhaps the difference could be clarified? In 2.1, the authors write: "in 12,13 we can find the following definition". Given that the definition`s authorship is originally from 12, it would be probably more appropriate to write something along the lines of "a definition elaborated by 12 was summarized by 13 as" There are several passages in both French and English. Wouldn't it be enough to keep the English versions (along with an indication that they were translated from French by the authors?) The authors write: "Finally, let us mention that, as in the case of publications, the research software production of a laboratory is decided and proposed by the lab’s members, and it is approved by the leading institutions during the usual laboratory evaluation and funding procedures". Is that true for every case of research software production and are there any references supporting that? Figure 1, illustrating concepts appearing in the study, could be improved, possibly by matching label colors to the colors of the rectangles for easier identification. Is not clear why there are two screenshots of Plume (one in French and one in English). One would probably be enough? The authors write: "(...) we remark that there can be several classic articles associated to a single RS, and that could be used as its reference". Could references to some of those classic articles be provided? The study ends without (i) considerations on future research, (ii) clear indications on initiatives/pilots for the procedure, (iii) a call to action for the community to test it. Overall, it's not clear what the next steps are. Regarding the applicability of the procedure, the article would benefit from including a table or diagram (probably in Section 4) explaining the procedure, preferably featuring a clear, step-by-step description of the process. That should clarify how CDUR could be adopted, increasing the potential for its adoption and use. 