This is a well-designed and informative examination of data curation practices in support of the Elixir Data Platform, with a focus on exploring curator needs and pain points to identify where automated tools might help. The article presents results from a series of studies based on interviews with curators and responses from a questionnaire filled out by over 40 curators from 11 countries. These results are consistent with an earlier article from 2012, Text Mining for the Biocuration Workflow 1 , which summarized the findings of a workshop held at the third International Biocuration Conference. Below is a short from the 2012 paper: *** Curators wanted tools that were easy to use, easy to install and easy to maintain by the intended end user (ideally, a developer associated with the curation team, who will not necessarily be an expert in text mining or natural language processing). The tools do not have to be perfect, but they need to complement (not replace) the biocurator's function. A number of curation groups indicated that they would use the tools to do an initial batch processing, followed by biocurator validation, where the biocurator makes a yes/no decision and avoids having to type or look names up in a large database. Another important use was linking mentions of biological entities in text with the correct identifiers in biological databases, as well as linkage to the appropriate ontology terms. A number of curators felt that they would like text mining tools to aid in identifying and prioritizing papers for curation, to avoid wasting time on papers that did not have ‘relevant’ (e.g. curatable or novel) results. They also wanted tools to identify the sections of full-text papers containing curatable information. *** It would be informative to do a comparison of the findings from the 2012 paper with this paper, to see whether curator needs have changed – and to what extent automated tools have been able to address some of the curator needs. One of the paper’s most interesting findings from the curator feedback is the need to identify species . This was flagged as a particular pain point, taking ‘up to 75% of the curation effort’! This has also been a consistent stumbling block for text mining systems, because identification of species is essential to link a mention of a gene or protein to the correct accession number in databases such as UniProt or EntrezGene. This turns out to be a hard problem for text mining systems (as well as for curators) for several reasons: 1) mentions of species are often given as background information and may well not be mentioned in the same sentence (or even section) as mentions of the gene or protein being studied; 2) authors may want to generalize findings to other species (especially to humans) even when the experiments have been done on other species; 3) information about the specific experimental constructs may be buried in the methods section – and may involve inserting a gene from one organism into the genome of a different organism. One interesting omission is a discussion of the need for interactive curation tools. This has been a major theme of recent BioCreative evaluations (see, e.g., Overview of the interactive task in BioCreative V 2 ). If an interactive system could show the curator a prioritized list of candidates, this could speed up several curation activities. Specifically, an interactive system could, e.g., show a ranked list of candidate papers for curation, with evidence highlighted; or show a paper (section) with a gene or protein mention highlighted together with a selectable list of candidate species, so the curator could quickly select the correct species in order to link to the correct accession number; or show highlighted evidence sentences supporting protein-protein interaction, for quick validation or rejection by the curator. Given a goal of providing tools to speed the curation workflow, interactive systems offer a promising approach by putting the human in the loop to augment text mining, where automated tools do not, on their own, provide sufficient accuracy. Finally, in the Conclusion section, the authors discuss the need to tailor capabilities for different curation tasks or workflows. Identifying commonalities is indeed key, as the authors note; but there is also an urgent need to develop methods to quickly tailor tools to new tasks or specific requirements in a curation workflow. This is an underexplored area, but may be key to more widespread adoption of text mining tools. Specific comments: Add references to some of the older background work in this area. The Conclusions section of the Abstract mentions “actionable items” but doesn’t provide specifics. The list on p. 9, col 1 top is informative, and could be included in the abstract, e.g., prioritizing papers; filtering articles based on specific entity types; and retrieving specific sections of articles. A table summarizing the different interactions with curation teams and curators would be helpful, along with the specific types of curation being done by the teams. This information is presented in the text at different points, but it is hard to keep track without a summary, since there were several rounds and types of interactions. In Figure 3, it would be useful to know the denominator, as well as the actual number of curators identifying specific sections. Figure 4 is very useful, but hard to read. In addition, it would be interesting to know what specific controlled vocabularies are in use for each of these types of information. Figure 5 – again, it would be useful to know the denominator (the total number of respondents for that question). 