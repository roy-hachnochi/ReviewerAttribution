An important strength of this article is that it shows how teachers can help students learn a potentially dull topic actively by making scientific concepts personally relevant and interesting. There is growing interest in the application of embodiment theory in the field of health professions education so using students’ own bodies as a learning tool, which is a topically relevant feature of the intervention, is another strength of the article. There are, though, threats to the validity of the research, which prevent us agreeing with the authors’ claim that their ‘teaching method in which students assess their own body promotes deeper understanding’. The authors took a classical scientific experimental approach but there were shortcomings in various aspects of the research design: The authors did not frame a null hypothesis. This is a shortcoming because classical experiments depend on enabling researchers to reject a null hypothesis: in reality, we could not find any clearly-articulated hypothesis, which is particularly important when taken together with comment 4, below. Blinding as to whether a participant is subjected to the experimental or the control condition is an important feature of experiments. In reality, participants in this research knew what group they were in and observed members of other groups. It is plausible that this knowledge influenced participants’ mindsets and confidence levels and plausible, therefore, that this influenced the outcomes of the post-intervention tests. We have no way of knowing how strong that influence may have been, and even in which direction, but it may well have confounded the results of the experiment. The randomisation protocol is not described and the information available makes us question the rigour with which this was done. The authors do not appear to have defined, before the experiment, which statistical comparisons would allow them to draw inferences. As a result, they appear to have relied on post-hoc comparisons, and used the 5% level as an absolute arbiter of difference or lack of difference. The snag is that these comparisons were confounded by small but potentially important differences in baseline test scores, which directionally favoured the intervention over the control condition. The authors are at risk of Type II statistical error (false negative) when they conclude that there was no difference between the active and control groups in the pre-intervention test scores because, with larger numbers of subjects, there could have been. In reality, the difference in scores between groups was 0.10.05, which is not much above the 5% level that is deemed significant. We suggest that a comparison between absolute difference in scores from pre to post intervention, adjusted for the confounding effect of baseline levels, would have been a more appropriate analysis and that, at least with the numbers included in this experiment, no statistically significant difference would have been found. Figure 1 appears to contradict findings stated in the text and we found it hard to understand. We would expect any empirical article to include, in the discussion, consideration of the strengths and limitations of the study. We could not work out if there were two questionnaires - one assessing knowledge - or just a questionnaire assessing motivation and satisfaction? Wording needs clarification- ‘The assessment of students’ knowledge levels occurred through two written, closed-notes and closed-book tests and the application of questionnaires on the topics in the classroom’. If questionnaires and tests assessed knowledge, detail is needed to illustrate how data from the two sources were amalgamated. No detail was given about the questionnaire on satisfaction and motivation. There are two typos on page 7. Paragraph one ‘but s no’ and paragraph two missing full stop ‘may drive their attention’. Conclusion: On methodological grounds, we feel the authors cannot reject the null hypothesis that their intervention was ineffective. That is a shame because it might have been! It is more plausible that an active learning approach would be of educational value than that it would not be of educational value, but the study did not show this. We suggest that the authors were not just let down by flaws in their research design, but by their decision to subject education to classical scientific methodology. Regehr 1 has urged education researchers to move from the ‘imperative of proof’ (which was unsuccessfully applied here) to the ‘imperative of representing complexity well’, which the authors’ intervention, but not their evaluation, sought to do. Education is not so simple as comparing a pill and a placebo. Learners are complex, their subject matter is complex, and learning environments are complex. The education community is waking up to the fact that classical scientific experimentation, which is predicated on reducing complex systems to intervention-control comparisons, is ill-suited to complex social systems like education. Qualitative research, in the right hands, can examine relationships between conditions, mechanisms and outcomes in complex learning environments. 2 So whilst we are still in the dark about whether this particular intervention was effective, the study highlighted the need for education researchers both to conduct their work rigorously, and to extend their methodological skills-set. Dakota Armour (medical student) and Tim Dornan (doctor and education researcher) 