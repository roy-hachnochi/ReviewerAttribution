While the study is not novel and the technical sophistication is considerably low, the study addresses one of the most important challenges in biomedicine and I recommend accepting it if the authors agree to make substantial improvements to the manuscript, try out the other machine learning methods, research the prior art, and expand the methodology. Firstly, the study does not provide an overview of the other interpretable biomarkers of aging developed using the multiple data types. Some of the prior clocks are described here: Zhavoronkov et al . 1 . It is very similar to the study published in 2016 ( Putin et al . 2 ) which not only introduced the concept but also provided a comparison with the other machine learning methods including GBM, RF, DT, LR, kNN, ElasticNet, SVM and DNNs and an online testing platform for the hematological aging clocks. All of these machine learning methods allow for the various feature selection and feature importance techniques that provide very different results and pick the most important features differently. This paper explains the differences in how the different machine learning techniques prioritize different genes using the transcriptomic age predictor (Mamoshina et al . 3 ). This is not a recommendation for citing these papers but an example of the work that needs to be done. As it stands, the study looks like a student machine learning data processing exercise and application of the out-of-the-box of python library on the NHANES dataset rather than a complete research paper. The conclusion that SHAP library is a good tool for interpreting the results from a machine learning model is not surprising at all. The paper can be hardly called a methodological paper because it lacks novelty of both methods of age prediction and comparison with classical methods of age prediction using a common blood test. There is a number of issues I noticed that need to be addressed: The paper is lacking the information on how the train and test set were selected along with the age by sex distribution. Was the training and optimization of models performed without cross-validation? At the same time, NHANES data also contains people with various conditions including diabetes and kidney disease. Were those individuals excluded from the training process? These important questions are not clear from the paper and need to be clarified. Related to comment #1: how does the model perform on individuals with chronic diseases? It is not clear why the predicted age is referred to as ‘biological age’. Biological age should be predictive of mortality. The observed difference between predicted and actual age should be associated with outcome in terms of morbidity or mortality. This should be explored in details with respect to the interpretation of the age predictor results. NHANES data has information about mortality that can be used for this type of analysis. At this point, the analysis suggests that selected blood parameters are associated with age and so predictive of chronological age. This type of analysis was performed in one of the referenced papers utilizing the NHANES dataset but not in this paper. It needs to be performed in order for the paper to be published. The baseline is lacking. What would the performance be if you predict all samples as a median age for the population? Would it be higher or would it be the same as the test set error? In line with the above comments, because the performance evaluation is not rigorous and no hyperparameter selection was performed, it is not clear why this age prediction method was selected. One of the commonly used and extensively validated models is Klemera and Doubal. (Klemera P, Doubal S. A new approach to the concept and computation of biological age 4 ). I would suggest exploring KD age prediction model in terms of interoperability of the blood test markers. Would be the machine learning model better? If so, why? As mentioned above, there is no baseline model, comparison of different models or hyperparameters tuning. Without the interpretation of the difference between the predicted and actual chronological age (association with mortality or diseases for example), this difference is just an error of the model. How this error of the model would affect the results? Would the results change if the model is trained on samples that were initially predicted accurately? What about the samples predicted with a greater error? This need to be explored. Related to the point, age distribution plots of those randomly selected samples are needed. How would different age groups contribute to the results? Instead of using k-fold cross-validation authors used just random 80/20 train/test split, so results presented at figure 2 (SHAP summary plots) cannot be interpreted as stable. E.g. for men the first 5 biomarkers are very similar in terms of importance for age prediction, so the order of these five biomarkers probably will be changed using different random data split. Preprocessing is rather scarce. E.g. outlier analysis was not provided. Were they excluded from the analysis? If not, why and how they would contribute the SHAP summary plots? I would like to see the comparison of the estimated reference ranges with commonly accepted reference ranges. A linear fit line on figure 1 is barely visible because dots and line are plotted using the same color It is always a good practice to provide figures optimized color blind readers. Figure 2 colors are hardly distinguishable. Figure 3 is lacking the actual chronological age of the individual analyzed. My recommendation is to address these points and explore the prior art. Biological age prediction using machine learning is a very interesting and important field and the studies need to be consistent and comparable. 