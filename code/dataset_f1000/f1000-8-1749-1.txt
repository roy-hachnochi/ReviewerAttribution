I like the workflowr package and I like the paper. Nicely written. Reproducibility is a big thing and workflowr lowers the entrance barrier for non technical users. This is a huge benefit. The package has has already some visibility (judging based on GitHub stars) and is being adopted to various applications (based on examples presented in the paper). I recommend to accept the paper. Here are some comments that authors may consider: A. The point that I am missing the most is the comparison against the drake package. These two packages seems to be similar, maybe complementary. Can they be used together? It would be good to show pros and cons/similarities and differences. B. I like the workflowr package it is a useful tool. What I am missing is the methodology / description of a process / good practices of how the reproducible analysis should look like. This would be very useful for people that look for precise guidelines on how to integrate the workflowr with every day practice. Wet labs researchers are used to "protocols" that directly guide step by step what to do during the analysis. Maybe it would be possible to give such protocols for reproducible research with the workflowr package. Just to give an example, in the Model Development Process ( https://arxiv.org/abs/1907.04461 ) article there is an overview of phases and tasks shared across model development. In which phases the workflowr or similar tools shall be used? C. Authors have mentioned blogdown and bookdown packages. I think that even a closer match to the reproducibility problem is the package modelDown (see https://joss.theoj.org/papers/10.21105/joss.01444 or GitHub http://github.com/ModelOriented/modelDown ). 1 The modelDown package takes predictive models and creates a HTML website with information about session info, binary models, training/test data and model explanations. The website is created without any additional effort. ModelDown automates the most boring part of the modeling i.e. model documentation. D. When mentioning tools for archivisation of binary objects, it may be also useful to add the pins package recently developed by RStudio (https://cran.r-project.org/web/packages/pins/index.html). It is more limited than other mentioned packages (do not keep information about meta data) but quickly gains popularity. E. After the "Unfortunately, this ideal is not usually achieved in practice; most scientific articles do not come with code that can reproduce their results" maybe authors could share their thoughts why it is the case. It will be useful to list specific reasons why reproducibility fails. Is it primarily because we do not have proper software, or they software is too complex, or one needs to pay for the proper software, or researchers are not aware of the problem? 