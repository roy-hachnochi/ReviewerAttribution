The authors generalize their EmbedSOM approach to examine two additional ways of selecting the respective sets of landmarks in the high- and low-dimensional spaces, beyond the standard SOM, to address. Example data analyses are appreciated. That selection is the first stage in the embedSOM approach. The second stage is the actual embedding enrichment process. The authors explain: "EmbedSOM projection can be viewed as an embedding enrichment method: From a set of landmarks in the high-dimensional space and a set of corresponding landmarks in the low-dimensional space, it produces a smooth function that maps all points from the higher-dimensional space to the low-dimensional space and preserves the relative neighborhoods of the landmarks." Testing: We followed the paper’s guidance on some in-house fcs files and had success with embedSOM and the GQTSOM function. Some naming confusion: By "generalized EmbedSOM” the authors refer to using different ways of generating landmarks, other than the original (self-organizing map) SOM approach. It seems preferable to drop the “SOM” rather than refer to these variants as "generalized EmbedSOM” methods. The authors might use the more general notion of landmarks, rather than SOM. As they note, the random-sampling, followed by tSNE, version of “generalized EmbedSOM” doesn’t use SOMs at all. Re “compacting noise" The first reference of the manuscript includes some background on differences between the “generalized EmbedSOM” approach and what the authors call “plain tSNE and UMAP,” and attributes these differences to the respective designs of the algorithms. In that background paper, the authors explain: “neither UMAP nor tSNE aim to preserve local linearity of the transformation, which allows them to take apart the clusters with noisy data and attach the residual noise to nearest clusters.” They concluded in that paper: “Compacting the residual or unexplained noise is desirable for providing a clean display of the data for publication. On the contrary, almost-immediate availability of all information about very large datasets, including the (often informative) noise, is more important for producing comprehensive graphics for high-throughput analysis." This paper marks an attempt to explore those differences, and the apparent trade-offs, in more detail, so it would benefit from discussing these tradeoffs in the context of the algorithm designs. The authors noted in the first reference, "While the observed cluster separation may be desirable if the embedding is expected to approximate the population boundaries, it may be inappropriate if the population environment is relevant for analysis." GQTSOMs: The manuscript introduces a new landmark-generating algorithm that simplifies a hierarchical variant of an adaptive SOM approach, namely, growing quad tree SOMS (GQTSOMs) as a simplified growing hierarchical SOM (GHSOM), which is in turn a variant of growing SOMs (GSOMs). The aim is to identify and incorporate features in the input space more efficiently than random sampling, by using a "layered structure of SOMs". This is a natural thing to do to improve on SOM. On page 8 the authors report that GQTSOM leads to using a “smaller amount of more precise landmarks” and thus faster computation, and appears to be a nice contribution. 