The authors present a new method for detection of DTU from RNA-seq data, which uniquely leverages quantification uncertainty in the form of inferential replicates. I am not aware of other methods specifically designed to detect DTU as opposed to change in total expression level of the gene, which take into account quantification uncertainty. It is therefore a useful contribution to the methods literature. The authors have taken some length to assess their method against other popular methods on real and simulated datasets, and investigating individual genes with qRT-PCR validation in detail. I have some concerns about the conclusions from the evidence provided in the article, and additionally have requests for further details about the methods, which should be presented in the article itself. Major comments: 1) The methods are not sufficiently described, I have the following questions: What is the input to RATs? Is it TPM or counts or scaledTPM? Should the library size differences be removed prior to providing to RATs or does RATs take care of library size differences internally? Can the methods described all analyze the same type of experiment, are they all restricted to two-group analyses? Can any of them control for batch effects? What are the default pre-filtering and post-filtering settings? What is the default minimum abundance threshold or proportion threshold for an isoform to be considered expressed? What is the default effect size cutoff, and how is it implemented per isoform, per gene? What is the default fraction for determining that evidence of DTU is not substantiated across inferential replicates? Likewise, what default fraction for biological replicate variation? 2) I didn't understand why abundance thresholds were not used, as described here, " No transcript abundance pre-filter was imposed on any of the three DTU tools ," and also " As SUPPA2 offered no abundance pre-filtering, RATs and DRIM-Seq were run with abundance threshold values of 0. " As shown in Soneson et al. (2016 1 ) and Love et al. (2018 2 ), performance of a number of DTU methods is greatly improved by filtering out lowly expressed transcripts. It can be inferred from the title of the former paper: " Isoform prefiltering improves performance of count-based methods for analysis of differential transcript usage ". SUPPA2 does have an abundance pre-filtering option, which was used in Love et al. (2018 2 ): " We enabled a filter to remove transcripts with less than 1 TPM. TPM filtering is a command-line option available during the diffSplice step of SUPPA2 and this greatly improved the running time without loss of sensitivity ". From the SUPPA2 manual: " -th | --tpm-threshold: Minimum expression (calculated as average TPM value within-replicates and between-conditions) to be included in the analysis. (Default: 0). " Given that all the methods have abundance and/or proportion filters available, that filters are recommended by at least two of the three methods in their documentation (DRIMSeq and RATs), and at least two independent review papers (not introducing methods) have shown that abundance and/or proportion filtering improves performance of methods, I can't see why the choice was made to not use filters. 3) It is mentioned in the false positive analysis that the median FP fraction was less than 0.05 and a horizontal line is drawn on Fig 2A and B. This is misleading, as the adjusted p-values are being thresholded at 0.05 (I assume), and in a null comparison the rate of false positives from an adjusted p-values should be 0, not 0.05. Drawing or mentioning a 0.05 cutoff would be relevant for the p-values (uncorrected), but has no bearing on the adjusted p-values. This may confuse readers. 4) The authors repeatedly refer to the reported effect size in DRIMSeq being an issue for comparison across methods, e.g. " Direct comparison with DRIM-Seq is complicated by different methods for measuring DTU effect-sizes between the tools ", but this is only an issue to the extent that the authors wish to perform post-hoc filtering on effect size. It is not an issue for null hypothesis testing without post-hoc filtering, because all methods are testing against the null that the underlying proportion of expression across isoforms has the same distribution for control samples and treated samples. However, I agree that for post-hoc filtering, one may want to filter the methods in a similar manner. It should be easy to filter the DRIMSeq results directly on absolute difference in isoform proportion, for example in Love et al. (2018 2 ) we performed post-hoc filtering for DRIMSeq on the SD of proportions across all samples using a 6-line R function. As the likelihood ratio statistic should be 1-1 and monotonic with the p-value for DRIMSeq (if the degrees of freedom is constant across genes or transcripts), then I would not compare effect size filtering with likelihood ratio filtering, as the latter is simply filtering the p-value at a lower threshold. Minor comments: In the Introduction, the authors state " there is little justification for choosing DGE over DTE in the study of complex transcriptomes ". The authors imply that gene-level and transcript-level analysis are mutually exclusive analyses, when they are not, and so I would suggest to reword or reconsider this statement. I and others have encouraged assessment of total changes in gene expression (DGE) as well as changes in isoform proportion (DTU), as both may be present in an experiment and both may be of biological importance to the system being studied. DGE has the property that the majority of inferential uncertainty which exists in an RNA-seq sample is removed (because it occurs across isoforms within genes), leaving inferential uncertainty from reads mapping across gene loci, but this property of reduced uncertainty does not preclude a transcript-level analysis. While DTE has advantages, the above sentence claiming that DGE has none overstates a more complex situation in my opinion. Throughout the paper, the authors refer to "DRIMSeq" as "DRIM-Seq" which is minor but different than the software and publication. For what it's worth, the transcript-level test is similar conceptually to the current implementation of testForDEU() in DEXSeq which compares the expression of each feature to the sum of expression from all other features of the gene (this is also different from the test described in the original DEXSeq publication). Running DEXSeq on transcript estimated counts with testForDEU() was tested on simulated data in Soneson et al. (2016 1 ) and Love et al. (2018 2 ), and so such an approach has some evidence of working well for detection of isoform changes within a gene. I didn't understand what was meant by the following: " the tables record the full provenance of the results ". It is stated that, " The performance results of RATs on these simulated datasets are in good agreement with those presented in Love et al (2018) ". However, this seems to be not clearly the case, which may be due to differences in the simulated data in the two articles, or some other reason. In the present article, DRIMSeq is reported as having lower sensitivity with lower achieved FDR than other methods, SUPPA2 has higher sensitivity and higher FDR, and RATs with various filter thresholds falls in between. In Love et al. (2018 2 ), DRIMSeq had the opposite performance: higher sensitivity but higher FDR relative to SUPPA2 and RATs run with default filters. However interpretation is made difficult by all the filtering options in Figure 3. It would be easier to compare perhaps if an additional supplementary plot to Figure 3 was made with only the default filter thresholds instead of the filter threshold ranges for all methods. The main commonality across the two benchmarks seems to be that RATs can achieve higher sensitivity than SUPPA2 while maintaining the same precision, for the 5% nominal FDR threshold. This sentence needs to be made more specific, or else it could be misleading: " As a consequence, the qRT-PCR intensities measured in the original study are actually impossible to interpret in the context of the updated annotation and the originally reported conclusion is likely wrong. " Specifically which conclusion is likely wrong? From the analysis, it seemed like there is not a problem with the original qRT-PCR intensities and interpretation for at least one of the three genes. Why is it perplexing that " 18% of the SUPPA2 results are rejected due to the effect size filter ". I didn't follow the authors in that statement. It is stated, " Existing tools rely on the mean isoform abundances ...". This implies that the mean of inferential replicates is used for statistical testing. It's perhaps subtly different, other methods are typically using the maximum likelihood estimate, which may be different than the mean of the bootstrap distribution, and different than the mean of the Gibbs sampling distribution. I would just say that other tools do not make use of inferential replication. 