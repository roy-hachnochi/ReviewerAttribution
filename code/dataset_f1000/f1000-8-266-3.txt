The authors have performed an interesting study comparing differences in selected metrics (CiteScore, percent cited, SNIP, scholarly output) between medical OA and non-OA journals. This study, however, presents some limitations that require significant revisions: Authors are using a robust number of journals (5835 medical journals) that will allow the performance of parametric tests. I would like to see that they applied at least an ANOVA comparing three groups: OA, non-OA and OA articles in non-OA journals (hybrid journals). The hybrid group is fundamental, and they do not include it. The authors should mention to the reader the bias of this decision. Also in this point, it is essential the authors explain why, if previous studies have concluded that the Eigenfactorscore is the best predictor of citations 1 , 2 , they did not include this metric in their analyses? Please also explain to the readers, why a linear-mixed-model design analysis was not performed. It is necessary to mention references of recent articles 3 citing the existing correlations between the selected bibliometrics (CiteScore vs SNIP, Citescore vs IF, etc.) with at least two purposes: that the authors justified why they did not include a correlation analysis in their study, and that the readers be aware of the limitations in the correlation analysis, and also how the medical speciality may influence the results. It would be desirable to present a subgroup analysis of the medical specialities with the higher number of citations (for example oncology) as an example of the expected variability within subspecialties. If you report in the methods section that you used the SCImago Quartiles, why not control the effect of this variable using ANCOVA O MANCOVA? For example, if the authors are using the data from 5835 medical journals, this data allows a more robust analysis besides descriptive statistics and Mann-Whitney tests. 