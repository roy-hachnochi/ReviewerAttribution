This is important research given the impact of research funding on what science is funded and the flow-on benefits of research to the public. The authors examined the recent literature, but very usefully they also interview funders to find out what is happening in practice. I think this review could be very useful for funders and for researchers working in meta-research. My main issue was that important detail was lacking in a number of places concerning the strategies and changes used by funders. This is a list of places where I wanted more detail, which are mostly based on text in the tables: Table 5, the German Research Foundation have 300 equality measures. Is this a typo? That many measures seems to be far too many to be useful, and the funders could just end up drowning in information. Table 5, the German Research Foundation evaluate bias using "Progression of careers" and "Institutional bias" but some idea of how they do this would be useful. Do they only examine gender bias? Do they take any action if they detect bias? Table 5, NIHR, do they provide more money for the higher tiers? Table 5, NIHR, they evaluate bias using "Diversity of applications", but in terms of what? Gender and race? Table 5, ARC, does the ROPE statement have any impact? Especially as some Australian researchers have said that ROPE is "largely perceived as a tokenistic gesture put on forms and never taken into account by the people who make decisions and evaluate work". 1 Table 5, ARC, what does "ongoing access to all research projects" mean? Access for who? The ARC or the public? Table 5, ARC, they look at the discrepancy in scores, but what action do they take when they find a discrepancy? Table 5, ARC, is international benchmarking sensible given that funders in other countries could also be struggling with biases in terms of gender or race? Wouldn't it be better to set targets/benchmarks based on consultation with the research community? Table 7, German Research Foundation, how does have nine different schemes reduce burden? It could have the opposite effect if people apply for multiple schemes to increase their chance of winning some funding. Table 7, Medical Research Council, how does a report on the conduct and outcome of a project reduce burden? Table 7, ZonMw, does the pre-screening of applications based on CVs reduce burden because the reviewers therefore see fewer applications? Table 7, ZonMw, how does the mid- and end-point review evaluate burden? Table 9, CIHR, how does having over 50 percent of funding at investigator-driven research increase innovation? What if investigators only put forward conservative ideas? See for example work by Nicholson et al. 2 Table 9, ARC, how does a continuous funding round increase innovation or creativity? I can see how this could increase diversity by being more accommodating of researchers with family commitments. Table 9, ERC, "External program evaluation" needs more detail Table 10, more detail is needed for "Longitudinal", does this mean following those who did and did not win funding? Table 11, I did not understand the row entries in the table concerning "personal content of applications" Minor comments Is some mention of the EVIR (Ensuring Value In Research) collaboration of funders merited? Page 8, more cautious language may be needed when using the Levitt study given that it was an exploratory study that does not mention a protocol. In particular I am concerned about the use of quartiles as a threshold, as it's not clear if other thresholds were tried (e.g., tertiles, quintiles). Page 10, in terms of the Clarke et al study - for which I was an author - we actually concluded that the inter-rater agreement appeared higher than in previous studies, so I would not say 'comparable'. I think the most likely reason for this greater agreement was that our study concerned people-funding where the main idea is to rank past performance, whereas all other previous studies had examined project-funding where the main task is to predict future performance which is far harder. Table 6, order the rows by date or first author name? Table 6, Schroter et al 2010 row, the first paragraph in the "Methodology" column end abruptly. Table 6, Barnett et al 2015 study (for which I am an author), the streamlined funding did not lead to an increase in success rates because all the rounds were streamlined. The increase in success rates over time occurred because the scheme had far fewer proposals over time. Many of the proposals in the initial rounds were ineligible because it was a new funding scheme and a lot of researchers simply "had a go". Page 16, first column, first paragraph, the NIH "two-chances" policy has since been relaxed, see work by Kaiser. 3 Page 16, paragraph "Grant application length", it's fairer to say that a shorter form "was associated with" an increased application time, as this was a non-randomised study. Page 17, some funding schemes have used a "wildcard" option for panels, where each panel member was allowed to save one proposal from a culling stage. I cannot find the reference for this, but I can find where we have discussed this idea. 4 There was little on the transparency of funders. If funders opened up the anonymised application data for scrutiny, then this could be a great way of assessing innovation and diversity. 