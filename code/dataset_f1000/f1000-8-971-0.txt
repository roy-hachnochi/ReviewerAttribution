This review of “Preprints and Scholarly Communication: Adoption, Practices, Drivers and Barriers,” covers the work by Chiarelli et al. to survey a group of 38 stakeholders in the preprint and scholarly community in a limited set of scientific domains, specifically, biology, chemistry and psychology. Methodology: As a framework for assessing the adoption of Preprints, the Rogers model is a solid one, and the authors do a good job of crafting a survey around the five core elements of the basic conceptual model. One gap, and I feel it is an important one to be explicit about, particularly regarding Rogers’ conceptualization is the lack of focus on the discussion of motivations. Rogers defines this as the Knowledge-Attitude-Practice (KAP) Gap. To summarize, there is always a cost to make a change in any technical adoption process. The individual must be sufficiently motivated to make the change and thereby overcome the costs of making the change and adopting the new technology. With regards to motivations for adoption of preprints, this is a core element of the drivers of adoption, perhaps more so than the variables and findings outlined in Table 8 (which do follow Rogers’ approach). The potential ‘costs’ related to reputational damage, dissemination of erroneous results, intellectual property theft, or being blocked from formal publication. These costs change depending on the field and could correlate strongly with the willingness to use preprint systems. In table 2, the authors outline variables affecting the rate of adoption. Missing from this, based on KAP-Gap element of Rogers is the concept of the “conservative-ness” of the academic marketplace, where the risks and implications of failure can be significant. These risks increase in some academic domains, but remain regardless of the field. There is also a strong fixity and lack of receptivity of change in the academic social system which also inhibit adoption based on increased levels of ‘cost’ in the Rogers model. While this is not a critical flaw in the paper, focusing on this added element would draw out many of the themes that appear in a variety of the sections and give focus to the very real barriers that exist in the preprint marketplace. One can see that the authors skirt around this issue in some of their questioning and in their analysis. A stronger methodological framework would have highlighted these themes better. It seems that the interviews were intentionally selected to focus on particular domains and to include people with specific engagement with particular communities. While useful in a knowledge gathering process for a qualitative survey and the paper does a reasonable job justifying its selection rationale (page 8 9), the actual selection of participants seemed to be intentional, and this does add the concern about selection bias. References: While I did not review the earlier draft, it appears that the literature review is reasonably balanced and covers a significant swath of the relevant literature, though to be fair this isn’t a review article and I don’t expect it to be comprehensive of every paper in this space. Results: There is much within the results section to dig into intellectually and the feedback is useful qualitative reporting of the opinions of the subjects. Again, with the caveat that there is a potential for selection bias in these results, several additional details would improve the paper. Repeatedly, there references to approximations of the breadth of opinions expressed in the paper, such as page 10 “a small number of interviewees, or in table 5, where quantification of these amounts might be informative. I do understand that this is not a quantitative study and that including those figures might give that impression. There were inconsistencies or contradictions in a number of the responses, which are referenced in the reporting of the results, which is an indication of the variety of responses and perspectives covered in this survey. Some examples of these contradictions include the trust in preprints as a component of the publication workflow in which the manuscript would eventually be published, but yet preprint servers are also reported as a home for “orphaned” works that can’t find a ‘traditional’ publication home (page 13). The core value of preprints as a home for open access content delivery, but there is a rapidly growing acceptance of open access publication in the traditional space (page 13). That the readers of preprints should accept greater responsibility for assessing the quality of the papers, while also promoting their value as a source of information for the broader public, who likely are least well-positioned to assess the quality of a research output (page 14). The tension between the need for quality and trusted review of science and the desire to get results out the door as quickly as possible (page 14/15). This last tension is highlighted in the current environment by the rapid availability of manuscripts posted in preprint repositories on the coronavirus first identified in Wuhan, Hubei Province, China in the winter of 2020. I would have hoped for a fuller discussion and consideration of these results in the conclusion section of the paper. Again, I do not find this to be a fatal flaw of the paper. Perhaps the publication of these results will add to the urgency for a need to discuss these tensions of opinions related to preprints. The chicken-or-egg problem in the discussion section on whether the lack of repositories for a domain is limiting preprint deposit or the lack of demand is limiting creation of repositories. It seems in the current environment that much like there’s a journal for every paper, there is a repository for every preprint. This also seems out of place when the domains surveyed are ones in which specifically selected as having available repositories. Overall, the paper is well-written and well-reported. I recommend approval for indexing. 