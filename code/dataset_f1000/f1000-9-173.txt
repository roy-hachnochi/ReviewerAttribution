

    <!DOCTYPE html>
<html class="">

        
<head>
    <title>Applying machine learning EEG signal classification... | F1000Research</title>
    <meta charset="utf-8">
    <!--<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>-->
    <!--<meta lang="$locale">-->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="9-QVycOO2_ob3Z9QzRmXv2CF08A9oyYXqWyTiVdKPlU" />
    <!-- This is commented out to fix display problems on mobile devices.
    We may use it again once we implement a responsive design that supports native device resolutions.
    <meta name="viewport" content="width=device-width"> -->

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



    <link rel="alternate" title="Recent articles published in F1000Research" href="/rss" type="application/rss+xml">
            <link rel="alternate" type="application/pdf" title="PDF" href="https://f1000research.com/articles/9-173/pdf"/>
        <link rel="canonical" href="https://f1000research.com/articles/9-173" />
    
            <meta name="description" content="Read the original article in full on F1000Research: Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity" />
    
            <meta name="og:title" content="F1000Research Article: Applying machine learning EEG signal classification to emotion&#8209;related brain anticipatory activity.">
            <meta name="og:description" content="Read the latest article version by Marco Bilucaglia, Gian Marco Duma, Giovanni Mento, Luca Semenzato, Patrizio E. Tressoldi, at F1000Research.">
            <meta name="og:image" content="/img/sharing/og_research.png">
            <meta name="version-id" content="24487">
            <meta name="article-id" content="22202">
            <meta name="dc.title" content="Applying machine learning EEG signal classification to emotion&#8209;related brain anticipatory activity">
            <meta name="dc.description" content="Machine learning approaches have been fruitfully applied to several neurophysiological signal classification problems. Considering the relevance of emotion in human cognition and behaviour, an important application of machine learning has been found in the field of emotion identification based on neurophysiological activity. Nonetheless, there is high variability in results in the literature depending on the neuronal activity measurement, the signal features and the classifier type. The present work aims to provide new methodological insight into machine learning applied to emotion identification based on electrophysiological brain activity. For this reason, we analysed previously recorded EEG activity measured while emotional stimuli, high and low arousal (auditory and visual) were provided to a group of healthy participants. Our target signal to classify was the pre-stimulus onset brain activity. Classification performance of three different classifiers (linear discriminant analysis, support vector machine and k-nearest neighbour) was compared using both spectral and temporal features. Furthermore, we also contrasted the classifiers&rsquo; performance with static and dynamic (time evolving) features. The results show a clear increase in classification accuracy with temporal dynamic features. In particular, the support vector machine classifiers with temporal features showed the best accuracy (63.8 %) in classifying high vs low arousal auditory stimuli.">
            <meta name="dc.subject" content="EEG, brain anticipatory activity, machine learning, emotion">
            <meta name="dc.creator" content="Bilucaglia, Marco">
            <meta name="dc.creator" content="Duma, Gian Marco">
            <meta name="dc.creator" content="Mento, Giovanni">
            <meta name="dc.creator" content="Semenzato, Luca">
            <meta name="dc.creator" content="Tressoldi, Patrizio E.">
            <meta name="dc.date" content="2020/03/10">
            <meta name="dc.identifier" content="doi:10.12688/f1000research.22202.1">
            <meta name="dc.source" content="F1000Research 2020 9:173">
            <meta name="dc.format" content="text/html">
            <meta name="dc.language" content="en">
            <meta name="dc.publisher" content="F1000 Research Limited">
            <meta name="dc.rights" content="https://creativecommons.org/licenses/by/3.0/igo/">
            <meta name="dc.type" content="text">
            <meta name="prism.keyword" content="EEG">
            <meta name="prism.keyword" content="brain anticipatory activity">
            <meta name="prism.keyword" content="machine learning">
            <meta name="prism.keyword" content="emotion">
            <meta name="prism.publication.Name" content="F1000Research">
            <meta name="prism.publicationDate" content="2020/03/10">
            <meta name="prism.volume" content="9">
            <meta name="prism.number" content="173">
            <meta name="prism.versionIdentifier" content="1">
            <meta name="prism.doi" content="10.12688/f1000research.22202.1">
            <meta name="prism.url" content="https://f1000research.com/articles/9-173">
            <meta name="citation_title" content="Applying machine learning EEG signal classification to emotion&#8209;related brain anticipatory activity">
            <meta name="citation_abstract" content="Machine learning approaches have been fruitfully applied to several neurophysiological signal classification problems. Considering the relevance of emotion in human cognition and behaviour, an important application of machine learning has been found in the field of emotion identification based on neurophysiological activity. Nonetheless, there is high variability in results in the literature depending on the neuronal activity measurement, the signal features and the classifier type. The present work aims to provide new methodological insight into machine learning applied to emotion identification based on electrophysiological brain activity. For this reason, we analysed previously recorded EEG activity measured while emotional stimuli, high and low arousal (auditory and visual) were provided to a group of healthy participants. Our target signal to classify was the pre-stimulus onset brain activity. Classification performance of three different classifiers (linear discriminant analysis, support vector machine and k-nearest neighbour) was compared using both spectral and temporal features. Furthermore, we also contrasted the classifiers&rsquo; performance with static and dynamic (time evolving) features. The results show a clear increase in classification accuracy with temporal dynamic features. In particular, the support vector machine classifiers with temporal features showed the best accuracy (63.8 %) in classifying high vs low arousal auditory stimuli.">
            <meta name="citation_description" content="Machine learning approaches have been fruitfully applied to several neurophysiological signal classification problems. Considering the relevance of emotion in human cognition and behaviour, an important application of machine learning has been found in the field of emotion identification based on neurophysiological activity. Nonetheless, there is high variability in results in the literature depending on the neuronal activity measurement, the signal features and the classifier type. The present work aims to provide new methodological insight into machine learning applied to emotion identification based on electrophysiological brain activity. For this reason, we analysed previously recorded EEG activity measured while emotional stimuli, high and low arousal (auditory and visual) were provided to a group of healthy participants. Our target signal to classify was the pre-stimulus onset brain activity. Classification performance of three different classifiers (linear discriminant analysis, support vector machine and k-nearest neighbour) was compared using both spectral and temporal features. Furthermore, we also contrasted the classifiers&rsquo; performance with static and dynamic (time evolving) features. The results show a clear increase in classification accuracy with temporal dynamic features. In particular, the support vector machine classifiers with temporal features showed the best accuracy (63.8 %) in classifying high vs low arousal auditory stimuli.">
            <meta name="citation_keywords" content="EEG, brain anticipatory activity, machine learning, emotion">
            <meta name="citation_journal_title" content="F1000Research">
            <meta name="citation_author" content="Marco Bilucaglia">
            <meta name="citation_author_institution" content="Behavior and BrainLab, Universit&agrave; IULM, Milan, Italy">
            <meta name="citation_author" content="Gian Marco Duma">
            <meta name="citation_author_institution" content="Department of Developmental and Social Psychology (DPSS), Universit&agrave; degli Studi di Padova, Padova, Italy">
            <meta name="citation_author" content="Giovanni Mento">
            <meta name="citation_author_institution" content="Department of General Psychology, Universit&agrave; degli Studi di Padova, Padova, Italy">
            <meta name="citation_author" content="Luca Semenzato">
            <meta name="citation_author_institution" content="Department of General Psychology, Universit&agrave; degli Studi di Padova, Padova, Italy">
            <meta name="citation_author" content="Patrizio E. Tressoldi">
            <meta name="citation_author_institution" content="Science of Consciousness Research Group, Department of General Psychology, Universit&agrave; degli Studi di Padova, Padova, Italy">
            <meta name="citation_publication_date" content="2020/03/10">
            <meta name="citation_volume" content="9">
            <meta name="citation_publication_number" content="173">
            <meta name="citation_version_number" content="1">
            <meta name="citation_doi" content="10.12688/f1000research.22202.1">
            <meta name="citation_firstpage" content="173">
            <meta name="citation_pdf_url" content="https://f1000research.com/articles/9-173/v1/pdf">
    

    
    <link href="/img/favicon-research.ico" rel="shortcut icon" type="image/ico">
    <link href="/img/favicon-research.ico" rel="icon" type="image/ico">

        <link rel="stylesheet" href="/1597255280893/css/mdl/material-design-lite.css" type="text/css" media="all" />
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/2.2.43/css/materialdesignicons.min.css" />

        
            
    <link rel="stylesheet" href="/1597255280893/css/F1000Research.css" type="text/css" media="all" />
    <link rel="stylesheet" href="/css/F1000ResearchFontIcons/F1000ResearchFontIcons.css" type="text/css" media="all" />
    <link rel="stylesheet" href="/css/F1000ResearchFontIcons/animation.css" type="text/css" media="all" />

        <!--[if IE 7]><link rel="stylesheet" href="/css/F1000ResearchFontIcons/F1000ResearchFontIcons-ie7.css" media="all" /><![endif]-->

                    <script>dataLayer = [];</script>
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-54Z2SBK');</script>
        <!-- End Google Tag Manager -->
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.8.1.min.js"><\/script>')</script>
    <script src="/1597255280893/js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>
    <script src="/1597255280893/js/shared_scripts/sticky.js"></script>
    <script src="/1597255280893/js/shared_scripts/helpers.js"></script>
    <script src="/1597255280893/js/shared_scripts/menu.js"></script>
    <script src="/1597255280893/js/shared_scripts/navbar.js"></script>
    <script src="/1597255280893/js/shared_scripts/platforms.js"></script>
    <script src="/1597255280893/js/shared_scripts/object-polyfills.js"></script>
            <script src="/1597255280893/js/vendor/lodash.min.js"></script>
        <script>CKEDITOR_BASEPATH='https://f1000research.com/js/ckeditor/'</script>
    <script src="https://f1000researchdata.s3-eu-west-1.amazonaws.com/js/plugins.js"></script>
    <script src="/1597255280893/js/shared_scripts/helpers.js"></script>
    <script src="/1597255280893/js/app/research.js"></script>
    <script>window.reactTheme = 'research';</script>
    <script src="/1597255280893/js/public/bundle.js"></script>

    <script src="/1597255280893/js/app/research.ui.js"></script>
    <script src="/1597255280893/js/app/login.js"></script>
    <script src="/1597255280893/js/app/main.js"></script>
    <script src="/1597255280893/js/app/js-date-format.min.js"></script>
    <script src="/1597255280893/js/app/search.js"></script>
    <script src="/1597255280893/js/app/cookies_warning.js"></script>
    <script src="/1597255280893/js/mdl/mdl.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script src="https://f1000researchdata.s3-eu-west-1.amazonaws.com/js/ckeditor.js"></script>
            <script src="/js/ckeditor/adapters/jquery.js"></script>
                <script src="/js/article/article_scrolling_module.js"></script>
            <script src="/js/article/article_stats.js"></script>
            <script src="/js/article/article.js"></script>
            <script src="/js/shared_scripts/referee_timeline_pagination.js"></script>
            <script src="/js/app/text_editor_controller.js"></script>
            <script src="//s7.addthis.com/js/250/addthis_widget.js#pubid=ra-503e5e99593dc42c"></script>
            <script src="/js/article/article_metrics.js"></script>
        
                                                                            <script>
            if (window.location.hash == '#_=_'){
                window.location = window.location.href.split('#')[0]
            }
        </script>

                    
        
    <!-- pixelId: 1641728616063202 :: assetPixelId: 6034867600215 :: funderPixelId:  -->

            <!-- Facebook pixel code (merged with EP GTM code) -->
        <script>
            !function(f,b,e,v,n,t,s){if(f.fbq)return;n=f.fbq=function()

            {n.callMethod? n.callMethod.apply(n,arguments):n.queue.push(arguments)}
            ;if(!f._fbq)f._fbq=n;
            n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;
            t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,
            document,'script','https://connect.facebook.net/en_US/fbevents.js');

            fbq('init', '1641728616063202');

            
            fbq('track', "PixelInitialized", {});
        </script>

        <noscript><img height="1" width="1" style="display:none"
            src="https://www.facebook.com/tr?id=1641728616063202&noscript=1&amp;ev=PixelInitialized"
        /></noscript>
        <!-- End Facebook Pixel Code -->
    
                <script>
            (function(h,o,t,j,a,r){
                h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
                h._hjSettings={hjid:917825,hjsv:6};
                a=o.getElementsByTagName('head')[0];
                r=o.createElement('script');r.async=1;
                r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
                a.appendChild(r);
            })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
        </script>
    
</head>
<body  class="o-page-container no-js p-article o-layout-reset   ">

    
                            <!-- Google Tag Manager (noscript) -->
        <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-54Z2SBK"
        height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
        <!-- End Google Tag Manager (noscript) -->
    
        
    <div class="o-page">

        <div id="notify-container"></div>
        <div id="pageWarning"></div>
        <div id="pageMessage"></div>
        <div id="pageFooterMessage"></div>

                                <div id="f1r-ga-data" data-name="f1r-ga-data" class="f1r-ga-data"
                data-user-registered="false"
                data-user-module=""
                data-current-path=""
                data-location=""
                data-website="F1000Research"
                data-websiteDisplayName="F1000Research">
            </div>
        
                
        
        <div class="header-wrapper   js-navbar-space ">
                            


    










        
                            
    
            



<nav class="c-navbar js-navbar js-mini-nav js-sticky c-navbar--js-sticky c-navbar--userSite c-navbar__platform-bgcolor  c-navbar--bg-f1000research ">

    <div class="c-navbar__content">

                                <div class="c-navbar__extras">
            <div class="o-wrapper">
                <div class="o-actions o-actions--middle c-navbar__extras-row">
                    <div class="o-actions__primary">
                        
                                            </div>
                                    </div>
            </div>
        </div>

        <div class="o-wrapper t-inverted js-sticky-start">

            <div class="c-navbar__branding-row">
                <div class="c-navbar__row">


                                        
                    <div class="c-navbar__primary u-mr--2">

                                                                                                                                                                                                    <a href="/" class="c-navbar__branding u-ib u-middle"   data-test-id="nav_branding"  >
                                <img class="u-ib u-middle" src="/img/research/F1000Research_white_solid.svg" alt="F1000Research">
                            </a>
                                            </div>

                    <div class="c-navbar__secondary c-navbar__row">


                                                
                                                    <form action="/search" class="-navbar__secondary u-mr--2 c-search-form js-search-form u-hide u-show@navbar">
                                <label for="searchInput" class="c-search-form__label _mdl-layout">
                                    <input name="q" type="search" class="c-search-form__input" id="searchInput" placeholder="Search">
                                    <button type="submit" class="c-search-form__submit mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">search</i></button>
                                </label>
                            </form>
                        
                                                
                                                    <div class="c-navbar__primary u-hide u-show@navbar">
                                <div class="_mdl-layout c-navbar__cta">
                                    <a class="mdl-button mdl-js-button mdl-button--raised mdl-button--no-shadow mdl-button--multi-line mdl-js-ripple-effect mdl-button--inverted c-navbar__submit" href="/for-authors/publish-your-research"   data-test-id="nav_submit_research"  ><i class="material-icons">file_upload</i>Submit your research</a>
                                </div>
                            </div>
                        

                                                
                        <span class="u-hide@navbar _mdl-layout u-nowrap">

                                                            <button type="button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon c-navbar__toggle c-navbar__toggle--menu js-navbar-toggle" data-focus="#navbar_mob_search_input" data-toggle="navbarMenu" data-target="navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation"   data-test-id="nav_menu_search_mob"  ><i class="material-icons">search</i></button>
                            
                                                            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-button--multi-line c-navbar__toggle c-navbar__toggle--menu js-navbar-toggle" type="button" data-toggle="navbarMenu" data-target="navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation"   data-test-id="nav_menu_toggle_mob"  >
                                    <i class="material-icons c-navbar__toggle-open">menu</i>
                                    <i class="material-icons c-navbar__toggle-close">close</i>
                                </button>
                                                    </span>
                    </div>

                </div>
            </div>

                        
                            <div class="c-navbar__menu-row js-navbar-block is-collapsed" id="navbarMenu">

                                                                                                                                                
                                                                    
                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                            

                                                                
                                                                                                                                                                                                                                
                                            
                    <div class="c-navbar__menu-row-content">

                                                                            <div class="u-hide@navbar c-navbar__menu-bar-spacing">
                                <form action="/search" class="c-search-form js-search-form">
                                    <label for="navbar_mob_search_input" class="c-search-form__label _mdl-layout">
                                        <input id="navbar_mob_search_input" name="q" type="search" class="c-search-form__input" placeholder="Search">
                                        <button type="submit" class="c-search-form__submit mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">search</i></button>
                                    </label>
                                </form>
                            </div>
                        
                        <div class="o-actions o-actions--middle">

                            <div class="o-actions__primary">

                                                                
    <ul class="c-menubar c-navbar__menu-bar js-main-menu"   id="main-menu"   role="menubar" aria-label="Main Navigation"  data-menu-group="navbar" >

        
            
                                                                
                                
                                
                                    <li role="none"
                        data-index="0"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="/browse/articles" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                                                    tabindex="0"
                              data-test-id="nav_browse"                              >Browse</a>

                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="1"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="/gateways" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                                                    tabindex="-1"
                              data-test-id="nav_gatewaysViewAndBrowse"                              >Gateways & Collections</a>

                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="2"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                               c-menubar__item--selected c-navbar__menu-bar-item--parent ">

                        <a href="#" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                         aria-haspopup="true" aria-expanded="false"                             tabindex="-1"
                              data-test-id="nav_for-authors"                              >How to Publish</a>

                                                    
    <ul class="c-menu js-menu
                is-collapsed                c-menubar__menu c-navbar__menu"
                  role="menu"
        aria-label="How to Publish">

        
            
                                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_submit-manuscript"                      href="/for-authors/publish-your-research"
                    role="menuitem"
                    tabindex="0">Submit your Research</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-submissions"                      href="/for-authors/my-submissions"
                    role="menuitem"
                    tabindex="-1">My Submissions</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_article-guidelines"                      href="/for-authors/article-guidelines"
                    role="menuitem"
                    tabindex="-1">Article Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_article-guidelines-new-versions"                      href="/for-authors/article-guidelines-new-versions"
                    role="menuitem"
                    tabindex="-1">Article Guidelines (New Versions)</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_data-guidelines"                      href="/for-authors/data-guidelines"
                    role="menuitem"
                    tabindex="-1">Data Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_asset-guidelines"                      href="/for-authors/posters-and-slides-guidelines"
                    role="menuitem"
                    tabindex="-1">Posters and Slides Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_document-guidelines"                      href="/for-authors/document-guidelines"
                    role="menuitem"
                    tabindex="-1">Document Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_article-processing-charges"                      href="/for-authors/article-processing-charges"
                    role="menuitem"
                    tabindex="-1">Article Processing Charges</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_finding-referees"                      href="/for-authors/tips-for-finding-referees"
                    role="menuitem"
                    tabindex="-1">Finding Article Reviewers</a>

                </li>

                        </ul>
                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="3"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                               c-menubar__item--selected c-navbar__menu-bar-item--parent ">

                        <a href="#" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                         aria-haspopup="true" aria-expanded="false"                             tabindex="-1"
                              data-test-id="nav_about-contact"                              >About</a>

                                                    
    <ul class="c-menu js-menu
                is-collapsed                c-menubar__menu c-navbar__menu"
                  role="menu"
        aria-label="About">

        
            
                                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_about-page"                      href="/about"
                    role="menuitem"
                    tabindex="0">How it Works</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_referee-guidelines"                      href="/for-referees/guidelines"
                    role="menuitem"
                    tabindex="-1">For Reviewers</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_advisoryPanel"                      href="/advisors"
                    role="menuitem"
                    tabindex="-1">Our Advisors</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_policy-page"                      href="/about/policies"
                    role="menuitem"
                    tabindex="-1">Policies</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_glossary-page"                      href="/glossary"
                    role="menuitem"
                    tabindex="-1">Glossary</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_faqs-page"                      href="/faqs"
                    role="menuitem"
                    tabindex="-1">FAQs</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              u-hide u-show@navbar"
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      u-hide u-show@navbar"
                                          data-test-id="nav_for-developers"                      href="/developers"
                    role="menuitem"
                    tabindex="-1">For Developers</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_newsroom-page"                      href="/newsroom"
                    role="menuitem"
                    tabindex="-1">Newsroom</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_contact-page"                      href="/contact"
                    role="menuitem"
                    tabindex="-1">Contact</a>

                </li>

                        </ul>
                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="4"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="https://blog.f1000.com/blogs/f1000research/" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                             target="_blank"                                                         tabindex="-1"
                              data-test-id="nav_blog"                              >Blog</a>

                                            </li>
                
            
        
    </ul>



                            </div>

                            <div class="o-actions__secondary">

                                                                
    <ul class="c-menubar c-navbar__menu-bar js-main-menu"   id="secondary-items"   role="menubar" aria-label="My Account"  data-menu-group="navbar" >

        
            
                                                                
                                
                                
                                    <li role="none"
                        data-index="0"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                               c-menubar__item--selected c-navbar__menu-bar-item--parent ">

                        <a href="#" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                         aria-haspopup="true" aria-expanded="false"                             tabindex="0"
                              data-test-id="nav_my-research"                              >My Research</a>

                                                    
    <ul class="c-menu js-menu
                is-collapsed                c-menubar__menu c-navbar__menu"
                  role="menu"
        aria-label="My Research">

        
            
                                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-submissions"                      href="/login?originalPath=/my/submissions"
                    role="menuitem"
                    tabindex="0">Submissions</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-email-alerts"                      href="/login?originalPath=/my/email-alerts"
                    role="menuitem"
                    tabindex="-1">Content and Tracking Alerts</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-user-details"                      href="/login?originalPath=/my/user-details"
                    role="menuitem"
                    tabindex="-1">My Details</a>

                </li>

                        </ul>
                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="1"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="/login?originalPath=/articles/9-173.html" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                                                    tabindex="-1"
                              data-test-id="nav_sign-in"                              >Sign In</a>

                                            </li>
                
            
        
    </ul>



                            </div>

                                                                                        <div class="_mdl-layout c-navbar__cta u-hide@navbar c-navbar__menu-bar-spacing">
                                    <a class="mdl-button mdl-js-button mdl-button--raised mdl-button--multi-line mdl-button--no-shadow mdl-js-ripple-effect mdl-button--inverted c-navbar__submit" href="/for-authors/publish-your-research"   data-test-id="nav_submit_research_mob"  ><i class="material-icons">file_upload</i>Submit your research</a>
                                </div>
                                                    </div>

                    </div>

                </div>
            
        </div>

    </div>

</nav>
                    </div>

        <div class="content-wrapper o-page__main row ">
            <div id="highlight-area" class="content ">
                





<div id=article-metadata class=hidden> <input type=hidden name=versionId value=24487 /> <input type=hidden id=articleId name=articleId value=22202 /> <input type=hidden id=xmlUrl value="/articles/9-173/v1/xml"/> <input type=hidden id=xmlFileName value="-9-173-v1.xml"> <input type=hidden id=article_uuid value=5a400a02-ecc2-4eae-a670-e08eb50cd63e /> <input type=hidden id=referer value=""/> <input type=hidden id=meta-article-title value="Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity"/> <input type=hidden id=workspace-export-url value="https://sciwheel.com/work/api/import/external?doi=10.12688/f1000research.22202.1"/> <input type=hidden id=versionDoi value="10.12688/f1000research.22202.1"/> <input type=hidden id=usePmcStats value=true /> </div> <main class="o-wrapper p-article__wrapper js-wrapper"> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://f1000research.com/articles/9-173"
  },
  "headline": "Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity",
  "datePublished": "2020-03-10T12:23:28",
  "dateModified": "2020-03-10T12:23:28",
  "author": [
    {
      "@type": "Person",
      "name": "Marco Bilucaglia"
    },    {
      "@type": "Person",
      "name": "Gian Marco Duma"
    },    {
      "@type": "Person",
      "name": "Giovanni Mento"
    },    {
      "@type": "Person",
      "name": "Luca Semenzato"
    },    {
      "@type": "Person",
      "name": "Patrizio E. Tressoldi"
    }  ],
  "publisher": {
    "@type": "Organization",
    "name": "F1000Research",
    "logo": {
      "@type": "ImageObject",
      "url": "https://f1000research.com/img/AMP/F1000Research_image.png",
      "height": 480,
      "width": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://f1000research.com/img/AMP/F1000Research_image.png",
    "height": 1200,
    "width": 150
  },
  "description": "Machine learning approaches have been fruitfully applied to several neurophysiological signal classification problems. Considering the relevance of emotion in human cognition and behaviour, an important application of machine learning has been found in the field of emotion identification based on neurophysiological activity. Nonetheless, there is high variability in results in the literature depending on the neuronal activity measurement, the signal features and the classifier type. The present work aims to provide new methodological insight into machine learning applied to emotion identification based on electrophysiological brain activity. For this reason, we analysed previously recorded EEG activity measured while emotional stimuli, high and low arousal (auditory and visual) were provided to a group of healthy participants. Our target signal to classify was the pre-stimulus onset brain activity. Classification performance of three different classifiers (linear discriminant analysis, support vector machine and k-nearest neighbour) was compared using both spectral and temporal features. Furthermore, we also contrasted the classifiers&rsquo; performance with static and dynamic (time evolving) features. The results show a clear increase in classification accuracy with temporal dynamic features. In particular, the support vector machine classifiers with temporal features showed the best accuracy (63.8 %) in classifying high vs low arousal auditory stimuli."
}
</script> <div class="o-layout o-layout--right-gutter"> <div id=article_secondary-column class="p-article__main o-layout__item u-font-size--legal u-2/3@article not-expanded "> <div class=float-left> <script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
              {
          "@type": "ListItem",
          "position": "1",
          "item": {
            "@id": "https://f1000research.com/",
            "name": "Home"
          }
        },              {
          "@type": "ListItem",
          "position": "2",
          "item": {
            "@id": "https://f1000research.com/browse/articles",
            "name": "Browse"
          }
        },              {
          "@type": "ListItem",
          "position": "3",
          "item": {
            "@id": "https://f1000research.com/articles/9-173",
            "name": "Applying machine learning EEG signal classification to emotion‑related..."
          }
        }          ]
  }
  </script> <div class="breadcrumbs js-breadcrumbs"> <a href="/" class=f1r-standard-link>Home</a> <span class=item_separator></span> <a href="/browse/articles" class=f1r-standard-link>Browse</a> <span class=item_separator></span> Applying machine learning EEG signal classification to emotion‑related... </div> </div> <div class="article-badges-container u-mb--2"> <div class=crossmark-new> <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script> <a data-target=crossmark><img height=30 width=150 src="https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg"/></a> </div> <div id=crossmark-dialog style="display: none;" title=""> <iframe id=crossmark-dialog-frame frameborder=0></iframe> </div> <div class=clearfix></div> </div> <div class=article-interaction-container> <div id=main-article-count-box class=article-count-box> <div class="article-metrics-wrapper metrics-icon-wrapper" data-version-id=24487 data-id=22202 data-downloads="" data-views="" data-scholar="10.12688/f1000research.22202.1" data-recommended="" data-doi="10.12688/f1000research.22202.1" data-f1r-ga-helper="Article Page Metrics (Desktop)"> <span class="metrics-on-browse article-metrics-icon f1r-icon icon-89_metrics"></span> <div class="count-title article-metrics-text">ALL Metrics</div> <div class=js-article-metrics-container></div> </div> <div> <div class=count-delimiter></div> <div title="Total views from F1000Research and PubMed Central"> <div class="count-container view-count js-views-count">-</div> <div class=count-title><span class="count-title-icon count-title-views-icon"></span>Views</div> </div> <div class=download-counts hidden> <div class=count-delimiter></div> <div title="Total downloads from F1000Research and PubMed Central"> <div class="count-container js-downloads-count"></div> <div class=count-title><span class="count-title-icon f1r-icon icon-76_download_file"></span>Downloads</div> </div> </div> </div> </div> <div id=main-article-interaction-box class="article-interaction-box has-control-tab"> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-102_download_pdf"></span> <a href="https://f1000research.com/articles/9-173/v1/pdf?article_uuid=5a400a02-ecc2-4eae-a670-e08eb50cd63e" title="Download PDF" class="button-link download pdf-download-helper" target=_blank>Get PDF</a> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-103_download_xml"></span> <a id=download-xml href="#" class="button-link download" title="Download XML">Get XML</a> </div> </div> <div class="article-interaction-info article-page"> <div class="cite-article-popup-wrapper article-page-interaction-box"> <div class="article-interaction-button cite-article-button" title="Cite this article" data-windowref=cite-article-popup-22202-1> <span class="f1r-icon icon-82_quote"></span> <a href="#" class="button-link cite-article-popup-link" title="Cite Article">Cite</a> </div> <div id=cite-article-popup-22202-1 class="popup-window-wrapper is-hidden"> <div class=cite-popup-background></div> <div class="popup-window top-popup cite-this-article-box research-layout"> <div class="popup-window-title small cite-title">How to cite this article</div> <span id=cite-article-text-22202-1 data-test-id=copy-citation_text> <span class="article-title-and-info in-popup">Bilucaglia M, Duma GM, Mento G <em>et al.</em> Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity [version 1; peer review: awaiting peer review]</span>. <i>F1000Research</i> 2020, <b>9</b>:173 (<a class=new-orange href="https://doi.org/10.12688/f1000research.22202.1" target=_blank>https://doi.org/10.12688/f1000research.22202.1</a>) </span> <div class="popup-window-title small margin-top-20 margin-bottom-20 note"> <strong>NOTE:</strong> it is important to ensure the information in square brackets after the title is included in all citations of this article. </div> <div class=float-right> <button class="secondary no-fill orange-text-and-border margin-right-20 close-cite-popup uppercase">Close</button> <button id=copy-citation-details class="secondary orange copy-cite-article-version uppercase js-clipboard" title="Copy the current citation details to the clipboard." data-clipboard-target="#cite-article-text-22202-1" data-test-id=copy-citation_button>Copy Citation Details</button> </div> </div> </div> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-76_download_file"></span> <a id=export-citation href="#" class="button-link download" title="Export Citation">Export</a> </div> <div class="modal-window-wrapper is-hidden"> <div id=export-citation-popup class="modal-window padding-20"> <div class=modal-window-close-button></div> <div class=modal-window-title>Export Citation</div> <div class=modal-window-row> <div> <input type=radio name=export-citation-option value=WORKSPACE /> <span class=radio-label>Sciwheel</span> </div> <div> <input type=radio name=export-citation-option value=ENDNOTE /> <span class=radio-label>EndNote</span> </div> <div> <input type=radio name=export-citation-option value=REF_MANAGER /> <span class=radio-label>Ref. Manager</span> </div> <div> <input type=radio name=export-citation-option value=BIBTEX /> <span class=radio-label>Bibtex</span> </div> <div> <input type=radio name=export-citation-option value=PROCITE /> <span class=radio-label>ProCite</span> </div> <div> <input type=radio name=export-citation-option value=SENTE /> <span class=radio-label>Sente</span> </div> </div> <div class=modal-window-footer> <button class=general-white-orange-button id=export-citation-submit>EXPORT</button> </div> <div class=default-error style="display: none;">Select a format first</div> </div> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-90_track"></span> <a class="button-link track-article" data-article-id=22202 id=track-article-signin-22202 title="Receive updates on new activity such as publication of new versions, peer reviews or author responses." href="/login?originalPath=/trackArticle/22202?target=/articles/9-173">Track</a> </div> </div> <div class="article-interaction-info article-page"> <div class="article-interaction-button email-article"> <span class="f1r-icon icon-6_email"></span> <a href="#" class=button-link title="Email this article">Email</a> </div> <div class="email-article-version-container small-tooltip _chrome-fix"> <div class=close-icon><span class="f1r-icon icon-3_close_big"></span></div> <script src='https://www.recaptcha.net/recaptcha/api.js'></script> <form class="recommend-version-form research-layout"> <p>All fields are required.</p> <input name=versionId type=hidden value=24487 /> <input name=articleId type=hidden value=22202 /> <input name=senderName class="form-input-field reg-form" value="" type=text placeholder="Your name"/> <input name=senderEmail class="form-input-field reg-form margin-top" value="" type=text placeholder="Your email address"/> <textarea name=recipientEmails class="form-textarea-field ninetynine-percent-wide margin-top no-resize" placeholder="Recipient email address(es) (comma delimited)"></textarea> <input class="form-input-field reg-form margin-top" name=subject type=text value="Interesting article on F1000Research" placeholder=Subject /> <textarea name=message class="form-textarea-field reg-form margin-top no-resize">I thought this article from F1000Research (https://f1000research.com) would be of interest to you.</textarea> <div class="g-recaptcha margin-top" data-sitekey=6LcHqxoUAAAAANP3_0TzpGG6qFvl4DhbUcuRzw7W></div> <input value="" name=captcha type=hidden /> <p>A full article citation will be automatically included.</p> <p><img class="ticker-email-article-details hidden" src="/img/ticker.gif" alt=loading /></p> <button class="secondary orange margin-bottom" data-test-id=version_share_email_send>SEND EMAIL</button> <div class="orange-message margin-bottom is-hidden" data-test-id=version_share_email_message></div> </form> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-34_share"></span> <a href="#" class="button-link last addthis_button share-article" title="Share this article">Share</a> </div> </div> </div> <div id=article-interaction-control-tab class=article-interaction-control-tab> <div id=hide-article-interaction class=article-interaction-control title="Hide Toolbox">&#9644;</div> <div id=show-article-interaction class="article-interaction-control open" title="Show Toolbox">&#10010;</div> </div> </div> <div class="article-header-information article-page"> <div class="f1r-article-mobile article-heading-bar"></div> <div class="article-type article-display">Method Article </div> <div class="article-title-and-info article-view highlighted-article" id=anchor-title> <h1>Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity</h1><span class=other-info> [version 1; peer review: awaiting peer review]</span> </div> <div class=article-subtitle></div> <div class=f1r-article-desk> <div class="authors _mdl-layout"><span class="">Marco Bilucaglia<a href="https://orcid.org/0000-0002-8718-2825" target=_blank id=author-orcid-0><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=author-orcid-0><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0002-8718-2825</div><sup>1</sup>,&nbsp;</span><span class="">Gian Marco Duma<sup>2</sup>,&nbsp;</span><span class="">Giovanni Mento<sup>3</sup>,&nbsp;</span><span class="">Luca Semenzato<sup>3</sup>,&nbsp;</span><span class=""><a href="mailto:patrizio.tressoldi@unipd.it" title="Send email" class="cauthor research-layout"><span class='f1r-icon icon-6_email orange'></span><span>Patrizio E. Tressoldi</span></a><a href="https://orcid.org/0000-0002-6404-0058" target=_blank id=author-orcid-4><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=author-orcid-4><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0002-6404-0058</div><sup>4</sup></span></div> </div> <div class=f1r-article-mobile> <div class="authors _mdl-layout"><span class="">Marco Bilucaglia<a href="http://orcid.org/0000-0002-8718-2825" target=_blank id=mauthor-orcid-0><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=mauthor-orcid-0><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0002-8718-2825</div><sup>1</sup>,&nbsp;</span><span class="">Gian Marco Duma<sup>2</sup>,&nbsp;</span><a href="#" class=article-page-expand-authors>[...]&nbsp;</a><span class=article-page-hidden-authors>Giovanni Mento<sup>3</sup>,&nbsp;</span><span class="">Luca Semenzato<sup>3</sup>,&nbsp;</span><span class=""><a href="mailto:patrizio.tressoldi@unipd.it" title="Send email" class="cauthor research-layout"><span class='f1r-icon icon-6_email orange'></span><span>Patrizio E. Tressoldi</span></a><a href="http://orcid.org/0000-0002-6404-0058" target=_blank id=mauthor-orcid-4><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=mauthor-orcid-4><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0002-6404-0058</div><sup>4</sup></span><span class="author-display-control article-page-expand-authors f1r-icon icon-20_plus_big"></span><span class="author-display-control article-page-hidden-authors f1r-icon icon-12_minus_big"></span></div> </div> <div class=f1r-article-mobile> <div class=article-pubinfo-mobile> PUBLISHED 10 Mar 2020 </div> </div> <span class=Z3988 title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info:ofi/fmt:kev:mtx:journal&amp;rft_id=info:doi/10.12688%2Ff1000research.22202.1"></span> <div class=f1r-article-desk> <div class="contracted-details first"> <a href="#" class="contracted-details-label author-affiliations"><span class=contracted></span>Author details</a> <a href="#" class=section-title>Author details</a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details affiliations is-hidden"> <sup>1</sup> Behavior and BrainLab, Universit&agrave; IULM, Milan, Italy<br/> <sup>2</sup> Department of Developmental and Social Psychology (DPSS), Universit&agrave; degli Studi di Padova, Padova, Italy<br/> <sup>3</sup> Department of General Psychology, Universit&agrave; degli Studi di Padova, Padova, Italy<br/> <sup>4</sup> Science of Consciousness Research Group, Department of General Psychology, Universit&agrave; degli Studi di Padova, Padova, Italy<br/> <p> <div class=margin-bottom> Marco Bilucaglia <br/> <span>Roles: </span> Conceptualization, Data Curation, Formal Analysis, Methodology, Validation, Writing – Original Draft Preparation, Writing – Review & Editing </div> <div class=margin-bottom> Gian Marco Duma <br/> <span>Roles: </span> Conceptualization, Data Curation, Formal Analysis, Methodology, Writing – Original Draft Preparation, Writing – Review & Editing </div> <div class=margin-bottom> Giovanni Mento <br/> <span>Roles: </span> Conceptualization, Supervision, Writing – Review & Editing </div> <div class=margin-bottom> Luca Semenzato <br/> <span>Roles: </span> Software, Validation </div> <div class=margin-bottom> Patrizio E. Tressoldi <br/> <span>Roles: </span> Conceptualization, Supervision, Writing – Review & Editing </div> </p> </div> </div> </div> <div class=f1r-article-mobile> <div class="article-page-section-box margin-bottom-40 research-layout"> <span class=box-title> <span class="f1r-icon icon-85_peer_review"></span> OPEN PEER REVIEW </span> <div class="status-row referee-reports-container"> REVIEWER STATUS <span class=status-icons> <em>AWAITING PEER REVIEW</em> </span> </div> </div> </div> <div class="f1r-article-desk padding-top"> <div class=article-collection-wrapper> <div class="o-media o-media--sm o-media--center"> <div class=o-media__img> <a href="https://f1000research.com/gateways/incf" title="Open Gateway"><img src="https://f1000researchdata.s3.amazonaws.com/thumbnails/b423c7c6-3e4e-448a-b329-89af769f28ee_collection.thumbnail"></a> </div> <div class=o-media__body> <p class="u-mt--0 u-mb--0">This article is included in the <a href="https://f1000research.com/gateways/incf">INCF</a> gateway.</p> </div> </div> </div> </div> </div> <h2 class="article-headings article-page-abstract" id=anchor-abstract> <span class="f1r-article-mobile-inline abstract-heading-border"></span> Abstract </h2> <div class="article-abstract article-page-general-text-mobile research-layout"> <div class="abstract-text is-expanded"> Machine learning approaches have been fruitfully applied to several neurophysiological signal classification problems. Considering the relevance of emotion in human cognition and behaviour, an important application of machine learning has been found in the field of emotion identification based on neurophysiological activity. Nonetheless, there is high variability in results in the literature depending on the neuronal activity measurement, the signal features and the classifier type. The present work aims to provide new methodological insight into machine learning applied to emotion identification based on electrophysiological brain activity. For this reason, we analysed previously recorded EEG activity measured while emotional stimuli, high and low arousal (auditory and visual) were provided to a group of healthy participants. Our target signal to classify was the pre-stimulus onset brain activity. Classification performance of three different classifiers (linear discriminant analysis, support vector machine and k-nearest neighbour) was compared using both spectral and temporal features. Furthermore, we also contrasted the classifiers’ performance with static and dynamic (time evolving) features. The results show a clear increase in classification accuracy with temporal dynamic features. In particular, the support vector machine classifiers with temporal features showed the best accuracy (63.8 %) in classifying high vs low arousal auditory stimuli. </div> <div class=abstract-for-mobile> <div class="margin-top-30 padding-bottom-30 research-layout is-centered"> <button class="primary orange-text white-bg bigger-text abstract-expand-button-mobile with-border show" style="display: none;"> READ ALL <span class="f1r-icon icon-14_more_small orange vmiddle big"></span> </button> <button class="primary orange-text white-bg bigger-text abstract-expand-button-mobile with-border hide"> READ LESS <span class="f1r-icon icon-10_less_small orange vmiddle big"></span> </button> </div> </div> </div> <div class=clearfix></div> <div class="article-context no-divider"> <div class="article-abstract article-page-general-text-mobile research-layout generated-article-body"> <h2 class=main-title>Keywords</h2> <p class="u-mb--0 u-pb--2"> EEG, brain anticipatory activity, machine learning, emotion </p> </div> </div> <div class=article-information> <span class="info-separation padding-bottom"> <div id=corresponding-author-icon class="email-icon float-left"> <span class="f1r-icon icon-6_email orange"></span> <div id=corresponding-author-window class="margin-top-20 popup-window-wrapper is-hidden"> <div class="popup-window corresponding-authors-popup"> <div class=corresponding-author-container> <div class="popup-window-title small">Corresponding Author(s)</div> <div class=authors> Patrizio E. Tressoldi (<a href="mailto:patrizio.tressoldi@unipd.it">patrizio.tressoldi@unipd.it</a>) </div> </div> <div class="margin-top margin-bottom float-left"> <button id=close-popup-window class=general-white-orange-button>Close</button> </div> </div> </div> </div> <span class="icon-text float-left" data-test-id=box-corresponding-author> <b>Corresponding author:</b> Patrizio E. Tressoldi </span> <div class=clearfix></div> </span> <span class="info-separation padding-bottom competing-interests-display"> <span class=competing-interests-title>Competing interests:</span> No competing interests were disclosed. </span> <div class="info-separation padding-bottom grant-information-display"> <span class=grant-information-title>Grant information:</span> The author(s) declared that no grants were involved in supporting this work. </div> <span class="f1r-article-desk info-separation padding-bottom"> <span class="copywrite-icon float-left"> <span class="f1r-icon icon-100_open_access"></span> </span> <span class="icon-text float-left" data-test-id=box-copyright-text> <b>Copyright:</b>&nbsp; © 2020 Bilucaglia M <em>et al</em>. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/" target=_blank data-test-id=box-licence-link>Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. </span> <div class=clearfix></div> </span> <span class="info-separation padding-bottom" data-test-id=box-how-to-cite> <b>How to cite:</b> <span class="article-title-and-info in-article-box"> Bilucaglia M, Duma GM, Mento G <em>et al.</em> Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity [version 1; peer review: awaiting peer review]</span>. <i>F1000Research</i> 2020, <b>9</b>:173 (<a href="https://doi.org/10.12688/f1000research.22202.1" target=_blank>https://doi.org/10.12688/f1000research.22202.1</a>) </span> <span class=info-separation data-test-id=box-first-published><b>First published:</b> 10 Mar 2020, <b>9</b>:173 (<a href="https://doi.org/10.12688/f1000research.22202.1" target=_blank>https://doi.org/10.12688/f1000research.22202.1</a>)</span> <span class=info-separation data-test-id=box-latest-published><b>Latest published:</b> 10 Mar 2020, <b>9</b>:173 (<a href="https://doi.org/10.12688/f1000research.22202.1" target=_blank>https://doi.org/10.12688/f1000research.22202.1</a>)</span> </div> <div class=clearfix></div> <div id=article-context class=article-context> <div id=article1-body class=generated-article-body><h2 class=main-title id=d2010e206>Introduction</h2><p class="" id=d2010e209>In last decades, the vision of the brain has moved from a passive stimuli elaborator to an active reality builder. In other words, the brain is able to extract information from the environment, building up inner models of external reality. These models are used to optimize the behavioural outcome when reacting to upcoming stimuli<sup><a href="#ref-1">1</a>–<a href="#ref-4">4</a></sup>.</p><p class="" id=d2010e219>One of the main theoretical models assumes that the brain, in order to regulate body reaction, runs an internal model of the body in the world, as described by embodied simulation framework<sup><a href="#ref-5">5</a></sup>. A much investigated hypothesis is that the brain functions as a Bayesian filter for incoming sensory input; that is, it activates a sort of prediction based on previous experiences about what to expect from the interaction with the social and natural environment, including emotion<sup><a href="#ref-6">6</a></sup>. In light of this, it is possible to consider emotions, not only as a reaction to the external world, but also as partially shaped by our internal representation of the environment, which help us to anticipate possible scenarios and therefore to regulate our behaviour.</p><p class="" id=d2010e230>The construction model of emotion<sup><a href="#ref-7">7</a></sup> argues that the human being actively builds-up his/her emotions in relation to the everyday life and social context in which they are placed. We actively generate a familiar range of emotions in our reality, based on their usefulness and relevance in our environment. In this scenario, in a familiar context we are able to anticipate which emotions will be probably elicited, depending on our model. As a consequence, the study of the anticipation of/preparation for forthcoming stimuli may represent a precious window for understanding the individual internal model and emotion construction process, resulting in a better understanding of human behaviour.</p><p class="" id=d2010e237>A strategy to study preparatory activity could be related to the experimental paradigm in which cues are provided regarding the forthcoming stimuli, allowing the investigation of the brain activity dedicated to the elaboration of incoming stimuli<sup><a href="#ref-8">8</a>,<a href="#ref-9">9</a></sup>. A cue experiment to predict the emotional valence of the forthcoming stimuli showed that the brain’s anticipatory activation facilitates, for example, successful reappraisal via reduced anticipatory prefrontal cognitive elaboration and better integration of affective information in the paralimbic and subcortical systems<sup><a href="#ref-10">10</a></sup>. Furthermore, preparation for forthcoming emotional stimuli also has relevant implications for clinical psychological conditions, such as mood disorders or anxiety<sup><a href="#ref-11">11</a>,<a href="#ref-12">12</a></sup>.</p><p class="" id=d2010e259>Recently, the study of brain anticipatory activity has been extended to statistically unpredictable stimuli<sup><a href="#ref-13">13</a>–<a href="#ref-15">15</a></sup>, providing experimental hints of specific anticipatory activity before stimuli are randomly presented. Starting from the abovementioned studies, we focused on the extension of brain anticipatory activity to statistically unpredictable emotional stimuli.</p><p class="" id=d2010e269>According to the so called dimensional model, emotion can be defined in terms of three different attributes (or dimensions): valence, arousal and dominance. Valence measures the positiveness (ranging from unpleasant to pleasant), arousal measures the activation level (ranging from boredom to frantic excitement) and dominance measures the controllability (i.e. the sense of control)<sup><a href="#ref-16">16</a></sup>.</p><p class="" id=d2010e276>Emotions can be estimated from various physiological signals<sup><a href="#ref-17">17</a></sup>, such as via skin conductance, electrocardiogram (ECG) and electroencephalogram (EEG). The latter has received a considerable amount of attention in the last decade, introducing several machine learning and signal processing techniques, originally developed in other contexts, such as brain computer interfaces<sup><a href="#ref-18">18</a></sup>. Emotion recognition has been re-drawn as a machine learning problem, where proper EEG related features are used as inputs to specific classifiers.</p><p class="" id=d2010e287>The most common features belong the spectral domain, in the form of spectral powers in delta, theta, alpha and gamma bands<sup><a href="#ref-19">19</a></sup>, as well as power spectral density (PSD) bins<sup><a href="#ref-20">20</a></sup>. The remaining belong to the time domain, in the form of event-related de/synchronizations (ERD/ERS) and event-related potentials (ERP)<sup><a href="#ref-19">19</a></sup>, as well as shape related indices such as the Hjorth parameters and the fractal dimension<sup><a href="#ref-20">20</a></sup>.</p><p class="" id=d2010e306>The most commonly used classifier is the support vector machine (SVM) with the radial basis function (RBF) kernel, followed by the k-nearest neighbour (kNN) and the linear discriminant analysis (LDA)<sup><a href="#ref-19">19</a>,<a href="#ref-20">20</a></sup>. Finally, most of the classifiers are implemented as non-adaptive (i.e. static)<sup><a href="#ref-19">19</a></sup>, in contrast to the dynamic versions that take into account the temporal variability of the features<sup><a href="#ref-21">21</a></sup>.</p><p class="" id=d2010e324>The classification performances are very variable because of the different features and classifiers adopted. The following examples are taken from <a href="#ref-19">19</a> - in particular, from the subset (17 out of 63) of reviewed papers that focused on arousal classification. Using an SVM (RBF kernel) and spectral features (e.g. short-time Fourier transform), Lin and colleagues obtained 94.4% accuracy (i.e. percentage of corrected classification)<sup><a href="#ref-22">22</a></sup>, while using similar spectral features (e.g. PSD) and classifier (SVM with no kernel), Koelstra and colleagues obtained an accuracy of 55.7%<sup><a href="#ref-23">23</a></sup>. Liu and Sourina obtained an accuracy of 76.5% using temporal features (e.g. fractal dimension) with an SVM (no kernel)<sup><a href="#ref-24">24</a></sup>, while Murugappan and Murugappan obtained a an accuracy of 63% using similar temporal features and an SVM with a polynomial kernel<sup><a href="#ref-25">25</a></sup>. Finally, Thammasan and collegues obtained an accuracy of 85.3% using spectral features (e.g. PSD), but with a kNN (with k=3)<sup><a href="#ref-26">26</a></sup>. All the classifiers were static.</p><p class="" id=d2010e352>The purpose of the present work is to provide new methodological advancements on the machine learning classification of emotions, based on the brain anticipatory activity. For this purpose, we compared the performances of tree different classifiers (namely LDA, SVM, kNN) trained using two types of EEG features (namely, spectral and temporal). In addition, each classifier was dynamically trained, to take into account the temporal variability of the features. The results provide useful insights regarding the best classifier-features configuration to better discriminate emotion-related brain anticipatory activity.</p><p class="" id=d2010e355>A more detailed description of the machine learning algorithms is provided as <i>Extended data</i><sup><a href="#ref-27">27</a></sup><i>.</i></p><div class=section><a name=d2010e365 class=n-a></a><h3 class=section-title>Classification performances</h3><p class="" id=d2010e370>In introducing pattern recognition, we underlined that the classifiers are built using a set of previously annotated class-prototypical features for the training set. It is common practice to extract from the training set a subset of annotated features (the test set) and use it to evaluate the performances of the trained classifiers – but not to train it.</p><p class="" id=d2010e373>Since the training set is limited, the specific train/test splitting introduce a bias in both the training and performance evaluation. This can be avoided following the so-called <i>k</i> -fold cross validation scheme. The original training set <i>D</i> is partitioned into <i>k</i> disjoint and equal sized sets, <span class=inline-formula><math display=inline id=M1 overflow=scroll><mrow><mi>D</mi><mo>=</mo><msubsup><mo>∪</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mi>D</mi><mi>k</mi></msub></mrow></math></span>. The classifier is then trained <i>k</i>-times using, each time, as the test set a different partition <i>D<sub>j</sub></i> and as the training set the remaining <span class=inline-formula><math display=inline id=M20 overflow=scroll><mrow><mstyle displaystyle=true><msub><mo>∪</mo><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></msub><mrow><msub><mi>D</mi><mi>i</mi></msub></mrow></mstyle><mo>.</mo></mrow></math></span> Finally, the overall performance is computed as the average over the <i>k</i> single performances<sup><a href="#ref-28">28</a></sup> (pp. 483–485).</p><p class="" id=d2010e450>With the general term performance, we mostly refer to the classification accuracy <i>ACC</i>, defined as the ratio between the number of correctly classified features and the total number of features. Introducing the chance-level accuracy <i>ACC</i><sub>0</sub> as the ratio between the number of features for each class (i.e. how balanced is the training set), we can additionally define as performance the Kappa statistic: <i>κ</i> = (<i>ACC</i> – <i>ACC</i><sub>0</sub>)/<i>ACC</i><sub>0</sub><sup><a href="#ref-29">29</a></sup>.</p><p class="" id=d2010e481>Compared to <i>ACC</i>, <i>κ</i> is a more robust performance measure, since it is normalized by the class unbalances. Another solution to take into account the class unbalances, is to compare (using for example a t-test) the <i>k</i> cross-validated accuracies against <i>k</i> random accuracies, obtained from a random classifier<sup><a href="#ref-29">29</a>–<a href="#ref-35">35</a></sup>.</p></div><div class=section><a name=d2010e504 class=n-a></a><h3 class=section-title>Dynamic classifiers</h3><p class="" id=d2010e509>To classify a time-varying signal (i.e. to perform a dynamic classification), an ordered sequence of features <span class=inline-formula><math display=inline id=M2 overflow=scroll><mrow><msubsup><mrow><mrow><mtext>{</mtext><mrow><msub><mo mathvariant=bold>x</mo><mi>i</mi></msub></mrow><mtext>}</mtext></mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></math></span> (i.e. temporal features), corresponding to <i>N</i> adjacent temporal windows, is extracted. The temporal features are fed into either “dynamic” classifiers, such as the Hidden Markow Model (HMM)<sup><a href="#ref-21">21</a></sup>, or an ordered sequence of “static” classifiers <span class=inline-formula><math display=inline id=M3 overflow=scroll><mrow><msubsup><mrow><mrow><mtext>{</mtext><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><mtext>}</mtext></mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></math></span> <sup><a href="#ref-36">36</a>–<a href="#ref-39">39</a></sup>. The former fully takes into account the signal’s temporal variability, since it uses the entire sequence during the training phase. The latter train each static classifier <i>f<sub>i</sub></i>, using only the corresponding features <b>x</b><sub><i>i</i></sub>, but provides an ordered sequence of accuracies <span class=inline-formula><math display=inline id=M4 overflow=scroll><mrow><msubsup><mrow><mrow><mo>{</mo><mrow><mi>A</mi><mi>C</mi><msub><mi>C</mi><mi>i</mi></msub></mrow><mo>}</mo></mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></math></span>. where each <i>ACC<sub>i</sub></i> corresponds to <i>f<sub>i</sub></i>.</p></div><div class=section><a name=d2010e634 class=n-a></a><h3 class=section-title>Feature selection</h3><p class="" id=d2010e639>As stated in the previous sections, the curse of dimensionality arises when the number of available training features is small compared to the feature dimension <i>m</i>. In such situations, the parameter estimation becomes problematic (see for example the problem of the singularity of the estimated covariance matrix described in the <i>LDA</i> sub-section) and the trained classifier usually underperforms.</p><p class="" id=d2010e648>As a rule of thumb, the number of training features <i>N</i> should be an exponential function of the dimensionality (e.g. <i>N</i> = 10<sup><i>m</i></sup>), with the ratio growing with the complexity of the classifiers<sup><a href="#ref-40">40</a></sup>. By fixing the feature dimension <i>m</i>, linear classifiers require, for example, a less numerous training set. Additionally, even with an adequate training set, feature dimensionality impacts on both the training and classification speed. In fact, as stated in the <i>SVM</i> sub-section, linear classification requires <i>O</i>(<i>m</i>) multiplications and sums to compute each scalar product. Reducing the feature dimensionality by means of so called feature selection algorithms, a classifier can be made more robust (i.e. less sensitive to the curse of dimensionality) and efficient (in terms of computational speed).</p><p class="" id=d2010e678>Feature selection can be broadly described as a mapping function <span class=inline-formula><math display=inline id=M5 overflow=scroll><mrow><mi>s</mi><mo>:</mo><mspace width="0.2em"></mspace><msup><mi>ℝ</mi><mi>m</mi></msup><mo>→</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow></math></span> such as:</p><p class="" id=d2010e702><a name=e1 class=n-a></a><div class="disp-formula panel"><math display=block id=math1 overflow=scroll><mrow><mrow><mi>s</mi><mo stretchy=false>(</mo><mtext mathvariant=bold>x</mtext><mo stretchy=false>)</mo><mspace width="0.2em"></mspace><mo>=</mo><mspace width="0.2em"></mspace><msup><mrow><mo stretchy=false>(</mo><msub><mi>x</mi><mrow><msub><mi>s</mi><mn>1</mn></msub></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><msub><mi>s</mi><mn>2</mn></msub></mrow></msub><mo>,</mo><mspace width="0.2em"></mspace><mn>...</mn><mspace width="0.2em"></mspace><mo>,</mo><mspace width="0.2em"></mspace><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>n</mi></msub></mrow></msub><mo stretchy=false>)</mo></mrow><mi>T</mi></msup></mrow><mspace width=25em></mspace><mo stretchy=false>(</mo><mn>13</mn><mo stretchy=false>)</mo></mrow></math></div> </p><p class="" id=d2010e776>where <i>n</i> &lt; <i>m</i> and {<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, ... , <i>s</i><sub>n</sub>} ⊂ {1, 2, ... , <i>m</i>}. In other words, a feature selection algorithm performs a projection of the original feature vector onto a lower dimensional subspace defined by a subset of scalar features. The best subspace, as selected among all the possible 2<sup><i>m</i></sup>, should not significantly decrease the classification performances, both globally (i.e. how features are classified overall) and locally (i.e. how the single feature is classified)<sup><a href="#ref-41">41</a></sup></p><p class="" id=d2010e811>Feature selection algorithms can be broadly grouped according to the following criteria<sup><a href="#ref-42">42</a></sup>:</p><p class="" id=d2010e818>1. Label information. Supervised algorithms take into account the class information, while unsupervised algorithms do not, in assigning the training features as belonging to the same class.</p><p class="" id=d2010e821>2. Search strategy. Filter algorithms (also known as classifier-independent) are based on a two-step “ranking and selecting” criterium: scalar features are first ranked according to a proper criterion; then only the “best” ones are selected. Wrapper methods (also known as classifier dependent methods) use the selected classifier, following an “ad hoc” approach: the selected scalar features are those that give the best classification performance</p><p class="" id=d2010e824>An example of a supervised filter algorithm is the biserial correlation coefficient. Given a training set <i>D</i> composed by <i>N</i><sub>+</sub> features belonging to the class +1 and <i>N</i><sub>–</sub> features belonging to the class –1, the biserial correlation coefficient for the <i>k</i>-th scalar feature <i>x<sub>k</sub></i> is given by <a href="#ref-43">43</a>:</p><p class="" id=d2010e852><a name=e2 class=n-a></a><div class="disp-formula panel"><math display=block id=math2 overflow=scroll><mrow><msubsup><mi>r</mi><mi>k</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mrow><msqrt><mrow><msub><mi>N</mi><mo>+</mo></msub><msub><mi>N</mi><mo>−</mo></msub></mrow></msqrt></mrow><mrow><msub><mi>N</mi><mo>+</mo></msub><mspace width="0.2em"></mspace><mo>+</mo><mspace width="0.2em"></mspace><msub><mi>N</mi><mo>−</mo></msub></mrow></mfrac><mfrac><mrow><mi>m</mi><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mi>k</mi><mo>+</mo></msubsup></mrow><mo>)</mo></mrow><mo>−</mo><mi>m</mi><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mi>k</mi><mo>−</mo></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><mi>s</mi><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><mo>)</mo></mrow></mrow></mfrac></mrow><mspace width=25em></mspace><mo stretchy=false>(</mo><mn>13</mn><mo stretchy=false>)</mo></math></div></p><p class="" id=d2010e951>where <i>m</i>(·), <i>s</i>(·) are the sample mean and sample standard deviation operators, respectively, and <span class=inline-formula><math display=inline id=M6 overflow=scroll><mrow><msubsup><mi>x</mi><mi>k</mi><mo>+</mo></msubsup></mrow></math></span>, <span class=inline-formula><math display=inline id=M7 overflow=scroll><mrow><msubsup><mi>x</mi><mi>k</mi><mo>-</mo></msubsup></mrow></math></span> are the subset of <i>x<sub>k</sub></i> belonging to the classes +1 and –1, respectively. The total feature score is obtained by summing the <i>m</i> coefficients of each scalar feature <i>x<sub>k</sub></i>. Once the scores <span class=inline-formula><math display=inline id=M8 overflow=scroll><mrow><msubsup><mi>r</mi><mi>k</mi><mo>2</mo></msubsup></mrow></math></span> are sorted in descending order, the feature selection is made simply by selecting the first scalar features whose summing score get a percentage (e.g. 95%) of the total feature score.</p></div></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d2010e1011>Methods</h2><div class=section><a name=d2010e1014 class=n-a></a><h3 class=section-title>Ethical statement</h3><p class="" id=d2010e1019>The data of the present study were obtained in the experiment described in <a href="#ref-37">37</a>, which was approved by the Ethical Committee of the Department of General Psychology, University of Padova (No. 2278). Before taking part in the experiment, each subject gave his/her informed consent in writing after having read a description of the experiment. In line with department policies, this re-analysis of an original study approved by the ethics committee did not require new ethical approval.</p></div><div class=section><a name=d2010e1026 class=n-a></a><h3 class=section-title>Stimuli and experimental paradigm</h3><p class="" id=d2010e1031>In the present study, we reanalysed the EEG data<sup><a href="#ref-27">27</a></sup> of the experiment described in <a href="#ref-37">37</a>, applying an original static and dynamic features selection and classification by using the three different algorithms explained above.</p><p class="" id=d2010e1041>A more detailed description of the experimental design is available in the original study. Here we describe only the main characteristics.</p><p class="" id=d2010e1044>Two sensory categories of stimuli (i.e. visual and auditory), were extracted according to their arousal value from two standardized international archives. Visual stimuli consisted of pictures of 28 faces, 14 neutral faces and 14 fearful faces were extracted from the NIMSTIM archive<sup><a href="#ref-44">44</a></sup>, whereas auditory stimuli consisted of 28 sounds, and 14 low- and 14 high-arousal sounds were chosen from the International Affective Digitized Sounds (IADS) archive<sup><a href="#ref-45">45</a></sup>.</p><p class="" id=d2010e1055>To all 28 adult healthy participants, two different experimental tasks, which were delivered in separate blocks were presented. The two tasks are described in <a href="#f1">Figure 1</a>, which illustrates the sequence of events and the temporal trial structure relative to the passive (top) and the active (bottom) tasks. Within each task, the stimuli were randomly presented and equally distributed according to either sensory category (faces or sounds) and arousal level (high or low). Full details of these tasks have been described previously in <a href="#ref-37">37</a>.</p><a name=f1 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure1.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure1.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure1.gif"></a><div class=caption><h3>Figure 1. Experimental tasks.</h3></div></div></div><div class=section><a name=d2010e1080 class=n-a></a><h3 class=section-title>EEG recording</h3><p class="" id=d2010e1085>During the entire experiment, the EEG signal was continuously recorded using a Geodesic high density EEG system (EGI GES-300) through a pre-cabled 128-channel HydroCel Geodesic Sensor Net (HCGSN-128) referenced to the vertex (CZ), with a sampling rate of 500 Hz. The impedance was kept below 60kΩ for each sensor. To reduce the presence of EOG artefacts, subjects were instructed to limit both eye blinks and eye movements, as much as possible.</p></div><div class=section><a name=d2010e1089 class=n-a></a><h3 class=section-title>EEG preprocessing</h3><p class="" id=d2010e1094>The continuous EEG signal was off-line band-pass filtered (0.1–45Hz) using a Hamming windowed sinc finite impulse response (FIR) filter (order = 16500) and then downsampled at 250 Hz. The EEG was epoched starting from 200 ms before the cue onset and ending at the stimulus onset. The initial epochs were 1300 ms long from the cue onset, including 300 ms of cue/fixation cross presentation and 1000 ms of interstimulus interval (ISI).</p><p class="" id=d2010e1097>All epochs were visually inspected to remove bad channels and rare artefacts. Artefact-reduced data were then subjected to independent component analysis (ICA)<sup><a href="#ref-45">45</a></sup>. All independent components were visually inspected, and those that related to eye blinks, eye movements, and muscle artefacts, according to their morphology and scalp distribution, were discarded. The remaining components were back-projected to the original electrode space to obtain cleaner EEG epochs.</p><p class="" id=d2010e1104>The remaining ICA-cleaned epochs that still contained excessive noise or drift (±100 μV at any electrode) were rejected and the removed bad channels were reconstructed. Data were then re-referenced to the common average reference (CAR) and the epochs were baseline-corrected by subtracting the mean signal amplitude in the pre-stimulus interval. From the original 1300 ms long epochs, final epochs were obtained only from the 1000 ms long ISI.</p></div><div class=section><a name=d2010e1109 class=n-a></a><h3 class=section-title>Static spectral features</h3><p class="" id=d2010e1114>From each epoch and each channel <i>k</i>, the PSD was estimated by a Welch’s periodogram using 250 points long Hamming’s windows with 50% overlapping. PSD was first log transformed to compensate the skewness of power values<sup><a href="#ref-46">46</a></sup>, then the spectral bins corresponding to alpha, beta and theta bands – defined as 13~30<i>Hz</i>, 6~13<i>Hz</i> and 4~6<i>Hz</i>, respectively<sup><a href="#ref-47">47</a></sup> – were summed together. Finally, alpha, beta and theta total powers were computed as:</p><p class="" id=d2010e1138><a name=e3 class=n-a></a><div class="disp-formula panel"><math display=block id=math3 overflow=scroll><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mi>k</mi></msubsup><mo>=</mo><mstyle displaystyle=true><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo stretchy=false>[</mo><mn>13</mn><mo>;</mo><mn>30</mn><mo stretchy=false>]</mo></mrow></munder><mrow><mi>P</mi><mi>S</mi><msup><mi>D</mi><mi>k</mi></msup><mo stretchy=false>(</mo><mi>i</mi><mo stretchy=false>)</mo></mrow><mspace width="23.4em"></mspace></mstyle></mrow></math></div></p><p class="" id=d2010e1194><a name=e4 class=n-a></a><div class="disp-formula panel"><math display=block id=math4 overflow=scroll><mrow><msubsup><mi>α</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mi>k</mi></msubsup><mo>=</mo><mstyle displaystyle=true><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo stretchy=false>[</mo><mn>6</mn><mo>;</mo><mn>13</mn><mo stretchy=false>]</mo></mrow></munder><mrow><mi>P</mi><mi>S</mi><msup><mi>D</mi><mi>k</mi></msup><mo stretchy=false>(</mo><mi>i</mi><mo stretchy=false>)</mo></mrow></mstyle><mspace width=22em></mspace><mo stretchy=false>(</mo><mn>14</mn><mo stretchy=false>)</mo></mrow></math></div></p><p class="" id=d2010e1257><a name=e5 class=n-a></a><div class="disp-formula panel"><math display=block id=math5 overflow=scroll><mrow><msubsup><mi>θ</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mi>k</mi></msubsup><mo>=</mo><mstyle displaystyle=true><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo stretchy=false>[</mo><mn>4</mn><mo>;</mo><mn>6</mn><mo stretchy=false>]</mo></mrow></munder><mrow><mi>P</mi><mi>S</mi><msup><mi>D</mi><mi>k</mi></msup><mo stretchy=false>(</mo><mi>i</mi><mo stretchy=false>)</mo></mrow><mspace width="24.4em"></mspace></mstyle></mrow></math></div></p><p class="" id=d2010e1314>As a measure of emotional arousal, we computed the ratio between beta and alpha total powers <span class=inline-formula><math display=inline id=M9 overflow=scroll><mrow><mrow><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mi>k</mi></msubsup></mrow><mo>/</mo><mrow><msubsup><mi>α</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mi>k</mi></msubsup></mrow></mrow><mspace width="0.2em"></mspace><mspace width="0.2em"></mspace><mspace width="0.2em"></mspace></mrow></math></span><sup><a href="#ref-48">48</a></sup>, while to measure cognitive arousal, we computed the ratio between beta and theta total powers <span class=inline-formula><math display=inline id=M10 overflow=scroll><mrow><mrow><mrow><msubsup><mi>θ</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mi>k</mi></msubsup></mrow><mo>/</mo><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mi>k</mi></msubsup></mrow></mrow><mspace width="0.2em"></mspace><mspace width="0.2em"></mspace><mspace width="0.2em"></mspace></mrow></math></span><sup><a href="#ref-49">49</a></sup>.</p><p class="" id=d2010e1396>For each epoch, the feature (with a dimensionality of 256) was obtained, concatenating the beta-over-alpha and beta-over-theta ratio of all the channels:</p><p class="" id=d2010e1399><a name=e6 class=n-a></a><div class="disp-formula panel"><math display=block id=math6 overflow=scroll><mrow><mi>v</mi><mo>≐</mo><mrow><mo>[</mo><mrow><mfrac><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>1</mn></msubsup></mrow><mrow><msubsup><mi>α</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>1</mn></msubsup></mrow></mfrac><mo>,</mo><mfrac><mrow><msubsup><mi>θ</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>1</mn></msubsup></mrow><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>1</mn></msubsup></mrow></mfrac><mo>,</mo><mfrac><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>2</mn></msubsup></mrow><mrow><msubsup><mi>α</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>2</mn></msubsup></mrow></mfrac><mo>,</mo><mfrac><mrow><msubsup><mi>θ</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>2</mn></msubsup></mrow><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mn>2</mn></msubsup></mrow></mfrac><mo>,</mo><mn>...</mn><mo>,</mo><mfrac><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mrow><mn>128</mn></mrow></msubsup></mrow><mrow><msubsup><mi>α</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mrow><mn>128</mn></mrow></msubsup></mrow></mfrac><mo>,</mo><mfrac><mrow><msubsup><mi>θ</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mrow><mn>128</mn></mrow></msubsup></mrow><mrow><msubsup><mi>β</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow><mrow><mn>128</mn></mrow></msubsup></mrow></mfrac></mrow><mo>]</mo></mrow><mspace width=16em></mspace><mo stretchy=false>(</mo><mn>15</mn><mo stretchy=false>)</mo></mrow></math></div></p></div><div class=section><a name=d2010e1605 class=n-a></a><h3 class=section-title>Static temporal features</h3><p class="" id=d2010e1610>It has been previously shown that arousal level (high or low) can be estimated from the contingent negative variation potentials<sup><a href="#ref-37">37</a></sup>. The feature extraction procedure, therefore, follows the classical approach for event-related potentials<sup><a href="#ref-50">50</a></sup>. Each epoch from each channel was first band pass filtered (0.05~10<i>Hz</i>) using a zero-phase 2<sup>nd</sup> order Butterworth filter and decimated to a sample frequency of 20<i>Hz</i>. EEG signal was thus normalized (i.e. z-scored) according to the temporal mean and the temporal standard deviation:</p><p class="" id=d2010e1630><a name=e7 class=n-a></a><div class="disp-formula panel"><math display=block id=math7 overflow=scroll><mrow><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy=false>)</mo><mo>=</mo><mo stretchy=false>(</mo><msub><mover accent=true><mi>x</mi><mo>˜</mo></mover><mi>i</mi></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy=false>)</mo><mo>−</mo><msub><mi>m</mi><mi>i</mi></msub><mo stretchy=false>)</mo><mo>/</mo><msub><mi>s</mi><mi>i</mi></msub></mrow></math></div></p><p class="" id=d2010e1687>where <span class=inline-formula><math display=inline id=M11 overflow=scroll><mrow><msub><mover accent=true><mi>x</mi><mo>˜</mo></mover><mi>i</mi></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy=false>)</mo></mrow></math></span> is the raw signal from i-th channel at time point <i>t<sub>k</sub></i>, and <i>m<sub>i</sub></i> and <i>s<sub>i</sub></i> are, respectively, the temporal mean and the temporal standard deviation of the i-th channel. For each epoch, the feature (with a dimensionality of 2560) was obtained, concatenating all normalized signal from each channel:</p><p class="" id=d2010e1726><a name=e8 class=n-a></a><div class="disp-formula panel"><math display=block id=math8 overflow=scroll><mrow><mi>v</mi><mo>≐</mo><mrow><mo>[</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=false>)</mo><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy=false>)</mo><mo>,</mo><mn>...</mn><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mrow><mn>20</mn></mrow></msub><mo stretchy=false>)</mo><mo>,</mo><mn>...</mn><mo>,</mo><msub><mi>x</mi><mrow><mn>128</mn></mrow></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=false>)</mo><mo>,</mo><msub><mi>x</mi><mrow><mn>128</mn></mrow></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy=false>)</mo><mo>,</mo><mn>...</mn><mo>,</mo><msub><mi>x</mi><mrow><mn>128</mn></mrow></msub><mo stretchy=false>(</mo><msub><mi>t</mi><mrow><mn>20</mn></mrow></msub><mo stretchy=false>)</mo></mrow><mo>]</mo></mrow><mspace width=8em></mspace><mo stretchy=false>(</mo><mn>16</mn><mo stretchy=false>)</mo></mrow></math></div></p></div><div class=section><a name=d2010e1864 class=n-a></a><h3 class=section-title>Dynamic features</h3><p class="" id=d2010e1869>Each epoch was partitioned into 125 temporal segments, 500 ms long and shifted by 1/250 s (one sample). Within each time segment, we extracted the dynamic spectral and temporal features, following the same approaches described in <i>Static spectral features</i> and <i>Static temporal features</i> sub-sections, respectively. Dynamic temporal features had a dimensionality of 1280, corresponding to 0.5 × 20 = 10 samples per channel. Dynamic spectral features had the same dimensionality as their static counterparts (256), but the Welch’s periodogram was computed using a 16 points long Hamming’s window (zero-padded to 250 points) with 50% overlapping.</p></div><div class=section><a name=d2010e1879 class=n-a></a><h3 class=section-title>Feature reduction and classification</h3><p class="" id=d2010e1884>The extracted features (both static and dynamic) were grouped according to the stimulus type (sound or image) and the task (active or passive), in order to classify the group-related arousal level (high or low). A total of four binary classification problems (high arousal vs low arousal) were performed: active image (Ac_Im), active sound (Ac_So), passive image (Ps_Im) and passive sound (Ps_So).</p><p class="" id=d2010e1887>Static features were reduced by means of the biserial correlation coefficient <i>r</i><sup>2</sup> with the threshold set at 90% of the total feature score. In order to identify the discriminative power of each EEG channel, a series of scalp plots (one for each feature type and each group) of the coefficients were drawn. Since each channel is associated with <i>N</i> &gt; 1 features (as well as <i>N</i> <i>r</i><sup>2</sup> coefficients), the coefficients (one coefficient for each channel) are calculated as a mean value. In other words, spectral and temporal features had two and 20 scalar features, respectively, for each EEG channel. To compute their scalp plots, we averaged 2 and 20 <i>r</i><sup>2</sup> coefficients of each channel. To enhance the visualization of the plots, the coefficients were finally normalized to the total score and expressed as a percentage.</p><p class="" id=d2010e1912>Each classification problem was addressed by the mean of three classifiers: LDA with pseudo-inverse covariance matrix; soft-margin SVM with penalty parameter <i>C</i> = 1 and RBF kernel; and kNN with Euclidean distance and k=1. Additionally, a random classifier, giving a uniform pseudo-random class (Pr{HA} = Pr{LA} = 0.5), served as a benchmark<sup><a href="#ref-29">29</a></sup>. The accuracy of the classifiers was measured, repeating 10 times for a 10-fold cross-validation scheme. The feature selection was computed within each cross-validation step, to avoid overfitting and reduce biased results<sup><a href="#ref-43">43</a></sup>.</p><p class="" id=d2010e1926>For each group (Ac_Im, Ac_So, Ps_Im, Ps_So) and each feature type (static spectral, static temporal), the classification produced a 10 × 4 matrix containing the mean accuracies (one for each of the 10-fold cross-validation repetitions) of each classifier.</p><p class="" id=d2010e1930>Dynamic features were reduced and classified similarly to the static ones. For each temporal segment, the associated features were reduced by means of the biserial correlation coefficient (threshold at 90%) and the classifiers (SVM, kNN, LDA and random) were evaluated using a 10-fold cross-validation scheme – repeated 10 times.</p><p class="" id=d2010e1933>For each group, each feature type (dynamic spectral, dynamic temporal), each temporal segment and each classifier, the classification produced 10 sequences of mean accuracies <span class=inline-formula><math display=inline id=M16 overflow=scroll><mrow><msubsup><mrow><mrow><mo>{</mo><mrow><mi>A</mi><mi>C</mi><msub><mi>C</mi><mi>i</mi></msub></mrow><mo>}</mo></mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>125</mn></msubsup></mrow></math></span> – one for each repetition of the 10-fold cross-validation scheme.</p></div><div class=section><a name=d2010e1967 class=n-a></a><h3 class=section-title>Data analysis</h3><p class="" id=d2010e1972>The syntax in MATLAB used for all analyses is available on GitHub along with the instructions on how to use it (see <i>Software availability</i>)<sup><a href="#ref-51">51</a></sup>. The software can also be used with the open source program Octave.</p></div><div class=section><a name=d2010e1983 class=n-a></a><h3 class=section-title>Statistical analysis</h3><p class="" id=d2010e1988>The results of the static classifications were compared against the benchmark classifier by means of a two-sample t-test (right tail).</p><p class="" id=d2010e1991>The results of dynamic classifications (i.e. based on dynamic spectral or dynamic temporal features) were compared following a segment-by-segment approach. For each group, the accuracy sequences of the dynamic classifiers (SVM, kNN and LDA) were compared with the benchmark accuracy sequence. Each sample <span class=inline-formula><math display=inline id=M12 overflow=scroll><mrow><mi>A</mi><mi>C</mi><msubsup><mi>C</mi><mi>i</mi><mi>k</mi></msubsup></mrow></math></span>, with <i>k</i> = {SVM, kNN, LDA}, was tested against <span class=inline-formula><math display=inline id=M13 overflow=scroll><mrow><mi>A</mi><mi>C</mi><msubsup><mi>C</mi><mi>i</mi><mrow><mtext>Random</mtext></mrow></msubsup></mrow></math></span> by means of two-sample t-tests (right tail). The corresponding p-value sequences <span class=inline-formula><math display=inline id=M14 overflow=scroll><mrow><msubsup><mrow><mrow><mo>{</mo><mrow><msubsup><mi>p</mi><mi>i</mi><mi>k</mi></msubsup></mrow><mo>}</mo></mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>125</mn></mrow></msubsup></mrow></math></span> were Bonferroni-Holm corrected for multiple comparisons. Finally, the best accuracy point was detected as the left extreme of the temporal window corresponding to the highest significant accuracy.</p></div></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d2010e2064>Results</h2><div class=section><a name=d2010e2067 class=n-a></a><h3 class=section-title>Static features</h3><p class="" id=d2010e2072>In <a href="#f2">Figure 2</a> and <a href="#f3">Figure 3</a>, the scalp distributions of <i>r</i><sup>2</sup> coefficients for each binary static classification problem, grouped for feature (spectral, temporal) and groups (Ps_Im, Ps_So, Ac_Im, Ac_So), are shown.</p><a name=f2 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure2.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure2.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure2.gif"></a><div class=caption><h3>Figure 2. Spectral features.</h3><p id=d2010e2096>Scalp distribution of the <i>r</i><sup>2</sup> coefficients (normalized to the total score and expressed as percentage) grouped for tasks and stimulus type. (<b>a</b>) Active task: left Image, right Sound; (<b>b</b>) Passive task: left Image, right Sound.</p></div></div><a name=f3 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure3.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure3.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure3.gif"></a><div class=caption><h3>Figure 3. Temporal features.</h3><p id=d2010e2124>Scalp distribution of the <i>r</i><sup>2</sup> coefficients (normalized to the total score and expressed as percentage), grouped for tasks and stimulus type. (<b>a</b>) Active task: left Image, right Sound; (<b>b</b>) Passive task: left Image, right Sound.</p></div></div><p class="" id=d2010e2142>The temporal feature gave the most consistent topographical pattern, showing that the regions that best differentiate between high vs low stimuli (auditory and visual) were located over the central-parietal electrodes, whereas a more diffuse pattern in the scalp topography emerged for the spectral features.</p><p class="" id=d2010e2146>In <a href="#f4">Figure 4</a> and <a href="#f5">Figure 5</a>, box plots of the accuracies of static temporal and spectral classifications, grouped for condition, are shown. Note that SVM accuracies (the 2<sup>nd</sup> boxplot from the left) are always shown as lines because the accuracies were constant within each cross-validation step (see also <a href="#T1">Table 1</a>, <a href="#T2">Table 2</a> and <a href="#T3">Table 3</a>).</p><a name=f4 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure4.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure4.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure4.gif"></a><div class=caption><h3>Figure 4. Box-plots of the accuracies of the static spectral classifications.</h3><p id=d2010e2178>From left: Active Image (Ac_Im), Active Sound (Ac_So), Passive Image (Ps_Im) and Passive Sound (Ps_So).</p></div></div><a name=f5 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure5.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure5.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure5.gif"></a><div class=caption><h3>Figure 5. Box-plots of the accuracies of the static temporal classifications.</h3><p id=d2010e2195>From left: Active Image (Ac_Im), Active Sound (Ac_So), Passive Image (Ps_Im) and Passive Sound (Ps_So).</p></div></div><a name=T1 class=n-a></a><div class="table-wrap panel clearfix"><div class=caption><h3>Table 1. Static features.</h3><p id=d2010e2212>Ordered accuracies grouped for classifier, feature and group.</p></div><table xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class=article-table frame=below><a name=d2010e2216 class=n-a></a><thead><a name=d2010e2218 class=n-a></a><tr><a name=d2010e2220 class=n-a></a><th align=center colspan=1 rowspan=1><a name=d2010e2222 class=n-a></a>Classifier</th><th align=center colspan=1 rowspan=1><a name=d2010e2225 class=n-a></a>Accuracy</th><th align=center colspan=1 rowspan=1><a name=d2010e2228 class=n-a></a>Feature</th><th align=center colspan=1 rowspan=1><a name=d2010e2231 class=n-a></a>Group</th></tr></thead><tbody><a name=d2010e2236 class=n-a></a><tr><a name=d2010e2238 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2240 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e2243 class=n-a></a>51.80%</td><td align=center colspan=1 rowspan=1><a name=d2010e2246 class=n-a></a>Spectral</td><td align=center colspan=1 rowspan=1><a name=d2010e2249 class=n-a></a>Ps_Im</td></tr><tr><a name=d2010e2253 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2255 class=n-a></a>LDA</td><td align=center colspan=1 rowspan=1><a name=d2010e2258 class=n-a></a>51.40%</td><td align=center colspan=1 rowspan=1><a name=d2010e2261 class=n-a></a>Spectral</td><td align=center colspan=1 rowspan=1><a name=d2010e2264 class=n-a></a>Ps_Im</td></tr><tr><a name=d2010e2268 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2270 class=n-a></a>kNN</td><td align=center colspan=1 rowspan=1><a name=d2010e2273 class=n-a></a>51%</td><td align=center colspan=1 rowspan=1><a name=d2010e2276 class=n-a></a>Temporal</td><td align=center colspan=1 rowspan=1><a name=d2010e2279 class=n-a></a>Ac_So</td></tr><tr><a name=d2010e2283 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2285 class=n-a></a>kNN</td><td align=center colspan=1 rowspan=1><a name=d2010e2288 class=n-a></a>50.90%</td><td align=center colspan=1 rowspan=1><a name=d2010e2291 class=n-a></a>Spectral</td><td align=center colspan=1 rowspan=1><a name=d2010e2294 class=n-a></a>Ac_So</td></tr><tr><a name=d2010e2298 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2300 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e2303 class=n-a></a>50.90%</td><td align=center colspan=1 rowspan=1><a name=d2010e2306 class=n-a></a>Spectral</td><td align=center colspan=1 rowspan=1><a name=d2010e2309 class=n-a></a>Ac_So</td></tr><tr><a name=d2010e2314 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2316 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e2319 class=n-a></a>50.90%</td><td align=center colspan=1 rowspan=1><a name=d2010e2322 class=n-a></a>Temporal</td><td align=center colspan=1 rowspan=1><a name=d2010e2325 class=n-a></a>Ac_So</td></tr><tr><a name=d2010e2329 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2331 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e2334 class=n-a></a>50.40%</td><td align=center colspan=1 rowspan=1><a name=d2010e2337 class=n-a></a>Temporal</td><td align=center colspan=1 rowspan=1><a name=d2010e2340 class=n-a></a>Ps_So</td></tr></tbody></table><div class=table-wrap-foot><div class=footnote><a name=d2010e2348 class=n-a></a><p id=d2010e2350> SVM, support vector machine; LDA, linear discriminant analysis; kNN, k-nearest neighbour.</p></div></div></div><a name=T2 class=n-a></a><div class="table-wrap panel clearfix"><div class=caption><h3>Table 2. Mean (M) and standard deviations (SD) of the accuracies of the static spectral classifications.</h3><p id=d2010e2366>Active Image (Ac_Im), Active Sound (Ac_So), Passive Image (Ps_Im) and Passive Sound (Ps_So).</p></div><table xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class=article-table frame=below><a name=d2010e2370 class=n-a></a><thead><a name=d2010e2372 class=n-a></a><tr><a name=d2010e2374 class=n-a></a><th align=center colspan=1 rowspan=1><a name=d2010e2376 class=n-a></a>Group</th><th align=center colspan=1 rowspan=1><a name=d2010e2379 class=n-a></a>LDA</th><th align=center colspan=1 rowspan=1><a name=d2010e2382 class=n-a></a>SVM</th><th align=center colspan=1 rowspan=1><a name=d2010e2385 class=n-a></a>kNN</th><th align=center colspan=1 rowspan=1><a name=d2010e2388 class=n-a></a>Random</th></tr></thead><tbody><a name=d2010e2393 class=n-a></a><tr><a name=d2010e2395 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2397 class=n-a></a>Ac_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e2400 class=n-a></a>M=0.496, SD=0.007</td><td align=center colspan=1 rowspan=1><a name=d2010e2403 class=n-a></a>M=0.510, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2406 class=n-a></a>M=0.500, SD=0.010</td><td align=center colspan=1 rowspan=1><a name=d2010e2409 class=n-a></a>M=0.505, SD=0.011</td></tr><tr><a name=d2010e2413 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2415 class=n-a></a>Ac_So</td><td align=center colspan=1 rowspan=1><a name=d2010e2418 class=n-a></a>M=0.492, SD=0.004</td><td align=center colspan=1 rowspan=1><a name=d2010e2421 class=n-a></a>M=0.509, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2424 class=n-a></a>M=0.509, SD=0.007</td><td align=center colspan=1 rowspan=1><a name=d2010e2427 class=n-a></a>M=0.503, SD=0.009</td></tr><tr><a name=d2010e2431 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2433 class=n-a></a>Ps_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e2436 class=n-a></a>M=0.514, SD=0.010</td><td align=center colspan=1 rowspan=1><a name=d2010e2439 class=n-a></a>M=0.518, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2442 class=n-a></a>M=0.496, SD=0.010</td><td align=center colspan=1 rowspan=1><a name=d2010e2445 class=n-a></a>M=0.495, SD=0.008</td></tr><tr><a name=d2010e2449 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2451 class=n-a></a>Ps_So</td><td align=center colspan=1 rowspan=1><a name=d2010e2454 class=n-a></a>M=0.488, SD=0.005</td><td align=center colspan=1 rowspan=1><a name=d2010e2457 class=n-a></a>M=0.504, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2460 class=n-a></a>M=0.493, SD=0.007</td><td align=center colspan=1 rowspan=1><a name=d2010e2463 class=n-a></a>M=0.503, SD=0.013</td></tr></tbody></table><div class=table-wrap-foot><div class=footnote><a name=d2010e2471 class=n-a></a><p id=d2010e2473> SVM, support vector machine; LDA, linear discriminant analysis; kNN, k-nearest neighbour.</p></div></div></div><a name=T3 class=n-a></a><div class="table-wrap panel clearfix"><div class=caption><h3>Table 3. Mean (M) and standard deviations (SD) of the accuracies of the static temporal classifications.</h3><p id=d2010e2489>Active Image (Ac_Im), Active Sound (Ac_So), Passive Image (Ps_Im) and Passive Sound (Ps_So).</p></div><table xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class=article-table frame=below><a name=d2010e2493 class=n-a></a><thead><a name=d2010e2495 class=n-a></a><tr><a name=d2010e2497 class=n-a></a><th align=center colspan=1 rowspan=1><a name=d2010e2499 class=n-a></a>Group</th><th align=center colspan=1 rowspan=1><a name=d2010e2502 class=n-a></a>LDA</th><th align=center colspan=1 rowspan=1><a name=d2010e2505 class=n-a></a>SVM</th><th align=center colspan=1 rowspan=1><a name=d2010e2508 class=n-a></a>kNN</th><th align=center colspan=1 rowspan=1><a name=d2010e2511 class=n-a></a>Random</th></tr></thead><tbody><a name=d2010e2516 class=n-a></a><tr><a name=d2010e2518 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2520 class=n-a></a>Ac_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e2523 class=n-a></a>M=0.492, SD=0.010</td><td align=center colspan=1 rowspan=1><a name=d2010e2526 class=n-a></a>M=0.510, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2529 class=n-a></a>M=0.500, SD=0.008</td><td align=center colspan=1 rowspan=1><a name=d2010e2532 class=n-a></a>M=0.498, SD=0.007</td></tr><tr><a name=d2010e2536 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2538 class=n-a></a>Ac_So</td><td align=center colspan=1 rowspan=1><a name=d2010e2541 class=n-a></a>M=0.501, SD=0.007</td><td align=center colspan=1 rowspan=1><a name=d2010e2544 class=n-a></a>M=0.509, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2547 class=n-a></a>M=0.510, SD=0.006</td><td align=center colspan=1 rowspan=1><a name=d2010e2550 class=n-a></a>M=0.498, SD=0.012</td></tr><tr><a name=d2010e2554 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2556 class=n-a></a>Ps_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e2559 class=n-a></a>M=0.500, SD=0.012</td><td align=center colspan=1 rowspan=1><a name=d2010e2562 class=n-a></a>M=0.518, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2565 class=n-a></a>M=0.492, SD=0.005</td><td align=center colspan=1 rowspan=1><a name=d2010e2568 class=n-a></a>M=0.499, SD=0.006</td></tr><tr><a name=d2010e2572 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e2574 class=n-a></a>Ps_So</td><td align=center colspan=1 rowspan=1><a name=d2010e2577 class=n-a></a>M=0.499, SD=0.008</td><td align=center colspan=1 rowspan=1><a name=d2010e2580 class=n-a></a>M=0.504, SD=0.000</td><td align=center colspan=1 rowspan=1><a name=d2010e2583 class=n-a></a>M=0.492, SD=0.006</td><td align=center colspan=1 rowspan=1><a name=d2010e2586 class=n-a></a>M=0.498, SD=0.008</td></tr></tbody></table><div class=table-wrap-foot><div class=footnote><a name=d2010e2594 class=n-a></a><p id=d2010e2596> SVM, support vector machine; LDA, linear discriminant analysis; kNN, k-nearest neighbour.</p></div></div></div><p class="" id=d2010e2603>Note that all the accuracies refer to the same static classification problem (high arousal vs low arousal), performed using different classifiers (SVM, LDA, kNN) and features (spectral, temporal), on different groups (Ps_Im, Ps_So, Ac_Im, Ac_So).</p><p class="" id=d2010e2606>Using spectral features, in only two groups did some classifiers show an accuracy greater than the benchmark. In the Ac_So group, <i>ACC<sub>SVM</sub></i> = 50.9% (t(18)=2.371, p=0.015) and <i>ACC<sub>kNN</sub></i> = 50.9% (t(18)=1.828, p=0.042), while for Ps_Im, <i>ACC<sub>LDA</sub></i> = 51.4% (t(18)=4.667, p&lt;0.001) and <i>ACC<sub>SVM</sub></i> = 51.8% (t(18)=9.513, p&lt;0.001).</p><p class="" id=d2010e2629>Using temporal features, in all the groups some classifiers showed an accuracy greater than the benchmark. In the Ac_So group, <i>ACC<sub>SVM</sub></i> = 50.9% (t(18)=2.907, p=0.005) and <i>ACC<sub>kNN</sub></i> = 51% (t(18)=2.793, p=0.006) and in the Ps_So group, <i>AAC<sub>SVM</sub></i> = 50.4% (t(18)=9.493, p&lt;0.001).</p></div><div class=section><a name=d2010e2648 class=n-a></a><h3 class=section-title>Dynamic features</h3><p class="" id=d2010e2653>In <a href="#f6">Figure 6</a>–<a href="#f12">Figure 12</a>, the results of the significant dynamic classifications are shown. In the upper section of the plots, the mean (bold line) and the standard deviation (shaded) of the accuracy sequence are shown. In the lower section of the plot (black dashed line), the Bonferroni-Holm corrected p-values sequence, discretized (as a stair graph) as significant (p&lt;0.05) or non-significant (p&gt;0.05) is shown.</p><a name=f6 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure6.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure6.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure6.gif"></a><div class=caption><h3>Figure 6. Spectral dynamic features.</h3><p id=d2010e2672>Accuracy (mean value, coloured line; standard deviation, shaded line) and p-values (black dotted line) in Ac_Im group for LDA (<b>a</b>) and SVM (<b>b</b>) classifiers.</p></div></div><a name=f7 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure7.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure7.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure7.gif"></a><div class=caption><h3>Figure 7. Spectral dynamic features.</h3><p id=d2010e2695>Accuracy (mean value, coloured line; standard deviation, shaded line) and p-values (black dotted line) in Ac_So group for LDA (<b>a</b>) and SVM (<b>b</b>) classifiers.</p></div></div><a name=f8 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure8.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure8.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure8.gif"></a><div class=caption><h3>Figure 8. Spectral dynamic features.</h3><p id=d2010e2718>Accuracy (mean value, coloured line; standard deviation, shaded line) and p-values (black dotted line) in Ps_Im group for LDA (<b>a</b>) and SVM (<b>b</b>) classifiers.</p></div></div><a name=f9 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure9.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure9.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure9.gif"></a><div class=caption><h3>Figure 9. Spectral dynamic features.</h3><p id=d2010e2742>Accuracy (mean value, coloured line; standard deviation, shaded line) and p-values (black dotted line) in Ac_So group for SVM (<b>a</b>) and kNN (<b>b</b>) classifiers.</p></div></div><a name=f10 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure10.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure10.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure10.gif"></a><div class=caption><h3>Figure 10. Temporal dynamic features.</h3><p id=d2010e2765>Accuracy (mean value, coloured line; standard deviation, shaded line) and p-values (black dotted line) in Ac_So group for LDA classifier.</p></div></div><a name=f11 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure11.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure11.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure11.gif"></a><div class=caption><h3>Figure 11. Temporal dynamic features.</h3><p id=d2010e2782>Accuracy (mean value, coloured line; standard deviation, shaded line) and p-values (black dotted line) in Ps_Im group for LDA (<b>a</b>), SVM (<b>b</b>) and kNN (<b>c</b>) classifiers.</p></div></div><a name=f12 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure12.gif"><img alt="6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure12.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24487/6c4b67cc-f5d6-41b1-88b2-b1edee302a8b_figure12.gif"></a><div class=caption><h3>Figure 12. Temporal dynamic features.</h3><p id=d2010e2808>Accuracy (mean value, coloured line; standard deviation, shaded line) and p-values (black dotted line) in Ps_So group for LDA (<b>a</b>) and kNN (<b>b</b>) classifiers.</p></div></div><p class="" id=d2010e2821>Note that all the accuracy plots refer to the same dynamic classification problem (high arousal vs low arousal), performed using different classifiers (SVM, LDA, kNN) and features on different groups. Spectral: Ac_Im (<a href="#f6">Figure 6</a>), Ac_So (<a href="#f7">Figure 7</a>), Ps_Im (<a href="#f8">Figure 8</a>) and Ps_So (<a href="#f9">Figure 9</a>); temporal: Ac_So (<a href="#f10">Figure 10</a>), Ps_Im (<a href="#f11">Figure 11</a>) and Ps_So (<a href="#f12">Figure 12</a>).</p><p class="" id=d2010e2846>Using spectral features, in all the groups some classifiers showed an accuracy greater than the benchmark. In the Ac_Im group, <i>ACC<sub>LDA</sub></i> = 51.97% @<i>t</i> = 0.080<i>s</i> (t(18)=6.291, p&lt;0.001) and <i>ACC<sub>SVM</sub></i> = 51.07% @<i>t</i> = 0.416<i>s</i> (t(18)=6.531, p&lt;0.001). In the Ac_So group, <i>ACC<sub>LDA</sub></i> = 53.04% @<i>t</i> = 0.332<i>s</i> (t(18)=8.583, p&lt;0.001) and <i>ACC<sub>SVM</sub></i> = 51.16% @<i>t</i> = 0.146<i>s</i> (t(18)=8.612, p&lt;0.001). In the Ps_Im group, <i>ACC<sub>LDA</sub></i> = 53.12% @<i>t</i> = 0.156<i>s</i> (t(18)=6.372, p=0.000) and <i>ACC<sub>SVM</sub></i> = 51.83% @<i>t</i> = 0.140<i>s</i> (t(18)=6.668, p&lt;0.001). In the Ps_So group, <i>ACC<sub>SVM</sub></i> = 50.62% @<i>t</i> = 0.024<i>s</i> (t(18)=5.236, p=0.003) and <i>ACC<sub>kNN</sub></i> = 51.41% @<i>t</i> = 0.476<i>s</i> (t(18)=4.307, p=0.026).</p><p class="" id=d2010e2942>Using temporal features, in only three groups did some classifiers show an accuracy greater than the benchmark. In the Ac_So group, <i>ACC<sub>SVM</sub></i> = 63.80% @<i>t</i> = 0.100<i>s</i> (t(18)=6.113, p=0.001). In the Ps_Im group, <i>ACC<sub>LDA</sub></i> = 63.68% @<i>t</i> = 0.024<i>s</i> (t(18)=12.108, p&lt;0.001) and <i>ACC<sub>SVM</sub></i> = 51.43% @<i>t</i> = 0.084<i>s</i> (t(18)=4.881, p=0.008). In the Ps_So group, <i>ACC<sub>LDA</sub></i> = 64.30% @<i>t</i> = 0.0276<i>s</i> (t(18)=11.092, p&lt;0.001) and <i>ACC<sub>kNN</sub></i> = 63.70% @<i>t</i> = 0.480<i>s</i> (t(18)=16.621, p&lt;0.001).</p><p class="" id=d2010e3002><a href="#T4">Table 4</a> reports the accuracies for dynamic features, ordered in descending order and grouped for classifier, feature group and time.</p><a name=T4 class=n-a></a><div class="table-wrap panel clearfix"><div class=caption><h3>Table 4. Dynamic features.</h3><p id=d2010e3017>Ordered accuracies grouped for classifier, feature and group.</p></div><table xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class=article-table frame=below><a name=d2010e3021 class=n-a></a><thead><a name=d2010e3023 class=n-a></a><tr><a name=d2010e3025 class=n-a></a><th align=center colspan=1 rowspan=1><a name=d2010e3027 class=n-a></a>Classifier</th><th align=center colspan=1 rowspan=1><a name=d2010e3030 class=n-a></a>Accuracy</th><th align=center colspan=1 rowspan=1><a name=d2010e3033 class=n-a></a>Time [s]</th><th align=center colspan=1 rowspan=1><a name=d2010e3036 class=n-a></a>Group</th><th align=center colspan=1 rowspan=1><a name=d2010e3039 class=n-a></a>Feature</th></tr></thead><tbody><a name=d2010e3044 class=n-a></a><tr><a name=d2010e3046 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3048 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e3051 class=n-a></a>63.80%</td><td align=center colspan=1 rowspan=1><a name=d2010e3054 class=n-a></a>0.1</td><td align=center colspan=1 rowspan=1><a name=d2010e3057 class=n-a></a>Ac_So</td><td align=center colspan=1 rowspan=1><a name=d2010e3060 class=n-a></a>Temporal</td></tr><tr><a name=d2010e3064 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3066 class=n-a></a>kNN</td><td align=center colspan=1 rowspan=1><a name=d2010e3069 class=n-a></a>63.70%</td><td align=center colspan=1 rowspan=1><a name=d2010e3072 class=n-a></a>0.048</td><td align=center colspan=1 rowspan=1><a name=d2010e3075 class=n-a></a>Ps_So</td><td align=center colspan=1 rowspan=1><a name=d2010e3078 class=n-a></a>Temporal</td></tr><tr><a name=d2010e3082 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3084 class=n-a></a>LDA</td><td align=center colspan=1 rowspan=1><a name=d2010e3087 class=n-a></a>63.68%</td><td align=center colspan=1 rowspan=1><a name=d2010e3090 class=n-a></a>0.024</td><td align=center colspan=1 rowspan=1><a name=d2010e3093 class=n-a></a>Ps_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e3096 class=n-a></a>Temporal</td></tr><tr><a name=d2010e3100 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3102 class=n-a></a>LDA</td><td align=center colspan=1 rowspan=1><a name=d2010e3105 class=n-a></a>63.30%</td><td align=center colspan=1 rowspan=1><a name=d2010e3108 class=n-a></a>0.0276</td><td align=center colspan=1 rowspan=1><a name=d2010e3111 class=n-a></a>Ps_So</td><td align=center colspan=1 rowspan=1><a name=d2010e3114 class=n-a></a>Temporal</td></tr><tr><a name=d2010e3118 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3120 class=n-a></a>LDA</td><td align=center colspan=1 rowspan=1><a name=d2010e3123 class=n-a></a>53.12%</td><td align=center colspan=1 rowspan=1><a name=d2010e3126 class=n-a></a>0.156</td><td align=center colspan=1 rowspan=1><a name=d2010e3129 class=n-a></a>Ps_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e3132 class=n-a></a>Spectral</td></tr><tr><a name=d2010e3137 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3139 class=n-a></a>LDA</td><td align=center colspan=1 rowspan=1><a name=d2010e3142 class=n-a></a>53.04%</td><td align=center colspan=1 rowspan=1><a name=d2010e3145 class=n-a></a>0.3332</td><td align=center colspan=1 rowspan=1><a name=d2010e3148 class=n-a></a>Ac_So</td><td align=center colspan=1 rowspan=1><a name=d2010e3151 class=n-a></a>Spectral</td></tr><tr><a name=d2010e3155 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3157 class=n-a></a>LDA</td><td align=center colspan=1 rowspan=1><a name=d2010e3160 class=n-a></a>51.97%</td><td align=center colspan=1 rowspan=1><a name=d2010e3163 class=n-a></a>0.08</td><td align=center colspan=1 rowspan=1><a name=d2010e3166 class=n-a></a>Ac_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e3169 class=n-a></a>Spectral</td></tr><tr><a name=d2010e3173 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3175 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e3178 class=n-a></a>51.83%</td><td align=center colspan=1 rowspan=1><a name=d2010e3181 class=n-a></a>0.14</td><td align=center colspan=1 rowspan=1><a name=d2010e3184 class=n-a></a>Ps_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e3187 class=n-a></a>Spectral</td></tr><tr><a name=d2010e3191 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3193 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e3196 class=n-a></a>51.43%</td><td align=center colspan=1 rowspan=1><a name=d2010e3199 class=n-a></a>0.084</td><td align=center colspan=1 rowspan=1><a name=d2010e3202 class=n-a></a>Ps_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e3205 class=n-a></a>Temporal</td></tr><tr><a name=d2010e3209 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3211 class=n-a></a>kNN</td><td align=center colspan=1 rowspan=1><a name=d2010e3214 class=n-a></a>51.41%</td><td align=center colspan=1 rowspan=1><a name=d2010e3217 class=n-a></a>0.476</td><td align=center colspan=1 rowspan=1><a name=d2010e3220 class=n-a></a>Ps_So</td><td align=center colspan=1 rowspan=1><a name=d2010e3223 class=n-a></a>Spectral</td></tr><tr><a name=d2010e3227 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3229 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e3232 class=n-a></a>51.16%</td><td align=center colspan=1 rowspan=1><a name=d2010e3235 class=n-a></a>0.146</td><td align=center colspan=1 rowspan=1><a name=d2010e3238 class=n-a></a>Ac_So</td><td align=center colspan=1 rowspan=1><a name=d2010e3241 class=n-a></a>Spectral</td></tr><tr><a name=d2010e3246 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3248 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e3251 class=n-a></a>51.07%</td><td align=center colspan=1 rowspan=1><a name=d2010e3254 class=n-a></a>0.416</td><td align=center colspan=1 rowspan=1><a name=d2010e3257 class=n-a></a>Ac_Im</td><td align=center colspan=1 rowspan=1><a name=d2010e3260 class=n-a></a>Spectral</td></tr><tr><a name=d2010e3264 class=n-a></a><td align=center colspan=1 rowspan=1><a name=d2010e3266 class=n-a></a>SVM</td><td align=center colspan=1 rowspan=1><a name=d2010e3269 class=n-a></a>50.62%</td><td align=center colspan=1 rowspan=1><a name=d2010e3272 class=n-a></a>0.024</td><td align=center colspan=1 rowspan=1><a name=d2010e3275 class=n-a></a>Ps_So</td><td align=center colspan=1 rowspan=1><a name=d2010e3278 class=n-a></a>Spectral</td></tr></tbody></table><div class=table-wrap-foot><div class=footnote><a name=d2010e3286 class=n-a></a><p id=d2010e3288> SVM, support vector machine; LDA, linear discriminant analysis; kNN, k-nearest neighbour.</p></div></div></div></div></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d2010e3298>Discussion</h2><p class="" id=d2010e3301>The aim of the study was to provide new methodological insights regarding machine learning approaches for the classification of anticipatory emotion-related EEG signals, by testing the performance of different classifiers on different features.</p><p class="" id=d2010e3304>From the ISIs (i.e. the 1000 ms long window preceding each stimulus onset), we extracted two kinds of “static” features, namely spectral and temporal, the most commonly used features in the field of emotion recognition<sup><a href="#ref-19">19</a>,<a href="#ref-20">20</a></sup>. As spectral features, we used the beta-over-alpha and the beta-over-theta ratio, whereas for the temporal feature we concatenated the decimated EEG values.</p><p class="" id=d2010e3314>Additionally, we extracted the temporal sequences of both static spectral and temporal features, using a 500 ms long window moving along the ISI to build dynamic spectral and temporal features, respectively. This step is crucial for our work since, considering the temporal resolution of the EEG, an efficient classification should take into account the temporal dimension, to provide information about when the difference between two conditions are maximally expressed and therefore classified.</p><p class="" id=d2010e3317>We trained and tested three different classifiers (LDA, SVM, kNN, the most commonly used in the field of emotion recognition<sup><a href="#ref-19">19</a>,<a href="#ref-20">20</a></sup>) using both static and dynamic features, comparing their accuracies against a random classifier that served as benchmark.</p><p class="" id=d2010e3328>Our goal was to identify the best classifier (static vs dynamic) and the best feature type (spectral vs temporal) to classify the arousal level (high vs low) of 56 auditory/visual stimuli. The stimuli, extracted from two standardized datasets (NIMSTIM<sup><a href="#ref-52">52</a></sup> and IADS<sup><a href="#ref-44">44</a></sup>), for visual and auditory stimuli, respectively) were presented in a randomized order, triggered by a TrueRNG™ hardware random number generator.</p><p class="" id=d2010e3339>Considering the number of groups (four), the number of classifiers (three) and the number of feature types (two), each classification (static or dynamic) produced a total of 24 accuracies, whose significances were statistically tested (using a two-sample t-test and the benchmark’s accuracies).</p><p class="" id=d2010e3342>Within the nine significant accuracies obtained using static features, the classifier that obtained the highest number of accuracies was the SVM (six significant accuracies), followed by kNN (two significant accuracies) and LDA (one significant accuracy). The most frequent feature was the temporal (five significant accuracies). Finally, the best (static) feature-classifier combination was the SVM with spectral features (51.8%), followed by LDA with spectral features (51.4%) and kNN with temporal features (51%).</p><p class="" id=d2010e3345>Within the 13 significant accuracies obtained using dynamic features, the classifier that obtained the highest number of accuracies was the SVM (six significant accuracies), followed by LDA (four significant accuracies) and kNN (three significant accuracies). The most frequent feature was the spectral (eight significant accuracies). Finally, the best (dynamic) feature-classifier combination was the SVM with temporal features (63.8%), followed by kNN with temporal features (63.70%) and LDA with temporal features (63.68%). Spectral features produced only the 5th highest accuracy (53.12% with LDA). The three best accuracies were all within the first 100ms of the ISI, although a non-significant Spearman’s correlation between accuracy and time was observed (r=-0.308, p=0.306).</p><p class="" id=d2010e3348>Our results show that globally the SVM presents the best accuracy, independent from feature type (temporal or spectral), but more importantly, the combination of SVM with the dynamic temporal feature produced the best classification performance. This finding is particularly relevant, considering the application of EEG in cognitive science. In fact, due to its high temporal resolution, EEG is often applied to investigate the timing of neural processes in relation to behavioural performance.</p><p class="" id=d2010e3351>Our results therefore suggest that, in order to best classify emotions based on electrophysiological brain activity, the temporal dynamic of the EEG signal should be taken into account with a dynamic feature and consequently with a dynamic classifier. In fact, by including also time evolution of the feature in the machine learning model, it is possible to infer when two different conditions maximally diverge, allowing possible interpretation of the timing of the cognitive processes and the behaviour of the underlying neural substrate.</p><p class="" id=d2010e3355>Finally, the main contribution of our results for the scientific community is that they provide a methodological advancement that is generally valid both for the investigation of emotion based on a machine learning approach with EEG signals and also for the investigation of preparatory brain activity.</p></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d2010e3361>Data availability</h2><div class=section><a name=d2010e3364 class=n-a></a><h3 class=section-title>Underlying data</h3><p class="" id=d2010e3369>Figshare: EEG anticipation of random high and low arousal faces and sounds. <a target=xrefwindow href="https://doi.org/10.6084/m9.figshare.6874871.v8" id=d2010e3371>https://doi.org/10.6084/m9.figshare.6874871.v8</a><sup><a href="#ref-27">27</a></sup></p><p class="" id=d2010e3377>This project contains the following underlying data: <div class=list><a name=d2010e3379 class=n-a></a><ul style="list-style-type: none"><li><p id=d2010e3386><span class=label>- </span> EEG metafile (DOCX)</p></li><li><p id=d2010e3395><span class=label>- </span> EEG data related to the Passive, Active and Predictive conditions (CSV)</p></li><li><p id=d2010e3404><span class=label>- </span> Video clips of the EEG activity before stimulus presentation (MPG)</p></li></ul></div> </p></div><div class=section><a name=d2010e3411 class=n-a></a><h3 class=section-title>Extended data</h3><p class="" id=d2010e3416>Figshare: EEG anticipation of random high and low arousal faces and sounds. <a target=xrefwindow href="https://doi.org/10.6084/m9.figshare.6874871.v8" id=d2010e3418>https://doi.org/10.6084/m9.figshare.6874871.v8</a><sup><a href="#ref-27">27</a></sup></p><p class="" id=d2010e3424>This project contains the following extended data: <div class=list><a name=d2010e3426 class=n-a></a><ul style="list-style-type: none"><li><p id=d2010e3433><span class=label>- </span> Detailed description of LDA, SVM and kNN machine learning algorithms (DOCX)</p></li></ul></div> </p><p class="" id=d2010e3439>Data are available under the terms of the <a target=xrefwindow href="https://creativecommons.org/licenses/by/4.0/legalcode" id=d2010e3441>Creative Commons Attribution 4.0 International license</a> (CC-BY 4.0).</p></div></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d2010e3450>Software availability</h2><p class="" id=d2010e3453>Source code available from: <a target=xrefwindow href="https://github.com/mbilucaglia/ML_BAA" id=d2010e3455>https://github.com/mbilucaglia/ML_BAA</a></p><p class="" id=d2010e3458>Archived source code at time of publication: <a target=xrefwindow href="https://doi.org/10.5281/zenodo.3666045" id=d2010e3460>https://doi.org/10.5281/zenodo.3666045</a><sup><a href="#ref-51">51</a></sup></p><p class="" id=d2010e3466>License: <a target=xrefwindow href="https://opensource.org/licenses/GPL-3.0" id=d2010e3468>GPL-3.0</a></p></div><div id=article1-back class=generated-article-footer><div class=back-section><a name=d2010e3475 class=n-a></a><span class="research-layout prime-recommended-wrapper reference-heading is-hidden"><span class="f1r-icon icon-79_faculty_recommended_badge red vmiddle default-cursor"></span><span class="prime-red big">F1000 recommended</span></span><h2 class=main-title id=d2423>References</h2><div class="section ref-list"><a name=d2010e3475 class=n-a></a><ul><li><a name=ref-1 class=n-a></a><span class=label>1. </span>&nbsp;<span class=citation><a name=d2010e3482 class=n-a></a>Friston K: A theory of cortical responses. <i>Philos Trans R Soc Lond B Biol Sci.</i> 2005; <b>360</b>(1456): 815–836. <a target=xrefwindow id=d2010e3490 href="http://www.ncbi.nlm.nih.gov/pubmed/15937014">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3493 href="https://doi.org/10.1098/rstb.2005.1622">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3496 href="http://www.ncbi.nlm.nih.gov/pmc/articles/1569488">Free Full Text </a></span></li><li><a name=ref-2 class=n-a></a><span class=label>2. </span>&nbsp;<span class=citation><a name=d2010e3505 class=n-a></a>Nobre AC: Orienting attention to instants in time. <i>Neuropsychologia.</i> 2001; <b>39</b>(12): 1317–1328. <a target=xrefwindow id=d2010e3513 href="http://www.ncbi.nlm.nih.gov/pubmed/11566314">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3516 href="https://doi.org/10.1016/s0028-3932(01)00120-8">Publisher Full Text </a></span></li><li><a name=ref-3 class=n-a></a><span class=label>3. </span>&nbsp;<span class=citation><a name=d2010e3525 class=n-a></a>Mento G, Vallesi A: Spatiotemporally dissociable neural signatures for generating and updating expectation over time in children: A High Density-ERP study. <i>Dev Cogn Neurosci.</i> 2016; <b>19</b>: 98–106. <a target=xrefwindow id=d2010e3533 href="http://www.ncbi.nlm.nih.gov/pubmed/26946428">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3536 href="https://doi.org/10.1016/j.dcn.2016.02.008">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3539 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6988099">Free Full Text </a></span></li><li><a name=ref-4 class=n-a></a><span class=label>4. </span>&nbsp;<span class=citation><a name=d2010e3548 class=n-a></a>Mento G, Tarantino V, Vallesi A, <i> et al.</i>: Spatiotemporal neurodynamics underlying internally and externally driven temporal prediction: A high spatial resolution ERP study. <i>J Cogn Neurosci.</i> 2015; <b>27</b>(3): 425–439. <a target=xrefwindow id=d2010e3559 href="http://www.ncbi.nlm.nih.gov/pubmed/25203276">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3562 href="https://doi.org/10.1162/jocn_a_00715">Publisher Full Text </a></span></li><li><a name=ref-5 class=n-a></a><span class=label>5. </span>&nbsp;<span class=citation><a name=d2010e3571 class=n-a></a>Barsalou LW: Grounded Cognition. <i>Annu Rev Psychol.</i> 2008; <b>59</b>: 617–645. <a target=xrefwindow id=d2010e3579 href="http://www.ncbi.nlm.nih.gov/pubmed/17705682">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3582 href="https://doi.org/10.1146/annurev.psych.59.103006.093639">Publisher Full Text </a></span></li><li><a name=ref-6 class=n-a></a><span class=label>6. </span>&nbsp;<span class=citation><a name=d2010e3592 class=n-a></a>Barrett LF: The theory of constructed emotion: an active inference account of interoception and categorization. <i>Soc Cogn Affect Neurosci.</i> 2017; <b>12</b>(11): 1833. <a target=xrefwindow id=d2010e3600 href="http://www.ncbi.nlm.nih.gov/pubmed/28472391">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3603 href="https://doi.org/10.1093/scan/nsx060">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3606 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5691871">Free Full Text </a></span></li><li><a name=ref-7 class=n-a></a><span class=label>7. </span>&nbsp;<span class=citation><a name=d2010e3615 class=n-a></a>Bruner JS: Acts of meaning. Harvard University Press, 1990. <a target=xrefwindow id=d2010e3617 href="https://books.google.co.in/books/about/Acts_of_Meaning.html?id=YHt_M41uIuUC">Reference Source</a></span></li><li><a name=ref-8 class=n-a></a><span class=label>8. </span>&nbsp;<span class=citation><a name=d2010e3626 class=n-a></a>Miniussi C, Wilding EL, Coull JT, <i> et al.</i>: Orienting attention in time. Modulation of brain potentials. <i>Brain.</i> 1999; <b>122</b>(Pt 8): 1507–1518. <a target=xrefwindow id=d2010e3637 href="http://www.ncbi.nlm.nih.gov/pubmed/10430834">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3640 href="https://doi.org/10.1093/brain/122.8.1507">Publisher Full Text </a></span></li><li><a name=ref-9 class=n-a></a><span class=label>9. </span>&nbsp;<span class=citation><a name=d2010e3649 class=n-a></a>Stefanics G, Hangya B, Hernádi I, <i> et al.</i>: Phase entrainment of human delta oscillations can mediate the effects of expectation on reaction speed. <i>J Neurosci.</i> 2010; <b>30</b>(41): 13578–13585. <a target=xrefwindow id=d2010e3660 href="http://www.ncbi.nlm.nih.gov/pubmed/20943899">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3663 href="https://doi.org/10.1523/JNEUROSCI.0703-10.2010">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3667 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4427664">Free Full Text </a></span></li><li><a name=ref-10 class=n-a></a><span class=label>10. </span>&nbsp;<span class=citation><a name=d2010e3676 class=n-a></a>Denny BT, Ochsner KN, Weber J, <i> et al.</i>: Anticipatory brain activity predicts the success or failure of subsequent emotion regulation. <i>Soc Cogn Affect Neurosci.</i> 2014; <b>9</b>(4): 403–411. <a target=xrefwindow id=d2010e3687 href="http://www.ncbi.nlm.nih.gov/pubmed/23202664">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3690 href="https://doi.org/10.1093/scan/nss148">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3694 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3989121">Free Full Text </a></span></li><li><a name=ref-11 class=n-a></a><span class=label>11. </span>&nbsp;<span class=citation><a name=d2010e3703 class=n-a></a>Abler B, Erk S, Herwig U, <i> et al.</i>: Anticipation of aversive stimuli activates extended amygdala in unipolar depression. <i>J Psychiatr Res.</i> 2007; <b>41</b>(6): 511–522. <a target=xrefwindow id=d2010e3714 href="http://www.ncbi.nlm.nih.gov/pubmed/17010993">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3717 href="https://doi.org/10.1016/j.jpsychires.2006.07.020">Publisher Full Text </a></span></li><li><a name=ref-12 class=n-a></a><span class=label>12. </span>&nbsp;<span class=citation><a name=d2010e3727 class=n-a></a>Morinaga K, Akiyoshi J, Matsushita H, <i> et al.</i>: Anticipatory anxiety-induced changes in human lateral prefrontal cortex activity. <i>Biol Psychol.</i> 2007; <b>74</b>(1): 34–38. <a target=xrefwindow id=d2010e3738 href="http://www.ncbi.nlm.nih.gov/pubmed/16893600">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3741 href="https://doi.org/10.1016/j.biopsycho.2006.06.005">Publisher Full Text </a></span></li><li><a name=ref-13 class=n-a></a><span class=label>13. </span>&nbsp;<span class=citation><a name=d2010e3750 class=n-a></a>Duma GM, Mento G, Manari T, <i> et al.</i>: Driving with Intuition: A Preregistered Study about the EEG Anticipation of Simulated Random Car Accidents. <i>PLoS One.</i> 2017; <b>12</b>(1): e0170370. <a target=xrefwindow id=d2010e3761 href="http://www.ncbi.nlm.nih.gov/pubmed/28103303">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3764 href="https://doi.org/10.1371/journal.pone.0170370">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3768 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5245833">Free Full Text </a></span></li><li><a name=ref-14 class=n-a></a><span class=label>14. </span>&nbsp;<span class=citation><a name=d2010e3777 class=n-a></a>Radin DI, Vieten C, Michel L, <i> et al.</i>: Electrocortical activity prior to unpredictable stimuli in meditators and nonmeditators. <i>Explore (NY).</i> 2011; <b>7</b>(5): 286–299. <a target=xrefwindow id=d2010e3788 href="http://www.ncbi.nlm.nih.gov/pubmed/21907152">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3791 href="https://doi.org/10.1016/j.explore.2011.06.004">Publisher Full Text </a></span></li><li><a name=ref-15 class=n-a></a><span class=label>15. </span>&nbsp;<span class=citation><a name=d2010e3800 class=n-a></a>Mossbridge JA, Tressoldi P, Utts J, <i> et al.</i>: Predicting the unpredictable: critical analysis and practical implications of predictive anticipatory activity. <i>Front Hum Neurosci.</i> 2014; <b>8</b>: 146. <a target=xrefwindow id=d2010e3811 href="http://www.ncbi.nlm.nih.gov/pubmed/24723870">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3814 href="https://doi.org/10.3389/fnhum.2014.00146">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3818 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3971164">Free Full Text </a></span></li><li><a name=ref-16 class=n-a></a><span class=label>16. </span>&nbsp;<span class=citation><a name=d2010e3827 class=n-a></a>Gunes H, Pantic M: Automatic, Dimensional and Continuous Emotion Recognition. <i>Int J Synth Emot.</i> 2010; <b>1</b>(1): 32. <a target=xrefwindow id=d2010e3835 href="https://doi.org/10.4018/jse.2010101605">Publisher Full Text </a></span></li><li><a name=ref-17 class=n-a></a><span class=label>17. </span>&nbsp;<span class=citation><a name=d2010e3844 class=n-a></a>Shu L, Xie J, Yang M, <i> et al.</i>: A Review of Emotion Recognition Using Physiological Signals. <i>Sensors (Basel).</i> 2018; <b>18</b>(7): pii: E2074. <a target=xrefwindow id=d2010e3855 href="http://www.ncbi.nlm.nih.gov/pubmed/29958457">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3858 href="https://doi.org/10.3390/s18072074">Publisher Full Text </a> | <a target=xrefwindow id=d2010e3862 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6069143">Free Full Text </a></span></li><li><a name=ref-18 class=n-a></a><span class=label>18. </span>&nbsp;<span class=citation><a name=d2010e3872 class=n-a></a>Calvo RA, D’Mello S: Affect detection: An interdisciplinary review of models, methods, and their applications. <i>IEEE Trans Affect Comput.</i> 2010; <b>1</b>(1): 18–37. <a target=xrefwindow id=d2010e3880 href="https://doi.org/10.1109/T-AFFC.2010.1">Publisher Full Text </a></span></li><li><a name=ref-19 class=n-a></a><span class=label>19. </span>&nbsp;<span class=citation><a name=d2010e3889 class=n-a></a>Alarcao SM, Fonseca MJ: Emotions Recognition Using EEG Signals: A Survey. <i>IEEE Trans Affect Comput.</i> 2017; <b>3045</b>: 1–20. <a target=xrefwindow id=d2010e3897 href="https://doi.org/10.1109/TAFFC.2017.2714671">Publisher Full Text </a></span></li><li><a name=ref-20 class=n-a></a><span class=label>20. </span>&nbsp;<span class=citation><a name=d2010e3906 class=n-a></a>Al-Nafjan A, Hosny M, Al-Ohali Y, <i> et al.</i>: Review and Classification of Emotion Recognition Based on EEG Brain-Computer Interface System Research: A Systematic Review. <i>Appl Sci.</i> 2017; <b>7</b>(12): 1239. <a target=xrefwindow id=d2010e3917 href="https://doi.org/10.3390/app7121239">Publisher Full Text </a></span></li><li><a name=ref-21 class=n-a></a><span class=label>21. </span>&nbsp;<span class=citation><a name=d2010e3926 class=n-a></a>Lotte F, Congedo M, Lécuyer A, <i> et al.</i>: A review of classification algorithms for EEG-based brain-computer interfaces. <i>J Neural Eng.</i> 2007; <b>4</b>(2): R1–R13. <a target=xrefwindow id=d2010e3937 href="http://www.ncbi.nlm.nih.gov/pubmed/17409472">PubMed Abstract </a> | <a target=xrefwindow id=d2010e3940 href="https://doi.org/10.1088/1741-2560/4/2/R01">Publisher Full Text </a></span></li><li><a name=ref-22 class=n-a></a><span class=label>22. </span>&nbsp;<span class=citation><a name=d2010e3949 class=n-a></a>Lin YP, Wang CH, Wu TL, <i> et al.</i>: EEG-based emotion recognition in music listening: A comparison of schemes for multiclass support vector machine. In: <i>Proceedings of the ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings</i>. 2009. <a target=xrefwindow id=d2010e3957 href="https://doi.org/10.1109/ICASSP.2009.4959627">Publisher Full Text </a></span></li><li><a name=ref-23 class=n-a></a><span class=label>23. </span>&nbsp;<span class=citation><a name=d2010e3966 class=n-a></a>Koelstra S, Yazdani A, Soleymani M, <i> et al.</i>: Single trial classification of EEG and peripheral physiological signals for recognition of emotions induced by music videos. In: <i>Proceedings of the Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</i>. 2010. <a target=xrefwindow id=d2010e3974 href="https://doi.org/10.1007/978-3-642-15314-3_9">Publisher Full Text </a></span></li><li><a name=ref-24 class=n-a></a><span class=label>24. </span>&nbsp;<span class=citation><a name=d2010e3984 class=n-a></a>Liu Y, Sourina O: EEG-based valence level recognition for real-time applications. In: <i>Proceedings of the Proceedings of the 2012 International Conference on Cyberworlds, Cyberworlds</i>. 2012; <b>2012</b>. <a target=xrefwindow id=d2010e3992 href="https://doi.org/10.1109/CW.2012.15">Publisher Full Text </a></span></li><li><a name=ref-25 class=n-a></a><span class=label>25. </span>&nbsp;<span class=citation><a name=d2010e4001 class=n-a></a>Murugappan M, Murugappan S: Human emotion recognition through short time Electroencephalogram (EEG) signals using Fast Fourier Transform (FFT). In: <i>Proceedings of the Proceedings - 2013 IEEE 9th International Colloquium on Signal Processing and its Applications, CSPA</i>. 2013; <b>2013</b>. <a target=xrefwindow id=d2010e4009 href="https://doi.org/10.1109/CSPA.2013.6530058">Publisher Full Text </a></span></li><li><a name=ref-26 class=n-a></a><span class=label>26. </span>&nbsp;<span class=citation><a name=d2010e4018 class=n-a></a>Thammasan N, Fukui KI, Numao M: Application of deep belief networks in EEG-based dynamic music-emotion recognition. In: <i>Proceedings of the Proceedings of the International Joint Conference on Neural Networks</i>. 2016. <a target=xrefwindow id=d2010e4023 href="https://doi.org/10.1109/IJCNN.2016.7727292">Publisher Full Text </a></span></li><li><a name=ref-27 class=n-a></a><span class=label>27. </span>&nbsp;<span class=citation><a name=d2010e4032 class=n-a></a>Tressoldi P, Duma GM, Mento G: EEG anticipation of random high and low arousal faces and sounds. <i>figshare.</i> 2018; Dataset. <a target=xrefwindow id=d2010e4037 href="http://www.doi.org/10.6084/m9.figshare.6874871.v8">http://www.doi.org/10.6084/m9.figshare.6874871.v8</a></span></li><li><a name=ref-28 class=n-a></a><span class=label>28. </span>&nbsp;<span class=citation><a name=d2010e4046 class=n-a></a>Duda RO, Hart PE, Stork DG: Pattern classification, 2nd edition. Wiley, 2000. <a target=xrefwindow id=d2010e4048 href="https://www.wiley.com/en-us/Pattern+Classification%2C+2nd+Edition-p-9780471056690">Reference Source</a></span></li><li><a name=ref-29 class=n-a></a><span class=label>29. </span>&nbsp;<span class=citation><a name=d2010e4057 class=n-a></a>Bilucaglia M, Pederzoli L, Giroldini W, <i> et al.</i>: EEG correlation at a distance: A re-analysis of two studies using a machine learning approach [version 2; peer review: 2 approved]. <i>F1000Research.</i> 2019; <b>8</b>: 43. <a target=xrefwindow id=d2010e4068 href="http://www.ncbi.nlm.nih.gov/pubmed/31497288">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4071 href="https://doi.org/10.12688/f1000research.17613.2">Publisher Full Text </a> | <a target=xrefwindow id=d2010e4075 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6713066">Free Full Text </a></span></li><li><a name=ref-30 class=n-a></a><span class=label>30. </span>&nbsp;<span class=citation><a name=d2010e4085 class=n-a></a>Bishop C: Pattern Recognition and Machine Learning. 1st ed Springer, 2006. <a target=xrefwindow id=d2010e4087 href="https://www.elsevier.com/books/pattern-recognition-and-machine-learning/anzai/978-0-08-051363-8">Reference Source</a></span></li><li><a name=ref-31 class=n-a></a><span class=label>31. </span>&nbsp;<span class=citation><a name=d2010e4096 class=n-a></a>Rubinstein YD, Hastie T: Discriminative vs Informative Learning. In: <i>Proceedings of the Proceedings of the The Third International Conference on Knowledge Discovery and Data Mining</i>. 1997; 49–59. <a target=xrefwindow id=d2010e4101 href="https://dl.acm.org/doi/10.5555/3001392.3001401">Reference Source</a></span></li><li><a name=ref-32 class=n-a></a><span class=label>32. </span>&nbsp;<span class=citation><a name=d2010e4110 class=n-a></a>Raudys S, Duin RPW: Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix. <i>Pattern Recognit Lett.</i> 1998; <b>19</b>(5–6): 385–392. <a target=xrefwindow id=d2010e4118 href="https://doi.org/10.1016/S0167-8655(98)00016-6">Publisher Full Text </a></span></li><li><a name=ref-33 class=n-a></a><span class=label>33. </span>&nbsp;<span class=citation><a name=d2010e4127 class=n-a></a>Burges CJC: A Tutorial on Support Vector Machines for Pattern Recognition. <i>Data Min Knowl Discov.</i> 1998; <b>2</b>: 121–167. <a target=xrefwindow id=d2010e4135 href="https://doi.org/10.1023/A:1009715923555">Publisher Full Text </a></span></li><li><a name=ref-34 class=n-a></a><span class=label>34. </span>&nbsp;<span class=citation><a name=d2010e4144 class=n-a></a>Müller KR, Mika S, Rätsch G, <i> et al.</i>: An introduction to kernel-based learning algorithms. <i>IEEE Trans Neural Networks.</i> 2001; <b>12</b>(2): 181–201. <a target=xrefwindow id=d2010e4155 href="http://www.ncbi.nlm.nih.gov/pubmed/18244377">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4158 href="https://doi.org/10.1109/72.914517">Publisher Full Text </a></span></li><li><a name=ref-35 class=n-a></a><span class=label>35. </span>&nbsp;<span class=citation><a name=d2010e4167 class=n-a></a>Atiya AF: Estimating the posterior probabilities using the K-nearest neighbor rule. <i>Neural Comput.</i> 2005; <b>17</b>(3): 731–740. <a target=xrefwindow id=d2010e4175 href="http://www.ncbi.nlm.nih.gov/pubmed/15802013">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4178 href="https://doi.org/10.1162/0899766053019971">Publisher Full Text </a></span></li><li><a name=ref-36 class=n-a></a><span class=label>36. </span>&nbsp;<span class=citation><a name=d2010e4188 class=n-a></a>Correia JM, Jansma B, Hausfeld L, <i> et al.</i>: EEG decoding of spoken words in bilingual listeners: From words to language invariant semantic-conceptual representations. <i>Front Psychol.</i> 2015; <b>6</b>: 71. <a target=xrefwindow id=d2010e4199 href="http://www.ncbi.nlm.nih.gov/pubmed/25705197">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4202 href="https://doi.org/10.3389/fpsyg.2015.00071">Publisher Full Text </a> | <a target=xrefwindow id=d2010e4206 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4319403">Free Full Text </a></span></li><li><a name=ref-37 class=n-a></a><span class=label>37. </span>&nbsp;<span class=citation><a name=d2010e4215 class=n-a></a>Duma GM, Mento G, Semenzato L, <i> et al.</i>: EEG anticipation of random high and low arousal faces and sounds [version 2; peer review: 1 approved, 1 not approved]. <i>F1000Research.</i> 2018; <b>8</b>: 1508. <a target=xrefwindow id=d2010e4226 href="https://doi.org/10.12688/f1000research.20277.2">Publisher Full Text </a></span></li><li><a name=ref-38 class=n-a></a><span class=label>38. </span>&nbsp;<span class=citation><a name=d2010e4235 class=n-a></a>Mo C, Lu J, Wu B, <i> et al.</i>: Competing rhythmic neural representations of orientations during concurrent attention to multiple orientation features. <i>Nat Commun.</i> 2019; <b>10</b>(1): 5264. <a target=xrefwindow id=d2010e4246 href="http://www.ncbi.nlm.nih.gov/pubmed/31748562">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4249 href="https://doi.org/10.1038/s41467-019-13282-3">Publisher Full Text </a> | <a target=xrefwindow id=d2010e4253 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6868242">Free Full Text </a></span></li><li><a name=ref-39 class=n-a></a><span class=label>39. </span>&nbsp;<span class=citation><a name=d2010e4262 class=n-a></a>Roberts T, Cant JS, Nestor A: Elucidating the Neural Representation and the Processing Dynamics of Face Ensembles. <i>J Neurosci.</i> 2019; <b>39</b>(39): 7737–7747. <a target=xrefwindow id=d2010e4270 href="http://www.ncbi.nlm.nih.gov/pubmed/31413074">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4273 href="https://doi.org/10.1523/JNEUROSCI.0471-19.2019 ">Publisher Full Text </a> | <a target=xrefwindow id=d2010e4276 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6764198">Free Full Text </a></span></li><li><a name=ref-40 class=n-a></a><span class=label>40. </span>&nbsp;<span class=citation><a name=d2010e4285 class=n-a></a>Jain AK, Duin RP, Mao J: Statistical pattern recognition: A review. <i>IEEE Trans Pattern Anal Mach Intell.</i> 2000; <b>22</b>(1): 4–37. <a target=xrefwindow id=d2010e4293 href="https://doi.org/10.1109/34.824819">Publisher Full Text </a></span></li><li><a name=ref-41 class=n-a></a><span class=label>41. </span>&nbsp;<span class=citation><a name=d2010e4302 class=n-a></a>Tang J Alelyani S, Liu H: Feature selection for classification: A review. In <i>Data Classification: Algorithms and Applications</i> . Aggarwal, C.C.,Ed.; 2014; 37–64. <a target=xrefwindow id=d2010e4307 href="https://asu.pure.elsevier.com/en/publications/feature-selection-for-classification-a-review">Reference Source</a></span></li><li><a name=ref-42 class=n-a></a><span class=label>42. </span>&nbsp;<span class=citation><a name=d2010e4317 class=n-a></a>Miao J, Niu L: A Survey on Feature Selection. <i>Procedia Comput Sci.</i> 2016; <b>91</b>: 919–926. <a target=xrefwindow id=d2010e4325 href="https://doi.org/10.1016/j.procs.2016.07.111">Publisher Full Text </a></span></li><li><a name=ref-43 class=n-a></a><span class=label>43. </span>&nbsp;<span class=citation><a name=d2010e4334 class=n-a></a>Müller K, Krauledat M, Dornhege G, <i> et al.</i>: Machine learning techniques for brain-computer interfaces. <i>Biomed Tech (Biomed Tech).</i> 2004; <b>49</b>: 11–22. <a target=xrefwindow id=d2010e4345 href="https://www.google.com/search?client=firefox-b&amp;ei=-C5aXobLHK3az7sPvIye8Ao&amp;q=Machine+learning+techniques+for+brain-computer+interfaces.+2004&amp;oq=Machine+learning+techniques+for+brain-computer+interfaces.+2004&amp;gs_l=psy-ab.3...20402.22856..23076...0.0..0.164.948.1j7......0....2j1..gws-wiz.......0i22i30.JdoWd9xfDug&amp;ved=0ahUKEwiGp9u5ufbnAhUt7XMBHTyGB64Q4dUDCAo&amp;uact=5#">Reference Source</a></span></li><li><a name=ref-44 class=n-a></a><span class=label>44. </span>&nbsp;<span class=citation><a name=d2010e4354 class=n-a></a>Stevenson RA, James TW: Affective auditory stimuli: characterization of the International Affective Digitized Sounds (IADS) by discrete emotional categories. <i>Behav Res Methods.</i> 2008; <b>40</b>(1): 315–21. <a target=xrefwindow id=d2010e4362 href="http://www.ncbi.nlm.nih.gov/pubmed/18411555">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4365 href="https://doi.org/10.3758/brm.40.1.315">Publisher Full Text </a></span></li><li><a name=ref-45 class=n-a></a><span class=label>45. </span>&nbsp;<span class=citation><a name=d2010e4374 class=n-a></a>Stone JV: Independent component analysis: an introduction. <i>Trends Cogn Sci.</i> 2002; <b>6</b>(2): 59–64. <a target=xrefwindow id=d2010e4382 href="http://www.ncbi.nlm.nih.gov/pubmed/15866182">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4385 href="https://doi.org/10.1016/s1364-6613(00)01813-1">Publisher Full Text </a></span></li><li><a name=ref-46 class=n-a></a><span class=label>46. </span>&nbsp;<span class=citation><a name=d2010e4394 class=n-a></a>Allen JJ, Coan JA, Nazarian M: Issues and assumptions on the road from raw signals to metrics of frontal EEG asymmetry in emotion. <i>Biol Psychol.</i> 2004; <b>67</b>(1–2): 183–218. <a target=xrefwindow id=d2010e4402 href="http://www.ncbi.nlm.nih.gov/pubmed/15130531">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4405 href="https://doi.org/10.1016/j.biopsycho.2004.03.007">Publisher Full Text </a></span></li><li><a name=ref-47 class=n-a></a><span class=label>47. </span>&nbsp;<span class=citation><a name=d2010e4414 class=n-a></a>Babiloni C, Stella G, Buffo P, <i> et al.</i>: Cortical sources of resting state EEG rhythms are abnormal in dyslexic children. <i>Clin Neurophysiol.</i> 2012; <b>123</b>(12): 2384–2391. <a target=xrefwindow id=d2010e4425 href="http://www.ncbi.nlm.nih.gov/pubmed/22658819">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4428 href="https://doi.org/10.1016/j.clinph.2012.05.002">Publisher Full Text </a></span></li><li><a name=ref-48 class=n-a></a><span class=label>48. </span>&nbsp;<span class=citation><a name=d2010e4438 class=n-a></a>Mert A, Akan A: Emotion recognition from EEG signals by using multivariate empirical mode decomposition. <i>Pattern Anal Appl.</i> 2018; <b>21</b>: 81–89. <a target=xrefwindow id=d2010e4446 href="https://doi.org/10.1007/s10044-016-0567-6">Publisher Full Text </a></span></li><li><a name=ref-49 class=n-a></a><span class=label>49. </span>&nbsp;<span class=citation><a name=d2010e4455 class=n-a></a>Clarke AR, Barry RJ, Karamacoska D, <i> et al.</i>: The EEG Theta/Beta Ratio: A marker of Arousal or Cognitive Processing Capacity? <i>Appl Psychophysiol Biofeedback.</i> 2019; <b>44</b>(2): 123–129. <a target=xrefwindow id=d2010e4466 href="http://www.ncbi.nlm.nih.gov/pubmed/30604100">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4469 href="https://doi.org/10.1007/s10484-018-09428-6">Publisher Full Text </a></span></li><li><a name=ref-50 class=n-a></a><span class=label>50. </span>&nbsp;<span class=citation><a name=d2010e4478 class=n-a></a>Blankertz B, Lemm S, Treder M, <i> et al.</i>: Single-trial analysis and classification of ERP components - A tutorial. <i>Neuroimage.</i> 2011; <b>56</b>(2): 814–825. <a target=xrefwindow id=d2010e4489 href="http://www.ncbi.nlm.nih.gov/pubmed/20600976">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4492 href="https://doi.org/10.1016/j.neuroimage.2010.06.048">Publisher Full Text </a></span></li><li><a name=ref-51 class=n-a></a><span class=label>51. </span>&nbsp;<span class=citation><a name=d2010e4501 class=n-a></a>Marco B: BAA - Matlab Code (Version 1). <i>Zenodo.</i> 2020. <a target=xrefwindow id=d2010e4506 href="http://www.doi.org/10.5281/zenodo.3666045">http://www.doi.org/10.5281/zenodo.3666045</a></span></li><li><a name=ref-52 class=n-a></a><span class=label>52. </span>&nbsp;<span class=citation><a name=d2010e4515 class=n-a></a>Tottenham N, Tanaka JW, Leon AC, <i> et al.</i>: The NimStim set of facial expressions: Judgments from untrained research participants. <i>Psychiatry Res.</i> 2009; <b>168</b>(3): 242–249. <a target=xrefwindow id=d2010e4526 href="http://www.ncbi.nlm.nih.gov/pubmed/19564050">PubMed Abstract </a> | <a target=xrefwindow id=d2010e4529 href="https://doi.org/10.1016/j.psychres.2008.05.006">Publisher Full Text </a> | <a target=xrefwindow id=d2010e4533 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3474329">Free Full Text </a></span></li></ul></div></div></div> </div> <div class=f1r-article-desk> <section class="o-box o-box--medium u-bg--2 u-mt--2"> <h4 class="u-mt--0 u-mb--2 t-h3 u-weight--md">Looking for the Open Peer Review Reports?</h4> <p class="u-mt--0 u-mb--0 t-h4">They can now be found at the top of the panel on the right, linked from the box entitled Open Peer Review. Choose the reviewer report you wish to read and click the 'read' link. You can also read all the peer review reports by <a target=_blank href="https://f1000research.com/articles/9-173/v1/pdf?article_uuid=5a400a02-ecc2-4eae-a670-e08eb50cd63e">downloading the PDF</a>.</p> </section> </div> <div id=article-comments class="article-comments padding-bottom-20"> <div class=current-article-comment-section> <h2 class=main-title name=add-new-comment id=add-new-comment> <span class="research-layout f1r-article-mobile-inline valign-middle"> <span class="f1r-icon icon-104_comments size30"></span> </span> <span class=f1r-article-desk-inline>Comments on this article</span> <span class=f1r-article-mobile-inline>Comments (0)</span> <span class="f1r-article-mobile-inline float-right"> <span class="f1r-icon icon-14_more_small"></span> <span class="f1r-icon icon-10_less_small"></span> </span> </h2> </div> <div class="f1r-article-desk-inline referee-report-info-box referee-report-version-box"> Version 1 </div> <div class="f1r-article-mobile research-layout mobile-version-info padding-top-30"> <span class=mversion>VERSION 1</span> <span class=details>PUBLISHED 10 Mar 2020</span> <span class="article-pubinfo-mobile versions-section"> </span> </div> <div class="f1r-article-mobile research-layout margin-top-20 is-centered"> <a href="/login?originalPath=/articles/9-173&scrollTo=add-new-comment" class=register-report-comment-button data-test-id=add-comment_mob> <button class="primary orange extra-padding comment-on-this-report">ADD YOUR COMMENT</button> </a> </div> <a href="/login?originalPath=/articles/9-173&scrollTo=add-new-comment" class="f1r-article-desk register-report-comment-button" data-test-id=add-comment> <span class=contracted></span>Comment </a> </div> <div class="f1r-article-mobile margin-bottom-30"> <div class=contracted-details> <a href="#" class="contracted-details-label author-affiliations"><span class=contracted></span>Author details</a> <a href="#" class=section-title>Author details</a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details affiliations is-hidden"> <sup>1</sup> Behavior and BrainLab, Universit&agrave; IULM, Milan, Italy<br/> <sup>2</sup> Department of Developmental and Social Psychology (DPSS), Universit&agrave; degli Studi di Padova, Padova, Italy<br/> <sup>3</sup> Department of General Psychology, Universit&agrave; degli Studi di Padova, Padova, Italy<br/> <sup>4</sup> Science of Consciousness Research Group, Department of General Psychology, Universit&agrave; degli Studi di Padova, Padova, Italy<br/> <p> <div class=margin-bottom> Marco Bilucaglia <br/> <span>Roles: </span> Conceptualization, Data Curation, Formal Analysis, Methodology, Validation, Writing – Original Draft Preparation, Writing – Review & Editing </div> <div class=margin-bottom> Gian Marco Duma <br/> <span>Roles: </span> Conceptualization, Data Curation, Formal Analysis, Methodology, Writing – Original Draft Preparation, Writing – Review & Editing </div> <div class=margin-bottom> Giovanni Mento <br/> <span>Roles: </span> Conceptualization, Supervision, Writing – Review & Editing </div> <div class=margin-bottom> Luca Semenzato <br/> <span>Roles: </span> Software, Validation </div> <div class=margin-bottom> Patrizio E. Tressoldi <br/> <span>Roles: </span> Conceptualization, Supervision, Writing – Review & Editing </div> </p> </div> </div> <div class=contracted-details> <a href="#" class=section-title>Article Versions (1)</a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details grant-information article-page is-hidden"> <div> <a href="https://f1000research.com/articles/9-173/v1" title="Open version 1 of this article." class="" gahelper=1>version 1</a> <span class="article-pubinfo-mobile versions-section"> </span> </div> <div> Published: 10 Mar 2020, 9:173 </div> <div class=""><a href="https://doi.org/10.12688/f1000research.22202.1">https://doi.org/10.12688/f1000research.22202.1</a></div> </div> </div> <div class=contracted-details> <a href="#" class=section-title> <span class="f1r-icon icon-100_open_access"></span> Copyright </a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details grant-information article-page is-hidden"> © 2020 Bilucaglia M <em>et al</em>. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/" target=_blank data-test-id=box-licence-link>Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. </div> </div> <div class="padding-top-30 research-layout"> <div class=article-toolbox-wrapper-mobile> <div class=article-tools-icon-mobile data-section=download> <span class="f1r-icon icon-76_download_file white"></span> </div> <div class="article-tools-icon-mobile mobile-metrics article-metrics-wrapper metrics-icon-wrapper" data-section=metrics data-version-id=24487 data-id=22202 data-downloads="" data-views="" data-scholar="10.12688/f1000research.22202.1" data-recommended="" data-f1r-ga-helper="Article Page Metrics (Mobile)"> <span class="f1r-icon icon-89_metrics white"></span> </div> <div class=article-tools-divider-mobile></div> <div class=article-tools-icon-mobile data-section=cite> <span class="f1r-icon icon-82_quote white"></span> </div> <div class="article-tools-icon-mobile " data-section=track> <span class="f1r-icon icon-90_track white"></span> </div> <div class=article-tools-divider-mobile></div> <div class=article-tools-icon-mobile data-section=share> <span class="f1r-icon icon-34_share white"></span> </div> <span class=article-toolbox-stretch></span> </div> <div class=article-toolbox-content-mobile> <div class="toolbox-section download"> <div class=toolbox-section-heading>Download</div> <div class=toolbox-section-content> <a href="https://f1000research.com/articles/9-173/v1/pdf?article_uuid=5a400a02-ecc2-4eae-a670-e08eb50cd63e" title="Download PDF" class="no-decoration pdf-download-helper"> <span class="f1r-icon icon-102_download_pdf toolbox-section-icon"></span> </a> <div class=toolbox-section-option-divider>&nbsp;</div> <a id=mobile-download-xml class=no-decoration href="#" title="Download XML"> <span class="f1r-icon icon-103_download_xml toolbox-section-icon"></span> </a> </div> <div class=toolbox-section-divider></div> <div class=toolbox-section-heading>Export To</div> <div class=toolbox-section-content> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=WORKSPACE>Sciwheel</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=BIBTEX>Bibtex</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=ENDNOTE>EndNote</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=PROCITE>ProCite</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=REF_MANAGER>Ref. Manager (RIS)</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=SENTE>Sente</button> </div> </div> <div class="toolbox-section metrics"> <div class="toolbox-section-heading no-top-border">metrics</div> <div class="toolbox-section-divider toolbox-section-divider--no-height"></div> <div class=article-metrics-pageinfo> <div class=c-article-metrics-table> <table class=c-article-metrics-table__table> <thead> <tr> <th></th> <th><span class=c-article-metrics-table__heading>Views</span></th> <th><span class=c-article-metrics-table__heading>Downloads</span></th> </tr> </thead> <tbody> <tr> <th class=c-article-metrics-table__row-heading><span class="c-article-metrics-table__platform u-platform--" data-test-id=metrics_platform_name_mob>F1000Research</span></th> <td class="c-article-metrics-table__value js-article-views-count" data-test-id=metrics_platform_views_mob>-</td> <td class="c-article-metrics-table__value js-article-downloads-count" data-test-id=metrics_platform_downloads_mob>-</td> </tr> <tr> <th class=c-article-metrics-table__row-heading> <span class="u-ib u-middle c-article-metrics-table__pmc" data-test-id=metrics_pmc_name_mob>PubMed Central</span> <div class="c-block-tip c-block-tip--centered c-article-metrics-table__tooltip c-block-tip--md-padding c-block-tip--small-arrow u-ib u-middle"> <button type=button class="u-black--medium u-black--high@hover u-ib u-middle c-button--icon c-button--text c-block-tip__toggle"><i class="material-icons c-button--icon__icon">info_outline</i></button> <div class=c-block-tip__content>Data from PMC are received and updated monthly.</div> </div> </th> <td class="c-article-metrics-table__value js-pmc-views-count" data-test-id=metrics_pmc_views_mob>-</td> <td class="c-article-metrics-table__value js-pmc-downloads-count" data-test-id=metrics_pmc_downloads_mob>-</td> </tr> </tbody> </table> </div> </div> <span class=metrics-citations-container> <div class=toolbox-section-divider></div> <div class="toolbox-section-heading u-mb--1">Citations</div> <div> <div class=toolbox-section-colsplit> <div class=citations-scopus-logo> <a href="" target=_blank class="is-hidden metrics-citation-icon" title="View full citation details at www.scopus.com"><i class="material-icons scopus-icon">open_in_new</i></a> </div> <div class="toolbox-section-count scopus"> <a href='' class='scopus-citation-link is-hidden' target=_blank title='View full citation details at www.scopus.com'>0</a> </div> </div> <div class=toolbox-section-colsplit> <div class="citations-pubmed-logo f1000research"> <a href="" target=_blank class="is-hidden metrics-citation-icon f1000research" title="View full citation details"><i class="material-icons scopus-icon">open_in_new</i></a> </div> <div class="toolbox-section-count pubmed"> <a href='' class='pubmed-citation-link is-hidden' target=_blank title='View full citation details'>0</a> </div> </div> <div class=toolbox-section-divider></div> <div class=toolbox-section-content> <div class="citations-scholar-logo f1000research"> <a href="" target=_blank class="is-hidden metrics-citation-icon google-scholar f1000research" title="View full citation details" data-scholar="10.12688/f1000research.22202.1"><i class="material-icons scopus-icon">open_in_new</i></a> </div> </div> </div> </span> <span class=metrics-details-container> <div class=toolbox-section-divider></div> <div class="toolbox-section-content altmetric-section"> <div class=altmetrics-image></div> <div class=altmetrics-more-link> <a href="" target=_blank class=f1r-standard-link>SEE MORE DETAILS</a> </div> <div class=altmetric-mobile-column-counts></div> <div class=altmetric-mobile-column-readers></div> <div class=toolbox-section-divider></div> </div> </span> </div> <div class="toolbox-section cite"> <div class="toolbox-section-heading no-top-border">CITE</div> <div class=toolbox-section-divider></div> <div class=toolbox-section-heading>how to cite this article</div> <div id=citation-copy-mobile class="toolbox-section-content text-content heading9 small" data-test-id=mob_copy-citation_text> Bilucaglia M, Duma GM, Mento G <em>et al.</em> Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity [version 1; peer review: awaiting peer review] <i>F1000Research</i> 2020, <b>9</b>:173 (<a href="https://doi.org/10.12688/f1000research.22202.1" target=_blank>https://doi.org/10.12688/f1000research.22202.1</a>) </div> <div class=toolbox-section-divider></div> <div class="toolbox-section-content text-content heading9 small"> NOTE: <em>it is important to ensure the information in <b>square brackets after the title</b> is included in all citations of this article.</em> </div> <div class=toolbox-section-content> <button class="primary orange extra-padding copy-cite-article-mobile js-clipboard" data-clipboard-target="#citation-copy-mobile" title="Copy the current citation details." data-test-id=mob_copy-citation_button-mob>COPY CITATION DETAILS</button> </div> </div> <div class="toolbox-section track"> <div class="toolbox-section-heading no-top-border">track</div> <div class=toolbox-section-divider></div> <div class=toolbox-section-heading>receive updates on this article</div> <div class="toolbox-section-content padding-left-20 padding-right-20 heading9 small"> Track an article to receive email alerts on any updates to this article. </div> <div class=toolbox-section-content> <a data-article-id=22202 id=mobile-track-article-signin-22202 title="Receive updates on new activity such as publication of new versions, peer reviews or author responses." href="/login?originalPath=/trackArticle/22202?target=/articles/9-173"> <button class="primary orange extra-padding"> TRACK THIS ARTICLE </button> </a> </div> </div> <div class="toolbox-section share"> <div class="toolbox-section-heading no-top-border">Share</div> <div class=toolbox-section-divider></div> <div class=toolbox-section-content> <a target=_blank class="f1r-shares-icon-square f1r-shares-email" title="Email this article"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-twitter" title="Share on Twitter"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-facebook" title="Share on Facebook"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-linkedin" title="Share on LinkedIn"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-reddit" title="Share on Reddit"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-mendelay" title="Share on Mendeley"></a> <div class="email-article-wrapper email-article-version-container"> <div class=toolbox-section-divider></div> <script src='https://www.recaptcha.net/recaptcha/api.js'></script> <form class="recommend-version-form-mobile research-layout"> <p>All fields are required.</p> <input name=versionId type=hidden value=24487 /> <input name=articleId type=hidden value=22202 /> <input name=senderName class="form-input-field reg-form" value="" type=text placeholder="Your name"/> <input name=senderEmail class="form-input-field reg-form margin-top" value="" type=text placeholder="Your email address"/> <textarea name=recipientEmails class="form-textarea-field ninetynine-percent-wide margin-top no-resize" placeholder="Recipient email address(es) (comma delimited)"></textarea> <input class="form-input-field reg-form margin-top" name=subject type=text value="Interesting article on F1000Research" placeholder=Subject /> <textarea name=message class="form-textarea-field reg-form margin-top no-resize">I thought this article from F1000Research (https://f1000research.com) would be of interest to you.</textarea> <div class="g-recaptcha margin-top" data-sitekey=6LcHqxoUAAAAANP3_0TzpGG6qFvl4DhbUcuRzw7W></div> <input value="" name=captcha type=hidden /> <p>A full article citation will be automatically included.</p> <p><img class="ticker-email-article-details hidden" src="/img/ticker.gif" alt=loading /></p> <button class="secondary orange margin-bottom" data-test-id=version_share_email_send>SEND EMAIL</button> <div class="orange-message margin-bottom is-hidden" data-test-id=version_share_email_message></div> </form> </div> </div> </div> </div> </div> <a name=article-reports></a> <div id=article-reports class="u-mt--3 reports-comments no-divider"> <div class="current-referee-status no-border-and-margin "> <h2 class=main-title id=current-referee-status> <span class="research-layout f1r-article-mobile-inline valign-middle"> <span class="f1r-icon icon-85_peer_review size30"></span> </span> Open Peer Review <span class="f1r-article-mobile-inline float-right"> <span class="f1r-icon icon-14_more_small"></span> <span class="f1r-icon icon-10_less_small"></span> </span> </h2> <a name=current-referee-status></a> <div class=current-referee-status__content name=add-new-report-comment id=add-new-report-comment> Current Reviewer Status: <div class=f1r-article-desk-inline><em>AWAITING PEER REVIEW</em></div> <div class="f1r-article-mobile-inline float-right"><em>AWAITING PEER REVIEW</em></div> <span class="circle-icon-small to-right" data-window=about-referee-status title=Help>?</span> <span class="research-layout f1r-article-mobile"> <div class=mobile-ref-status-help> Key to Reviewer Statuses <span class=referee-status-pointer></span> <span class="view-control float-right">VIEW</span> <span class="view-control float-right is-hidden">HIDE</span> <div class=mobile-ref-status-help-content> <div class="cf margin-top"> <span class="f1r-icon icon-86_approved status-green smaller float-left margin-bottom-40 margin-right" title=Approved></span> <span class=title>Approved</span>The paper is scientifically sound in its current form and only minor, if any, improvements are suggested </div> <div class="cf margin-top"> <span class="f1r-icon icon-87_approved_reservations status-green smaller float-left margin-bottom-40 margin-right" title="Approved with Reservations"></span> <span class=title>Approved with reservations</span> A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit. </div> <div class="cf margin-top"> <span class="f1r-icon icon-88_not_approved status-red small float-left margin-bottom-30 margin-right" title="Not Approved"></span> <span class=title>Not approved</span>Fundamental flaws in the paper seriously undermine the findings and conclusions </div> </div> </div> </span> </div> </div> </div> <div class="f1r-article-mobile research-layout"> <div class="mobile-sections-divider before-comments"></div> </div> <div id=article-comments class="article-comments padding-bottom-20"> <div class=current-article-comment-section> <h2 class=main-title name=add-new-comment id=add-new-comment> <span class="research-layout f1r-article-mobile-inline valign-middle"> <span class="f1r-icon icon-104_comments size30"></span> </span> <span class=f1r-article-desk-inline>Comments on this article</span> <span class=f1r-article-mobile-inline>Comments (0)</span> <span class="f1r-article-mobile-inline float-right"> <span class="f1r-icon icon-14_more_small"></span> <span class="f1r-icon icon-10_less_small"></span> </span> </h2> </div> <div class="f1r-article-desk-inline referee-report-info-box referee-report-version-box"> Version 1 </div> <div class="f1r-article-mobile research-layout mobile-version-info padding-top-30"> <span class=mversion>VERSION 1</span> <span class=details>PUBLISHED 10 Mar 2020</span> <span class="article-pubinfo-mobile versions-section"> </span> </div> <div class="f1r-article-mobile research-layout margin-top-20 is-centered"> <a href="/login?originalPath=/articles/9-173&scrollTo=add-new-comment" class=register-report-comment-button data-test-id=add-comment_mob> <button class="primary orange extra-padding comment-on-this-report">ADD YOUR COMMENT</button> </a> </div> <a href="/login?originalPath=/articles/9-173&scrollTo=add-new-comment" class="f1r-article-desk register-report-comment-button" data-test-id=add-comment> <span class=contracted></span>Comment </a> </div> </div> </div> <div id=article_main-column class="p-article__sidebar o-layout__item u-1/3 not-expanded js-article-sidebar"> <div class="o-tab p-article__column-toggle-container"> <button class="c-tab c-tab--left js-column-toggle p-article__column-toggle not-expanded " type=button data-target-main=article_main-column data-target-secondary=article_secondary-column><i class="c-tab__icon material-icons u-hide@expanded">keyboard_arrow_left</i><i class="c-tab__icon material-icons u-show@expanded">keyboard_arrow_right</i></button> </div> <div class=p-article__sidebar-content> <div class="p-article__sidebar-scroller js-article-sidebar-scroller"> <section class="p-article__sidebar-view js-article-sidebar-view js-article-sidebar-main u-pt u-pb--8" data-view=peer-review> <div class="o-layout o-layout--flush"> <div class="o-layout__item u-pl"> <h3 class="u-mt--0 u-mb--2 t-h3 u-weight--md u-pl" data-test-id=article_sidebar_heading>Open Peer Review</h3> </div> <div class=o-layout__item> <div class="p-article__sidebar-highlight u-mb--2"> <h4 class="u-mt--0 u-mb--0 u-ib u-middle t-h4 u-weight--md u-mr--1/2">Reviewer Status</h4> <div class="u-ib u-middle"><em>AWAITING PEER REVIEW</em></div> </div> </div> <section class="o-layout__item u-pl"> <div class=u-pl> <hr class="c-hr c-hr--low c-hr--md u-mb--3"> <h4 class="t-h3 u-weight--md u-mt--0 u-mb--2">Comments on this article</h4> <div class=u-mb--4> <p class="u-mt--0 u-mb--1 t-h4"><a class=p-article__color--light href="#article-comments">All Comments</a><span class=" u-ib u-ml--1/2 p-article__color--light">(0)</span></p> <a class=t-h4 href="/login?originalPath=/articles/9-173&scrollTo=add-new-comment" data-test-id=add-comment>Add a comment</a> </div> <hr class="c-hr c-hr--low c-hr--md u-mb--4"> <div class="research-layout f1r-article-desk"> <div class="heading6 c-ribbon-wrapper c-ribbon-wrapper--etoc f1000research "> <div class=c-ribbon-wrapper__body>Sign up for content alerts</div> </div> </div> <div class="research-layout sidebar-sign-up-form f1r-article-desk u-mb--4 "> <form class=js-email-alert-signup action="#" method=POST data-email=tocAlertWeekly> <input type=hidden name=isUserLoggedIn class=js-email-alert-signup-logged-in value=N /> <input type=hidden name=userId class=js-email-alert-signup-user-id value=""/> <input type=hidden name=frequency class=js-email-alert-signup-frequency value=WEEKLY /> <div class="o-actions o-actions--middle"> <div class=o-actions__primary> <input type=email name=emailAddress class="form-input-field js-email-alert-signup-address u-1/1 u-bb" required=required placeholder=Email /> </div> <div class=o-actions__secondary> <div class="_mdl-layout u-ml--1/2"> <button class="mdl-button mdl-js-button mdl-button--colored mdl-button--small mdl-button--filled js-email-alert-signup-submit">Sign Up</button> </div> </div> </div> </form> <div id=sidebar-sign-up-message class="section-text js-email-alert-signup-msg is-hidden">You are now signed up to receive this alert</div> </div> <section class=js-terms-container> <hr class="c-hr c-hr--low c-hr--md u-mb--3"> <h4 class="t-h3 u-weight--md u-mt--0 u-mb--1">Browse by related subjects</h4> <div class="article-subcontainer article-subcontainer--sidebar"> <ul class=js-terms-list></ul> </div> </section> </div> </section> </div> </section> </div> <script src="/js/shared_scripts/modal-dialogue.js"></script> <script src="/js/shared_scripts/read-more.js"></script> <script src="/js/article/article-router.js"></script> <script src="/js/article/article-sidebar.js"></script> <script src="/js/referee/new/referee_helpers.js"></script> <script src="/js/article/article-column-toggle.js"></script> </div> </div> </div> </main> <input type=hidden id=_articleVersionUrl value="https://f1000research.com/articles/9-173/v1/"> <div class=research-help id=about-referee-status> <div class="research-layout research-help-content about-referee-status"> <span class="close-research-help dark-cross" title=Close></span> Alongside their report, reviewers assign a status to the article: <div class="cf research-help-row"> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved></span> <span class=research-help-text>Approved - the paper is scientifically sound in its current form and only minor, if any, improvements are suggested</span> </div> <div class="cf research-help-row"> <span class="f1r-icon icon-87_approved_reservations status-green smaller" title="Approved with Reservations"></span> <span class=research-help-text>Approved with reservations - A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit. </span> </div> <div class="cf research-help-row"> <span class="f1r-icon icon-88_not_approved status-red small" title="Not Approved"></span> <span class=research-help-text>Not approved - fundamental flaws in the paper seriously undermine the findings and conclusions</span> </div> </div> </div> <div id=datasets-info class=is-hidden> </div> <div class=article-interactive-content-container style="display: none;"> <a name=f-template class=n-a></a> <div class="interactive-content-wrapper padding-20"> <img src="" class=interactive-content-image title="Open the interactive image display"> <div class=interactive-content-title></div> <div class=interactive-content-text></div> <div class="f1r-article-desk interactive-content-ribbon" data-interactive-content-type=R-Script> <div class=interactive-content-label>Adjust parameters to alter display</div> <div class=interactive-content-button></div> </div> <div class="f1r-article-mobile mobile-interactive-note"> View on desktop for interactive features <img src="/img/icon/interactive_content.png" class="float-right margin-right-40"/> </div> <div class=clearfix></div> </div> </div> <div id=article-interactive-omero-container class=article-interactive-omero-container style="display: none;"> <div class=interactive-content-wrapper> <div class="interactive-omero-button omero-content" title="Open the interactive content window." data-interactive-content-type=Omero></div> <div class=has-interactive-content-image> <span class=box-arrow></span> <span class=box-middle>Includes Interactive Elements</span> <span class=box-end></span> </div> <div class="fig panel clearfix" style="margin: 0; padding-bottom: 20px;"> <a name=templatelink class=n-a></a> <a target=_blank href="" class=link-for-omero-image> <img src="" class=interactive-omero-image title="Open the image display window."> </a> <div class=caption> <div class=interactive-content-title></div> <div class=interactive-content-text></div> </div> <div class="is-hidden omero-image-list"></div> </div> <div class="f1r-article-mobile mobile-interactive-note omero"> View on desktop for interactive features <img src="/img/icon/interactive_content.png" class="float-right margin-right-40"/> </div> <div class=clearfix></div> </div> </div> <div class="add-comment-container shadow-box is-hidden" id=save-comment-container> <span id=save-comment-text class=intro-text>Edit comment</span> <textarea id=new-comment name=new-comment class="global-textarea comment margin-bottom margin-top"></textarea> <p><strong>Competing Interests</strong></p> <textarea id=new-competing-interests name=competing-interests class="global-textarea competing-interests margin-bottom check-xss"></textarea> <div class=clearfix></div> <button id=cancelComment type=button class="general-white-orange-button float-right no-background-button margin-left"> Cancel </button> <button id=save-comment-button commentId="" type=button class="general-white-orange-button float-right"> Save </button> <div class=clearfix></div> <div class="green-message margin-top is-hidden comment-is-saved">The comment has been saved.</div> <div class="red-message margin-top is-hidden comment-not-added">An error has occurred. Please try again.</div> <div class="red-message margin-top is-hidden comment-enter-text ucf">Your must enter a comment.</div> <div class="red-message margin-top is-hidden comment-references-error references">References error.</div> </div> <div class="modal-window-wrapper is-hidden"> <div id=conflicts-interests class="modal-window padding-20"> <div class=modal-window__content> <h2 class=h2-title>Competing Interests Policy</h2> <p> Provide sufficient details of any financial or non-financial competing interests to enable users to assess whether your comments might lead a reasonable person to question your impartiality. Consider the following examples, but note that this is not an exhaustive list: </p> <div class=heading5>Examples of 'Non-Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>Within the past 4 years, you have held joint grants, published or collaborated with any of the authors of the selected paper.</li> <li class=standard-padding>You have a close personal relationship (e.g. parent, spouse, sibling, or domestic partner) with any of the authors.</li> <li class=standard-padding>You are a close professional associate of any of the authors (e.g. scientific mentor, recent student).</li> <li class=standard-padding>You work at the same institute as any of the authors.</li> <li class=standard-padding>You hope/expect to benefit (e.g. favour or employment) as a result of your submission.</li> <li class=standard-padding>You are an Editor for the journal in which the article is published.</li> </ol> <div class="heading5 padding-top">Examples of 'Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>You expect to receive, or in the past 4 years have received, any of the following from any commercial organisation that may gain financially from your submission: a salary, fees, funding, reimbursements.</li> <li class=standard-padding>You expect to receive, or in the past 4 years have received, shared grant support or other funding with any of the authors.</li> <li class=standard-padding>You hold, or are currently applying for, any patents or significant stocks/shares relating to the subject matter of the paper you are commenting on.</li> </ol> </div> <div class="f1r-article-mobile research-layout"> <span class=modal-window-close-button></span> </div> </div> </div> <div class="o-modal c-email-alert-popup js-email-alert-popup is-hidden"> <div class="o-modal__body js-modal-body c-email-alert-popup__inner"> <h3 class=c-email-alert-popup__title>Stay Updated</h3> <p class=c-email-alert-popup__sub>Sign up for content alerts and receive a weekly or monthly email with all newly published articles</p> <p><a href="/register?originalPath=" class="c-email-alert-popup__lnk c-email-alert-popup__button js-email-alert-popup-action">Register with F1000Research</a></p> <p>Already registered? <a href="/login?originalPath=" class="c-email-alert-popup__lnk js-email-alert-popup-action">Sign in</a></p> <p class=c-email-alert-popup__footer><a class="c-email-alert-popup__lnk js-email-alert-popup-cancel" href="#">Not now, thanks</a></p> </div> </div> <div id=addCommentModal role=dialog aria-labelledby=addCommentModal_title aria-describedby=addCommentModal_description> <div class="c-modal js-modal is-closed p-article__add-comment-modal c-modal--xlarge js-article-comment-modal c-modal--scroll "> <aside class="c-modal__content u-black--high o-box u-pb--0 u-bg--2 c-modal--scroll-always "> <div class=c-modal__close> <button type=button class="c-button c-button--icon c-button--medium c-button--full@hover js-modal__close"><i class="c-button--icon__icon material-icons">close</i></button> </div> <div id=addCommentModal_description class="c-modal__description js-modal__content t-h4 u-mt--0 u-mb--2 u-black--medium"> <div class="js-add-comment-container t-caption u-black--high"> <div class=u-weight--bd>PLEASE NOTE</div> <div class=u-mt--1> <span class="red u-weight--bd">If you are an AUTHOR of this article,</span> please check that you signed in with the account associated with this article otherwise we cannot automatically identify your role as an author and your comment will be labelled as a &ldquo;User Comment&rdquo;. </div> <div class=u-mt--1> <span class="red u-weight--bd">If you are a REVIEWER of this article,</span> please check that you have signed in with the account associated with this article and then go to your <a href="/my/referee">account</a> to submit your report, please do not post your review here. </div> <div class="u-mt--1 u-mb--1"> If you do not have access to your original account, please <a href="mailto:research@f1000.com">contact us</a>. </div> <p class=no-top-margin>All commenters must hold a formal affiliation as per our <a href="/about/policies#commentspolicy" target=_blank>Policies</a>. The information that you give us will be displayed next to your comment.</p> <p>User comments must be in English, comprehensible and relevant to the article under discussion. We reserve the right to remove any comments that we consider to be inappropriate, offensive or otherwise in breach of the <a href="/about/legal/usercommenttermsandconditions" target=_blank>User Comment Terms and Conditions</a>. Commenters must not use a comment for personal attacks. When criticisms of the article are based on unpublished data, the data should be made available.</p> <div class="comments-note margin-bottom" id=accept-user-comments> <input class=js-add-comment-accept-terms type=checkbox id=acceptedTermsAndConditions name=acceptedTermsAndConditions> I accept the <a href="/about/legal/usercommenttermsandconditions" target=_blank> User Comment Terms and Conditions</a> <span class=required>&nbsp;</span> </div> <div class="default-error margin-top is-hidden comment-accept-conditions utac">Please confirm that you accept the User Comment Terms and Conditions.</div> <div class="research-layout registration-form u-mb--2"> <div class="u-mb--1 u-mt--2"> <strong>Affiliation</strong> </div> <div class=form-field> <input type=text name=institution class="form-input-field check-xss js-add-comment-institution" placeholder="Organization *" autocomplete=off /> <div class="default-error margin-top is-hidden comment-enter-institution institution">Please enter your organisation.</div> </div> <div class=form-field> <input type=text name=place class="form-input-field check-xss js-add-comment-place" placeholder=Place> </div> <div class=form-field> <div class="form-input-wrapper hundred-percent-wide"> <div class="new-select-standard-wrapper half-width inline-display heading10"> <select name=countryId id=country class="form-select-menu smaller js-add-comment-country"> <option value=-1>Country*</option> <option value=840>USA</option> <option value=826>UK</option> <option value=124>Canada</option> <option value=156>China</option> <option value=250>France</option> <option value=276>Germany</option> <optgroup label=-----------------------------------------------></optgroup> <option value=4>Afghanistan</option> <option value=248>Aland Islands</option> <option value=8>Albania</option> <option value=12>Algeria</option> <option value=16>American Samoa</option> <option value=20>Andorra</option> <option value=24>Angola</option> <option value=660>Anguilla</option> <option value=10>Antarctica</option> <option value=28>Antigua and Barbuda</option> <option value=32>Argentina</option> <option value=51>Armenia</option> <option value=533>Aruba</option> <option value=36>Australia</option> <option value=40>Austria</option> <option value=31>Azerbaijan</option> <option value=44>Bahamas</option> <option value=48>Bahrain</option> <option value=50>Bangladesh</option> <option value=52>Barbados</option> <option value=112>Belarus</option> <option value=56>Belgium</option> <option value=84>Belize</option> <option value=204>Benin</option> <option value=60>Bermuda</option> <option value=64>Bhutan</option> <option value=68>Bolivia</option> <option value=70>Bosnia and Herzegovina</option> <option value=72>Botswana</option> <option value=74>Bouvet Island</option> <option value=76>Brazil</option> <option value=86>British Indian Ocean Territory</option> <option value=92>British Virgin Islands</option> <option value=96>Brunei</option> <option value=100>Bulgaria</option> <option value=854>Burkina Faso</option> <option value=108>Burundi</option> <option value=116>Cambodia</option> <option value=120>Cameroon</option> <option value=124>Canada</option> <option value=132>Cape Verde</option> <option value=136>Cayman Islands</option> <option value=140>Central African Republic</option> <option value=148>Chad</option> <option value=152>Chile</option> <option value=156>China</option> <option value=162>Christmas Island</option> <option value=166>Cocos (Keeling) Islands</option> <option value=170>Colombia</option> <option value=174>Comoros</option> <option value=178>Congo</option> <option value=184>Cook Islands</option> <option value=188>Costa Rica</option> <option value=384>Cote d'Ivoire</option> <option value=191>Croatia</option> <option value=192>Cuba</option> <option value=196>Cyprus</option> <option value=203>Czech Republic</option> <option value=180>Democratic Republic of the Congo</option> <option value=208>Denmark</option> <option value=262>Djibouti</option> <option value=212>Dominica</option> <option value=214>Dominican Republic</option> <option value=218>Ecuador</option> <option value=818>Egypt</option> <option value=222>El Salvador</option> <option value=226>Equatorial Guinea</option> <option value=232>Eritrea</option> <option value=233>Estonia</option> <option value=231>Ethiopia</option> <option value=238>Falkland Islands</option> <option value=234>Faroe Islands</option> <option value=583>Federated States of Micronesia</option> <option value=242>Fiji</option> <option value=246>Finland</option> <option value=250>France</option> <option value=254>French Guiana</option> <option value=258>French Polynesia</option> <option value=260>French Southern Territories</option> <option value=266>Gabon</option> <option value=268>Georgia</option> <option value=276>Germany</option> <option value=288>Ghana</option> <option value=292>Gibraltar</option> <option value=300>Greece</option> <option value=304>Greenland</option> <option value=308>Grenada</option> <option value=312>Guadeloupe</option> <option value=316>Guam</option> <option value=320>Guatemala</option> <option value=831>Guernsey</option> <option value=324>Guinea</option> <option value=624>Guinea-Bissau</option> <option value=328>Guyana</option> <option value=332>Haiti</option> <option value=334>Heard Island and Mcdonald Islands</option> <option value=336>Holy See (Vatican City State)</option> <option value=340>Honduras</option> <option value=344>Hong Kong</option> <option value=348>Hungary</option> <option value=352>Iceland</option> <option value=356>India</option> <option value=360>Indonesia</option> <option value=364>Iran</option> <option value=368>Iraq</option> <option value=372>Ireland</option> <option value=376>Israel</option> <option value=380>Italy</option> <option value=388>Jamaica</option> <option value=392>Japan</option> <option value=832>Jersey</option> <option value=400>Jordan</option> <option value=398>Kazakhstan</option> <option value=404>Kenya</option> <option value=296>Kiribati</option> <option value=901>Kosovo (Serbia and Montenegro)</option> <option value=414>Kuwait</option> <option value=417>Kyrgyzstan</option> <option value=418>Lao People's Democratic Republic</option> <option value=428>Latvia</option> <option value=422>Lebanon</option> <option value=426>Lesotho</option> <option value=430>Liberia</option> <option value=434>Libya</option> <option value=438>Liechtenstein</option> <option value=440>Lithuania</option> <option value=442>Luxembourg</option> <option value=446>Macao</option> <option value=807>Macedonia</option> <option value=450>Madagascar</option> <option value=454>Malawi</option> <option value=458>Malaysia</option> <option value=462>Maldives</option> <option value=466>Mali</option> <option value=470>Malta</option> <option value=584>Marshall Islands</option> <option value=474>Martinique</option> <option value=478>Mauritania</option> <option value=480>Mauritius</option> <option value=175>Mayotte</option> <option value=484>Mexico</option> <option value=581>Minor Outlying Islands of the United States</option> <option value=498>Moldova</option> <option value=492>Monaco</option> <option value=496>Mongolia</option> <option value=499>Montenegro</option> <option value=500>Montserrat</option> <option value=504>Morocco</option> <option value=508>Mozambique</option> <option value=104>Myanmar</option> <option value=516>Namibia</option> <option value=520>Nauru</option> <option value=524>Nepal</option> <option value=530>Netherlands Antilles</option> <option value=540>New Caledonia</option> <option value=554>New Zealand</option> <option value=558>Nicaragua</option> <option value=562>Niger</option> <option value=566>Nigeria</option> <option value=570>Niue</option> <option value=574>Norfolk Island</option> <option value=580>Northern Mariana Islands</option> <option value=408>North Korea</option> <option value=578>Norway</option> <option value=512>Oman</option> <option value=586>Pakistan</option> <option value=585>Palau</option> <option value=275>Palestinian Territory</option> <option value=591>Panama</option> <option value=598>Papua New Guinea</option> <option value=600>Paraguay</option> <option value=604>Peru</option> <option value=608>Philippines</option> <option value=612>Pitcairn</option> <option value=616>Poland</option> <option value=620>Portugal</option> <option value=630>Puerto Rico</option> <option value=634>Qatar</option> <option value=638>Reunion</option> <option value=642>Romania</option> <option value=643>Russian Federation</option> <option value=646>Rwanda</option> <option value=654>Saint Helena</option> <option value=659>Saint Kitts and Nevis</option> <option value=662>Saint Lucia</option> <option value=666>Saint Pierre and Miquelon</option> <option value=670>Saint Vincent and the Grenadines</option> <option value=882>Samoa</option> <option value=674>San Marino</option> <option value=678>Sao Tome and Principe</option> <option value=682>Saudi Arabia</option> <option value=686>Senegal</option> <option value=688>Serbia</option> <option value=690>Seychelles</option> <option value=694>Sierra Leone</option> <option value=702>Singapore</option> <option value=703>Slovakia</option> <option value=705>Slovenia</option> <option value=90>Solomon Islands</option> <option value=706>Somalia</option> <option value=710>South Africa</option> <option value=239>South Georgia and the South Sandwich Is</option> <option value=410>South Korea</option> <option value=724>Spain</option> <option value=144>Sri Lanka</option> <option value=736>Sudan</option> <option value=740>Suriname</option> <option value=744>Svalbard and Jan Mayen</option> <option value=748>Swaziland</option> <option value=752>Sweden</option> <option value=756>Switzerland</option> <option value=760>Syria</option> <option value=158>Taiwan</option> <option value=762>Tajikistan</option> <option value=834>Tanzania</option> <option value=764>Thailand</option> <option value=270>The Gambia</option> <option value=528>The Netherlands</option> <option value=626>Timor-Leste</option> <option value=768>Togo</option> <option value=772>Tokelau</option> <option value=776>Tonga</option> <option value=780>Trinidad and Tobago</option> <option value=788>Tunisia</option> <option value=792>Turkey</option> <option value=795>Turkmenistan</option> <option value=796>Turks and Caicos Islands</option> <option value=798>Tuvalu</option> <option value=800>Uganda</option> <option value=826>UK</option> <option value=804>Ukraine</option> <option value=784>United Arab Emirates</option> <option value=850>United States Virgin Islands</option> <option value=858>Uruguay</option> <option value=840>USA</option> <option value=860>Uzbekistan</option> <option value=548>Vanuatu</option> <option value=862>Venezuela</option> <option value=704>Vietnam</option> <option value=876>Wallis and Futuna</option> <option value=905>West Bank and Gaza Strip</option> <option value=732>Western Sahara</option> <option value=887>Yemen</option> <option value=894>Zambia</option> <option value=716>Zimbabwe</option> </select> </div> </div> <div class="default-error margin-top is-hidden comment-enter-country country">Please select your country.</div> </div> </div> <textarea data-test-id=article_add-comment_comment name=new-comment class="js-add-comment-comment comment margin-bottom margin-top"></textarea> <div class="default-error margin-top comment-enter-text comment-error is-hidden ">You must enter a comment.</div> <label class="comments-note u-mt--2 u-mb--2" for=competingInterests_1> <div class="u-mb--1 u-mt--2"><strong data-test-id=article_report-add-comment_competing-interests-title>Competing Interests</strong></div> <p class="u-mb--2 u-mt--0" data-test-id=article_report-add-comment_competing-interests-description>Please disclose any <a href="#article-competing-intersts-policy" class=js-modal-competing-intersts-toggle>competing interests</a> that might be construed to influence your judgment of the article's or peer review report's validity or importance.</p> </label> <div id=article-competing-intersts-policy class=js-article-competing-interests-policy style="display: none;"> <h2 class="h2-title u-mt--0 u-pt--0">Competing Interests Policy</h2> <p> Provide sufficient details of any financial or non-financial competing interests to enable users to assess whether your comments might lead a reasonable person to question your impartiality. Consider the following examples, but note that this is not an exhaustive list: </p> <div class=heading5>Examples of 'Non-Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>Within the past 4 years, you have held joint grants, published or collaborated with any of the authors of the selected paper.</li> <li class=standard-padding>You have a close personal relationship (e.g. parent, spouse, sibling, or domestic partner) with any of the authors.</li> <li class=standard-padding>You are a close professional associate of any of the authors (e.g. scientific mentor, recent student).</li> <li class=standard-padding>You work at the same institute as any of the authors.</li> <li class=standard-padding>You hope/expect to benefit (e.g. favour or employment) as a result of your submission.</li> <li class=standard-padding>You are an Editor for the journal in which the article is published.</li> </ol> <div class="heading5 padding-top">Examples of 'Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>You expect to receive, or in the past 4 years have received, any of the following from any commercial organisation that may gain financially from your submission: a salary, fees, funding, reimbursements.</li> <li class=standard-padding>You expect to receive, or in the past 4 years have received, shared grant support or other funding with any of the authors.</li> <li class=standard-padding>You hold, or are currently applying for, any patents or significant stocks/shares relating to the subject matter of the paper you are commenting on.</li> </ol> </div> <div id=competingInterests_1 class="c-inline-editor js-inline-editor js-form_input u-bb--all u-mb--2 js-comment-competing-interests" data-name=competing-interests data-rows=3> <textarea id=competingInterests_1_input name=competing-interests placeholder="&nbsp;" required=false class="u-hide-visually js-inline-editor_input" tabindex=-1></textarea> <div class="c-inline-editor__editor js-inline-editor_editor o-box c-box o-box--tiny u-bg--11" data-target=competingInterests_1_input role=textbox required=false data-placeholder="&nbsp;" contenteditable=true></div> <span class="c-inline-editor__error js-inline-editor-message">Please state your competing interests</span> </div> <div data-test-id=article_add-comment_saved class="green-message comments is-hidden comment-is-saved">The comment has been saved.</div> <div data-test-id=article_add-comment_error class="default-error comments is-hidden comment-not-added">An error has occurred. Please try again.</div> <div class=clearfix></div> <div class=js-hook></div> </div> <div class="c-modal__extra-message js-modal__extra-message t-h4 u-black--medium"></div></div> <div class="c-modal__actions o-box__actions"> <a href="#" data-test-id=article_add-comment_cancel class="c-button c-button--full js-modal__close c-button--secondary">Cancel</a> <a href="#" data-test-id=article_add-comment_post class="c-button c-button--full js-modal__confirm c-button--primary">Post</a> </div> </aside> </div> </div> <style>
                .at-icon-wrapper {
        background-size: 100% !important;
    }
</style> <script src="/js/namespace.js"></script> <script src="/js/constants.js"></script> <script src="/js/utilities.js"></script> <script src="/js/article/alert-signup.js"></script> <script type='text/javascript'>
    var lTitle = "Applying machine learning EEG signal classification...".replace("'", '');
    var linkedInUrl = "http://www.linkedin.com/shareArticle?url=https://f1000research.com/articles/9-173/v1" + "&title=" + encodeURIComponent(lTitle) + "&summary=" + encodeURIComponent('Read the article by ');

    var deliciousUrl = "https://del.icio.us/post?url=https://f1000research.com/articles/9-173/v1&title=" + encodeURIComponent(lTitle);

    var redditUrl = "http://reddit.com/submit?url=https://f1000research.com/articles/9-173/v1" + "&title=" + encodeURIComponent(lTitle);

            linkedInUrl += encodeURIComponent('Bilucaglia M et al.');
    
    var offsetTop = /chrome/i.test( navigator.userAgent ) ? 4 : -10; 
    var addthis_config = {
            ui_offset_top: offsetTop,
                                    services_compact : "facebook,twitter,www.linkedin.com,www.mendeley.com,reddit.com",
            services_expanded : "facebook,twitter,www.linkedin.com,www.mendeley.com,reddit.com",
            services_custom : [
                {
                    name: "LinkedIn",
                    url:  linkedInUrl,
                    icon:"/img/icon/at_linkedin.svg"
                },
                {
                    name: "Mendeley",
                    url:  "http://www.mendeley.com/import/?url=https://f1000research.com/articles/9-173/v1/mendeley",
                    icon:"/img/icon/at_mendeley.svg"
                },
                {
                    name: "Reddit",
                    url:  redditUrl,
                    icon:"/img/icon/at_reddit.svg"
                },
            ]
        };


    var addthis_share = {
            url: "https://f1000research.com/articles/9-173",
            templates : {
                twitter : "Applying machine learning EEG signal classification to emotion\u2011related.... Bilucaglia M et al., published by " + 
               "@F1000Research"
       + ", https://f1000research.com/articles/9-173/v1"
            }
        };

    if (typeof(addthis) != "undefined"){
        addthis.addEventListener('addthis.ready', checkCount);
        addthis.addEventListener('addthis.menu.share', checkCount);
    }

        $(".f1r-shares-twitter").attr("href", "https://twitter.com/intent/tweet?text=" + addthis_share.templates.twitter);
    $(".f1r-shares-facebook").attr("href", "https://www.facebook.com/sharer/sharer.php?u=" + addthis_share.url);
    $(".f1r-shares-linkedin").attr("href", addthis_config.services_custom[0].url);
    $(".f1r-shares-reddit").attr("href", addthis_config.services_custom[2].url);
    $(".f1r-shares-mendelay").attr("href", addthis_config.services_custom[1].url);

    function checkCount(){
        setTimeout(function(){
            $(".addthis_button_expanded").each(function(){
                var count = $(this).text();
                if (count !== "" && count != "0")
                    $(this).removeClass("is-hidden");
                else
                    $(this).addClass("is-hidden");
            });
        }, 1000);
    }
</script> <div id=citeReportModal role=dialog aria-labelledby=citeReportModal_title aria-describedby=citeReportModal_description> <div class="c-modal js-modal is-closed c-modal--large js-cite-report-modal "> <aside class="c-modal__content u-black--high o-box u-pb--0 u-black--high "> <div class=c-modal__close> <button type=button class="c-button c-button--icon c-button--medium c-button--full@hover js-modal__close"><i class="c-button--icon__icon material-icons">close</i></button> </div> <h1 id=citeReportModal_title class="c-modal__title t-h3 u-mt--0 u-mb--2 u-weight--md">How to cite this report</h1> <div id=citeReportModal_description class="c-modal__description js-modal__content t-h4 u-mt--0 u-mb--2 u-black--medium"> <div id="" class=js-report-citation-container>{{reportCitation}}</div> <div class="c-modal__extra-message js-modal__extra-message t-h4 u-black--medium"></div></div> <div class="c-modal__actions o-box__actions"> <a href="#" class="c-button c-button--full js-modal__close c-button--secondary">Cancel</a> <a href="#" title="Copy the current citation details to the clipboard." data-clipboard-target="#referee-report-citation" data-test-id=report_copy-citation_button class="c-button c-button--full js-modal__confirm c-button--primary js-clipboard c-mini-tooltip--above">Copy Citation Details</a> </div> </aside> </div> </div> <script src="/js/referee/new/referee_validators.js"></script> <script src="/js/referee/new/referee_helpers.js"></script> <script src="/js/referee/new/referee_checkbox-input.js"></script> <script src="/js/referee/new/referee_inline-editor.js"></script> <script type="text/javascript">
    $(function(){
        var gaCat = "F1000Research";
        if (gaCat === "") {
            gaCat = $("body").hasClass("wellcome-brand") ? "Wellcome Open Research" : "F1000Research";
        }
        GAHelper.track({category: gaCat, action: "Article Page: Applying machine learning EEG signal classification to emotion\u2011related brain anticipatory activity", label: "pageviews"});
        GAHelper.track({category: gaCat, action: "Article Type: Method Article", label: "Article Page"});
        $(".f1r-article-desk .collection-image").each(function (idx, el) {
            var whatChannel = $(el).find("a").attr("href"),
                channelName = $.trim($(el).parent().find(".collection-detail a").text()),
                gaRef = "(ID: " + whatChannel.replace("/collections/", "") + ") " + channelName;
            GAHelper.track({category: 'ChannelStats', action: "Article Page: Applying machine learning EEG signal classification to emotion\u2011related brain anticipatory activity", label: gaRef});
        });
    });
</script> <script>
    $(function(){R.ui.buttonDropdowns('.dropdown-for-downloads');});
    $(function(){R.ui.toolbarDropdowns('.toolbar-dropdown-for-downloads');});
</script> <script src="/js/article/track_article.js" type="text/javascript"></script> <script type="text/javascript">
    $.get("/articles/acj/22202/24487")
</script> <script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script type="text/javascript" src="/js/app/messenger.js"></script> <script type="text/javascript" src="/js/article/article_mobiles.js"></script> <script src="/js/vendor/clipboard.min.js"></script> <script src="/js/shared_scripts/modal-dialogue.js"></script> <script src="/js/shared_scripts/clipboard.js"></script> <script src="/js/article/thesaurus-terms-display.js"></script> <script>
    new F1000.Clipboard();
    new F1000.ThesaurusTermsDisplay("articles", "article", "24487");
</script> <script>
    $(document).ready(function() {
        $( "#frame1" ).on('load', function() {
            var mydiv = $(this).contents().find("div");
            var h     = mydiv.height();
            console.log(h)
        });

        
        var tooltipLivingFigure = jQuery(".interactive-living-figure-label .icon-more-info"),
            titleLivingFigure = tooltipLivingFigure.attr("title");
        tooltipLivingFigure.simpletip({
            fixed: true,
            position: ["-115", "30"],
            baseClass: 'small-tooltip',
            content:titleLivingFigure + "<div class='tooltip-arrow'></div>"
        });
        tooltipLivingFigure.removeAttr("title");

        $("body").on("click", ".cite-living-figure", function(e) {
            e.preventDefault();
            var ref = $(this).attr("data-ref");
            $(this).closest(".living-figure-list-container").find("#" + ref).fadeIn(200);
        });
        $("body").on("click", ".close-cite-living-figure", function(e) {
            e.preventDefault();
            $(this).closest(".popup-window-wrapper").fadeOut(200);
        });

                $(document).on("mouseup", function(e) {
            var metricsContainer = $(".article-metrics-popover-wrapper");
            if (!metricsContainer.is(e.target) && metricsContainer.has(e.target).length === 0) {
                $(".article-metrics-close-button").click();
            }
        });

        var articleId = $('#articleId').val();

        if($("#main-article-count-box").attachArticleMetrics) {
            $("#main-article-count-box").attachArticleMetrics(articleId, {
                articleMetricsView: true
            });
        }
    });

    var figshareWidget = $(".new_figshare_widget");
    if (figshareWidget.length > 0) {
        window.figshare.load("f1000", function(Widget) {
            // Select a tag/tags defined in your page. In this tag we will place the widget.
            _.map(figshareWidget, function(el){
                var widget = new Widget({
                    articleId: $(el).attr("figshare_articleId")
                    //height:300 // this is the height of the viewer part. [Default: 550]
                });
                widget.initialize(); // initialize the widget
                widget.mount(el); // mount it in a tag that's on your page
                // this will save the widget on the global scope for later use from
                // your JS scripts. This line is optional.
                //window.widget = widget;
            });
        });
    }
</script>

<script>
    $(document).ready(function () {

        
        var reportIds = {
                           "67553": 0,
                           "61152": 0,
                           "62113": 0,
                           "61153": 0,
                           "62530": 0,
                           "61155": 0,
                           "61157": 0,
                           "69095": 0,
                           "69097": 0,
                           "69096": 0,
                           "69621": 0,
                           "65077": 0,
                           "69623": 0,
                           "69622": 0,
                           "69624": 0,
                           "69626": 0,
                           "69629": 0,
                           "69628": 0,
                           "61151": 0,
                    };

        $(".referee-response-container,.js-referee-report").each(function(index, el) {
            var reportId = $(el).attr("data-reportid"),
                reportCount = reportIds[reportId] || 0;
            $(el).find(".comments-count-container,.js-referee-report-views").html(reportCount);
        });

        var uuidInput = $("#article_uuid"),
            oldUUId = uuidInput.val(),
            newUUId = "68cc0d29-71ad-4545-9364-66b6f8343e55";
        uuidInput.val(newUUId);

        $("a[href*='article_uuid=']").each(function(index, el) {
            var newHref = $(el).attr("href").replace(oldUUId, newUUId);
            $(el).attr("href", newHref);
        });

    });
</script>              </div>
        </div>

        
            
            <div class="o-page__footer sticky-email-wrapper">
                
                

                


<footer class="c-footer t-inverted">

    <div class="o-wrapper">
        <div class="o-layout">


                        
            <div class="o-layout__item u-mb--3">
                <div class="c-branding c-branding--research">
                    <img src="/img/research/F1000Research_white.svg" alt="F1000Research">
                </div>
            </div>


                        
            <div class="o-layout__item u-1/3@md u-mb--3">

                <span class="c-hr c-hr--thick c-hr--low u-mb--2"></span>

                <p class="t-h3 u-mt--0 u-mb--0">An innovative open access publishing platform offering rapid publication and open peer review, whilst supporting data deposition and sharing.</p>

            </div>


                        
            <div class="o-layout__item u-2/3@md">

                <span class="c-hr c-hr--thick c-hr--low u-mb--2"></span>

                <div class="o-layout">
                    <nav class="c-footer__nav">

                            <div class="o-layout__item u-3/5@sm u-mb--3">


                                <div class="o-columns o-columns--2">

                                                                            <a href="/browse/articles" class="t-body c-footer__nav-item "      >Browse</a>
                                                                            <a href="/gateways" class="t-body c-footer__nav-item "      >Gateways</a>
                                                                            <a href="/collections" class="t-body c-footer__nav-item "      >Collections</a>
                                                                            <a href="/about" class="t-body c-footer__nav-item "      >How it Works</a>
                                                                            <a href="https://blog.f1000.com/blogs/f1000research/" class="t-body c-footer__nav-item "      >Blog</a>
                                                                            <a href="/contact" class="t-body c-footer__nav-item "      >Contact</a>
                                                                            <a href="/developers" class="t-body c-footer__nav-item u-hide u-show@navbar"      >For Developers</a>
                                                                            <a href="/published/rss" class="t-body c-footer__nav-item "   title="RSS feed of published articles"     >RSS</a>
                                    
                                </div>

                            </div>

                            <div class="o-layout__item u-2/5@sm u-center u-right@sm u-mb--3">

                                <div class="u-hide u-show@lg">
                                    <div class="_mdl-layout">
                                        <a class="mdl-button mdl-js-button mdl-button--inverted mdl-button--no-shadow mdl-js-ripple-effect mdl-button--outline" href="/for-authors/publish-your-research"   data-test-id="footer_submit_research"  >Submit Your Research</a>
                                    </div>
                                </div>

                            </div>

                    </nav>
                </div>

            </div>

            <div class="o-layout__item u-mb--2">
                <div class="c-footer__share">
                        <div class="c-footer__share">
        <span class="c-footer__share-icon" title="Open Access">
            <span class="f1r-icon icon-100_open_access license-icon"></span>
        </span>

        <a class="c-footer__share-icon" href="//creativecommons.org/licenses" target="_blank" title="Creative Commons License CC-BY">
            <span class="f1r-icon icon-116_cc license-icon license-icon-cc"></span>
            <span class="f1r-icon icon-117_ccby license-icon license-icon-cc"></span>
        </a>

        <a class="c-footer__share-icon" href="//creativecommons.org/about/cc0" target="_blank" title="Creative Commons License CC0">
            <span class="f1r-icon icon-118_cco license-icon"></span>
        </a>

    </div>
                </div>
            </div>


                        
            <div class="o-layout__item u-1/3@md u-mb--3">

                <span class="c-hr c-hr--low u-mb--3"></span>

                <p class="c-footer__social u-mt--0 u-mb--0 u-white--low-med">Follow us
                    <a href="https://www.facebook.com/F1000" target="_blank" class="c-footer__social-icon f1r-icon icon-55_footer_facebook"></a>
                    <a href="https://twitter.com/#!/F1000Research" target="_blank" class="c-footer__social-icon f1r-icon icon-56_footer_twitter"></a>
                    <a href="http://www.youtube.com/user/F1000research" target="_blank" class="c-footer__social-icon f1r-icon icon-57_footer_youtube"></a></p>

            </div>


                        
            <div class="o-layout__item u-2/3@md u-right@md">

                <span class="c-hr c-hr--low u-mb--3"></span>

                <p class="t-caption u-white--low-med">&copy; 2012-2020 F1000 Research Ltd. ISSN 2046-1402 | <a href="/about/legal" class="copyrightLegal">Legal</a> | Partner of <a target="_blank" href="http://www.who.int/hinari/en/">HINARI</a>  &bull; <a target="_blank" href="http://crossref.org/">CrossRef</a> &bull; <a target="_blank" href="http://about.orcid.org/">ORCID</a> &bull; <a target="_blank" href="http://www.fairsharing.org">FAIRSharing</a></p>

            </div>
        </div>
    </div>

</footer>            </div>
        
    </div>

            <div class="js-cookie-spacer"></div>
        <div class="cookie-warning">
            <div class="instruction">The F1000Research website uses cookies. By continuing to browse the site, you are agreeing to our use of cookies. <a class="js-scroll-to" href="/about/legal/privacypolicy#use-of-cookies" data-scroll-target="#use-of-cookies">Find out more &raquo;</a></div>
            <div class="close-button"></div>
        </div>
    
    <script>
                    R.templateTests.simpleTemplate = R.template('<p class="$variable.one">$text</p><p class="${variable.two}">$text</p><p class="$!variable.three">$text</p><p class="$!{variable.four}">$text</p><p class="${selector}.five">$text</p>');
            R.templateTests.runTests();
        
        var F1000platform = new F1000.Platform({
            name: "f1000research",
            displayName: "F1000Research",
            hostName: "f1000research.com",
            id: "1",
            editorialEmail: "research@f1000.com",
            infoEmail: "info@f1000.com",
            usePmcStats: true
        });

                    $(function(){R.ui.dropdowns('.dropdown-for-authors, .dropdown-for-about, .dropdown-for-myresearch');});
            // $(function(){R.ui.dropdowns('.dropdown-for-referees');});

            $(document).ready(function () {
                if ($(".cookie-warning").is(":visible")) {
                    $(".sticky").css("margin-bottom", "35px");
                    $(".devices").addClass("devices-and-cookie-warning");
                }
                $(".cookie-warning .close-button").click(function (e) {
                    $(".devices").removeClass("devices-and-cookie-warning");
                    $(".sticky").css("margin-bottom", "0");
                });

                $("#tweeter-feed .tweet-message").each(function (i, message) {
                    var self = $(message);
                    self.html(linkify(self.html()));
                });

                $(".partner").on("mouseenter mouseleave", function() {
                    $(this).find(".gray-scale, .colour").toggleClass("is-hidden");
                });
            });
        
    </script>

            
<div class="sign-in-popup">
	<!-- <a href="#" class="sign-in shadow">Sign in <span class="sign-in-image-active"></span></a> -->
	<a href="#" class="sign-in ${locale}">Sign In <span class="arrow-closed sign-in-arrow-padding arrow-opened"></span></a>
	<div class="sign-in-form">

            <form action="https://f1000research.com/j_spring_oauth_security_check" id="googleOAuth" method="post"  target="_top" >
                                    <input class="target-field" type="hidden" name="target" value="/articles/9-173.html"/>
                            <input id="google-remember-me" name="_spring_security_oauth_remember_me" type="hidden" value="true"/>
        <input id="system-google" name="system" type="hidden" value="GOOGLE"/>
    </form>
            <form action="https://f1000research.com/j_spring_oauth_security_check" id="facebookOAuth" method="post"  target="_top" >
                                    <input class="target-field" type="hidden" name="target" value="/articles/9-173.html"/>
                            <input id="facebook-remember-me" name="_spring_security_oauth_remember_me" type="hidden" value="true"/>
        <input id="system-fb" name="system" type="hidden" value="FACEBOOK"/>
    </form>
            <form action="https://f1000research.com/j_spring_oauth_security_check" id="orcidOAuth" method="post"  target="_top" >
                                    <input class="target-field" type="hidden" name="target" value="/articles/9-173.html"/>
                            <input id="orcid-remember-me" name="_spring_security_oauth_remember_me" type="hidden" value="true"/>
        <input id="system-orcid" name="system" type="hidden" value="ORCID"/>
    </form>
		<form id="sign-in-form" class="login-container" action="https://f1000research.com/login" method="post" name="f">
           <div id="sign-in-form-gfb-popup"></div>

                                                            <input class="target-field" type="hidden" name="target" value="/articles/9-173.html"/>
                            			<input type="text" name="username" id="signin-email-box" class="sign-in-input" placeholder="Email address" autocomplete="email">
			<input type="password" name="password" id="signin-password-box" class="sign-in-input" placeholder="Password" autocomplete="current-password">
			<div class="sign-in-remember">
                <div class="checkbox-wrapper">
    				<input type="checkbox" id="remember-me" name="remember_me" class="checkbox is-hidden">
                </div>
                <span class="checkbox-label">Remember me</span>
			</div>
			<a href="#" class="sign-in-link" id="forgot-password-link">Forgotten your password?</a>
			<div class="sign-in-button-container margin-top margin-left-20 margin-bottom">
				<button type="submit" id="sign-in-button" class="sign-in-buttons general-white-orange-button">Sign In</button>
				<button type="button" id="sign-in-cancel" class="sign-in-buttons sign-in-cancel-button margin-left">Cancel</button>
				<div class="clearfix"></div>
			</div>
			<div class="sign-in-error">Email or password not correct. Please try again</div>
			<div class="sign-in-loading">Please wait...</div>
		</form>
		<div class="forgot-password-container">
			
<script type="text/javascript">
	$(function(){
		// Note: All the setup needs to run against a name attribute and *not* the id due the clonish
		// nature of facebox...
		$("a[id=googleSignInButton]").click(function(event){
            event.preventDefault();
            $("input[id=oAuthSystem]").val("GOOGLE");
            $("form[id=oAuthForm]").submit();
        });
        $("a[id=facebookSignInButton]").click(function(event){
            event.preventDefault();
            $("input[id=oAuthSystem]").val("FACEBOOK");
            $("form[id=oAuthForm]").submit();
        });
        $("a[id=orcidSignInButton]").click(function(event){
            event.preventDefault();
            $("input[id=oAuthSystem]").val("ORCID");
            $("form[id=oAuthForm]").submit();
        });
	});
</script>

<span class="text first">
	If you've forgotten your password, please enter your email address below and we'll send you instructions on how to reset your password.
    <p>The email address should be the one you originally registered with F1000.</p>
</span>
<input name="email" class="sign-in-input" id="email-forgot-password" type="text" placeholder="Email address">
<div class="forgot-password-email-error">
	Email address not valid, please try again
</div>
<div class="forgot-password-google-email-error">
    <p>You registered with F1000 via Google, so we cannot reset your password.</p>
	<p>To sign in, please click <a href="#" id="googleSignInButton">here</a>.</p>
    <p>If you still need help with your Google account password, please click <a href="https://www.google.com/accounts/recovery">here</a>.</p>
</div>
<div class="forgot-password-facebook-email-error">
    <p>You registered with F1000 via Facebook, so we cannot reset your password.</p>
    <p>To sign in, please click <a href="#" id="facebookSignInButton">here</a>.</p>
	<p>If you still need help with your Facebook account password, please click <a href="https://www.facebook.com/recover/initiate">here</a>.</p>
</div>
<div class="clearfix"></div>
<div class="forgot-password-captcha-error">
	Code not correct, please try again
</div>
<div class="clearfix"></div>
<div class="sign-in-button-container margin-left-20 margin-bottom">
	<button type="button" id="sign-in-reset-password" class="sign-in-buttons general-white-orange-button">Reset password</button>
	<button type="button" id="forgot-password-cancel" class="sign-in-buttons sign-in-cancel-button margin-left">Cancel</button>
	<div class="clearfix"></div>
</div>
<span class="text last">
	<a href="mailto:">Email us</a> for further assistance.
</span>
<form action="https://f1000research.com/j_spring_oauth_security_check" id="oAuthForm" method="post" target="_top">
                        <input class="target-field" type="hidden" name="target" value="/articles/9-173.html"/>
                <input id="oAuthSystem" name="system" type="hidden"/>
</form>
			<div class="forgot-password-server-error">Server error, please try again.</div>
			<div class="sign-in-success">
                <p>We have sent an email to <span id="email-value"></span>, please follow the instructions to reset your password.</p>
                <p>If you don't receive this email, please check your spam filters and/or contact .</p>
            </div>
			<div class="sign-in-loading">Please wait...</div>
		</div>

		<div class="sign-in-form-register-section">
			<div class="sign-in-button-container margin-left-20 margin-bottom">
				<a href="/register" title="Register"><button type="button" id="sign-in-register-button" class="sign-in-buttons general-white-orange-button">Register</button></a>
				<div class="clearfix"></div>
			</div>
		</div>

	</div>
</div>

<script type="text/javascript">
$(document).ready(function () {

    signIn.createSignInAsRow($("#sign-in-form-gfb-popup"));

    $(".target-field").each(function () {
        var uris = $(this).val().split("/");
        if (uris.pop() === "login") {
        	$(this).val(uris.toString().replace(",","/"));
        }
    });
});
</script>
        <div id="templateOverlay" class="is-hidden" hidden="hidden">
  <div class="o-overlay js-overlay is-hidden" hidden="hidden"></div>
</div>

<div id="templateExternalMessages" class="is-hidden" hidden="hidden">
  <div class="o-modal o-modal--auto@md js-external-messages is-hidden" hidden="hidden">
    <div class="o-modal__body">
      <section class="c-console">
        <div class="_mdl-layout c-console__bdy js-external-messages-body"></div>
        <footer class="_mdl-layout c-console__ftr o-flex o-flex--reverse js-external-messages-footer">
          <button type="button" class="mdl-button mdl-js-button mdl-button--raised mdl-button--colored c-console__btn js-external-messages-close" data-action="maintenance-close">I Understand</button>
        </footer>
      </section>
    </div>
  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.1/moment.min.js"></script>

<script src="/js/namespace.js"></script>
<script src="/js/constants.js"></script>
<script src="/js/utilities.js"></script>

<script>
                  F1000.ExtenalMaintenanceItems = [
    {
      start: '2018-12-10T14:21:00Z',
      end: '2018-12-13T16:00:00Z',
      msg: 'This site will be down for a short time on XX December. It is advisable not to start any submissions on that day or you may lose your work unless you save regularly.',
      cookieName: 'outage23122018',
      editor: false,
    }
  ];
</script>

<script src="/js/shared_scripts/cookie-helper.js"></script>
<script src="/js/shared_scripts/mdl-helper.js"></script>

<script src="/js/app/external-maintenance.js"></script>

                <script type="text/javascript">
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-5646075-11', 'auto');
            ga('require', 'displayfeatures');
            ga('send', 'pageview');
        </script>
        
                <script type="text/javascript" src="/js/app/research.analytics.js"></script>

        <!-- Start of HubSpot Embed Code -->
        <script type="text/javascript" id="hs-script-loader" async defer src="//js.hs-scripts.com/4475190.js"></script>
        <!-- End of HubSpot Embed Code -->
    
            <script src="https://my.hellobar.com/4e0495c6f18cbd68731a1dc1978195a144e767ba.js" type="text/javascript" charset="utf-8" async="async"></script>
    </body>

</html>