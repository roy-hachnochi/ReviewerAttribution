 Summary This is the review of the research paper “Assessment of a demonstrator repository for individual clinical trial data built upon the DSpace open source platform”. The paper describes the assessment and implementation of a repository demonstrator, for the storage and dissemination of clinical trial data, with a particular focus on Individual Participant Data (IPD). The developed demonstrator is built upon the open source and community developed DSpace repository platform ( https://duraspace.org/dspace/ ). This repository platform is data agnostic and can be used to both serve fully open content and content that requires some form of managed access. This paper will be very useful to those looking at evaluating repository platforms for archiving and disseminating research data more generally. The paper focuses on describing the technical criteria used for assessing the suitability of this platform for the storage and dissemination of clinical data, although a good overview of other operational aspects such as the development of guidelines, data deposition rules and quality review in the context of repository submission workflows, is also described. It also includes a summary of the technical requirements (software dependencies and deployment infrastructure) which can be useful to others evaluating the use of this repository platform for the storage and dissemination of research data. Research methodology Overall, the paper includes sufficient details about the methods and analysis undertaken. The authors have explored recent studies in the area, i.e. the suitability assessment presented builds upon a previous study looking at a range of existing repository platforms for sharing clinical trial data, and sensitive data more broadly. The results from this study are the basis for selecting the DSpace platform. In this respect, and although the authors include references to materials where the rationale for selecting this platform is presented, it would have been useful to include a summary table outlining key criteria and some details of the other platforms evaluated. The paper only mentions other platforms (e.g. Figshare, or Zenodo) in passing. One strength of the paper is that the authors reflect on and present the perceived weaknesses of their study. However, and given the sensitive nature of the data underpinning clinical trials, I found it quite surprising that data security features were not included as part of the key criteria defined for this initial assessment, given that not meeting these criteria could impact the suitability of this platform for the archival of clinical data. The authors acknowledge this weakness of their study and state that criteria relating to data security should be considered in future extensions of the study. As part of future assessment, the authors should consider looking at robust security testing of the platform, such as performing penetration testing. Another weakness of the study, even though the authors acknowledge it in the paper, is that they have only evaluated openly available documentation for the DSpace platform. Such documentation can often be incomplete in community-based projects, owing to potential lack of resources. More detail about why they took this approach would have been useful. Moreover, and given that DSpace is a very popular platform within the academic community as acknowledged in the paper, the authors could have informally contacted other institutions currently using the platform to find out more about their experiences of the platform when put to similar uses, and their opinion on the platform’s strengths and weaknesses. Content review The paper reads very well, and the content structure is appropriate. The “Introduction” section sets the scene nicely and provides sufficient background information, with relevant and current literature references. One minor observation is that, when authors introduce the work of a dedicated taskforce addressing the problem of current forms of sharing clinical data, and propose to use data repositories, there is no mention of the importance of gaining consent for data archiving, sharing and re-use from research participants. This is a key barrier to data sharing, and one that we encounter as providers of Research Data Management Services, when researchers wish to deposit their data with our Institutional Repository. The “Methods” section is well developed: the “Technical infrastructure for the demonstrator repository” section provides useful details for those seeking to use similar platforms; and sufficient information is provided so that a similar assessment can be performed on other platforms, or for study replication (even though the analysis is partially qualitative). As mentioned earlier, it would have been useful to include a summary table outlining key criteria and some details of the other platforms evaluated for completeness. In the “Assessment of quality criteria for the reference implementation”, the paragraph beginning with “To promote interoperability …” is a bit unclear and contradictory. It mentions the importance of using metadata standards for describing, structuring and formatting content, which I agree is very important; but they have excluded them as part of the assessment criteria. In particular, the sentence “Here we focus on standards for metadata” is very confusing as the examples given earlier all refer to metadata standards. Is the sentence intended to mean that the study is only concerned with metadata standards and does not consider data format standards? The “Results” section reads very well and is clear. The summary table together with the different criteria-based subsections include relevant, high-level information about the technical assessment that has been performed. With respect to requirement 2a around de-identification tools, perhaps it is worth mentioning that, although not specifically implemented by the community, the DSpace platform does have a mechanism / framework in place (i.e. curation system) that allows for easy integration of such tools within DSpace’s standard submission workflows (see https://wiki.lyrasis.org/display/DSDOC6x/Curation+System ). It is mentioned in the section “Formal contract regarding upload and storage” that the implemented demonstrator does not provide support for constructing and editing the distribution licence. However, the distribution licence text can be edited or customised, as we have done so in our Institutional DSpace repository instance. Perhaps, what the authors mean instead is that the platform does not provide a user interface to do this easily. The section about repository long-term preservation could have incorporated more detailed information about the DSpace’s platform’s capabilities around content preservation and relevant references and links to relevant literature. For example, open source integrations of the DSpace platform with preservation systems exist, e.g. integration with Archivematica ( https://figshare.com/articles/Automating_OAIS_compliant_digital_preservation_using_Archivematica_and_DSpace/11274143/1 ). The authors seem to mix the platform’s long-term availability based on a number of aspects such as technology sustainability plans, or wide use, with the platform’s capabilities for preservation of the repository content itself. The former is not directly related to preservation but to the long-term sustainability of the platform. Lastly, a number of sections in the paper talk about self-attestation functions in the context of access to repository content (requirement 7b – web-based self-attestation of the user). I am not familiar with this term, and the general reader would benefit with a clearer definition of the term and such functions. I can only guess, based on context and my knowledge of repository platforms, that the authors mean the repository’s ability for user self-registration to be able to access repository content, or functions for only giving access to content once certain information about the user has been collected and verified. E.g. the repository allows to incorporate a form asking content requesters to supply information about what uses they will make of the data, purpose of their research, contact information and /or email address to be verified, etc. If this is the case, this should be made much more explicit in the paper. Minor edits and structure comments In the “Results” subsection of the abstract, the sentence “Two requirements could not be demonstrated (inability to incorporate de-identification tools in the submission workflow, lack of a self-attestation system) …” is not clear. It needs to be rephrased, e.g. “ability to incorporate …” and “support for self-attestation …”. Otherwise it reads as though the things in parenthesis are actually the requirements. In the “Conclusions” subsection of the abstract, “productive repository” should read “production ready repository” or similar. In the “Introduction” section, first sentence, “evironment” should read “environment. Table 3, third row “Control of quality of data”, C6 should read “Quality assurance” instead of “insurance”. Also, Table 3 appears much earlier (page 7) than its reference within the paper (page 11). I found this quite confusing when reading the paper as it appeared straight after Table 2, and completely out of context. It would be much clearer if the table was moved closer to its reference in the text, towards the end of the paper. 