

    <!DOCTYPE html>
<html class="">

        
<head>
    <title>Recent advances in understanding object recognition... | F1000Research</title>
    <meta charset="utf-8">
    <!--<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>-->
    <!--<meta lang="$locale">-->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="9-QVycOO2_ob3Z9QzRmXv2CF08A9oyYXqWyTiVdKPlU" />
    <!-- This is commented out to fix display problems on mobile devices.
    We may use it again once we implement a responsive design that supports native device resolutions.
    <meta name="viewport" content="width=device-width"> -->

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



    <link rel="alternate" title="Recent articles published in F1000Research" href="/rss" type="application/rss+xml">
            <link rel="alternate" type="application/pdf" title="PDF" href="https://f1000research.com/articles/9-590/pdf"/>
        <link rel="canonical" href="https://f1000research.com/articles/9-590" />
    
            <meta name="description" content="Read the original article in full on F1000Research: Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context" />
    
            <meta name="og:title" content="F1000Research Article: Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context.">
            <meta name="og:description" content="Read the latest article version by Susan G. Wardle, Chris I. Baker, at F1000Research.">
            <meta name="og:image" content="/img/sharing/og_research.png">
            <meta name="version-id" content="24595">
            <meta name="article-id" content="22296">
            <meta name="dc.title" content="Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context">
            <meta name="dc.description" content="Object recognition is the ability to identify an object or category based on the combination of visual features observed. It is a remarkable feat of the human brain, given that the patterns of light received by the eye associated with the properties of a given object vary widely with simple changes in viewing angle, ambient lighting, and distance. Furthermore, different exemplars of a specific object category can vary widely in visual appearance, such that successful categorization requires generalization across disparate visual features. In this review, we discuss recent advances in understanding the neural representations underlying object recognition in the human brain. We highlight three current trends in the approach towards this goal within the field of cognitive neuroscience. Firstly, we consider the influence of deep neural networks both as potential models of object vision and in how their representations relate to those in the human brain. Secondly, we review the contribution that time-series neuroimaging methods have made towards understanding the temporal dynamics of object representations beyond their spatial organization within different brain regions. Finally, we argue that an increasing emphasis on the context (both visual and task) within which object recognition occurs has led to a broader conceptualization of what constitutes an object representation for the brain. We conclude by identifying some current challenges facing the experimental pursuit of understanding object recognition and outline some emerging directions that are likely to yield new insight into this complex cognitive process.">
            <meta name="dc.subject" content="object recognition, human vision, fMRI, MEG, DNN, visual perception">
            <meta name="dc.creator" content="Wardle, Susan G.">
            <meta name="dc.creator" content="Baker, Chris I.">
            <meta name="dc.date" content="2020/06/11">
            <meta name="dc.identifier" content="doi:10.12688/f1000research.22296.1">
            <meta name="dc.source" content="F1000Research 2020 9:590">
            <meta name="dc.format" content="text/html">
            <meta name="dc.language" content="en">
            <meta name="dc.publisher" content="F1000 Research Limited">
            <meta name="dc.rights" content="https://creativecommons.org/licenses/by/3.0/igo/">
            <meta name="dc.type" content="text">
            <meta name="prism.keyword" content="object recognition">
            <meta name="prism.keyword" content="human vision">
            <meta name="prism.keyword" content="fMRI">
            <meta name="prism.keyword" content="MEG">
            <meta name="prism.keyword" content="DNN">
            <meta name="prism.keyword" content="visual perception">
            <meta name="prism.publication.Name" content="F1000Research">
            <meta name="prism.publicationDate" content="2020/06/11">
            <meta name="prism.volume" content="9">
            <meta name="prism.number" content="590">
            <meta name="prism.versionIdentifier" content="1">
            <meta name="prism.doi" content="10.12688/f1000research.22296.1">
            <meta name="prism.url" content="https://f1000research.com/articles/9-590">
            <meta name="citation_title" content="Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context">
            <meta name="citation_abstract" content="Object recognition is the ability to identify an object or category based on the combination of visual features observed. It is a remarkable feat of the human brain, given that the patterns of light received by the eye associated with the properties of a given object vary widely with simple changes in viewing angle, ambient lighting, and distance. Furthermore, different exemplars of a specific object category can vary widely in visual appearance, such that successful categorization requires generalization across disparate visual features. In this review, we discuss recent advances in understanding the neural representations underlying object recognition in the human brain. We highlight three current trends in the approach towards this goal within the field of cognitive neuroscience. Firstly, we consider the influence of deep neural networks both as potential models of object vision and in how their representations relate to those in the human brain. Secondly, we review the contribution that time-series neuroimaging methods have made towards understanding the temporal dynamics of object representations beyond their spatial organization within different brain regions. Finally, we argue that an increasing emphasis on the context (both visual and task) within which object recognition occurs has led to a broader conceptualization of what constitutes an object representation for the brain. We conclude by identifying some current challenges facing the experimental pursuit of understanding object recognition and outline some emerging directions that are likely to yield new insight into this complex cognitive process.">
            <meta name="citation_description" content="Object recognition is the ability to identify an object or category based on the combination of visual features observed. It is a remarkable feat of the human brain, given that the patterns of light received by the eye associated with the properties of a given object vary widely with simple changes in viewing angle, ambient lighting, and distance. Furthermore, different exemplars of a specific object category can vary widely in visual appearance, such that successful categorization requires generalization across disparate visual features. In this review, we discuss recent advances in understanding the neural representations underlying object recognition in the human brain. We highlight three current trends in the approach towards this goal within the field of cognitive neuroscience. Firstly, we consider the influence of deep neural networks both as potential models of object vision and in how their representations relate to those in the human brain. Secondly, we review the contribution that time-series neuroimaging methods have made towards understanding the temporal dynamics of object representations beyond their spatial organization within different brain regions. Finally, we argue that an increasing emphasis on the context (both visual and task) within which object recognition occurs has led to a broader conceptualization of what constitutes an object representation for the brain. We conclude by identifying some current challenges facing the experimental pursuit of understanding object recognition and outline some emerging directions that are likely to yield new insight into this complex cognitive process.">
            <meta name="citation_keywords" content="object recognition, human vision, fMRI, MEG, DNN, visual perception">
            <meta name="citation_journal_title" content="F1000Research">
            <meta name="citation_author" content="Susan G. Wardle">
            <meta name="citation_author_institution" content="Laboratory of Brain and Cognition, National Institute of Mental Health, National Institutes of Health, Bethesda, MD, 20892, USA">
            <meta name="citation_author" content="Chris I. Baker">
            <meta name="citation_author_institution" content="Laboratory of Brain and Cognition, National Institute of Mental Health, National Institutes of Health, Bethesda, MD, 20892, USA">
            <meta name="citation_publication_date" content="2020/06/11">
            <meta name="citation_volume" content="9">
            <meta name="citation_publication_number" content="590">
            <meta name="citation_version_number" content="1">
            <meta name="citation_doi" content="10.12688/f1000research.22296.1">
            <meta name="citation_firstpage" content="590">
            <meta name="citation_pdf_url" content="https://f1000research.com/articles/9-590/v1/pdf">
    

    
    <link href="/img/favicon-research.ico" rel="shortcut icon" type="image/ico">
    <link href="/img/favicon-research.ico" rel="icon" type="image/ico">

        <link rel="stylesheet" href="/1597255280893/css/mdl/material-design-lite.css" type="text/css" media="all" />
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/2.2.43/css/materialdesignicons.min.css" />

        
            
    <link rel="stylesheet" href="/1597255280893/css/F1000Research.css" type="text/css" media="all" />
    <link rel="stylesheet" href="/css/F1000ResearchFontIcons/F1000ResearchFontIcons.css" type="text/css" media="all" />
    <link rel="stylesheet" href="/css/F1000ResearchFontIcons/animation.css" type="text/css" media="all" />

        <!--[if IE 7]><link rel="stylesheet" href="/css/F1000ResearchFontIcons/F1000ResearchFontIcons-ie7.css" media="all" /><![endif]-->

                    <script>dataLayer = [];</script>
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-54Z2SBK');</script>
        <!-- End Google Tag Manager -->
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.8.1.min.js"><\/script>')</script>
    <script src="/1597255280893/js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>
    <script src="/1597255280893/js/shared_scripts/sticky.js"></script>
    <script src="/1597255280893/js/shared_scripts/helpers.js"></script>
    <script src="/1597255280893/js/shared_scripts/menu.js"></script>
    <script src="/1597255280893/js/shared_scripts/navbar.js"></script>
    <script src="/1597255280893/js/shared_scripts/platforms.js"></script>
    <script src="/1597255280893/js/shared_scripts/object-polyfills.js"></script>
            <script src="/1597255280893/js/vendor/lodash.min.js"></script>
        <script>CKEDITOR_BASEPATH='https://f1000research.com/js/ckeditor/'</script>
    <script src="https://f1000researchdata.s3-eu-west-1.amazonaws.com/js/plugins.js"></script>
    <script src="/1597255280893/js/shared_scripts/helpers.js"></script>
    <script src="/1597255280893/js/app/research.js"></script>
    <script>window.reactTheme = 'research';</script>
    <script src="/1597255280893/js/public/bundle.js"></script>

    <script src="/1597255280893/js/app/research.ui.js"></script>
    <script src="/1597255280893/js/app/login.js"></script>
    <script src="/1597255280893/js/app/main.js"></script>
    <script src="/1597255280893/js/app/js-date-format.min.js"></script>
    <script src="/1597255280893/js/app/search.js"></script>
    <script src="/1597255280893/js/app/cookies_warning.js"></script>
    <script src="/1597255280893/js/mdl/mdl.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script src="https://f1000researchdata.s3-eu-west-1.amazonaws.com/js/ckeditor.js"></script>
            <script src="/js/ckeditor/adapters/jquery.js"></script>
                <script src="/js/article/article_scrolling_module.js"></script>
            <script src="/js/article/article_stats.js"></script>
            <script src="/js/article/article.js"></script>
            <script src="/js/shared_scripts/referee_timeline_pagination.js"></script>
            <script src="/js/app/text_editor_controller.js"></script>
            <script src="//s7.addthis.com/js/250/addthis_widget.js#pubid=ra-503e5e99593dc42c"></script>
            <script src="/js/article/article_metrics.js"></script>
        
                                                                            <script>
            if (window.location.hash == '#_=_'){
                window.location = window.location.href.split('#')[0]
            }
        </script>

                    
        
    <!-- pixelId: 1641728616063202 :: assetPixelId: 6034867600215 :: funderPixelId:  -->

            <!-- Facebook pixel code (merged with EP GTM code) -->
        <script>
            !function(f,b,e,v,n,t,s){if(f.fbq)return;n=f.fbq=function()

            {n.callMethod? n.callMethod.apply(n,arguments):n.queue.push(arguments)}
            ;if(!f._fbq)f._fbq=n;
            n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;
            t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,
            document,'script','https://connect.facebook.net/en_US/fbevents.js');

            fbq('init', '1641728616063202');

            
            fbq('track', "PixelInitialized", {});
        </script>

        <noscript><img height="1" width="1" style="display:none"
            src="https://www.facebook.com/tr?id=1641728616063202&noscript=1&amp;ev=PixelInitialized"
        /></noscript>
        <!-- End Facebook Pixel Code -->
    
                <script>
            (function(h,o,t,j,a,r){
                h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
                h._hjSettings={hjid:917825,hjsv:6};
                a=o.getElementsByTagName('head')[0];
                r=o.createElement('script');r.async=1;
                r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
                a.appendChild(r);
            })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
        </script>
    
</head>
<body  class="o-page-container no-js p-article o-layout-reset   ">

    
                            <!-- Google Tag Manager (noscript) -->
        <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-54Z2SBK"
        height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
        <!-- End Google Tag Manager (noscript) -->
    
        
    <div class="o-page">

        <div id="notify-container"></div>
        <div id="pageWarning"></div>
        <div id="pageMessage"></div>
        <div id="pageFooterMessage"></div>

                                <div id="f1r-ga-data" data-name="f1r-ga-data" class="f1r-ga-data"
                data-user-registered="false"
                data-user-module=""
                data-current-path=""
                data-location=""
                data-website="F1000Research"
                data-websiteDisplayName="F1000Research">
            </div>
        
                
        
        <div class="header-wrapper   js-navbar-space ">
                            


    










        
                            
    
            



<nav class="c-navbar js-navbar js-mini-nav js-sticky c-navbar--js-sticky c-navbar--userSite c-navbar__platform-bgcolor  c-navbar--bg-f1000research ">

    <div class="c-navbar__content">

                                <div class="c-navbar__extras">
            <div class="o-wrapper">
                <div class="o-actions o-actions--middle c-navbar__extras-row">
                    <div class="o-actions__primary">
                        
                                            </div>
                                    </div>
            </div>
        </div>

        <div class="o-wrapper t-inverted js-sticky-start">

            <div class="c-navbar__branding-row">
                <div class="c-navbar__row">


                                        
                    <div class="c-navbar__primary u-mr--2">

                                                                                                                                                                                                    <a href="/" class="c-navbar__branding u-ib u-middle"   data-test-id="nav_branding"  >
                                <img class="u-ib u-middle" src="/img/research/F1000Research_white_solid.svg" alt="F1000Research">
                            </a>
                                            </div>

                    <div class="c-navbar__secondary c-navbar__row">


                                                
                                                    <form action="/search" class="-navbar__secondary u-mr--2 c-search-form js-search-form u-hide u-show@navbar">
                                <label for="searchInput" class="c-search-form__label _mdl-layout">
                                    <input name="q" type="search" class="c-search-form__input" id="searchInput" placeholder="Search">
                                    <button type="submit" class="c-search-form__submit mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">search</i></button>
                                </label>
                            </form>
                        
                                                
                                                    <div class="c-navbar__primary u-hide u-show@navbar">
                                <div class="_mdl-layout c-navbar__cta">
                                    <a class="mdl-button mdl-js-button mdl-button--raised mdl-button--no-shadow mdl-button--multi-line mdl-js-ripple-effect mdl-button--inverted c-navbar__submit" href="/for-authors/publish-your-research"   data-test-id="nav_submit_research"  ><i class="material-icons">file_upload</i>Submit your research</a>
                                </div>
                            </div>
                        

                                                
                        <span class="u-hide@navbar _mdl-layout u-nowrap">

                                                            <button type="button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon c-navbar__toggle c-navbar__toggle--menu js-navbar-toggle" data-focus="#navbar_mob_search_input" data-toggle="navbarMenu" data-target="navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation"   data-test-id="nav_menu_search_mob"  ><i class="material-icons">search</i></button>
                            
                                                            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-button--multi-line c-navbar__toggle c-navbar__toggle--menu js-navbar-toggle" type="button" data-toggle="navbarMenu" data-target="navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation"   data-test-id="nav_menu_toggle_mob"  >
                                    <i class="material-icons c-navbar__toggle-open">menu</i>
                                    <i class="material-icons c-navbar__toggle-close">close</i>
                                </button>
                                                    </span>
                    </div>

                </div>
            </div>

                        
                            <div class="c-navbar__menu-row js-navbar-block is-collapsed" id="navbarMenu">

                                                                                                                                                
                                                                    
                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                            

                                                                
                                                                                                                                                                                                                                
                                            
                    <div class="c-navbar__menu-row-content">

                                                                            <div class="u-hide@navbar c-navbar__menu-bar-spacing">
                                <form action="/search" class="c-search-form js-search-form">
                                    <label for="navbar_mob_search_input" class="c-search-form__label _mdl-layout">
                                        <input id="navbar_mob_search_input" name="q" type="search" class="c-search-form__input" placeholder="Search">
                                        <button type="submit" class="c-search-form__submit mdl-button mdl-js-button mdl-button--icon"><i class="material-icons">search</i></button>
                                    </label>
                                </form>
                            </div>
                        
                        <div class="o-actions o-actions--middle">

                            <div class="o-actions__primary">

                                                                
    <ul class="c-menubar c-navbar__menu-bar js-main-menu"   id="main-menu"   role="menubar" aria-label="Main Navigation"  data-menu-group="navbar" >

        
            
                                                                
                                
                                
                                    <li role="none"
                        data-index="0"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="/browse/articles" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                                                    tabindex="0"
                              data-test-id="nav_browse"                              >Browse</a>

                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="1"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="/gateways" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                                                    tabindex="-1"
                              data-test-id="nav_gatewaysViewAndBrowse"                              >Gateways & Collections</a>

                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="2"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                               c-menubar__item--selected c-navbar__menu-bar-item--parent ">

                        <a href="#" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                         aria-haspopup="true" aria-expanded="false"                             tabindex="-1"
                              data-test-id="nav_for-authors"                              >How to Publish</a>

                                                    
    <ul class="c-menu js-menu
                is-collapsed                c-menubar__menu c-navbar__menu"
                  role="menu"
        aria-label="How to Publish">

        
            
                                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_submit-manuscript"                      href="/for-authors/publish-your-research"
                    role="menuitem"
                    tabindex="0">Submit your Research</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-submissions"                      href="/for-authors/my-submissions"
                    role="menuitem"
                    tabindex="-1">My Submissions</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_article-guidelines"                      href="/for-authors/article-guidelines"
                    role="menuitem"
                    tabindex="-1">Article Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_article-guidelines-new-versions"                      href="/for-authors/article-guidelines-new-versions"
                    role="menuitem"
                    tabindex="-1">Article Guidelines (New Versions)</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_data-guidelines"                      href="/for-authors/data-guidelines"
                    role="menuitem"
                    tabindex="-1">Data Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_asset-guidelines"                      href="/for-authors/posters-and-slides-guidelines"
                    role="menuitem"
                    tabindex="-1">Posters and Slides Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_document-guidelines"                      href="/for-authors/document-guidelines"
                    role="menuitem"
                    tabindex="-1">Document Guidelines</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_article-processing-charges"                      href="/for-authors/article-processing-charges"
                    role="menuitem"
                    tabindex="-1">Article Processing Charges</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_finding-referees"                      href="/for-authors/tips-for-finding-referees"
                    role="menuitem"
                    tabindex="-1">Finding Article Reviewers</a>

                </li>

                        </ul>
                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="3"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                               c-menubar__item--selected c-navbar__menu-bar-item--parent ">

                        <a href="#" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                         aria-haspopup="true" aria-expanded="false"                             tabindex="-1"
                              data-test-id="nav_about-contact"                              >About</a>

                                                    
    <ul class="c-menu js-menu
                is-collapsed                c-menubar__menu c-navbar__menu"
                  role="menu"
        aria-label="About">

        
            
                                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_about-page"                      href="/about"
                    role="menuitem"
                    tabindex="0">How it Works</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_referee-guidelines"                      href="/for-referees/guidelines"
                    role="menuitem"
                    tabindex="-1">For Reviewers</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_advisoryPanel"                      href="/advisors"
                    role="menuitem"
                    tabindex="-1">Our Advisors</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_policy-page"                      href="/about/policies"
                    role="menuitem"
                    tabindex="-1">Policies</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_glossary-page"                      href="/glossary"
                    role="menuitem"
                    tabindex="-1">Glossary</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_faqs-page"                      href="/faqs"
                    role="menuitem"
                    tabindex="-1">FAQs</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              u-hide u-show@navbar"
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      u-hide u-show@navbar"
                                          data-test-id="nav_for-developers"                      href="/developers"
                    role="menuitem"
                    tabindex="-1">For Developers</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_newsroom-page"                      href="/newsroom"
                    role="menuitem"
                    tabindex="-1">Newsroom</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_contact-page"                      href="/contact"
                    role="menuitem"
                    tabindex="-1">Contact</a>

                </li>

                        </ul>
                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="4"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="https://blog.f1000.com/blogs/f1000research/" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                             target="_blank"                                                         tabindex="-1"
                              data-test-id="nav_blog"                              >Blog</a>

                                            </li>
                
            
        
    </ul>



                            </div>

                            <div class="o-actions__secondary">

                                                                
    <ul class="c-menubar c-navbar__menu-bar js-main-menu"   id="secondary-items"   role="menubar" aria-label="My Account"  data-menu-group="navbar" >

        
            
                                                                
                                
                                
                                    <li role="none"
                        data-index="0"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                               c-menubar__item--selected c-navbar__menu-bar-item--parent ">

                        <a href="#" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                         aria-haspopup="true" aria-expanded="false"                             tabindex="0"
                              data-test-id="nav_my-research"                              >My Research</a>

                                                    
    <ul class="c-menu js-menu
                is-collapsed                c-menubar__menu c-navbar__menu"
                  role="menu"
        aria-label="My Research">

        
            
                                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-submissions"                      href="/login?originalPath=/my/submissions"
                    role="menuitem"
                    tabindex="0">Submissions</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-email-alerts"                      href="/login?originalPath=/my/email-alerts"
                    role="menuitem"
                    tabindex="-1">Content and Tracking Alerts</a>

                </li>

                    
            
                
                                
                <li class="c-menu__item js-menu-item
                                              "
                        role="none">

                <a  class="c-menu__link js-menu-link
                                                      "
                                          data-test-id="nav_my-user-details"                      href="/login?originalPath=/my/user-details"
                    role="menuitem"
                    tabindex="-1">My Details</a>

                </li>

                        </ul>
                                            </li>
                
            
        
            
                                                                
                
                                
                                    <li role="none"
                        data-index="1"
                        class="c-menubar__item js-menu-item  c-navbar__menu-bar-item
                                                              ">

                        <a href="/login?originalPath=/articles/9-590.html" class="c-navbar__menu-bar-link js-menu-link " role="menuitem"
                                                                                    tabindex="-1"
                              data-test-id="nav_sign-in"                              >Sign In</a>

                                            </li>
                
            
        
    </ul>



                            </div>

                                                                                        <div class="_mdl-layout c-navbar__cta u-hide@navbar c-navbar__menu-bar-spacing">
                                    <a class="mdl-button mdl-js-button mdl-button--raised mdl-button--multi-line mdl-button--no-shadow mdl-js-ripple-effect mdl-button--inverted c-navbar__submit" href="/for-authors/publish-your-research"   data-test-id="nav_submit_research_mob"  ><i class="material-icons">file_upload</i>Submit your research</a>
                                </div>
                                                    </div>

                    </div>

                </div>
            
        </div>

    </div>

</nav>
                    </div>

        <div class="content-wrapper o-page__main row ">
            <div id="highlight-area" class="content ">
                





<div id=article-metadata class=hidden> <input type=hidden name=versionId value=24595 /> <input type=hidden id=articleId name=articleId value=22296 /> <input type=hidden id=xmlUrl value="/articles/9-590/v1/xml"/> <input type=hidden id=xmlFileName value="-9-590-v1.xml"> <input type=hidden id=article_uuid value=13152842-c463-4283-8828-d568ef9e3a84 /> <input type=hidden id=referer value=""/> <input type=hidden id=meta-article-title value="Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context"/> <input type=hidden id=workspace-export-url value="https://sciwheel.com/work/api/import/external?doi=10.12688/f1000research.22296.1"/> <input type=hidden id=versionDoi value="10.12688/f1000research.22296.1"/> <input type=hidden id=usePmcStats value=true /> </div> <main class="o-wrapper p-article__wrapper js-wrapper"> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://f1000research.com/articles/9-590"
  },
  "headline": "Recent advances in understanding object recognition in the human brain: deep neural networks, temporal...",
  "datePublished": "2020-06-11T11:47:08",
  "dateModified": "2020-06-11T11:47:08",
  "author": [
    {
      "@type": "Person",
      "name": "Susan G. Wardle"
    },    {
      "@type": "Person",
      "name": "Chris I. Baker"
    }  ],
  "publisher": {
    "@type": "Organization",
    "name": "F1000Research",
    "logo": {
      "@type": "ImageObject",
      "url": "https://f1000research.com/img/AMP/F1000Research_image.png",
      "height": 480,
      "width": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://f1000research.com/img/AMP/F1000Research_image.png",
    "height": 1200,
    "width": 150
  },
  "description": "Object recognition is the ability to identify an object or category based on the combination of visual features observed. It is a remarkable feat of the human brain, given that the patterns of light received by the eye associated with the properties of a given object vary widely with simple changes in viewing angle, ambient lighting, and distance. Furthermore, different exemplars of a specific object category can vary widely in visual appearance, such that successful categorization requires generalization across disparate visual features. In this review, we discuss recent advances in understanding the neural representations underlying object recognition in the human brain. We highlight three current trends in the approach towards this goal within the field of cognitive neuroscience. Firstly, we consider the influence of deep neural networks both as potential models of object vision and in how their representations relate to those in the human brain. Secondly, we review the contribution that time-series neuroimaging methods have made towards understanding the temporal dynamics of object representations beyond their spatial organization within different brain regions. Finally, we argue that an increasing emphasis on the context (both visual and task) within which object recognition occurs has led to a broader conceptualization of what constitutes an object representation for the brain. We conclude by identifying some current challenges facing the experimental pursuit of understanding object recognition and outline some emerging directions that are likely to yield new insight into this complex cognitive process."
}
</script> <div class="o-layout o-layout--right-gutter"> <div id=article_secondary-column class="p-article__main o-layout__item u-font-size--legal u-2/3@article not-expanded "> <div class=float-left> <script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
              {
          "@type": "ListItem",
          "position": "1",
          "item": {
            "@id": "https://f1000research.com/",
            "name": "Home"
          }
        },              {
          "@type": "ListItem",
          "position": "2",
          "item": {
            "@id": "https://f1000research.com/browse/articles",
            "name": "Browse"
          }
        },              {
          "@type": "ListItem",
          "position": "3",
          "item": {
            "@id": "https://f1000research.com/articles/9-590",
            "name": "Recent advances in understanding object recognition in the human brain:..."
          }
        }          ]
  }
  </script> <div class="breadcrumbs js-breadcrumbs"> <a href="/" class=f1r-standard-link>Home</a> <span class=item_separator></span> <a href="/browse/articles" class=f1r-standard-link>Browse</a> <span class=item_separator></span> Recent advances in understanding object recognition in the human brain:... </div> </div> <div class="article-badges-container u-mb--2"> <div class=crossmark-new> <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script> <a data-target=crossmark><img height=30 width=150 src="https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg"/></a> </div> <div id=crossmark-dialog style="display: none;" title=""> <iframe id=crossmark-dialog-frame frameborder=0></iframe> </div> <div class=clearfix></div> </div> <div class=article-interaction-container> <div id=main-article-count-box class=article-count-box> <div class="article-metrics-wrapper metrics-icon-wrapper" data-version-id=24595 data-id=22296 data-downloads="" data-views="" data-scholar="10.12688/f1000research.22296.1" data-recommended="" data-doi="10.12688/f1000research.22296.1" data-f1r-ga-helper="Article Page Metrics (Desktop)"> <span class="metrics-on-browse article-metrics-icon f1r-icon icon-89_metrics"></span> <div class="count-title article-metrics-text">ALL Metrics</div> <div class=js-article-metrics-container></div> </div> <div> <div class=count-delimiter></div> <div title="Total views from F1000Research and PubMed Central"> <div class="count-container view-count js-views-count">-</div> <div class=count-title><span class="count-title-icon count-title-views-icon"></span>Views</div> </div> <div class=download-counts hidden> <div class=count-delimiter></div> <div title="Total downloads from F1000Research and PubMed Central"> <div class="count-container js-downloads-count"></div> <div class=count-title><span class="count-title-icon f1r-icon icon-76_download_file"></span>Downloads</div> </div> </div> </div> </div> <div id=main-article-interaction-box class="article-interaction-box has-control-tab"> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-102_download_pdf"></span> <a href="https://f1000research.com/articles/9-590/v1/pdf?article_uuid=13152842-c463-4283-8828-d568ef9e3a84" title="Download PDF" class="button-link download pdf-download-helper" target=_blank>Get PDF</a> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-103_download_xml"></span> <a id=download-xml href="#" class="button-link download" title="Download XML">Get XML</a> </div> </div> <div class="article-interaction-info article-page"> <div class="cite-article-popup-wrapper article-page-interaction-box"> <div class="article-interaction-button cite-article-button" title="Cite this article" data-windowref=cite-article-popup-22296-1> <span class="f1r-icon icon-82_quote"></span> <a href="#" class="button-link cite-article-popup-link" title="Cite Article">Cite</a> </div> <div id=cite-article-popup-22296-1 class="popup-window-wrapper is-hidden"> <div class=cite-popup-background></div> <div class="popup-window top-popup cite-this-article-box research-layout"> <div class="popup-window-title small cite-title">How to cite this article</div> <span id=cite-article-text-22296-1 data-test-id=copy-citation_text> <span class="article-title-and-info in-popup">Wardle SG and Baker CI. Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context [version 1; peer review: 2 approved]</span>. <i>F1000Research</i> 2020, <b>9</b>(F1000 Faculty Rev):590 (<a class=new-orange href="https://doi.org/10.12688/f1000research.22296.1" target=_blank>https://doi.org/10.12688/f1000research.22296.1</a>) </span> <div class="popup-window-title small margin-top-20 margin-bottom-20 note"> <strong>NOTE:</strong> it is important to ensure the information in square brackets after the title is included in all citations of this article. </div> <div class=float-right> <button class="secondary no-fill orange-text-and-border margin-right-20 close-cite-popup uppercase">Close</button> <button id=copy-citation-details class="secondary orange copy-cite-article-version uppercase js-clipboard" title="Copy the current citation details to the clipboard." data-clipboard-target="#cite-article-text-22296-1" data-test-id=copy-citation_button>Copy Citation Details</button> </div> </div> </div> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-76_download_file"></span> <a id=export-citation href="#" class="button-link download" title="Export Citation">Export</a> </div> <div class="modal-window-wrapper is-hidden"> <div id=export-citation-popup class="modal-window padding-20"> <div class=modal-window-close-button></div> <div class=modal-window-title>Export Citation</div> <div class=modal-window-row> <div> <input type=radio name=export-citation-option value=WORKSPACE /> <span class=radio-label>Sciwheel</span> </div> <div> <input type=radio name=export-citation-option value=ENDNOTE /> <span class=radio-label>EndNote</span> </div> <div> <input type=radio name=export-citation-option value=REF_MANAGER /> <span class=radio-label>Ref. Manager</span> </div> <div> <input type=radio name=export-citation-option value=BIBTEX /> <span class=radio-label>Bibtex</span> </div> <div> <input type=radio name=export-citation-option value=PROCITE /> <span class=radio-label>ProCite</span> </div> <div> <input type=radio name=export-citation-option value=SENTE /> <span class=radio-label>Sente</span> </div> </div> <div class=modal-window-footer> <button class=general-white-orange-button id=export-citation-submit>EXPORT</button> </div> <div class=default-error style="display: none;">Select a format first</div> </div> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-90_track"></span> <a class="button-link track-article" data-article-id=22296 id=track-article-signin-22296 title="Receive updates on new activity such as publication of new versions, peer reviews or author responses." href="/login?originalPath=/trackArticle/22296?target=/articles/9-590">Track</a> </div> </div> <div class="article-interaction-info article-page"> <div class="article-interaction-button email-article"> <span class="f1r-icon icon-6_email"></span> <a href="#" class=button-link title="Email this article">Email</a> </div> <div class="email-article-version-container small-tooltip _chrome-fix"> <div class=close-icon><span class="f1r-icon icon-3_close_big"></span></div> <script src='https://www.recaptcha.net/recaptcha/api.js'></script> <form class="recommend-version-form research-layout"> <p>All fields are required.</p> <input name=versionId type=hidden value=24595 /> <input name=articleId type=hidden value=22296 /> <input name=senderName class="form-input-field reg-form" value="" type=text placeholder="Your name"/> <input name=senderEmail class="form-input-field reg-form margin-top" value="" type=text placeholder="Your email address"/> <textarea name=recipientEmails class="form-textarea-field ninetynine-percent-wide margin-top no-resize" placeholder="Recipient email address(es) (comma delimited)"></textarea> <input class="form-input-field reg-form margin-top" name=subject type=text value="Interesting article on F1000Research" placeholder=Subject /> <textarea name=message class="form-textarea-field reg-form margin-top no-resize">I thought this article from F1000Research (https://f1000research.com) would be of interest to you.</textarea> <div class="g-recaptcha margin-top" data-sitekey=6LcHqxoUAAAAANP3_0TzpGG6qFvl4DhbUcuRzw7W></div> <input value="" name=captcha type=hidden /> <p>A full article citation will be automatically included.</p> <p><img class="ticker-email-article-details hidden" src="/img/ticker.gif" alt=loading /></p> <button class="secondary orange margin-bottom" data-test-id=version_share_email_send>SEND EMAIL</button> <div class="orange-message margin-bottom is-hidden" data-test-id=version_share_email_message></div> </form> </div> </div> <div class="article-interaction-info article-page"> <div class=article-interaction-button> <span class="f1r-icon icon-34_share"></span> <a href="#" class="button-link last addthis_button share-article" title="Share this article">Share</a> </div> </div> </div> <div id=article-interaction-control-tab class=article-interaction-control-tab> <div id=hide-article-interaction class=article-interaction-control title="Hide Toolbox">&#9644;</div> <div id=show-article-interaction class="article-interaction-control open" title="Show Toolbox">&#10010;</div> </div> </div> <div class="article-header-information article-page"> <div class="f1r-article-mobile article-heading-bar"></div> <div class="article-type article-display">Review </div> <div class="article-title-and-info article-view highlighted-article" id=anchor-title> <h1>Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context</h1><span class=other-info> [version 1; peer review: 2 approved]</span> </div> <div class=article-subtitle></div> <div class=f1r-article-desk> <div class="authors _mdl-layout"><span class=""><a href="mailto:susan.wardle@nih.gov" title="Send email" class="cauthor research-layout"><span class='f1r-icon icon-6_email orange'></span><span>Susan G. Wardle</span></a><a href="https://orcid.org/0000-0003-2216-7461" target=_blank id=author-orcid-0><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=author-orcid-0><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0003-2216-7461</div>,&nbsp;</span><span class="">Chris I. Baker<a href="https://orcid.org/0000-0001-6861-8964" target=_blank id=author-orcid-1><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=author-orcid-1><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0001-6861-8964</div></span></div> </div> <div class=f1r-article-mobile> <div class="authors _mdl-layout"><span class=""><a href="mailto:susan.wardle@nih.gov" title="Send email" class="cauthor research-layout"><span class='f1r-icon icon-6_email orange'></span><span>Susan G. Wardle</span></a><a href="http://orcid.org/0000-0003-2216-7461" target=_blank id=mauthor-orcid-0><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=mauthor-orcid-0><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0003-2216-7461</div>,&nbsp;</span><span class="">Chris I. Baker<a href="http://orcid.org/0000-0001-6861-8964" target=_blank id=mauthor-orcid-1><span class=orcid-logo-for-author-list></span></a><div class="mdl-tooltip mdl-tooltip--wider" for=mauthor-orcid-1><span class=orcid-logo-for-author-list></span> https://orcid.org/0000-0001-6861-8964</div></span></div> </div> <div class=f1r-article-mobile> <div class=article-pubinfo-mobile> PUBLISHED 11 Jun 2020 </div> </div> <span class=Z3988 title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info:ofi/fmt:kev:mtx:journal&amp;rft_id=info:doi/10.12688%2Ff1000research.22296.1"></span> <div class=f1r-article-desk> <div class="contracted-details first"> <a href="#" class="contracted-details-label author-affiliations"><span class=contracted></span>Author details</a> <a href="#" class=section-title>Author details</a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details affiliations is-hidden"> Laboratory of Brain and Cognition, National Institute of Mental Health, National Institutes of Health, Bethesda, MD, 20892, USA<br/> <p> <div class=margin-bottom> Susan G. Wardle <br/> <span>Roles: </span> Conceptualization, Writing – Original Draft Preparation, Writing – Review & Editing </div> <div class=margin-bottom> Chris I. Baker <br/> <span>Roles: </span> Conceptualization, Writing – Original Draft Preparation, Writing – Review & Editing </div> </p> </div> </div> </div> <div class=f1r-article-mobile> <div class="article-page-section-box margin-bottom-40 research-layout"> <span class=box-title> <span class="f1r-icon icon-85_peer_review"></span> OPEN PEER REVIEW </span> <button class="tertiary grey float-right" data-scrollto=article-reports>DETAILS</button> <div class="status-row referee-reports-container"> REVIEWER STATUS <span class=status-icons> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved data-refInfo=68927-64605></span> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved data-refInfo=68926-64604></span> </span> </div> </div> </div> </div> <h2 class="article-headings article-page-abstract" id=anchor-abstract> <span class="f1r-article-mobile-inline abstract-heading-border"></span> Abstract </h2> <div class="article-abstract article-page-general-text-mobile research-layout"> <div class="abstract-text is-expanded"> Object recognition is the ability to identify an object or category based on the combination of visual features observed. It is a remarkable feat of the human brain, given that the patterns of light received by the eye associated with the properties of a given object vary widely with simple changes in viewing angle, ambient lighting, and distance. Furthermore, different exemplars of a specific object category can vary widely in visual appearance, such that successful categorization requires generalization across disparate visual features. In this review, we discuss recent advances in understanding the neural representations underlying object recognition in the human brain. We highlight three current trends in the approach towards this goal within the field of cognitive neuroscience. Firstly, we consider the influence of deep neural networks both as potential models of object vision and in how their representations relate to those in the human brain. Secondly, we review the contribution that time-series neuroimaging methods have made towards understanding the temporal dynamics of object representations beyond their spatial organization within different brain regions. Finally, we argue that an increasing emphasis on the context (both visual and task) within which object recognition occurs has led to a broader conceptualization of what constitutes an object representation for the brain. We conclude by identifying some current challenges facing the experimental pursuit of understanding object recognition and outline some emerging directions that are likely to yield new insight into this complex cognitive process. </div> <div class=abstract-for-mobile> <div class="margin-top-30 padding-bottom-30 research-layout is-centered"> <button class="primary orange-text white-bg bigger-text abstract-expand-button-mobile with-border show" style="display: none;"> READ ALL <span class="f1r-icon icon-14_more_small orange vmiddle big"></span> </button> <button class="primary orange-text white-bg bigger-text abstract-expand-button-mobile with-border hide"> READ LESS <span class="f1r-icon icon-10_less_small orange vmiddle big"></span> </button> </div> </div> </div> <div class=clearfix></div> <div class="article-context no-divider"> <div class="article-abstract article-page-general-text-mobile research-layout generated-article-body"> <h2 class=main-title>Keywords</h2> <p class="u-mb--0 u-pb--2"> object recognition, human vision, fMRI, MEG, DNN, visual perception </p> </div> </div> <div class=article-information> <span class="info-separation padding-bottom"> <div id=corresponding-author-icon class="email-icon float-left"> <span class="f1r-icon icon-6_email orange"></span> <div id=corresponding-author-window class="margin-top-20 popup-window-wrapper is-hidden"> <div class="popup-window corresponding-authors-popup"> <div class=corresponding-author-container> <div class="popup-window-title small">Corresponding Author(s)</div> <div class=authors> Susan G. Wardle (<a href="mailto:susan.wardle@nih.gov">susan.wardle@nih.gov</a>) </div> </div> <div class="margin-top margin-bottom float-left"> <button id=close-popup-window class=general-white-orange-button>Close</button> </div> </div> </div> </div> <span class="icon-text float-left" data-test-id=box-corresponding-author> <b>Corresponding author:</b> Susan G. Wardle </span> <div class=clearfix></div> </span> <span class="info-separation padding-bottom competing-interests-display"> <span class=competing-interests-title>Competing interests:</span> No competing interests were disclosed. </span> <div class="info-separation padding-bottom grant-information-display"> <span class=grant-information-title>Grant information:</span> This work is supported by the Intramural Research Program of the National Institute of Mental Health (ZIAMH002909). <br/> <i>The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</i> </div> <span class="f1r-article-desk info-separation padding-bottom"> <span class="copywrite-icon float-left"> <span class="f1r-icon icon-100_open_access"></span> </span> <span class="icon-text float-left" data-test-id=box-copyright-text> <b>Copyright:</b>&nbsp; © 2020 Wardle SG and Baker CI. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/" target=_blank data-test-id=box-licence-link>Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. The author(s) is/are employees of the US Government and therefore domestic copyright protection in USA does not apply to this work. The work may be protected under the copyright laws of other jurisdictions when used in those jurisdictions. </span> <div class=clearfix></div> </span> <span class="info-separation padding-bottom" data-test-id=box-how-to-cite> <b>How to cite:</b> <span class="article-title-and-info in-article-box"> Wardle SG and Baker CI. Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context [version 1; peer review: 2 approved]</span>. <i>F1000Research</i> 2020, <b>9</b>(F1000 Faculty Rev):590 (<a href="https://doi.org/10.12688/f1000research.22296.1" target=_blank>https://doi.org/10.12688/f1000research.22296.1</a>) </span> <span class=info-separation data-test-id=box-first-published><b>First published:</b> 11 Jun 2020, <b>9</b>(F1000 Faculty Rev):590 (<a href="https://doi.org/10.12688/f1000research.22296.1" target=_blank>https://doi.org/10.12688/f1000research.22296.1</a>)</span> <span class=info-separation data-test-id=box-latest-published><b>Latest published:</b> 11 Jun 2020, <b>9</b>(F1000 Faculty Rev):590 (<a href="https://doi.org/10.12688/f1000research.22296.1" target=_blank>https://doi.org/10.12688/f1000research.22296.1</a>)</span> </div> <div class=clearfix></div> <div id=article-context class=article-context> <div id=article1-body class=generated-article-body><h2 class=main-title id=d23660e142>Introduction</h2><p class="" id=d23660e145>Object recognition is one of the classic “problems” of vision<sup><a href="#ref-1">1</a></sup>. The underlying neural substrate in humans was revealed by classic neuropsychological studies which pointed to selective deficits in visual object recognition following lesions to specific brain regions<sup><a href="#ref-2">2</a>,<a href="#ref-3">3</a></sup>, yet we still do not understand how the brain achieves this remarkable behavior. How is it that we reliably<sup><a href="#ref-4">4</a></sup> and rapidly<sup><a href="#ref-5">5</a></sup> recognize objects despite considerable retinal image transformations arising from changes in viewing angle, position, image size, and lighting? Much experimental and computational work has focused on this problem of invariance<sup><a href="#ref-4">4</a>,<a href="#ref-6">6</a>–<a href="#ref-13">13</a></sup>. Early neuroimaging studies of object recognition using functional magnetic resonance imaging (fMRI) focused on regions in the lateral occipital and ventral temporal cortex, which were found to respond more strongly to the presentation of objects than to textures or scrambled objects<sup><a href="#ref-14">14</a>,<a href="#ref-15">15</a></sup>. More recently, the application of multivariate analysis techniques has led to broader investigation of the structure of object representations<sup><a href="#FN1">a</a></sup> throughout the ventral temporal cortex<sup><a href="#ref-16">16</a>,<a href="#ref-17">17</a></sup> and their temporal dynamics across the whole brain<sup><a href="#ref-18">18</a>,<a href="#ref-19">19</a></sup>. While these representations are assumed to contribute to object recognition behavior, they may also contribute to other tasks. This shift toward object representations has also accompanied a greater focus on revealing how a broad range of different object categories are represented rather than investigating the invariant representation of single objects. Such object categorization involves a similar issue of extrapolation across changes in visual features as invariance, since exemplars (e.g. Great Dane and Chihuahua) of a category (e.g. “dog”) often have significantly different visual features from one another.</p><p class="" id=d23660e203>The aim of this review is to provide an overview of recent advances in understanding object recognition in the human brain. In this review, we primarily consider contemporary work from the past three years in human cognitive neuroscience, identifying the current trends in the field rather than providing an exhaustive summary. In addition, we focus on the neural basis of visual object recognition in the human brain (for reviews including non-human primate studies, see <a href="#ref-20">20</a>,<a href="#ref-21">21</a>) rather than the related topics of computer vision, object memory, and semantic object knowledge. We define visual objects as meaningful conjunctions of visual features<sup><a href="#ref-13">13</a></sup> and object recognition as the ability to distinguish an object identity or category from all other objects<sup><a href="#ref-21">21</a></sup>. Face recognition is not covered in this review, as faces are a unique object class that are processed within a specialized network of regions<sup><a href="#ref-22">22</a>,<a href="#ref-23">23</a></sup>.</p><p class="" id=d23660e227>We identify three current trends in the approach towards understanding object recognition within the field of cognitive neuroscience. Firstly, the rapidly growing popularity of deep neural networks (DNNs) has influenced both the type of analytic approach used and the framework from which the questions are asked. Secondly, the adaptation of multivariate methods to time-series neuroimaging methods such as magnetoencephalography (MEG) and electroencephalography (EEG) has highlighted the importance of considering the temporal dynamics in the neural processing of object recognition at a resolution not accessible with fMRI. Finally, the field has begun to move away from examining single objects in isolation towards examining objects within more naturalistic contexts including a variety of both task and visual contexts. In the sections below, we examine each of these trends in turn.</p></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d23660e233>Deep neural networks as models of object vision</h2><p class="" id=d23660e236>DNNs are a class of brain-inspired computer vision algorithms<sup><a href="#ref-24">24</a>–<a href="#ref-26">26</a></sup>. Although there are many variants of the specific network architecture, the term DNN refers to artificial neural networks in which there are multiple (i.e. “deep”) layers in-between the input and output stages<sup><a href="#ref-27">27</a></sup>. DNNs have risen to prominence within cognitive neuroscience relatively recently given high levels of performance in object classification<sup><a href="#ref-28">28</a></sup>, in some cases even performing as well as humans<sup><a href="#ref-29">29</a></sup>. This has led to consideration of the utility of DNNs as potential models of biological vision<sup><a href="#ref-26">26</a>,<a href="#ref-30">30</a></sup>. However, overall performance does not necessarily indicate that the underlying processing is similar to that in the brain. In this section, we highlight several fundamental differences between state-of-the-art DNNs and the brain and consider the potential of DNNs to inform our understanding of human object recognition given these differences.</p><p class="" id=d23660e265>DNNs have recently achieved human levels of performance in terms of accuracy for image classification<sup><a href="#ref-29">29</a></sup>. Specifically, this had been achieved for images from the large database ImageNet and not yet for real world images taken in the wild. An interesting question is to what degree the pattern of successful classification and errors made by DNNs mirror those made by humans making perceptual judgments. Several studies have reported both similarities and differences between human behavior and DNNs. For example, while DNNs can capture human shape sensitivity (with stimuli very different to those on which they were trained)<sup><a href="#ref-31">31</a></sup>, they perform less well than simple categorical models in capturing similarity judgements<sup><a href="#ref-32">32</a>,<a href="#ref-33">33</a></sup> and do not capture human sensitivity to properties such as symmetry<sup><a href="#ref-33">33</a></sup>. One study that revealed clear differences between human and DNN representations compared the performance of humans, macaque monkeys, and DNNs on an invariant object recognition task<sup><a href="#ref-34">34</a></sup>. Stimuli were rendered 3D objects of 24 basic-level categories (e.g. zebra, calculator) superimposed on a natural image background at different orientations/viewpoints (<a href="#f1">Figure 1a</a>). Monkey and human subjects viewed these images and then a binary response screen with two objects in canonical view was shown, and their task was to match the object from the previous stimulus (<a href="#f1">Figure 1b</a>). Notably, while results for object-level confusion were similar among humans, monkeys, and DNNs (<a href="#f1">Figure 1c</a>), performance at the image level did not match between domains (<a href="#f1">Figure 1d</a>). This difference in error patterns suggests that accuracy is not an adequate measure of the similarity between humans and DNNs, as vastly different response patterns can yield comparable accuracy.</p><a name=f1 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24595/57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure1.gif"><img alt="57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure1.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24595/57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure1.gif"></a><div class=caption><h3>Figure 1. Deep neural networks and object recognition.</h3><p id=d23660e314>(<b>a</b>–<b>b</b>) Stimuli and behavioral experimental design used in <a href="#ref-34">34</a>. On each trial, human and monkey observers briefly viewed a synthetic test image of a rotated object placed on a random scene background. They then reported which object had been presented by making a binary choice from one of two objects presented in canonical view on the test screen. (<b>c</b>) Results showed that for object category, humans and several different deep neural networks (DNNs) performed similarly. However, humans made different errors than DNNs at the image level (<b>d</b>). (<b>e</b>) Example images from <a href="#ref-35">35</a>. A DNN was more likely to classify an image based on texture (Indian elephant) than shape (tabby cat), whereas human observers do the reverse. Figures a-d were adapted from Rajalingham <i>et al.</i><sup><a href="#ref-34">34</a></sup> under the terms of the Creative Commons Attribution 4.0 International license (CC-BY 4.0).</p></div></div><p class="" id=d23660e349>The observation that humans and DNNs do not show similar patterns of errors at the image level implies that DNNs and humans are not solving the task in the same way or are not relying on the same source of information. A striking demonstration showed that DNNs can be fooled into misclassifying an object by making small changes to the image that are barely perceptible to human observers<sup><a href="#ref-34">34</a></sup>. The human visual system is also better able to generalize classification across different forms of noise than DNNs<sup><a href="#ref-36">36</a></sup>. An example of clear divergence in the source of information used by humans compared to DNNs is the demonstration that DNNs may favor texture over shape in classifying objects, with the reverse true for human observers<sup><a href="#ref-35">35</a></sup>. For example, DNNs such as ResNet-50 trained on ImageNet labelled a picture of a tabby cat rendered with the texture of elephant skin as an "Indian elephant", whereas human observers would label it as "cat" (<a href="#f1">Figure 1e</a>). Interestingly, re-training the ResNet-50 architecture to learn a shape-based representation using stylized images in which texture was not predictive of object category led to performance more similar to human observers. Furthermore, there were surprising performance benefits that emerged from the shape-based network, such as greater tolerance to image distortions and better object detection performance.</p><p class="" id=d23660e368>Beyond comparing network performance with human behavior, recent studies have also compared the representations for objects and scenes within different layers of DNNs to human brain representations measured with fMRI or MEG<sup><a href="#ref-37">37</a>–<a href="#ref-45">45</a></sup>. Generally, these studies have found that lower layers of DNNs correlate more with earlier regions within the visual processing hierarchy and higher layers with later regions such as the ventral temporal cortex<sup><a href="#ref-39">39</a>,<a href="#ref-45">45</a>–<a href="#ref-48">48</a></sup>. Similarly, time-resolved neuroimaging methods (see also next section) such as MEG have revealed that lower layers of DNNs correlate with human brain activity earlier in time than higher network layers<sup><a href="#ref-37">37</a>,<a href="#ref-40">40</a>,<a href="#ref-47">47</a></sup>. However, substantial differences among the human brain, behavior, and DNN representations are also reported, which show that the relationship among them is complex<sup><a href="#ref-38">38</a>,<a href="#ref-39">39</a>,<a href="#ref-41">41</a>,<a href="#ref-44">44</a></sup>. For example, for a stimulus set that balanced animacy and appearance, DNNs represented animacy over visual appearance, with the opposite relationship in the ventral temporal cortex<sup><a href="#ref-38">38</a></sup>. Similarly, despite striking differences in the representational structure of behavior and fMRI responses, they both showed strong correlations with DNN representations<sup><a href="#ref-39">39</a></sup>. Critically, simply calculating correlations is not sufficient for characterizing the similarity between object representations in the human brain and the representations measured by human behavior or in artificial networks. This is because the correlation among these different representations (i.e. among the brain, behavior, and/or DNNs) can be equal in magnitude but explain different parts of the underlying variance. Fundamental progress will be made when we have better methods of revealing what is <i>driving</i> the correlation among representations in DNNs, behavior, and the human brain, where such correlations do exist.</p><p class="" id=d23660e423>There are several emerging directions that may increase the utility of DNNs for advancing our understanding of human object recognition. It is already clear that the link between visual object representations in the brain and DNN representations for the same objects is not straightforward<sup><a href="#ref-38">38</a>,<a href="#ref-39">39</a>,<a href="#ref-41">41</a></sup>. Most comparisons have been made with existing pre-trained DNNs; however, deeper insights are likely to emerge from training DNNs to test specific predictions<sup><a href="#ref-35">35</a></sup>, which requires systematically varying the task or stimulus set. The addition of biologically plausible architecture to DNNs such as spike-timing-dependent plasticity and latency coding<sup><a href="#ref-49">49</a>,<a href="#ref-50">50</a></sup> may further facilitate the comparison of DNNs and the human brain. For example, the inclusion of recurrent connections more closely captures the dynamic representation of objects in the human brain<sup><a href="#ref-51">51</a>,<a href="#ref-52">52</a></sup>. Similarly, transforming the input images to DNNs in a manner similar to the perturbations resulting from the optics of the human eye, for example by applying retinal filters<sup><a href="#ref-34">34</a></sup>, may increase the similarity in the underlying representations between these networks and the brain or behavior. One of the most interesting findings thus far has been that DNNs occasionally spontaneously demonstrate features of visual processing that mirror human perception such as generalization over shape or image distortion<sup><a href="#ref-31">31</a>,<a href="#ref-35">35</a></sup>. Examination of the conditions under which this occurs may be enlightening for understanding how the human brain achieves object recognition under much more varied viewing conditions and tasks than even state-of-the-art DNNs.</p></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d23660e469>The temporal dynamics in neural object representations</h2><p class="" id=d23660e472>In recent years, the application of multivariate analyses to time-series neuroimaging methods such as MEG and EEG has facilitated new investigation into the temporal dynamics of cognitive processes. Visual object recognition has been one of the main subfields of cognitive neuroscience to first adapt these methods<sup><a href="#ref-53">53</a></sup>. Object recognition is fast<sup><a href="#ref-5">5</a></sup>; we can recognize an object in tens of milliseconds. This is much faster than the typical resolution of BOLD fMRI (e.g. 2 seconds); thus, unpacking the temporal evolution of object representations requires alternative neuroimaging methods with millisecond precision. Here we focus on recent work that has revealed the temporal dynamics of object representations in the human brain.</p><p class="" id=d23660e483>Object representations potentially reflect a number of different properties, which together can be considered to form an “object concept”<sup><a href="#ref-54">54</a></sup>. For example, an object concept might include its visual features, the conceptual knowledge associated with an object such as its function, and its relationship to other objects. Neuroimaging methods with high temporal resolution offer the potential to examine the time course of the contribution of these different properties to the underlying object representations. MEG decoding studies have revealed that object identity and category can be decoded in under 100 milliseconds following visual stimulus onset<sup><a href="#ref-18">18</a>,<a href="#ref-19">19</a></sup>. The facilitation of objects presented in typical rather than atypical visual field locations occurs around 140 milliseconds<sup><a href="#ref-55">55</a></sup>, suggestive of a relatively early contribution of expectation based on visual experience. In contrast, contextual facilitation for classifying the animacy of degraded objects in scenes compared to the same objects presented in the absence of scene context occurs relatively late, 320 milliseconds after stimulus onset, suggestive of a feedback mechanism<sup><a href="#ref-56">56</a></sup>.</p><p class="" id=d23660e505>The contribution of conceptual information to object representations develops after initial visual processing. The emergence of categorical structure based on animacy and real-world object size occurs around 150 milliseconds<sup><a href="#ref-57">57</a></sup>. This is consistent with estimates of the lower bound of the formation of conceptual object representations<sup><a href="#ref-37">37</a></sup>. Using MEG data recorded for two stimulus sets of 84 object concepts, generalization across exemplars emerged ~150 milliseconds after onset. The shared semantic relationships between the objects was assessed with the Global Vectors for Word Representation (GloVe) model<sup><a href="#ref-58">58</a></sup>, an unsupervised algorithm trained on word co-occurrences. Consistent with the time course of generalization around 150 milliseconds, <a href="#f2">Figure 2a</a> shows that the correlation with the MEG data for behavioral similarity judgements on the stimuli and the GloVe model of semantic information based on word representations both peaked around this time and later than the correlation with representations of the stimuli from an early layer of a DNN. Similarly, the correlation between dynamic MEG representations of objects on their natural backgrounds and measures of behavioral similarity based on shape, color, function, background, or free arrangement is all before 200 milliseconds<sup><a href="#ref-41">41</a></sup> and consists of overlapping representations in time (<a href="#f2">Figure 2b</a>). For individual object representations, a model that combines a visual feature model (e.g. HMax<sup><a href="#ref-59">59</a></sup> or AlexNet<sup><a href="#ref-28">28</a></sup> DNN) with a model of semantic features better predicts neural representations measured with MEG than using visual features alone<sup><a href="#ref-60">60</a>,<a href="#ref-61">61</a></sup>. The contribution of semantic information to object representations has been linked to activity in the perirhinal cortex<sup><a href="#ref-62">62</a></sup> and anterior temporal cortex<sup><a href="#ref-63">63</a></sup>. Collectively, these results are indicative of a relatively early role for conceptual information in object representations that follows the initial visual processing.</p><a name=f2 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24595/57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure2.gif"><img alt="57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure2.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24595/57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure2.gif"></a><div class=caption><h3>Figure 2. Temporal dynamics of object recognition.</h3><p id=d23660e564>(<b>a</b>) The correlation over time between magnetoencephalography (MEG) whole-brain object representations and the representations from several models based on deep neural network (DNN) layers, behavioral similarity judgments, and the Global Vectors for Word Representations (GloVe) model<sup><a href="#ref-37">37</a></sup>. Note that the lower DNN layer has an earlier peak than the higher layer. (<b>b</b>) Correlation over time between whole-brain MEG object representations and models based on several different visual and conceptual features<sup><a href="#ref-41">41</a></sup>. (<b>c</b>) Functional magnetic resonance imaging–MEG “fusion” reveals a peak correlation between whole-brain MEG object representations and those in the primary visual cortex (V1) at 101 milliseconds (ms) and ventral temporal cortex at 132 ms<sup><a href="#ref-19">19</a></sup>. Figure b is adapted from Cichy <i>et al.</i><sup><a href="#ref-41">41</a></sup> under the terms of the Creative Commons Attribution 4.0 International license (CC-BY 4.0).</p></div></div><p class="" id=d23660e600>Another advantage of studying object representations with high temporal resolution is the potential to disentangle the role of feedforward versus feedback processes in their formation. Feedback is theoretically difficult to study empirically and although its role in visual perception has been acknowledged for decades, the advent of recurrent connections in DNNs<sup><a href="#ref-52">52</a></sup> has reignited interest in attempting to separate the contribution of feedback vs. feedforward processes in object recognition. For example, a computational model incorporating recurrent connections could partially account for occluded object representations measured with MEG, which had a decoding peak much later in time than un-occluded objects<sup><a href="#ref-43">43</a></sup>. This suggests feedback processes assist in processing objects under more ambiguous viewing conditions such as occlusion. One recent approach towards isolating the contribution of feedback has been to use the rapid serial visual presentation of objects at very brief presentations under the assumption that rapid presentation disrupts feedback processing of the preceding object(s)<sup><a href="#ref-64">64</a>,<a href="#ref-65">65</a></sup>.</p><p class="" id=d23660e618>One of the challenges the contribution of time-resolved neuroimaging has brought to light is how best to integrate fMRI results with MEG/EEG to elucidate the combined spatial and temporal processing of object recognition. One approach is to use source localization to model the spatial source of the MEG signal in the brain<sup><a href="#ref-52">52</a></sup>. An alternative method, fMRI–MEG “fusion”, correlates dissimilarity matrices constructed separately from fMRI and MEG data over time (MEG) and regions of interest (fMRI)<sup><a href="#ref-19">19</a>,<a href="#ref-66">66</a></sup>. This approach has been used successfully to demonstrate that whole-brain object representations measured with MEG have a peak correlation earlier in time with the primary visual cortex (V1) and later in time with the ventral temporal cortex<sup><a href="#ref-19">19</a>,<a href="#ref-66">66</a></sup> (<a href="#f2">Figure 2c</a>). Furthermore, fusion revealed temporal differences in the contribution of task versus object representations across the visual hierarchy<sup><a href="#ref-67">67</a></sup>. Although these results provide a useful validation of the method, the interpretation of fusion results is not straightforward, particularly because of the substantial differences in the spatial resolution between fMRI and MEG. For example, one pair of studies used an object stimulus set that controlled for shape (e.g. snake and rope) across category in order to examine the influence of perceptual and categorical similarity on object representations. Even though the studies used identical stimuli, the results were different between the two neuroimaging modalities: they found more evidence for categorical similarity with fMRI<sup><a href="#ref-68">68</a></sup> and perceptual similarity with MEG<sup><a href="#ref-69">69</a></sup>.</p><p class="" id=d23660e655>The results reviewed above demonstrate the importance of understanding the temporal dynamics of object recognition. So far, multivariate methods applied to MEG and EEG data with high temporal resolution have yielded new insights into the temporal dynamics of semantic versus visual features in object representations and highlighted a possible role for feedback from higher visual areas in the representation of degraded and occluded objects as well as in task-relevant representations. The development of a new generation of wearable MEG systems based on arrays of optically pumped magnetometers promises further advancement in the measurement of brain activity at a high temporal resolution in more varied contexts<sup><a href="#ref-70">70</a>,<a href="#ref-71">71</a></sup>. Significant progress will be made with further improvements in linking spatial and temporal neuroimaging data.</p></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d23660e668>Contextual effects on object representations</h2><p class="" id=d23660e671>Traditionally, object perception has been studied empirically by presenting single objects in isolation on blank backgrounds<sup><a href="#ref-17">17</a>,<a href="#ref-72">72</a>,<a href="#ref-73">73</a></sup>. This approach facilitates studying aspects of object recognition such as viewpoint and position invariance without a contribution from the background; however, it likely over-emphasizes the role of object shape. More recently, the context in which object recognition occurs has been increasingly considered in studies aiming to understand the underlying neural mechanisms. This can be the visual context, such as the placement of an object in a scene (either relevant<sup><a href="#ref-39">39</a>,<a href="#ref-56">56</a></sup> or irrelevant<sup><a href="#ref-34">34</a></sup>), the action of an agent (e.g. person) involving the object<sup><a href="#ref-74">74</a></sup>, or even the use of 3D real objects rather than 2D images<sup><a href="#ref-75">75</a>,<a href="#ref-76">76</a></sup>. Or, alternatively, this can be task context, with neural object representations measured as participants perform different tasks on the same object stimuli<sup><a href="#ref-77">77</a></sup>. An advantage of all of these approaches with broader scope is that they examine object recognition in circumstances that more closely mimic real-world perception. The results we review here suggest that both visual and task context play a significant role in object processing.</p><div class=section><a name=d23660e711 class=n-a></a><h3 class=section-title>Visual context: interactions with people and scenes</h3><p class="" id=d23660e716>The simplest form of visual context is to present two objects at a time instead of one. In object-selective cortex, the brain activation patterns to two objects are well-predicted by the average responses to the objects presented in isolation<sup><a href="#ref-78">78</a>,<a href="#ref-79">79</a></sup>. More recently, it has been shown that even without the visual context of a detailed scene, the brain representations of objects are affected by expectation driven by context. For example, a fMRI study looked at object pairs taken from scenes (such as a sofa and TV, car and traffic light) presented in their original location versus interchanged locations relative to each other on a blank background<sup><a href="#ref-80">80</a></sup>. In the object-selective cortex, the mean of the activation patterns for two isolated objects presented centrally was less similar to the activation patterns for the object pairs when they were in their original location compared with the interchanged location, but this was not the case in the early visual cortex. This suggests that the object-selective cortex is sensitive to the expected location of different objects relative to each other.</p><p class="" id=d23660e730>A related observation is that the location of objects within scenes in the real world is not arbitrary, and objects occur within relatively predictable locations related to their function<sup><a href="#ref-81">81</a></sup>. In some cases, this produces a statistical regularity in the visual field location (<a href="#f3">Figure 3c</a>). There is some evidence that object processing is facilitated when this expectation is adhered to and objects occur in their typical retinotopic visual field location (i.e. their position relative to the direction of eye gaze). For example, in the object-selective cortex, objects in their typical visual field location (e.g. hat in upper visual field, shoe in lower visual field) could be decoded at a higher rate from the fMRI activation patterns than when they were in the atypical portion of the visual field (<a href="#f3">Figure 3d</a>)<sup><a href="#ref-82">82</a></sup>. Other higher visual areas in ventral temporal cortex did not show such a difference. EEG results suggest there is a difference in the representation of objects in typical vs. atypical locations as early as 140 milliseconds after stimulus onset<sup><a href="#ref-55">55</a></sup>. Overall, the sensitivity of the object-selective cortex to statistical regularities in the location of objects is consistent with the idea of efficient coding in the visual system<sup><a href="#ref-83">83</a></sup>, which argues that statistical regularities in the environment can be exploited by neural coding in order to conserve the amount of brain resources engaged in representing the complex visual world.</p><a name=f3 class=n-a></a><div class="fig panel clearfix"><a target=_blank href="https://f1000researchdata.s3.amazonaws.com/manuscripts/24595/57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure3.gif"><img alt="57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure3.gif" src="https://f1000researchdata.s3.amazonaws.com/manuscripts/24595/57cdedb1-5cdf-49ac-9838-c296d1c17aa9_figure3.gif"></a><div class=caption><h3>Figure 3. Effect of visual context on the neural representation of objects.</h3><p id=d23660e766>(<b>a</b>) Some objects are associated with a typical visual field position in which they tend to occur<sup><a href="#ref-82">82</a></sup>. The top row shows object locations from a labelled image database, and the bottom row shows the placement of objects by human observers. (<b>b</b>) In the lateral occipital cortex (LOC), decoding accuracy was higher for objects presented in their typical (e.g. hat in upper visual field) than their atypical (e.g. hat in lower visual field) location. (<b>c</b>) Example stimuli used in <a href="#ref-74">74</a> of objects in “interacting” and “non-interacting” contexts. A decoding searchlight analysis revealed areas that had higher decoding accuracy for interacting than non-interacting objects. (<b>d</b>) Super-additive decoding accuracy in object-selective lateral occipital and posterior fusiform regions for degraded objects in scenes compared to decoding accuracy for isolated objects or scenes alone<sup><a href="#ref-56">56</a></sup>. EBA, extrastriate body area; pSTS, posterior superior temporal sulcus Figure d is adapted from Brandman and Peelan<sup><a href="#ref-56">56</a></sup> under the terms of the Creative Commons Attribution 4.0 International license (CC-BY 4.0).</p></div></div><p class="" id=d23660e801>Another consideration in the representation of multiple objects beyond their relative location is their function. An inherent property of objects is their manipulability, and several studies have investigated how this affects their neural representation<sup><a href="#ref-74">74</a>,<a href="#ref-84">84</a>,<a href="#ref-85">85</a></sup>. The degree to which interactions with people and scenes mediates object representations is not homogenous across brain regions. For example, one study examined the effect of interaction on object representations using a stimulus set of humans, guitars, and horses<sup><a href="#ref-74">74</a></sup> (<a href="#f3">Figure 3a</a>). They measured brain responses to the isolated objects and for object pairs when they were both interacting (e.g. person riding a horse) and not interacting (e.g. person in front of a horse). In some brain regions, the representation of meaningfully interacting objects was not well predicted by the responses to their individual parts, suggesting coding of the object interaction. For example, a decoding searchlight analysis of the fMRI data revealed areas overlapping with the body-selective extrastriate body area (EBA) and posterior superior temporal sulcus (pSTS) that had higher decoding accuracy for interacting than non-interacting objects (<a href="#f3">Figure 3b</a>).</p><p class="" id=d23660e825>Beyond simple object pairs, similar logic has also been applied to examine how scene context affects object representations. For example, one study measured BOLD activation patterns to degraded objects (either animate or inanimate) presented both in isolation and within intact scenes<sup><a href="#ref-56">56</a></sup>. A classifier was trained to distinguish activation patterns associated with animate versus inanimate objects on separate data from intact isolated objects and then tested on the patterns associated with the degraded objects both in isolation and within a scene. In both lateral occipital and posterior fusiform regions, cross decoding accuracy for object animacy was significantly higher for the degraded objects within scenes than that predicted by accuracy for isolated degraded objects and isolated intact scenes (<a href="#f3">Figure 3c</a>). However, in scene-selective regions, this was not the case, and decoding accuracy was only additive. These results suggest that object representations in object- but not scene-selective regions are enhanced by the presence of relevant visual context.</p><p class="" id=d23660e835>Collectively, the studies discussed above highlight the importance of considering the visual context in which objects occur. In the next section, we consider the importance of task context.</p></div><div class=section><a name=d23660e839 class=n-a></a><h3 class=section-title>Task context: the stability of object representations in visual cortex</h3><p class="" id=d23660e844>Given that objects are both recognizable and actionable things, an important question is how the neural representation of objects supports behavior. We can make a multitude of judgements about an object, as well as pick them up and use them in action. How do neural object representations change depending on the goal of the observer? In an experimental paradigm, this usually takes the form of keeping the visual stimuli constant and changing the task of the observer. Such changes in task may affect the relevant information and consequently change the distribution of attention. Within the higher visual cortex, where category-selectivity emerges, the majority of results seem to support fairly limited transformation of object representations as a function of task relative to the modulation by object type<sup><a href="#ref-66">66</a>,<a href="#ref-76">76</a>,<a href="#ref-85">85</a>–<a href="#ref-89">89</a></sup>. However, in the early visual cortex, there may be strong effects of task, potentially reflecting changes in spatial attention. Consistent with these generalizations, an MEG study found that the impact of task (semantic, e.g. classify the object as small or large, or perceptual, e.g. color discrimination) had a relatively late magnitude effect on object representations across the whole-brain MEG signal rather than a qualitative change to the nature of the representation<sup><a href="#ref-67">67</a></sup>. Furthermore, MEG–fMRI fusion suggested the effect of task increased further up the processing hierarchy. Together, this suggests that other brain regions in addition to higher-level visual cortex have an important role in task modulation<sup><a href="#ref-90">90</a></sup>. This is in contrast to the effect of visual context reviewed above, in which there was significant modulation of object representations in higher-level visual regions.</p><p class="" id=d23660e868>Consistent with a locus that is not restricted to the visual cortex, there is considerable evidence for a substantial role of parietal and frontal brain regions in the task modulation of object representations. For example, to address this question, one fMRI study used a stimulus set of 28 objects where semantic category and action associated with the objects were dissociated<sup><a href="#ref-86">86</a></sup>. Participants performed two tasks on the same stimuli while within the fMRI scanner: rate objects on a four-point scale from very similar to very different for either hand action similarity or category similarity. For example, pictures of a drum and hammer would be similar for action/manipulation similarity, but drum and violin would be more similar for categorical similarity (both musical instruments). An analysis of the similarity of the brain activation patterns for the different objects revealed that in parietal and prefrontal areas, an action model of the stimuli correlated more with the similarity of object activation patterns during the action task, and vice versa for the category task<sup><a href="#ref-86">86</a></sup>. Frontoparietal areas also showed greater within-task correlations than between-task, but this did not differ for occipitotemporal areas. Physical and perceived shape correlated with representations more in occipitotemporal regions. Consistent with this, there is evidence for a difference in the representational space of how objects are represented in occipitotemporal and posterior parietal regions<sup><a href="#ref-91">91</a></sup>, with more flexible representations modulated by task in the posterior parietal cortex<sup><a href="#ref-88">88</a></sup>.</p><p class="" id=d23660e887>Collectively, these findings suggest that while task context can affect object representations within the brain, these effects tend to be largest at higher stages of the visual hierarchy with strongest effects in the prefrontal and parietal cortex.</p></div></div><div id=article1-body class=generated-article-body><h2 class=main-title id=d23660e894>“Beyond” object recognition</h2><p class="" id=d23660e897>Here we have reviewed three current trends in the field of object recognition: the influence of DNNs, temporal dynamics, and the relevance of different forms of context. These trends have focused the field to consider object representations more broadly rather than object recognition <i>per se</i>. Such representations are likely critical for object recognition but will also contribute to many other behaviors<sup><a href="#ref-92">92</a></sup>. To conclude, we briefly consider some current challenges in the pursuit of understanding object representations in the human brain and outline some emerging trends that are likely to help push the field forward.</p><p class="" id=d23660e907>The first issue we consider is what should “count” as an object representation. A consequence of the relevance of visual and task context reviewed in the section above is that it suggests object representations are broader than the particular conjunction of visual properties that visually define the object. The frequent investigation of the neural representation of isolated objects without context may have over-emphasized the role of shape in the underlying representations of real-world objects. Indeed, shape has been found to be a strong predictor of the similarity of the neural representations for different objects<sup><a href="#ref-41">41</a></sup>. Similarly, the focus on functional object-selective brain regions, which are localized by contrasts between, for example, isolated objects and scrambled objects<sup><a href="#ref-73">73</a></sup>, emphasizes the role of brain regions which are sensitive to shape above other object properties. However, there is evidence that other high-level regions such as scene-selective cortex<sup><a href="#ref-85">85</a></sup> and parietal and prefrontal regions<sup><a href="#ref-86">86</a>,<a href="#ref-88">88</a>,<a href="#ref-90">90</a>,<a href="#ref-91">91</a></sup> are also engaged in object processing. Similar to the importance of visual and task context in object representations, further consideration of object-specific properties such as the role of color<sup><a href="#ref-93">93</a>,<a href="#ref-94">94</a></sup> and material properties<sup><a href="#ref-95">95</a></sup> is likely to provide a new perspective on the nature of object representations. Object representations in the human brain are also tied to other features such as conceptual knowledge<sup><a href="#ref-54">54</a></sup> about their function and relationship to other objects, which are yet to be emulated by DNNs in a way that produces the same flexibility as the human brain.</p><p class="" id=d23660e951>A second important issue in investigating the nature of object representations is stimulus selection and presentation. In the last decade, there has been concentrated effort to use larger stimulus sets (n = ~100) of objects in neuroimaging event-related designs in an effort to reveal the inherent organization of object categories in brain representations without imposing stimulus groupings in the experimental design<sup><a href="#ref-17">17</a></sup>. This is in contrast to blocked stimulus presentation, which is not desirable for investigating representational structure because of inherent biases in the experimental design arising from grouping stimuli together into blocks. However, a limitation of representational similarity analysis<sup><a href="#ref-96">96</a></sup> is that it is relative to the stimulus set used in the experiment and even with ~100 stimuli there are likely to be inherent biases in the stimulus selection. For example, a stimulus set in which shape is a critical difference between stimuli is likely to emphasize a significant role for shape in the organization of the representational space. One recent approach that has potential to move the field forward is the use of very large stimulus sets. Recent databases of 5,000<sup><a href="#ref-97">97</a></sup> and 26,000<sup><a href="#ref-98">98</a></sup> visual object images have potential to reveal new insight that has not been possible using experimenter-selected restricted stimulus sets of ~100 images. Additionally, the method used for image selection in the creation of these large stimulus sets is still important in avoiding biases. For example, the THINGS database<sup><a href="#ref-98">98</a></sup> was created by systematically sampling concrete picturable and nameable nouns from American English in order to avoid any explicit or implicit biases in stimulus selection.</p><p class="" id=d23660e974>Finally, there has been considerable debate over what degree object representations are reducible to the low- and mid-level visual features that co-vary with category membership<sup><a href="#ref-38">38</a>,<a href="#ref-68">68</a>,<a href="#ref-69">69</a>,<a href="#ref-99">99</a>–<a href="#ref-103">103</a></sup>. However, this question may be ill-posed. By definition, visual object representations must be characterized by visual features to some degree; even though different object images can be matched for some visual features (e.g. spatial frequency), they will always differ on others (e.g. global form).</p><p class="" id=d23660e994>In summary, progress in understanding object recognition over the last three years has been characterized by the influence of DNNs, inspection of the time course of neural responses in addition to their spatial organization, and a broader conceptualization of what constitutes an object representation that includes the influence of context. A cohesive understanding of the neural basis of object recognition will also require integrating our knowledge of visual object processing with related processes such as object memory<sup><a href="#ref-104">104</a></sup>, which are typically studied independently. Although DNNs have now reached human levels of performance<sup><a href="#ref-28">28</a>,<a href="#ref-29">29</a></sup> for object categorization under controlled conditions, humans perform this task daily under much more varied conditions and constraints. The continued evolution of the field in terms of sophisticated analytic tools, larger stimulus sets, and the consideration of the context in which object recognition occurs will provide further insight into the human brain's remarkable flexibility.</p></div><div id=article1-back class=generated-article-footer><div class=back-section><a name=d23660e1012 class=n-a></a><h2 class=main-title id=d23914>Footnotes</h2><div class=footnote><a name=FN1 class=n-a></a><p id=d23660e1018> <sup>a</sup> We use the term 'object representations' here to mean the measured patterns of response in the brain associated with object perception, rather than a specific internal representation.</p></div></div><div class=back-section><a name=d23660e1025 class=n-a></a><span class="research-layout prime-recommended-wrapper reference-heading is-hidden"><span class="f1r-icon icon-79_faculty_recommended_badge red vmiddle default-cursor"></span><span class="prime-red big">F1000 recommended</span></span><h2 class=main-title id=d24297>References</h2><div class="section ref-list"><a name=d23660e1025 class=n-a></a><ul><li><a name=ref-1 class=n-a></a><span class=label>1. </span>&nbsp;<span class=citation><a name=d23660e1032 class=n-a></a>Gauthier I, Tarr MJ: Visual Object Recognition: Do We (Finally) Know More Now Than We Did? <i>Annu Rev Vis Sci.</i> 2016; <b>2</b>: 377–96. <a target=xrefwindow id=d23660e1040 href="http://www.ncbi.nlm.nih.gov/pubmed/28532357">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1043 href="https://doi.org/10.1146/annurev-vision-111815-114621">Publisher Full Text </a></span></li><li><a name=ref-2 class=n-a></a><span class=label>2. </span>&nbsp;<span class=citation><a name=d23660e1052 class=n-a></a>Warrington EK: Neuropsychological studies of object recognition. <i>Philos Trans R Soc Lond B Biol Sci.</i> 1982; <b>298</b>(1089): 15–33. <a target=xrefwindow id=d23660e1060 href="http://www.ncbi.nlm.nih.gov/pubmed/6125967">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1063 href="https://doi.org/10.1098/rstb.1982.0069">Publisher Full Text </a></span></li><li><a name=ref-3 class=n-a></a><span class=label>3. </span>&nbsp;<span class=citation><a name=d23660e1072 class=n-a></a>Humphreys GW, Forde EME: Hierarchies, similarity, and interactivity in object recognition: “Category-specific” neuropsychological deficits.. <i>Behav Brain Sci.</i> 2001; <b>24</b>(3): 453–76. <a target=xrefwindow id=d23660e1080 href="http://www.ncbi.nlm.nih.gov/pubmed/11682799">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1083 href="https://doi.org/10.1017/S0140525X01004150">Publisher Full Text </a></span></li><li><a name=ref-4 class=n-a></a><span class=label>4. </span>&nbsp;<span class=citation><a name=d23660e1092 class=n-a></a>Biederman I: Recognition-by-components: A theory of human image understanding. <i>Psychol Rev.</i> 1987; <b>94</b>(2): 115–47. <a target=xrefwindow id=d23660e1100 href="http://www.ncbi.nlm.nih.gov/pubmed/3575582">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1103 href="https://doi.org/10.1037/0033-295X.94.2.115">Publisher Full Text </a></span></li><li><a name=ref-5 class=n-a></a><span class=label>5. </span>&nbsp;<span class=citation><a name=d23660e1112 class=n-a></a>Thorpe S, Fize D, Marlot C: Speed of processing in the human visual system. <i>Nature.</i> 1996; <b>381</b>(6582): 520–2. <a target=xrefwindow id=d23660e1120 href="http://www.ncbi.nlm.nih.gov/pubmed/8632824">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1123 href="https://doi.org/10.1038/381520a0">Publisher Full Text </a></span></li><li><a name=ref-6 class=n-a></a><span class=label>6. </span>&nbsp;<span class=citation><a name=d23660e1133 class=n-a></a>Ullman S, Soloviev S: Computation of pattern invariance in brain-like structures. <i>Neural Netw.</i> 1999; <b>12</b>(7–8): 1021–36. <a target=xrefwindow id=d23660e1141 href="http://www.ncbi.nlm.nih.gov/pubmed/12662643">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1144 href="https://doi.org/10.1016/s0893-6080(99)00048-9">Publisher Full Text </a></span></li><li><a name=ref-7 class=n-a></a><span class=label>7. </span>&nbsp;<span class=citation><a name=d23660e1153 class=n-a></a>Wallis G, Rolls ET: Invariant face and object recognition in the visual system. <i>Prog Neurobiol.</i> 1997; <b>51</b>(2): 167–94. <a target=xrefwindow id=d23660e1161 href="http://www.ncbi.nlm.nih.gov/pubmed/9247963">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1164 href="https://doi.org/10.1016/s0301-0082(96)00054-8">Publisher Full Text </a></span></li><li><a name=ref-8 class=n-a></a><span class=label>8. </span>&nbsp;<span class=citation><a name=d23660e1173 class=n-a></a>Logothetis NK, Pauls J, Poggio T: Shape representation in the inferior temporal cortex of monkeys. <i>Curr Biol.</i> 1995; <b>5</b>(5): 552–63. <a target=xrefwindow id=d23660e1181 href="http://www.ncbi.nlm.nih.gov/pubmed/7583105">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1184 href="https://doi.org/10.1016/s0960-9822(95)00108-4">Publisher Full Text </a></span></li><li><a name=ref-9 class=n-a></a><span class=label>9. </span>&nbsp;<span class=citation><a name=d23660e1193 class=n-a></a>Edelman S: Representation and Recognition in Vision. MIT Press, 1999. <a target=xrefwindow id=d23660e1195 href="https://mitpress.mit.edu/books/representation-and-recognition-vision">Reference Source</a></span></li><li><a name=ref-10 class=n-a></a><span class=label>10. </span>&nbsp;<span class=citation><a name=d23660e1204 class=n-a></a>DiCarlo JJ, Cox DD: Untangling invariant object recognition. <i>Trends Cogn Sci.</i> 2007; <b>11</b>(8): 333–41. <a target=xrefwindow id=d23660e1212 href="http://www.ncbi.nlm.nih.gov/pubmed/17631409">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1215 href="https://doi.org/10.1016/j.tics.2007.06.010">Publisher Full Text </a></span></li><li><a name=ref-11 class=n-a></a><span class=label>11. </span>&nbsp;<span class=citation><a name=d23660e1224 class=n-a></a>Ullman S: High-level Vision. MIT Press, 2000. <a target=xrefwindow id=d23660e1226 href="http://cognet.mit.edu/book/high-level-vision">Reference Source</a></span></li><li><a name=ref-12 class=n-a></a><span class=label>12. </span>&nbsp;<span class=citation><a name=d23660e1236 class=n-a></a>Ward EJ, Isik L, Chun MM: General Transformations of Object Representations in Human Visual Cortex. <i>J Neurosci.</i> 2018; <b>38</b>(40): 8526–37. <a target=xrefwindow id=d23660e1244 href="http://www.ncbi.nlm.nih.gov/pubmed/30126975">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1247 href="https://doi.org/10.1523/JNEUROSCI.2800-17.2018">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1250 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6596219">Free Full Text </a></span><div class=note><a name=d23660e1252 class=n-a></a><p class=first id=d23660e1253><a target=xrefwindow href="https://facultyopinions.com/prime/733846884" id=d23660e1254>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-13 class=n-a></a><span class=label>13. </span>&nbsp;<span class=citation><a name=d23660e1263 class=n-a></a>Kravitz DJ, Vinson LD, Baker CI: How position dependent is visual object recognition? <i>Trends Cogn Sci.</i> 2008; <b>12</b>(3): 114–22. <a target=xrefwindow id=d23660e1271 href="http://www.ncbi.nlm.nih.gov/pubmed/18262829">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1274 href="https://doi.org/10.1016/j.tics.2007.12.006">Publisher Full Text </a></span></li><li><a name=ref-14 class=n-a></a><span class=label>14. </span>&nbsp;<span class=citation><a name=d23660e1283 class=n-a></a>Malach R, Reppas JB, Benson RR, <i> et al.</i>: Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex. <i>Proc Natl Acad Sci U S A.</i> 1995; <b>92</b>(18): 8135–9. <a target=xrefwindow id=d23660e1294 href="http://www.ncbi.nlm.nih.gov/pubmed/7667258">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1297 href="https://doi.org/10.1073/pnas.92.18.8135">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1301 href="http://www.ncbi.nlm.nih.gov/pmc/articles/41110">Free Full Text </a></span></li><li><a name=ref-15 class=n-a></a><span class=label>15. </span>&nbsp;<span class=citation><a name=d23660e1310 class=n-a></a>Grill-Spector K, Kourtzi Z, Kanwisher N: The lateral occipital complex and its role in object recognition. <i>Vision Res.</i> 2001; <b>41</b>(10–11): 1409–22. <a target=xrefwindow id=d23660e1318 href="http://www.ncbi.nlm.nih.gov/pubmed/11322983">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1321 href="https://doi.org/10.1016/s0042-6989(01)00073-6">Publisher Full Text </a></span></li><li><a name=ref-16 class=n-a></a><span class=label>16. </span>&nbsp;<span class=citation><a name=d23660e1330 class=n-a></a>Haxby JV, Gobbini MI, Furey ML, <i> et al.</i>: Distributed and overlapping representations of faces and objects in ventral temporal cortex. <i>Science.</i> 2001; <b>293</b>(5539): 2425–30. <a target=xrefwindow id=d23660e1341 href="http://www.ncbi.nlm.nih.gov/pubmed/11577229">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1344 href="https://doi.org/10.1126/science.1063736">Publisher Full Text </a></span><div class=note><a name=d23660e1346 class=n-a></a><p class=first id=d23660e1347><a target=xrefwindow href="https://facultyopinions.com/prime/1000496" id=d23660e1348>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-17 class=n-a></a><span class=label>17. </span>&nbsp;<span class=citation><a name=d23660e1357 class=n-a></a>Kriegeskorte N, Mur M, Ruff DA, <i> et al.</i>: Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey. <i>Neuron.</i> 2008; <b>60</b>(6): 1126–41. <a target=xrefwindow id=d23660e1368 href="http://www.ncbi.nlm.nih.gov/pubmed/19109916">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1371 href="https://doi.org/10.1016/j.neuron.2008.10.043">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1375 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3143574">Free Full Text </a></span></li><li><a name=ref-18 class=n-a></a><span class=label>18. </span>&nbsp;<span class=citation><a name=d23660e1385 class=n-a></a>Carlson T, Tovar DA, Alink A, <i> et al.</i>: Representational dynamics of object vision: The first 1000 ms. <i>J Vis.</i> 2013; <b>13</b>(10): 1. <a target=xrefwindow id=d23660e1396 href="http://www.ncbi.nlm.nih.gov/pubmed/23908380">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1399 href="https://doi.org/10.1167/13.10.1">Publisher Full Text </a></span></li><li><a name=ref-19 class=n-a></a><span class=label>19. </span>&nbsp;<span class=citation><a name=d23660e1408 class=n-a></a>Cichy RM, Pantazis D, Oliva A: Resolving human object recognition in space and time. <i>Nat Neurosci.</i> 2014; <b>17</b>(3): 455–62. <a target=xrefwindow id=d23660e1416 href="http://www.ncbi.nlm.nih.gov/pubmed/24464044">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1419 href="https://doi.org/10.1038/nn.3635">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1422 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4261693">Free Full Text </a></span></li><li><a name=ref-20 class=n-a></a><span class=label>20. </span>&nbsp;<span class=citation><a name=d23660e1431 class=n-a></a>Kravitz DJ, Saleem KS, Baker CI, <i> et al.</i>: The ventral visual pathway: An expanded neural framework for the processing of object quality. <i>Trends Cogn Sci.</i> 2013; <b>17</b>(1): 26–49. <a target=xrefwindow id=d23660e1442 href="http://www.ncbi.nlm.nih.gov/pubmed/23265839">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1445 href="https://doi.org/10.1016/j.tics.2012.10.011">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1449 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3532569">Free Full Text </a></span></li><li><a name=ref-21 class=n-a></a><span class=label>21. </span>&nbsp;<span class=citation><a name=d23660e1458 class=n-a></a>DiCarlo JJ, Zoccolan D, Rust NC: How Does the Brain Solve Visual Object Recognition? <i>Neuron.</i> 2012; <b>73</b>(3): 415–34. <a target=xrefwindow id=d23660e1466 href="http://www.ncbi.nlm.nih.gov/pubmed/22325196">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1469 href="https://doi.org/10.1016/j.neuron.2012.01.010">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1472 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3306444">Free Full Text </a></span></li><li><a name=ref-22 class=n-a></a><span class=label>22. </span>&nbsp;<span class=citation><a name=d23660e1481 class=n-a></a>Kanwisher N, McDermott J, Chun MM: The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception. <i>J Neurosci.</i> 1997; <b>17</b>(11): 4302–11. <a target=xrefwindow id=d23660e1489 href="http://www.ncbi.nlm.nih.gov/pubmed/9151747">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1492 href="https://doi.org/10.1523/JNEUROSCI.17-11-04302.1997">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1495 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6573547">Free Full Text </a></span></li><li><a name=ref-23 class=n-a></a><span class=label>23. </span>&nbsp;<span class=citation><a name=d23660e1504 class=n-a></a>Grill-Spector K, Weiner KS, Kay K, <i> et al.</i>: The Functional Neuroanatomy of Human Face Perception. <i>Annu Rev Vis Sci.</i> 2017; <b>3</b>: 167–96. <a target=xrefwindow id=d23660e1515 href="http://www.ncbi.nlm.nih.gov/pubmed/28715955">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1518 href="https://doi.org/10.1146/annurev-vision-102016-061214">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1522 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6345578">Free Full Text </a></span></li><li><a name=ref-24 class=n-a></a><span class=label>24. </span>&nbsp;<span class=citation><a name=d23660e1532 class=n-a></a>Serre T: Deep Learning: The Good, the Bad and the Ugly. <i>Annu Rev Vis Sci.</i> 2019; <b>5</b>: 399–426. <a target=xrefwindow id=d23660e1540 href="http://www.ncbi.nlm.nih.gov/pubmed/31394043">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1543 href="https://doi.org/10.1146/annurev-vision-091718-014951">Publisher Full Text </a></span></li><li><a name=ref-25 class=n-a></a><span class=label>25. </span>&nbsp;<span class=citation><a name=d23660e1552 class=n-a></a>Kietzmann TC, McClure P, Kriegeskorte N: Deep neural networks in computational neuroscience. Oxford Research Encyclopedia of Neuroscience. 2019; <b>10</b>: 115. <a target=xrefwindow id=d23660e1557 href="https://doi.org/10.1093/acrefore/9780190264086.013.46">Publisher Full Text </a></span></li><li><a name=ref-26 class=n-a></a><span class=label>26. </span>&nbsp;<span class=citation><a name=d23660e1566 class=n-a></a>Kriegeskorte N: Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing. <i>Annu Rev Vis Sci.</i> 2015; <b>1</b>: 417–46. <a target=xrefwindow id=d23660e1574 href="http://www.ncbi.nlm.nih.gov/pubmed/28532370">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1577 href="https://doi.org/10.1146/annurev-vision-082114-035447">Publisher Full Text </a></span><div class=note><a name=d23660e1579 class=n-a></a><p class=first id=d23660e1580><a target=xrefwindow href="https://facultyopinions.com/prime/726501805" id=d23660e1581>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-27 class=n-a></a><span class=label>27. </span>&nbsp;<span class=citation><a name=d23660e1590 class=n-a></a>Kriegeskorte N, Golan T: Neural network models and deep learning. <i>Curr Biol.</i> 2019; <b>29</b>(7): R231–R236. <a target=xrefwindow id=d23660e1598 href="http://www.ncbi.nlm.nih.gov/pubmed/30939301">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1601 href="https://doi.org/10.1016/j.cub.2019.02.034">Publisher Full Text </a></span></li><li><a name=ref-28 class=n-a></a><span class=label>28. </span>&nbsp;<span class=citation><a name=d23660e1610 class=n-a></a>Krizhevsky A, Sutskever I, Hinton GE: Imagenet classification with deep convolutional neural networks. <i>NIPS.</i> 2012. <a target=xrefwindow id=d23660e1615 href="https://doi.org/10.1145/3065386">Publisher Full Text </a></span></li><li><a name=ref-29 class=n-a></a><span class=label>29. </span>&nbsp;<span class=citation><a name=d23660e1624 class=n-a></a>He K, Zhang X, Ren S, <i> et al.</i>: Deep residual learning for image recognition. in 2016-December, IEEE, 2016; 770–778. <a target=xrefwindow id=d23660e1629 href="https://doi.org/10.1109/CVPR.2016.90">Publisher Full Text </a></span></li><li><a name=ref-30 class=n-a></a><span class=label>30. </span>&nbsp;<span class=citation><a name=d23660e1639 class=n-a></a>Schrimpf M, Kubilius J, Hong H, <i> et al.</i>: Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? <i>bioRxiv.</i> 2018; 407007. <a target=xrefwindow id=d23660e1647 href="https://doi.org/10.1101/407007">Publisher Full Text </a></span></li><li><a name=ref-31 class=n-a></a><span class=label>31. </span>&nbsp;<span class=citation><a name=d23660e1656 class=n-a></a>Kubilius J, Bracci S, Op de Beeck HP: Deep Neural Networks as a Computational Model for Human Shape Sensitivity. <i>PLoS Comput Biol.</i> 2016; <b>12</b>(4): e1004896. <a target=xrefwindow id=d23660e1664 href="http://www.ncbi.nlm.nih.gov/pubmed/27124699">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1667 href="https://doi.org/10.1371/journal.pcbi.1004896">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1670 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4849740">Free Full Text </a></span></li><li><a name=ref-32 class=n-a></a><span class=label>32. </span>&nbsp;<span class=citation><a name=d23660e1679 class=n-a></a>Jozwik KM, Kriegeskorte N, Storrs KR, <i> et al.</i>: Deep Convolutional Neural Networks Outperform Feature-Based But Not Categorical Models in Explaining Object Similarity Judgments. <i>Front Psychol.</i> 2017; <b>8</b>: 1089. <a target=xrefwindow id=d23660e1690 href="http://www.ncbi.nlm.nih.gov/pubmed/29062291">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1693 href="https://doi.org/10.3389/fpsyg.2017.01726">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1697 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5640771">Free Full Text </a></span></li><li><a name=ref-33 class=n-a></a><span class=label>33. </span>&nbsp;<span class=citation><a name=d23660e1706 class=n-a></a>Pramod RT, Arun SP: Do Computational Models Differ Systematically From Human Object Perception? 2016; 1601–1609. <a target=xrefwindow id=d23660e1708 href="https://doi.org/10.1109/CVPR.2016.177">Publisher Full Text </a></span></li><li><a name=ref-34 class=n-a></a><span class=label>34. </span>&nbsp;<span class=citation><a name=d23660e1717 class=n-a></a>Rajalingham R, Issa EB, Bashivan P, <i> et al.</i>: Large-Scale, High-Resolution Comparison of the Core Visual Object Recognition Behavior of Humans, Monkeys, and State-of-the-Art Deep Artificial Neural Networks. <i>J Neurosci.</i> 2018; <b>38</b>(33): 7255–69. <a target=xrefwindow id=d23660e1728 href="http://www.ncbi.nlm.nih.gov/pubmed/30006365">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1731 href="https://doi.org/10.1523/JNEUROSCI.0388-18.2018">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1735 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6096043">Free Full Text </a></span><div class=note><a name=d23660e1737 class=n-a></a><p class=first id=d23660e1738><a target=xrefwindow href="https://facultyopinions.com/prime/733637257" id=d23660e1739>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-35 class=n-a></a><span class=label>35. </span>&nbsp;<span class=citation><a name=d23660e1748 class=n-a></a>Geirhos R, Rubisch P, Michaelis C, <i> et al.</i>: Imagenet-Trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy And Robustness. [online]. 2019. <a target=xrefwindow id=d23660e1753 href="https://arxiv.org/abs/1811.12231">Reference Source</a></span></li><li><a name=ref-36 class=n-a></a><span class=label>36. </span>&nbsp;<span class=citation><a name=d23660e1763 class=n-a></a>Geirhos R, <i> et al.</i>: Generalisation in humans and deep neural networks. <i>Advances in Neural Information Processing Systems.</i> 2018-December, 2018; 7538–7550. <a target=xrefwindow id=d23660e1771 href="https://arxiv.org/abs/1808.08750">Reference Source</a></span></li><li><a name=ref-37 class=n-a></a><span class=label>37. </span>&nbsp;<span class=citation><a name=d23660e1780 class=n-a></a>Bankson BB, Hebart MN, Groen IIA, <i> et al.</i>: The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks. <i>Neuroimage.</i> 2018; <b>178</b>: 172–82. <a target=xrefwindow id=d23660e1791 href="http://www.ncbi.nlm.nih.gov/pubmed/29777825">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1794 href="https://doi.org/10.1016/j.neuroimage.2018.05.037">Publisher Full Text </a></span></li><li><a name=ref-38 class=n-a></a><span class=label>38. </span>&nbsp;<span class=citation><a name=d23660e1803 class=n-a></a>Bracci S, Ritchie JB, Kalfas I, <i> et al.</i>: The Ventral Visual Pathway Represents Animal Appearance over Animacy, Unlike Human Behavior and Deep Neural Networks. <i>J Neurosci.</i> 2019; <b>39</b>(33): 6513–25. <a target=xrefwindow id=d23660e1814 href="http://www.ncbi.nlm.nih.gov/pubmed/31196934">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1817 href="https://doi.org/10.1523/JNEUROSCI.1714-18.2019">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1821 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6697402">Free Full Text </a></span><div class=note><a name=d23660e1823 class=n-a></a><p class=first id=d23660e1824><a target=xrefwindow href="https://facultyopinions.com/prime/735988247" id=d23660e1825>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-39 class=n-a></a><span class=label>39. </span>&nbsp;<span class=citation><a name=d23660e1834 class=n-a></a>King ML, Groen IIA, Steel A, <i> et al.</i>: Similarity judgments and cortical visual responses reflect different properties of object and scene categories in naturalistic images. <i>Neuroimage.</i> 2019; <b>197</b>: 368–82. <a target=xrefwindow id=d23660e1845 href="http://www.ncbi.nlm.nih.gov/pubmed/31054350">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1848 href="https://doi.org/ 10.1016/j.neuroimage.2019.04.079">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1852 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6591094">Free Full Text </a></span></li><li><a name=ref-40 class=n-a></a><span class=label>40. </span>&nbsp;<span class=citation><a name=d23660e1861 class=n-a></a>Seeliger K, Fritsche M, Güçlü U, <i> et al.</i>: Convolutional neural network-based encoding and decoding of visual object recognition in space and time. <i>Neuroimage.</i> 2018; <b>180</b>(Pt A): 253–66. <a target=xrefwindow id=d23660e1872 href="http://www.ncbi.nlm.nih.gov/pubmed/28723578">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1875 href="https://doi.org/10.1016/j.neuroimage.2017.07.018">Publisher Full Text </a></span><div class=note><a name=d23660e1877 class=n-a></a><p class=first id=d23660e1878><a target=xrefwindow href="https://facultyopinions.com/prime/727822953" id=d23660e1879>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-41 class=n-a></a><span class=label>41. </span>&nbsp;<span class=citation><a name=d23660e1888 class=n-a></a>Cichy RM, Kriegeskorte N, Jozwik KM, <i> et al.</i>: The spatiotemporal neural dynamics underlying perceived similarity for real-world objects. <i>Neuroimage.</i> 2019; <b>194</b>: 12–24. <a target=xrefwindow id=d23660e1899 href="http://www.ncbi.nlm.nih.gov/pubmed/30894333">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1902 href="https://doi.org/10.1016/j.neuroimage.2019.03.031">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1906 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6547050">Free Full Text </a></span><div class=note><a name=d23660e1908 class=n-a></a><p class=first id=d23660e1909><a target=xrefwindow href="https://facultyopinions.com/prime/735340835" id=d23660e1910>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-42 class=n-a></a><span class=label>42. </span>&nbsp;<span class=citation><a name=d23660e1920 class=n-a></a>Horikawa T, Kamitani Y: Generic decoding of seen and imagined objects using hierarchical visual features. <i>Nat Commun.</i> 2017; <b>8</b>: 15037.<a target=xrefwindow id=d23660e1928 href="http://www.ncbi.nlm.nih.gov/pubmed/28530228">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1931 href="https://doi.org/10.1038/ncomms15037">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1934 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5458127">Free Full Text </a></span></li><li><a name=ref-43 class=n-a></a><span class=label>43. </span>&nbsp;<span class=citation><a name=d23660e1943 class=n-a></a>Rajaei K, Mohsenzadeh Y, Ebrahimpour R, <i> et al.</i>: Beyond core object recognition: Recurrent processes account for object recognition under occlusion. <i>PLoS Comput Biol.</i> 2019; <b>15</b>(5): e1007001. <a target=xrefwindow id=d23660e1954 href="http://www.ncbi.nlm.nih.gov/pubmed/29513219">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1957 href="https://doi.org/10.7554/eLife.32962">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1961 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5860866">Free Full Text </a></span></li><li><a name=ref-44 class=n-a></a><span class=label>44. </span>&nbsp;<span class=citation><a name=d23660e1970 class=n-a></a>Groen II, Greene MR, Baldassano C, <i> et al.</i>: Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior. <i>eLife.</i> 2018; <b>7</b>: e32962. <a target=xrefwindow id=d23660e1981 href="http://www.ncbi.nlm.nih.gov/pubmed/29513219">PubMed Abstract </a> | <a target=xrefwindow id=d23660e1984 href="https://doi.org/10.7554/eLife.32962">Publisher Full Text </a> | <a target=xrefwindow id=d23660e1988 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5860866">Free Full Text </a></span></li><li><a name=ref-45 class=n-a></a><span class=label>45. </span>&nbsp;<span class=citation><a name=d23660e1997 class=n-a></a>Zeman AA, Ritchie JB, Bracci S, <i> et al.</i>: Orthogonal Representations of Object Shape and Category in Deep Convolutional Neural Networks and Human Visual Cortex. <i>Sci Rep.</i> 2020; <b>10</b>(1): 2453. <a target=xrefwindow id=d23660e2008 href="http://www.ncbi.nlm.nih.gov/pubmed/32051467">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2011 href="https://doi.org/10.1038/s41598-020-59175-0">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2015 href="http://www.ncbi.nlm.nih.gov/pmc/articles/7016009">Free Full Text </a></span></li><li><a name=ref-46 class=n-a></a><span class=label>46. </span>&nbsp;<span class=citation><a name=d23660e2024 class=n-a></a>Khaligh-Razavi SM, Kriegeskorte N: Deep supervised, but not unsupervised, models may explain IT cortical representation. <i>PLoS Comput Biol.</i> 2014; <b>10</b>(11): e1003915. <a target=xrefwindow id=d23660e2032 href="http://www.ncbi.nlm.nih.gov/pubmed/25375136">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2035 href="https://doi.org/10.1371/journal.pcbi.1003915">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2038 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4222664">Free Full Text </a></span></li><li><a name=ref-47 class=n-a></a><span class=label>47. </span>&nbsp;<span class=citation><a name=d23660e2047 class=n-a></a>Cichy RM, Khosla A, Pantazis D, <i> et al.</i>: Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence. <i>Sci Rep.</i> 2016; <b>6</b>: 27755. <a target=xrefwindow id=d23660e2058 href="http://www.ncbi.nlm.nih.gov/pubmed/27282108">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2061 href="https://doi.org/10.1038/srep27755">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2065 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4901271">Free Full Text </a></span><div class=note><a name=d23660e2067 class=n-a></a><p class=first id=d23660e2068><a target=xrefwindow href="https://facultyopinions.com/prime/726413891" id=d23660e2069>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-48 class=n-a></a><span class=label>48. </span>&nbsp;<span class=citation><a name=d23660e2079 class=n-a></a>Güçlü U, van Gerven MAJ: Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream. <i>J Neurosci.</i> 2015; <b>35</b>(27): 10005–14. <a target=xrefwindow id=d23660e2087 href="http://www.ncbi.nlm.nih.gov/pubmed/26157000">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2090 href="https://doi.org/10.1523/JNEUROSCI.5023-14.2015">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2093 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6605414">Free Full Text </a></span></li><li><a name=ref-49 class=n-a></a><span class=label>49. </span>&nbsp;<span class=citation><a name=d23660e2102 class=n-a></a>Mozafari M, Ganjtabesh M, Nowzari-Dalini A, <i> et al.</i>: Bio-inspired digit recognition using reward-modulated spike-timing-dependent plasticity in deep convolutional networks. <i>Pattern Recognit.</i> 2019; <b>94</b>: 87–95. <a target=xrefwindow id=d23660e2113 href="https://doi.org/10.1016/j.patcog.2019.05.015">Publisher Full Text </a></span></li><li><a name=ref-50 class=n-a></a><span class=label>50. </span>&nbsp;<span class=citation><a name=d23660e2122 class=n-a></a>Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, <i> et al.</i>: STDP-based spiking deep convolutional neural networks for object recognition. <i>Neural Netw.</i> 2018; <b>99</b>: 56–67. <a target=xrefwindow id=d23660e2133 href="http://www.ncbi.nlm.nih.gov/pubmed/29328958">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2136 href="https://doi.org/10.1016/j.neunet.2017.12.005">Publisher Full Text </a></span><div class=note><a name=d23660e2138 class=n-a></a><p class=first id=d23660e2139><a target=xrefwindow href="https://facultyopinions.com/prime/732453284" id=d23660e2140>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-51 class=n-a></a><span class=label>51. </span>&nbsp;<span class=citation><a name=d23660e2149 class=n-a></a>Kar K, Kubilius J, Schmidt K, <i> et al.</i>: Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior. <i>Nat Neurosci.</i> 2019; <b>22</b>(6): 974–83. <a target=xrefwindow id=d23660e2160 href="http://www.ncbi.nlm.nih.gov/pubmed/31036945">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2163 href="https://doi.org/10.1038/s41593-019-0392-5">Publisher Full Text </a></span><div class=note><a name=d23660e2165 class=n-a></a><p class=first id=d23660e2166><a target=xrefwindow href="https://facultyopinions.com/prime/735630495" id=d23660e2167>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-52 class=n-a></a><span class=label>52. </span>&nbsp;<span class=citation><a name=d23660e2176 class=n-a></a>Kietzmann TC, Spoerer CJ, Sörensen LKA, <i> et al.</i>: Recurrence is required to capture the representational dynamics of the human visual system. <i>Proc Natl Acad Sci U S A.</i> 2019; <b>116</b>(43): 21854–63. <a target=xrefwindow id=d23660e2187 href="http://www.ncbi.nlm.nih.gov/pubmed/31591217">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2190 href="https://doi.org/10.1073/pnas.1905544116">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2194 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6815174">Free Full Text </a></span></li><li><a name=ref-53 class=n-a></a><span class=label>53. </span>&nbsp;<span class=citation><a name=d23660e2203 class=n-a></a>Contini EW, Wardle SG, Carlson TA: Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions. <i>Neuropsychologia.</i> 2017; <b>105</b>: 165–76. <a target=xrefwindow id=d23660e2211 href="http://www.ncbi.nlm.nih.gov/pubmed/28215698">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2214 href="https://doi.org/10.1016/j.neuropsychologia.2017.02.013">Publisher Full Text </a></span></li><li><a name=ref-54 class=n-a></a><span class=label>54. </span>&nbsp;<span class=citation><a name=d23660e2224 class=n-a></a>Martin A: GRAPES—Grounding representations in action, perception, and emotion systems: How object properties and categories are represented in the human brain. <i>Psychon Bull Rev.</i> 2016; <b>23</b>(4): 979–90. <a target=xrefwindow id=d23660e2232 href="http://www.ncbi.nlm.nih.gov/pubmed/25968087">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2235 href="https://doi.org/10.3758/s13423-015-0842-3">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2238 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5111803">Free Full Text </a></span></li><li><a name=ref-55 class=n-a></a><span class=label>55. </span>&nbsp;<span class=citation><a name=d23660e2247 class=n-a></a>Kaiser D, Moeskops MM, Cichy RM: Typical retinotopic locations impact the time course of object coding. <i>Neuroimage.</i> 2018; <b>176</b>: 372–9. <a target=xrefwindow id=d23660e2255 href="http://www.ncbi.nlm.nih.gov/pubmed/29733954">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2258 href="https://doi.org/10.1016/j.neuroimage.2018.05.006">Publisher Full Text </a></span></li><li><a name=ref-56 class=n-a></a><span class=label>56. </span>&nbsp;<span class=citation><a name=d23660e2267 class=n-a></a>Brandman T, Peelen MV: Interaction between Scene and Object Processing Revealed by Human fMRI and MEG Decoding. <i>J Neurosci.</i> 2017; <b>37</b>(32): 7700–10. <a target=xrefwindow id=d23660e2275 href="http://www.ncbi.nlm.nih.gov/pubmed/28687603">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2278 href="https://doi.org/10.1523/JNEUROSCI.0582-17.2017">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2281 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6596648">Free Full Text </a></span></li><li><a name=ref-57 class=n-a></a><span class=label>57. </span>&nbsp;<span class=citation><a name=d23660e2290 class=n-a></a>Khaligh-Razavi SM, Cichy RM, Pantazis D, <i> et al.</i>: Tracking the Spatiotemporal Neural Dynamics of Real-world Object Size and Animacy in the Human Brain. <i>J Cogn Neurosci.</i> 2018; <b>30</b>(11): 1559–76. <a target=xrefwindow id=d23660e2301 href="http://www.ncbi.nlm.nih.gov/pubmed/29877767">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2304 href="https://doi.org/10.1162/jocn_a_01290">Publisher Full Text </a></span></li><li><a name=ref-58 class=n-a></a><span class=label>58. </span>&nbsp;<span class=citation><a name=d23660e2313 class=n-a></a>Pennington J, Socher R, Manning CD: GloVe: Global vectors for word representation. in 2014; 1532–1543. <a target=xrefwindow id=d23660e2315 href="https://doi.org/10.3115/v1/D14-1162">Publisher Full Text </a></span></li><li><a name=ref-59 class=n-a></a><span class=label>59. </span>&nbsp;<span class=citation><a name=d23660e2324 class=n-a></a>Riesenhuber M, Poggio T: Hierarchical models of object recognition in cortex. <i>Nat Neurosci.</i> 1999; <b>2</b>(11): 1019–25. <a target=xrefwindow id=d23660e2332 href="http://www.ncbi.nlm.nih.gov/pubmed/10526343">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2335 href="https://doi.org/10.1038/14819">Publisher Full Text </a></span></li><li><a name=ref-60 class=n-a></a><span class=label>60. </span>&nbsp;<span class=citation><a name=d23660e2345 class=n-a></a>Clarke A, Devereux BJ, Randall B, <i> et al.</i>: Predicting the Time Course of Individual Objects with MEG. <i>Cereb Cortex.</i> 2015; <b>25</b>(10): 3602–12. <a target=xrefwindow id=d23660e2356 href="http://www.ncbi.nlm.nih.gov/pubmed/25209607">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2359 href="https://doi.org/10.1093/cercor/bhu203">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2363 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4269546">Free Full Text </a></span></li><li><a name=ref-61 class=n-a></a><span class=label>61. </span>&nbsp;<span class=citation><a name=d23660e2372 class=n-a></a>Bruffaerts R, Tyler LK, Shafto M, <i> et al.</i>: Perceptual and conceptual processing of visual objects across the adult lifespan. <i>Sci Rep.</i> 2019; <b>9</b>(1): 13771. <a target=xrefwindow id=d23660e2383 href="http://www.ncbi.nlm.nih.gov/pubmed/31551468">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2386 href="https://doi.org/10.1038/s41598-019-50254-5">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2390 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6760174">Free Full Text </a></span></li><li><a name=ref-62 class=n-a></a><span class=label>62. </span>&nbsp;<span class=citation><a name=d23660e2399 class=n-a></a>Devereux BJ, Clarke A, Tyler LK: Integrated deep visual and semantic attractor neural networks predict fMRI pattern-information along the ventral object processing pathway. <i>Sci Rep.</i> 2018; <b>8</b>(1): 10636. <a target=xrefwindow id=d23660e2407 href="http://www.ncbi.nlm.nih.gov/pubmed/30006530">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2410 href="https://doi.org/10.1038/s41598-018-28865-1">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2413 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6045572">Free Full Text </a></span><div class=note><a name=d23660e2415 class=n-a></a><p class=first id=d23660e2416><a target=xrefwindow href="https://facultyopinions.com/prime/733635946" id=d23660e2417>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-63 class=n-a></a><span class=label>63. </span>&nbsp;<span class=citation><a name=d23660e2426 class=n-a></a>Chiou R, Lambon Ralph MA: The anterior temporal cortex is a primary semantic source of top-down influences on object recognition. <i>Cortex.</i> 2016; <b>79</b>: 75–86. <a target=xrefwindow id=d23660e2434 href="http://www.ncbi.nlm.nih.gov/pubmed/27088615">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2437 href="https://doi.org/10.1016/j.cortex.2016.03.007">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2440 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4884670">Free Full Text </a></span></li><li><a name=ref-64 class=n-a></a><span class=label>64. </span>&nbsp;<span class=citation><a name=d23660e2449 class=n-a></a>Mohsenzadeh Y, Qin S, Cichy RM, <i> et al.</i>: Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway. <i>Elife.</i> 2018; <b>7</b>: e36329. <a target=xrefwindow id=d23660e2460 href="http://www.ncbi.nlm.nih.gov/pubmed/29927384">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2463 href="https://doi.org/10.7554/eLife.36329">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2467 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6029845">Free Full Text </a></span></li><li><a name=ref-65 class=n-a></a><span class=label>65. </span>&nbsp;<span class=citation><a name=d23660e2476 class=n-a></a>Grootswagers T, Robinson AK, Carlson TA: The representational dynamics of visual objects in rapid serial visual processing streams. <i>Neuroimage.</i> 2019; <b>188</b>: 668–79. <a target=xrefwindow id=d23660e2484 href="http://www.ncbi.nlm.nih.gov/pubmed/30593903">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2487 href="https://doi.org/10.1016/j.neuroimage.2018.12.046">Publisher Full Text </a></span></li><li><a name=ref-66 class=n-a></a><span class=label>66. </span>&nbsp;<span class=citation><a name=d23660e2497 class=n-a></a>Mohsenzadeh Y, Mullin C, Lahner B, <i> et al.</i>: Reliability and Generalizability of Similarity-Based Fusion of MEG and fMRI Data in Human Ventral and Dorsal Visual Streams. <i>Vision.</i> 2019; <b>3</b>(1): 8. <a target=xrefwindow id=d23660e2508 href="http://www.ncbi.nlm.nih.gov/pubmed/31735809">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2511 href="https://doi.org/ 10.3390/vision3010008">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2515 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6802768">Free Full Text </a></span></li><li><a name=ref-67 class=n-a></a><span class=label>67. </span>&nbsp;<span class=citation><a name=d23660e2524 class=n-a></a>Hebart MN, Bankson BB, Harel A, <i> et al.</i>: The representational dynamics of task and object processing in humans. <i>eLife.</i> 2018; <b>7</b>: e32816. <a target=xrefwindow id=d23660e2535 href="http://www.ncbi.nlm.nih.gov/pubmed/29384473">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2538 href="https://doi.org/10.7554/eLife.32816">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2542 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5811210">Free Full Text </a></span></li><li><a name=ref-68 class=n-a></a><span class=label>68. </span>&nbsp;<span class=citation><a name=d23660e2551 class=n-a></a>Proklova D, Kaiser D, Peelen MV: Disentangling Representations of Object Shape and Object Category in Human Visual Cortex: The Animate-Inanimate Distinction. <i>J Cogn Neurosci.</i> 2016; <b>28</b>(5): 680–92. <a target=xrefwindow id=d23660e2559 href="http://www.ncbi.nlm.nih.gov/pubmed/26765944">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2562 href="https://doi.org/10.1162/jocn_a_00924">Publisher Full Text </a></span></li><li><a name=ref-69 class=n-a></a><span class=label>69. </span>&nbsp;<span class=citation><a name=d23660e2571 class=n-a></a>Proklova D, Kaiser D, Peelen MV: MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects. <i>Neuroimage.</i> 2019; <b>193</b>: 167–77. <a target=xrefwindow id=d23660e2579 href="http://www.ncbi.nlm.nih.gov/pubmed/30885785">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2582 href="https://doi.org/10.1016/j.neuroimage.2019.03.028">Publisher Full Text </a></span></li><li><a name=ref-70 class=n-a></a><span class=label>70. </span>&nbsp;<span class=citation><a name=d23660e2591 class=n-a></a>Iivanainen J, Stenroos M, Parkkonen L: Measuring MEG closer to the brain: Performance of on-scalp sensor arrays. <i>Neuroimage.</i> 2017; <b>147</b>: 542–53. <a target=xrefwindow id=d23660e2599 href="http://www.ncbi.nlm.nih.gov/pubmed/28007515">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2602 href="https://doi.org/10.1016/j.neuroimage.2016.12.048">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2605 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5432137">Free Full Text </a></span></li><li><a name=ref-71 class=n-a></a><span class=label>71. </span>&nbsp;<span class=citation><a name=d23660e2614 class=n-a></a>Boto E, Holmes N, Leggett J, <i> et al.</i>: Moving magnetoencephalography towards real-world applications with a wearable system. <i>Nature.</i> 2018; <b>555</b>(7698): 657–61. <a target=xrefwindow id=d23660e2625 href="http://www.ncbi.nlm.nih.gov/pubmed/29562238">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2628 href="https://doi.org/10.1038/nature26147">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2632 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6063354">Free Full Text </a></span></li><li><a name=ref-72 class=n-a></a><span class=label>72. </span>&nbsp;<span class=citation><a name=d23660e2642 class=n-a></a>Logothetis NK, Pauls J, Bülthoff HH, <i> et al.</i>: View-dependent object recognition by monkeys. <i>Curr Biol.</i> 1994; <b>4</b>(5): 401–14. <a target=xrefwindow id=d23660e2653 href="http://www.ncbi.nlm.nih.gov/pubmed/7922354">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2656 href="https://doi.org/10.1016/s0960-9822(00)00089-0">Publisher Full Text </a></span></li><li><a name=ref-73 class=n-a></a><span class=label>73. </span>&nbsp;<span class=citation><a name=d23660e2665 class=n-a></a>Kourtzi Z, Kanwisher N: Cortical Regions Involved in Perceiving Object Shape. <i>J Neurosci.</i> 2000; <b>20</b>(9): 3310–8. <a target=xrefwindow id=d23660e2673 href="http://www.ncbi.nlm.nih.gov/pubmed/10777794">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2676 href="https://doi.org/10.1523/JNEUROSCI.20-09-03310.2000">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2679 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6773111">Free Full Text </a></span></li><li><a name=ref-74 class=n-a></a><span class=label>74. </span>&nbsp;<span class=citation><a name=d23660e2688 class=n-a></a>Baldassano C, Beck DM, Fei-Fei L: Human-Object Interactions Are More than the Sum of Their Parts. <i>Cereb Cortex.</i> 2017; <b>27</b>(3): 2276–88. <a target=xrefwindow id=d23660e2696 href="http://www.ncbi.nlm.nih.gov/pubmed/27073216">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2699 href="https://doi.org/10.1093/cercor/bhw077">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2702 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5963823">Free Full Text </a></span></li><li><a name=ref-75 class=n-a></a><span class=label>75. </span>&nbsp;<span class=citation><a name=d23660e2711 class=n-a></a>Freud E, Macdonald SN, Chen J, <i> et al.</i>: Getting a grip on reality: Grasping movements directed to real objects and images rely on dissociable neural representations. <i>Cortex.</i> 2018; <b>98</b>: 34–48. <a target=xrefwindow id=d23660e2722 href="http://www.ncbi.nlm.nih.gov/pubmed/28431740">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2725 href="https://doi.org/10.1016/j.cortex.2017.02.020">Publisher Full Text </a></span><div class=note><a name=d23660e2727 class=n-a></a><p class=first id=d23660e2728><a target=xrefwindow href="https://facultyopinions.com/prime/730217988" id=d23660e2729>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-76 class=n-a></a><span class=label>76. </span>&nbsp;<span class=citation><a name=d23660e2738 class=n-a></a>Snow JC, Pettypiece CE, McAdam TD, <i> et al.</i>: Bringing the real world into the fMRI scanner: Repetition effects for pictures versus real objects. <i>Sci Rep.</i> 2011; <b>1</b>: 130. <a target=xrefwindow id=d23660e2749 href="http://www.ncbi.nlm.nih.gov/pubmed/22355647">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2752 href="https://doi.org/10.1038/srep00130">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2756 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3216611">Free Full Text </a></span></li><li><a name=ref-77 class=n-a></a><span class=label>77. </span>&nbsp;<span class=citation><a name=d23660e2765 class=n-a></a>Harel A, Kravitz DJ, Baker CI: Task context impacts visual object processing differentially across the cortex. <i>Proc Natl Acad Sci U S A.</i> 2014; <b>111</b>(10): E962–E971. <a target=xrefwindow id=d23660e2773 href="http://www.ncbi.nlm.nih.gov/pubmed/24567402">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2776 href="https://doi.org/10.1073/pnas.1312567111">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2779 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3956196">Free Full Text </a></span></li><li><a name=ref-78 class=n-a></a><span class=label>78. </span>&nbsp;<span class=citation><a name=d23660e2789 class=n-a></a>MacEvoy SP, Epstein RA: Decoding the representation of multiple simultaneous objects in human occipitotemporal cortex. <i>Curr Biol.</i> 2009; <b>19</b>(11): 943–7. <a target=xrefwindow id=d23660e2797 href="http://www.ncbi.nlm.nih.gov/pubmed/19446454">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2800 href="https://doi.org/10.1016/j.cub.2009.04.020">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2803 href="http://www.ncbi.nlm.nih.gov/pmc/articles/2875119">Free Full Text </a></span></li><li><a name=ref-79 class=n-a></a><span class=label>79. </span>&nbsp;<span class=citation><a name=d23660e2812 class=n-a></a>Zoccolan D, Cox DD, DiCarlo JJ: Multiple Object Response Normalization in Monkey Inferotemporal Cortex. <i>J Neurosci.</i> 2005; <b>25</b>(36): 8150–64. <a target=xrefwindow id=d23660e2820 href="http://www.ncbi.nlm.nih.gov/pubmed/16148223">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2823 href="https://doi.org/10.1523/JNEUROSCI.2058-05.2005">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2826 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6725538">Free Full Text </a></span></li><li><a name=ref-80 class=n-a></a><span class=label>80. </span>&nbsp;<span class=citation><a name=d23660e2835 class=n-a></a>Kaiser D, Peelen MV: Transformation from independent to integrative coding of multi-object arrangements in human visual cortex. <i>Neuroimage.</i> 2018; <b>169</b>: 334–41. <a target=xrefwindow id=d23660e2843 href="http://www.ncbi.nlm.nih.gov/pubmed/29277645">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2846 href="https://doi.org/10.1016/j.neuroimage.2017.12.065">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2849 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5857358">Free Full Text </a></span></li><li><a name=ref-81 class=n-a></a><span class=label>81. </span>&nbsp;<span class=citation><a name=d23660e2858 class=n-a></a>Kaiser D, Quek GL, Cichy RM, <i> et al.</i>: Object Vision in a Structured World. <i>Trends Cogn Sci.</i> 2019; <b>23</b>(8): 672–85. <a target=xrefwindow id=d23660e2869 href="http://www.ncbi.nlm.nih.gov/pubmed/31147151">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2872 href="https://doi.org/10.1016/j.tics.2019.04.013 ">Publisher Full Text </a></span></li><li><a name=ref-82 class=n-a></a><span class=label>82. </span>&nbsp;<span class=citation><a name=d23660e2881 class=n-a></a>Kaiser D, Cichy RM: Typical visual-field locations enhance processing in object-selective channels of human occipital cortex. <i>J Neurophysiol.</i> 2018; <b>120</b>(2): 848–53. <a target=xrefwindow id=d23660e2889 href="http://www.ncbi.nlm.nih.gov/pubmed/29766762">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2892 href="https://doi.org/10.1152/jn.00229.2018">Publisher Full Text </a></span></li><li><a name=ref-83 class=n-a></a><span class=label>83. </span>&nbsp;<span class=citation><a name=d23660e2901 class=n-a></a>Simoncelli EP: Vision and the statistics of the visual environment. <i>Curr Opin Neurobiol.</i> 2003; <b>13</b>(2): 144–9. <a target=xrefwindow id=d23660e2909 href="http://www.ncbi.nlm.nih.gov/pubmed/12744966">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2912 href="https://doi.org/10.1016/S0959-4388(03)00047-3">Publisher Full Text </a></span></li><li><a name=ref-84 class=n-a></a><span class=label>84. </span>&nbsp;<span class=citation><a name=d23660e2922 class=n-a></a>Zopf R, Butko M, Woolgar A, <i> et al.</i>: Representing the location of manipulable objects in shape-selective occipitotemporal cortex: Beyond retinotopic reference frames? <i>Cortex.</i> 2018; <b>106</b>: 132–50. <a target=xrefwindow id=d23660e2933 href="http://www.ncbi.nlm.nih.gov/pubmed/29940399">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2936 href="https://doi.org/10.1016/j.cortex.2018.05.009">Publisher Full Text </a></span></li><li><a name=ref-85 class=n-a></a><span class=label>85. </span>&nbsp;<span class=citation><a name=d23660e2945 class=n-a></a>Bainbridge WA, Oliva A: Interaction envelope: Local spatial representations of objects at all scales in scene-selective regions. <i>Neuroimage.</i> 2015; <b>122</b>: 408–16. <a target=xrefwindow id=d23660e2953 href="http://www.ncbi.nlm.nih.gov/pubmed/26236029">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2956 href="https://doi.org/10.1016/j.neuroimage.2015.07.066">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2959 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4655824">Free Full Text </a></span></li><li><a name=ref-86 class=n-a></a><span class=label>86. </span>&nbsp;<span class=citation><a name=d23660e2968 class=n-a></a>Bracci S, Daniels N, Op de Beeck H: Task Context Overrules Object- and Category-Related Representational Content in the Human Parietal Cortex. <i>Cerebral Cortex.</i> 2017; <b>27</b>(1): 310–321. <a target=xrefwindow id=d23660e2976 href="http://www.ncbi.nlm.nih.gov/pubmed/28108492">PubMed Abstract </a> | <a target=xrefwindow id=d23660e2979 href="https://doi.org/10.1093/cercor/bhw419">Publisher Full Text </a> | <a target=xrefwindow id=d23660e2982 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5939221">Free Full Text </a></span><div class=note><a name=d23660e2984 class=n-a></a><p class=first id=d23660e2985><a target=xrefwindow href="https://facultyopinions.com/prime/727227112" id=d23660e2986>Faculty Opinions Recommendation</a></p></div></li><li><a name=ref-87 class=n-a></a><span class=label>87. </span>&nbsp;<span class=citation><a name=d23660e2995 class=n-a></a>Vaziri-Pashkam M, Xu Y: Goal-Directed Visual Processing Differentially Impacts Human Ventral and Dorsal Visual Representations. <i>J Neurosci.</i> 2017; <b>37</b>(36): 8767–82. <a target=xrefwindow id=d23660e3003 href="http://www.ncbi.nlm.nih.gov/pubmed/28821655">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3006 href="https://doi.org/10.1523/JNEUROSCI.3392-16.2017">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3009 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5588467">Free Full Text </a></span></li><li><a name=ref-88 class=n-a></a><span class=label>88. </span>&nbsp;<span class=citation><a name=d23660e3018 class=n-a></a>Xu Y, Vaziri-Pashkam M: Task modulation of the 2-pathway characterization of occipitotemporal and posterior parietal visual object representations. <i>Neuropsychologia.</i> 2019; <b>132</b>: 107140. <a target=xrefwindow id=d23660e3026 href="http://www.ncbi.nlm.nih.gov/pubmed/31301350">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3029 href="https://doi.org/10.1016/j.neuropsychologia.2019.107140">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3032 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6857731">Free Full Text </a></span></li><li><a name=ref-89 class=n-a></a><span class=label>89. </span>&nbsp;<span class=citation><a name=d23660e3041 class=n-a></a>Bugatus L, Weiner KS, Grill-Spector K: Task alters category representations in prefrontal but not high-level visual cortex. <i>Neuroimage.</i> 2017; <b>155</b>: 437–49. <a target=xrefwindow id=d23660e3049 href="http://www.ncbi.nlm.nih.gov/pubmed/28389381">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3052 href="https://doi.org/10.1016/j.neuroimage.2017.03.062">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3055 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5518738">Free Full Text </a></span></li><li><a name=ref-90 class=n-a></a><span class=label>90. </span>&nbsp;<span class=citation><a name=d23660e3065 class=n-a></a>Erez Y, Duncan J: Discrimination of Visual Categories Based on Behavioral Relevance in Widespread Regions of Frontoparietal Cortex. <i>J Neurosci.</i> 2015; <b>35</b>(36): 12383–93. <a target=xrefwindow id=d23660e3073 href="http://www.ncbi.nlm.nih.gov/pubmed/26354907">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3076 href="https://doi.org/10.1523/JNEUROSCI.1134-15.2015">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3079 href="http://www.ncbi.nlm.nih.gov/pmc/articles/4563032">Free Full Text </a></span></li><li><a name=ref-91 class=n-a></a><span class=label>91. </span>&nbsp;<span class=citation><a name=d23660e3088 class=n-a></a>Vaziri-Pashkam M, Xu Y: An Information-Driven 2-Pathway Characterization of Occipitotemporal and Posterior Parietal Visual Object Representations. <i>Cereb Cortex.</i> 2019; <b>29</b>(5): 2034–50. <a target=xrefwindow id=d23660e3096 href="http://www.ncbi.nlm.nih.gov/pubmed/29659730">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3099 href="https://doi.org/10.1093/cercor/bhy080">Publisher Full Text </a></span></li><li><a name=ref-92 class=n-a></a><span class=label>92. </span>&nbsp;<span class=citation><a name=d23660e3108 class=n-a></a>Peelen MV, Downing PE: Category selectivity in human visual cortex: Beyond visual object recognition. <i>Neuropsychologia.</i> 2017; <b>105</b>: 177–83. <a target=xrefwindow id=d23660e3116 href="http://www.ncbi.nlm.nih.gov/pubmed/28377161">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3119 href="https://doi.org/10.1016/j.neuropsychologia.2017.03.033">Publisher Full Text </a></span></li><li><a name=ref-93 class=n-a></a><span class=label>93. </span>&nbsp;<span class=citation><a name=d23660e3128 class=n-a></a>Rosenthal I, Ratnasingam S, Haile T, <i> et al.</i>: Color statistics of objects, and color tuning of object cortex in macaque monkey. <i>J Vis.</i> 2018; <b>18</b>(11): 1. <a target=xrefwindow id=d23660e3139 href="http://www.ncbi.nlm.nih.gov/pubmed/30285103">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3142 href="https://doi.org/10.1167/18.11.1">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3146 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6168048">Free Full Text </a></span></li><li><a name=ref-94 class=n-a></a><span class=label>94. </span>&nbsp;<span class=citation><a name=d23660e3155 class=n-a></a>Teichmann L, Grootswagers T, Carlson TA, <i> et al.</i>: Seeing versus knowing: The temporal dynamics of real and implied colour processing in the human brain. <i>Neuroimage.</i> 2019; <b>200</b>: 373–81. <a target=xrefwindow id=d23660e3166 href="http://www.ncbi.nlm.nih.gov/pubmed/31254648">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3169 href="https://doi.org/10.1016/j.neuroimage.2019.06.062">Publisher Full Text </a></span></li><li><a name=ref-95 class=n-a></a><span class=label>95. </span>&nbsp;<span class=citation><a name=d23660e3178 class=n-a></a>Schmid AC, Doerschner K: Representing stuff in the human brain. <i>Curr Opin Behav Sci.</i> 2019; <b>30</b>: 178–85. <a target=xrefwindow id=d23660e3186 href="https://doi.org/10.1016/j.cobeha.2019.10.007">Publisher Full Text </a></span></li><li><a name=ref-96 class=n-a></a><span class=label>96. </span>&nbsp;<span class=citation><a name=d23660e3196 class=n-a></a>Kriegeskorte N, Kievit RA: Representational geometry: Integrating cognition, computation, and the brain. <i>Trends Cogn Sci.</i> 2013; <b>17</b>(8): 401–12. <a target=xrefwindow id=d23660e3204 href="http://www.ncbi.nlm.nih.gov/pubmed/23876494">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3207 href="https://doi.org/10.1016/j.tics.2013.06.007">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3210 href="http://www.ncbi.nlm.nih.gov/pmc/articles/3730178">Free Full Text </a></span></li><li><a name=ref-97 class=n-a></a><span class=label>97. </span>&nbsp;<span class=citation><a name=d23660e3219 class=n-a></a>Chang N, Pyles JA, Marcus A, <i> et al.</i>: BOLD5000, a public fMRI dataset while viewing 5000 visual images. <i>Sci Data.</i> 2019; <b>6</b>(1): 49. <a target=xrefwindow id=d23660e3230 href="http://www.ncbi.nlm.nih.gov/pubmed/31061383">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3233 href="https://doi.org/10.1038/s41597-019-0052-3">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3237 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6502931">Free Full Text </a></span></li><li><a name=ref-98 class=n-a></a><span class=label>98. </span>&nbsp;<span class=citation><a name=d23660e3246 class=n-a></a>Hebart MN, Dickter AH, Kidder A, <i> et al.</i>: THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images. <i>PLoS One.</i> 2019; <b>14</b>(10): e0223792. <a target=xrefwindow id=d23660e3257 href="http://www.ncbi.nlm.nih.gov/pubmed/31613926">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3260 href="https://doi.org/10.1371/journal.pone.0223792">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3264 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6793944">Free Full Text </a></span></li><li><a name=ref-99 class=n-a></a><span class=label>99. </span>&nbsp;<span class=citation><a name=d23660e3273 class=n-a></a>Coggan DD, Giannakopoulou A, Ali S, <i> et al.</i>: A data-driven approach to stimulus selection reveals an image-based representation of objects in high-level visual areas. <i>Hum Brain Mapp.</i> 2019; <b>40</b>(1): 4716–31. <a target=xrefwindow id=d23660e3284 href="https://doi.org/10.1002/hbm.24732">Publisher Full Text </a></span></li><li><a name=ref-100 class=n-a></a><span class=label>100. </span>&nbsp;<span class=citation><a name=d23660e3293 class=n-a></a>Bracci S, Ritchie JB, de Beeck HO: On the partnership between neural representations of object categories and visual features in the ventral visual pathway. <i>Neuropsychologia.</i> 2017; <b>105</b>: 153–64. <a target=xrefwindow id=d23660e3301 href="http://www.ncbi.nlm.nih.gov/pubmed/28619529">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3304 href="https://doi.org/10.1016/j.neuropsychologia.2017.06.010">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3307 href="http://www.ncbi.nlm.nih.gov/pmc/articles/5680697">Free Full Text </a></span></li><li><a name=ref-101 class=n-a></a><span class=label>101. </span>&nbsp;<span class=citation><a name=d23660e3316 class=n-a></a>Bracci S, Op de Beeck H: Dissociations and Associations between Shape and Category Representations in the Two Visual Pathways. <i>J Neurosci.</i> 2016; <b>36</b>(2): 432–44. <a target=xrefwindow id=d23660e3324 href="http://www.ncbi.nlm.nih.gov/pubmed/26758835">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3327 href="https://doi.org/10.1523/JNEUROSCI.2314-15.2016">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3330 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6602035">Free Full Text </a></span></li><li><a name=ref-102 class=n-a></a><span class=label>102. </span>&nbsp;<span class=citation><a name=d23660e3340 class=n-a></a>Long B, Yu CP, Konkle T: Mid-level visual features underlie the high-level categorical organization of the ventral stream. <i>Proc Natl Acad Sci U S A.</i> 2018; <b>115</b>(38): E9015–E9024. <a target=xrefwindow id=d23660e3348 href="http://www.ncbi.nlm.nih.gov/pubmed/30171168">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3351 href="https://doi.org/10.1073/pnas.1719616115">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3354 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6156638">Free Full Text </a></span></li><li><a name=ref-103 class=n-a></a><span class=label>103. </span>&nbsp;<span class=citation><a name=d23660e3363 class=n-a></a>Wardle SG, Ritchie JB: Can object category-selectivity in the ventral visual pathway be explained by sensitivity to low-level image properties? <i>J Neurosci.</i> 2014; <b>34</b>(45): 14817–9. <a target=xrefwindow id=d23660e3371 href="http://www.ncbi.nlm.nih.gov/pubmed/25378148">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3374 href="https://doi.org/10.1523/JNEUROSCI.3566-14.2014">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3377 href="http://www.ncbi.nlm.nih.gov/pmc/articles/6608364">Free Full Text </a></span></li><li><a name=ref-104 class=n-a></a><span class=label>104. </span>&nbsp;<span class=citation><a name=d23660e3386 class=n-a></a>Brady TF, Konkle T, Alvarez GA, <i> et al.</i>: Visual long-term memory has a massive storage capacity for object details. <i>Proc Natl Acad Sci U S A.</i> 2008; <b>105</b>(38): 14325–9. <a target=xrefwindow id=d23660e3397 href="http://www.ncbi.nlm.nih.gov/pubmed/18787113">PubMed Abstract </a> | <a target=xrefwindow id=d23660e3400 href="https://doi.org/10.1073/pnas.0803390105">Publisher Full Text </a> | <a target=xrefwindow id=d23660e3404 href="http://www.ncbi.nlm.nih.gov/pmc/articles/2533687">Free Full Text </a></span></li></ul></div></div></div> </div> <div id=article-comments class="article-comments padding-bottom-20"> <div class=current-article-comment-section> <h2 class=main-title name=add-new-comment id=add-new-comment> <span class="research-layout f1r-article-mobile-inline valign-middle"> <span class="f1r-icon icon-104_comments size30"></span> </span> <span class=f1r-article-desk-inline>Comments on this article</span> <span class=f1r-article-mobile-inline>Comments (0)</span> <span class="f1r-article-mobile-inline float-right"> <span class="f1r-icon icon-14_more_small"></span> <span class="f1r-icon icon-10_less_small"></span> </span> </h2> </div> <div class="f1r-article-desk-inline referee-report-info-box referee-report-version-box"> Version 1 </div> <div class="f1r-article-mobile research-layout mobile-version-info padding-top-30"> <span class=mversion>VERSION 1</span> <span class=details>PUBLISHED 11 Jun 2020</span> <span class="article-pubinfo-mobile versions-section"> </span> </div> <div class="f1r-article-mobile research-layout margin-top-20 is-centered"> <a href="/login?originalPath=/articles/9-590&scrollTo=add-new-comment" class=register-report-comment-button data-test-id=add-comment_mob> <button class="primary orange extra-padding comment-on-this-report">ADD YOUR COMMENT</button> </a> </div> <a href="/login?originalPath=/articles/9-590&scrollTo=add-new-comment" class="f1r-article-desk register-report-comment-button" data-test-id=add-comment> <span class=contracted></span>Comment </a> </div> <div class="f1r-article-mobile margin-bottom-30"> <div class=contracted-details> <a href="#" class="contracted-details-label author-affiliations"><span class=contracted></span>Author details</a> <a href="#" class=section-title>Author details</a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details affiliations is-hidden"> Laboratory of Brain and Cognition, National Institute of Mental Health, National Institutes of Health, Bethesda, MD, 20892, USA<br/> <p> <div class=margin-bottom> Susan G. Wardle <br/> <span>Roles: </span> Conceptualization, Writing – Original Draft Preparation, Writing – Review & Editing </div> <div class=margin-bottom> Chris I. Baker <br/> <span>Roles: </span> Conceptualization, Writing – Original Draft Preparation, Writing – Review & Editing </div> </p> </div> </div> <div class=contracted-details> <a href="#" class=section-title>Article Versions (1)</a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details grant-information article-page is-hidden"> <div> <a href="https://f1000research.com/articles/9-590/v1" title="Open version 1 of this article." class="" gahelper=1>version 1</a> <span class="article-pubinfo-mobile versions-section"> </span> </div> <div> Published: 11 Jun 2020, 9:590 </div> <div class=""><a href="https://doi.org/10.12688/f1000research.22296.1">https://doi.org/10.12688/f1000research.22296.1</a></div> </div> </div> <div class=contracted-details> <a href="#" class=section-title> <span class="f1r-icon icon-100_open_access"></span> Copyright </a> <span class="f1r-icon icon-14_more_small section-control"></span> <span class="f1r-icon icon-10_less_small section-control"></span> <div class="expanded-details grant-information article-page is-hidden"> © 2020 Wardle SG and Baker CI. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/" target=_blank data-test-id=box-licence-link>Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. The author(s) is/are employees of the US Government and therefore domestic copyright protection in USA does not apply to this work. The work may be protected under the copyright laws of other jurisdictions when used in those jurisdictions. </div> </div> <div class="padding-top-30 research-layout"> <div class=article-toolbox-wrapper-mobile> <div class=article-tools-icon-mobile data-section=download> <span class="f1r-icon icon-76_download_file white"></span> </div> <div class="article-tools-icon-mobile mobile-metrics article-metrics-wrapper metrics-icon-wrapper" data-section=metrics data-version-id=24595 data-id=22296 data-downloads="" data-views="" data-scholar="10.12688/f1000research.22296.1" data-recommended="" data-f1r-ga-helper="Article Page Metrics (Mobile)"> <span class="f1r-icon icon-89_metrics white"></span> </div> <div class=article-tools-divider-mobile></div> <div class=article-tools-icon-mobile data-section=cite> <span class="f1r-icon icon-82_quote white"></span> </div> <div class="article-tools-icon-mobile " data-section=track> <span class="f1r-icon icon-90_track white"></span> </div> <div class=article-tools-divider-mobile></div> <div class=article-tools-icon-mobile data-section=share> <span class="f1r-icon icon-34_share white"></span> </div> <span class=article-toolbox-stretch></span> </div> <div class=article-toolbox-content-mobile> <div class="toolbox-section download"> <div class=toolbox-section-heading>Download</div> <div class=toolbox-section-content> <a href="https://f1000research.com/articles/9-590/v1/pdf?article_uuid=13152842-c463-4283-8828-d568ef9e3a84" title="Download PDF" class="no-decoration pdf-download-helper"> <span class="f1r-icon icon-102_download_pdf toolbox-section-icon"></span> </a> <div class=toolbox-section-option-divider>&nbsp;</div> <a id=mobile-download-xml class=no-decoration href="#" title="Download XML"> <span class="f1r-icon icon-103_download_xml toolbox-section-icon"></span> </a> </div> <div class=toolbox-section-divider></div> <div class=toolbox-section-heading>Export To</div> <div class=toolbox-section-content> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=WORKSPACE>Sciwheel</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=BIBTEX>Bibtex</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=ENDNOTE>EndNote</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=PROCITE>ProCite</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=REF_MANAGER>Ref. Manager (RIS)</button> <button class="primary no-fill orange-text-and-border mobile-export" data-etype=SENTE>Sente</button> </div> </div> <div class="toolbox-section metrics"> <div class="toolbox-section-heading no-top-border">metrics</div> <div class="toolbox-section-divider toolbox-section-divider--no-height"></div> <div class=article-metrics-pageinfo> <div class=c-article-metrics-table> <table class=c-article-metrics-table__table> <thead> <tr> <th></th> <th><span class=c-article-metrics-table__heading>Views</span></th> <th><span class=c-article-metrics-table__heading>Downloads</span></th> </tr> </thead> <tbody> <tr> <th class=c-article-metrics-table__row-heading><span class="c-article-metrics-table__platform u-platform--" data-test-id=metrics_platform_name_mob>F1000Research</span></th> <td class="c-article-metrics-table__value js-article-views-count" data-test-id=metrics_platform_views_mob>-</td> <td class="c-article-metrics-table__value js-article-downloads-count" data-test-id=metrics_platform_downloads_mob>-</td> </tr> <tr> <th class=c-article-metrics-table__row-heading> <span class="u-ib u-middle c-article-metrics-table__pmc" data-test-id=metrics_pmc_name_mob>PubMed Central</span> <div class="c-block-tip c-block-tip--centered c-article-metrics-table__tooltip c-block-tip--md-padding c-block-tip--small-arrow u-ib u-middle"> <button type=button class="u-black--medium u-black--high@hover u-ib u-middle c-button--icon c-button--text c-block-tip__toggle"><i class="material-icons c-button--icon__icon">info_outline</i></button> <div class=c-block-tip__content>Data from PMC are received and updated monthly.</div> </div> </th> <td class="c-article-metrics-table__value js-pmc-views-count" data-test-id=metrics_pmc_views_mob>-</td> <td class="c-article-metrics-table__value js-pmc-downloads-count" data-test-id=metrics_pmc_downloads_mob>-</td> </tr> </tbody> </table> </div> </div> <span class=metrics-citations-container> <div class=toolbox-section-divider></div> <div class="toolbox-section-heading u-mb--1">Citations</div> <div> <div class=toolbox-section-colsplit> <div class=citations-scopus-logo> <a href="" target=_blank class="is-hidden metrics-citation-icon" title="View full citation details at www.scopus.com"><i class="material-icons scopus-icon">open_in_new</i></a> </div> <div class="toolbox-section-count scopus"> <a href='' class='scopus-citation-link is-hidden' target=_blank title='View full citation details at www.scopus.com'>0</a> </div> </div> <div class=toolbox-section-colsplit> <div class="citations-pubmed-logo f1000research"> <a href="" target=_blank class="is-hidden metrics-citation-icon f1000research" title="View full citation details"><i class="material-icons scopus-icon">open_in_new</i></a> </div> <div class="toolbox-section-count pubmed"> <a href='' class='pubmed-citation-link is-hidden' target=_blank title='View full citation details'>0</a> </div> </div> <div class=toolbox-section-divider></div> <div class=toolbox-section-content> <div class="citations-scholar-logo f1000research"> <a href="" target=_blank class="is-hidden metrics-citation-icon google-scholar f1000research" title="View full citation details" data-scholar="10.12688/f1000research.22296.1"><i class="material-icons scopus-icon">open_in_new</i></a> </div> </div> </div> </span> <span class=metrics-details-container> <div class=toolbox-section-divider></div> <div class="toolbox-section-content altmetric-section"> <div class=altmetrics-image></div> <div class=altmetrics-more-link> <a href="" target=_blank class=f1r-standard-link>SEE MORE DETAILS</a> </div> <div class=altmetric-mobile-column-counts></div> <div class=altmetric-mobile-column-readers></div> <div class=toolbox-section-divider></div> </div> </span> </div> <div class="toolbox-section cite"> <div class="toolbox-section-heading no-top-border">CITE</div> <div class=toolbox-section-divider></div> <div class=toolbox-section-heading>how to cite this article</div> <div id=citation-copy-mobile class="toolbox-section-content text-content heading9 small" data-test-id=mob_copy-citation_text> Wardle SG and Baker CI. Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context [version 1; peer review: 2 approved] <i>F1000Research</i> 2020, <b>9</b>(F1000 Faculty Rev):590 (<a href="https://doi.org/10.12688/f1000research.22296.1" target=_blank>https://doi.org/10.12688/f1000research.22296.1</a>) </div> <div class=toolbox-section-divider></div> <div class="toolbox-section-content text-content heading9 small"> NOTE: <em>it is important to ensure the information in <b>square brackets after the title</b> is included in all citations of this article.</em> </div> <div class=toolbox-section-content> <button class="primary orange extra-padding copy-cite-article-mobile js-clipboard" data-clipboard-target="#citation-copy-mobile" title="Copy the current citation details." data-test-id=mob_copy-citation_button-mob>COPY CITATION DETAILS</button> </div> </div> <div class="toolbox-section track"> <div class="toolbox-section-heading no-top-border">track</div> <div class=toolbox-section-divider></div> <div class=toolbox-section-heading>receive updates on this article</div> <div class="toolbox-section-content padding-left-20 padding-right-20 heading9 small"> Track an article to receive email alerts on any updates to this article. </div> <div class=toolbox-section-content> <a data-article-id=22296 id=mobile-track-article-signin-22296 title="Receive updates on new activity such as publication of new versions, peer reviews or author responses." href="/login?originalPath=/trackArticle/22296?target=/articles/9-590"> <button class="primary orange extra-padding"> TRACK THIS ARTICLE </button> </a> </div> </div> <div class="toolbox-section share"> <div class="toolbox-section-heading no-top-border">Share</div> <div class=toolbox-section-divider></div> <div class=toolbox-section-content> <a target=_blank class="f1r-shares-icon-square f1r-shares-email" title="Email this article"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-twitter" title="Share on Twitter"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-facebook" title="Share on Facebook"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-linkedin" title="Share on LinkedIn"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-reddit" title="Share on Reddit"></a> <a target=_blank class="f1r-shares-icon-square f1r-shares-mendelay" title="Share on Mendeley"></a> <div class="email-article-wrapper email-article-version-container"> <div class=toolbox-section-divider></div> <script src='https://www.recaptcha.net/recaptcha/api.js'></script> <form class="recommend-version-form-mobile research-layout"> <p>All fields are required.</p> <input name=versionId type=hidden value=24595 /> <input name=articleId type=hidden value=22296 /> <input name=senderName class="form-input-field reg-form" value="" type=text placeholder="Your name"/> <input name=senderEmail class="form-input-field reg-form margin-top" value="" type=text placeholder="Your email address"/> <textarea name=recipientEmails class="form-textarea-field ninetynine-percent-wide margin-top no-resize" placeholder="Recipient email address(es) (comma delimited)"></textarea> <input class="form-input-field reg-form margin-top" name=subject type=text value="Interesting article on F1000Research" placeholder=Subject /> <textarea name=message class="form-textarea-field reg-form margin-top no-resize">I thought this article from F1000Research (https://f1000research.com) would be of interest to you.</textarea> <div class="g-recaptcha margin-top" data-sitekey=6LcHqxoUAAAAANP3_0TzpGG6qFvl4DhbUcuRzw7W></div> <input value="" name=captcha type=hidden /> <p>A full article citation will be automatically included.</p> <p><img class="ticker-email-article-details hidden" src="/img/ticker.gif" alt=loading /></p> <button class="secondary orange margin-bottom" data-test-id=version_share_email_send>SEND EMAIL</button> <div class="orange-message margin-bottom is-hidden" data-test-id=version_share_email_message></div> </form> </div> </div> </div> </div> </div> <a name=article-reports></a> <div id=article-reports class="u-mt--3 reports-comments no-divider"> <div class="current-referee-status current-referee-status--faculty "> <h2 class=main-title id=current-referee-status> <span class="research-layout f1r-article-mobile-inline valign-middle"> <span class="f1r-icon icon-85_peer_review size30"></span> </span> Open Peer Review <span class="f1r-article-mobile-inline float-right"> <span class="f1r-icon icon-14_more_small"></span> <span class="f1r-icon icon-10_less_small"></span> </span> </h2> <a name=current-referee-status></a> <div class=current-referee-status__content name=add-new-report-comment id=add-new-report-comment> Current Reviewer Status: <span class="research-layout f1r-article-desk-inline"> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved></span> </span> <span class="research-layout f1r-article-mobile float-right"> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved></span> </span> <span class="research-layout f1r-article-desk-inline"> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved></span> </span> <span class="research-layout f1r-article-mobile float-right"> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved></span> </span> <span class="research-layout f1r-article-mobile"> <div class=mobile-ref-status-help> Key to Reviewer Statuses <span class=referee-status-pointer></span> <span class="view-control float-right">VIEW</span> <span class="view-control float-right is-hidden">HIDE</span> <div class=mobile-ref-status-help-content> <div class="cf margin-top"> <span class="f1r-icon icon-86_approved status-green smaller float-left margin-bottom-40 margin-right" title=Approved></span> <span class=title>Approved</span>The paper is scientifically sound in its current form and only minor, if any, improvements are suggested </div> <div class="cf margin-top"> <span class="f1r-icon icon-87_approved_reservations status-green smaller float-left margin-bottom-40 margin-right" title="Approved with Reservations"></span> <span class=title>Approved with reservations</span> A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit. </div> <div class="cf margin-top"> <span class="f1r-icon icon-88_not_approved status-red small float-left margin-bottom-30 margin-right" title="Not Approved"></span> <span class=title>Not approved</span>Fundamental flaws in the paper seriously undermine the findings and conclusions </div> </div> </div> </span> <div class=editorial-note> <h5 class=editorial-note__title>Editorial Note on the Review Process</h5> <p class=editorial-note__text><a href="/browse/faculty-reviews">Faculty Reviews</a> are review articles written by the prestigious Members of <a href="https://facultyopinions.com/prime/home">Faculty Opinions</a>. The articles are commissioned and peer reviewed before publication to ensure that the final, published version is comprehensive and accessible. The reviewers who approved the final version are listed with their names and affiliations.</p> </div> <div class=approved-referee> <h4 class=approved-referee__title>Reviewers who approved this article</h4> <ol class=approved-referee__list> <li class="approved-referee__list-item "> <span class=approved-referee__co-referee> <strong>Aude Oliva</strong>, Computer Science and Artificial Intelligence Laboratory, MIT, USA </span> <span class=approved-referee__competing-list> <strong>Competing interests:</strong> No competing interests were declared. (for version 1) <br> </span> </li> <li class="approved-referee__list-item margin-top-20"> <span class=approved-referee__co-referee> <strong>Marius Peelen</strong>, Donders Institute for Brain, Cognition and Behaviour, Radboud University, The Netherlands </span> <span class=approved-referee__competing-list> <strong>Competing interests:</strong> No competing interests were declared. (for version 1) <br> </span> </li> </ol> </div> </div> </div> </div> <div class="f1r-article-mobile research-layout"> <div class="mobile-sections-divider before-comments"></div> </div> <div id=article-comments class="article-comments padding-bottom-20"> <div class=current-article-comment-section> <h2 class=main-title name=add-new-comment id=add-new-comment> <span class="research-layout f1r-article-mobile-inline valign-middle"> <span class="f1r-icon icon-104_comments size30"></span> </span> <span class=f1r-article-desk-inline>Comments on this article</span> <span class=f1r-article-mobile-inline>Comments (0)</span> <span class="f1r-article-mobile-inline float-right"> <span class="f1r-icon icon-14_more_small"></span> <span class="f1r-icon icon-10_less_small"></span> </span> </h2> </div> <div class="f1r-article-desk-inline referee-report-info-box referee-report-version-box"> Version 1 </div> <div class="f1r-article-mobile research-layout mobile-version-info padding-top-30"> <span class=mversion>VERSION 1</span> <span class=details>PUBLISHED 11 Jun 2020</span> <span class="article-pubinfo-mobile versions-section"> </span> </div> <div class="f1r-article-mobile research-layout margin-top-20 is-centered"> <a href="/login?originalPath=/articles/9-590&scrollTo=add-new-comment" class=register-report-comment-button data-test-id=add-comment_mob> <button class="primary orange extra-padding comment-on-this-report">ADD YOUR COMMENT</button> </a> </div> <a href="/login?originalPath=/articles/9-590&scrollTo=add-new-comment" class="f1r-article-desk register-report-comment-button" data-test-id=add-comment> <span class=contracted></span>Comment </a> </div> </div> </div> <div id=article_main-column class="p-article__sidebar o-layout__item u-1/3 not-expanded js-article-sidebar"> <div class="o-tab p-article__column-toggle-container"> <button class="c-tab c-tab--left js-column-toggle p-article__column-toggle not-expanded " type=button data-target-main=article_main-column data-target-secondary=article_secondary-column><i class="c-tab__icon material-icons u-hide@expanded">keyboard_arrow_left</i><i class="c-tab__icon material-icons u-show@expanded">keyboard_arrow_right</i></button> </div> <div class=p-article__sidebar-content> <div class="p-article__sidebar-scroller js-article-sidebar-scroller"> <section class="p-article__sidebar-view js-article-sidebar-view js-article-sidebar-main u-pt u-pb--8" data-view=peer-review> <div class="o-layout o-layout--flush"> <div class="o-layout__item u-pl"> <h3 class="u-mt--0 u-mb--2 t-h3 u-weight--md u-pl" data-test-id=article_sidebar_heading>Open Peer Review</h3> </div> <div class=o-layout__item> <section class=""> <div class="p-article__sidebar-highlight u-mb--2 u-pr--1"> <div class="o-actions o-actions--middle"> <div class=o-actions__primary> <h4 class="u-mt--0 u-mb--0 u-ib u-middle t-h4 u-weight--md u-mr--1/2">Reviewer Status</h4> <div class="c-referee-status__icons u-ib u-middle"> <i class="c-icn--f1r-icon c-icn--approved " title=Approved #data-refInfo=68927-64605></i> <i class="c-icn--f1r-icon c-icn--approved " title=Approved #data-refInfo=68926-64604></i> </div> </div> <div class="o-actions__secondary _mdl-layout"> <div class="c-block-tip p-article__sidebar-tooltip c-block-tip--below c-block-tip--small-arrow c-block-tip--sm-padding"> <button type=button class="c-button c-button--icon c-button--text c-block-tip__toggle"><i class="material-icons c-button--icon__icon">info_outline</i></button> <div class=c-block-tip__content> <p class="t-body u-mt--0 u-mb--0"><em class=u-weight--md>Alongside their report, reviewers assign a status to the article:</em></p> <dl class=c-definitions> <dt class="c-definitions__term u-upper t-control u-weight--md u-black--high"><span class="u-ib u-middle"> <i class="c-icn--f1r-icon c-icn--approved " title=Approved #data-refInfo=""></i> </span> <span class="u-middle u-ib u-ml--1/2">Approved</span></dt> <dd class="c-definitions__description t-caption">The paper is scientifically sound in its current form and only minor, if any, improvements are suggested</dd> <dt class="c-definitions__term u-upper t-control u-weight--md u-black--high"><span class="u-ib u-middle"> <i class="c-icn--f1r-icon c-icn--reservations" title="Approved with Reservations" #data-refInfo=""></i> </span> <span class="u-middle u-ib u-ml--1/2">Approved with reservations</span></dt> <dd class="c-definitions__description t-caption"> A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit. </dd> <dt class="c-definitions__term u-upper t-control u-weight--md u-black--high"><span class="u-ib u-middle"> <i class="c-icn--f1r-icon c-icn--not-approved " title="Not Approved" #data-refInfo=""></i> </span> <span class="u-middle u-ib u-ml--1/2">Not approved</span></dt> <dd class="c-definitions__description t-caption">Fundamental flaws in the paper seriously undermine the findings and conclusions</dd> </dl> </div> </div> </div> </div> </div> <div class="o-layout__item u-mb--1"><h4 class="u-mt--0 u-mb--0 u-ib u-middle t-h4 u-weight--md">Reviewer Reports</h4></div> <table class="c-report-timeline u-mb--2"> <thead class=c-report-timeline__headings> <tr> <th></th> <th class="u-pb--1/2" colspan=2><em class="t-body u-weight--rg">Invited Reviewers</em></th> </tr> <tr class=c-report-timeline__headings-row> <th></th> <th class="c-report-timeline__headings-version p-article__color--dark">1</th> <th class="c-report-timeline__headings-version p-article__color--dark">2</th> </tr> </thead> <tbody> <tr class="c-report-timeline__row c-report-timeline__row--selected "> <th class=c-report-timeline__version> <a data-test-id=sidebar_timeline_v1_version href="https://f1000research.com/articles/9-590/v1">Version 1</a><br/> <span class=p-article__color--dark> <span data-test-id=sidebar_timeline_v1_date class=c-report-timeline__date>11 Jun 20</span> </span> </th> <td class="c-report-timeline__report c-report-timeline__cell "> <i class="c-icn--f1r-icon c-icn--approved small" title=Approved #data-refInfo=""></i> </td> <td class="c-report-timeline__report c-report-timeline__cell "> <i class="c-icn--f1r-icon c-icn--approved small" title=Approved #data-refInfo=""></i> </td> </tr> </tbody> </table> <div class=o-layout__item> <hr class="c-hr c-hr--low u-mb--2"> </div> </section> </div> <div class=o-layout__item> <div class="u-pl u-pr"> <p class="t-body u-mt--0 u-mb--2"><a href="/browse/faculty-reviews">Faculty Reviews</a> are review articles written by the prestigious Members of <a href="https://facultyopinions.com/prime/home" target=_blank class=in-text-link>Faculty Opinions</a>. The articles are commissioned and peer reviewed before publication to ensure that the final, published version is comprehensive and accessible. The reviewers who approved the final version are listed with their names and affiliations.</p> <ol class="p-article__faculty-referee-list t-caption"> <li> <p class=" u-mt--0 u-mb--1/2"><strong>Aude Oliva</strong>, Computer Science and Artificial Intelligence Laboratory, MIT, USA </p> <div class="c-read-more js-read-more " data-lines=3> <div class="c-read-more__content js-read-more-content "> <p class="u-mt--0 u-mb--0 p-article__color--light"> <strong class=u-weight--md>Competing interests:</strong> No competing interests were declared. </p> </div> <a href="#" class="c-read-more__toggle js-read-more-toggle t-caption"> <span class=c-read-more__expand>View more</span> <span class=c-read-more__contract>View less</span> </a> </div> </li> <li> <p class=" u-mt--0 u-mb--1/2"><strong>Marius Peelen</strong>, Donders Institute for Brain, Cognition and Behaviour, Radboud University, The Netherlands </p> <div class="c-read-more js-read-more " data-lines=3> <div class="c-read-more__content js-read-more-content "> <p class="u-mt--0 u-mb--0 p-article__color--light"> <strong class=u-weight--md>Competing interests:</strong> No competing interests were declared. </p> </div> <a href="#" class="c-read-more__toggle js-read-more-toggle t-caption"> <span class=c-read-more__expand>View more</span> <span class=c-read-more__contract>View less</span> </a> </div> </li> </ol> </div> </div> <section class="o-layout__item u-pl"> <div class=u-pl> <hr class="c-hr c-hr--low c-hr--md u-mb--3"> <h4 class="t-h3 u-weight--md u-mt--0 u-mb--2">Comments on this article</h4> <div class=u-mb--4> <p class="u-mt--0 u-mb--1 t-h4"><a class=p-article__color--light href="#article-comments">All Comments</a><span class=" u-ib u-ml--1/2 p-article__color--light">(0)</span></p> <a class=t-h4 href="/login?originalPath=/articles/9-590&scrollTo=add-new-comment" data-test-id=add-comment>Add a comment</a> </div> <hr class="c-hr c-hr--low c-hr--md u-mb--4"> <div class="research-layout f1r-article-desk"> <div class="heading6 c-ribbon-wrapper c-ribbon-wrapper--etoc f1000research "> <div class=c-ribbon-wrapper__body>Sign up for content alerts</div> </div> </div> <div class="research-layout sidebar-sign-up-form f1r-article-desk u-mb--4 "> <form class=js-email-alert-signup action="#" method=POST data-email=tocAlertWeekly> <input type=hidden name=isUserLoggedIn class=js-email-alert-signup-logged-in value=N /> <input type=hidden name=userId class=js-email-alert-signup-user-id value=""/> <input type=hidden name=frequency class=js-email-alert-signup-frequency value=WEEKLY /> <div class="o-actions o-actions--middle"> <div class=o-actions__primary> <input type=email name=emailAddress class="form-input-field js-email-alert-signup-address u-1/1 u-bb" required=required placeholder=Email /> </div> <div class=o-actions__secondary> <div class="_mdl-layout u-ml--1/2"> <button class="mdl-button mdl-js-button mdl-button--colored mdl-button--small mdl-button--filled js-email-alert-signup-submit">Sign Up</button> </div> </div> </div> </form> <div id=sidebar-sign-up-message class="section-text js-email-alert-signup-msg is-hidden">You are now signed up to receive this alert</div> </div> <section class=js-terms-container> <hr class="c-hr c-hr--low c-hr--md u-mb--3"> <h4 class="t-h3 u-weight--md u-mt--0 u-mb--1">Browse by related subjects</h4> <div class="article-subcontainer article-subcontainer--sidebar"> <ul class=js-terms-list></ul> </div> </section> </div> </section> </div> </section> </div> <script src="/js/shared_scripts/modal-dialogue.js"></script> <script src="/js/shared_scripts/read-more.js"></script> <script src="/js/article/article-router.js"></script> <script src="/js/article/article-sidebar.js"></script> <script src="/js/referee/new/referee_helpers.js"></script> <script src="/js/article/article-column-toggle.js"></script> </div> </div> </div> </main> <input type=hidden id=_articleVersionUrl value="https://f1000research.com/articles/9-590/v1/"> <div class=research-help id=about-referee-status> <div class="research-layout research-help-content about-referee-status"> <span class="close-research-help dark-cross" title=Close></span> Alongside their report, reviewers assign a status to the article: <div class="cf research-help-row"> <span class="f1r-icon icon-86_approved status-green smaller" title=Approved></span> <span class=research-help-text>Approved - the paper is scientifically sound in its current form and only minor, if any, improvements are suggested</span> </div> <div class="cf research-help-row"> <span class="f1r-icon icon-87_approved_reservations status-green smaller" title="Approved with Reservations"></span> <span class=research-help-text>Approved with reservations - A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit. </span> </div> <div class="cf research-help-row"> <span class="f1r-icon icon-88_not_approved status-red small" title="Not Approved"></span> <span class=research-help-text>Not approved - fundamental flaws in the paper seriously undermine the findings and conclusions</span> </div> </div> </div> <div id=datasets-info class=is-hidden> </div> <div class=article-interactive-content-container style="display: none;"> <a name=f-template class=n-a></a> <div class="interactive-content-wrapper padding-20"> <img src="" class=interactive-content-image title="Open the interactive image display"> <div class=interactive-content-title></div> <div class=interactive-content-text></div> <div class="f1r-article-desk interactive-content-ribbon" data-interactive-content-type=R-Script> <div class=interactive-content-label>Adjust parameters to alter display</div> <div class=interactive-content-button></div> </div> <div class="f1r-article-mobile mobile-interactive-note"> View on desktop for interactive features <img src="/img/icon/interactive_content.png" class="float-right margin-right-40"/> </div> <div class=clearfix></div> </div> </div> <div id=article-interactive-omero-container class=article-interactive-omero-container style="display: none;"> <div class=interactive-content-wrapper> <div class="interactive-omero-button omero-content" title="Open the interactive content window." data-interactive-content-type=Omero></div> <div class=has-interactive-content-image> <span class=box-arrow></span> <span class=box-middle>Includes Interactive Elements</span> <span class=box-end></span> </div> <div class="fig panel clearfix" style="margin: 0; padding-bottom: 20px;"> <a name=templatelink class=n-a></a> <a target=_blank href="" class=link-for-omero-image> <img src="" class=interactive-omero-image title="Open the image display window."> </a> <div class=caption> <div class=interactive-content-title></div> <div class=interactive-content-text></div> </div> <div class="is-hidden omero-image-list"></div> </div> <div class="f1r-article-mobile mobile-interactive-note omero"> View on desktop for interactive features <img src="/img/icon/interactive_content.png" class="float-right margin-right-40"/> </div> <div class=clearfix></div> </div> </div> <div class="add-comment-container shadow-box is-hidden" id=save-comment-container> <span id=save-comment-text class=intro-text>Edit comment</span> <textarea id=new-comment name=new-comment class="global-textarea comment margin-bottom margin-top"></textarea> <p><strong>Competing Interests</strong></p> <textarea id=new-competing-interests name=competing-interests class="global-textarea competing-interests margin-bottom check-xss"></textarea> <div class=clearfix></div> <button id=cancelComment type=button class="general-white-orange-button float-right no-background-button margin-left"> Cancel </button> <button id=save-comment-button commentId="" type=button class="general-white-orange-button float-right"> Save </button> <div class=clearfix></div> <div class="green-message margin-top is-hidden comment-is-saved">The comment has been saved.</div> <div class="red-message margin-top is-hidden comment-not-added">An error has occurred. Please try again.</div> <div class="red-message margin-top is-hidden comment-enter-text ucf">Your must enter a comment.</div> <div class="red-message margin-top is-hidden comment-references-error references">References error.</div> </div> <div class="modal-window-wrapper is-hidden"> <div id=conflicts-interests class="modal-window padding-20"> <div class=modal-window__content> <h2 class=h2-title>Competing Interests Policy</h2> <p> Provide sufficient details of any financial or non-financial competing interests to enable users to assess whether your comments might lead a reasonable person to question your impartiality. Consider the following examples, but note that this is not an exhaustive list: </p> <div class=heading5>Examples of 'Non-Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>Within the past 4 years, you have held joint grants, published or collaborated with any of the authors of the selected paper.</li> <li class=standard-padding>You have a close personal relationship (e.g. parent, spouse, sibling, or domestic partner) with any of the authors.</li> <li class=standard-padding>You are a close professional associate of any of the authors (e.g. scientific mentor, recent student).</li> <li class=standard-padding>You work at the same institute as any of the authors.</li> <li class=standard-padding>You hope/expect to benefit (e.g. favour or employment) as a result of your submission.</li> <li class=standard-padding>You are an Editor for the journal in which the article is published.</li> </ol> <div class="heading5 padding-top">Examples of 'Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>You expect to receive, or in the past 4 years have received, any of the following from any commercial organisation that may gain financially from your submission: a salary, fees, funding, reimbursements.</li> <li class=standard-padding>You expect to receive, or in the past 4 years have received, shared grant support or other funding with any of the authors.</li> <li class=standard-padding>You hold, or are currently applying for, any patents or significant stocks/shares relating to the subject matter of the paper you are commenting on.</li> </ol> </div> <div class="f1r-article-mobile research-layout"> <span class=modal-window-close-button></span> </div> </div> </div> <div class="o-modal c-email-alert-popup js-email-alert-popup is-hidden"> <div class="o-modal__body js-modal-body c-email-alert-popup__inner"> <h3 class=c-email-alert-popup__title>Stay Updated</h3> <p class=c-email-alert-popup__sub>Sign up for content alerts and receive a weekly or monthly email with all newly published articles</p> <p><a href="/register?originalPath=" class="c-email-alert-popup__lnk c-email-alert-popup__button js-email-alert-popup-action">Register with F1000Research</a></p> <p>Already registered? <a href="/login?originalPath=" class="c-email-alert-popup__lnk js-email-alert-popup-action">Sign in</a></p> <p class=c-email-alert-popup__footer><a class="c-email-alert-popup__lnk js-email-alert-popup-cancel" href="#">Not now, thanks</a></p> </div> </div> <div id=addCommentModal role=dialog aria-labelledby=addCommentModal_title aria-describedby=addCommentModal_description> <div class="c-modal js-modal is-closed p-article__add-comment-modal c-modal--xlarge js-article-comment-modal c-modal--scroll "> <aside class="c-modal__content u-black--high o-box u-pb--0 u-bg--2 c-modal--scroll-always "> <div class=c-modal__close> <button type=button class="c-button c-button--icon c-button--medium c-button--full@hover js-modal__close"><i class="c-button--icon__icon material-icons">close</i></button> </div> <div id=addCommentModal_description class="c-modal__description js-modal__content t-h4 u-mt--0 u-mb--2 u-black--medium"> <div class="js-add-comment-container t-caption u-black--high"> <div class=u-weight--bd>PLEASE NOTE</div> <div class=u-mt--1> <span class="red u-weight--bd">If you are an AUTHOR of this article,</span> please check that you signed in with the account associated with this article otherwise we cannot automatically identify your role as an author and your comment will be labelled as a &ldquo;User Comment&rdquo;. </div> <div class=u-mt--1> <span class="red u-weight--bd">If you are a REVIEWER of this article,</span> please check that you have signed in with the account associated with this article and then go to your <a href="/my/referee">account</a> to submit your report, please do not post your review here. </div> <div class="u-mt--1 u-mb--1"> If you do not have access to your original account, please <a href="mailto:research@f1000.com">contact us</a>. </div> <p class=no-top-margin>All commenters must hold a formal affiliation as per our <a href="/about/policies#commentspolicy" target=_blank>Policies</a>. The information that you give us will be displayed next to your comment.</p> <p>User comments must be in English, comprehensible and relevant to the article under discussion. We reserve the right to remove any comments that we consider to be inappropriate, offensive or otherwise in breach of the <a href="/about/legal/usercommenttermsandconditions" target=_blank>User Comment Terms and Conditions</a>. Commenters must not use a comment for personal attacks. When criticisms of the article are based on unpublished data, the data should be made available.</p> <div class="comments-note margin-bottom" id=accept-user-comments> <input class=js-add-comment-accept-terms type=checkbox id=acceptedTermsAndConditions name=acceptedTermsAndConditions> I accept the <a href="/about/legal/usercommenttermsandconditions" target=_blank> User Comment Terms and Conditions</a> <span class=required>&nbsp;</span> </div> <div class="default-error margin-top is-hidden comment-accept-conditions utac">Please confirm that you accept the User Comment Terms and Conditions.</div> <div class="research-layout registration-form u-mb--2"> <div class="u-mb--1 u-mt--2"> <strong>Affiliation</strong> </div> <div class=form-field> <input type=text name=institution class="form-input-field check-xss js-add-comment-institution" placeholder="Organization *" autocomplete=off /> <div class="default-error margin-top is-hidden comment-enter-institution institution">Please enter your organisation.</div> </div> <div class=form-field> <input type=text name=place class="form-input-field check-xss js-add-comment-place" placeholder=Place> </div> <div class=form-field> <div class="form-input-wrapper hundred-percent-wide"> <div class="new-select-standard-wrapper half-width inline-display heading10"> <select name=countryId id=country class="form-select-menu smaller js-add-comment-country"> <option value=-1>Country*</option> <option value=840>USA</option> <option value=826>UK</option> <option value=124>Canada</option> <option value=156>China</option> <option value=250>France</option> <option value=276>Germany</option> <optgroup label=-----------------------------------------------></optgroup> <option value=4>Afghanistan</option> <option value=248>Aland Islands</option> <option value=8>Albania</option> <option value=12>Algeria</option> <option value=16>American Samoa</option> <option value=20>Andorra</option> <option value=24>Angola</option> <option value=660>Anguilla</option> <option value=10>Antarctica</option> <option value=28>Antigua and Barbuda</option> <option value=32>Argentina</option> <option value=51>Armenia</option> <option value=533>Aruba</option> <option value=36>Australia</option> <option value=40>Austria</option> <option value=31>Azerbaijan</option> <option value=44>Bahamas</option> <option value=48>Bahrain</option> <option value=50>Bangladesh</option> <option value=52>Barbados</option> <option value=112>Belarus</option> <option value=56>Belgium</option> <option value=84>Belize</option> <option value=204>Benin</option> <option value=60>Bermuda</option> <option value=64>Bhutan</option> <option value=68>Bolivia</option> <option value=70>Bosnia and Herzegovina</option> <option value=72>Botswana</option> <option value=74>Bouvet Island</option> <option value=76>Brazil</option> <option value=86>British Indian Ocean Territory</option> <option value=92>British Virgin Islands</option> <option value=96>Brunei</option> <option value=100>Bulgaria</option> <option value=854>Burkina Faso</option> <option value=108>Burundi</option> <option value=116>Cambodia</option> <option value=120>Cameroon</option> <option value=124>Canada</option> <option value=132>Cape Verde</option> <option value=136>Cayman Islands</option> <option value=140>Central African Republic</option> <option value=148>Chad</option> <option value=152>Chile</option> <option value=156>China</option> <option value=162>Christmas Island</option> <option value=166>Cocos (Keeling) Islands</option> <option value=170>Colombia</option> <option value=174>Comoros</option> <option value=178>Congo</option> <option value=184>Cook Islands</option> <option value=188>Costa Rica</option> <option value=384>Cote d'Ivoire</option> <option value=191>Croatia</option> <option value=192>Cuba</option> <option value=196>Cyprus</option> <option value=203>Czech Republic</option> <option value=180>Democratic Republic of the Congo</option> <option value=208>Denmark</option> <option value=262>Djibouti</option> <option value=212>Dominica</option> <option value=214>Dominican Republic</option> <option value=218>Ecuador</option> <option value=818>Egypt</option> <option value=222>El Salvador</option> <option value=226>Equatorial Guinea</option> <option value=232>Eritrea</option> <option value=233>Estonia</option> <option value=231>Ethiopia</option> <option value=238>Falkland Islands</option> <option value=234>Faroe Islands</option> <option value=583>Federated States of Micronesia</option> <option value=242>Fiji</option> <option value=246>Finland</option> <option value=250>France</option> <option value=254>French Guiana</option> <option value=258>French Polynesia</option> <option value=260>French Southern Territories</option> <option value=266>Gabon</option> <option value=268>Georgia</option> <option value=276>Germany</option> <option value=288>Ghana</option> <option value=292>Gibraltar</option> <option value=300>Greece</option> <option value=304>Greenland</option> <option value=308>Grenada</option> <option value=312>Guadeloupe</option> <option value=316>Guam</option> <option value=320>Guatemala</option> <option value=831>Guernsey</option> <option value=324>Guinea</option> <option value=624>Guinea-Bissau</option> <option value=328>Guyana</option> <option value=332>Haiti</option> <option value=334>Heard Island and Mcdonald Islands</option> <option value=336>Holy See (Vatican City State)</option> <option value=340>Honduras</option> <option value=344>Hong Kong</option> <option value=348>Hungary</option> <option value=352>Iceland</option> <option value=356>India</option> <option value=360>Indonesia</option> <option value=364>Iran</option> <option value=368>Iraq</option> <option value=372>Ireland</option> <option value=376>Israel</option> <option value=380>Italy</option> <option value=388>Jamaica</option> <option value=392>Japan</option> <option value=832>Jersey</option> <option value=400>Jordan</option> <option value=398>Kazakhstan</option> <option value=404>Kenya</option> <option value=296>Kiribati</option> <option value=901>Kosovo (Serbia and Montenegro)</option> <option value=414>Kuwait</option> <option value=417>Kyrgyzstan</option> <option value=418>Lao People's Democratic Republic</option> <option value=428>Latvia</option> <option value=422>Lebanon</option> <option value=426>Lesotho</option> <option value=430>Liberia</option> <option value=434>Libya</option> <option value=438>Liechtenstein</option> <option value=440>Lithuania</option> <option value=442>Luxembourg</option> <option value=446>Macao</option> <option value=807>Macedonia</option> <option value=450>Madagascar</option> <option value=454>Malawi</option> <option value=458>Malaysia</option> <option value=462>Maldives</option> <option value=466>Mali</option> <option value=470>Malta</option> <option value=584>Marshall Islands</option> <option value=474>Martinique</option> <option value=478>Mauritania</option> <option value=480>Mauritius</option> <option value=175>Mayotte</option> <option value=484>Mexico</option> <option value=581>Minor Outlying Islands of the United States</option> <option value=498>Moldova</option> <option value=492>Monaco</option> <option value=496>Mongolia</option> <option value=499>Montenegro</option> <option value=500>Montserrat</option> <option value=504>Morocco</option> <option value=508>Mozambique</option> <option value=104>Myanmar</option> <option value=516>Namibia</option> <option value=520>Nauru</option> <option value=524>Nepal</option> <option value=530>Netherlands Antilles</option> <option value=540>New Caledonia</option> <option value=554>New Zealand</option> <option value=558>Nicaragua</option> <option value=562>Niger</option> <option value=566>Nigeria</option> <option value=570>Niue</option> <option value=574>Norfolk Island</option> <option value=580>Northern Mariana Islands</option> <option value=408>North Korea</option> <option value=578>Norway</option> <option value=512>Oman</option> <option value=586>Pakistan</option> <option value=585>Palau</option> <option value=275>Palestinian Territory</option> <option value=591>Panama</option> <option value=598>Papua New Guinea</option> <option value=600>Paraguay</option> <option value=604>Peru</option> <option value=608>Philippines</option> <option value=612>Pitcairn</option> <option value=616>Poland</option> <option value=620>Portugal</option> <option value=630>Puerto Rico</option> <option value=634>Qatar</option> <option value=638>Reunion</option> <option value=642>Romania</option> <option value=643>Russian Federation</option> <option value=646>Rwanda</option> <option value=654>Saint Helena</option> <option value=659>Saint Kitts and Nevis</option> <option value=662>Saint Lucia</option> <option value=666>Saint Pierre and Miquelon</option> <option value=670>Saint Vincent and the Grenadines</option> <option value=882>Samoa</option> <option value=674>San Marino</option> <option value=678>Sao Tome and Principe</option> <option value=682>Saudi Arabia</option> <option value=686>Senegal</option> <option value=688>Serbia</option> <option value=690>Seychelles</option> <option value=694>Sierra Leone</option> <option value=702>Singapore</option> <option value=703>Slovakia</option> <option value=705>Slovenia</option> <option value=90>Solomon Islands</option> <option value=706>Somalia</option> <option value=710>South Africa</option> <option value=239>South Georgia and the South Sandwich Is</option> <option value=410>South Korea</option> <option value=724>Spain</option> <option value=144>Sri Lanka</option> <option value=736>Sudan</option> <option value=740>Suriname</option> <option value=744>Svalbard and Jan Mayen</option> <option value=748>Swaziland</option> <option value=752>Sweden</option> <option value=756>Switzerland</option> <option value=760>Syria</option> <option value=158>Taiwan</option> <option value=762>Tajikistan</option> <option value=834>Tanzania</option> <option value=764>Thailand</option> <option value=270>The Gambia</option> <option value=528>The Netherlands</option> <option value=626>Timor-Leste</option> <option value=768>Togo</option> <option value=772>Tokelau</option> <option value=776>Tonga</option> <option value=780>Trinidad and Tobago</option> <option value=788>Tunisia</option> <option value=792>Turkey</option> <option value=795>Turkmenistan</option> <option value=796>Turks and Caicos Islands</option> <option value=798>Tuvalu</option> <option value=800>Uganda</option> <option value=826>UK</option> <option value=804>Ukraine</option> <option value=784>United Arab Emirates</option> <option value=850>United States Virgin Islands</option> <option value=858>Uruguay</option> <option value=840>USA</option> <option value=860>Uzbekistan</option> <option value=548>Vanuatu</option> <option value=862>Venezuela</option> <option value=704>Vietnam</option> <option value=876>Wallis and Futuna</option> <option value=905>West Bank and Gaza Strip</option> <option value=732>Western Sahara</option> <option value=887>Yemen</option> <option value=894>Zambia</option> <option value=716>Zimbabwe</option> </select> </div> </div> <div class="default-error margin-top is-hidden comment-enter-country country">Please select your country.</div> </div> </div> <textarea data-test-id=article_add-comment_comment name=new-comment class="js-add-comment-comment comment margin-bottom margin-top"></textarea> <div class="default-error margin-top comment-enter-text comment-error is-hidden ">You must enter a comment.</div> <label class="comments-note u-mt--2 u-mb--2" for=competingInterests_1> <div class="u-mb--1 u-mt--2"><strong data-test-id=article_report-add-comment_competing-interests-title>Competing Interests</strong></div> <p class="u-mb--2 u-mt--0" data-test-id=article_report-add-comment_competing-interests-description>Please disclose any <a href="#article-competing-intersts-policy" class=js-modal-competing-intersts-toggle>competing interests</a> that might be construed to influence your judgment of the article's or peer review report's validity or importance.</p> </label> <div id=article-competing-intersts-policy class=js-article-competing-interests-policy style="display: none;"> <h2 class="h2-title u-mt--0 u-pt--0">Competing Interests Policy</h2> <p> Provide sufficient details of any financial or non-financial competing interests to enable users to assess whether your comments might lead a reasonable person to question your impartiality. Consider the following examples, but note that this is not an exhaustive list: </p> <div class=heading5>Examples of 'Non-Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>Within the past 4 years, you have held joint grants, published or collaborated with any of the authors of the selected paper.</li> <li class=standard-padding>You have a close personal relationship (e.g. parent, spouse, sibling, or domestic partner) with any of the authors.</li> <li class=standard-padding>You are a close professional associate of any of the authors (e.g. scientific mentor, recent student).</li> <li class=standard-padding>You work at the same institute as any of the authors.</li> <li class=standard-padding>You hope/expect to benefit (e.g. favour or employment) as a result of your submission.</li> <li class=standard-padding>You are an Editor for the journal in which the article is published.</li> </ol> <div class="heading5 padding-top">Examples of 'Financial Competing Interests'</div> <ol class="numbered-list no-padding"> <li class=standard-padding>You expect to receive, or in the past 4 years have received, any of the following from any commercial organisation that may gain financially from your submission: a salary, fees, funding, reimbursements.</li> <li class=standard-padding>You expect to receive, or in the past 4 years have received, shared grant support or other funding with any of the authors.</li> <li class=standard-padding>You hold, or are currently applying for, any patents or significant stocks/shares relating to the subject matter of the paper you are commenting on.</li> </ol> </div> <div id=competingInterests_1 class="c-inline-editor js-inline-editor js-form_input u-bb--all u-mb--2 js-comment-competing-interests" data-name=competing-interests data-rows=3> <textarea id=competingInterests_1_input name=competing-interests placeholder="&nbsp;" required=false class="u-hide-visually js-inline-editor_input" tabindex=-1></textarea> <div class="c-inline-editor__editor js-inline-editor_editor o-box c-box o-box--tiny u-bg--11" data-target=competingInterests_1_input role=textbox required=false data-placeholder="&nbsp;" contenteditable=true></div> <span class="c-inline-editor__error js-inline-editor-message">Please state your competing interests</span> </div> <div data-test-id=article_add-comment_saved class="green-message comments is-hidden comment-is-saved">The comment has been saved.</div> <div data-test-id=article_add-comment_error class="default-error comments is-hidden comment-not-added">An error has occurred. Please try again.</div> <div class=clearfix></div> <div class=js-hook></div> </div> <div class="c-modal__extra-message js-modal__extra-message t-h4 u-black--medium"></div></div> <div class="c-modal__actions o-box__actions"> <a href="#" data-test-id=article_add-comment_cancel class="c-button c-button--full js-modal__close c-button--secondary">Cancel</a> <a href="#" data-test-id=article_add-comment_post class="c-button c-button--full js-modal__confirm c-button--primary">Post</a> </div> </aside> </div> </div> <style>
                .at-icon-wrapper {
        background-size: 100% !important;
    }
</style> <script src="/js/namespace.js"></script> <script src="/js/constants.js"></script> <script src="/js/utilities.js"></script> <script src="/js/article/alert-signup.js"></script> <script type='text/javascript'>
    var lTitle = "Recent advances in understanding object recognition...".replace("'", '');
    var linkedInUrl = "http://www.linkedin.com/shareArticle?url=https://f1000research.com/articles/9-590/v1" + "&title=" + encodeURIComponent(lTitle) + "&summary=" + encodeURIComponent('Read the article by ');

    var deliciousUrl = "https://del.icio.us/post?url=https://f1000research.com/articles/9-590/v1&title=" + encodeURIComponent(lTitle);

    var redditUrl = "http://reddit.com/submit?url=https://f1000research.com/articles/9-590/v1" + "&title=" + encodeURIComponent(lTitle);

            linkedInUrl += encodeURIComponent('Wardle SG and Baker CI');
    
    var offsetTop = /chrome/i.test( navigator.userAgent ) ? 4 : -10; 
    var addthis_config = {
            ui_offset_top: offsetTop,
                                    services_compact : "facebook,twitter,www.linkedin.com,www.mendeley.com,reddit.com",
            services_expanded : "facebook,twitter,www.linkedin.com,www.mendeley.com,reddit.com",
            services_custom : [
                {
                    name: "LinkedIn",
                    url:  linkedInUrl,
                    icon:"/img/icon/at_linkedin.svg"
                },
                {
                    name: "Mendeley",
                    url:  "http://www.mendeley.com/import/?url=https://f1000research.com/articles/9-590/v1/mendeley",
                    icon:"/img/icon/at_mendeley.svg"
                },
                {
                    name: "Reddit",
                    url:  redditUrl,
                    icon:"/img/icon/at_reddit.svg"
                },
            ]
        };


    var addthis_share = {
            url: "https://f1000research.com/articles/9-590",
            templates : {
                twitter : "Recent advances in understanding object recognition in the human.... Wardle SG and Baker CI, published by " + 
               "@F1000Research"
       + ", https://f1000research.com/articles/9-590/v1"
            }
        };

    if (typeof(addthis) != "undefined"){
        addthis.addEventListener('addthis.ready', checkCount);
        addthis.addEventListener('addthis.menu.share', checkCount);
    }

        $(".f1r-shares-twitter").attr("href", "https://twitter.com/intent/tweet?text=" + addthis_share.templates.twitter);
    $(".f1r-shares-facebook").attr("href", "https://www.facebook.com/sharer/sharer.php?u=" + addthis_share.url);
    $(".f1r-shares-linkedin").attr("href", addthis_config.services_custom[0].url);
    $(".f1r-shares-reddit").attr("href", addthis_config.services_custom[2].url);
    $(".f1r-shares-mendelay").attr("href", addthis_config.services_custom[1].url);

    function checkCount(){
        setTimeout(function(){
            $(".addthis_button_expanded").each(function(){
                var count = $(this).text();
                if (count !== "" && count != "0")
                    $(this).removeClass("is-hidden");
                else
                    $(this).addClass("is-hidden");
            });
        }, 1000);
    }
</script> <div id=citeReportModal role=dialog aria-labelledby=citeReportModal_title aria-describedby=citeReportModal_description> <div class="c-modal js-modal is-closed c-modal--large js-cite-report-modal "> <aside class="c-modal__content u-black--high o-box u-pb--0 u-black--high "> <div class=c-modal__close> <button type=button class="c-button c-button--icon c-button--medium c-button--full@hover js-modal__close"><i class="c-button--icon__icon material-icons">close</i></button> </div> <h1 id=citeReportModal_title class="c-modal__title t-h3 u-mt--0 u-mb--2 u-weight--md">How to cite this report</h1> <div id=citeReportModal_description class="c-modal__description js-modal__content t-h4 u-mt--0 u-mb--2 u-black--medium"> <div id="" class=js-report-citation-container>{{reportCitation}}</div> <div class="c-modal__extra-message js-modal__extra-message t-h4 u-black--medium"></div></div> <div class="c-modal__actions o-box__actions"> <a href="#" class="c-button c-button--full js-modal__close c-button--secondary">Cancel</a> <a href="#" title="Copy the current citation details to the clipboard." data-clipboard-target="#referee-report-citation" data-test-id=report_copy-citation_button class="c-button c-button--full js-modal__confirm c-button--primary js-clipboard c-mini-tooltip--above">Copy Citation Details</a> </div> </aside> </div> </div> <script src="/js/referee/new/referee_validators.js"></script> <script src="/js/referee/new/referee_helpers.js"></script> <script src="/js/referee/new/referee_checkbox-input.js"></script> <script src="/js/referee/new/referee_inline-editor.js"></script> <script type="text/javascript">
    $(function(){
        var gaCat = "F1000Research";
        if (gaCat === "") {
            gaCat = $("body").hasClass("wellcome-brand") ? "Wellcome Open Research" : "F1000Research";
        }
        GAHelper.track({category: gaCat, action: "Article Page: Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context", label: "pageviews"});
        GAHelper.track({category: gaCat, action: "Article Type: Review", label: "Article Page"});
        $(".f1r-article-desk .collection-image").each(function (idx, el) {
            var whatChannel = $(el).find("a").attr("href"),
                channelName = $.trim($(el).parent().find(".collection-detail a").text()),
                gaRef = "(ID: " + whatChannel.replace("/collections/", "") + ") " + channelName;
            GAHelper.track({category: 'ChannelStats', action: "Article Page: Recent advances in understanding object recognition in the human brain: deep neural networks, temporal dynamics, and context", label: gaRef});
        });
    });
</script> <script>
    $(function(){R.ui.buttonDropdowns('.dropdown-for-downloads');});
    $(function(){R.ui.toolbarDropdowns('.toolbar-dropdown-for-downloads');});
</script> <script src="/js/article/track_article.js" type="text/javascript"></script> <script type="text/javascript">
    $.get("/articles/acj/22296/24595")
</script> <script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script type="text/javascript" src="/js/app/messenger.js"></script> <script type="text/javascript" src="/js/article/article_mobiles.js"></script> <script src="/js/vendor/clipboard.min.js"></script> <script src="/js/shared_scripts/modal-dialogue.js"></script> <script src="/js/shared_scripts/clipboard.js"></script> <script src="/js/article/thesaurus-terms-display.js"></script> <script>
    new F1000.Clipboard();
    new F1000.ThesaurusTermsDisplay("faculty-reviews", "article", "24595");
</script> <script>
    $(document).ready(function() {
        $( "#frame1" ).on('load', function() {
            var mydiv = $(this).contents().find("div");
            var h     = mydiv.height();
            console.log(h)
        });

        
        var tooltipLivingFigure = jQuery(".interactive-living-figure-label .icon-more-info"),
            titleLivingFigure = tooltipLivingFigure.attr("title");
        tooltipLivingFigure.simpletip({
            fixed: true,
            position: ["-115", "30"],
            baseClass: 'small-tooltip',
            content:titleLivingFigure + "<div class='tooltip-arrow'></div>"
        });
        tooltipLivingFigure.removeAttr("title");

        $("body").on("click", ".cite-living-figure", function(e) {
            e.preventDefault();
            var ref = $(this).attr("data-ref");
            $(this).closest(".living-figure-list-container").find("#" + ref).fadeIn(200);
        });
        $("body").on("click", ".close-cite-living-figure", function(e) {
            e.preventDefault();
            $(this).closest(".popup-window-wrapper").fadeOut(200);
        });

                $(document).on("mouseup", function(e) {
            var metricsContainer = $(".article-metrics-popover-wrapper");
            if (!metricsContainer.is(e.target) && metricsContainer.has(e.target).length === 0) {
                $(".article-metrics-close-button").click();
            }
        });

        var articleId = $('#articleId').val();

        if($("#main-article-count-box").attachArticleMetrics) {
            $("#main-article-count-box").attachArticleMetrics(articleId, {
                articleMetricsView: true
            });
        }
    });

    var figshareWidget = $(".new_figshare_widget");
    if (figshareWidget.length > 0) {
        window.figshare.load("f1000", function(Widget) {
            // Select a tag/tags defined in your page. In this tag we will place the widget.
            _.map(figshareWidget, function(el){
                var widget = new Widget({
                    articleId: $(el).attr("figshare_articleId")
                    //height:300 // this is the height of the viewer part. [Default: 550]
                });
                widget.initialize(); // initialize the widget
                widget.mount(el); // mount it in a tag that's on your page
                // this will save the widget on the global scope for later use from
                // your JS scripts. This line is optional.
                //window.widget = widget;
            });
        });
    }
</script>

<script>
    $(document).ready(function () {

        
        var reportIds = {
                           "64604": 1,
                           "64605": 1,
                    };

        $(".referee-response-container,.js-referee-report").each(function(index, el) {
            var reportId = $(el).attr("data-reportid"),
                reportCount = reportIds[reportId] || 0;
            $(el).find(".comments-count-container,.js-referee-report-views").html(reportCount);
        });

        var uuidInput = $("#article_uuid"),
            oldUUId = uuidInput.val(),
            newUUId = "cb183b5a-411a-4a07-b8e1-06ff8f6c2a03";
        uuidInput.val(newUUId);

        $("a[href*='article_uuid=']").each(function(index, el) {
            var newHref = $(el).attr("href").replace(oldUUId, newUUId);
            $(el).attr("href", newHref);
        });

    });
</script>              </div>
        </div>

        
            
            <div class="o-page__footer sticky-email-wrapper">
                
                

                


<footer class="c-footer t-inverted">

    <div class="o-wrapper">
        <div class="o-layout">


                        
            <div class="o-layout__item u-mb--3">
                <div class="c-branding c-branding--research">
                    <img src="/img/research/F1000Research_white.svg" alt="F1000Research">
                </div>
            </div>


                        
            <div class="o-layout__item u-1/3@md u-mb--3">

                <span class="c-hr c-hr--thick c-hr--low u-mb--2"></span>

                <p class="t-h3 u-mt--0 u-mb--0">An innovative open access publishing platform offering rapid publication and open peer review, whilst supporting data deposition and sharing.</p>

            </div>


                        
            <div class="o-layout__item u-2/3@md">

                <span class="c-hr c-hr--thick c-hr--low u-mb--2"></span>

                <div class="o-layout">
                    <nav class="c-footer__nav">

                            <div class="o-layout__item u-3/5@sm u-mb--3">


                                <div class="o-columns o-columns--2">

                                                                            <a href="/browse/articles" class="t-body c-footer__nav-item "      >Browse</a>
                                                                            <a href="/gateways" class="t-body c-footer__nav-item "      >Gateways</a>
                                                                            <a href="/collections" class="t-body c-footer__nav-item "      >Collections</a>
                                                                            <a href="/about" class="t-body c-footer__nav-item "      >How it Works</a>
                                                                            <a href="https://blog.f1000.com/blogs/f1000research/" class="t-body c-footer__nav-item "      >Blog</a>
                                                                            <a href="/contact" class="t-body c-footer__nav-item "      >Contact</a>
                                                                            <a href="/developers" class="t-body c-footer__nav-item u-hide u-show@navbar"      >For Developers</a>
                                                                            <a href="/published/rss" class="t-body c-footer__nav-item "   title="RSS feed of published articles"     >RSS</a>
                                    
                                </div>

                            </div>

                            <div class="o-layout__item u-2/5@sm u-center u-right@sm u-mb--3">

                                <div class="u-hide u-show@lg">
                                    <div class="_mdl-layout">
                                        <a class="mdl-button mdl-js-button mdl-button--inverted mdl-button--no-shadow mdl-js-ripple-effect mdl-button--outline" href="/for-authors/publish-your-research"   data-test-id="footer_submit_research"  >Submit Your Research</a>
                                    </div>
                                </div>

                            </div>

                    </nav>
                </div>

            </div>

            <div class="o-layout__item u-mb--2">
                <div class="c-footer__share">
                        <div class="c-footer__share">
        <span class="c-footer__share-icon" title="Open Access">
            <span class="f1r-icon icon-100_open_access license-icon"></span>
        </span>

        <a class="c-footer__share-icon" href="//creativecommons.org/licenses" target="_blank" title="Creative Commons License CC-BY">
            <span class="f1r-icon icon-116_cc license-icon license-icon-cc"></span>
            <span class="f1r-icon icon-117_ccby license-icon license-icon-cc"></span>
        </a>

        <a class="c-footer__share-icon" href="//creativecommons.org/about/cc0" target="_blank" title="Creative Commons License CC0">
            <span class="f1r-icon icon-118_cco license-icon"></span>
        </a>

    </div>
                </div>
            </div>


                        
            <div class="o-layout__item u-1/3@md u-mb--3">

                <span class="c-hr c-hr--low u-mb--3"></span>

                <p class="c-footer__social u-mt--0 u-mb--0 u-white--low-med">Follow us
                    <a href="https://www.facebook.com/F1000" target="_blank" class="c-footer__social-icon f1r-icon icon-55_footer_facebook"></a>
                    <a href="https://twitter.com/#!/F1000Research" target="_blank" class="c-footer__social-icon f1r-icon icon-56_footer_twitter"></a>
                    <a href="http://www.youtube.com/user/F1000research" target="_blank" class="c-footer__social-icon f1r-icon icon-57_footer_youtube"></a></p>

            </div>


                        
            <div class="o-layout__item u-2/3@md u-right@md">

                <span class="c-hr c-hr--low u-mb--3"></span>

                <p class="t-caption u-white--low-med">&copy; 2012-2020 F1000 Research Ltd. ISSN 2046-1402 | <a href="/about/legal" class="copyrightLegal">Legal</a> | Partner of <a target="_blank" href="http://www.who.int/hinari/en/">HINARI</a>  &bull; <a target="_blank" href="http://crossref.org/">CrossRef</a> &bull; <a target="_blank" href="http://about.orcid.org/">ORCID</a> &bull; <a target="_blank" href="http://www.fairsharing.org">FAIRSharing</a></p>

            </div>
        </div>
    </div>

</footer>            </div>
        
    </div>

            <div class="js-cookie-spacer"></div>
        <div class="cookie-warning">
            <div class="instruction">The F1000Research website uses cookies. By continuing to browse the site, you are agreeing to our use of cookies. <a class="js-scroll-to" href="/about/legal/privacypolicy#use-of-cookies" data-scroll-target="#use-of-cookies">Find out more &raquo;</a></div>
            <div class="close-button"></div>
        </div>
    
    <script>
                    R.templateTests.simpleTemplate = R.template('<p class="$variable.one">$text</p><p class="${variable.two}">$text</p><p class="$!variable.three">$text</p><p class="$!{variable.four}">$text</p><p class="${selector}.five">$text</p>');
            R.templateTests.runTests();
        
        var F1000platform = new F1000.Platform({
            name: "f1000research",
            displayName: "F1000Research",
            hostName: "f1000research.com",
            id: "1",
            editorialEmail: "research@f1000.com",
            infoEmail: "info@f1000.com",
            usePmcStats: true
        });

                    $(function(){R.ui.dropdowns('.dropdown-for-authors, .dropdown-for-about, .dropdown-for-myresearch');});
            // $(function(){R.ui.dropdowns('.dropdown-for-referees');});

            $(document).ready(function () {
                if ($(".cookie-warning").is(":visible")) {
                    $(".sticky").css("margin-bottom", "35px");
                    $(".devices").addClass("devices-and-cookie-warning");
                }
                $(".cookie-warning .close-button").click(function (e) {
                    $(".devices").removeClass("devices-and-cookie-warning");
                    $(".sticky").css("margin-bottom", "0");
                });

                $("#tweeter-feed .tweet-message").each(function (i, message) {
                    var self = $(message);
                    self.html(linkify(self.html()));
                });

                $(".partner").on("mouseenter mouseleave", function() {
                    $(this).find(".gray-scale, .colour").toggleClass("is-hidden");
                });
            });
        
    </script>

            
<div class="sign-in-popup">
	<!-- <a href="#" class="sign-in shadow">Sign in <span class="sign-in-image-active"></span></a> -->
	<a href="#" class="sign-in ${locale}">Sign In <span class="arrow-closed sign-in-arrow-padding arrow-opened"></span></a>
	<div class="sign-in-form">

            <form action="https://f1000research.com/j_spring_oauth_security_check" id="googleOAuth" method="post"  target="_top" >
                                    <input class="target-field" type="hidden" name="target" value="/articles/9-590.html"/>
                            <input id="google-remember-me" name="_spring_security_oauth_remember_me" type="hidden" value="true"/>
        <input id="system-google" name="system" type="hidden" value="GOOGLE"/>
    </form>
            <form action="https://f1000research.com/j_spring_oauth_security_check" id="facebookOAuth" method="post"  target="_top" >
                                    <input class="target-field" type="hidden" name="target" value="/articles/9-590.html"/>
                            <input id="facebook-remember-me" name="_spring_security_oauth_remember_me" type="hidden" value="true"/>
        <input id="system-fb" name="system" type="hidden" value="FACEBOOK"/>
    </form>
            <form action="https://f1000research.com/j_spring_oauth_security_check" id="orcidOAuth" method="post"  target="_top" >
                                    <input class="target-field" type="hidden" name="target" value="/articles/9-590.html"/>
                            <input id="orcid-remember-me" name="_spring_security_oauth_remember_me" type="hidden" value="true"/>
        <input id="system-orcid" name="system" type="hidden" value="ORCID"/>
    </form>
		<form id="sign-in-form" class="login-container" action="https://f1000research.com/login" method="post" name="f">
           <div id="sign-in-form-gfb-popup"></div>

                                                            <input class="target-field" type="hidden" name="target" value="/articles/9-590.html"/>
                            			<input type="text" name="username" id="signin-email-box" class="sign-in-input" placeholder="Email address" autocomplete="email">
			<input type="password" name="password" id="signin-password-box" class="sign-in-input" placeholder="Password" autocomplete="current-password">
			<div class="sign-in-remember">
                <div class="checkbox-wrapper">
    				<input type="checkbox" id="remember-me" name="remember_me" class="checkbox is-hidden">
                </div>
                <span class="checkbox-label">Remember me</span>
			</div>
			<a href="#" class="sign-in-link" id="forgot-password-link">Forgotten your password?</a>
			<div class="sign-in-button-container margin-top margin-left-20 margin-bottom">
				<button type="submit" id="sign-in-button" class="sign-in-buttons general-white-orange-button">Sign In</button>
				<button type="button" id="sign-in-cancel" class="sign-in-buttons sign-in-cancel-button margin-left">Cancel</button>
				<div class="clearfix"></div>
			</div>
			<div class="sign-in-error">Email or password not correct. Please try again</div>
			<div class="sign-in-loading">Please wait...</div>
		</form>
		<div class="forgot-password-container">
			
<script type="text/javascript">
	$(function(){
		// Note: All the setup needs to run against a name attribute and *not* the id due the clonish
		// nature of facebox...
		$("a[id=googleSignInButton]").click(function(event){
            event.preventDefault();
            $("input[id=oAuthSystem]").val("GOOGLE");
            $("form[id=oAuthForm]").submit();
        });
        $("a[id=facebookSignInButton]").click(function(event){
            event.preventDefault();
            $("input[id=oAuthSystem]").val("FACEBOOK");
            $("form[id=oAuthForm]").submit();
        });
        $("a[id=orcidSignInButton]").click(function(event){
            event.preventDefault();
            $("input[id=oAuthSystem]").val("ORCID");
            $("form[id=oAuthForm]").submit();
        });
	});
</script>

<span class="text first">
	If you've forgotten your password, please enter your email address below and we'll send you instructions on how to reset your password.
    <p>The email address should be the one you originally registered with F1000.</p>
</span>
<input name="email" class="sign-in-input" id="email-forgot-password" type="text" placeholder="Email address">
<div class="forgot-password-email-error">
	Email address not valid, please try again
</div>
<div class="forgot-password-google-email-error">
    <p>You registered with F1000 via Google, so we cannot reset your password.</p>
	<p>To sign in, please click <a href="#" id="googleSignInButton">here</a>.</p>
    <p>If you still need help with your Google account password, please click <a href="https://www.google.com/accounts/recovery">here</a>.</p>
</div>
<div class="forgot-password-facebook-email-error">
    <p>You registered with F1000 via Facebook, so we cannot reset your password.</p>
    <p>To sign in, please click <a href="#" id="facebookSignInButton">here</a>.</p>
	<p>If you still need help with your Facebook account password, please click <a href="https://www.facebook.com/recover/initiate">here</a>.</p>
</div>
<div class="clearfix"></div>
<div class="forgot-password-captcha-error">
	Code not correct, please try again
</div>
<div class="clearfix"></div>
<div class="sign-in-button-container margin-left-20 margin-bottom">
	<button type="button" id="sign-in-reset-password" class="sign-in-buttons general-white-orange-button">Reset password</button>
	<button type="button" id="forgot-password-cancel" class="sign-in-buttons sign-in-cancel-button margin-left">Cancel</button>
	<div class="clearfix"></div>
</div>
<span class="text last">
	<a href="mailto:">Email us</a> for further assistance.
</span>
<form action="https://f1000research.com/j_spring_oauth_security_check" id="oAuthForm" method="post" target="_top">
                        <input class="target-field" type="hidden" name="target" value="/articles/9-590.html"/>
                <input id="oAuthSystem" name="system" type="hidden"/>
</form>
			<div class="forgot-password-server-error">Server error, please try again.</div>
			<div class="sign-in-success">
                <p>We have sent an email to <span id="email-value"></span>, please follow the instructions to reset your password.</p>
                <p>If you don't receive this email, please check your spam filters and/or contact .</p>
            </div>
			<div class="sign-in-loading">Please wait...</div>
		</div>

		<div class="sign-in-form-register-section">
			<div class="sign-in-button-container margin-left-20 margin-bottom">
				<a href="/register" title="Register"><button type="button" id="sign-in-register-button" class="sign-in-buttons general-white-orange-button">Register</button></a>
				<div class="clearfix"></div>
			</div>
		</div>

	</div>
</div>

<script type="text/javascript">
$(document).ready(function () {

    signIn.createSignInAsRow($("#sign-in-form-gfb-popup"));

    $(".target-field").each(function () {
        var uris = $(this).val().split("/");
        if (uris.pop() === "login") {
        	$(this).val(uris.toString().replace(",","/"));
        }
    });
});
</script>
        <div id="templateOverlay" class="is-hidden" hidden="hidden">
  <div class="o-overlay js-overlay is-hidden" hidden="hidden"></div>
</div>

<div id="templateExternalMessages" class="is-hidden" hidden="hidden">
  <div class="o-modal o-modal--auto@md js-external-messages is-hidden" hidden="hidden">
    <div class="o-modal__body">
      <section class="c-console">
        <div class="_mdl-layout c-console__bdy js-external-messages-body"></div>
        <footer class="_mdl-layout c-console__ftr o-flex o-flex--reverse js-external-messages-footer">
          <button type="button" class="mdl-button mdl-js-button mdl-button--raised mdl-button--colored c-console__btn js-external-messages-close" data-action="maintenance-close">I Understand</button>
        </footer>
      </section>
    </div>
  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.1/moment.min.js"></script>

<script src="/js/namespace.js"></script>
<script src="/js/constants.js"></script>
<script src="/js/utilities.js"></script>

<script>
                  F1000.ExtenalMaintenanceItems = [
    {
      start: '2018-12-10T14:21:00Z',
      end: '2018-12-13T16:00:00Z',
      msg: 'This site will be down for a short time on XX December. It is advisable not to start any submissions on that day or you may lose your work unless you save regularly.',
      cookieName: 'outage23122018',
      editor: false,
    }
  ];
</script>

<script src="/js/shared_scripts/cookie-helper.js"></script>
<script src="/js/shared_scripts/mdl-helper.js"></script>

<script src="/js/app/external-maintenance.js"></script>

                <script type="text/javascript">
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-5646075-11', 'auto');
            ga('require', 'displayfeatures');
            ga('send', 'pageview');
        </script>
        
                <script type="text/javascript" src="/js/app/research.analytics.js"></script>

        <!-- Start of HubSpot Embed Code -->
        <script type="text/javascript" id="hs-script-loader" async defer src="//js.hs-scripts.com/4475190.js"></script>
        <!-- End of HubSpot Embed Code -->
    
            <script src="https://my.hellobar.com/4e0495c6f18cbd68731a1dc1978195a144e767ba.js" type="text/javascript" charset="utf-8" async="async"></script>
    </body>

</html>