Thank you for the opportunity to review this interesting manuscript. The authors describe an evaluation of the predictive validity of the Developmental and Well-being Assessment (DAWBA) for ASD in a complex clinical sample. They conclude that, compared to the gold standard multi-disciplinary team assessment, the DAWBA had high sensitivity but low specificity. Strengths of the study include a very clear and transparent methodology and a very clear diagnostic process. I do however have several concerns with the way that the data has been analysed and reported. Looking at the results, particularly those shown in table 7, it is clear that whilst the authors comments on the sensitivity and specificity of the DAWBA assessment are clearly supported by the date, there are other interesting findings that have not been discussed. For example when compared to MDT assessment although the DAWBA has the poorest specificty, which is very important, it had a better classification accuracy that either the ADI-Algorithm for either social reciprocity and communication or the ADI when repetetive behaviour was added to this. Does this mean that if the DAWBA was substituted for the ADI (actually the 3 di - see below) in the MDT assessment there would be no loss of accuracy and perhaps an improvement for this type of complex case? In this respect Table 5 is interesting as it suggests that the ADI algorithm scores did not differentiate between ASD and no ASD in this sample. I guess that this may be due to the complex nature and previous diagnostic uncertainty around these cases but this is not a line of reasoning that has been explored here. I therefore think that the authors could have addressed this data in a broader way rather than just focusing on the DAWBA aspects. It would have been interesting to see the 2x2 table for the ADI findings and the ADOS also so that we could have explored this further. The negative comments about success of the DAWBA to discriminate ASD made in the discussion could equally be applied to the 3DI/ADI data. I know this is mentioned in the discussion but I think there could be more balance to this. I also found table 8 confusing. At first look it seems similar to table 7 but in fact it is 'the other way round' I think showing that the consensus diagnosis behaved similarly when compared to the separate ASD specific assessments as it did compared to the MDT diagnosis. Whilst on tables I think that tables 2 and 3 could probably be combined as could tables 4 and 5. I also found the data in the first row of table 9 confusing as it looks like there were 11 true positives and 3 false positives (but perhaps an reading incorrectly in which case the table is confusing). If this is so I cant quite see how only 42% are correctly classified and how sensitivity is so low. Whilst there is a lot of data presented I missed not seeing the confidence intervals around the Sn, Sp, PPV, NPV. I was not clear why the 3 DI results were translated into ADI and not reported for what they are. I understand that the process has been validated but any transformation like this will increase error and reduce accuracy. This may be part of the explanation why the ADI did so poorly as discussed above Similarly the potential parental biases discussed for the DAWBA in the discussion could equally apply to the ADI. The authors spend quite some time in the introduction pointing out that a potential benefit of the DAWBA is that it assesses a broad range of mental health disorders and symptoms. Indeed this is a criticism of many clinical assessments for ASD that start and finish with the question of whether someone does or does not have ASD and fails to make the comprehensive assessment that identifies either comorbid problems or alternative diagnoses. I was looking forward to reading about how the DAWBA performed in this respect and whether it allowed a more complete assessment. But there is not mention of this past the introduction which was disappointing for me at least. Was this data gathered and if so could it be presented? Minor issues There are quite a lot of grammatical and stylistic issues that need to be addressed. It is true that the sample are different to a regular population and many clinical samples but they are certainly not (at least as a group) unique. If they were there would be no reason to publish as the data and findings would not inform any other service providers. As it is there are many other specialist clinics who see similarly complex cases who will benefit from these findings we are told in the discussion that a high proportion of the consensus diagnoses were based on free text responses. this has not been mentioned before. can it be quantified? In the introduction is the prevalence of 1.3% for ASD in the UK an epidemiological prevalence or the current rate of diagnosis (administrative prevalence)? For me the discussion is too long and describes other studies in too much detail for a paper. I think it could be edited down considerably to make sure it addresses the main issues in a more focused clear and concise manner. 