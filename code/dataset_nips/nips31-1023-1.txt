The work focuses o duplicate removal in object detection frameworks. It proposes a two-stage framework to effectively select candidates for objects: first easy negative objects proposals are eliminated and then true positives are selected from the reduced set of candidates. The two stages are based on an encoder and a decoder implemented as a recurrent neural network that uses global contextual information. The authors carried out extensive experiments showing how the approach outperforms other proposals on a benchmark dataset.   The paper is well written and technically sound to the extent I checked (this is not my area of expertise). I liked the idea of a two-stage process in order to incorporate contextual knowledge and being more efficiently in the selection.  Particular comments: - In section 3 there is no reference to the data set used for the preliminary empirical study. Is it the same as used later  for performance?  - "This is because that within any single" --> remove "that" 