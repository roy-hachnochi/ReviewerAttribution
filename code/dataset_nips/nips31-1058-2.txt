This paper proposes an interesting method to asymptotically achieve a better posterior predictive to the true model. The method itself can be seen as a intermediate between standard Bayesian inference and Bayesian bootstrap. The strategy is to integrate a Dirichlet process to the model, where likelihood shows up as the base measure. By tuning the total mass of the mean measure, we are able to control the 'objective level' of the entire inference. Even though not drawing from the true posterior, the method is asymptotically more efficient and it can parallel.  I think the overall contribution of the paper is pretty good. But the writing could improve. For example [S1], the method is not solving the model misspecification problem, and the best it can achieve is still to find an optimal $f_\theta\in\mathcal{F}_0$ that minimize its KL divergence towards the true model. For [S2], since the authors do not show any non-asymptotic result about the gap between the posterior they derive and the true Bayesian posterior, I cannot quantity how good they 'approximate' the true posterior. For [S4], I don't think that is a main focus of the paper.  The paper proposes a nice algorithm, but my concern is that sampling a DP posterior can be expensive. So I'm not sure whether the overall computational complexity is affordable.   Overall this is a nice paper that balance a subjective and an objective view of Bayesian inference via a mixture of Dirichlet processes. My only concern is that the claimed points in the paper are not fully developed.