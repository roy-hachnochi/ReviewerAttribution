The paper presents a modification of a previous baseline method in the application area of inductive program synthesis (IPS). This new architecture, PCCoder, shows some considerable improvement over DeepCoder, the baseline main method it compared against. Within the setting of IPS, the goal of the model is, provided with some input/output examples meant to represent a partial program specification, output a program that successfully maps from input to output for each I/O example in the set. A program is considered correct if it accomplishes the mapping on all provided I/O pairs (there is no notion of generalization to unseen I/O pairs as sometimes done in previous work, especially in string processing domains). The input/output pairs consist of integers, and the program is constructed from the relatively expressive Domain Specific Language defined in the DeepCoder paper. The model maintains a program state as it iteratively chooses which functions to execute on the input examples, and this state contains a set of variables that can be used to store partial results. In contrast to DeepCoder, PCCoder additionally has a garbage collector that can delete unused variables.  The paper is very clearly written and easy to follow. I found that most details of the architecture and the method used to train it can be clearly understood from the text. They also released their code with the paper, so that their specific implementation details are provided.  The architecture itself is closely related to DeepCoder, but contains some noticeable differences in activations used (SeLU v.s. sigmoid in I/O embedding function) and using a deep network (10-layer) of dense blocks to process the program state, among other differences. It also includes a novel garbage collection operation that allows variable deletion (potentially enabling a much larger class of programs to be generated) and used a beam-search-like method called CAB in place of the DFS, enumeration, sketch and lambda^2 search techniques used in DeepCoder.   The experimental results section shows some considerable improvement over the DeepCoder baseline, including on program lengths much longer than originally reported in the DeepCoder paper (up to 14, while DeepCoder only evaluated on program lengths up to 5). The results seem impressive, with a 2-3 order of magnitude speedup of DeepCoder shown in Table 2. Due to the lack of an open-source implementation of DeepCoder, the authors had to reimplement it but obtained slightly worse results due to less optimized code. Despite the poorer performance, they do provide results that compare their reimplementation with the results in the original paper side-by-side in Table 1. A relatively comprehensive ablation analysis reveals that no single architectural change from DeepCoder is responsible for all the improvement in performance. An interesting thing to observe would be to run DeepCoder with the improved search method of CAB to see if it provides any significant improvement (since in the ablation analysis, using DFS with PCCoder caused a large decrease in performance).  In summary, the paper is well-written and the results seem to show a clear improvement over a previous deep-network-based IPS baseline. Some weaknesses are that the paperâ€™s results are largely engineering-related, as the architecture itself does not contain many novel components but combines previous work (dense layers, SeLU, CAB) into a higher performing model. Despite this, the results show a definite improvement over the baseline and extend deep IPS models to work on longer program lengths. 