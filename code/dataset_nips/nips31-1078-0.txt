Overview: The paper revisits the notion of variance regularization, where instead of a renormalization layer, a loss term is added to prevent fluctuations in batch statistics across minibatches. The authors present numerical results, connect the idea to BatchNormalization, and show theoretical properties of their approach in restricted cases.   Strengths: The problem being studied is of great importance to the community. Renormalization strategies such as BatchNorm or LayerNom have shown to greatly speed up training of deep networks but are poorly understood theoretically. Further, in many cases such as low mini-batch, GAN training, reinforcement learning, these strategies might do more harm than good and knowing that a-priori is difficult if not impossible. The authors propose a loss-based regularization instead of architectural changes and share interesting theoretical insights about this approach. The numerical results presented are comprehensive and I especially liked the UCI experiments. As means to aggregate information in such experiments, I suggest that the authors look at Dolan-More profiles which are ubiquitous in the nonlinear optimization community.   Weaknesses: - What is the time comparison of VCL relative to BatchNorm and having no normalization? - The argument tying modes, BatchNorm, and VCL could be better explained. It seems that the observations about modes and normalization outcome is new but the authors don't describe it sufficiently.  - I recommend that the authors format their mathematical equations better. For instance, Equations (4), (14), (18), and others, would be easier to parse if the bracketing and indexing were fixed.  - Line 177 typo: "batchsized" - It would aid a reader if the authors summarized the loss and how it is computed at the end of Section 2.2.  - How sensitive is the framework to the choice of n? - How does \beta vary over time? Could the authors include a graph for this in the Appendix?  Questions: - Will the authors be open-sourcing the code for the experiments?  - Have you experimented with a constant \beta?  - Have you experimented with having _both_ BN and VCL?   Post-rebuttal  I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal. 