The paper presents a novel reinforcement adversarial learning approach for image enhancement. The method parameterizes image enhancement operations in local regions that are analyzed separately and then merged together to produce the final version. The method learns to apply local enhancements as part of a decision process optimized with reinforcement learning. In order to provide feedback to the enhancement agent, the method learns a reward function in an adversarial setting: images enhanced by the agent are discriminated from images enhanced by expert photographers. Since the image generator is a non-differentiable function, the policy network and the discriminator are trained separately in an alternating fashion using asynchronous updates and unpaired data samples, respectively. The overall strategy is very interesting, the formulation of the solution is sound, and the results show consistent improvements over baseline methods.  Comments: * The organization of the paper is good, and the flow is generally clear. However, writing style and grammar needs to be improved in general throughout the manuscript. * In Table 1, it is not clear which method is Ours 1 and Ours 2. The main text does not seem to clarify either. Please explain. * Equations 7 to 9 seem to be the standard equations for DPG. Are these details necessary in the discussion? Maybe the space can be used to discuss more results or information specific to this paper. * Line 93: s_{t+1} = a_t o s_t = p(a_t, s_t). The function $p$ has the parameters in the opposite order wrt equations 5 and 6.