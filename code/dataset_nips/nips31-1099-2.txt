The paper presents three pieces of results regarding normalization techniques: 1) weight decay has an effect similar to modulating the learning rate; 2) L1 normalization works with low precision and achieves performance comparable to L2 batchnorm in high-precision settings; 3) fixing the scale of weight norm improves performance on large-scale data.  The paper presents positive results on the performance of L1 normalization under low-precision setting, which might have potential impact. The experiments in Section 3 appear to be interesting and might potentially inspire future work. However, I have the following concerns.  First, the L1 normalization experiments are weak. Under the high-precision setting, the authors claim that L1 normalization is faster than the standard L2 batchnorm, but no results regarding computational time have been reported. Under the low-precision setting, L1 is shown to have much better convergence compared to L2 normalization. However, the experimental setting is not documented (e.g., the dataset, model, hyperparameters). It is unclear whether L2 BN could work by tuning related hyperparams such as the batch size and the learning rate. It is also not clear what baseline is presented in Figure 3 and how it is compared to state-of-the-art performance. More importantly, all claims made in Section 4 are only supported by at most one experiment on one dataset without comparison to any state-of-the-art results, which poses questions on the generalization of these phenomena.  Second, Section 5 is below the standard of NIPS both in terms of novelty and significance. Fixing the scale of weight norm is pretty straightforward, and it is worse than the batchnorm.  Third, the paper is loosely organized. The three pieces of analysis and results seem unrelated. The authors tempt to glue them by telling a story of viewing batchnorm as decoupling weight scales and directions, but I don't think weight decay and weight norm should really be relevant. I would suggest that in the later version of the paper, the authors should focus on the L1 normalization part of the paper and improve the experiments by considering a much more diverse setting.  Update: The authors' response addressed some of my concerns. I still feel that the scope of the experiments is somewhat limited, but I agree with the other reviewers that the paper brings insight and has potential. And thus I raised my score to vote for acceptance.