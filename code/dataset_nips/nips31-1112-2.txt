This paper proposes a new variation of the matrix completion problem, called mixture matrix completion (MMC), where each entry of the matrix is drawn from one of a few low-rank matrices, rather than the same low-rank matrix. The proposed problem seems to be valid with motivations from a few applications. This paper makes two contributions: 1) an information-theoretical lower bound on the sample complexity; and 2) a heuristic algorithm to solve the MMC problem based on alternating minimization. The paper is written clearly with sufficient backgrounds information and provides extensive numerical experiments.    The theoretical results of the paper are rather weak. The info-theoretical bound is straightforward and directly follows from previous studies on the matrix completion problem in [3], and [4] combined with a combinatorial enumeration.  -Moreover, the statement of Theorem 1 is a bit hard to follow, and in some parts the meaning is unclear. For example, it is not clear how one can use Theorem 1 to verify if a given pattern can be used to solve the MMC problem in a computational-efficient way. Does "it is possible to" mean there exist an algorithm to recover ..?  -In Theorem 2, it requires the number of columns needs to be about r times larger than the number of rows, which is a strong assumption. For example, this eliminates the applicability of this result on square matrices. Is this requirement always needed?   For the alternating algorithm (AMMC), the main issues are 1) how to select the initialization in a data-driven manner or adaptively; I haven't found any discussions on it; 2) an analysis of the computational complexity of the proposed AMMC algorithm. A major issue in the real-data experiment is that the AMMC algorithm uses down-sampled data, and for such, many details in the images for the segmentation experiments are lost. For example, the background trees may be quite smoothed after down sampling, and much easier to separate. Therefore, the performance improvement showed in Figure 3 may not come directly from the new algorithm but an artifact of downsampling. In Figure 5 of the supplementary material, row 3 and row 7, there show more people than appeared in the original frame, can you explain why?  In summary, the paper proposed an interesting problem (MMC) to study, but the results are rather immature to be published in its current form.  Small typos: -line 168, the word "entries" appeared twice -line 170, the word "and" appeared twice  update: I have read the authors' rebuttals and below are updates of my review. First, thanks for the authors' clarifications of many aspects of the work which helped my understanding. My main concerns are: - clarity as well as the novelty of the theory: The theorems of this paper are built heavily on existing results in [3] and [4] and the additional arguments appear to me as incremental. Furthermore, the applicability of Theorem 2 to square matrices is still unclear form the rebuttal; the authors claim it is applicable but it is not clear how since there is an assumption in Theorem 2 that explicitly prevents it from being applied. It seems a lot of work are needed to make the statements of the theorems clear (I do appreciate the authors' efforts in the rebuttal to make them more clear than the submitted version already); - the authors' acknowledged the unfairness in the comparison between RPCA with full data and their algorithm with subsampled data and updated the simulations. I am not sure why the tree in the backgrounds, while stay blurred in the reconstruction of the backgrounds, its residual didn't show up in the foreground in the authors' algorithm.   