In this paper, the authors propose a TrajectoryNet for action recognition in videos.  With the guidance of optical flow, a trajectory convolution operation is designed to learn the dynamics along the motion paths.   *Positive:  The idea is interesting. In practice, the feature maps across frames are not well aligned. Hence, Learning temporal dynamics along the motion paths seems to be a preferable design, compared to the standard temporal convolution.   *Negative: (1) Model Design # The trajectory is generated from optical flow. This would require either the high computation (e.g., TV-L1) or extra model training (e.g., MotionNet). Both cases reduce the conciseness and practicability of 3D CNN for action recognition in videos.  # Since the approach requires optical flow anyway, have you tried trajectory convolution on optical-flow-stream 3D CNN? (2) Experiments # In Table 2, how to generate motion trajectories? TV-L1 or MotionNet? When using TV-L1, the pre-computation burden of optical flow is ignored. When using MotionNet, the parameters of this model is ignored ([48]: Tiny-Motionnet / Motionnet is 8M / 170M).  Furthermore, two-stream S3D achieved the best result with a large margin. In this case, it is controversial to exchange high accuracy for model complexity, especially under the same condition (i.e., both two-stream S3D and TrajectoryNet requires optical flow). # In Table 1 and Table 6, the influence of trajectory convolution seems to be small. Especially in Table 6,  most of state-of-the-art approaches do not require the guidance of optical flow.