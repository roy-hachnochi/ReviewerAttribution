The paper applies the two-part description approach from the minimum description length principle (or the Kolmogorov structure function) to understand how to find a good "compressed neural network model" for given data. This approach provides an apparently more principled way to compressing a large number of parameters in a deep neural network, which has lots of practical interests.  Although the theoretical development is somewhat shallow (and sometimes misleading), the overall idea is quite sound, and in the end, the proposed prequential description approach provides an impressive performance. The overall flow is somewhat anecdotal and sometimes confusing (and certainly not the most straightforward path to the conclusion), but the storyline is still interesting.  Although this point is not clearly stated (which the authors should address in the revised manuscript), the paper considers the problem of finding an efficient description of the output sequence of a given input sequence, when the pair is related through a given model (a deep neural network). One sufficient method of such a description is a two-part method -- first, describe the model and then compress the output sequence according to the model (conditioned on the input). This is at best a heuristic method, and it is not surprising that the method in itself performs poorly in practice.  As a more average-case alternative, a variational approach, once again can be decomposed into two parts of descriptions, solves the efficient two-part description for random data. Although it provides a significantly better compression performance, the accuracy of the description is poor.  As a third alternative, the paper develops a two-part description based on blockwise prequential model selection, which achieves both high compression efficiency and test accuracy.  Although there is not much of a theoretical justification (who knows why two-part description is the right way -- or even an answer to the problem of describing the output given the input), the experimental result is quite remarkable and brings lots of discussions on the interpretations of what's going on. In this sense, we'd better err on accepting a half-baked idea, instead of waiting for a fully grown theoretical development, which might never occur.  The literature citation is somewhat kinky throughout, often omitting key papers and promoting subsequent, secondary sources. For example, how come Rissanen's papers are omitted, yet Grunwald's monograph appears in the abstract? Similarly, Kolmogorov and Chaitin should receive the same spotlight as Solomonoff.