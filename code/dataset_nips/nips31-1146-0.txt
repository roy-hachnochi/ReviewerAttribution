Update: I think the authors did a good job in the rebuttal, and I appreciate the nice work. -----------------------------  This paper addresses the problem of mode collapsed in GAN, and proposes a theoretically grounded approach to resolve the problem based on metric embedding. The basic is the define the latent space as a mixture of Gaussian to address the difficult of mapping a single mode distribution such as Gaussian to a multi-mode data distribution. The proposed Gaussian mixture works like kernel density estimation, which takes a set of predefined latent representations obtained from metric embedding of the original data as the mean of Gaussians.  Quality: The paper is of high quality in the way that it relates the proposed method with theory.  Clarity: The paper is well written in general, with the proposed method clearly written.  Originality: The paper is original in my opinion.  Significance: The paper is significant in the sense that it address an important problem of GAN: how to deal with mode collapsed.  I also have some concerns. It seems to me that the theory provides guarantees on the approximation, but not on how good mode collapse could be addressed. It would be great to guarantee to what extent the modes could be recovered by the generator, though I admit it would be very challenging.  In the experiments, though the proposed method seems to recover more modes, I wonder if the quality of the generated images is as good as other methods. This might be done by comparing some scores such as the inception score. Have the authors done this?