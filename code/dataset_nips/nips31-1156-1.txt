Summary  The paper investigates two classification/regression rules that do not make any error on the training set, but for which certain guarantees about their generalization performance can still be established. While the first rule is only of  theoretical interest illustrating possible effects, the second may also be of practical interest as it basically is a weighted  nearest neighbor rule.    Impression   The results are certainly very interesting and as such deserve being accepted as they try to address an observation which has recently been made eg for neural networks: good generalization despite zero training error in the presence of noisy labels. However, the introduction is maybe a bit overselling, and the  discussion on related work on page two is misleading since there mostly generalization bounds, that is, bounds that relate  training to test error with high probability, are discussed,  while the paper actually establishes bounds of a rather different  flavor, see e.g. Theorem 3.2.