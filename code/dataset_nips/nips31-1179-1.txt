This paper proposes a communication efficient distributed approximation to the Newton method called GIANT). The key idea is to approximate the global Hessian matrix by the local Hessian matrix in the worker nodes and to send the approximate Newton direction instead of the local Hessian matrix itself from the worker nodes to the driver to reduce the communication cost. For quadratic objective functions, the number of iterations of GIANT, and hence the communication complexity, is logarithmically dependent on the condition number of the Hessian matrix.  The given analysis not only shows that GIANT is overall more communication efficient than state-of-the-art distributed methods but even converges in fewer iterations. This is also confirmed experimentally in a wide variety of settings. Also, in contrast to existing methods, GIANT has only one tuning parameter.  Clearly, the topic of the paper is of high interest to the NIPS community. To the best of my knowledge, this is the first work to propose or at least to carefully analyze the properties of sending only approximate Newton directions. Also, the paper is well written and organized (One minor point: theorem numberings in Appendix do not have one-one correspondence to the theorems in the main paper). Weak points are that the experimental evaluation only uses two datasets. Increasing this seems particularly relevant in order to demonstrate the practicality of the technical assumption of incoherence. Also, the implementation seems to be not publicly available, which hinders reproducibility.