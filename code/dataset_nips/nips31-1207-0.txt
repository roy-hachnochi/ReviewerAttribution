This paper proposes a method of improving upon existing meta-learning approaches by augmenting the training with a GAN setup. The basic idea has been explored in the context of semi-supervised learning:   add an additional class to the classifier's outputs and train the classifier/discriminator to classify generated data as this additional fake class. This paper extends the reasoning for why it might work for semi supervised learning to why is might work for few-shot meta learning.   The clarity of this paper could be greatly improved. They are presenting many different variants of few-shot learning in supervised and semi-supervised setting, and the notation is a bit tricky to follow initially. The idea proposed in this paper is a very simple one, yet the paper was a bit laborious to read.  Overall, this paper on the border of acceptance.  The idea is simple, and although it has been used in other contexts, its application in this area is novel. However, since the basic ideas in this paper a not new, and rather just a combination of existing work, I would hope for a bit more empirical evaluations (for example, in the RL setting as MAML is applied). That said, the method is simple and improves upon existing approaches and so I am (very marginally) on the positive side of acceptance boundary, provided the paper could edited for clarity.   Detailed comments: - The notation in "few shot learning section" is a bit hard to read. For example, the support set is given as S_T = S^s_T U S^u_T and then S^s_T is described as the supervised support set, so I would assume S^u_T is the unsupervised support set. But then U_T is defined as the unlabelled support set , so what is S^u_T? Also, what do the *'s on line 71 mean? Why are some of the Y's *-ed and others not? In general, this paragraph could use some editing.  - In table 2, why do you only apply the metaGAN framework to MAML with 1 step, first order? Why not the other versions, which are better than that one? - how significant are the omniglot improvements. Most gains look to be within error bars. - could the metaGAN approach be combed with the k-means method in table 3 in order to improve upon it?  