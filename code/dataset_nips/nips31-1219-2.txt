Summary: This paper introduces a gradient-based meta-learning method that learns to tune the hyper-parameters for TD ($\lambda$) with discount factor $\gamma$ and bootstrapping $\lambda$ in order to optimize the estimation of return.  The meta-learning algorithm is online and able to adapt to the non-stationarity of return function from interacting with the environment. Extensive results on Atari games demonstrate the effectiveness of the proposed meta-learning algorithm. This paper looks very interesting and I vote for the acceptance.   Quality: This work is of high quality. This meta-RL is based on online cross-validation: training with update rule function and evaluating the performance of the updated new parameter with successive experience samples alternatively. This paper introduces the the hyper-parameter of return function $\eta$ as the meta-parameter, incorporating it into update rule function $f(\tau, \theta, \eta)$. The update rule function $f$ is differential and is used to update the parameter of interest $\theta$, e.g. for value function or policy. Therefore, we can learn the new parameter $\theta'$ by the update rule. On the other hand, this paper introduces the evaluation function $J'(\theta', \tau', \eta')$ to measure the performance of new updated parameter $\theta'$ on an independent sample $\tau'$ - this evaluation is denoted as meta-objective. This meta-objective is further used to update the meta-parameter $\eta$ in the descent direction of the meta-gradient. The trick here is that to find a reasonable return proxy for the performance evaluation, we still need to set the hyper-parameter $\eta' = \{\gamma', \lambda' \}'$ manually, which is sensitive to the return function. Furthermore, additional hyper-parameters such as the learning rate is introduced to learn the meta-parameter.  To make the value function and policy adaptive to the non-stationary meta-parameter throughout the training process, the authors propose to argument the meta-parameter as the input 'state' of the value function and policy.    Clarity: The paper is generally well-written and structured clearly and I enjoyed reading it. This paper is easy to understand even for those with only a basic background in machine learning.  One minor comment is for Eq. (12) between line 128 and 129: it might be better to provide the objective function $J(\tau, \theta, \eta)$ for a clear understanding of the gradient w.r.t. $\theta$ (Eq.12).  Originality: The idea of meta-RL, or more specifically, gradient-based meta-learning adapting to the environment, is natural and no longer new. The authors cite the relevant papers, e.g. Finn et. al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (ICML 17), Al-Shedivat et. al. Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments (ICLR 18). Recently, Xu et. al. Learning to Explore via Meta-Policy Gradient (ICML 18) is a new, gradient-based, meta-RL method that learns to do exploration. However, adopting the meta-RL method to learn the return function adaptively is certainly novel and interesting.   Significance: It is important to estimate and optimize the non-stationary return function in Reinforcement Learning by trading-off the bias and variance. This paper provides an interesting perspective from meta-RL and is able to adjust the sensitive hyper-parameters automatically to achieve better results.  -------- After Rebuttal ------- After reading others' reviews, discussions and authors' feedback, I am a little bit concerned about the approximation ($\mu=0$) in Eq.4. Very curious to see what is the $\mu$'s impact empirically and what is the reasoning? Is $\mu$ sensitive to the performance? 