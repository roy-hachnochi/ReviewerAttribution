Summary: This paper studies the problem of absence of spurious local optimality for a class of non-convex optimization problems. Building on nice properties of the so-called global functions, the authors show that a class of non-convex and non-smooth optimization problems arising in tensor decomposition applications have no spurious local minimum.  Comments: The paper discusses several properties of global functions, which are nice. Here I have some comments on the paper: 1. Although in the introduction the authors mention matrix completion and sensing as the motivation of the paper, it seems that the main result of this paper, i.e., Proposition 1.1, does not really deal with the missing data related problems. Indeed, the optimization problems in Proposition 1.1 are an extension of low-rank approximation to the tensor format with non-smooth objective, but requiring that the decomposition has the right rank as the given matrix and the given matrix is fully observed. This indeed is less interesting comparing with missing data cases, having fewer applications in reality.  2. The paper claims that the l1 norm can handle outlier. But dosen't problem (8) has the same global solution as the quadratic loss? --- 0 will be the optimal loss for both problems. So what is the advantage of studying l1 norm loss compared with studying quadratic loss, as all previous works have done? ================ I read the authors' rebuttal and I think the authors agree my concern in the first round of review. Please address them in the further revision of this paper.