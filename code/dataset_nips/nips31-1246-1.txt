Batch normalization accelerates the training process of deep networks and has become de facto standard. Despite the widespread popularity, we have little evidence of where such improvement comes from. This paper explores the roots of success for batch normalization, which was believed to the reduction of internal covariate shift phenomenon, as suggested by the original batch normalization paper. This paper argues it is not because batch norm reduces internal covariate shift. Instead, it is because batch norm smoothens the loss gradient landscape such that gradient is more predictable even with large step sizes.   My main question is regarding Section 2.2 “Is BatchNorm reducing internal covariate shift?”. I was a bit confused about the new definition of internal covariate shift. In the original paper, by internal covariate shift, they refer to the changes of each layer’s input distribution. In this sense, the reduction of internal covariate shift appears to be one of the simple consequences of batch norm’s mechanics. But when we adopt the new definition, the internal covariate shift goes up as we optimize. I can see the new definition being more convenient from an optimization perspective, but perhaps it is a bit strange to conclude batch norm does not reduce internal covariate shift at all from empirical results based on a different definition. Despite so, the results based on the new definition are still interesting to think about.   As suggested in the end, the smoothening effect of batch norm could encourage the training process to converge to more flat minima. I am curious if other normalization strategies, which aim at the same smoothening effect, also tend to improve generalization as well.   A minor concern:  Activation histograms in Figure 2 and Figure 7: It is a bit hard for me to interpret such histograms without the explanation of what each axis means.  