The author propose an algorithm for model interpretability which doesn't have the typical sidesteps the typical accuracy-interpretability trade-offs. It focuses on example-based and local explanations but is also able detect global patterns and diagnose limitations in its local explanations.  The authors provide a clear overview of the different types of explanation and related work as well as random forests and their use for explainability (SILO and DSTump).  SLIM (the author's model) uses a local linear model, where the coefficients determine the estimated local effect of each feature. For features with 0 coefficient, the authors determine the efficacy of a local explanation to understand if the feature is important for global patterns. They also propose a method to pick exemplar explanations using the local training distribution.  The results are evaluated on multiple datasets and compared to state of the art algorithms for both accuracy and explainability, both as a predictive model and a black box explainer.  It would be interesting to discuss how/if the method can generalize to other models than tree based approaches