 In the paper, the author provides the proof of stochastic gradient sparsification and quantization for non-convex problem and proposes a combined method to reduce the communication time. Sparse gradient [1] [2] and quantized gradient [3][4] have been explored before, however, the deep understanding of its convergence rate for non-convex problem is missing. The author fills an important gap between practical application and theoretical analysis. Periodic quantized averaging SGD is a straightforward extension of sparse gradient and quantized gradient. Experimental results also verify that the proposed method can obtain better speedup.   The following are my concerns:   1) the sparse parameter average is the same as communication-efficient distributed methods. There are missing citations about these methods, for example elastic averaging sgd[5].   2) according to [5], simply averaging the parameters in a period may lead to divergence. Did you meet this problem? Can you report the accuracies of the compared methods?    [1] A. F. Aji and K. Heafield. Sparse communication for distributed gradient descent. CoRR, abs/1704.05021, 2017 [2] Lin, Yujun, et al. "Deep gradient compression: Reducing the communication bandwidth for distributed training." arXiv preprint arXiv:1712.01887 (2017) [3]F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns, September 2014 [4] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1509â€“1519. Curran Associates, Inc., 2017 [5] Zhang, Sixin, Anna E. Choromanska, and Yann LeCun. "Deep learning with elastic averaging SGD." Advances in Neural Information Processing Systems. 2015  ======== after rebuttal======  My concerns are addressed. I will update my score to accept. 