summary: This paper provides theoretical convergence analysis on two recent methods that reduces communication cost in distributed learning, i.e., sparse parameter averaging and gradient quantization. This paper also provides a new method along with theoretical convergence analysis that combines these two strategies. Experiments are performed on CIFAR-10 image classification and speech recognition.  Quality: The paper is well organized, and combination of these two strategies are well motivated. The overall technical content of the paper appears to be correct. There are many notations involved in the paper, making some parts a bit hard to follow. There is certainly room for improvements in the clarity of the paper. Some concerns about the theoretical results are:  - The scaling factor of the O(1/\sqrt{MK}) rate may become larger due to sparsification or quantization compared to FULLSGD, is it possible that it exceeds \sqrt{M}? In particular, the scaling is proportional to the quantization error q which may be related to the data dimension N (line 219), and may be much larger than \sqrt{M}. In this case, the communication cost reduced by distributed learning cannot even compensate for the slowed convergence due to sparsificaiton and quantization. - In line 220, the quantization level s is proportional to the square root of the data dimension N, which can be very large empirically. In this case, the number of bits used to encode the gradients can also be large. Does this indicate that the O(1/\sqrt{MK}) rate only works for gradient quantization with large number of bits? How many bits are required to guarantee the convergence rate in the paper? How many bits are used empirically?   Some points about the experiments: - Currently, there exists much wider bandwidth networks than 1Gbps, what is the performance of the proposed method using network with larger bandwidth? - Besides CIFAR-10, what about the results on larger datasets like imagenet? Will the proposed method still have similar performance as FULLSGD and the other compared methods? - What is the exact number of bits used for QSGD in the experiments? Does QSGD also have requirements on the number of bits for their theory? Will the proposed method still have improvement when smaller number of bits is used for QSGD? 