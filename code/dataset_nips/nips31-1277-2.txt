The paper presents a neural architecture for programming language translation based on tree-to-tree LSTM model and attention. The architecture is overall well described, there are sufficient details to replicate the results in the paper. The ideas are clearly described and the problem is an interesting area explored in the software engineering community. The approach with the attention on trees is original, but otherwise the tree-to-tree model is straightforward.  Pros: The problem is interesting, with several competing works. Extensive evaluation and state-of-the-art performance. I particularly like that evaluation is broad and handles a range of programming language pairs. The metric is also realistic -- program accuracy is the realistic metric for such kind of app.  Cons: I expect a bit more depth for NIPS. In particular it is not clear what is the "expressibility" of the proposed architecture. For example, for CoffeeScript to JavaScript one would expect that accuracy of 100% should be achievable. Is there a particular construct that cannot be captured by the model? Probably some form of attention that locates a source subtree based on a current position may demonstrate that the approach is at least comparable in expressibility to recursively defined tree-to-tree transformation functions.  Minor: the paper discusses token accuracy, but the metric is only used in appendix.  Author remarks: Thanks for the explanation. The cases in which the model fails to translate CS to JS likely points out that the model lacks expressibility, but nevertheless works well on many examples. It would be good if the authors expand the appendix in the final version to discuss the cases where the model fails, I am sure there are more cases than just temporary variables. 