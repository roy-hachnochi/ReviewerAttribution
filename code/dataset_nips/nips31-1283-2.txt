I have changed by score to 6 after reading the rebuttal and other reviews. I highly like the main point of this paper, that it is helpful to selectively normalize some styles. I agree that simplicity is not a bad thing if the algorithm indeed performs well, but I still think the margin of improvement (both in ImageNet classification and style transfer) is not very impressive. So I will not give it a higher score. ==================================== This paper proposes a simple normalization layer that linearly combines the IN output and BN output, named BIN. Experiments show BIN performs better than BN and IN in terms of image classification and style transfer.  Pros: 1. The paper is well-written and easy to read. 2. The interpretation that "BIN learns to selectively normalize only disturbing styles while preserving useful styles" makes sense to me. It is also supported by the fact that earlier layers tend to use IN more while higher layers tend to use BN. 3. It is shown that BIN generalizes well to different architectures and different tasks.  Cons: 1. The proposed method is quite straightforward - adding a gate to switch between BN and IN. The technical novelty is limited. 2. BIN introduces extra computational cost compared with BN or IN. Although I expect the difference to be minimal, the authors should mention it somewhere.  Overall I think this is an interesting paper that shows empirical improvement over baselines. However, my main concern is its technical depth seems low.