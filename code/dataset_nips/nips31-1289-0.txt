— This paper introduces an approach to approximate the answerer within the questioner for the GuessWhat?! game involving goal-oriented visually-grounded dialog between a questioner + guesser and answerer.  — The proposed questioner has 3 components — the question-sampler (proposing candidate questions), the approximated answerer (predicting answers for each candidate question; and pretrained on the training set and/or via imitation of the actual answerer), object class guesser, and the question-picking calculation module.  — The question-picking module calculates mutual information of the object class and answer given history and a candidate question, and the argmax informative question is picked.  — Experiments are performed on MNIST counting dialog and GuessWhat. For MNIST, the approximated answerer is count-based and its recognition accuracy can be controlled proportional to the actual answerer’s accuracy. For GuessWhat, the approximated answerer is trained in a variety of ways — on the same training data as the actual answerer, on predicted answers from the actual answerer, on a different training data split as the actual answerer, and on a different training data split as the actual answerer followed by imitation of predicted answers on the other split.  — On MNIST, the authors compare against a random question-picker module. And the proposed approach outperforms the random baseline.  — On GuessWhat, the proposed approach outperforms prior work using supervised and reinforcement learning without an explicit model of the other side. Interestingly, the authors find that the depA* models perform better than the indA* models — showing that training on predicted answers is a stronger signal for building an accurate mental model than just sharing training data.  — Overall, the paper is clearly written and thorough, the proposed formulation is novel and makes sense, and results outperform prior work. I’m happy to recommend this for publication.  — Minor comment: RL and AQM objectives are indeed closely related, just that model-free RL (or how the game is set up in prior work) models the other side implicitly in its policy / value function. The two approaches seem complementary though — in that RL training can be augmented with these dialog agents having a model of the other side.