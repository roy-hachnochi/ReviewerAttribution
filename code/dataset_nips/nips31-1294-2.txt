Paper proposed a joint feature disentanglement and translation across multiple data domains. Architecture consists of an encoder that can encode an image from any of the domains of interest into domain-invariant latent space; and a generator that given a latent space vector and domain can reconstruct the image in that domain. The architecture his a combination of the VAE and adversarial GAN training. In particular, VAE encoder is used to encode an image into the latent space. An adversarial discriminator is trained on the latent codes to ensure that the latent code vectors are not able to distinguish a domain of the input. At the same time latent vector augmented with domain label are propagated through generator that is also trained in adversarial fashion, by ensuring that the generated image is real and is in the “right” domain after the reconstruction.   Overall the approach is sensible and sound. Paper is reasonably easy to read and formulation is compelling. However, comparisons to important baselines are missing. In particular, there are a number of recent methods for multi-domain image translation that should be compared against. The best example is StarGAN, which experiments also include pair-wise variants of CycleGAN among others. These should certainly be compared against. Other papers that is probably worth looking at are the following:  Unsupervised Multi-Domain Image Translation with Domain-Specific Encoders/Decoders L. Hui, X. Li, J. Chen, H. He, C. Gong, J. Yang  and   Modular Generative Adversarial Networks B. Zhao, B. Cheng, Z. Jie, L. Sigal  Although these were “unpublished” at the time of NIPS submission, so direct comparison might not be necessary. Comparison with StarGAN is however. The argument made for not including such comparison is that authors define “domains” differently, but this is just a matter of semantics. Certainly an experiment under attribute-based definition of StarGAN would be possible and really necessary for validation of the approach.  