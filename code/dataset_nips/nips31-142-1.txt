The paper is a natural extension to a very fundamental observation by Dasgupta, Stevens, and Navlakha in [8]. This is useful in nearest neighbour search comes from studying the olfactory circuit of a fruit fly. The observation in [8] is that using sparse binary random projections while increasing the dimensionality of the data gives nearest neighbour results that are comparable to that obtained using LSH (locality sensitive hashing) that uses dense gaussian random projections while lowering the dimension. In this paper the authors suggest computing a sparse binary projection while maintaining distances from the training data and then using this projection while performing nearest neighbour searches. They show accuracy improvements and running time comparisons using experiments with a few datasets.  Quality: The overall quality of the paper is good. It discusses a natural extension for the observation made in [8]. The heuristic to compute the sparse projection is explained clearly.   Clarity: The theoretical part of the paper is clearly explained though I find that section 2.2 is from previous work and might have been avoided to save space. In the experimental section, it is not clear to me what “Hash Length(k)” in the figures means. Also, when you say 20k, does it mean that it is 20 times k the hash length.  Originality: The paper can be seen as an extension of the previous work [8]. The authors set up an optimisation problem satisfying the high level observations in [8]. They suggest known techniques of alternating minimisation to the solve the optimisation problem and then perform experiments. There are no new techniques, explanations, or observations (except that the heuristic works well for the analysed datasets).  Significance: I think the observation in [8] provides a new direction to nearest neighbour search and I would consider the work in this paper significant as it further explores this new direction.