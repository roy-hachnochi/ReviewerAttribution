This paper investigates the distributed optimization of non-smooth convex function using a network of computing units. Both the global and the local Lipschitz continuities are considered. An optimal algorithm is proposed under the local Lipschitz continuity, while a simple but efficient algorithm is proposed for the global Lipschitz continuity. Both results are interesting to me. A few questions need to be clarified in rebuttal:  - Eq. (9), is the upper bound of the time complexity to achieve precision epsilon. Is the *precision* for deterministic measure or expectation measure? I recall that it should be deterministic measure. If so, the comparison between your result (13) and existing result (9) may be not be fair enough. - In Theorem 1, it is not very clear how the number of working units affect the time complexity? It would be interesting to discuss if or when the linear speedup can be achieved. - Alg1 is essentially the accelerated dual averaging algorithm plus the smoothing trick. But I am not sure if using the Gaussian smoother is the best way, since it brings additional uncertainty to estimate the gradient of the smoothed function. What if using the uniform smoother like the one "A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order, NIPS, 2016", that can exactly obtain the gradient of the smoothed function. - For the analysis of Alg 2, the gossip matrix is assumed to be PSD. It may not be necessary, see "Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent, NIPS, 2017". - Line 212, the constraint is equal to \Theta * A = 0, that is not correct. The definition of A should be not be AA' = W. It should be AA' = W - I or others.