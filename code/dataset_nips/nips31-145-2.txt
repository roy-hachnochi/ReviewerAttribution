The paper proposes a loss function based on f-statistic for learning embeddings using deep neural networks and with weak supervision. The proposed loss is based on the statistics of the training datasets as compared to the common approach of using the instance level similarities/dissimilarities. Further, the class separability loss on the embedding is active only on a subset of the dimensions. The authors claim that this helps in preserving the intra-class variation information and leads to a more disentangled embeddings.  Strengths: 1. The idea of using training set statistics for learning embeddings is quite interesting.   2. The properties of f-statistics are well described in the context of the problem addressed.  3. Use of weak supervision for disentanglement is useful in real scenarios.   Weakness: 1. It is not clear from limited results/experiments/datasets that the disentanglement is actually helping the few-shot learning performance the main goal of the approach.  2. Mutual Information based metric could have some stability issues. It will be useful to discuss the limitations of the metrics and perhaps more discussion on the recent/related disentanglement metrics.  3. In general the paper is well written, however, some information such as training details can be made clearer.    It will be good to clarify the details of how the relevant dimensions are chosen. This information seems to be spread between section 1.1. 2.1 and 4.1. Does the mini-batch size affect the f-statistics computation? It will also be nice to see some qualitative results for the disentanglement on the embedding.  Post rebuttal comments: Thanks for the clarifications. I suggest the authors make the training details more organized so the results are easily reproduced. 