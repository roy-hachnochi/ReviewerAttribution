This paper introduces a framework for efficient matrix and tensor differentiation. The main conceptual contribution, in comparison to existing automatic differentiation frameworks, is to work with expressions in Ricci calculus, which explicitly distinguish between covariant and contravariant indices. As tensor contraction is associative and commutative, this results in an elegant, expressive, and principled way to do automatic differentiation on tensor expressions, compatible with forward-mode, backward-mode, and symbolic differentiation.  I believe this work is a useful and exciting contribution to the ML community at large. The authors have clearly put thoughtful and extensive engineering effort into this work, and go as far as to provide an anonymized web API for their implementation of symbolic differentiation using this framework. However, I am somewhat concerned about its suitability with regard to the scope of ICML. Software framework papers (including related works cited by this paper) have tended to appear in other conferences. Though the work is clearly applicable in ML systems, its originality seems to derive chiefly from the interface design.  Some comments, orthogonal to the above:  - Since I think this falls in the category of a "systems paper", a more in-depth discussion of performance concerns might be warranted. In particular, it would be enlightening to see where the performance boost over existing frameworks comes from, especially in the GPU experiments in the appendix. If the time bottleneck consists of CUDA linear algebra operations, then does this framework produce expressions that are less redundant to evaluate? The paper's conceptual advantages would be greatly highlighted by a concrete analysis of how each other framework is handling Hessians suboptimally. On a similar note, a more thorough documentation of the simplifications and optimizations would be helpful.  - Another way to make the paper a better fit for the ICML scope might be to outline an end-to-end optimization use case for the framework, showing a performance boost on some end-to-end task.  - Though full second-order methods are currently non-standard in deep learning due to high model dimension, there has been some attention paid to optimization algorithms based on the primitives of Hessian-vector products (see, e.g., [1]). It would be timely and exciting to see Hessian-vector products implemented in this framework, and a performance comparison. As it stands, a state-of-the-art comparison on large-scale models is somewhat difficult, as is reflected by the instance sizes described in the appendix.  - The paper is very clearly written, and provides a precise and helpful exposition of background knowledge.  [1] N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, T. Ma. Finding Approximate Local Minima Faster than Gradient Descent. STOC, 2017.