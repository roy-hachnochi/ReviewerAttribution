Edit: I have read the authors feedback.  The authors propose a framework to compute high order derivative of function specified by a computation DAG. Their approach rely on two crucial insights: - This requires to be able to differentiate matrix expressions - Common matrix calculus language is not accurate enough to express this. The authors propose to use Ricci calculus, which was famously applied to general relativity and differential geometry. They describe an automatic differentiation framework based on this tool as well as simple differentiation rules. They illustrate the presented concepts based on simple examples and present numerical results suggesting that the proposed framework significantly outperforms existing architectures for automatic differentiation when applied to vector expressions (here a gradient to obtain a Hessian).  Main comments: This paper was really convincing and pleasant to read. The context, concepts and problems are explained in a clear way. The proposed approach is illustrated through pedagogic examples. The numerical results speak for themselves.   I clearly see the potential of the proposed framework to broaden and enhance accessibility to learning techniques through efficient and flexible computational tools. I think that this constitutes a very relevant contribution.  On the other hand my knowledge about automatic differentiation is close to negligible so I cannot really question the novelty of the approach beyond the convincing arguments given in the paper. I am a bit surprised not to see referecences recently published in the litterature context paragraph. See details bellow.  Furthermore, I am not competent to judge about potential loopholes in the numerical experiments, beyond, again, the fact that they are presented in a very convincing way.  Minor questions: - The authors limit themselves to binary operators. What is the impact of this and how does it limit the approach. - Could the author explain why they only focus on backward differentiation? - The experiments do not account for the complexity of constructing the expression graph. Could the authors give a sense about how high the corresponding computational cost is and how it compares to concurent approaches?   Biblio:  A. Griewank and U. Naumann, Accumulating Jacobians as chained sparse matrix products, Mathematical Programming, 95 (2003), pp. 555– 571.  R. M. Gower and M. P. Mello, A new framework for the computation of Hessians, Optimization Methods and Software, 27 (2012), pp. 251–273.  Gower, R. M., & Gower, A. L. (2016). Higher-order reverse automatic differentiation with emphasis on the third-order. Mathematical Programming, 155(1-2), 81-103.  Walther, A. (2008). Computing sparse Hessians with automatic differentiation. ACM Transactions on Mathematical Software (TOMS), 34(1), 3.  