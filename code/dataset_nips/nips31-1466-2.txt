This paper proposes a simple method on knowledge distillation. The teacher summarizes its state into a low dimensional embedded factor trained with convolution auto-endocoder, then the student learns to fit the embedded factors.  - The evaluation seems to suggest that the proposed method is effective compared to existing knowledge transfer method. - The approach is similar to FitNet in nature, except that additional auto-encoding factors are being used. It would be great if authors can provide additional insights into why this is necessary. - Specifically, it might be helpful to train several variants of factor architecture, starting from  0 layers to proposed layers to evaluate the significance of the auto-encoding structure. -  As mentioned in the paper, the training of auto-encoder incurs additional cost and the authors should give quantitative numbers of this cost.  In summary, this is an interesting paper with good empirical results, it can be improved by addressing the comments and shed more lights on why the proposed works.  UPDATE ---------- I have read the author's response, clarifying the relation to FitNet and impact of the auto-encoding structure would help enhance the paper a lot, and I would recommend the authors to do so