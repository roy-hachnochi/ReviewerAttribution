This paper builds on a successful line of research from Yu and colleagues on change point detection. The paper presents some interesting theoretical results linking the Bayes-optimal solution to computationally efficient and neurally plausible approximations. The paper also presents a cursory analysis of empirical data using the approximations.  The paper is well-written and technically rigorous. There were a number of important useful insights from the theoretical analysis. I was somewhat disappointed by the empirical analysis, as discussed below.  Specific comments:  The simple delta rule model with fixed learning rate is a bit of a straw man. More relevant are variants of the delta rule that use an adaptive learning rate (see Sutton, 1992).  I'm not sure what we learn from the observation that the best fitting parameters allows the EXP approximation to still approximate DBM well when alpha is close to 1. This would be useful if the agent could estimate those parameters from observations.  Do the analytical results suggest a context-dependent division of labor between parallel learning systems, which could be tested experimentally? I'm thinking specifically of the statement at the end of section 2.4: "simple, cheap prediction algorithms can perform well relative to complex models, since each data point contains little information and there are many consecutive opportunities to learn from the data." For example, it would be interesting to parametrically vary alpha in an experiment and test whether different approximations came into play depending on the alpha value.  I was hoping for more from the empirical analysis. Is there any positive evidence for the EXP approximation over DBM? For example, a formal model comparison result, or some qualitative pattern in the data that would indicate people are using EXP? As it currently stands, there isn't any clear evidence distinguishing EXP and DBM.  Although the paper cites the work by Wilson, Nassar and Gold, there isn't any substantive discussion of their work, which is rather similar: they use a mixture of delta-rules to approximate the Bayes-optimal solution.  Minor comments:  p. 5: "Exact Bayesian" -> "Exact Bayesian inference"  -------------------  I think the authors gave fairly reasonable responses to my comments. I already gave the paper a fairly high score, and I'm not inclined to raise it because the main issue I raised was the non-identifiability of the models based on the behavioral data, which the authors concede is true. Apart from that issue (which is a pretty big one) I think the paper may be good enough for NIPS.