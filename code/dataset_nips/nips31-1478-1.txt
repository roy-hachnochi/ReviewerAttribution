UPDATE: After reading all reviews and rebuttal, I am more confident in my initial rating. I think, this is a good work and would recommend its acceptance. I also like Table 2 in the supplementary on summary of the characteristics of the different methods concerning fairness. In the missing reference "Recycling privileged learning and distribution matching for fairness", the authors have already used the notion of epsilon fairness (7b), albeit it was not formally defined as in this work.   This paper proposes fair classifier learning based on empirical risk minimization framework with fairness constrains (FERM). The authors define the concept of epsilon-fairness, i.e. a fair model commits approximately the same error across the sensitive group on the positive class. This definition accommodates the definition of equality of opportunity (when epsilon=0 and the hard loss function is used to compute the error rate) and can be extended to equalized odds.    The authors derive both risk and fairness bounds to show that the FERM method produces statistically consistent estimator (Theorem 1).  Furthermore, because the fairness constraints are non-convex, the authors derive convex relaxations of the constraints using the linear loss (1-yf(x))/2, and also use the Hinge loss instead of the hard loss in the risk functional to derive a convex FERM problem (10). The authors also explored the quality of such approximation in terms of classification accuracy and fairness (Proposition 1).   The theoretical analysis and the prosed method are appealing. It also has a nice interpretation. When FERM is embedded in the RKHS and eps=0, the learned parameter vector w has to be orthogonal to the difference between the sensitive groups (more precisely, orthogonal to the difference of their barycenters). Because the difference in the sensitive groups is due to sensitive information in the first place, this can be also reinterpreted as w has to be orthogonal to the sensitive information. In the special case with linear kernels, the orthogonality (fairness) constraint can be enforced via simple data pre-processing step (15).   The empirical evaluations are convincing. I like that the authors evaluate both scenarios when s is present/not present in x.   My only concern with this work is that the main idea of matching the error rates as per definition 1 has been addressed in the last year NIPS paper "Recycling privileged learning and distribution matching for fairness" by Quadrianto and Sharmanska, where the authors matched the distributions over error rates across sensitive groups using MMD criterion. MMD criterion matches potentially infinitely many moments when embedded in the RKHS. To the best of my understanding, using linear loss in fairness constraints, as it is proposed in this work, means matching first order moments (barycenters) in the context of distribution matching. I wonder whether the authors could comment on the relation to this work.   