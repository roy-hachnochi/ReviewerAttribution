------------------After authors' feedback----------------- Authors will update the paper according to my review. After reading other reviews, I still think that it is a good paper.  ----------------Before authors' feedback----------------- Summary: I recommend acceptance of this paper, for its quality and clarity. Even if the originality of the algorithm is not outstanding, the results are.  Description: $\Delta$-encoder: an effective sample synthesis method for few-shot object recognition addresses few-shot learning through data augmentation. More specifically, the authors designed a neural network that creates synthetic images, in the feature space, by modifying -- or in their words applying deltas to -- unseen real images. The neural network is a particular auto-encoder that works on pairs of images. Finally, the real unseen images and the synthetic ones are used to train a linear classifier. Their approach is evaluated on several datasets and outperforms a wide range of competitive algorithms from the state-of-the-art. In addition, as their approach performs data augmentation in the feature space, it allows to use pretrained neural network to extract the features. Doing so improves significantly the performances and still outperforms others approaches combined with a pretrained feature extractor neural network. The authors also performed an ablation study. Some directions for future works are proposed at the end.  Quality: The submission is technically sound. Experimentations are done thoroughly and carefully to be fair against the wide variety of algorithms addressing few-shot or zero-shot learning. A lot of different datasets are used for evaluation. The ablation study reveals interesting findings and justify the design of their algorithm. I like the fact that authors included computation times as it proves their algorithm runs a reasonable time. I found the part analysis the quality of the synthetic samples a bit weak and less convincing than the rest of the paper. In particular, I am not sure that Figure 5a really demonstrates anything. Finally, to be completely fair, the authors should highlight that training the linear classifier with the unseen examples and their associated generated ones is necessary, whereas some other approaches, like the ones using the nearest neighbors, donâ€™t rely on any kind of retraining. In addition, I would have like to see more details on the linear classifier and its implementation and a comparison to kNN to see how much replacing the classic 1-NN by a linear classifier improved the result. Overall, the quality is good, but additional details on the linear classifier should be given in the final version.  Clarity: The paper is very well written, pleasant to read and well organized. The numerous illustrations boost the clarity of the paper. Results are clearly stated and are easy to review. The implementation is very well detailed. However, no details is given on the linear classifier (cost function?). This prevents to exactly reproduce the results. On the other hand, authors will share the code with the final version. To summarize, clarity is outstanding. Originality: Although simple and quite standard, the proposed algorithm achieves very good results. Related work section is well organized and reviews all the different approaches to few/zero-shot learning. The authors justified their approach using results in the previous works. As they mentioned, they incrementally built on top of [3]. Overall, originality is ok.  Significance: Few-shot learning is a very important field in machine learning that has drawn a lot of attention recently. As the proposed algorithm outperforms by a large margin the other algorithms, it constitutes a good advance in the state of the art for few-shot learning.  Line 241: then -> than