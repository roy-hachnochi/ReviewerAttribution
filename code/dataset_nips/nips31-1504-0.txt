The paper explores the tradeoffs involved when selecting a student architecture for models trained with distillation. While much of the research so far has focused on simpler architectures (fewer layers, less depth in each layer), this paper proposes keeping the depth and number of layers of a state-of-the-art architecture and instead achieving parameter reduction by replacing convolutional blocks with simpler convolutional blocks.   Three techniques present in previous research are proposed to simplify the convolutional blocks: grouping (where the channels are divided in groups, each group has a convolution of the desired depth applied to it, then a 1x1 convolution is used to mix the channels), bottlenecking (where a convolution with N channels is replaced with three convolutions; first a 1x1 convolution to reduce the depth; then a full convolution; finally a 1x1 convolution to increase back the depth), and a combination of grouping and bottlenecking.  Interestingly it seems like the different simplified convolutional blocks can lead to a better tradeoff between number of parameters / computation and accuracy, when distilling from a state-of-the-art teacher model, than reducing the number of layers and depth.  The evaluation could have been a little more thorough as it doesn't look like the hyperparameters were retuned for each architecture, which might hide some potential improvements. That said the idea of replacing convolutions with simpler convolutions when building student models for distillation is interesting, and should be further investigated.