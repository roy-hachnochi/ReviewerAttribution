This paper provides an extensive experimental study showing that one is able to distill the knowledge of large networks using much smaller networks, and notably the paper highlights the fact via the use of "cheap convolutions" - group convolutions and 1x1 convolutions as proposed in the papers in the recent years.  I find the experimentation part of the paper to be sufficient, and I believe it would be very useful data points for the community. However, I am not very certain about the novelty of the paper; it summarizes the two reasonably known approaches (knowledge distillation and attention transfer), and utilizes them in the specific case of training cheap convolutions. The convolution types are known for a while too, for example the 1x1 convolution, first appearing in the network-in-network paper, and group convolution which was first in AlexNet and recently seeing more interest since ResNet. As a result, this paper seems to be more on experiment verification instead of proposing novel approaches.  I am personally fine with the theme of the paper, but do feel that when comparing to other novel papers, it might fall short of the originality axis a little bit. As a result I would like to recommend a marginal acceptance.