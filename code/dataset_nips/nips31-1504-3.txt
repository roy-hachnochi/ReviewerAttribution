The paper under review examines the problem of designing smaller CNNs for lightweight devices. The approach advocated by the paper is to start from a given (heavyweight) teacher architecture and replace its building blocks with faster ones. Most experiments in the paper use a wide ResNet as the reference heavyweight architecture and replace the standard convolutional layers in the ResNet blocks with grouped/bottleneck CNN layers which use fewer weights and lend themselves to faster inference. For training the resulting lightweight architecture, the paper advocates the use of distillation, where the training loss contains both the standard classification loss coming from training data and knowledge distillation or attention transfer from the more accurate teacher network. The paper reports results mainly on the CIFAR-10 and CIFAR-100 datasets, as well as more limited experiments on the Imagenet classification task and Cityscapes semantic segmentation task. The paper is well written and I find the proposed approach well motivated. However, a major shortcoming of the paper in its current version is that it only compares the different architectures in terms of number of parameters (which determines the memory footprint). The other major axis to compare different CNNs is in terms of number of operations (MAdds) which is more correlated to the runtime. This is unfortunate, as the corresponding MAdd numbers are very easy to compute for a given input image size, and does not allow direct runtime comparison of the resulting networks to other recent papers that propose alternative small network designs.