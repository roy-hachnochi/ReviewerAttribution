This paper proposes a stochastic cubic regularized Newton method to escape saddle points of non-convex functions. In particular, it explores the second-order information to escape the saddle points by utilizing the cubic regularization. The convergence rate of the proposed method can match the existing best-known result. Overall, this paper is well structured and presented clearly.  I have some concerns as follows: 1. In the experiment, is the employed SGD the classical SGD? If not, it’s better to compare the proposed method with the variant proposed in [1].  2. Since the proposed method utilizes the second-order information. It’s better to compare it with the second-order counterparts, such as Natasha2.  [1]. Ge, Rong, et al. "Escaping from saddle points—online stochastic gradient for tensor decomposition." Conference on Learning Theory. 2015.  After feedback: Since the purpose of this algorithm is to escape the saddle point, it is recommended to compare it with the perturbed SGD, which is designed to escape saddle points, in your final version.