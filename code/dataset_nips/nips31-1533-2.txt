This paper aims at leveraging human demonstration to improve exploration in a deep RL setting. In particular, this paper deals with the problem of overcoming domain gaps in the imitation learning setting and without access to the actions and rewards that led to the demonstrator’s observation trajectory.  The paper is well-written and provides interesting results.  Equation (2) defines a positive reward when the zero-centered and l2-normalized embeddings of the agent is similar to "checkpoints" of a single youtube video. In that case, the agent may want to repeatedly stay in the same set of states to repeatedly gather the reward and may be blocked in a local optimum. I suppose that this is why the checkpoints should be visited in a "soft order". It would be interesting to clearly state this.  In the experiments, four YouTube videos of human gameplay are used. It is not explicitly stated whether the domain gap of these four youtube videos are the ones illustrated in Figure 1. A good addition to the paper would be to provide for all three games (MONTEZUMA’S REVENGE, PITFALL! and PRIVATE EYE) an illustration of the frames after pre-processing with keypoint-based (i.e. Harris corners) affine transformation so that one can visualize the domain gap. In addition, it could be a good addition to provide a visual illustration of the held-aside video so that the reader can understand the domain gap closed between the video used to generate the "checkpoints" and the one from ALE. It would also be interesting to show that the algorithm is robust when using another video from the set of four. Indeed, the ALE frame is close to some of the frames from the youtube video in Figure 1b and it would be possible that the algorithm is particularly efficient for some of them but not all.  Additional remarks: - typo: line 82 "the the". - In equation 2, it may be wiser to choose a different sign than \gamma, because that one is usually used for the discount factor in RL.