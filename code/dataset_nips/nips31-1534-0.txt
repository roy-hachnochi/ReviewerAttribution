This paper proposes a sequence-to-sequence model to tackle the conversational knowledge based QA. Unlike existing seq2seq models, they additionally incorporate a conversation memory into the model, which enables the model to copy previous action subsequences. Experimental results show that this model significantly outperforms the KVMem based model. The paper is well written.  I have a few concerns about this paper:  (1) There is a basic assumption throughout the paper, that is, a logical form could be generated using the grammar.     This requires that the logical form could be represented as a tree. However, I do think this assumption always holds given the fact that     many meaning representations are represented in more complicated structure. For example, the Abstract Meaning Representation (AMR) is represented in a graph structure.  (2) It is very hard to follow in the section 6.2. Many symbols in the equation are not described. For example, s_{t} and a_{<t} in equation 2.     I also do not understand how you train the model. In section 6.3, you mentioned that to train the model, you first generate action     sequences for each example. What is exactly in this atomic example, is it question-answer wise or dialog wise. You should give     one example describing this training data generation.  (3) the main contribution of this paper may be the introduction of memory management. This motivation makes sense. But the     huge performance different between this method and KVMem really surprises me. The authors should explain this rather than jut list the results.     I think this could provide more insights about the limitation of the KVMem.  