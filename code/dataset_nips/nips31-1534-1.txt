This paper proposes a semantic parsing method for dialog-based QA over a large-scale knowledge base. The method significantly outperforms the existing state of the art on CSQA, a recently-released conversational QA dataset. One of the major novelties of this paper is breaking apart the logical forms in the dialog history into smaller subsequences, any of which can be copied over into the logical form for the current question. While I do have some concerns with the method and the writing (detailed below), overall I liked this paper and I think that some of the ideas within it could be useful more broadly for QA researchers.   Detailed comments: - I found many parts of the paper to be confusing, requiring multiple reads to fully understand. Section 4, for example, describes a "dialog memory", but it is unclear whether and how any of the subcomponents (e.g., predicates, actions) are represented by the model until much later. The paper could really benefit from a concise summary of the method before Section 4, as currently readers are just thrown into details with no context. - The inclusion of actions in the dialog memory is very interesting! This could benefit models trained on other sequential QA datasets as well and strikes me as the most unique part of this paper.  - Q2 in Figure 2 ("Where did he graduate from?") is different than the Q2 referenced in the text ("Where is its capital?"), and the figure appears to contain the logical form for the latter question.  - The context here is just the previous question. It would be nice if the authors would describe the challenges associated with scaling the approach to the entire dialog history, and perhaps offer a small-scale analysis of the impact of increasing the context size. - The implementation of the "replication actions" are a bit confusing. From my understanding, the decoder first decides to choose a replication action (e.g., A19), and only then decides what entities/predicates/numbers from the dialog memory to replicate. Was this done for efficiency reasons? ICould all of the "action subsequences" be enumerated and then added as separate question-specific actions? - What is the purpose of the "gates" in the replication actions (Sec 6.2)? What does it mean for a gate to be chosen? Don't you already know beforehand where the entities/etc. in the memory come from? Relatedly, why is there only one gate for numbers? Surely numbers can occur in the previous question and previous response... - It would be nice to have some discussion of how time-intensive the learning process detailed in 6.3 / Appendix B is. As the search space is huge, doing BFS is likely very expensive! - Since the dataset is new and the comparisons are only against baselines in the original dataset paper, it's hard to be overly impressed with the huge performance gains. That said, the different ablated versions of the proposed model make up for this and offer nice insights into the relative importance of the different components in the dialog memory.  - Section 7.3 is much appreciated; I'd like to see a bit more in the "Unsupported Actions" section about questions that require reasoning across multiple questions.