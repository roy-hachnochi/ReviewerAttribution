UPDATE AFTER AUTHORS' RESPONSE  I need to clarify: in my opinion, the thing that must be fixed is clarifying the analysis applies to unsupervised RF.  My comment about a little lower relevance is to explain why I did not give the paper a higher score.  SUMMARY  The paper is a theoretical analysis of how data point subsampling impacts unsupervised random forests (unsupervised = tree structure not based on response variable Y).  Suppose the forest is grown with an infinite number of trees, using infinite data.  It shows that two scenarios produce forests that are asymptotically inconsistent: a) building forests of deep trees without subsampling, and b) building forests with fixed subsample size and fully grown trees (one data point per leaf).  REVIEW  The question of when random forests fail or do poorly is interesting because those situations seem to be rare in practice.  Anecdotally, I can recall only a couple examples from a decade of applied work. Understanding such corner cases improves the community's ability to use these tools scientifically.  This paper has three strengths and two weaknesses.  Strengths:  * This is the first inconsistency analysis for random forests.   (Verified by quick Google scholar search.) * Clearly written to make results (mostly) approachable.  This is a   major accomplishment for such a technical topic. * The analysis is relevant to published random forest variations;   these include papers published at ICDM, AAAI, SIGKDD.  Weaknesses:  * Relevance to researchers and practitioners is a little on the low   side because most people are using supervised random forest   algorithms. * The title, abstract, introduction, and discussion do not explain   that the results are for unsupervised random forests.  This is a   fairly serious omission, and casual readers would remember the wrong   conclusions.  This must be fixed for publication, but I think it   would be straightforward to fix.  Officially, NIPS reviewers are not required to look at the supplementary material.  Because of having only three weeks to review six manuscripts, I was not able to make the time during my reviewing. So I worry that publishing this work would mean publishing results without sufficient peer review.   DETAILED COMMENTS  * p. 1: I'm not sure it is accurate to say that deep, unsupervised   trees grown with no subsampling is a common setup for learning   random forests.  It appears in Geurts et al. (2006) as a special   case, sometimes in mass estimation [1, 2], and sometimes in Wei   Fan's random decision tree papers [3-6].  I don't think these are   used very much.  * You may want to draw a connection between Theorem 3 and isolation   forests [7] though.  I've heard some buzz around this algorithm, and   it uses unsupervised, deep trees with extreme subsampling.  * l. 16: "random" => "randomized"  * l. 41: Would be clearer with forward pointer to definition of deep.  * l. 74: "ambient" seems like wrong word choice  * l. 81: Is there a typo here?  Exclamation point after \thereexists   is confusing.  * l. 152; l. 235: I think this mischaracterizes Geurts et al. (2006),   and the difference is important for the impact stated in Section 4.   Geurts et al. include a completely unsupervised tree learning as a   special case, when K = 1.  Otherwise, K > 1 potential splits are   generated randomly and unsupervised (from K features), and the best   one is selected *based on the response variable*.  The supervised   selection is important for low error on most data sets.  See Figures   2 and 3; when K = 1, the error is usually high.  * l. 162: Are random projection trees really the same as oblique trees?  * Section 2.2: very useful overview!  * l. 192: Typo?  W^2?  * l. 197: No "Eq. (2)" in paper?  * l. 240: "parameter setup that is widely used..." This was unclear.   Can you add references?  For example, Lin and Jeon (2006) study   forests with adaptive splitting, which would be supervised, not   unsupervised.  * Based on the abstract, you might be interested in [8].   REFERENCES  [1] Ting et al. (2013).  Mass estimation.  Machine Learning, 90(1):127-160.  [2] Ting et al. (2011).  Density estimation based on mass.  In ICDM.  [3] Fan et al. (2003).  Is random model better?  On its accuracy and efficiency. In ICDM.  [4] Fan (2004).  On the optimality of probability estimation by random decision trees.  In AAAI.  [5] Fan et al. (2005).  Effective estimation of posterior probabilities: Explaining the accuracy of randomized decision tree approaches.  In ICDM.  [6] Fan el al. (2006).  A general framework for accurate and fast regression by data summarization in random decision trees.  In KDD.  [7] Liu, Ting, and Zhou (2012).  Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data, 6(1).  [8] Wager.  Asymptotic theory for random forests. https://arxiv.org/abs/1405.0352 