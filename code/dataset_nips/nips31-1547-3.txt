In this paper, the authors proposed sufficient conditions for random forests to be inconsistent. By linking random forests to local average estimators, they showed that a random forests model is consistent only if the diversity and locality conditions hold. The diversity condition requires that training examples have asymptotic zero weights. The locality condition requires that the total training weight outside a fixed ball to be asymptotically 0. Both of the conditions make intuitive sense -- to make an estimator consistent, we need to average over asymptotically infinite amount of data. The authors then continue on to study the properties of specific tree structures.   There are a couple points that I think the authors could improve on this paper: 1. The authors should explicitly mention that the tree partition needs to be unsupervised. Although this is implied from the UW-property, where the weights depend only on the unlabeled data, I think the authors should explicitly emphasize that. 2. Although the paper is mostly theoretical, a numerical study is still helpful. The authors could empirically show that as the sample size grows, predictions from random forests that do not satisfy the diversity and localness conditions do not converge to the true value. If the authors need more space, they could shorten Section 2, in which most of the backgrounds should be assumed known by readers.  The strength of this paper is it is easy to follow and is interesting from a theoretical and practical point of view. The weakness of the paper is that it lacks a numerical section that empirically examines the authors findings.