This paper presented the new task of event captioning in videos with weak supervision, where the set of captions is available but not the temporal segment of each event. To tackle this problem, the authors decompose the global problem into a cycle of dual problems: event captioning and sentence localization. This cycle process repeatedly optimizes the sentence localizer and the caption generator by reconstructing the caption. The performances are evaluated on the ActivityNet Captions dataset.  I have several concerns about this paper: - The motivation of this task is not clear. For which applications is it easy to get the event captions without the temporal segment?  - I am not convinced by the testing procedure where random segments are used to generate a set of event captions for each video because a temporal segment can overlap several events or no event. I think that using pre-trained temporal segment proposal like [101] could be better.  - The authors should analyze the sensibility to the initial candidate segments because they are generated randomly? The authors should report standard deviation in the results. - The authors should give information about the hyperparameters tuning and how the model is sensible to these hyperparameters. - The authors should explain how they chose the number of candidate temporal segments in test videos. This number seems important because they used one-to-one correspondence strategy. - The authors proposed Crossing Attention mechanism but they did not analyze the importance of this attention mechanism. The authors should compare Crossing Attention mechanism with existing attention mechanisms.  The proposed method combines several existing works, but the results seems interesting. However this new task is not well motivated and  I am not convinced by the testing procedure where random candidate temporal segments are generated.  [101] Shyamal Buch and Victor Escorcia and Chuanqi Shen and Bernard Ghanem and Juan Carlos Niebles. SST: Single-Stream Temporal Action Proposals. In CVPR, 2017.   I have read the author response and I am globally satisfied. But I am still not completely convinced that generating random temporal segments is a good strategy. However the authors show that their model is robust to these random segments. I think that using temporal segment proposal trained on external data can be useful to generate better segment. 