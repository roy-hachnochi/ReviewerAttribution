This paper provides two algorithms for the dynamic assortment planning problem with the multinomial-logit probabilistic model. It is well written and clearly conveys the ideas. The proofs in the supplementary are also very clear, with nice order.   In my opinions, the contribution of this paper mainly falls into two parts:  1. An upper bound independent of item number N. The upper bound of worst-case regret of these two algorithms are both independent of the item number N, while in the previous work on this problem at least logN will appear. This is a very important and also surprising progress in dynamic assortment planning problems. It can greatly improve the algorithm performance in practice in cases when N is large, e.g., online sales and advertisement. The numerical experiment in the paper also shows very good and stable performance, compared with previous algorithms, because of the aforementioned better upper bound independent of N.  2. Removing logT and using loglogT instead. The second algorithm (LIL-trisection) in this paper, using Hoeffding’s inequality, improves the worst-case upper bound of regret to O(sqrt{TloglogT}) from O(sqrt{TlogT}). This is also highly nontrivial. Since the paper in the meanwhile gives a lower bound for the worst-case regret by O(sqrt{T}), the new loglogT bound goes a big step towards the best case. The paper also conjectures that the worst-case regret actually has the same upper and lower bound up to a constant. The small gap left here makes the target of further research quite clear.  Some other comments on the paper (Please correct me if any of my comments is wrong):  The paper transfers the search for the optimal assortment to the search for the maximizer of the revenue potential function, which is the expected revenue of a “level-set” type assortment. This step is very beautiful, and the paper gives a very detailed description of the revenue potential function and proves several useful properties that can benefit further research.   For the experiment part, I am curious why all r_i’s are constrained within [.4, .5] while they are initially assumed to be normalized to [0, 1]. This point might be trivial, but some explanations of why it is more convenient, or consistency of the numbers are recommended. 