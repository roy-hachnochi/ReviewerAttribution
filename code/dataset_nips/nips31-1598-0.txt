The authors analyze ULA, SGLD and SVRGLD for nonconvex optimisation. They show that all the algorithms converge globally to an 'almost minimizer.' They use alternative proof techniques to the paper by Raginsky et al. (2018), which looks at a similar problem. This enables them to get better complexity bounds; SVRGLD is also not considered by Raginsky et al. (2018), which the authors show can give better complexity results in certain situations.  I found the paper very easy to read and well presented, which I appreciate is hard to do with a theoretical paper such as this. While the paper follows quite closely to Raginsky et al. (2018), the author makes the novel contributions clear. The new proof method, improved complexity results, and analysis of SVRGLD seem to me to have considerable impact; though I admit I am more familiar with the sampling literature than the optimisation literature. I have a few minor points I might suggest:  - I think more discussion of the dissipative assumption would be useful for the reader. Some intuition for which models will satisfy this assumption or not would be useful.  - I thought the 'proof roadmap' was a nice way of presenting the theoretical ideas in a digestible way. Maybe clarifying what is meant by a 'sufficiently small stepsize' in Lemma 4.1? Is there an exact bound for this?  - The footnote ^2 in the abstract looks a little like a squared term, perhaps that footnote can be left to the comma or end of the sentence?  - I'm not sure its needed to state all 3 algorithms (ULA, SGLD and SVRGLD) in their own algorithm boxes, since the descriptions already contain many of the details and it takes up a lot of space. Perhaps SVRGLD is the only necessary algorithm statement since it is less well known?