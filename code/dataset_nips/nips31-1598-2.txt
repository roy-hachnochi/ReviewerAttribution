After the rebuttal: Thank you for your answers.   In this paper, the authors analyze the global convergence behavior of Langevin-dynamics-based algorithms, namely GLD (aka ULA), SGLD, and variance-reduced-SGLD. The paper and the proofs are very well written and clear.  The authors provide error bounds that are tighter than the ones that were recently established in Raginsky et al 2017. The proof strategy consists of first bounding the error of GLD, then bounding the error of the other two algorithms by relating them to GLD. For the error bounds of GLD, the authors follow a different path compared to Raginsky et al, and this way they are able to prove tighter bounds.   Overall, I think the improved bounds do not bring any practicality if we compare them with Raginsky et al, since the improvements are still dominated by the exponential dependence on the dimension. In this sense, the paper has very limited practical importance. Moreover, I believe that some remarks, e.g. Remark 3.9 are not realistic.   On the other hand, the theoretical analysis can be seen rather straightforward. For GLD, the error decomposition is quite a standard one and the individual bounds for the terms (that are obtained after decomposition) can be obtained by slightly extending existing results, e.g. using a scaling argument with Mattingly et al and Chen et al, then using Raginsky et al directly. For the other methods, the results are also obtained by using very standard techniques, mainly Girsanov's theorem.    However, I still think that the paper is a good contribution to the community since it contains a unified analysis framework and slightly better bounds. I believe the framework would be useful for other practitioners/theoreticians.   I only have minor comments:  * The second footnote in the abstract is confusing since it looks like a square. * Line 254: typo in nevertheless * The authors should explicitly mention that Lemma 4.3 is coming from Raginsky et al, like they do in Lemma C3. * The authors assume that the Poisson equation has a solution. Is it implied by the assumptions? If not they should explicitly mention it. In any case this points needs more clarification. * There are new papers on variance reduction in SG-MCMC. The authors should cite them as well:  Stochastic Variance-Reduced Hamilton Monte Carlo Methods Stochastic Gradient Hamiltonian Monte Carlo with Variance Reduction for Bayesian Inference On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo  * The authors might also be interested in citing this recent paper:  Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization