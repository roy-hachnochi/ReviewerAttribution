This paper extends recent ideas in training structured predictors using randomized algorithms to the latent variable setting. Traditional large-margin training for structured prediction defines the loss function in terms of constraints or an optimization over the entire structured output space, which in general makes inference and learning NP-hard. Recent work has considered defining the loss over only samples in the output space from a proposal distribution. In this case, the approximate loss function bounds a robust learning objective over the entire output space up to known approximation error.        This paper considers extending this technique and analysis to the setting where there are latent variables. The key technical contribution is an analysis that is tighter than a naive application of Rademacher complexity analysis, leading to a sample complexity that is only logarithmic in the size of the latent space.  Experiments show that the proposed method is as accurate but >10x faster than traditional large-margin learning techniques on synthetic data and an image alignment problem.  The paper is clear, and makes an interesting contribution.  Update: Thank you to the authors for their response. The other reviewers raised good points about clarifications that would enhance the paper. I encourage the authors to take those suggestions.