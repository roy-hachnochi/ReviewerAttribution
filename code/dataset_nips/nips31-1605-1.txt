Overview: This paper presents an approach to train latent structural SVMs. The objective function is based on a slack rescaling loss which uses a latent margin form (eq 5). The maximization over all possible outputs in the loss is replaced with a maximization over a (small) random sample of structured outputs from a proposal distribution (that satisfies a few assumptions). A PAC-Bayes generalization bound is derived, with robustness to Gaussian perturbations. Assuming that the maximization over the latent space is tractable, this approach yields an efficient training algorithm with generalization guarantees. The paper seems to be technically sound and the motivation is clear, although the proofs are quite involved and I have not verified all of them carefully. The main limitation in my opinion is incremental contribution over the work of Honorio & Jaakkola [11], possibly better as an extended journal version of [11] than a standalone conference paper?  Detailed comments: * This work follows [11] very closely. The structure and theorems are analogous to [11]. Some sentences are even copied as is -- for example, lines 71, 86-87, 125,... * Line 95: an example of “parts” can be very helpful for clarity. * For footnote 1, I suggest to move it to the main text and add an example in section 5. This is essentially an additional assumption -- necessary for obtaining a tractable algorithm. What if the maximum can be approximated efficiently? Do some guarantees still hold? * In the image-matching experiment, is it possible to add results for an LSSVM or other baseline besides [9]? * Line 206: I suggest to provide high-level intuition of the idea behind the tighter bound, the current wording is rather mysterious. * For training, is the upper bound in Thm 2 minimized? Adding the details of the training algorithm (perhaps in the appendix for lack of space) would improve readability. 