This paper shows that the Kozachenko–Leonenko estimator for differential entropy is near optimal considering the minimax risk of differential entropy estimation. In particular, the assumption for density functions to be away from zero is not used, while previous methods use the assumption for the convergence proof.  The paper shows a thorough literature review and explain what is the contribution of this paper very clearly. Many parts of the paper are based on the book of Biau and Devroye [4], and the proof explanation in the Appendix covers the fundamental parts of the derivation in the main paper very well.  The main result of this paper is also interesting. The Kozachenko–Leonenko estimator and the Kullback-Leibler divergence estimator based on this estimator is known to work well in practice compared with other methods even though nearest neighbor methods are usually considered as a simple but poor method. The derived bound of the error is interestingly has a similar form to the minimum error of such estimators (minimax bound), and the bound is very close to the minimum error as well. This makes reading the paper enjoyable. 