This paper is a very interesting contribution bridging reinforcement learning (RL) and point processes (PP), designed for a specific RL scenario when the action and feedback are not synchronised, and the temporal intervals between action and feedback can affect the reward function. The authors illustrate two very persuasive application scenarios in tutoring and in social networks.  Source code and data are provided - great. I have not run the code but inspected it somewhat; nor was I able to devote time to the proofs in the appendix. This is reflected in my confidence score. That aside, I think this is a valuable contribution.   Main feedback:  (1) The two demonstrations, empirical though using pre-existing datasets, are convincing. Some baseline systems are shown and perform worse - the baselines appear broadly plausible. However I would have liked the authors to show specifically that the Marked PP approach is an improvement over other RL methods. Could the authors, now or in the future, perform experiments using a simplified RL using the same neural net but with discretised time, or with the exponential decay term w_t hard-coded to zero?  (2) The authors claim this method "does not make any assumptions on the form of the intensity" (line 311). However, in equation 5, in the regions between events, the intensity is constrained to be an exponentially-decaying (or -growing) function. This is not a strong limitation on the global form because the intensity can change dramatically at event instants (see eg the sketched function in the top of Fig 2a, or the left of Fig 1); however it is quite a specific form which may be undesirable in some applications.   Minor feedback:  * The title is a little unclear. It would be good to convey more clearly that RL is part of the application domain, not just a tool used to solve a PP problem. 