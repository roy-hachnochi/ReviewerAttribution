The paper "Deep Reinforcement Learning of Marked Temporal Point Processes" proposes a new deep neural architecture for reinforcement learning in situations where actions are taken and feedbacks are received in asynchronous continous time. This is the main novelty of the work: dealing with non discrete times and actions and feedbacks living in independent timelines.   I like the proposed architecture and I think the idea can be of interest for the community. However, from my point of view several key points are missing from the paper to well understand the approach and its justification, and also for a researcher which would like to re-implement it:       - For me, it would be very important to discuss more about marked temporal process. Why is it better to model time like this rather than using for instance an exponential law ? Also, more insights should be given to explain where comes equation 2 from (although I know that is quite classical). It would great help the understanding from my point of view to explain all the components of this (the survival function and so on).      - The problem, that is developped a bit in appendix C, that a feedback can arrive before the next sampled time should be greatly more discussed in the paper, since it is a key point of the approach.        - Greatly more explanations should be given in appendix C. Please give the formula of CDF1 and CDF2 (probably something depending on the second part of eq 2 but... ?) and please give the derivation to obtain "perhaps surprisingly" the closed-forms for CDF1^-1 and CDF2^-1.         - "In our experiments, we will use a quadratic regularizer ..." => should be greatly more detailled. Unclear to me.       - In the experimental setup, the function H is not defined, while it is another important component of the approach.   Minore remarks:        - I multiple usages of b in the definition of the input layer. Somewhat confusing. Set flags to another notation      - "perhaps surprisingly" for proposition 1. Why ? It is a well-known trick, not really surprising     -  "REINFORCE trick is still valid if the expectation is taken over realizations..." => why this wouldn't be the case? This trick is very general