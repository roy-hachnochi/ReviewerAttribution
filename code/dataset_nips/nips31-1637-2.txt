This paper considers the problem of searching for an action sequence leading to goal states in a deterministic MDP. Two tree search algorithms, obtained by incorporating a stochastic policy (constructed using reinforcement learning in the paper) as guidance in the Levin search and Luby search, are presented, with complexity analysis. The algorithms are evaluated on 1,000 computer generated levels of Sokoban.  Comments: - Line 56: T(n) not been defined yet. - LevinTS is obtained from Levin search by taking 'computation time' as 'depth of a node', and LubyTS seems to be a direct application of Luby search. It'll be helpful to describe what the novelty is, particularly on the analysis, which is a major part of the paper. (I'm not familiar with Levin search and Luby search) - In the experiments, LubyTS does not perform well as compared to LevinTS and LAMA. For LevinTS, it is not clear whether the stochastic policy obtained using reinforcement learning is helpful. In fact, LevinTS using a noisy version of the stochastic policy performs slightly better, and suggests a need to compare with LevinTS using uniform stochastic policy.   *** After rebuttal The paper contains interesting ideas, but I still have some concern on the effectiveness of the proposed approach because the experimental evaluation was limited to one problem and had mixed performance (LubyTS does not perform well, and LevinTS traded off time for shorter solutions as compared to LAMA). That said, I won't be upset if the paper is accepted.