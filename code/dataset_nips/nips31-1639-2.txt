Summary ===============  The authors tackle the problem of crowd-clustering in a semi-supervised setting. They use a deep generative model, more specifically a density network, to model the instances’ features. This is combined with a latent variable model to represent the noise in the labels provided by the different annotators. The propose a structural variational inference algorithm based on natural gradient descend to infer the expertise of the annotators, the ground truth (for both, the instances of the training with and without annotations) and the parameters of the deep generative model that is used to predict the ground truth of new unobserved instances. The novelty of the paper lies in the “instance model” (the relation between the ground truth clustering and the observations) but relying in state-of-the-art deep generative model. However, there is no novelty in the “annotation model”  (relation between the ground clustering and the annotations). The experiments need more details to evaluate the fairness of the comparison against the different baselines they propose.  Details ===============  The model proposed in the paper can be divided in the “annotation model” (generative model of the annotations given the latent ground truth clustering) and the “instance model” (generative model relating the ground truth clustering with the features of the instances).  1) Annotation model: The authors adapt the method proposed by David and Skene for classification to the problem of clustering: each instance is associated with a latent variable representing the ground truth label (cluster the instance belongs to). Based on this unobserved ground truth partition and the latent expertise of the annotators (sensitivity and specificity) the annotations are generated following a Bernoulli distributions.  This model is not novel: it has been previously been proposed in the literature [3] Multiple clustering views from multiple uncertain experts (ref. in paper).  2) Instance model: Here is where the novelty of the paper lies (from a modeling perspective). Instead of using a discriminative model (p(x|o)) they use a generative model (p(o|x)). This allows the authors to take advantage of unlabeled instances.  The paper is clear and well-written however the details given in the experimental section are not enough to understand why the method is outperforming the baselines.   a) The authors do not give any details about the deep generative model they use for modeling p(o|x) (Experiments are not reproducible with the data provided in the paper. Although the authors claim that the code will be publish, these details should also appear in the paper or in the supplementary material). Secondly, it is not clear to me whether the method is performing better because of the generative nature of the “instance model” or just because this model is just most powerful. What would be the outcome of [3] if it is trained with a state of the art DL network? Given z (or an estimation of z) the problem is a standard semi-supervised learning algorithm and a better semi-supervised model will perform better. b) The annotations are artificially simulated: The simulation is a simplification of reality that does not take into account several effects (correlations between the annotators, homogeneous expertise of the annotators across the instance space, not-uniform distribution of annotations across instances/annotators …).  From the inference perspective, the authors resort to two different methods: Amortized structured variational inference for the SCDC method (similar to the semi-supervised variational auto encoder) and a natural gradient with a recognition network for the BayesSCDC that it is nicely motivated by the fact that it avoids the linear scaling with the number of clusters. More details about the inference and the recognition network should be provided, either in the paper or in the supplementary material.