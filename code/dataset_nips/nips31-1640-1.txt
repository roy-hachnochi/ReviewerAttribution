This paper has two contributions to the committee machine, which is a two-layered neural network with a fixed hidden-to-output connectivity and trainable input-to-hidden connectivity.  First, Author(s) proposed a rigorous proof of the replica formula by using the adaptive interpolation method. Second, they derived the AMP algorithm for this model and mainly argued the computational gap, which implies that it is hard for polynomial algorithms to reach the information-theoretical performance.   The rigorous proof part (Section 2 &4) seems to be a bit too technical,  and it might be hard to collect a great deal of interests.  However, this types of theory would be useful to more develop the statistical mechanical frameworks to analyze learning systems. To collect more general interests,  I think that it would be better for Author(s) to give comments on the following things.  * How trivial is it to generalize the committee machines to ones with bias parameters (that is, local magnetic fields in statistical physics)?  In particular, I am wondering whether the phase transition suffers from any qualitative change.   *Recently, studies in machine learning have intensively analyzed the existence of spurious local minima in shallow networks. For instance,  in shallow ReLU networks,  if the teacher and student networks have the same number of hidden units, there exist spurious minima [Safran&Shamir ICML2018]. In contrast, the number of hidden units of the teacher net are less than that of the student net, the spurious minima empirically disappear. Other related works are also mentioned in this paper.  I think it would be better for Author(s) to mention this topic and to relate it to your results. In particular, I am wondering whether the computational gap occurs because the AMP gets stuck in the local spurious solutions or not.  * Does the AMP (algorithm 1) always converge to a unique solution?   It would be better to mention the suboptimality or possible explosion of the algorithm if they might happen.   Typo (probably): in Line 188, lower error that -> lower error than 