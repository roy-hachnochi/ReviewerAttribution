This paper evaluates popular semi-supervised learning (SSL) algorithms, namely, Pi-Model, Mean Teacher, Virtual Adversarial Training, Entropy Minimization and Pseudo-labelling in a unified manner. The authors point out several issues in the current practices for evaluating SSL algorithms, like not using the same budget for identifying hyper-parameters for the fully-supervised baseline, using an unrealistically large validation set to tune hyper-parameters, etc. Based on extensive numerical experiments on CIFAR-10 and SVHN datasets, the authors recommend best practices for evaluating SSL algorithms as well as suggest settings where SSL makes sense.   Strengths: • The paper is well-written. The sections on summary of key findings and recommendations are useful to the SSL community.  • The experiments on varying the amount of labeled and unlabeled data, class mismatch between labeled and unlabeled examples are a nice contribution. • The unified implementation used in this work is made available to the public.  Comments: • At various places, key statements are highlighted in bold font, which is quite unusual and distracting. These statements are clearly summarized in the beginning as well as the end of the paper, so the use of bold sentences throughout the paper seems rather unnecessary. • Line 161: The line should read “for more details”. • In Section 4.2, the meaning of the phrase “averaged over 5 runs” is a little bit ambiguous. I am not sure which of the following is true: o The model is trained 5 times to calculate 5 error rates, and the average error rate is reported. If so, then how many models are trained for the results displayed in Table 1? o The predictions are averaged across 5 runs, and the error rate of the resulting ensemble of models is reported. If so, it is not fair to compare this with the results in Table 1, as ensembling will improve the performance of SSL algorithms as well.  • A few numbers describing the supervised learning baseline results shown by others would help back up the statement that the baseline results are often underreported (similar to what is done in Table 3).