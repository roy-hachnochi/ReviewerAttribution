Summary of the paper: This paper presents a hierarchical reinforcement learning method for training two-layer hierarchical policy. The higher-level policy determines the sub-goal and the lower-level policy tries to achieve the given sub-goal. Both policies can be trained in an off-policy manner.   Relation to previous work: The policy structure is similar to the one in Feudal network, but the sub-goals generated by the higher-level policy in this work is in the raw form.  Most of the prior work on HRL is on-policy, but this paper presents off-policy training of a hierarchical policy. The policy update in this work is based on TD3.  Strength: - Since most of prior work on HRL is on-policy, presenting an off-policy method for HRL is one of the contribution of this work. - The experimental results show that the proposed method outperforms existing HRL methods in fairly complex tasks with continuous control.  Weakness: - Theoretical contribution is minor. There is no novelty in the policy update and the policy structure. The proposed method looks like a minor modification of FuN to make it work in an off-policy manner.  - It seems that authors have employed various tricks to make the method work in re-labeling the goal.  - The benefit of the off-policy correction is not sufficiently discussed in the experiment section.   