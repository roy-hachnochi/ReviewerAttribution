This paper introduces a new method for visual navigation based on language description.  The work effectively exploits the usefulness of "pragmatic speaker" for the language-and-vision navigation problem, during training stage and test phase. The three contributions: 1) Speaker-Driven Data Augmentation, 2) Speaker-Driven Route Selection, and 3) Panoramic Action Space are clean, easy to understand, and effective. The paper is clearly written, and I really enjoyed reading it.   a few minor comments:    The value of some parameters are chosen without much discussion. For instance, the augmented data adds both useful information and noise into the system. A careful trade-off needs to be made. An evaluation on the performance as a function of the ratio of augmented data will be very interesting.   In line 193, the "built-in mapping" needs further explanation and specification.   In [a], the authors use visual context to rescore the candidates of referring expressions generated by beam search from speak's spoken language for the task of 'object referring in visual scenes with spoken language'. It is relevant to this work and should be discussed.   [a] "Object Referring in Visual Scene with Spoken Language", Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool, WACV 2018. 