This paper presents a neural network module that operates on a graph to perform multiple steps of relational reasoning. The method is simple and general (in terms of you could choose other modules to compute the representation of the objects and also the node updates) and uses message passing for relational reasoning.   My concerns are:  1.The proposed module and the message passing style computation is quite similar to other graph neural networks, so not sure this is very novel.   2.The evaluations are mostly done on synthetic datasets. The increase in performance on 6 and 7 steps in Figure 2b is concerning and indicates the flaws in the proposed "Pretty-CLEVR" dataset.   3.The evaluation on Sudoku is interesting. If you drop the position information and use only the edges and the number in the cell as input, how much would the performance drop? In my view, this makes the question more “relational”.   4.Although the model is general and in theory can be combined with complex neural modules. The representations in the evaluated domains are simple feature vectors. bAbI uses LSTM, but the language is quite simple and synthetic. It would be interesting to see how this module works with complex neural modules to deal with real images or natural language.   --------------  I have updated the overall score from 5 to 6 since the author feedback addressed some of my concerns. 