This paper deals with designing a new neural network structure for problems involving one or more steps of relational reasoning. The authors propose a generalization of the Relational Network (RN) architecture proposed by Santoro 2017. Whereas RN does one step of processing on a fully-connected graph composed of nodes representing objects, the Recurrent Relational Network does multiple time steps of processing, maintaining a hidden state per node and parametrizing messages between nodes at each time step as a MLP.   The new architecture is evaluated on several benchmarks. The first is bAbI, which is a popular text-based question-answering dataset consisting of 20 different type of tasks, each of which involves receiving several supporting facts and answering a question related to those facts. The proposed method solves all 20 tasks and seems to display less variability between different training runs in answering these questions compared to other published methods. The authors observe that bAbI is solved by their model without requiring multiple time-steps of processing in their model and so create a more difficult variant of CLEVR dataset (called Pretty-CLEVR), which is specifically built so that questions have varying degrees of reasoning. The authors evaluate their method, while varying the number of time steps of processing, and show that as they run the Recurrent Relational Network for more time steps, it is able to answer more difficult questions (in terms of more degrees of reasoning being required). Lastly, the method is evaluated on solving 9 x 9 Sudoku puzzles and achieves higher accuracy than a simple convolutional network and other graphical model based methods.   Pros The proposed model has a simple definition but seems powerful based on its performance on various benchmarks. The Pretty-CLEVR experiment explicitly shows the benefit of allowing more time steps of processing with regard to questions that require more degrees of reasoning  Cons Why not evaluate method on CLEVR dataset to see where method’s performance is relative to other published methods?  Comments  The model suggested in this work bears some similarity to the one used in Sainbayar 2016, which involves learning communication between multiple agents performing an RL-based task and the communication vector for one agent there is similarly summed across all other agents. Might be worth mentioning in the related works section. For the methods evaluated against in the Sudoku benchmark, are the results there from listed papers or based on implementation of those methods by the authors in order to compare? It would be good to make a note if any of the results are based on author’s implementation.  Santoro, Adam et al. A simple neural network module for relational reasoning. 2017. Sukhbaatar, Sainbayar et al. Learning Multiagent Communication with Backpropagation. NIPS 2016.