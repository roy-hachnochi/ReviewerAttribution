The authors prove that under low noise assumptions, the support vector machine with a few random features (RFSVM) (fewer than the sample size) can achieve fast learning rate,  extending the previous fast rate analysis of random features method from least square loss to 0-1 loss.   The results are interesting, assuming that Assumption 2 is reasonable and the proof is correct.  The presentation of the paper needs to be polished a bit.  I only have one major comment.  Is there any practical example for Assumption 2. What will Theorem 1 be without Assumption 2?  ==After rebuttal==== I am a bit disappointed that the authors did not manage to address my comment about Assumption 2. The first part of Assumption 2 (on the random features) is less common, although it has been used in the literature. I kept my evaluation unchanged and I agree that the paper should be accepted as there are some interesting results. But more elaboration on Assumption 2, or deriving results without Assumption 2 would make the paper a great one.