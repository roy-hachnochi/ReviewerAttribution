The paper presented a pipeline that is able to effectively search in a program-configuration space for more performant tensor programs, under various operator types and hardware architectures. Although methodology wise this paper doesn't introduce novel techniques, I still find the overall execution (i.e. piecing together the many different components with sensible choices of techniques and demonstrating convincing empirical improvements through carefully designed experiments) impressive and believe it will have a tangible impact.  Detailed comments: * When you say "Each choice creates a logically equivalent program ..." under Figure 1, is it because, **by design**, none of the configurations would change the logic of the program (in which case I think this worth a more explicit emphasis)?  * The actual search space is missing a more detailed and formal breakdown (except from the verbal descriptions on L.75 - 76 on P.2).  * I'd like to see a more direct evaluation of the performance of the learnt statistical cost model itself (local or global), i.e. in terms of metrics like training/test error, ranking loss, etc.  * The way the estimated running costs are used in Eq.(3) (the summation and the balancing against "coverage" with alpha) seems to suggest the actual scales of the cost values do matter in this case, while the ranking loss in Eq.(2) is shift-invariant, which may introduce a slight discrepancy in the modeling setup (and hence partially contribute to why this "diversity-aware" exploration didn't play out as effectively as expected)? Also, how is alpha decided?  * When you say "the search space specification can change for different a workload" in L.136 on P.4, it's not immediately clear within the context indicated by Table 1 why it should be the case. If you are implying a different type of workload (e.g. "Matmul"?) here please make it clearer.  * In Eq.(4), is the global model being kept fixed during transfer learning or is it being updated alongside the local model?  * 2 curves seem missing in Fig.9 (c) and (d), and as a result, the claim that "The flattened AST features ... operator types" seems ungrounded.  * In Fig.10 (a), none of the results have improved over the baseline for C7 on ARM Mali-T860. Any explanation why?  * "operator fusion" is mentioned twice in the paper without much explanation nor reference. Also it would be nice to have a breakdown of the speed improvement with/without "operator fusion" just to have a more detailed picture.