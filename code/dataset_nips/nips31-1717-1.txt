This paper proposes to optimize tensor programs, specifically, perform automatic implementation generation for a certain set of Tensor programs (declared by users) in order to improve the execution efficiency over the (kernel) implementation provided by existing tensor computation libraries.  The problem is interesting and the framework proposed in this paper seems to be technically sound. The evaluation of this paper contains three parts of results: (1) empirical studies for the specific design choice of each component of the framework (2) verifying the improvement of transfer; (3) improvement on end-to-end DL workloads. The experiments reveal that the proposed framework can result in considerable improvement on efficiency.  My major concern about this paper is how useful it will be compared to using a static tensor library. Per my understanding, given a specific tensor program and a hardware configuration, regardless of whether transferring is enabled or not, the proposed framework has to perform substantial training for each parameterized modules in order for it to work, which I believe will yield substantial overhead compared to the improvement over static libraries that the framework can bring. How much exactly is the cost?  Also, once the hardware changed or the tensor program changed, the previously built model on a specific program or a specific hardware environment is not adaptable, meaning the parameters probably need to be re-trained. While static tensor computation libs (e.g. CUDA, MKL) usually are optimized for a certain (board) group of hardware and software programs (i.e., therefore, one can always anticipate a certain amount of benefits by using them, as long as within its scope, such as CUDA for most GPUs).  An argument made by the authors is that the “Learn to optimize Tensor Programs” frameworks can enable the automatic generation of some math operations that are not covered by de facto tensor computing libs. But how many of these operations exist and how often are they used? My experience with deep learning is that nowadays the development of DL models and the deployment of production-level DL code/workloads have been standardized a lot and those operators that are less optimized are rarely explored. While “Learning to optimize tensor program” offers some automation on generating better kernel implementation for some long-tail operators, it is questionable how useful it is in practice.   - Other questions The current framework is built by collecting data through experimental trials, and learning a cost model based on it and then minimizing certain objectives given the cost model. I am wondering whether it is possible to build the framework based on reinforcement learning?  The current framework seems to be built to be friendly with inference, Will it be useful for training, and how useful it will be (considering the problem I raised in my previous comments)  - Writing: My second major concern about this paper is about this presentation. This current writing assumes a lot of out-of-domain contextual knowledge, which tends to prevent most NIPS audiences from clearly understanding technical details. Below are a few places. There are more places that need to be clarified.  *L26: What do you mean by “preventing kernel fusion that can produce unsupported operators”  *L42: Why we need to enable automatic code generation? I feel some context is missing here. Per my understanding the goal of learning to optimize a tensor program is to eventually be able to automatically generate code; you probably want to point out it for the context.  *L46-47, what is S_e and what does s \in S_e specifically map to? It would be good to define it first and provide more details and context.  *L87: what is AST?  *L102: Could you clarify in L102 what is the “selection process” and why “in the selection process, we only care about the relative order of the run times of programs rather than their absolute values”?  *In section 5 experiment, could you explain in detail for each figure what metrics in Y axis are and how they are generated (specifical experimental configurations)? For example, in Fig 5 - 7, how are the TFLOPS values generated? Are they the number of TFLOPS of a learned implementation compared to random implementations?  *It would be good to provide more detailed experimental configurations in sec 6.3 about the NNs and the details of the figures. In Figure 11, is the time in ms (in Y axis) for a single inference pass?   - Post rebuttal After reading the authors' response and the other two reviewers' comments, I am willing to adjust my score to be 1 point higher (from 5 to 6). The authors partially answered some of my questions about the adaptability of this work; While I am not fully convinced on this will be a competitive alternative to libraries such as CUDA/cuDNN (due to hardware-specific optimizations and running cost of the data-driven approaches), I think this work poses an interesting problem and a sensible solution to it.  For concerns (Reviwer 1 and myself) on presentation, I think it could be improved in the camera-ready version (and I would urge the authors to do so).