The authors consider allowing more flexible variational approximations in black box variational inference, borrowing ideas from boosting/Franke-Wolfe optimization. The key contributions are theoretical: looser conditions for guaranteed convergence are found allowing more straightforward implementation, degenerate solutions are avoided via regularizing the entropy, and a bound on the primal error is used as a principled stopping criterion. Promising empirical results are presented on a) a simple synthetic mixture b) logistic regression c) matrix factorization.   The paper is well-written and clearly explained. Having not looked carefully at boosted VI before however it would be helpful if the paper explained how elements in conv(Q) are represented explicitly. I think this is as a mixture of elements in Q, with a new "component" added at every iteration. However this appears not to be the case for the matrix factorization example? The intro on vanilla VI could be compressed to allow for this discussion (I think most people know that material by this point). As a minor point you should specify how the 3 variants in fig 1 correspond to those in algo 1 (I know it's pretty obvious but anyway).  The empirical results all seem pretty encouraging and I'm pleased to see the authors used Edward rather than trying to roll their own solution (unless they're Edward people in which case fair enough too). Please do make code available.   It would be interesting to see some discussion/quantificaiton of the copmutation/memory/storage requirements as the complexity of the variational posterior grows through the algorithm run. Does this limit applicability to larger models?   Overall I felt this paper made solid theoretical contributions and demonstrated their empirical utility sufficently well. 