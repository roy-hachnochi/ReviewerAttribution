This paper aims to infer a 3D representation of a scene in terms of a 3D bounding box per (detected) object (represented via rotation+size+translation), the scene layout (box-like extent), and the camera pose w.r.t. the scene. In addition to independent (regression) losses for each of the predictions (typically what previous methods rely on), this paper also leverages some joint 'cooperative' losses for the training. In particular, a) the per-object rotation/translation/size predictions should yield the correct 3d box,  b) the 3D box when projected via the estimated pose should project to the known 2D box, c) the 3D boxes should not lie outside the layout. The experiments indicate the benefits of these additional losses and the paper shows improvements over previous methods for both, layout estimation and 3d object detection.  This is a generally well written paper. The primary contributions of the work are the additional joint losses that tie together the global and per-object predictions. I feel that these are simple but novel additions to various existing approaches that typically use individual terms, and that the experiments justify the benefits of these additions. Further, I feel the parametrization of the translation of an object is also a clever detail (though similar to concurrent work '3D-RCNN' by Kundu et. al, CVPR 18).  An additional interesting aspect is that the approach can allow learning (not as good, but still meaningful) 3D box prediction using only 2D supervision (S4 in Table 4). I wish the paper provided more visualizations and emphasis for this setting.  One aspect I'm not sure about is the difference between the 'world frame' and the camera frame. I'm assuming that the 'world frame' is some axis-aligned frame, where the room is along manhattan directions, and the 'camera pose' In Fig 2 Is w.r.t to this. I am not sure why the layout etc. cannot just be predicted in the camera frame - this would remove the need to infer a global camera pose at all.  Additionally, while I generally like the paper, I am hesitant to give a strong acceptance rating given the paper's contributions. While the additional losses are neat and might be applicable to other general scene 3D prediction work, they are arguably obvious additional terms, and I'm also not sure if these contributions are best suited for a learning conference. 