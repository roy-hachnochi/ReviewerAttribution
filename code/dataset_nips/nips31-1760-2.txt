In this paper the authors consider actor-critic algorithms for POMDPs in a multiagent setting. They derive policy update rules, prove convergence properties of those and evaluate those learned policies empirically.  I am not an expert in the field of multi-agent RL but got the feeling that the presented results are novel and extend current knowledge in an interesting and valuable way. Although there is no clear advantage of the proposed algorithms empirically, I think the authors make a sensible step forward by enabling the use of model-free algorithms for this multi-agent setting under partial observability.  The paper could be improved by cleaning up notation (e.g. u_i in line 87, REG without subscripts, symbol in line 187 .... are never defined --- their definitions following implicitly). I would also recommend to give some more details on CFR to make the paper more self contained as I had a hard time following without referring back to the original paper.  Furthermore it would make the paper more compelling if the proposed algorithms were studied in more detail, e.g. highlighting sensitivity to the hyperparameters. For instance, in section 4.4 the authors comment on the need to interleave policy improvements with multiple updates to the critic. It would be helpful to report the authors precise findings (and the actually used update frequencies).  --  I have read the author response and the authors have clarified my questions. Upon another reading of the paper and the other reviews, I decided to increase my score to reflect the relevance of the paper.