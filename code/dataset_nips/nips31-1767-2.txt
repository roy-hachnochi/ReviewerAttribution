The paper provides an interesting analysis of gradient descent for deep linear networks as well as ReLU networks with only one hidden layer. Both cases use squared loss. The paper is well-written and well-organized and builds on tools from discrete-time dynamical systems. The paper has several interesting (and to the best of my knowledge original) components: 1. Providing upper bounds on step size for reaching each solution of a linear network. 2. For learning positive definite matrices by linear networks, identity initialization of weights allows the largest step size. 3. Analysis of the solution obtained by linear networks initialized by identity weights when the target matrix has negative eigenvalues. 4. Relationship between step size and model's output for a single hidden layer network with ReLU activations.  Question: Q1. Figure 2 shows with the smaller learning rate, the estimated function converges to a solution, and with the larger one it oscillates. I am wondering if the convergence/oscillation claim here is made based on mathematically analysis the resulted dynamical system or is judged based on numerically computed notion residual? If the latter, couldn't it be the case that the oscillation are not really perfect and are slowly converging to the target function? Since your point here is not speed of convergence, but rather realization of a perfect convergence or oscillation, numerical assessment may be misleading if that is what you are doing.  Q2. The paper has a general result for deep linear networks to learn an arbitrary nonzero matrix (Theorem 1 and Corollary 1). Specifically, in Corollary 1 authors provide an upper bound on the learning rate based on the largest singular value of the matrix. Later in the paper, when they add further assumption that the matrix is positive definite and weight matrices are initialized by identity, the step size bound becomes tighter. I am wondering if the authors have any intuition about what a worst case matrix and initialization would be for the general linear network case, to get a better understanding of the quality of the general-case bound.  Minor comment: In Example 1, the specification of the set S as S={0,d^2,-d^2,...} is ambiguous. Adding one more pair before "..." would help.  POST REBUTTAL: I had some minor questions which the authors replied. My rating for the paper is 8.