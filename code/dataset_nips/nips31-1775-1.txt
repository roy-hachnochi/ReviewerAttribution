The authors propose and analyze zeroth-order stochastic approximation algorithms for non-convex and convex optimization. Specifically, they analyze a classical version of CG algorithm and provide results on the convergence rates in the low-dimensional setting; they show a modified algorithm and show that it attains improved rates in the low-dimensional setting; and finally, they consider a zeroth-order stochastic gradient algorithm in the high-dimensional nonconvex (and convex) setting and illustrate an implicit regularization phenomenon. The authors show that this algorithm achieves rates that depend only poly-logarithmically on dimensionality.   My detailed comments are as follows:  1. Overall, the paper is very well written and was a pleasure to read. It was clear in its assumptions, definitions and notations and was very easy to follow. 2. The work will be of significance to a lot of readers of the NIPS proceedings especially to the optimization community since it gives an improved algorithm for convex functions in a low-dimensional setting. Moreover, they give an improved bound for the Zeroth-order stochastic Gradient Method for non-convex problems by considering a sparsity assumption. 3. Since this is a theoretical paper, pushing all proofs to the supplementary section makes it a bit hard for the reader. If the authors can give an overview of the proof and point the reader to the details, that would make the paper even better. 4. In section D, the authors use a different termination criterion than the FW-gap. Do earlier results also change with a different termination criteria? Some insights on this would greatly improve the paper.  Minor Comments: 1. Algorithms 1 and 3, should also have the input of alpha_k, which is missing.