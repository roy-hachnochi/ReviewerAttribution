The authors proposed and analyzed zeroth-order stochastic condition gradient and zeroth-order stochastic gradient descent algorithms for nonconvex and convex optimization. They also established the iteration complexities for the proposed algorithms.  Pros The zeroth-order stochastic condition gradient algorithm is the first algorithm that explore the stochastic condition gradient by merely using the zeroth-order information of objective. The iteration complexities are well-established.  Cons (1) The parameters in the proposed algorithms, such as $\mu_k$ and $m_k$, all depend on the pre-given maximum iteration $N$, which may be hard to estimate an approximated parameter $N$. Why not use adaptive parameters $\mu_k$ and $m_k$ that are independent of the maximum iterations $N$ ?  (2) In Theorem 2.1 and 2.2, to reduce the variance and obtain a better iteration complexity rates, the authors require the sample size $mk$ to be $O(N^2)$ and $O(N^3)$, respectively. If $N$ is large, it requires too many function queries.  (3) The authors did not provide the iteration complexity of the Algorithm 5(Tuncated ZSGD). Hence, in Table 1, the iteration complexity for Truncated ZSGD should be suitably corrected.  (4)  In the Algorithm 5, the authors considered an additional projective step onto the sparsity constraint $||x||_0 <= 0$. The iteration complexity for Tuncated ZSGD algorithm is missing. (5) Although the main contributions of this paper are focused on the theoretical iteration complexities, we strongly recommend the authors providing additional experiments to evaluate the efficiency for the new zeroth-order stochastic condition gradient algorithms.  