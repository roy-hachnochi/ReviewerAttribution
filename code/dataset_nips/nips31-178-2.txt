This paper considers a new model for PAC learning in the presence of what the paper terms “evasion adversaries”.  This setting is one where an attacker changes the examples to generate adversarial examples to make learning difficult.  The main contents of the paper include a notion of dimension for this model and learnability results for half-spaces.  For half-spaces, the learnability result comes from being able to explicitly analyze the newly-defined “adversarial dimension”, which turns out to be equal to the standard VC-dimension; this type of analysis (as is often the case for other notions of dimension) turns out to be non-trivial and extends to other related classes.  Interestingly, at least for me, the new notion of dimension can be larger or smaller than the standard VC dimension.  Why this notion could be smaller confused me at first (since VC dimension is used for standard PAC without corruptions), but this notion has an extra parameter, which does away with the contradiction.  In the end, I think this is a useful and interesting problem to analyze, but I feel the paper oversells the results.  It is not the case, as stated in l.71 “We are the first to provide sample complexity bounds for the problem  of PAC-learning in the presence of an adversary”. Moreover: - For example: given that “adversarial dimension”  is also a function of the nearness relation, I think calling it a dimension is a bit too much.  - The noise condition is reminiscent of others that should have been better placed in context (eg Tsybakov noise and others).  Section 6 does some of job, but is also strangely placed late in the paper. Instead, I think what is interesting is the end-to-end analysis of a particular class in this setting, together with the introduction of techniques that can point to more general progress. But due to some of the criticisms I’ve outlined earlier, my vote is for a “weak accept.”  minor:  - l.169: shatter->shattering - l.182: Massart lemma -> Massart’s lemma - l.216:  “the inequality 1 is tight“ is unclear, as (1) has multiple inequalities. Also the “the” should be removed. - l.229 and later: what are “error patterns”? Do you mean classification pattern? - l.276 “framework of Angluin and Laird which looked at noisy training data” — this is called “classification noise”   - VC dimension capitalization is inconsistent, even across section titles (eg section 4 vs 5)