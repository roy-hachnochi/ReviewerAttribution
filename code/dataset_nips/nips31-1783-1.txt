This works provides a model for GP inference on univariate temporal series with general likelihoods. Using state-space modeling, the complexity is only O(m^3n) for a state of dimension m and n data points. This is further reduced to O(m^2n) using an infinite-limit approximation.  The paper is presented clearly and the authors provide enough information to understand the model in detail. Not many baselines are provided. A relevant reference (and possibly, baseline) is missing: CsatÃ³, L. and Opper, M., 2002. Sparse on-line Gaussian processes. Neural computation, 14(3), pp.641-668.  The formulation of a time series GP as a state space-model with the corresponding computational savings is not novel. Similarly, using EP (or in this case, ADF) to handle non-Gaussian likelihoods is not new.   The novelty of this model is the infinite-limit approximation. This has two effects: a) Decreased computational cost b) Removes the "boundary effect" that increases the uncertainty towards the end of the data  There seems to be little practical advantage to a). The computational cost decreases from O(m^3n) to O(m^2n), where m is the dimension of the state space. For practical cases of interest (for instance, the Mattern covariance with nu=3/2), we have m=2. The reduction in computational cost thus seems negligible. This is even more the case when non-Gaussian likelihoods are involved. Table 1 seems to support this understanding. The authors try to make the case for IHGP showing how the gap in computational cost widens as a function of m in Figure 3. However, the need for, say m > 10, is not substantiated, and it might never be required for good practical performance.  Effect b), the removal of the "boundary effect", is cast in a positive light at the end of section 3. However, the boundary effect is actually providing a more accurate mean and variance by taking into account the lack of knowledge about the future when making a prediction for the next sample. Therefore, one could expect SS predictions to be more accurate than those of IHGP. This seems to be supported by Table 1 again, where the NLL of the IHGP is consistently higher than that of the SS model.  All in all, IHGP seems to sacrifice some accuracy for very little computational gain wrt the SS model, so its practical applicability is questionable, being this the main drawback of an otherwise appealing and well-written work.  Minor: Figure placement is not the best throughout the text. It'd be helpful to place figures closer to where they are cited. Many typos throughout the paper, here are some of them: - We are motiviated - While genral-purpose methods - they they scale poorly - In one-dimensonal GPs  - the filter gain convereges - stablising - insipired - optimization  I have increased my score as a result of the authors' clarifications in their rebuttal.