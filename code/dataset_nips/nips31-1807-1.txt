The paper focuses on discrimination in machine learning. As opposed to the existing work on discrimination-aware machine learning, the goal of the paper is to actually go beyond the traditional discrimination-accuracy tradeoffs. Instead, the paper proposes a framework to pinpoint the precise sources of discrimination in prediction outcomes, and propose interventions that might enhance both fairness as well as the accuracy of the model.  As the paper notes, just constraining the existing (discriminatory) models to remove outcome disparities may not be sufficient. Decreasing accuracy of (one, or) both groups just to achieve equality is definitely an unsatisfactory tradeoff. While previous authors alluded to the possibility of gathering more data to potentially reduce discrimination (https://arxiv.org/abs/1701.08230 and https://arxiv.org/abs/1610.02413), this paper is the first (to the best of the knowledge of this reviewer) to formally tackle this extremely important direction. While the technical contribution is not necessarily huge, the conceptual contribution of the paper would definitely make it a valuable addition to the existing literature on the topic.   Detailed comments:  - hat{Y} in line 61 is defined as a function of both X and A. However, in Figure 1, both the outcomes as well as predictions are thought to be unaware of A (same as the experimental section). How does one reconcile this difference? Does this have any implications for the analysis that follows?  - Figure 1 and Section 3 (until line 113) are very difficult to read. The paper does briefly state that it uses the loss decomposition of (Domingos, 2000), but for a reader not familiar with this framework, it is not entirely clear as to what precisely the Bias and Variance terms defined here are trying to measure. Domingos does provide intuitive explanations for these terms. Perhaps the authors can expand a bit more on these terms, or point the reader to the relevant publication (https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf). Fixing this issue can greatly increase the readability of the paper.  - It would be nice to see discussion on some other clustering techniques to identify regions where the discrimination between the groups is high. Using uncertainty estimation (e.g., in Gaussian processes) might be one way to do that.  - How would the results extend to the statistical parity notion of fairness (https://arxiv.org/abs/1705.09055)?  - While not necessary for this submission, it would be great to see some discussion on how the results would extend to the cases of intersectional fairness (https://arxiv.org/abs/1711.05144) -- the cases when one has more fine-grained groups such as gender and race (e.g., African-American men, White women)  - Though not necessary for the current submission, it would be interesting to see how the proposed strategy of gathering more data (examples or features) would interact with the delayed-impact-of-fairness framework proposed in the recent study by Liu et al (https://arxiv.org/pdf/1803.04383.pdf). Specifically, is gathering more data on the discriminated group likely to lessen the long term stagnation or decline of these groups?  