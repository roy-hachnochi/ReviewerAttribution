The paper describes a technique for distributed computations in settings where the parameters are modeled to lie on a manifold. They derive convergence results for their proposed algorithm and evaluate it on a synthetic problem and the Netflix data.  In my opinion, the big weakness of the paper is its insufficient motivation of the problem, and thus, I fail to see the relevance. Specifically, the authors do not show that the process communication is indeed a problem in practice. This is particularly surprising as typical empirical risk minimization (ERM) problems naturally decompose as sum of losses and the gradient is the sum of the local loss gradients. Thus, the required inter-process communication limits to one map-reduce and one broadcast per iteration. The paper lacks experiments reporting runtime vs. number of processors, in particular with respect to different communication strategies. In effect, I cannot follow _why_ the authors would like to proof convergence results for this setting, as I am not convinced _why_ this setting is preferable to other communication and optimization strategies. I am uncertain whether this is only a problem of presentation and not of significance, but it is unfortunately insufficient in either case.  Here are some suggetions which I hope the authors might find useful for future presentations of the subject:  - I did not get a clear picture from the goal of the paper in the introduction. My guess is that the examples chosen did not convince me that there are problems which require a lot of inter-process communication. This holds particularly for the second paragraph where sampling-based Bayesian methods are particularly mentioned as an example where the paper's results are irrelevant as they are already embarrassingly parallel. Instead, I suggest the authors try to focus on problems where the loss function does not decompose as the sum of sample losses and other ERM-based distributed algorithms such as Hogwild.  - From the discussion in lines 60-74 (beginning of Sect. 2), I had the impression that the authors want to focus on a situation where the gradient of the sum is not the sum of the individual gradients, but this is not communicated in the text. In particular, the paragraph lines 70-74 is a setting that is shared in all ERM approaches and could be discussed in less space. A situation where the the gradient of the sum of the losses is not the sum of the individual loss gradients is rare and could require some space.  - Line 114, the authors should introduce the notation for audiences not familiar with manifolds ('D' in "D R_theta ...")  - From the presentations in the supplement, I cannot see how the consideration of the events immediately imply the Theorem. I assume this presentation is incomplete? Generally, the authors could point which parts explicitely of the proof need to be adapted, why, and how it's done. The authors also seem to refer to statements in Lemmata defined in other texts (Lemma 3, Lemma 4). These should be restated or more clearly referenced.  - In algorithm 1, the subscript 's' denotes the master machine, but also iteration times. I propose to use subscript '1' to denote to the master machine (problematic lines are line 6 "Transmit the local ..." and lines 8-9 "Form the surrogate function ...")  - the text should be checked for typos ("addictive constant" and others) --- Post-rebuttal update: I appreciate the author's feedback. I am afraid my missing point was on a deeper level than the authors anticipated: I fail to see in what situations the access to the global higher-derivatives is required which seems to be the crux and the motivation of the analysis. In particular, to me this is still in stark contrast with the rebuttal to comment 2. If I'm using gradient descent and if the gradient of the distributed sum is the sum of the distributed gradients, how is the implementation more costly than a map-reduce followed-up by a broadcast? For gradient descent to converge, no global higher-order derivative information is required, no? So this could be me not having a good overview of more elaborate optimization algorithms or some other misunderstanding of the paper. But I'm seeing that my colleagues seem to grasp the significance so I'm looking forward to be convinced in the future.