The paper presented a way of learning statement embeddings and demonstrated its effectiveness through empirical studies (from analogy tests and as inputs to downstream tasks around code comprehension).  The paper is largely easy to follow and well written, although I'd still like to see improvements in the following aspects  - the main technical novelty here seems to be defining the proper "context" and "granularity" for statements to then allow the skip-gram model to be naturally applied. I would therefore like to see some comparisons between the different configurations (even all under the same framework proposed in the paper, so sth. like an ablation study), e.g. when only data or control flow is considered, to gain more insights/justifications for the adopted paradigm.  - in L.200 on P.6, please elaborate what you mean exactly by "colored by statement and data type".  - for the 3 code comprehension experiments, please clarify whether the embedding layer, once initialized from inst2vec embeddings, is kept fixed during training, or still being fine-tuned along with the other network parameters.