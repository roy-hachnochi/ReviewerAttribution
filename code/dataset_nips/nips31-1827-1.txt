The aim of this paper is to show that deep neural networks can generalize well on small datasets, contrarily as what is accepted in the community. To do that, deep neural networks are presented/interpreted as ensembles of poor classifiers and this is achieved creatively by restricting the power of representation of the subnetworks. The paper is well organized and presented.  Mathematics is sound and clear and the experiments are well chosen, including a toy model problem. The results of the restricted neural networks (with and without dropout) are compared to a random forest classifier. The paper can be a very interesting contribution to the conference and can be accepted as is.