This paper describes Snap ML, a framework that improves training time for generalized linear models in a distributed computing setup comprising of CPUs and GPUs using large datasets. The contributions include communication-cost aware design that allows for significant gains in the training time compared to existing relevant frameworks.   There are several strengths of this work:  - Snap ML builds on multiple state-of-the art algorithmic building blocks that allows it to make progress leveraging existing advances.  - The evaluation is thorough. The authors have used a large dataset to evaluate the work and have provided a comparison with other existing approaches. SnapML achieves significant gains in improving training time.  - The paper is generally pretty readable, but could use some more finishing. Especially the description of convergence in section 2.1 did not seem entirely clear (this might be due to space-limitation).  There are also some feasible area of improvement:  - I suggest devoting some space to explicitly note the challenges in implementing Snap ML starting with the non-hierarchical version of CoCoA.  - Since Snap ML enables cloud-hosted deployments for training of generalized linear models, a discussion on the inherent performance variability observed in public cloud environments (due to shared, commodity hardware, and lack of complete control over the underlying resource to the users) should help make a stronger case.  - Some experiments in public cloud setting, such as AWS EC2, using VMs that offer GPUs should definitely make it more convincing.  - A comparison with standalone manycore architecture should strengthen the paper. 