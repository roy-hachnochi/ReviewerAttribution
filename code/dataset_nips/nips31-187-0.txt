Paper Summary: This paper presented a novel approach that performs chain of reasonings on the object level to generate answer for visual question answering. Object-level visual embeddings are first extracted through object detection networks as visual representation and sentence embedding of the question are extract question representation. Based on these, a sequential model that performs multi-steps of relational inference over (compound) object embeddings with the guidance of question is used to obtain the final representation for each sub-chain inference. A concatenation of these embeddings are then used to perform answer classification. Extensive experiments have been conducted on four public datasets and it achieves state-of-the-art performance on all of them.   The paper is well written and easy to follow in most parts. Extensive experiments has been done to thoroughly studied each component of the proposed algorithm. Qualitative visualization in the section 4.5 provides an intuitive understanding about the model's behavior of reasoning.   Minor Comments:  1. In the equation (10), it seems there is no subscript k in the main body of the equation? What does the k in summation refer to?  2. It seems like some implementation details are missed in both the main paper and supplementary material. For example, does the Faster RCNN need pre-training? On which dataset? Is the final answer classification network predicting the top-frequent answers or entire answer space? Although authors guarantee to make source code available, it would be nice to list those specific details in somewhere (e.g supplementary material ). 3. It would be nice to see some results on CLEVR, to evaluate model's performance based on different types of the questions, and also its performance on the CLEVR's compositional split, as a diagnosis of model's capability.  Post Rebuttal Feedback:  Thanks for the feedback. I believe the authors have addressed most of my concerns raised in the review and glad to see that CoR2 outperforms relational networks on CLEVR. It would be great if authors could report detailed results with respect to the question types on the CLEVR.   I believe that a revised version that fix most concerns reviewers has raised (e.g. CELVR results, implementation details, self-contained content, typos, etc) would deserve an acceptance decision. 