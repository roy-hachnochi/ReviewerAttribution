In this paper, the authors propose a novel variance reduced zeroth-order method for nonconvex optimization, prove theoretical results for three different gradient estimates and demonstrate the performance of the method on two machine learning tasks. The theoretical results highlight the differences and trade-offs between the gradient estimates, and the numerical results show that these trade-offs (estimate accuracy, convergence rate, iterations and function queries) are actually realized in practice. Overall, the paper is well structured and thought out (both the theoretical and empirical portions) and the results are interesting in my opinion (for both the ML and Optimization communities), and as such I recommend this paper for publication at NIPS.  - The paper is very well written and motivated, and is very easy to read.   - There are some questions/comments that arise about the numerical results presented in the paper: — How is \mu (finite difference interval) chosen? — How are the step lengths chosen for all the methods? — A comparison with other derivative-free optimization methods (e.g., finite difference quasi-Newton methods and/or model based trust region methods) would be of interest. — The authors should show Black-box Attack loss versus Queries (function evaluations) for the second experiment. — The authors should mention how u is chosen in the experiments.  - Other minor comments: — The proposed method has many similarities to ZO-SVRC [26]. The authors should clearly state the differences both algorithmic and theoretical. — The authors mention that “this seemingly minor difference yields an essential difficulty in the analysis of ZO-SVRG”, which of course is due to the biased nature of the gradient approximations. Is it fair to say that this is due to the errors in the gradient estimates. If so, the authors should mention that this is the root of the difficulty.  — Caption of Figure 3: versus iterations -> versus epochs — Line 287: Sentence starting with “Compared to ZO-SGD,…” Do the authors mean faster convergence in terms of iterations?  — What do the authors mean by “accelerated variants”? Accelerated because of the better gradient approximations? — Do the results of Proposition 2 (and as a consequence all the results of the paper) assume precise arithmetic? If so, this should be mentioned by the authors in the paper. Moreover, these results assume that the random directions u are uniform random variables over the Euclidean ball. Could these results be extended to other distributions? — Is it correct to say that CoordGradEst gradient approximations are stochastic (based on subsample of size b) Central Difference approximations to the gradient? — The authors should mention over what the expectations in the theoretical results are taken. — There is proliferating number of derivative-free optimization methods (e.g., finite difference quasi-Newton methods, direct and pattern search methods and model based trust region methods) in the Optimization literature. The authors should mention some of these methods in the paper, and discuss why these methods are not considered.