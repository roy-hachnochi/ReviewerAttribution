This paper studies the use of variance reduction technique in zeroth order stochastic optimization. The paper particularly focus on plugging in couple of two point gradient estimator in SVRG, e.g. average random gradient estimator and coordinate gradient estimator. Overall, the results make sense and are interesting.  The paper is easy to follow.   1. It is a bit difficult to consume some parameters with a long and complicated equation in those theorems. If they are not critical to understand the theorem, they could be simpler. I like Table 1.   2. If combining random coordinate descent and SVRG, the d times more function queries could be reduced to 1.  You can refer to the following paper for combing random coordinate descent and SVRG.  Randomized Block Coordinate Descent for Online and Stochastic Optimization Huahua Wang, Arindam Banerjee