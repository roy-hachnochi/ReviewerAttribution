This paper considered accelerating smooth convex function optimization via reduction to computing the Nash equilibrium of a particular zero-sum convex-concave game. The authors showed by using optimistic follow the leader, they can achieve a rate of convergence to Nash equilibrium to O(1/T^2), broadening the class of cost functions where this result holds compared to [1]. Furthermore, this result can be translated to the acceleration rate for minimizing smooth convex functions via Fenchel game. As a result, the authors showed that the algorithm derived in this framework with certain parameter setting coincide with the Nesterov acceleration algorithm. In addition, the authors showed that by modifying the parameters, they can derive several acceleration algorithms (e.g., HeavyBall, Nesterov’s 1-memory and \inf-memory), thus unifying them under the same framework.    Overall, I enjoyed reading this paper, the presentation is clear, and I think it is interesting to the machine learning/optimization community to get a better understanding about the proof and intuition behind Nesterov’s acceleration results. This paper provides a neat and relatively simple framework to tie in a few widely recognized algorithms, which is beneficial for more researchers to understand the proofs of these algorithms. 