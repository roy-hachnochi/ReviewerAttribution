This paper studies the Lipschitz constant of deep neural networks, proving that an exact computation is NP-hard and providing an algorithm for computing an upper bound using automatic differentiation. The proposed method and its variants outperform baselines and provide the first bounds for some large-scale networks used in practice.  The Lipschitz constant of deep neural networks is an important quantity characterizing sensitivity to small perturbations in the input, which may be relevant for understanding aspects of trainability, generalization, and robustness to adversarial examples. For these reasons I think the topic of this paper is salient and the tools and ideas developed here will be useful and interesting to the deep learning community. Although I did not review the proofs in detail, the analysis appears sound and the paper is well-written.  One important question is how important this type of worst-case analysis is for practice, i.e. do typical real-world high-dimensional data distributions typically include datapoints for which the maximum singular value of the Jacobian is close to the Lipschitz constant? I could imagine that typically these worst-case inputs are very unnatural, and that datapoints from the actual data distribution end up exhibiting close to average-case behavior. It might be worth including a discussion of these considerations.  As a minor point, I'd encourage sampling the synthetic function in Figure 2 with a finer mesh to produce a smoother figure.  Overall, I think this is a good paper that would be appreciated by the NIPS community and vote for acceptance.