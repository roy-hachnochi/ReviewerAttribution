*Update after authors response*  Thank you for clarifying a few points. This paper makes an interesting point and is well written, a good submission in my opinion. Further experiments to look at the likelihood blow-up in practice and quantify the extend of the problem (how often, under which conditions, ...) would make the paper even stronger, this is the main reason why I keep my score at 7.   This paper presents three main contributions. First, the authors demonstrate that the likelihood of Deep Latent Variable Model (DLVM) with Gaussian latent variables can be unbounded by providing an example of parameter choices leading to a likelihood blow-up. They further show this cannot happen for Bernoulli latents and that a spectral constraint on the covariance can prevent the issue in the Gaussian case. Second, an upper bound on the likelihood is derived using the fact that DLVM are particular cases of mixture models, the proposed bound is estimated using EM for the corresponding nonparametric mixture model. Third, a metropolis-within-Gibbs exact sampling is proposed to replace the pseudo-Gibbs scheme performed to infer missing input features in DLVM.   The analysis of maximum likelihood performed for Gaussian and Bernoulli DLVM provide some interesting and potentially actionable insights. It would be very interesting to see more experiments to study the impact of a non-bounded likelihood in Gaussian DLVM, in particular how often this becomes an issue in practice. While the reasoning linking unboundedness to mode collapse seems sensible it remains unclear if cases of mode collapse experienced in practice are due to this issue. The experimental section presents only one example of likelihood blow-up and further experiments would make the paper stronger. The practical interest of the upper bound provided seems limited by the fact that EM estimation on the corresponding mixture model is needed. Experiments performed with missing data seem more compelling and show a clear improvement on different datasets.   About Proposition 2. Does the particular form of the bound provided hold with the assumption that z ~ N(0, I) ? The proof seems to make that assumption.   In the proof of Theorem 2. Could the authors explain more precisely how the assumption of a spectral constraint for the covariance is used in the proof of the Gaussian case? Is it needed to obtain the zero limit of the density when eta goes to infinity?  About 4.1  We see in figure 2 a collapse of the test log-likelihood for the unconstrained model. How common is this phenomenon, does it depend on the initialisation or the learning rate? It would be interesting to see which values of the parameters correspond to this blow-up scenario and to see if there is indeed a mode collapse in this case. The gap between naive and nonparametric bounds seems relatively small, do the author have any insight on that?