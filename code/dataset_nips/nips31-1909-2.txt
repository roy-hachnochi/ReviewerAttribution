MAIN IDEAS  The paper shows that the likelihood for a deep latent variable model, p(x | z) p(z), can be made arbitrarily big if both the mean and variance of p(x | z) is learned, and p(x | z) is Gaussian. Here, unlike conventional Bayesian settings, no prior is placed on the parameters of p(x | z); they remain unregularized too. The proof construction relies on an argument that one could construct a p(x | z) that would spike with zero variance at one particular data point in the training set, and have bounded variance elsewhere.  It therefore shows that the maximum likelihood estimator is ill-posed.  As an aside, it would have been nice if the paper cleanly defined "ill-posed" and maybe linked it to Tikhonov and Arsenin's "Solutions of Ill-Posed Problems"...  The paper proceeds to give a simple (spectral contraints) condition on the covariance of p(x | z) to ensure the likelihood is always bounded.  Some of the ideas -- that a Bernoulli observation model has bounded likelihood; that its log likelihood is always negative -- is well known.  There are two more contributions:  1. We can choose p(x | z) to be more or less flexible, and its capacity limit is characterized by a "parsimony gap". The supplementary material gives very nice proofs (note: the paper doesn't always state that the proofs are in the supplementary material.)  2. An MCMC method for doing approximate missing data imputation. A Metropolis Hastigs accept-reject is added when sampling between x_missing and latent z.  (authors check typo: should \rho_t = ... contain a min(..., 1) in Algorithm 1?)  STRENGTHS  The handy take-home message from this paper is that the Gaussian variance for a VAE has to be constrained, or practitioners risk their models "perfectly modelling" one datapoint in the training set, with zero predictive variance. This overfitting nicely illustrated in Figure 1.  It is also immensely interesting to see how the results from finite mixture models (a mixture component describing only one data point, with zero variance on the mixture component) translates to the VAE / deep generative setting.  Furthermore, an improved algorithm is provided for sampling x_missing | x_observed, using a VAE encoder / decoder.  WEAKNESSES  The paper doesn't have one message.  Theorem 3 is not empirically investigated.  TYPOS, ETC  - Abstract. To state that the papers "draws useful connections" is uninformative, if the abstract doesn't state *what* connections are drawn.  - Theorem 1. Is subscript k (overloaded later in Line 178, etc) necessary? It looks like one can simply restate the theorem in terms of alpha -> infinity?  - Line 137 -- do the authors confuse VAEs with GANs's mode collapse here?  - The discussion around equation (10) is very terse, and not very clearly explained.  - Line 205. True posterior over which random variables?  - Line 230 deserves an explanation, i.e. why conditioning p(x_missing | x_observed, x) is easily computable.  - Figure 3: which Markov chain line is red and blue? Label?