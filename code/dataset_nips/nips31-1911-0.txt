This paper studies the sensitivity-based regularization and pruning of neural networks. Authors have introduced a new update rule based on the sensitivity of parameters and derived an overall regularization term based on this novel update rule. The main idea of this paper is indeed novel and interesting. The paper is clearly written. There are few concerns about the simulation results.   1- While the idea introduced in this paper is novel for modern deep learning, the general idea of sensitivity-based regularization for neural networks has been previously studied. As an example, authors could cite “A New Sensitivity-Based Pruning Technique for Feed-Forward Neural Networks That Improves Generalization” by Mrazova et al. published in IJCNN 2011.   2- One important question here is choosing the regularization factor and threshold value. Is there any systematic way for choosing these parameters such as cross validation? Or are these parameters tuned heuristically?  3- Regarding simulation results, it is surprising that authors have not included the result from [14] in Table 2. According to the Table 1 of [14], variational dropout (VD) outperforms [9] the pruning method of [9] significantly. Also, according to this table VD outperforms the method introduced in this paper as well. However, authors have not included VD in Table 2 and have mistakenly reported 0.8% as the best Top-1 error reported by [9] ignoring 0.75% reported by VD in [14]. The same concern holds for Table 3, as well.  4- Minor comments: -- Latex has a nice command \eqref{}. It is recommended that authors use this command instead of writing eq. 9. -- Line 235, Fig. 2 shows -- Line 239, factor can be interpreted   -------------------------------------------------------------  The rebuttal did not address concerns on simulation results. In particular, authors should demonstrate how their method compares against other approaches on convolutional layers. This would make the manuscript much stronger.