 Summary of the paper --------------------  The paper considers online F-measure optimization (OFO). While the previous approaches (except for [3]) either make use of convex surrogates for F-measure to achieve convergence or turn the problem into binary cost-sensitive classification, the reviewed work optimizes F-measure directly by simultaneously estimating the conditional class distribution \eta(x) = P(y=1|x), and the optimal threshold \theta on \eta to eventually converge to the population maximizer of F-measure (which is guaranteed by the past work to be a threshold function on \eta). The most related reference is [3], in which a similar procedure is undertaken. The main difference between [3] and the reviewed work is that in [3] the current threshold is simply set to F/2, a property satisfied by the optimal classifier based on true \eta, but not necessarily by the classifier based on the estimate \hat{\eta}; whereas the current paper uses a different method for estimating the threshold, which is based on stochastic gradient descent. The main advantage of the current work is that [3] only gives asymptotic convergence guarantee, while here the authors are able to show O(1/\sqrt{n}) converge rate with high probability. This advantage is further confirmed in the experimental study, where it is shown that the proposed algorithm is competitive or outperforms all existing approaches, including the one from [3].  The analysis in the paper makes use of the recent advances in the analysis of stochastic algorithms for strongly convex objectives where the strong convexity constant is unknown [8]. The idea is, given n samples, to split them into m = O(log(n)) phases, and in each phase to run a standard SGD algorithm on T=n/m samples with O(1/sqrt(T)) learning rate, but shrink the set of parameters to a ball of exponentially (w.r.t. to the number of phases) decreasing radius at the previous parameter. This approach guarantees fast O(1/n) converge to the optimum without knowing the strong convexity constant.  Overall evaluation ------------------  I found the paper to be interesting and contributing significantly to the current state of online optimization of composite objective functions for binary classification. The main novelty is dealing with simultaneous estimation of class conditional distribution and optimization of the threshold, which requires handling gradient estimates which are biased and optimizing a function with unknown strong convexity constant.  The main drawback is that the paper makes very strong assumption about the underlying probabilistic model, i.e. that the true conditional distribution \eta(x) = P(y=1|x) comes from a generalized linear model and can be computed as a logistic function of the dot product w^T x (the model is "well-specified"). From what I understand, this assumption can be relaxed to having some underlying model \eta(x) Lipschitz in w under the condition that an online algorithm is available which is guaranteed to converge to w^* at the rate O(1/sqrt(T)), where w^* is the weight vector which give rise to the true conditional distribution eta(x). This is, however, still a constraining assumptions, unlikely to be met in practice.   I suggest to overally improve the language, as there are many typos and language mistakes (some of which are mentioned below). Also, the presentation of the proof should be made more readable. I checked all the proofs in the appendix and they seem to be correct, with several exceptions mentioned below.  Despite the above shortcomings, I lean towards accepting the paper.  Technical remarks -----------------  Lemma 2: The constant in the lemma is distribution-dependent, which should be clearly stated (it depends on \pi, the positive class prior).  Assumption 1 and discussion thereafter: - Assumption 1 might be impossible to satisfy in practice, even under well-specified model assumption made in this paper. This is because for some distributions on a compact support, convergence of L(w) to L(w^*) does not imply that ||w-w^*|| converges to zero (e.g., when the distribution is concentrated on a single x, then all w such that w^T x = w*^T x give the optimal value of L(w)). In fact, w^* may not be uniquely defined. - (related to the previous remarks) the objective L(w) is not necessarily strongly convex, contrary to what is being claimed. Indeed, if the distribution of x is degenerate and concentrated on a single x value, the Hessian of L(w) is a rank one matrix.   Proof of Theorem 2: Bounding the term IV -- Lemma 3 is used separately in each trial t (with probability at least 1-\delta_t), so by the union bound, the upper bound on IV holds with probability at least 1 - \sum_t \delta_t, not with probability 1 - \delta/3 as claimed (unless a stronger version of Hoeffding would be used with also holds over all t).  Despite devoting some time into the proof of Theorem 3, I was unable to check its correctness in detail, as the proofs seems to be directly adapted from [8] and does not not give much guidance to the reader.  Minor remarks -------------  - line 26: "this is because that" -> "this is because" - 39: depends -> depend - 93: "In order for online prediction" -- ? - 95: "predication" -> "prediction" - 126: Sine -> Since - Algorithm 1, line 4: ,, -> , - Algorithm 2: what is \theta_1 in the first line? - line 213: "in high probability" -> "with high probability" - Theorem 3: "Convergence Result of SFO" -> "Convergence Result of FOFO" (?) - Supplement, Eq. between lines 52-53: T is missing in the denominator in the first term on the right-hand side; also, \theta should be \theta^* on the left-hand side? - Supplement, Eq. (9) is missing   -------------------- I have read the rebuttal, and two issues related to strong convexity and union bound were completely clarified by the authors, thank you.