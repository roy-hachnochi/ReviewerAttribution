This paper talks about new method based on first order variance reduced optimization called Stochastic Nested Variance Reduced Gradient Descent (SNVRG). It provides better convergence rate than the current state-of-the-art.  Main novelty comes from usage K+1 reference points for variance reduction. SVRG can be special case of SNVRG, but analysis, which was used in this paper, generally can't due  to requirement B=n(^)2C_1^2\sigma^2/\eps^2, which usually implies K>1, thus this does not provide better rate for SVRG. On the other hand, it shows that n^(2/3) can be improved to n^(1/2) in the rate, which brings huge contribution to non-convex smooth optimization. This work also uses additional assumption on the upperbound of the stochastic gradient variance, which then provides better rate comparing to SDCA.  For the experiment part. Parameter settings does not follow theory, but instead grid search is used, which due to high number of parameters to be tuned makes this algorithm hard to use in practice. On the other hand, it outperforms other algorithms substantially, but it would be hard to reproduce the result since authors did not include values for stepsizes and also grid search values.   There are some minors mistakes in the proofs, for instance T = \prod_{l=1}^{k} T_l = (B/2)^(1/2) instead B^(1/2). (A.1)  *** I've read the author feedback ***