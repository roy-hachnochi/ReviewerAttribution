Deep neural networks are computationally efficient when they use only a few bits to express weights and activation. Previous work use fixed number of bits for the entire  network. This paper argues that it is inefficient for some values if a small number (1 or 2) of bits is already enough. Then the paper propose to use a hybrid approach, which allows numbers to be truncated to 1, 2, or 3 bits. The method ranks all values according to their truncation errors and then decides the proportion of values using 1, 2, or 3 bits respectively. In general, values with large proportion error use more bits.   The main contribution of the paper is using truncation error to decide the number of bits. In my view, the paper definitely has a contribution to the field, but the contribution is limited.   The paper shows that the performance of the proposed approach with 1.4 bits is comparable to that of previous approach with 2 bit. Does the heterogeneous structure increases the implementation difficulty?   Writing issues:  Section 3 describes the proposed method. I propose to include then entire method in section 3. For example, equation (5) should include the entire quantization procedure and the gradient calculation.