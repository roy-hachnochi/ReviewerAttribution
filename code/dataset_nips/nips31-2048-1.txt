Summary:  The authors propose a new method that combines a latent Dirichlet allocation (LDA) model with a neural network architecture for the application of supervised text classification –– a model that can be trained end-to-end. In particular, they use a network structure to approximate the intractable inference equations that solve the KL-divergence between the LDA posterior and its approximation which is based on marginal distributions.  The authors show that an embedding in a Hilbert space can allow for the approximation of the inference equations, and they choose neural networks to parametrize the functional mapping. Finally, based on two applications, the authors demonstrate an incremental advancement over previous models.   Clarity:  The overall writing is good, especially as it is a very technical paper with many mathematical details. However, there are some details that cloud the overall impression: - Line 54: the references [4] and [5] do not both refer to the same author. - Equations 2–4:  w and z are vectors - Algorithms 1 and 2 suffer from a sloppy typesetting of subscripts, superscripts, and multiplications. - Algorithm 2: o In gradient descent, the gradient should be subtracted and not added. Except r_t is defined as a negative step size ( I could not find the definition for r_t in the paper) o The elements in P^{(0)} have not been defined previously. o The calligraphic T is overloaded, it has been used as an arbitrary operator in Subsection 3.3.    Quality: The overall quality of the paper is good. Some additional information could reinforce the reproducibility of the paper, for example more details about how the network is trained. In general, a publication of source code would be very beneficial for the understanding and reproducibility of this work.   Originality: To my knowledge, the work is original, and the authors do not merely propose a new architecture based on experimental results but start with theoretical considerations.    Significance: The theoretical achievement of approximating the LDA posterior with a PGM is for sure a significant achievement. It would be interesting to know whether the (incremental) performance improvement comes with a tradeoff in terms of computational cost compared to the other models.   Minor comments and typos:  Line 21: constrain  -- >constraint References: inconsistent names (e.g. [13] vs. [16], [20] vs. [21]) and missing journals or online sources ([23], [25]).   