This paper presents an alternative to variational autoencoders and other generative models with latent variables that rely on the wake-sleep algorithm for training. The main problem with the wake-sleep algorithm is its bias: the recognition model has different conditional independencies than the generative model and it's trained to optimize a different objective. DDC-HM solves this by instead working with sufficient statistics, and using those to implicitly define the maximum entropy distribution consistent with those statistics. The measurements chosen are random functions. The methods are evaluated on synthetic data and two small vision datasets (image patches and MNIST), comparing against two baselines using the MMD metric.  The idea sounds promising. I don't know the related work well enough to evaluate the novelty with confidence. The evidence in support of the method is adequate. If this paper were to be revised or expanded, I would be interested to know if there are any convergence guarantees (perhaps given certain assumptions or conditions). The bias of HMs was listed as one of the motivations for developing DDC-HM, but the biases of DDC-HMs are not analyzed theoretically or empirically -- just overall performance on a few datasets.   A discussion of efficiency would also be welcome. Can DDC-HMs be applied to datasets with many more variables and data points? How do their scalability compare to other methods? With probabilistic inference, there's often a tradeoff between efficiency and accuracy. Since the claim is that DDC-HMs are "flexible and accurate", it would be good to quantify the cost of that flexibility and accuracy.  One other curiosity: in the experiments on synthetic data, the recognition model has many more parameters than the generative model. Presumably, this is because the inverse function is hard to describe analytically, and thus requires more hidden units to approximate well. Is it normally the case that the recognition model should be so much more complex, relative to the generative mdoel? Is the greater complexity of the recognition model balanced by the fact that it can be trained on many generated samples, and is thus less likely to overfit? It's not essential, but I'd be interested in some discussion about this.  Overall, this paper proposes an interesting way to make deep generative models more flexible and less biased, and shows promising empirical results. More theoretical and empirical analysis could make it stronger, but I think this paper is acceptable for publication as-is.  Update after the author response: As before, I think the paper is ok as-is, although my confidence is only moderate. I hope the authors add more analysis, as time and page limits allow.