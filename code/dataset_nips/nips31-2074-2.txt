The paper proposes an extension of previous work on seeding to large/continuous problems for concurrent RL agents. This paper's idea builds on previous work on seed sampling and randomized value function learning. In general, this is an interesting problem to tackle given a recent exciting interest in this topic. The extension of the previous seed sampling approach for concurrent RL to higher-dimensional problems via the use of function approximation is neccessary and important.   In overall, the paper is well written and easy to follow. The discussion on motivations, background and related work are sufficient. The descriptive example and sanity check example are good to understand the idea. The experiment in 4.2 is though on toy domain, but interesting to see how it works in reality.   1. Section 2: it would be more complete if the authors also present background on the work: randomized value function learning. This piece of work is also one of the building block of the proposed idea in this paper.  2. In Section 3.1: It would be clearer if the authors elaborate how the noise term z for diversification can be used to purturb the same observation in the buffer. This might be related to the comment 1 too, as the foundation on randomized value function learning is not introduced.   3. Section 3.2.1: Seed LSVI is related to randomized LSVI in [12]? How it is linked to the previous seeding method in [1]? More discussion and analysis on sampling of z and its use in value function updates are needed. Similar to the extension of Seed TD and Seed Ensemble. More analysis on their committment, Diversification, and Adaptation: why, and how. This might be related to the comment 1 too, as the foundation on randomized value function learning is not introduced. Though refering the the paper [9] but there is still gap in understanding the proposed idea that is based on the idea of randomized value function learning in [9].    4. Section 3.2.3: The description of the Seed Ensemle is so short that it's not clear to see how ensemble model would be used in each step for each agent.  5. The idea has been proved in a toy domain, but it would be much nicer and make the paper stronger if more experiments on larger domains or more complex domains as discussed in Section 5 for future work can be added.   Given the above concerns, I thought that the paper has interesting ideas but seems to be premature to publish at NIPS.   -------------------------------------------  I have read the response and decide to raise my rating to borderline (5)  