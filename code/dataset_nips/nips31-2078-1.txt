Motivated by the latency in network performance between machines during iterative training, this work proposes a new gradient synchronization algorithm that runs on BCube network. The performance of the algorithm shows significant decrease in the synchronization time with fewer number of switches.   The paper is written well. While the authors mention that common distributed ml algorithm use is PS on a Fat Tree network, their choice of BCube for BML is not well justified.   In terms of the presentation of the paper, too much details on the background and motivation is provided whereas more focus on the details of the algorithm and the evaluation would have added value.   The primary concern of this paper is evaluation. In comparison to its competitor PS algorithm [1], the evaluation seems weak and not well done. While the primary take away in the evaluation is that the BML algorithm performs better than Fat Tree network, it is unclear if the synchronization algorithm is more valuable or the choice of BCube. An study or discussion to understand the contributions of choices to the performance is necessary.   Background and motivation section in the paper does not represent and discuss related work which is also missing from the results section. It is important to have clear description of related work which also discusses comparisons to the presented approach. A couple of missing related literature [2, 3].   [1] Scaling Distributed Machine Learning with the Parameter Server [2] Distributed Training Strategies for the Structured Perceptron [3] http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf  #### After author's reponse #### While, the authors have responded well to my comments, I am unsure how they would include the content in the paper, given the page limits. Based on author's response and reading other reviews, I am inclined to give the paper a positive score. 