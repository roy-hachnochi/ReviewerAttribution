This paper proposed a Propagative Convergent Network (PCN) to bridge the gaps between model optimization and deep propagation. Different from most existing unrolling approaches, which lack theoretical guarantees, the convergence of PCN is theoretically proved. These convergence results are solid and convincing for me. A relaxed PCN is also proposed to perform end-to-end collaborative learning.   It seems that this work actually addressed an important issue in learning and vision areas. That is, it provided a theoretically guaranteed manner to integrate the advantage of optimization models and heuristic networks for challenging learning tasks.    In the experimental part, authors compared PCN and relaxed PCN with state-of-the-art algorithms on different applications. These results seem convincing for a theoretical paper.   Some minor problems: 1.      In relaxed PCN, the network architecture is not clear. 2.      The training details should be explained in the experimental part. 3.      The parameter setting in experiments should also be given. Such as “p” in sparse coding and “a, b” in image restoration. ######################After Rebuttal ################################ Reviewer 2  spot a mistake in Eq.(10) .I also noticed the minor mistake in Eq.(10) on supplemental materials. In non-convex problem, it should be $\in$ not “=”. The author seems to build a minimize problem as Eq.(10) so that $x_g^l$ is one of the minimizers by introducing $\epsilon$. I think it is a constructing proof strategy. Obviously, this constructed optimization can make the inequality in (11) holds since $x_g^l$ is one of the minimizers. I also noticed the similar mistake happened in Eq.(12). It also should be $\in$ rather than $=$. But it doesn’t affect the deduction from Eq.(12) to (13). Similar results could be found from the traditional nonconvex optimization paper e.g.“Proximal Alternating Linearized Minimization for Non-convex and Non-smooth Problems ”. I think these minor mistakes don’t affect the final convergence conclusion.  About the effectiveness of bounding condition, Fig.2(d) in manuscript partially shown the condition is always satisfied in their experiments. I do agree that, from the optimization perspective, this paper suffers from somewhat limited novelty. However, from the deep learning perspective, this paper integrates deep learning module with the non-convex proximal gradient methods, which bridges the network and traditional iteration. Thus it makes sense for deep learning methods. I think it provides a significant evidence for the issue that deep learning strategies can learn a better descent gradient in a meta-learning-like manner.  From the computer vision perspective, we see the proposed method could be successfully applied to low-level computer vision tasks such as super-resolution based sparse coding and image restoration.  Overall, from deep learning and computer vision perspective, I think this paper is qualified, I will insist on my initial choice. 