The paper formulates a way to incorporate side information about the hidden state space to learn the predictive state representation more efficiently. As a result, an augmented PSR framework is defined along with the adapted spectral learning approach, backing these up with multiple theoretical statements (about dimensions, consistency, etc.). The side information can be imperfect, and it is taken care of by using only linear projections of this information that are mostly aligned with the empirical evidence. Several experiments are performed highlighting the fact that simple but imperfect side information can improve the performance of the learned model, compared to the standard spectral learning for PSRs.   Clarity: The paper is well written. There are several aspects that are worth clarifying though: 1. Example that follows Eq. 4: It is useful to highlight that the assumption of knowing some independent tests means also knowing the values of those tests for any history (not merely knowing what some core tests are). 2. There is a lack of clarity for what makes “f” useful enough or not for learning. Lines 137-143 mention that “f” that is too irrelevant may be detrimental to learning due to large core f-histories. So, should one pick “f” whose core histories are within the dimensions of the estimated P_TH, for example? 3. Although some discussion is provided (mainly in supp material) about lambda in algorithm 3, it still feels like this is an additional parameter that should be optimized, possibly through cross validation. Is it the case? If not, a better justification is missing.  Originality: This is new material as far as I know that is sufficiently different from related work.  Quality: All the proofs appear in the supplementary material, so I didn’t check all of them carefully, but the statements make sense. The experiments appear to be done correctly as well. My only small concern is about the “Mkv” baseline in the gene splice dataset: a more convincing baseline is taking 4 most important projections based on SVD (of the 16dim matrix) instead of 4 random projections. This would correspond to a rank-4 PSR compression of the 16 state markov chain. Ideally, PSR-f should perform better than this as well. Also, good job on including the discussion on the “negative results” in the supplementary material, this is a valuable piece of information that requires further investigation.  Importance: Finding ways to incorporate domain knowledge easily in the PSR framework is a meaningful direction to study and could result in learning better representations. A better understanding of the limitations and advantages of this approach, as well as what kind of functions “f” are useful is required to have stronger impact. 