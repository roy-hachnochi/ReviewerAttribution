This paper studies zero-order optimization with noisy observations. The authors study an "adaptive sampling" setting, where every query can depend on the results of previous queries. Although adaptive sampling gives the analyst an intuitive advantage over uniform sampling, the existing theoretical result doesn't confirm a more superior convergence rate for it. Through an information theoretic analysis, the paper shows that the classical rate is unimprovable even by active sampling. Further more, it presents a local convergence analysis, showing that if the true function lies in the neighborhood of some desirable reference function, then active sampling does enjoy a better convergence rate than uniform sampling. The gap is presented in both an upper bound and an almost matching lower bound.  Overall, I enjoyed reading the paper. The technical results are solid, interesting and well-presented. The derivation of the lower bound for the active estimator involves some interesting technique. I have two minor questions:  1. The upper bound is presented in a way that with probability at most 1/4, then risk will be bounded by the c*R_n. Why don't the authors prove an upper bound on the expectation of the risk, or prove a high-probability upper bound? A in-expectation upper bound is apparently stronger. If a high-probability bound can be derived from the existing fixed probability bound by repeating the estimation multiple times, it is at least worth mentioning in the paper.  2. It looks like that many constants in the paper depend on the dimension d. For example, the constant C_0 can be exponential in d. Therefore the gap between the upper bound and the lower bound may also have this exponential dependence. If this is true, then it should be explicitly stated in the paper, because in real applications d can be large.