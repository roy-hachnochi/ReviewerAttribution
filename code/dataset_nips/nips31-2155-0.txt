Summary:  In this paper, the authors propose an extension of the Mallows model, originally defined on the space of full rankings/permutations, to the case of top-k lists, a special kind of partial rankings. The size k of the top-k lists, additionally to the central ranking and the spread parameter, can be seen as parameter (fixed initially).  They consider two problems. The first problem concerns the generation of a sample from this model. For this, they propose two algorithms (with different complexities/dependance over the whole set of items n and k).  Then, they consider the problem of learning the center of such a distribution and learning a mixture of several of these distributions. To learn the center of the distributions, the proposed algorithm compares pairs of elements and decides over their order based on some threshold for the empirical pairwise probabilities. The learning mixture algorithm relies on this previous algorithms to learn the centers of the mixtures, with applying previously a single-linkage clustering step.  Qualitative assessment:  Clarity: The paper is well written, I understood it all reasonably well.  Significance: This paper is considering an important problem and the results certainly make some contribution to the literature. Indeed, tt is true that their models builds upon/corrects some defaults of previous extensions of Mallows model to partial rankings (see l64 to l 77), since they minimize the importance of the tail elements (the one not appearing in the central top-k ranking). Their model can be also generalized to other interesting settings (see l78-91) and thus covers a diversity of rankings/applications. Their work differ from previous extensions of Mallows model on the assumptions and the distances chosen. The authors also propose several algorithms to tackle sampling/reconstruction problems.  I have thus several critics. I think the bound on the relaxation time for the markov chain algorithm is still big (k^5 log(k)) even for small values of k, but the total variation mixing time is reasonable (k^3).  Then, concerning the center reconstruction problem: in the proof of theorem 9, the authors propose an algorithm which order pairs of items, given that the probability of its order is below some threshold K.  But in the end, in the experiments, they apply Copeland method to find the center, which is, that’s the least you can say, not a new algorithm.   Then, I would have preferred some algorithms to be explained/at least sketched/have their complexities explained in the main text: the one from Theorem 2 and the one from Theorem 9.   Minor comments: title: In my opinion, the title is too vague and should include "Mallow's model" l34: "most of the work has been less formal in nature » -> this formulation is a bit vague. l47: most of the pairwise relations are potentially statistically spurious: i guess that you underline the fact that some pairs of items, compared to some others, are very less observed. this may not be evident for someone non familiar with ranking. l69 to 70: the example is not very clear/ illustrative at this point l 78: typo, misses a « be »  l82: why klog(n) and nlog(n)? l89: a number of our results can be generalized…-> are there limitations for the generalization?  l111-113: would be more appropriate in the introduction. l161: I don’t agree on n^k. I would say n!/k!(n-k)! Proof of Lemma 8 is not very clear  l271: replace o(1) with o(n^-3) experiments section: the plots are way too small Figure 1: the L1 distance is increasing for beta=1 after 200 steps, why? does the  number of samples chosen in the experiments correspond to Theorem 9? l329-336: this is called Copeland method. typo in ref 19. 