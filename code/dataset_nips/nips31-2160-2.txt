Thank you for an interesting read.  The authors consider over-fitting issues of latent variable model training and propose regularising the encoder for better generalisation. Two existing regularisation techniques -- denoising VAE and weight normalisation for encoder parameters -- are analysed in both theoretical and empirical way. The denoising VAE regularisation mechanism is further extended to IWAE.  I think this is an interesting paper, which explicitly point out the folklore wisdom that the encoder in VAE introduces bias for the generator training. The analysis also has some interesting point. However there are a few concerns that needs to be addressed, which drives me away from recommending strong acceptance. I will be happy to read through the authors' feedback and see if I need to adjust my score.  1. training procedure of p and q: VAE can be viewed as a coupled update process of variational EM, which trains q with variational inference, and uses q in the M-step for training p. But generally speaking, it is unnecessary to tie the objective of training p and q: I can train p with E_q(z|x)[\log p(x, z)], but train q(z|x) with what-ever method that make sense to me.   So I think there are several questions here:  (1a) Do you want to be Bayesian for inference? If no then you can definitely claim the denoising VAE as some other inference method that make sense. (1b) Should I treat q(z|x) as part of my model or not, since I will use q(z|x) to compute all the statistics for test anyway? If yes then it is very natural to motivate regularisations on q. But people who think the answer is no will go for direct regularisation of the p model, and I'm confident that they will be able to figure out a way.  I would be happy to see your answers on these questions. This would help me a lot to better understand the scope of your proposed research.  2. Below your definition 1, what do you mean by "scalar addition to share the same minimum"?  3. In your proposition 1, if their exist a model p that factorises, then this means p can ignore z, do you think your method will push p towards this solution when sigma is large? Consider sigma tends to infinity for example.  4. In Table 1, is that possible to report ESS for your estimate? Just for sanity check, if the ESS are roughly the same across evaluated models then your results are more reliable.  5. In the discussion of Proposition 2, you mentioned as the smoothness increases mu tends to an empirical average of sufficient statistics, meaning that p is the maximum entropy solution. Can you discuss why maximum entropy solution might be something you want for better generalisation?  6. Your generalisation of denoising VAE to IWAE: does the \tilde{D} definition always non-negative? Why not directly start from a proper definition of unnormalised KL? I had this question already when I saw [22] quite a while ago...  7. You reported inference gap in your tables in the main text. I'm not exactly sure what you want to demonstrate here. From my understanding of appendix C you trained another q(z|x) on test data, and I guess you used the same q(z|x) to estimate log p(x) and L(x) (or L_8(x))? Then in principle I don't need to use the same family of q distribution in training/test time, and the inference gap can be reduced to arbitrary small if the test-time q(z|x) is super powerful.    ==========================================  I have read the feedback and engaged in discussions with other reviewers.   My rating stays the same, and I hope the authors could better motivate the purpose of the paper (e.g. discuss more on other regularisation methods say beta-VAE and discuss what they can achieve).   I feel the inference gap results are still confusing. Also better to have a discussion on how the quality of amortised inference relates to generalisation.