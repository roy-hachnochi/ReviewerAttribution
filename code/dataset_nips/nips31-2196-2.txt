===Update=== As Review 1 pointed out, the main result of the paper, in particular the generative process, is similar to existing literature. I downgrade my review to 6.   == Original review == This paper provides a novel framework that utilizes importance weighting strategy to control the Jenson bound in KL-variational inference.  It also explains re-parametrization in elliptical distributions, making the variational inference family more flexible -- especially for capturing heavy tails.    This paper is clearly written and was easy to follow. Overall I believe it is an outstanding paper.  Concerned with the usage of importance weighting, I have following questions.  1 In VAE, a tighter bound does not necessarily lead to a better inference. So I am also curious if the main goal is make inference (of the target distribution p), what it the advantage of IW? I am not extremely convinced.  2.  Changing the importance weighting sample size M from 1 to infinity will change the final optimum q from mode-seeking to tail matching, without tuning the divergence. This might motivates a better tuning of M?   3 . The quick concentration of R_M to p(x) relies on Var(R), which is essentially the same asymptotic 1/M convergence rate in Theorem 3 given clt. But this result relies on the finite variance Var(R), in which case a naive importance sampling will work given the proposal q.  Finite var(R) is not trivial. What will happen if Var(R)=infinity or impractically large? For example, if the target has heavier tail than the variational family, then Var(R) is never finite with any proposal q.  (Sure, other tail matching methods like chi square vi will fail in this situation, too)  possible typos in the manuscript:     tilte: varational <- variational    line 219: pairs pairs   line 219: F^{-1} v <-  F_w^{-1} v     line 277: reference 