** Summary    The authors present a new scoring function for neural link    prediction that allows one to interpolate between    expressivity/complexity and computational tractability. They define    a model which generalises an existing low-complexity model HolE by    stacking a number of instances of HolE, each perturbed with a    perturbation vector c. The authors show how, for an appropriately    chosen set of c vectors, this model is equivalent to RESCAL, a    high-complexity model. They provide a number of theoretical results    characterising their model for two different classes of    perturbation vectors. Finally, they demonstrate that their model    improves on existing methods on the FB15K dataset.  ** Quality    The work presented here seems to be theoretically and empirically    sound. I do have one comment regarding their choice of benchmark    dataset. A recent paper [1] showed how one can get quite good    performance on FB15K using a trivial model, suggesting it might not    be such a good benchmark for link prediction. The alternative    dataset FB15K-237 does not have this issue. I would recommend the    authors re-run their experiments with FB15K and either replace the    results in the paper, or add a new results table. Knowledge graph completion    papers also tend to evaluate on WN18 and WN18RR, e.g. see Table 2 in [2]   ** Clarity    The paper is very clearly written and was a pleasure to read     On line 115, the authors state RESCAL becomes infeasible for    modestly sized KGs. RESCAL scales poorly with the number of    relations, so it would be more accurate to say RESCAL becomes    infeasible for KGs with a modestly large number of    relations. RESCAL will scale no worse than other methods on graphs    with a large number of entities but a relatively small number of    relations  ** Novelty    The model presented is a non-trivial generalisation of existing    methods and the theoretical results provide useful insight.  ** Significance    I think this is a good contribution to the link prediction    literature, providing a nice way to balance model complexity with    expressivity. I would imagine others will try to build on this work    and use it in their applications.  ** Overall    I think this is a good quality submission, with interesting    theoretical results. However, I strongly suggest the authors include the suggested    additional experiments.  ** Author Response     Thank you for your response. I am glad to hear you are experimenting with other      datasets and metrics: the inclusion of these will make the empirical evaluation of      your model more convincing. Reviewer #4 made a good suggestion to compare to      truncated identity and I hope this comparison will also be included in the final      paper, as well as a discussion as to why using the truncated identity matrix of      rank k can be seen to be less expressive than other rank k matrices one could use      for Cd (for example, by comparing rank 1 truncated identity with HolE).   [1] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel: Convolutional 2D Knowledge Graph Embeddings. AAAI 2018  [2] Timoth√©e Lacroix, Nicolas Usunier, Guillaume Obozinski: Canonical Tensor Decomposition for Knowledge Base Completion. ICML 2018