This paper investigates the problem of inverse reinforcement learning from multiple tasks in a manner that allows transfer of knowledge from some tasks to others. A model using a matrix factorization of task reward functions is presented to realize this. Online methods are then developed that enable this learning to occur in a lifelong manner requiring computation that does not badly scale with the number of tasks.  The problem explored is well-motivated in the paper and under-investigated in the existing literature. The paper makes a solid contribution to address this problem.  The proposed method adapts the efficient lifelong learning algorithm [33] to maximum entropy inverse reinforcement learning [42]. This seems like a relatively straight-forward combination of ideas, reducing the novelty of contribution.  It is unclear what guarantees, if any, are provided by the alternating optimization method along with the approximations introduced by using the Taylor expansion and the sample approximation to compute the Hessian.   The experiments show the benefits of the developed methods over reasonable alternative approaches and the authors provide a number of additional experiments in the supplementary materials.  My recommendation at this point is “weak accept” since the paper makes a solid technical contribution to the inverse reinforcement learning literature, but without a substantial amount of novelty due to the combination of existing methods or theoretical guarantees.  ==== Thank you for addressing my questions in your response.