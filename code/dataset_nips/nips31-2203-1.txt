This paper investigates the problem of inverse reinforcement learning in which the agent faces a series of related tasks and has to learn the reward function for each. The framework is fairly straightforward; the paper is well written and easy to read. The proposed framework is based on previous work on life long learning in RL settings with minor incremental changes but still significant.   Some additional comments:  - intuitively, it looks like this framework will work well when certain aspects of the reward function are shared as in the example of dropping objects. However, what happens when in some tasks an action like dropping an object is bad (e.g., when  delivering a tray) while in others it is desirable in some contexts (e.g., dropping an object in the trash bin when cleaning up)? This is not immediately clear and some clarification along with that example would help.  - The description of the Objectworld domain could be improved by explicitly stating the goal of the agent in such a task.   - A more extensive evaluation with actual human demonstrations would be a big plus.   - From the evaluation, it doesn't seem that this framework can scale well for more complicated tasks - if 32 (or 256) trajectories are needed per task and it takes dozens of tasks to converge, would this work in practice where the trajectories come from real people and the tasks are much more complex? If the motivation really is to enable robots to be taught by end-users, it doesn't look like this solution will work well in practice.   - Why wasn't the proposed method benchmarked against the method proposed in [11]?   - The authors make the case in the related work section that previous methods did not deal with different action spaces; however, it looks like in the experiments all tasks shared the action space.   - The conclusion is too short -- no limitations are discussed and little directions for future work