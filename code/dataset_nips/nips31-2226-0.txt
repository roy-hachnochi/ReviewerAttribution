Post-rebuttal:  I have adjusted the score by raising one point, if the authors incorporate all the changes promised, including the clarification on skip connection, the experiments on attention, and the correction of inconsistent results. My evaluation of the overall paper remains that the experimental results are convincing but the contribution seems incremental.  =================================================  This paper proposes two ingredients to improve the performance of GCN: (1) an adaptive sampling approach to addressing scalability and (2) a skip connection approach to improving classification accuracy.  The adaptive sampling is an extension of FastGCN. The innovative point is that the proposal distribution is learned through variance reduction. A slight drawback is that the loss function includes the variance part for only the top layer.  The skip connection is reasonable, but judged from the experimental results, this approach does not seem to offer noticeable advantage.  The authors draw a connection with GAT, which is interesting. However, it is unclear how this part is connected to the rest of the paper. For example, the authors seem to propose an alternative formula for computing the attention (Equation (14)), but no experiments are presented.  The self-implementation results in Table 1 are not quite consistent with the published ones from GraphSAGE and FastGCN. Could the authors explain the difference? Would the inconsistency be caused by hyperparameter tuning?  The writing and language could be improved. Word usage, such as "visitable" and "impracticable", should be proofread.  Overall, while the experiments confirm that the proposed ideas work, the reader feels that the contribution is incremental. 