The paper proposed a new adaptive sampling scheme for graph convolutional networks and gained extremely good results on several benchmarks.   The work is an extension of GraphSAGE and FastGCN. Different from GraphSAGE which samples fixed-size neighborhoods for each node, the proposed method sample nodes layer-wise. FastGCN is the most related work, which has the same ideas of layer-wise sampling and variance induction. The improvement of the proposed method on FastGCN mainly lies on the following aspects:  1. The proposed method sample nodes conditioned on the samples of previous layers instead of using layer-independent sampling scheme in FastGCN. 2. For computational efficiency, FastGCN removed the hidden feature part in the optimal sampling distribution in variance reduction, the proposed method used a linear function of node features instead and add the variance in the final objective function. 3. Adding skip connection to preserve second-order proximities.   The paper is well written and technically correct. The ideas of layer-wise sampling and variance reduction are not new, and the improvement on FastGCN is not very significant. But the layer-dependent sampling is interesting, and it does solve the potential drawbacks of FastGCN and leads to a significant performance gain. So the technical contribution is acceptable.  The experimental results are promising. However, the running curve of FastGCN is quite different from the original implementation. I directly run the public FastGCN codes on one dataset (pubmed), the running curve is consistent with the original paper. As there are public codes for FastGCN and GraphSAGE, I think it is better to use their original codes. Especially for FastGCN, as there is a performance gap, I suggest the authors re-check their own implementation or use original FastGCN codes instead. If the paper is accepted, I hope the authors could modify the running curves in Figure2.  It is better to add another recent sampling method in the related work: Chen, Jianfei, Jun Zhu, and Le Song. "Stochastic Training of Graph Convolutional Networks with Variance Reduction." International Conference on Machine Learning. 2018.