[Summary]  This paper studies stochastic gradient descent to solve a convex function minimization problem in which, at each time t=1,...,T, a master algorithm selects a point x_t and gets objective function's noisy gradients at x_t from m workers, \alpha m of which are Byzantine who can return any value. The authors proposed algorithm ByzantineSGD and proved i) upper bounds on the difference between the objective function values at the mean of the selected points {x_t} and the optimal point in the case with smooth or Lipschitz continuous objective functions, ii) sample upper bounds to obtain \epsilon-close objective function values   in the case with strongly convex smooth or Lipschitz continuous objective functions, and iii) sample lower bounds in the case with non-strong convex or strong convex objective functions.  [Originality] The same Byzantine convex function minimization problem has been independently considered by Yin et al. [31] but in their setting, each worker returns the gradient averaged over n samples, which is not SGD but GD. Their algorithm is completely different from ByzantineSGD.  [Strengths]  * First SGD algorithm for Byzantine setting * Nice theoretical analyses * Comprehensive theoretical sample complexity comparison with related algorithms  [Weakness]  * The same optimization problem has been independently studied using a similar approach. * The master algorithm cannot be seen as SGD. * No empirical comparison with the existing algorithms  [Recommendation]  I recommend this paper to be evaluated as "Marginally above the acceptance threshold". This is a theoretical paper and no empirical result is shown. Theoretical comparison with existing algorithms has been done comprehensively. One anxiety is practical usefulness of this algorithm. SGD needs many iteration until convergence, so communication cost between the master and workers may become larger than GD.  [Detailed Comments]  p.5 \Delta_i, \Delta_{med} and \nabla_i are not defined. 