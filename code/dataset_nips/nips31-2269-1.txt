This paper proposes an algorithm for learning parametric intrinsic rewards for policy-gradient based learning which is built on optimal rewards framework (Singh et al. [2010]). The main part of paper (Sec 3.2) is about how to calculate the gradients and does parameter updates. Since their method needs two episodes to calculate the gradients and update parameters, samples from the previous episode are used in order to improve data efficiency.  Even though this paper addresses the deficiency of reward function in RL with learnable ones, I am not completely convinced about the contribution and novelty of this paper. Becuase parametric intrinsic rewards have been studied before [Jaderberg et al., 2016, Pathak 2017, Singh et al. 2010]  in both policy and value-based methods. I might be missing something here but this paper only shows how to derive the gradients and it doesn't introduce any new type of intrinsic reward function or learning?!  The experiments sections contain a various comparison with policy gradient methods. The proposed model shows improvement over using policy gradient methods without any intrinsic reward, but comparisons with a method that uses a form of intrinsic reward would have been very informative.     The writing is not clear in some parts of paper and needs to be improved:   - Lines 30-34: it is very unclear and lots of terms used without proper definition, e.g. potential-based reward or order-preserving reward - Lines 38-40: ' Thus, in addressing the reward-design problem one may want to consider ... ' is unclear to me   Question: What were the criteria for selecting 15 games among all others for the experiments?