The authors propose a mathematical programming approach to build interpretable machine learning models. In this case, the interpretable model is a system of Boolean rules in disjunctive (or conjunctive) normal form which is constructed using column generation for the linear relaxation of a mixed integer program (MIP) designed to minimize the number of positive samples classified incorrectly and the complexity of the learned system subject to complexity constraints. To remedy the fact that there are exponentially many potential clauses to optimize over, the authors propose a standard column generation approach that prices potential columns to add and solves a secondary integer program to find such potential columns. The authors also note that the column generation can also be done via heuristics or a greedy algorithm. Once the linear programming program is solved or reaches its time limit, the approach then solves the global mixed integer formulation to get a final set of rules. The authors perform many experiments comparing their model to a wide range of benchmark models testing improvements in model complexity as well as accuracy. Several benchmark models are built with only one of these two objectives in mind and the proposed model seems to give good tradeoffs in both objectives.  The goal of having an interpretable decision making model is very well motivated. Additionally, the proposed methodology is very clearly explained and the experimental design is nicely laid out. Overall, the column generation mechanism seems to scale better than previous work and the mathematical programming formulation allows easy access to bound the optimal accuracy and complexity. Furthermore, the mathematical programming formulation is very flexible and allows for various loss formulations as well as differing tradeoffs between accuracy and complexity. This methodology is seemingly extensible enough to account for other desired traits of a machine learning model such as fairness constraints. The approach can also be extended to explicitly trade off complexity and accuracy. Finally, the approach seems to do very well against several benchmark in terms of test accuracy and model complexity.   For these reasons, I recommend that this paper be accepted, and hope the authors can improve it by addressing the following questions/suggestions:  - The MIP is missing bounds on the 0 <= \xi_i <= 1 (same for \delta_i in the pricing problem). While you are right that you don’t need \xi_i \in \{0,1\}, if you don’t include lower/upper bounds, then if for a given i in constraint (2), the sum of w_k variables is large, then the optimal solution would make the corresponding \xi_i negative (to make the objective smaller) while maintaining feasibility of constraint (2). I think you probably have the bounds right in the implementation, but not in the text. - Derivation of the pricing problem: since this IP approach may be new to many NIPS readers, it may be better to give a more gentle derivation of the pricing problem, defining reduced costs; just a suggestion.  - Tables 1 and 2 are somewhat difficult to read and would benefit from either bolding the best value in the row or writing the value of the best entry in a final column. It would also be helpful to explain some of the more interesting rows such as the tic-tac-toe dataset having 100 test accuracy with 0.0 standard error for CG while the random forest has less accuracy. This could be resolved by reporting training accuracy or demonstrating that the CG approach enables better generalization due to its lack of complexity.  - It would be helpful to demonstrate the ability for the system to give lower bounds on complexity by reporting these metrics for the different experiments. - Can you elaborate on the FICO example end of page 7? What are these features, and why is this an intuitive rule? Without a few sentences explaining it, the example is kind of useless. - The related work section could include brief references to work that uses column generation for machine learning tasks, examples below.  [1] Demiriz, Ayhan, Kristin P. Bennett, and John Shawe-Taylor. "Linear programming boosting via column generation." Machine Learning 46.1-3 (2002): 225-254. [2] Bi, Jinbo, Tong Zhang, and Kristin P. Bennett. "Column-generation boosting methods for mixture of kernels." Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2004. [3] Li, Xi, et al. "Learning hash functions using column generation." arXiv preprint arXiv:1303.0339 (2013).   