This submission proposes a new recurrent unit, namely, recurrently controlled recurrent networks (RCRN). It is inspired by LSTM, but the sigmoid gates and content cells are replaced with additional recurrent networks. In this way, the gates would not depend on the previous hidden states, using the notations from the paper, h^4, but are implemented as separate gating recurrent units instead. The method appears novel and interesting to me, and one can imagine there are straightforward extensions under such a framework, just as the extensive efforts the community have been devoted to improving gating RNNs. The paper conducts extensive empirical evaluations on multiple NLP tasks, and the proposed achieves strong performance. Last but not least, the paper is well-written and easy to follow.   Details:  - Although the experimental part is already quite extensive, I would be excited to see results on language modeling and machine translation.  - The 3-layer LSTM might not be the most comparable baseline, since there is no vertical connection in a RCNRN.  - The notations in the equations and those in Figure 1 are not consistent.  - Some of the experimental details are missing.