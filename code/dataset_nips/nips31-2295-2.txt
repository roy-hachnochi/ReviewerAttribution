The proposes to extend gating mechanisms with recurrency. The idea is extremely simple and, to be honest, I am shocked that it hasn’t been done before. The technical exposition is clear, which I attribute mostly to the simplicity of the architecture. It’s difficult to imagine any criticism of this paper: a simple idea with strong empirical results. In fact, the only argument I can see against accepting this paper is that it’s a bit boring. I wouldn’t be surprised if you couldn’t find this very idea in the footnotes of a handful of other papers.   Questions:  Do the authors actually control for parameters? The experiments demonstrate that the RCN network is better. Does it also have more parameters? I didn’t see this explicitly mentioned and it would indicate that the experiments aren’t fair. Why not use a task with strong auto-regressive tendencies? I would have loved to see a language-modeling experiment or a neural MT experiment. It’s hard to imagine any of the proposed tasks have strong interesting gating tendencies since bag-of-words baselines are very strong on simple classification tasks.   