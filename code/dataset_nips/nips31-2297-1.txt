This paper studies the problem of dictionary selection, where the goal is to pick k vectors among a collection of n d-dimensional vectors such that these vectors can approximate T data points in a sparse representation. This problem is well-studied and the authors propose a new algorithm with theoretical guarantees which is faster than previous algorithms and which can handle more general constraints. This algorithm is based on a previous algorithm for the related problem of two stage submodular maximization called replacement greedy. It is first shown that replacement greedy also enjoys approximation guarantees for dictionary selection. Then, the authors further improve this algorithm to obtain replacement OMP, which is faster. This algorithm is also extended to the online setting. The authors evaluate their algorithm empirically and compare it to previous algorithms for both dictionary selection and dictionary learning (the continuous version of the problem). It is shown that replacement OMP is either significantly faster with similar performance or has better performance than any benchmark.  Overall, this paper makes a solid contribution to the well-motivated and well-studied problem of dictionary selection. Previous algorithms had very slow running time, so the significant improvement in running time is important. The techniques are nice and port recent ideas from the related problem of two stage submodular maximization (it is not straightforward that these techniques can be used for this problem with approximation guarantees), and the authors also give non-trivial further improvements to the algorithm by using a nice idea of orthogonal matching pursuit. Another important contribution is that this algorithm can be used under generalized sparsity constraints, which are well-motivated and relevant to the problem studied. The approximation guarantees obtained in this paper are incomparable to those from previous papers, but the experiments clearly demonstrate that the proposed algorithm either performs comparably to previous algorithms which are much slower or outperforms previous algorithms which are faster. I thought it was nice that the authors gave a comparison of discrete and continuous methods, showing a significant improvement in the running time compared to the algorithms from the continuous setting for dictionary learning.  Minor cons: - the empirical performance of SDSma does not seem as “poor” as the paper says. - it would be nice if the authors discussed the approximation guarantees obtained in the different experimental settings, since these approximations are incomparable to those of other algorithms in general. In other words, what are the constants ms, Ms, Ms2,… for the datasets used for the experiments?