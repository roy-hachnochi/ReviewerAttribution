The paper explores how smooth maximization oracles can be used to optimize structured support vector machines with recent first-order incremental optimization methods which have fast convergence rates.  The paper explores the notion of an l2-smoothed and an entropy-smoothed maximization oracle. The appendix explains how to implement an l2-smoothed maximization oracle in practice with a top-k oracle, which seems necessary to implement and understand the algorithms in the paper.  While simply coupling this smoothing oracle with existing algorithms such as SVRG can provide good results the paper also explores Nesterov-style accelerated optimization methods, in the catalyst framework. These have the advantage that it's straightforward to anneal the amount of smoothing during training to better approximate the non-smoothed loss.   I did not carefully check all the theorems and convergence rates, but the basic derivations of the algorithms is clear and easy to follow.  The experimental results are somewhat convincing, but I'd like to see more thorough baselines. For example, it's possible to change the learning rate of Pegasos and similar algorithms in ways that do not correspond to the original paper but get better performance. Similarly, it'd be interesting to see more results on how methods such as SVRG perform when no smoothing is added, to understand the value of smoothing.  That said, the idea of smoothing for structured prediction is interesting, and this paper leaves many avenues open for future work, so I think this belongs in NIPS.   After reading the author response my score is unchanged; I still think this paper belongs in NIPS. I encourage the authors to add the extra experiments in the response to the paper to clarify better what aspects of the methods are important.