The main message of this paper is to introduce a smooth inference for structured prediction, which practically is not novel as other methods such as gradient descent inference use a continuous output variables as a relation of discrete variables and smoothen the inference using L2 or using entropy, and projects the output variables into simplex of probabilities after each steps of gradient descent.  However, this work presents the idea in a carefully described framework with theoretical guarantees that was missing from previous works. The next contribution of the paper is to use the smooth SVRG as the M in the catalyst framework.  My main problem with the paper is that I am not sure why these two parts are presented in a single paper since each targeted different aspect of the structured prediction problem (the first one is smoothing inference, and the second one targeted smoothing the weight optimization by introducing well-conditioned convex problem).  I guess the message of these two parts is very different if not contradictory.  As I expect and confirmed by Figures 8-10, I can conclude that smoothing inference actually helps test set generalization rather than having a better rate for train loss convergence (comparing to BCFG that mostly has better rates but does not generalize well to the test data), which is not very surprising since the inference returns a combination of output structures instead the most likely one. 