PAPER SUMMARY The authors propose a value-based reinforcement learnig (RL) method based on least-squares temporal difference learning (LSTD). Concretely, they find upper bounds on the action-values learned by LSTD under a fixed policy. These bounds can then be used by an optimistic approach (upper confidence bound (UCB)) to guide exploration in a data-efficient manner. The authors illustrate the practical benefits of the proposed method by comparing it to a number of other methods on a range of simple simulated RL problems.  REVIEW The paper is written in a clear and rigorous fashion. The derived bound is theoretically interesting and its practical benefits have also been illustrated. The experimental tasks are relatively simple, but I think this is acceptable given the theoretical nature of this paper. Since I am not an expert in value-based RL, it is not completely clear to me how the proposed method compares to related work, but as far as I can judge this article would constitute a valuable publication.