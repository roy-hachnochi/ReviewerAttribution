The paper proposes a variant of 2-layer neural networks called porcupine networks which are obtained by  defining an alternative formulation (objective function) by constraining the weight vectors to lie on a favorable landscape. The advantage of this being that all local optima are also global.  The idea is appealing to me theoretically as a proof of concept, however I would like to see some empirical evaluation (on synthetic datasets) using some simple neural nets. The authors have delved deeper into the theoretical guarantees and that is a plus of this work. I like the fact that  constraining the non-convex optimization landscape can help in obtaining global optimum solutions.  I would like to see the authors present a discussion of the true merits (or selling points) of using PNNs on real-world datasets and applications. Are they attractive from a memory storage point of view?  Also, can the authors shed some light on whether PNNs be extended to other types of neural networks - for e.g. sequential data (RNNs, LSTMs)? 