This work proposed a system that converts mono audio into spatial audios while given visual input. The system learns in a self-supervised way from 360 videos.   Strength: (1) This paper is the first to solve the problem of spatial audio generation, and adopts the popular audio-visual self-supervision. The formulation and proposed model makes good sense. (2) Both objective and subjective evaluations are performed to verify the effectiveness of the model. (3) Applications of this work are straightforward: converting legacy mono-audio 360 videos into spatial ones for AR/VR applications.  Questions: (1)  In the proposed architecture, it is not clear where and what is the loss function. As there are two tasks, separation and localization, how are the losses designed for each task? The authors should give details on that. (2) How is the spatial localization done? It seems that the output of RGB and Flow streams have already thrown away the spatial information. (2) How are the ground truth energy maps generated in Figure 3? (3) Notations in the paper is a bit confusing, eg. How are w, f, \Phi_(t, w), phi_(t) in the model (Figure 1) correspond to notations in Equation 1 and 2? Unifying the notations is needed to improve the clarity.  (4) More details are needed so that results can be reproduced by other researchers, eg. audio and video frame rate, input audio length,  loss functions.