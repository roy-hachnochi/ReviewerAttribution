SUMMARY  This paper considers an optimistic variant of Q-learning in an episodic reinforcement learning setting where the transition matrix at each stage is fixed but may be different from the transition matrix of the other stages. Regret bounds are derived that match a lower bound derived in this particular setting up to a factor of sqrt{H}, where H is the episode horizon.   EVALUATION  This is an ok paper that adapts the episodic RL analysis of Azar et al (ICML 2017) to an optimistic Q-learning variant and generalizes it to the case where transition probabilities may be different at different stages. Due to the latter particularity the lower bound that can be achieved is worse than the lower bound in Azar et al (ICML 2017) - which should be pointed out in the main part of the paper and not only in the appendix however. As far as I can tell, apart from the (rather artifical) generalization about the stage-dependent transitions, the analysis is quite similar to Azar et al (ICML 2017). Due to the more general setting this gives regret bounds that are worse by a factor of sqrt{H}.    In my opinion the authors try to oversell their results however. First of all, while the title suggests an analysis of Q-learning, the paper considers a particular variant of Q-learning with exploration bonus and a predefined fixed learning rate, and the analysis is restricted to a rather particular episodic setting. The paper claims to give the first analysis of a model-free RL algorithm, yet does not provide a clear definition what precisely distinguishes a model-free from a model-based algorithm. The vague claim that "model-free algorithms (...) directly update the value function" may as well apply to the original paper of Azar et al (ICML 2017), which employs optimistic Q- and value-function, too. Finally, the comparison to other algorithms is rather unfair. In particular, mixing up results for the episodic and the non-episodic case the way it is done in Table 1 is in my view highly misleading. Moreover, it is obvious that if the other algorithms used for comparison would be given the same information as the Q-learning algorithm considered in the paper uses (i.e., that transitions are the same at the same stage) would perform better than claimed in Table 1.   Overall, in my view the paper's main shortcoming is that it does not discuss the differences between the given optimistic Q-learning algorithm and the algorithm of Azar et al (ICML 2017). I think it would be necessary to argue that the latter algorithm cannot be rewritten as a Q-learning variant. Otherwise, the paper at hand would add little to the contribution of Azar et al (ICML 2017).   COMMENTS  - I did not see why when considering the episodic setting as a special case of MDPs (footnote 1 on p.3) the diameter D=H. Depending on how easy it is to reach a particular state, I think the diameter may be much larger than that.  - In the algorithm, p should be given as parameter. The alpha's as given in (3.1) should be specified either in the algorithm or in Thm 1.   - In l.149, there seems to be missing something in the last part of the sentence.  - Thm 3: As said, it should be explicitly mentioned here why this result is not in contradiction to the paper of Azar et al (ICML 2017).  - While I believe that one can derive PAC bounds from the regret bounds, I could not follow the argument between lines 171 and 172. In particular, I could not see why random policy selection would be considered for that.    - I found notation a bit unusual. Typically, t denotes a time step, and delta would be a more common notation for the error probability.   - The paper uses two different *-notations.  - There are a few typos, missing words (mainly articles and 'that'). In general. the paper should be proof-read for English style and grammar issues.  ---  POST-REBUTTAL  I acknowledge the definition of model-free and model-based algorithms given in the author feedback and that the presented algorithm has an improved space complexity over UCBVI. Still, in my opinion this is not such a significant advantage, in particular (as Reviewer #1 mentions) in the tabular setting. Also, it is still not clear to me that a reformulation of UCBVI could not give model-free space complexity, too. Given that the paper considers a (for the addressed practitioners) hardly relevant episodic setting, my overall assessment does not change for the moment. 