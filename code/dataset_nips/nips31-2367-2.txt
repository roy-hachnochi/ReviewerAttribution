This paper addresses explainability  for classification problems that use neural networks with ReLU activations. The gist of the approach is to estimate the minimal perturbation to apply to a given instance to flip the output of the classifier. The authors specifically focus on the binary classification problem. They also focus on explaining "negative" decisions or rejections motivated by the need to explain to end users that may be the object of the learning why an AI system is making an unfavorable decision (e.g., denying a loan).  This is a very interesting paper but there are two issues that I think should be addressed prior to publications:  1) In the experimental section, a key figure seems to be missing, Figure 2. I have found it very hard to assess how well the technique work. There are little guarantees that the generated interpretations do actually make sense for the end user. I suggest that the authors revisit this question.   2) There is a related piece of work that appeared in February 2018 that I think should be added in the reference list and more importantly discussed or compared with this work: https://arxiv.org/pdf/1802.07623.pdf This reference talks about the generation of contrastive explanations, something that can be interpreted as the missing elements that justify a classification. I recommend doing a thorough comparison with this paper in the related work section.   While I understand the argument set forth to only focus on negative predictions, I think that there could be lots of value in explaining also why positive decision are made and how safe they are from becoming negative. That's something that I wished could be explained further or at least addressed in the paper.   The algorithm also rely on an input n that represent the number of features that could be tweaked while estimating the distance. Defining this number could be tricky in my view. I also wonder if the authors have considered incorporating prior knowledge on the importance of features to ease some of the computational burden on the algorithm.   Finally, while the (popular) ReLU activations are assumed for this work and ingrained in the algorithm (exploiting the fact that these activation functions are piece-wise linear), is it possible to extend this work beyond these activation functions?