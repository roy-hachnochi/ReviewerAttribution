Given the authors' response with additional data points on SQuAD and ablation tests for FM, I have gladly increased the score to 7. Good luck!  --- This paper has presented several novel ideas for reading comprehension (RC) that I really like: (a) the Bidirectional Attention Connectors (BAC), which captures interaction between two sequences from the "attention" point-of-view and then cheaply compresses the output into a few (3) scalars. BAC seems to be broadly useful for many networks.  (b) Densely connect layers: an inspiration from DenseNet applied to RC  The authors also presented strong results on 4 RC benchmarks and have an ablation test which looks convincing. I also appreciate the authors reported on the hyperparameters they tune over.  The reason that I give this submission a score of 6 is because there's no results on SQuAD. I really don't buy the argument that the other 4 datasets are more challenging, so there's no point trying SQuAD. SQuAD has a lot of baselines to be compared over as already cited by the authors, so it's very natural to know how much these innovations bring.  I'm confident on my rating because I do research on RC as well.  A few other questions for the authors: (a) The choice of factorization machines (FM) seems interesting but also arbitrary. Do the authors have results that replace FM with other choices, e.g., a feedforward network?  (b) For the FM formula (lines 97-98), is k=n?  (c) For DecaCore, instead of [U1, U2, Q_i], why don't we consider [U1, U2, Q_i, P_i] ?  (d) For the normalized frequency feature, is it applied to both the passage and the query?