Summary: This paper proposes a general framework CROWN to efficiently certify robustness of neural networks with general activation functions.  CROWN adaptively bounds a given activation function with linear and quadratic functions, so it can tackle general activation functions including but not limited to the four popular choices: ReLU, tanh, sigmoid, and arctan. Experimental results demonstrate the effectiveness, efficiency, and flexibility of the proposed framework.  Quality: We are glad to find a work which conducts the efficiently certifying of the non-trivial robustness for general activation functions in neural networks. It is also interesting that the proposed framework can flexibly select upper bounds and lower bounds which can reduce the approximation error. This paper is of high quality. The problem is well-motivated, and the experimental comparisons and analysis are sufficient and complete. The proofs of the theorems and bounds are reasonable.     Clarity: This paper is very well written and enjoyable to read. The theory analysis and proof are clear and easy to follow.  Originality: This is an original research work especially of the robustness certification with the general activation functions, which may provide a good direction for the future research on this problem.  Overall impression, this is a high-quality paper with solid analysis, proof, and experimental results.   