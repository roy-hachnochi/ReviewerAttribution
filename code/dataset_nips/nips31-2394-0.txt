General conclusion. Good paper, interesting idea, slightly underwhelming results.  I enjoyed reading it.  The authors describe two neural architectures for sequence-to-sequence learning.  And claim that this is specifically interesting for biological sequence tasks such as predicting secondary protein structure from the amino acids. The first is "Edit Invariant Neural Network" (EINN).  The EINN is a generalization of a convolutional layer (1d convolutions over sequence steps) where filter elements can be "skipped" or can "skip" over input elements to maximize the matching between input and filter.  The second is just a 1d convolutional network over sequence data, so not exactly new.  The authors demonstrate that CNNs can act as matchers for star-free regular expressions, and that in some cases EINNs can outperform simple CNNs, although they can be much more computationally expensive.  The paper seems to make two interesting contributions: - The description of an EINN layer to use for sequence matching, based on a novel differentiable variant of the Needleman-Wunsch algorithm. - The analysis of how CNNs can be used as learnable regular expression matchers  The paper was generally well written, and the introduction of a differentiable sequence matching layer seems like it could be useful as the basis for future work.  It is a bit unfortunate that the authors could not find a more scalable variant of EINN, since for the larger datasets EINN was not even used because it was too slow to run, so it just came down to running a plain cnn.  It's also unfortunate that the improvements due to EINN are so minor - and leads one to wonder whether this idea is useful in practice.  However, the novelty of introducing a new idea like this makes it, in my opinions, worth publishing, despite not having stunning results.  It seemed like the EINN could have been slightly better described.  Rather than include Algorithms 2 and 3, which are barely referenced in the text, and seem to be just the result of applying chain rule to Alg. 1 (unless i'm missing something), it would be good to have an algorithm which explicitely spells out the function of an einn layer.  Note: Not sure if this is just a one-off problem, but in my printout many figure elements (e.g. the shaded box in Fig3, ) did not show up, which led to some confusion. Maybe check to make sure this is not happening to everyone.   It might be helpful to have a small regular-expressions cheatsheet in the appendix.  Minor notes by line: 21: cnns are not shift invariant.  They are shift equivariant, and the pooling introduces some local shift invariance.  48: figure 1: should indicate in caption what red line represents, and that numbers in boxes are from alg. 1 72: Unless I'm not getting something, alg 2 and 3 just implement chain rule and would just be done internally by an automatic differentiation package, i dont understand the value of having them here. 72: You define phi_gamma(a,b) in the text but then only use them in alg 2,3, so maybe just put it in caption (if you keep algs 2,3 at all) 80: "show a similar value" should maybe be more specific - "do not differ by more than g"? Fig 3: explain greyed-out 1's 159,161: deepness -> depth 243: invariant -> constant 274: i don't know what residues are 