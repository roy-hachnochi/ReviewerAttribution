This paper considers the sample complexity to learn robust classifiers. It studies the problem in the l_\infty attack setting and in two data models, the first assuming the data is from a mixture of two class-conditional Gaussians, and the second assuming the data is from some distributional model with binary features. In the first model, it gives sample complexity bounds for standard classification error and for robust classification error. Most interestingly, it provides an information-theoretic lower bound showing that a much larger number of examples are necessary for learning robust classifiers. The blowup factor can be as large as \sqrt{d} where d is the dimension of the data. In the second model, it shows that if linear classifiers are used then a similar increase in sample complexity is observed. But with a variant with thresholding, the sample complexity for robust error can be similar to that of the standard error. The work also provides a range of experiments on several practical datasets frequently used in adversarial ML.   Pros: 1. The topic is interesting and important. As far as I know, it has not been systematically studied before (from the theoretical or empirical side). The work provides interesting results along this direction. 2. The lower bound in the Gaussians model is very interesting. It's information-theoretic, so it suggests that the vulnerability to attacks can stem from the sample size, not necessarily from algorithmic design. It's proved in a very simple model so it's reasonable that this insight carries over to more general/practical cases. It's also practically useful, suggesting that one needs to check if the dataset size is large enough for robustness, before trying to improve the learning algorithms.  Cons: I'm not fully convinced that the analysis in the second model explains why adversarial robustness can be achieved on MNIST with low sample complexity. First, the model assumes that conditioned on the class, the pixels are independent. This abstracts away the semantic relations between different pixels in the image, which intuitively should be an important factor for achieving robustness. Second, there are empirical results on datasets CIFAR10 and ImageNet showing that discretizing the pixels to a few fixed discrete values does not hurt the standard error, but does not help improve the robustness. It suggests that only discretization/thresholding may not be sufficient.  