Consider synchronous non-stochastic gradient descent where data is distributed across M workers and each worker computes the gradient at each time step and sends it to the server.  Here at each round, the server needs to send the model to all workers and all workers need to send their updates to the server.  Authors propose lazy aggregated gradient for distributed learning, where at each round only a fraction of them send their updates to a server. They call this lazily aggregated descent and show that it’s optimization performance empirically is similar to that of the full gradient descent. Further, they show that the theoretical performance is also similar for strongly convex, convex, and smooth non-convex functions.   The results are interesting and I recommend acceptance. I have few high level questions and minor comments:  1. Gradient descent results can be improved using accelerated methods (e.g., to sqrt{condition number}). Do similar results hold for LAG? 2. Is this method related to methods such as (SAG / SAGA / SVRG), for example see Algorithm 1 in https://arxiv.org/pdf/1309.2388.pdf, where they choose \mathcal{M} randomly.  3. It might be interesting to see the performance of these models on neural networks. 4. What is capital D in Equation 11?  5. In line 133, alpha is set to 1/L. But wouldn’t any alpha < 1/L also works? It is not clear to me how to find the exact smoothness constant. 