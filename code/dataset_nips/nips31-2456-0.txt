This paper proposed generating text that reflects a desired set of attributes while preserving content by using an interpolated loss to promote content preservation and an adversarial loss to adhere to the desired attributes. They evaluate the results with both quantitatively and with human evaluations, principally along three qualities: attribute accuracy, content preservation, and fluency. This is a challenging task to both execute and evaluate, and I appreciated the authors' focus on both pieces.  Generally, the paper was clear and addressed an interesting problem. Section 3 could use more clarity about which pieces are novel, with citations to support the pieces that are not.  Some questions: -I was excited for the multiple attribute aspect, but the verb task wasn't what I was expecting. It seems like most of these attributes can be injected into a sentence in a very templated way. Could you motivate why you chose this task as opposed to extending the sentiment task with other attributes (or even adding the mood/tense/voice/negation attributes to the sentiment task)? -On a related note, how much diversity is there in the way that the attributes are incorporated in the generated sentences? -How did you decide how to set the interpolation probability between auto-encoding and back-translation losses?  Minor comments: -The caption in Table 4 says query sentences are indicated by color, but the table is in black and white. -"english" should be capitalized, particularly noticeable in section 4.4