Thanks for the response. The added discussions make sense to me so I've increased my rating. It'd be necessary to include these discussions in the reviesed verison.  ------- Original reviews:  This paper develops a new model for unsupervised text style transfer, which includes components that encourage content compatibility and attribute compatibility. Experiments show improved performance.  * The task of text style transfer has two well-adopted goals, namely “content compatibility" and "attribute compatibility”, using the terminology from the paper. For these goals, many techniques/components have been invented in the literature. It is easy to pick an arbitrary subset of these components and compose a "new" model. It is thus critical to make clear what difference (and advantages) the particular components and the particular combination used in the proposed model have. However,    - I didn’t see many such discussions when the authors present the model. The discussion in section 3.4 instead focuses on the sampling strategy which seems relatively minor. It’s necessary to provide more discussions. For example, back-translation is used in [22] as well; Adversarial loss on decoder hidden state is also used in [21]. What is the difference of the respective components used in the new model? Try to make it clearer.    - Some ablation study is conducted trying to validate the advantages of some of the specifications. For example, the effect of the auto-encoding loss and the interpolated loss. But I’m wondering what is the performance of using only the back-translation loss? And, what is the performance of simply combining both the auto-encoding loss and the back-translation loss with fixed weight (without annealing)? Such specifications are simpler than the interpolated loss. Does the interpolation really necessary?    - Similarly, if there is any difference between the adversarial loss and that of [21], is it possible to conduct ablation study to see how the new adversarial loss compared to that of [21]?  * The paper claims this is the first work to modify multiple attributes without parallel data. However, [18] has also simultaneously controlled more than one attributes with non-parallel and separate data.  * Section 3.4 makes a discussion of soft-sampling and hard-sampling, and claims hard-sampling is better. But there is no ablation study to support the claim.   Questions:  * It looks the results (e.g., attribute accuracy) on the Yelp Reviews data differ greatly from those reported in [21]. Is there any difference regarding the experimental settings?