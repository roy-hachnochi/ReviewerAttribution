This paper studies a variant of the classical stochastic multi-armed bandit problem they call the "known-compensation multi-arm bandit" problem (KCMAB), which serves as a model for the problem of incentivizing exploration. In this model, the principal (i.e. the system controller) cannot pull an arm directly; instead, each round a short-term (/myopic) agent arrives, and the principal can try to convince this agent (by offering some coompensation depending on which arm the agent picks) to pick an arm of the principal's choice. The agents see all previous rewards, so in the absence of any compensation they will each pick the arm with the largest historical average reward so far, essentially executing Follow-The-Leader (which is known to have regret linear in T). The principal can convince an agent to play some other arm i by offering compensation (if the agent picks that arm) equal to the difference between the historical average of arm i and the largest historical average of any arm.   The authors show two matching lower and upper bounds for the best compensation possible while optimizing regret (up to constant factors). The lower bound shows that there is some inherent tradeoff between regret and compensation: if a strategy for the principal achieves sub-polynomial regret (o(T^alpha) for any alpha > 0), then the principal must pay at least Omega(log T) in compensation. The upper bound shows that UCB (with appropriate compensation so that arms play according to UCB) essentially achieves this lower bound, achieving both O(log T) regret and compensation. (The actual statements are a bit more precise, with the explicit dependence of both regret/compensation on the distributions/means of the arms).   The paper also studies two other algorithms, the epsilon-greedy algorithm and a variant of Thompson sampling, and show that both achieve O(log T) regret/compensation. In addition, they run some simple experiments for all of these algorithms on synthetic data, and show that they match up with the theoretical results.  I think this is an interesting paper. It is true that very similar notions of "compensation" have been studied before in the incentivizing exploration literature -- Frazier-Kempe-Kleinberg-Kleinberg in EC 2014 essentially study this quantity in the case of an unbounded time horizon for a principal with discounted utility, and their setting also displays many of the phenomena discovered in this paper (in fact in their model they can get a much more precise version of the tradeoff between regret and compensation). That said, I'm surprised that (to the best of my knowledge) this specific notion of "compensation of a bandit algorithm" (for a finite time horizon) has not been explicitly studied, and I think it is an interesting quantity to understand. I think there are also a number of interesting follow up questions that can be asked (e.g. can we say anything about compensation required to implement adversarial bandit algorithms like EXP3?).  In some sense, the upper/lower bounds are not too surprising; both regret and compensation come from "exploring" (whereas "exploiting" does not cost any compensation, and only costs regret if you have not explored sufficiently). Of course, this intuition is far from a proof. The proof of Theorem 1 (the main lower bound) is quite nice, and is probably the most interesting technical result of the paper. The upper bounds generally follow from the analysis of the corresponding bandit algorithms and examining when/how much compensation is required.  This paper is well-written and easy to read. I recommend this paper for acceptance.