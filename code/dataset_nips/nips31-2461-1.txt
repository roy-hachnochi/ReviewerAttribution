This paper proposes to boost the speed of distributed CNN training by compressing the gradient vectors before sending among the workers. The authors make an argument    that the linear nature of the compression function (based on PCA) are convenient for the ring all-reduce aggregation protocal. The authors also provided empirical justification for using PCA.     However, I find it hard to understand Fig 4. What exactly is plotted? What does 3 adjacent gradients mean? Do they belong to the same filter? How are they chosen?    Furthermore, the authors don't seem to provide any justification for the time domain invariance.    In the experiments, Fig. 7(a) on page 8 appears to show that 4-bit-QSGD achieves a higher training accuracy than GradiVeQ for the same clock time.     Also, the authors used CPU rather than GPU for the experiments. Would the results be still relevant given that such neural nets train a magnitude faster with GPUs?    To sum up, I do feel unconvinced that the proposed method would have a significant impact for the reasons above.     