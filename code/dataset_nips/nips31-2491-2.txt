This paper presents the concept of contour entropy which helps locating the contour of a function using multiple information sources under limited budget.   Main strengths:  - Definition of contour entropy  - Derivation of acquisition function for multiple information sources  - Interesting simulation and real world experiment   Main weaknesses:  - Missing references to several areas of Machine Learning (see below)  - No comparison with baseline approaches (there is one in the appendix, but it does not seem to be showing significant improvement)  It looks like the authors have a good grasp of the contour location literature, but there are several areas of ML that are related to the overall problem studied here, which should be referenced. The acquisition function derived in eqn 6 and 7 are basically a form of expected utility, and it has been used in a similar context by Melville et al.  An Expected Utility Approach to Active Feature-value Acquisition http://www.prem-melville.com/publications/afa-icdm-05.pdf (Please also see their follow up work)  Another related area in the context of learning from multiple weak, and possibly noisy approximations of a function is ensamble methods. I would again refer to a thesis by Prem Melville, Creating Diverse Ensemble Classifiers to Reduce Supervision.   On the experimental front, the simulation experiment is interesting, as is the experiment from chemical engineering domain. However, it's not clear what you're comparing your proposed approach to. I would recommend your baseline comparison from appendix to the main paper, although those results do not seem to be significantly better than the baseline.   Question for authors: line 138: for the evaluation of the entropy, you mention that it is deterministic from equations (4) and (5). Are these always tractable for complex functions?   