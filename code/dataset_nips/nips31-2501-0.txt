This paper provides the analysis of T-greedy Policy Iteration methods, both in the  online and approximate settings. In this context the paper makes several points. First, it shows that multiple-step greedy policies using soft, step-sized updates is not guaranteed to make improvements. Second, to address the above issue, the authors propose a "cautious" improvement operator that works at two time scales: first it solves the optimal policy of small-horizon MDP with shaped reward, and then it uses the computed value to shape the reward for the next iteration. Third, the paper quantifies the tradeoff between short-horizon bootstrap bias and long-rollout variance, which corresponds to the classic \lambda tradeoff in TD(\lambda).  This is a dense, theoretical paper. While the authors try to provide intuition behind their results, I still found the paper rather difficult to follow. Ideally, the paper would provide some experimental results to aid the intuition, but it doesn't. This is a missed opportunity in my opinon. There are many questions such expriments could answer. What is the impact of "cautious" updates on convergence? Quantitatively quantify the tradeoff between short-horizon bootstrap bias and long-rollout variance, and many more.   Otherwise, the results presented in the paper are significant. In particular, I can see the "cautious" updates having a considerable impact in practice, assuming experimental results are going to convincingly illustarte that there are cases of practical interest in which just using soft, step-sized updates fails to converge.  As a final note, I was not able to double-check the proofs so please take this review with a (big) grain of salt.