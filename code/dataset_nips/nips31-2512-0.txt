Summary:   The paper provides a new convex duality framework to understand the training for GANs when the discriminator is constrained - which is a a relevant practical and useful departure from the theory in vanilla GAN proposed in Goodfellow et al 2014, where the treatment was if discriminator was allowed an unconstrained set of functions.   Detailed Comments:  Clarity and Originality :  The paper is well written and the proposed framework of convex duality is quite novel and rigorously treated. It is an important contribution to NIPS this year.   Quality and Significance :  The problem is quite significant as in general discriminators are constrained to smaller classes such as neural nets which certainly change the JS divergence treatment as in Goodfellow et al 2014. They show that if the class F is convex, then the minimizing JS divergence for unconstrained class is equivalent to minimizing JS divergence between the real distribution and the closest within the generative class penalized to up to sharing of same moments. This is quite a neat formulation and perhaps one of its kind. This framework accounts not only for vanilla GAN but also for f-GAN and WGAN as shown in Section 4.  While in general the neural nets may not be convex, in Section 5 the authors circumvent this by considering an approximate solution in the convex hull of neural nets, which also helps suggest using a uniform combination of multiple discriminator nets. The only lacunae which can be felt is lack of enough experimental validation. It will be nice to add further tests on more datasets. However, overall this is a sure accept paper.   