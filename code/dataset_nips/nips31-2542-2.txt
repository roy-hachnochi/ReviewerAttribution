The paper studies an important problem of sparse linear regression where the features at each example are missing or unobserved. The unobserved patterns are different from samples but we are allowed to have a control to what features are selected at each iteration of the algorithm. To find the true underlying sparse parameter, a standard method is to solve the popular risk l2 minimization with sparse constraint.   Contribution: The authors propose to solve this sparse constraint optimization problem via stochastic iterative hard thresholding method. This algorithm has been studied before but the authors make a twist to go around the unobserved features issue. They also propose a refining algorithm that can fine tune the solution after the correct support is attained. They additionally provide a hybrid method that combines the searching and fine tuning algorithms in a iterative manner. These algorithms enjoy theoretical guarantee with sample complexity being better that previous methods, e.g. the sample complexity is linearly proportional with data dimension and inversely proportional with the function value error.    Main problem: The paper is clear and easy to follow until the algorithm description in paper 4. In particular, it is unclear to me what does x_i^{(b)} mean in the Algorithm 1. Is the subscript i meant an entry of the vector x^{(b)} where b is the sample index. What is x_i^{(b)} | J_i?  These notations are not defined anywhere in the paper. For that notation problem, I can only follow to prove up the the equation (4) in the supplement. Up to that point, the proof is pretty standard and follow closely the analysis of the citation [1]. The authors may want to emphasize which part of the analysis is novel and is not seen by previous works.   Although I canâ€™t not judge the correctness of the proof, the theorems sound right to me. Overall, the paper offers a good theoretical improvement, its clarity needs to be improved in order to reach to wide audience.   