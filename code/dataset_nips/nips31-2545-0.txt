The paper proposes a way to leverage unlabeled data for training deep kernel learning methods. The authors construct a loss function in the form of the marginal likelihood regularized by the variance of the predictive distribution at the unlabeled points and prove that minimizing their semisupervised objective is equivalent to solving a regularized posterior optimization problem.  The paper is well written, easy to follow, and has a clear motivation and strong empirical results (the method compares favorably against popular baselines).  A few questions/comments for the authors:  - Does the proposed regularizer impact the calibration of the model? The intuition for regularizing the model using unlabeled data is clear (e.g., from the representation learning point of view), but it is unclear whether or not directly minimizing the variance on unlabeled points results into an over-confident model. It would be interesting to see how that turns out in practice.  - The authors mention inductive and transductive types of semi-supervised learning in the introduction but it seems like they do experiments only in the inductive setting (if I understand correctly). Would be nice to see results for the transductive setup (or make it explicit that the focus of the paper is on inductive regime).  - Would be nice if the authors can include a note on the implementation of their models and whether they plan to release code/data.