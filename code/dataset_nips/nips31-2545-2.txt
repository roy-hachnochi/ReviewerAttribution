 Thanks for the clarification. It'd be good to add the complexity discussion to the revised version as it's usually a big concern for GP-based methods.  ----  This paper extends the deep kernel learning approach to enable semi-supervised learning. The method is to minimize the predictive variance on unlabeled data. The approach is connected to the posterior regularization and the model is learned with the variational framework. Improved results are obtained on an extensive set of datasets/tasks.  The intuition of minimizing the predictive variance on unlabeled data is reasonable and well motivated in the paper. The math development of the algorithm looks sound. The experimental performance is impressive compared to baselines like DKL.    A practical issue of Gaussian processes is the computational efficiency. The base DKL algorithm put much emphasis on approximate computations that reduce the computation complexity to linear wrt to data size. Did the algorithm here use the same approximations? What's the computation complexity wrt data size? What is the training time of the experiments?