The paper studies the dynamic effects of the continuous gradient flow upon the scaling of the weights in multilayer Neural Networks. IMHO, the main result of the paper is Theorem 2.1 (line 148) that establishes the existance of (N-1) integral of motions where N is the number of layers. It follows directly from eq. (6) [line 149] that the L2-norm of all the layers are are at the constant offsets from each other and that it is sufficient to follow the dynamics of L2 norm of the 1st layer to recover all the other norms provided the initial offsets are known. This is an interesting result and if true it warrant further studies.  However the paper is incomplete, for it failed to consider several very important cases:  (1) Run a simple experiment with a Feed Forward Neural Network (e.g. 3-layer MLP) and observe the dynamics of L2-norms of Weights at each layer to confirm or disprove eq (6).  (2) Effects of discrete time flow on validity of Theorem 2.1 are not studied.  (3) Prove of Theorem 2.1 is given based on the gradient flow (5) [line 143]. Will it hold for SGD with Momentum?  (4) Batch-normalization introduces extra scaling in X-space that can effect the relationships used in derivation of Theorem 2.1. 