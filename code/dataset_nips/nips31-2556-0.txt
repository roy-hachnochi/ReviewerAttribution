The paper introduces a novel approach for learning models that generalize to new target distributions different from the training one. This is especially interesting for settings where target distributions are unknown, i.e. there is no available data for them (labelled or unlabelled). The paper follows an adversarial training approach similar to (Sinha et al., 2018), i.e. a min-max objective which model parameters minimize an expected loss, where the expectation is taken over the furthest data distributions that are at most ρ distance away from training distribution P0. Crucially, the paper defines a Wasserstein distance between distributions in the semantic space, and proposes an efficient iterative algorithm for optimizing the loss. Experimental evaluation is performed in both digit classification and scene segmentation settings, where the model is trained on one dataset (e.g. MNIST in classification) and tested on multiple different datasets (e.g. SVHN and MNIST-M in classification).  Strengths  The approach is novel and very interesting  The paper is very well written  Experimental results are promising, especially the digit classification setting  Weakness  It is not clear if the semantic Wasserstein distance would be very informative, especially early in training, since representations won’t necessarily be very informative. I would like the authors to comment on this and on any practical issues in training.  Results on scene segmentation are not as successful as in digit classification  In conclusion, I think this is a strong paper both theoretically and practically which makes a worthy contribution to NIPS.