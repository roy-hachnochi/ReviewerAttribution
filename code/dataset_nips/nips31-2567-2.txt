This paper addresses one major limitation of Importance Sampling (IS) estimators for RL tasks, that is its exponentially growing worst case variance with the trajectory length. They name this limitation as "curse of horizon". By considering an alternative definition for IS estimators which is based on the state visitation distribution, the remove the dependence of the estimator in trajectory length. With the new definition, the only difficulty that remains is to estimate the state visitation ratio between evaluation policy and behavior policy. For this, they propose a minimax loss function which simplifies to a closed-form solution under some assumptions. The numerical experiments illustrate the superior performance of the estimator with respect to IS estimators.    The paper is well-written. I believe that the contribution of the work is original and significant to the field. In my point of view, the only missing point that it is not well-discussed is how they can replace IS estimators. Following, I have several suggestions/questions that might improve the presentation of the work.    1) I think that several steps are missing including an implementation guide between the theoretical section and experiments. For example: 1-1) How hard is its implementation w.r.t. the IS estimators? It seems that the only extra step is a supervised learning before the RL training, right? 1-2) Once \hat{w} is available, what is the complexity of computing $R(\pi)?$ Better or worse than IS estimators? 1-3) I think that providing an algorithm (maybe in Appendix) which summarizes the steps would help the reader. 2) I would suggest comparing the numerical experiments with doubly-robust estimators (Jiang et al. 2015) as the state-of-the-art off-policy evaluation estimators. The current baselines are sufficient for showing the capability of your proposed method, but one cannot confirm whether they are SOTA or not. 3) How restrictive is it to have $f$ in a unit ball of RKHS (Thm2)? Please elaborate.  Minor: In (7), $m$ is not defined beforehand -- it is introduced in Appendix A for the first time.   =========== After reading the rebuttal, I found clear explanation to all my questions/comments. Thanks. 