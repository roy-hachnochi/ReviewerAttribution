The authors present an approach for learning loss functions for reinforcement learning via a combination of evolutionary strategies as an outer loop and a simple policy gradient algorithm in the inner loop.  Overall I found this to be a very interesting paper. My one criticism is that I would have liked to see a bit more of a study of what parts of the algorithm and the loss architecture are important.  The algorithm itself is relatively simple. Although I appreciate the detail of Algorithm 1, to some degree I feel that this obscures the algorithm. In essense this approach corresponds to "use policy gradient in the inner-loop, and ES in the outer loop".More interesting is the structure of the loss architecture. It feels like this is giving an interesting inductive bias to the problem of learning the loss. It would be informative to see more of an ablation study of this architecture, i.e. if the loss were simplified to be a sum over "learned instantaneous rewards" and perhaps some adaptive discounting factor how much would the performance drop off.  The bootstrapping process with PPO also seems quite important. But how is the performance affected without this bootstrapping? Does EPG fail to learn without this priming?  The experiments are performed on a number of continuous control tasks and are quite thorough. I would have liked, however, to see more of an examination of the failure modes of EPG. It seems like there is no way to guarantee performance for this method when applied to problems outside of the training set, but I would have liked to see a more thorough examination of where this starts to fail. To some degree this is examined in the GoalAnt setting, but we don't see how far before this starts to break down.  Minor comments:  - In the introduction I'm not sure that the statement "RL methods optimize for short-term returns" is a fair statement. It's more that many current techniques may under-explore and not see more far-reaching rewards. I would go further in classifying "RL" as a problem rather than a set of solutions, under which the proposed EPG method is another "RL method".  - line 82: typo "oftent".