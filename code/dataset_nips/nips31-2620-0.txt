The authors pose and study an interesting question: Is it possible to use small amount of data to effectively estimate a likely performance metric of a predictor learned using large data?  The authors show that the answer is yes, in some interesting cases. Assuming the data comes from an isotropic distribution in d-dimensions and the labels are arbitrary noisy functions of datapoints, they show that the variance of the best linear classifier can be accurately estimated using only O(sqrt(d)) samples. This is surprising because even in the noiseless setting, \Omega(d) sample size is required to learn any function coorelated with the underlying model. They show similar results for binary classification.  The technical part and the idea of using higher moment estimation to approximate the first moment is very interesting. The authors also have empirical resuls to support their claims. Overall, I find this a clear accept.  