The authors study the problem of estimating the noise in settings of linear regression. More precisely, given (x,y) pairs which are generated as y = beta*x + Z where Z ~ N(0,delta^2), how many samples are needed to estimate the value of delta? This is useful to estimate how many samples are required to truly perform accurate regression. Interestingly, when the x's are distributed as a Gaussian (say), sqrt(d) samples are sufficient to get an estimate. Note that even with no noise, d samples would be required to learn the classifier, as otherwise the system may be underdetermined. The authors also have results for arbitrary covariances, binary classification, matching lower bounds, and some neat experimental evaluation.  This is pretty much a prototypical example of what I want to see in an ICML paper. The problem is natural and meaningful, the method is elegant and practical and requires some non-trivial technical insight, and the experiments are compelling and interesting. The paper is well written for the most part.              The goal of the algorithm is to estimate the variance of the added noise. The method is to estimate the variance of the labels, and then subtract out the variance due to the variance in the data, leaving the variance of the noise. In general, it isn't clear how to design an unbiased estimator for the label variance, so instead the authors use higher order moments (which are estimatable) and carefully combine them to get an estimator for the label variance with low bias. This is a neat method, which has also been similarly used by Kong and Valiant. It's nice to see another application of this method, and this paper might be more illustrative than the other (as the problem is a bit less cumbersome, in my mind). 