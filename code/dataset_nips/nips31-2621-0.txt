This paper introduces the problem of algorithmic assurance where they want to systematically check if a (supervised) machine learning algorithm is not deviating from their "true goal" (approximating a function given by a set of observations). The authors propose to use Bayesian optimization to automatically find the input locations (examples/decisions) where the maximum deviation occurs, which means that the overall system won't be producing larger errors. The authors justify their approach and also extend it to "Multi-Task Algorithmic Testing" where they combine this BO problem with a multi-arm bandit setting, developing a Hedge-Bayesian optimization algorithm.   The abstract and introduction of this paper are very strong --- probably the best of my batch! Unfortunately, I didn't get too excited about their approach since it's mainly a combination of well-known algorithms. So, my overall score is really a mix of these two feelings.   Considering the quality and clarity of this manuscript, I do think this paper can be accepted. But, with respect to the amount of novelty and the impact of this work on a very research-focus community as NIPS, I'm tending to say 'week reject'.   Maybe, a small section (or paragraph) of related work will help the reader to appreciate the difference from your analysis to the current research in MAB.   minors:   line 91: constructing?  line 153: x_0being  -----  After the discussion and author's response:   I think the innovative application of this paper is the strongest feature of this paper. And after reading the author's response I think the main research contribution is also reasonable.   I would stress however that Theorem 1 is not formal or particularly interesting. Making it an illustrative example is definitely a good idea.  My final recommendation is `weak accept`. 