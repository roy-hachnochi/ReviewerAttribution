This paper presents a method to find the highest divergence between an ML model their goals by mapping the problem to a Bayesian optimization problem. The idea is very original and I found the paper very creative. The text is easy to follow.       Theorem I is not properly defined. The proof is based on an example, which the authors claim can be easily generalized, but they do not provide such generalization. In fact, for that given example, their reasoning is limited to certain stationary kernels. With a general kernel, like a nonstationary kernel or a periodic kernel, there is no guarantee that the narrow peak is avoided. In fact, there are previous works in the literature which explicitly have to add extra features to the standard BO to avoid such optimum, because standard BO will eventually find them. See for example [A].       The combination of a hedging algorithm with BO is not new. It was previously used in [B] to find the best acquisition function. Although the use of hedging algorithm and the application is different in this paper, due to the resemblance, it should be mentioned. In fact, the work of [B] had some issues to seems to be reproduced here, about the hedging algorithm not exploring enough.       Concerning the exploration, the experiments show how the multi-tasks scenarios are biased towards a single "task". This is specially surprising in the MNIST case, where there are many confusions between 1 and 7 or between 4, 5 and 6. In fact, 3 is confused with 8 or 5, but those are not explored as much.       Also for the MNIST problem, it is surprising that, in the single task scenario, the most difficult setup is where the digits are barely distorted (almost no displacement in X or rotation).       Finally, the authors should address how, in their experiments, the round-robin BO is faster and the final results are competitive with the Hedge BO. Why is that happening?        [A] Jos√© Nogueira, Ruben Martinez-Cantin, Alexandre Bernardino and Lorenzo Jamone (2016) Unscented Bayesian Optimization for Safe Robot Grasping. In Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems.       [B] Matthew Hoffman, Eric Brochu and Nando de Freitas. Portfolio Allocation for Bayesian Optimization. 27th Conference on Uncertainty in Artificial Intelligence (UAI2011) --- Given the authors response, I think this work might be worth for the NIPS community as a preliminary resource in an interesting line of research.