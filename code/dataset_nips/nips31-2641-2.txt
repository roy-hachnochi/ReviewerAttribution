Summary -------  The paper introduces a low rank + sparse additive effect model to jointly estimate main effects and interactions. Statistical guarantees are derived and an optimization algorithm combining Frank-Wolfe and coordinate descent is proposed.  The paper seems technically sound, well written and overall easy to follow.  Regarding statistical guarantees, the authors mention [27, 4, 5, 13] (noiseless setting) and [17] (additive noise setting) as prior analyses but it's not clear what's the difference with the proposed analysis. I would explicitly explain the difference right from line 85.  Unfortunately, the numerical experiments were quite limited. The first experiment (Table 1) is essentially a sanity check on data generated from the model. The second experiment is interesting but only compares two variants of the proposed approach and there is no comparison with other baselines.  Overall, this is a fine paper but the experiments are too lacking to make it a clear accept.  Detailed comments -----------------  * Line 56: often referred to as Frobenius inner product  * Line 81: Could you briefly explain in what way LORIS generalizes [4], so that the reader doesn't need to read [4] to figure it out.  * I believe the work below could be used to solve a constrained version of (9)  Frank-Wolfe Splitting via Augmented Lagrangian Method Gauthier Gidel, Fabian Pedregosa and Simon Lacoste-Julien. Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS), 2018.   * Figure 1: the labels are too small  * Line 294: what do you mean by mixed data model here? could you be more explicit, perhaps give the exact model specification?  * I feel the paper doesn't discuss related work sufficiently. The discussion in the introduction is quite limited. The authors should spend more time explaining what's missing from the existing works and why it's important. The authors could create a related work section after the introduction, move the solver comparison ("Comparison to Prior Work") there and add more discussion on existing low-rank + sparse models and their statistical guarantees.