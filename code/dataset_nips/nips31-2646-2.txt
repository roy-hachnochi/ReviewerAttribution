This paper builds an inference-aided reinforcement mechanism leveraging Bayesian inference for crowdsourcing, and reinforcement learning techniques to learn the optimal policy to incentivize high-quality labels. The proposed method is proved to be incentive compatible, and its benefits have been empirically shown. This paper is well written, and the technical part is sound. The idea of using reinforcement learning for crowdsourcing incentive mechanism design is interesting and novel. Different from existing solutions, it can adaptively change the payment based on the performance of workers. Moreover, it’s impressive that the proposed mechanism works well for different kind of workers, e.g. rational, QR, and MWU. I don’t have much concern about the paper’s quality, but have several questions about the details for the authors: +Line 124, you claim “the EM and VI methods tend to heavily bias toward over-estimating the  aggregated label accuracy when the quality of labels is low.  ” Can you explain why this happened? Moreover, there exist many other label aggregation methods, such as minimax entropy estimator; will these methods bring further improvements comparing with the Bayesian estimator you used here? +You use a lot of estimated values, such as accuracy, utilities, etc., instead of the exact values when defining the MDP. Can you analyze the effect of this approximation? For example, you can show the performance under different approximating levels. +Why you choose the RTE dataset for experiments? There exist many public available benchmark datasets such as WebSearch, Age, TEMP, etc., so you at least should give some explanation about the dataset selection. + You mix data with random noise in order to simulate strategic behaviors of workers. However, the give crowdsourced labels already contain noises. So will this influence the conclusion we observed from the experiments? + For empirical analysis on RIL, the hyperparameter selection seems arbitrary. Can you give some hints on how to select these values? +It’s better to conduct some real people labeling experiments with the given mechanism, and compare the performance and efficiency with other mechanisms.  After Rebuttal: Thanks for the reply, some of my concerns have been addressed. To further improve the paper, I still suggest the authors to do some real people labeling experiments.