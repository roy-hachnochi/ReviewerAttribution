The paper presents a new score to estimate the accuracy of any classifier. Knowing when to trust or not to trust a model output is important for many application of machine learning models, thus this task is well motivated.   The paper is well structured and well written. The paper first introduces the goal, covers related work, then introduce its algorithm for computing the new score: the trust score. The trust score is defined as the ratio between the distance from the testing sample to the alpha-high-density-set of the nearest class different from the predicted, and the distance from the test sample to the alpha-high-density-set of the class predicted by the classifier. Algorithm 1 covers how to estimate alpha-high-density set, and algorithm 2 covers how to compute the trust score.   The section 4 gives theoretical guarantees of both algorithms. To be honest, I was not able to follow all the proofs in the appendix, making it hard for me to properly evaluate this paper.   The section 5 gives empirical performance of the trust scores across different datasets, different classifiers, and different representation of the dataset. The evaluation is easy to understand and was persuasive. All the graph in Figure 1 and Figure 2 shows that the proposed “trust score” outperforms either the model performance or 1-NN ratio.    Questions to the authors: - How is B(x, r) defined in the algorithm 1?  - Does this method apply as is to a binary classification case? If it works better than the model’s confidence score, this implies that this can improve model’s accuracy at varying threshold.  - How sensitive is the method to hyper parameters, especially to k?  - Is the model performances on par with the state-of-the-art for these tasks? (Not important, but would be good to include) - when the axis are the same, ‘when to predict’ and ‘when not to predict’ is simply inverse of each other?   Minor comments: - Some papers, like 12, is cited with ArXiv instead of its official venue (ICLR). Please fix accordingly. 