The proposed ProxSVRG+ generalizes and extends previous SCSG and ProxSVRG. Using a new convergence analysis compared to ProxSVRG, the paper shows that ProxSVRG+ is faster than ProxGD even for small mini-batch size. Besides, a moderate mini-batch size can be needed for ProxSVRG+ to achieve the best convergence result. This provides a solution to an open problem from ProxSVRG paper. At last, the paper proves linear convergence rate of nonconvex function satisfying PL condition without restart, unlike ProxSVRG.   Overall, the paper is well-written and easy to follow. Figure 1 is informative to show the relation among ProxSVRG+ and other related algorithms. The experiments shows clear the advantage of ProxSVRG+ and its preference of using small mini-batch size. Regarding the paper, I have the following concerns  1. The paper argues theoretically that the best mini-batch size required for ProxSVRG+ is 1/{\epsillon^{2/3}}, where \epsillon is the convergence criterion. Considering the claims in the comparison with ProxSVRG, \epsilon should be greater than 1/n for a smaller mini-batch size, would it be a problem to claim a good convergence for ProxSVRG+?  2. As in the ProxSVRG+ Algorithm, a subsample gradient control step is used (step 3-5) with the batch-size B. However, in the experiment, B is not mentioned. could you specify the B you use in your experiments and how the performance of ProxSVRG+ is changed regarding different B. (please refer to SCSG paper.)  3. At line 225, it says ProxSVRG+ always performs better than others, is it overclaimed, due to you are using fixed step size (for example ProxSGD is suggested to have diminished step-size rather than a constant one. please refer to ProxSVRG paper) for all algorithms and the mini-batch size are limited to 256, which is later on claimed as the best for ProxSVRG+.  minor:  1. In Algorithm 1, T is not defined and used. And S should not be so-called number of epochs (1 epoch = n SFOs)? or please define it.