Summary:  The paper shows (1) that it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even a reasonable simplicity prior on the set of compatible decompositions cannot distinguish between the true decomposition and others that lead to high regret. Together, these two theoretical results demonstrate that in order to accurately infer a human's reward function from demonstrations, we need to make normative assumptions about human planning that cannot be derived from observations of the human policy.   Most prior work in inverse reinforcement learning models suboptimal behavior using noisy actions, e.g., Boltzmann rationality in maximum entropy IRL. A few recent papers propose extensions that model systematic biases in human planning, like false beliefs, temporal inconsistency, risk sensitivity, and internal dynamics model misspecification. This paper gives a theoretical motivation for further work on modeling human irrationality in order to accurately infer preferences from actions.  Strengths:  The problem setup is quite general, which enables the results to apply broadly to the various inverse reinforcement learning and Bayesian inverse planning frameworks.  Each theorem is accompanied by intuitive explanations; for example, the connection to Hume's is-ought problem in Section 4.1, and the characterization of the function B in lines 239-240.  Weaknesses:  I was a little confused by the introduction of the function B in line 255, but I finally understood its usefulness when I saw it vanish in the last step of line 265 to establish the complexity lower bound!  A few minor corrections:  - Line 91 should start with <S, A, T, s> instead of <S, A, R, T, s>  - Line 253 has some typos  - Line 255 should contain (-pa, -Rr) instead of (pa, Rr)  Overall, the paper has excellent quality, originality, and significance. Clarity could be improved a bit by addressing typos.