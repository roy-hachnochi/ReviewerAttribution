- The authors adapt the framework from [8] (A consistent regularization approach for structured prediction) to solve the problem of manifold regression. The framework from [8] allows the manifold regression solution to be represented as a minimizer of the linear combination of loss over training samples (similar to the kernel trick). Their main theoretical contribution is to show that the Hilbert space condition required by [8] over the loss function is satisfied by the squared geodesic distance. This part is novel and interesting.   - The part on generalization bounds and consistency is highly similar to [8], and is a relatively small contribution.   - Experiments on learning synthetic positive definite matrices, and two real datasets with fingerprint reconstruction and multilabel classification, show the method performs better than previous approaches. However, there is no comparison with manifold regression (MR[30]) on the matrix experiments and the multilabel classification experiments. It is easy to beat kernel least square because it doesn't take into account the manifold structure, so it would be good to provide comparison with manifold-based methods on these two sets of experiments.   - In terms of efficiency, what is the runtime of the method in training and prediction? The prediction function in equation (2) grows in complexity with the training set size, and performing iterative optimization on it as in section 4.1 can be quite expensive.  