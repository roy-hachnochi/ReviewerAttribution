  Main idea:  This paper applies Monte-Carlo sampling to interactive POMDPs. In interactive POMDPs agents’ belief over the state includes a belief over the belief of other agents.  In particular the authors show that intentional models produces better results on two standard POMDP benchmarks (Tiger and UAV). An intentional models other agents are as pursuing goals rather modeling their behaviour.  Comment:  This paper is currently hard to follow unless the reader is already familiar with the I-POMDP framework.  Results: The results seem rather inconclusive. There are no standard baselines provided for comparison. The only takeaway is that the intentional models (of any level) do better than the subintentional one. No p-values are stated for the results.  It is surprising that the results are not compared to standard multi-agent RL methods for addressing partially observable settings (this is left for future work).  Algorithm 2: ‘sum’ variable is never initialised. Line 9: P(o) + O * P: This line has no effect. Possibly a typo? These two issues make the interpretation of the algorithm difficult.  Minor issues: Line 183: “agent’s strategy level. And then the model” [run-on sentence]  Line 165: “belief over intentional model space” -> “belief over the intentional model space” (‘the’ is missing in more places in the text, probably good to run a grammar checker).   I found the rebuttal and other reviews insightful and my concerns mostly addressed. I have improved my score to reflect this. 