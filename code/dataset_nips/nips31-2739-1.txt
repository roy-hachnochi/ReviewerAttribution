After rebuttal: The authors addressed my minor concerns, and my score (accept) remains unchanged.  The authors provide two successive overapproximation algorithms that efficiently sample from the rows of a kernel matrix according to an approximation of their leverage scores. They then provide a preconditioned ridge regression algorithm that uses these samples to efficiently approximately solve kernel ridge regression problems.  The algorithms are refinements of previous algorithms published in AISTATS and NIPS, with superior dependence on the parameters of the problem. In particular, all three algorithms have run time that depend only on the effective dimensionality of the dataset, not the number of points in the training dataset. These are the first such algorithms, so the paper represents a significant contribution towards our understanding of the time complexity of solving kernel problems. The approach itself is also novel as far as I am aware: it is essentially a homotopy approach that samples a number of points that depends on the condition number of the dataset and the current regularization parameter, samples from these points using the current overestimate of the leverage scores, then uses the resulting points to estimate less overestimated leverage scores, and continues this process until the overestimation factor is 1.    Strengths: - novel approach to estimating ridge leverage scores that may lead to more interesting work in the future - provide algorithms whose runtimes are independent of the size of the dataset - strong experimental evidence supports the claim that these algorithms are the current best available for these problems, measured both with respect to time and accuracy  Weaknesses: - The paper is very dense, and hard to follow/digest. Rather than present to empirically and theoretically equivalent algorithms for leverage score sampling, I would prefer the authors present the rejection sampling algorithm which is simpler, and dedicate more time to explaining why it works intuitively - The paper needs thorough proofreading for readability, grammar, typos, inconsistent and wrong capitalization, bad and unexplained notation, and missing references to equations, theorems, and citations. As examples, I point to the two almost consecutive typos on line 112; the incorrect definition of the indicator variable on line 111, the unused input q_2 to Algorithm 2; the incorrect dependence on lambda instead of lambda_h in line 3 of Algorithm 1; the incorrect inequality on line 90; the dependence on lambda instead of lambda_h on line 108; the unexplained constant c2 on line 133; and the way that t is used in half of the paper to mean an accuracy parameter then it later becomes a number of iterations. - The experimental section uses the terms Opt LS and REJLS in the plots and tables, but refers to them as BLESS and BLESS-R in the text. Similarly it uses RRLS and "musco" interchangeably without specifying these are the same. The plots cannot be read when printed in black and white: please add markers and change the line styles, and use larger labels on the axes. Figures 4 and 5 would be more informative if they plotted the AUC vs time, as described in the text.   - What is Nystrom-RR, referenced on line 189? What is the function g in Theorem 2?