 The paper deals with the problem of comparing different neural network representations, proposing projection weighted CCA (Canonical Correlation Analysis) which builds on top of another framework [22]. Both CNN and RNN are taken into account. The improvement wrt [22] is this latter one does not effectively distinguish between the signal and the noise in the representation.  Section 2-2.1 give some preliminary notions, stating essentially that CCA directions are orthogonal, but I appreciated the view of CCA w.r.t. neural network representations. Section 2.2 details why the proposed method is better than [22]: essentially, whereas [22] assumes that all CCA vectors are equally important to the representations, PWCCA proposes the projection weighting technique, which assign a weight to the single directions given by the sorted CCA coefficients.  Results are of different kinds, built upon CIFAR 10 for cnn and Penn Treebank and WikiText-2 language modelling tasks for rnn:  -networks which generalize converge to more similar solutions than those which memorize  -wider networks converge to more similar solutions than narrow networks  -across otherwise identical networks with different random initializations, networks converge to diverse clusters of solutions (Section 3.3).  -RNN exhibit bottom-up convergence over the course of training  -Across sequence timesteps, RNN representations vary nonlinearly  My question is whether these conclusions could have been reached with [22] too, and how the cons of [22] would be bad for this kind of analysis. 