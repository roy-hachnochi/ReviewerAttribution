This paper proposes an adaptive f-divergence for variational inference (VI), that both guarantees that importance weights mean is finite, and reduces fat-tail distributions. The approach entails representing f-divergence as a Hessian enforce its convexity, and using rank of the importance weights at each gradient descent of posterior approximation. In the process of evaluation, the authors provide an improved variant of the soft actor critic reinforcement learning algorithm.  The paper is exceptionally well motivated and placed within the related work. The concepts are introduced clearly. The new method is elegant, results in a simple algorithm that lies on top of in-depth theoretical analysis. The key insight is a clever representation of function as a Hessian. The results are promising and convincing. And that the paper contains additional reinforcement learning algorithm as a side effect is quite impressive. This work is very significant and original.   The only concern with the paper is the lack of proofs for the propositions 4.1.a and 5.1. Given the convincing results this does not invalidate the paper, although it would make it stronger. Also, please make a note that the proofs for Propositions 4.2 and 4.3 are in the appendix. 