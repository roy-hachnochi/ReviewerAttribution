After reading the response: Thanks for the clarification on semantic networks; It is interesting that a walk on a semantic network would not predict the pattern because similar to a semantic space, a network can capture the similarity of nouns other than animals.  It would be helpful if the authors clarify the assumptions underlying the representation of the semantic space and the required evaluation to back the main arguments of the paper: The assumption is that people's semantic space is patchy (which is backed up by previous work); the authors simulate a semantic space that has this property and show that their sampling method produces the desired levy-flight behavior. * The authors either need to show why the simulated representation is a good proxy of people mental space or use the human data (like the animal free recall) to create the semantic space. A comparison with different types of semantic representations (semantic space, topic model, etc) would be helpful. Does the choice of sampling algorithm interact with the semantic representation? * The authors need to evaluate the method on another dataset (to replicate human behavior); one example is the IRT pattern observed in the free recall data. For example, see Abbott el al (2015); Nematzadeh et al (2016).  Summary and Quality  Some of the patterns observed in human and animal (search) behavior suggest that the mental  (or geographical) space have a patchy structure (which sometimes is a small world). More specifically, the authors argue that the distance between mental samples follows a heavy-tailed power-law distribution (levy flight); moreover, the autocorrelation between these samples follow a 1/f scaling law. They then study a few sampling algorithms (random walk, Metropolis-Hasting, and Metropolis-coupled MCMC). Their results suggest that only the last algorithm produces samples that satisfy both levy flight and autocorrelation properties.  In the results section, the authors start with a simulated patchy environment but compare the results with human data. I do not understand the logic behind this analysis; more specifically, in Figure 2B, the authors compare the power-law exponents of the sampling algorithms on the simulated environment with human data.   The authors acknowledge that the choice of the proposal distribution for Metropolis-Hasting affects the results and the two distribution they have examined does not predict the desired patterns. (Although they again compare the simulated environment with human data.) This observation -- if generalizable to a large number of proposal distributions) is interesting because it shows that having multiple chains (as in Metropolis-coupled MCMC) is important when sampling from patchy environments. The paper needs to better justify this claim.  The paper seems to assume a vector-space representation for semantics (not a semantic network); the authors should explain the consequences of this decision; especially given that the previous work by Abbott et al shows that a random walk on a semantic network is enough to predict human search behavior.  Clarity and significance The paper is mostly clear but the results section needs more details on what the examined space is and what human behavior is predicted. The appendix is not really used as a supplementary material; it is needed for understanding the paper.  Parts of the appendix should be added to the paper such that it is self-contained.  It is not clear to me what range of human behavior the suggested sampling algorithm can predict because the paper mostly focuses on a simulated environment. I am not sure how much this work tells us about mental sampling. 