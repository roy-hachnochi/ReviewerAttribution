This paper presents an end-to-end framework for multitask learning called L2MT. The L2MT is trained on previous multitask problems, and selects the best multitask model for a new multitask problem. Authors also proposed a effective algorithm for selecting the best model on test data.  Pros: - This is an clean and straightforward framework for multitask learning, and it is end-to-end. - The authors presented a nice unified formulation for multitask learning, which served as a good motivation for the L2MT framework.  Cons: - The 'label' in this learning task is the relative test error of eps_{MTL}/eps_{STL}. In my understanding, eps_{STL} is roughly measuring the level of difficulty of a multitask problem, but is there a better baseline to use here? For instance, can we use the average of STL models as a single model, and use its performance on the test set as a baseline? The reason I am asking is that,  eps_{STL} could vary a lot across multitask learning problems, and when eps_{STL} is large, it is not meaningful to predict eps_{MTL}/eps_{STL} accurately, as eps_{MTL} will be large as well. It will be better to have a more robust baseline to replace eps_{STL}.  - It is fine to assume that the tasks under a single multitask problem have the same data dimension.  But the authors also assume the same data dimension across multitask problems, as there is a single LGNN used across problems. I think this does not always hold if you want to train on historic multitask problems of various kinds, e.g. images of different sizes.  - In the experiment, the authors constructed the set of multitask problems by basically bootstrapping from a single dataset,  while I would be more interested if the set of multitask problems are from different datasets, and that will make the experiment results more interesting and realistic.  - The generalization analysis uses multiple Lipschitz assumptions, some of which are not that obvious to me. For instance, could you please explain why in Theorem 4 you can assume \bar F to be Lipschitz constant? The function \bar f seems really complicated as it involves an implicit relation between the task covariance matrix \Omega and the task embedding matrix E. 