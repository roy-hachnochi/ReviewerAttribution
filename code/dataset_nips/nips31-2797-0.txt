This paper proposes a novel approach to more interpretable learning in neural networks. In particular, it addresses the common criticism that the computations performed by neural networks are often hard to intuitively interpret, which can be a problem in applications, e.g. in the medical or financial fields. The authors suggest adding a novel regulariser to the weights of first layer of a neural network to discover and preserve non-additive interactions in the data features up to a chosen order and preserve these relationships without entangling them together (e.g. x1*x2 + x3*x4 would result in hidden neurons representing the interactions {x1, x2} and {x3, x4} but not {x1, x2, x3, x4}). These interactions could then be further processed by the separate columns of the neural network. The approach was evaluated on a number of datasets and seems to perform similarly to the baselines on regression and classification tasks, while being more interpretable and less computationally expensive.  While this work appears promising, I find that a few details are missing that would make me fully convinced about the utility of this approach. I am willing to increase my score if these questions are adequately addressed.  Suggestions for improvement:  -- Section 2.1. It might be worth expanding this section to give the reader a bit more intuition about the problem set up. I found it hard to understand the notation when first reading the paper, since I was lacking the high level picture of what the authors were trying to achieve. The notation became clearer once I finished reading the paper.  -- Line 122. Are there any non-linearities in your network?  -- Line 123. It would be useful to know how the data was generated. In particular, what ranges were the x_{1-4} sampled from? How were the train/validation/test splits done? Currently it is hard to assess whether there was a danger for significant overlap in the splits due to the small ranges/inadequate splits.  -- Figure 1. This is an important motivational figure, yet it is hard to understand due to the lack of detail. For example, % neurons entangled metric is explained in a single line in a footnote. This is something that require a lot more details to understand. Furthermore, the plot is small and the majority of lines are impossible to see.  -- Line 202. What is the architecture for your MLP baseline? How does it compare to the architecture of your approach. What optimiser was used for the experiments?   -- Section 5.1. Why do you only use binary classification datasets? It would be interesting to see how the model performs on the full MNIST dataset.  -- Line 228. "phase stabilises" - what does that mean? "Reinitialised parameters" - would be good to see the results when the parameters are not reinitialised.  -- Line 252. How did you check whether these patterns really exist in the dataset?  Minor details:  -- Line 48. The acronym GAM is used for the first time without explaining what it means. It should be introduced in line 34.  -- Line 62. The authors are encouraged to include a reference to the beta-VAE work (Higgins et al, 2017)  -- GAMs section (lines 65-74). This section might read better before the Interpretability section.  -- Line 83. I think there should be a comma after [p] ("...all input features [p], with [I]>=2..."), as otherwise the definition reads as being circular  -- Line 203. "fully-complexity models" - not sure what that means  -- Line 258. optimisation in non-convex --> optimisation is non-convex    ___________________________________________________  After reading the authors' response, I am happy to increase my score to 6. I hope the authors increase the readability of their paper for the camera ready version as promised in their response.   