This is an interesting paper that proposes a novel method for using a neural network to efficiently learn a GAM model with interactions by applying regularization to disentangle interactions within the network. They show that the resulting model achieves similar performance to an unrestricted neural network model on the chosen datasets.  I would want to see this approach tested on more typical neural network application domains, such as image classification (e.g. CIFAR-10). The only image data they used was MNIST, which is much simpler than most image datasets. The chosen data sets have a relatively small number of features, so it's not clear if the approach would yield similar results on richer data and thus provide a general way to make neural networks more interpretable. Richer data could also have a high optimal value of K, which would further slow down the search process.  It would also be useful to give more detail about the MLP model to show that it is a strong baseline. Does it have the same number of units and layers as the DI model?  Section 4.3 states that "a neural-network based GAM like ours tends to provide a smoother visualization" - it is unclear to me what 'smoothness' means here. If I understood correctly, the visualizations in Figure 4 were produced by plotting variables and interactions that were found by the learned GAM model, and I don't see how the visualizations produced by a tree-based GAM model would be different. It would help to clarify this in the paper.  The paper is generally well-written and clear. As a minor nitpick, in Figure 4, it would be helpful to label the y-axis and legend in the plots as "chance of readmission". The caption refers to [2] for an exact description of the outcome scores - adding outcome labels to the plots would make the figure more self-contained.  UPDATE after reading the author feedback: The rebuttal has addressed my concerns, so I have increased my score. It was particularly good to see the authors were able to provide results for CIFAR-10. I am still a bit concerned that the MLP baselines are smaller than the corresponding DI models, so it might not be an entirely fair comparison. 