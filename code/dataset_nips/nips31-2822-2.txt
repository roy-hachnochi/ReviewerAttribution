The paper considers the problem of high-dimensional image modeling and generation, and presents a new evaluation scheme, and insightful experiments. The paper compares a Gaussian Mixture Model's performance with that of a state-of-the-art Generative Adversarial Network, and shows that the former is arguably doing a better job of modeling the full data distribution, and while the samples generated by the GMM is realistic, they lack the sharp features of samples from GAN. The paper concludes that perhaps more research could result in models which are easy to train and infer and yet capture the finer details of images like GANs.  The paper shows that techniques from the literature, such as factor analysis and the Woodbury matrix inversion lemma, allow training high-dimensional GMMs efficiently.  The paper presents a new simple evaluation scheme which does not rely on pre-trained classifiers, but which however do rely on using L2 distance on image pixels.  The experiments show that the GMM trained does better on the proposed evaluation scheme compared to GANs.  Since the paper relies on the evaluation scheme so much, it would help if the evaluation scheme itself could be evaluated by doing user studies, for instance.  The authors also study a method to transform the blurry images produced by the GMM to richer images using pix2pix. This is not central to the central thesis of the paper. In my view, the GMM model is merely an instrument to study a SOTA model, and as the authors conclude as well, not meant as the ultimate solution to the problem of learning generative models. Therefore, I haven't judged this paper by the pretty samples in the supplementary section.  In summary, the paper has a very significant topic, proposes an interest idea ("How does a model as simple as GMM compare to SOTA GANs?"), evaluates the idea sufficiently exhaustively, and provides good analysis of the results to inform readers' intuitions.  