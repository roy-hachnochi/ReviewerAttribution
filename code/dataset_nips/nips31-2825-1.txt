The paper focuses on the Sinkhorn approximation of Wasserstein distance, which is used to compare probability distributions and finds applications e.g. in image analysis. In particular, the authors derive methods for efficiently computing the “sharp” Sinkhorn approximation, in place of its regularized version, which the authors show to be inferior to the sharp version. The authors provide theoretical analysis of the two approximations, including proof of consistency of supervised learning with with the two Sinkhorn losses. In experiments the authors demonstrate the benefits of the sharp Sinkhorn in providing better classification results.  Update: The author response is convincing, especially the comparison to AD approach. A good job!