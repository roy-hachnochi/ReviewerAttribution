This paper describes a novel algorithm for learning disentangled representations in an autoencoder architecture. The methods relies on the availability of at least 10% of weakly supervised pairs of images (where for each pair the common factor is labelled). The method then uses this information to perform a swap in the latent encoding, where the latents swapped are assigned to represent the common generative factor. This encourages disentanglement and bootstraps the second "dual-stage" part of the approach, which performs double swaps on pairs of images that do not contain any labelled information. The approach appears to perform well on a number of datasets, however the paper requires further quantitative evaluation against the state of the art baselines in order to be fully convincing.   Suggestions for improvement:  -- The authors are encouraged to cite DC-IGN by Kulkarni et al (2015) as another weakly-labelled approach to disentangled representation learning (Lines 48-53 and 239). It would also be useful to run this approach as a baseline.  -- Line 146. The authors mention using m>=1 latent dimensions to represent each of the generative factors. The paper would benefit from a discussion on how m is set. Is m fixed to be the same for all generative factors? How does the performance of the approach change with various values of m? How do the authors select which subset of the latent representation corresponds to each generative factor? In particular, how do the authors make sure that the random swaps in the dual stage do not cross the factor boundaries? Are all latent dimensions allocated to represent certain generative factors or are certain dimensions set to be "free"? What happens to these dimensions in the dual stage?  -- How many supervised examples were seen in the CAS-PEAL-R1 and Mugshot experiments?  -- When training the beta-VAE baseline, the authors used 5 latent dimensions. This is too low for achieving a good performance with a beta-VAE. The authors are encouraged to re-run the experiments with at least 10 latent dimensions for MNIST. We have run similar experiments with a beta-VAE and found that we could get up to 90% accuracy on MNIST classification using the latent space of a pre-trained disentangled beta-VAE.   -- Line 245. I am not sure what the authors meant by "variable (length = 1) that correspond to digit identity"  -- How do the authors separate out the digit identity part of the encoding for the Autoencoder baseline?  -- Figure 3. The plot is not particularly useful, since it is hard to spot the difference between the different versions of the model. I suggest that the authors move the figure to the appendix and replace it with a table of quantitative results comparing the disentanglement score (e.g. from Kim and Mnih, 2018) of DSD and beta-VAE trained on the dSprites dataset (Matthey et al, 2017). The dSprites dataset is a unit test of disentanglement with clearly established metrics. It is also a dataset that comes with the ground truth labels for all the factors of variation, and where beta-VAE struggles to learn the sprite identity information. If the authors are able to demonstrate that their approach achieves better disentanglement scores than beta-VAE on this dataset while also learning about object identity at various levels of supervision, it will be a very powerful and persuasive demonstration of the DSD performance. I am willing to increase my score if this demonstration is provided.  -- Section 4.4. I don't find this section particularly convincing, in particular given the strong wording ("proves") used by the authors. I would recommend running the quantitative analysis of the primary vs dual ablation as suggested above, preferably with error bounds to justify the strong conclusions of this section.    Minor points:  -- Lines 35-37. The authors mention that the unsupervised methods to disentanglement learn representations that are "uninterpretable", where the methods may fail to represent hair colour when learning about faces. The authors are advised to reword that paragraph, since the choice of words and example are mis-representative. For example, the beta-VAE approach (Higgins et al, 2017) learnt to represent hair colour on the CelebA dataset. Furthermore, the approaches would not be published if they learnt "uninterpretable" representations, since this is the whole point of disentangled representation learning.  -- Line 38. contains --> contain  -- Line 66. "which attribute of the two input samples is sharing" --> "which attribute the two input samples are sharing"  -- Line 110. unpair --> unpaired  -- Line 153. encourages --> encourage  -- Line 181. pairs is consists --> pairs consists  -- Line 190. "is able to conduct disentangled encodings" --> "is able to infer (??) disentangled encodings"  -- Line 212. of an teapot --> of a teapot  -- Line 225. hairs --> hair  -- Line 226. are not exceptional --> being not exceptional  -- Table 1 caption. digital --> digit  -- Line 289. disentangled with --> disentangled representations with  -- Line 273. are informativeness --> are more informative    ------------------------- After reading the authors' response, I have increased my mark to 7. I believe that the paper warrants an acceptance.  