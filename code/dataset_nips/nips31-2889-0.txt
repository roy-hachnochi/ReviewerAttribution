Update after rebuttal:  I continue to think this paper presents a significant result, which may spawn a variety of future follow-ups. Deep linear networks with the L2 loss are undoubtedly a simple model class, but they are nonconvex and exhibit several of the pathologies in nonlinear networks. It is great to have a clear, exact analysis of a way of solving this issue. More generally, I think the results in this paper are an essential prerequisite for tackling more complex versions of the problem. If we don't understand how NG works in this simple linear case, why should we expect to jump straight to a more complex situation? Pedagogically I find this paper very useful in understanding how NG is improving the situation (at least from an optimization perspective).  The paper should explicitly discuss and expand on the connection between the proposed algorithm and K-FAC. The algorithm proposed in this paper appears to be identical, suggesting that K-FAC is exactly correct for the deep linear case. K-FAC has been applied in deep nonlinear settings, at large scale, and this may provide an additional argument for its usefulness.  ________________  Summary:  This paper derives an exact, efficient expression for the natural gradient for the specific case of deep linear networks. The main result is that the natural gradient completely removes pathological curvature introduced by depth, yielding exponential convergence in the total weights (as though it were a shallow network). The paper traces connections to a variety of previous methods to approximate the Fisher information matrix, and shows a preliminary application of the method to nonlinear networks (for which it is no longer exact), where it appears to speed up convergence.  Major comments:  This paper presents an elegant analysis of learning dynamics under the natural gradient. Even though the results are obtained for deep linear networks, they are decisive for this case and suggest strongly that future work in this direction could bring principled benefits for the nonlinear case (as shown at small scale in the nonlinear auto encoder experiment).  The analysis provides solid intuitions for prior work on approximating second order methods, including an interesting observation on the structure of the Hessian: it is far from block diagonal, a common assumption in prior work. Yet off diagonal blocks are repeats of diagonal blocks, yielding similar results.  Regarding the exponential convergence result, it seems like this cannot be fully general. If the initial weights are all zero such that the system begins at a degenerate saddle point, it is hard to see how rescaling the updates can correct this. The projection matrices in Eqn 19 are said to depend only on the network architecture, but it seems they might also depend on the parameter initialization: networks initialized with zero weights might effectively be dropping rank, which cannot be recovered. It seems like the statement should be something like, provided one does not initialize at a saddle point, convergence is exponential (and random initialization will start at a saddle point with probability zero).  