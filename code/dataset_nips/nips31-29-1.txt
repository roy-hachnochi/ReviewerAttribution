In this paper the authors consider the problem of minimizing a continuous submodular function subject to ordering (isotonic) constraints. They first show that the problem can be solved if we first discretize it (per coordinate, not in [0,1]^n), and then solve the resulting discrete optimization problem using convex optimization. The fact that the problem is solvable in polynomial time is of course not surprising, because, as pointed out by the authors in lines 29-36, we can add a penalty to the objective that will implicitly enforce the constraints. However, this can significantly increase the Lipschitz constant of the objective, and that is why the authors take on an alternative approach. First, they prove that seen in the space of measures, the isotonic constraints correspond to dominating inequalities of the CDFs, which I guess is an intuitive result given the results known for the unconstrained case. For the discrete problems this adds another set of inequality constraints that have to be satisfied. Projection onto these constraints can be done using parametric maxflow among other techniques, so that the authors are able to achieve rates for this problem similar to those for the unconstrained one (sections 4.2 and 4.3). How this is exactly done is not clear, and I would suggest the authors to perhaps show how they reduce their problem to say parametric maxflow in 4.4, or at least in the supplementary.  The authors later go on to discuss improved discretization algorithms. I would like to point out that all these schemes are uniform, in the sense that the points are equally spaced. What the authors analyze is the number of points that you need under different smoothness assumptions. Under uniform bounds on the gradients, they construct a surrogate for the function that is submodular and whose minimization results in faster rates. However, it is hard to evaluate as it requires the minimization of polynomials over box constraints - hence, I think of this algorithm to be of a more theoretical nature. Furthermore, the given guarantees, if I'm not mistaken, assume an exact evaluation of the surrogate. It is not clear if we instead solve approximate methods (e.g. SDP relaxations).  Finally, I would like to remark that I did not read the proofs of the numerous convergence rates provided in the paper.  Questions / Comments ---- 1. You project onto the constraints using parametric maxflow by solving a graph-cut problem with very large weights corresponding to E? How do you incorporate the box [0, 1] constraints? 2. Don't orthogonal projections and separable problems (section 4.4) reduce to the same problem? Can't you use one parametric flow also for the separable problems? If yes, I would suggest to present these together. 3. What is the purpose of section 5.2? I can not see how it is related to the discretization strategies. 4. l286 - Why only chain constraints? It is not clear from the Section 4.2, as there you add one constraint for each edge in E. 5. Is \Delta H in Fig.3. the difference of the computed optimum between two consecutive discretizations?  Post-rebuttal: The rebuttal addressed all of my questions and comments. However, the more fundamental issues with the method that I have outlined (surrogate hard to evaluate, no guarantee under approximate surrogate evaluation) seem to hold, and that is why I will keep my score.