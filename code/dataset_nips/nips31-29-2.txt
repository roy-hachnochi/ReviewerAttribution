This paper studies continuous submodular minimization subject to ordering constraints. The main motivation is isotonic regression with a separable, nonconvex loss function. The starting point is the relationship between submodular minimization and convex minimization over a space of measures. It is demonstrated that ordering constraints correspond to stochastic dominance, and algorithms are proposed to enforce these constraints during optimization. The starting point is a simple way of discretizing the CDF corresponding to each variable. Then, improved discretization schemes are presented, depending on the availability of higher-order derivatives. Experimental results show that using a nonconvex loss function can improve robustness to corrupted data in isotonic regression.   This paper is well-written, and makes a good set of technical contributions in showing how to incorporate ordering constraints and improve the discretization schemes. The experimental results nicely show the conditions under which the new techniques improve over standard methods.   A couple questions:  1) Is there some intuition for why nonconvex losses like the proposed logarithmic function are more robust to outliers than squared or absolute value loss?  2) It makes sense that higher-order derivatives would improve the convergence rate in terms of number of iterations. However, this requires additional computation to get the Hessian (or even higher-order terms). Is the tradeoff worth it in terms of actual runtime?