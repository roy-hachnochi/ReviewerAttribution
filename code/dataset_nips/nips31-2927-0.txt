The manuscript proposes a principled Bayesian technique for simultaneously optimizing a blackbox function and learning which is the most likely function class that explains the observations from the blackbox. The method supports a large class of functions by exploring the space of kernels via composition of two base kernels. It seems like the technique trivially extends to more base kernels and composition operations---or perhaps eventually prior mean functions. The experimental results show a significant benefit to intelligently searching the space of models (in this case kernels) when compared to fixed mixtures or models and random walks in model space.  The paper is very clearly written and goes into an appropriate level of detail for a Bayesian optimization audience. If I had but one nitpick in this regard, it would be that the ABOMS algorithm would benefit from a pseudocode description; indeed, I am still unsure of the difference between ABO and ABOMS.  The algorithm is principled and technically sound yet relatively simple to implement, which should make it accessible and impactful, especially with promised released code.  The results show the performance and robustness of the algorithm when compared to random search, a fixed model, a fixed mixture of models, and a random walk through models; though the latter seems to work pretty well as well.