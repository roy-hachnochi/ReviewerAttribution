The authors propose a novel approximation for multi-class Gaussian process classification. The proposed approximation allows GP classification problems to be solved as GP regression problems and thus yielding a significant speed up. The core idea is to represent the likelihood as a categorical distribution with a Dirichlet prior on the parameters and then use the fact that a Dirichlet distribution can be represented using a set of independent gamma distributions. These independent gamma distributions are then approximated using log-normal distributions via moment matching. The result is a model with heteroscedastic Gaussian likelihood. The downside is that the approximation introduces a new parameter that cannot be tuned using marginal likelihood. Besides the model approximation introduced in the paper, the authors also rely on standard approximate inference methods to make their method scale, i.e. variational inference and inducing points.  The proposed method is evaluated and compared to benchmark methods using 9 different datasets. The experiments shows that the performance of the proposed method is comparable to the reference methods in terms of classification error,  mean-negative log likelihood and expected calibration error.     The paper appears technically sound and correct. The author do not provide any theoretical analysis for the proposed method, but the claims for the method are supported by experimental results for 9 datasets showing that the method is comparable to the reference methods.   My only concern is the parameter alpha_eps introduced in the model approximation. In figure 3, the authors show that the performance is quite sensitive to the value of alpha_eps. However, they also show that optimal value for alpha_eps (in terms of MNLL) is strongly correlated between the training and test set. However, they only show this for 4 out of the 9 data sets included in the paper. It would be more convincing if the authors would include the corresponding figures for the remaining 5 datasets in the supplementary material along with the reliability diagrams.  The paper is in general clear, well-organized, and well-written. However, there are a couple details that could be made more clear. If I understand the method correctly, then solving a classification problem with C classes essentially boils down to solving C independent GP regression problems each with different likelihoods, where each regression problem is basically solving a one-vs-rest classification problem. After solving the C regression problems, the posterior class probabilities can be computed by sampling from C GP posteriors using eq. (7).   1) If this is correct, the authors should mention explicitly that each regression problem is solved independently and discuss the implications.    2) How are the hyperparameters handled? Is each regression problem allowed to have different hyperparameters or do they share the same kernel?  3) In my opinion, the authors should also state the full probabilistic model before and after the approximation. That would also help clarifying point 1).  The idea of solving classification problems as regression problem is not new. However, to the best of my knowledge, the idea of representing the multi-class classification likelihood in terms of independent gamma distributions and then approximating each gamma distribution using a lognormal distribution is indeed original and novel.  The authors propose a method for making GP classification faster. I believe that these results are important and that both researchers and practitioners will have an interest in using and improving this method.   Further comments Line 89: “The observable (non-Gaussian) prior is obtained by transforming”. What is meant by an observable prior?   Line 258: “Figure 5 summarizes the speed-up achieved by the used of GPD as opposed to the variational GP classification approach“. 1) Typo,  2) For completeness, the authors should add a comment on where the actual speed up comes from. I assume it’s due to the availability of closed-form results for the optimal variational distribution in (Titsias, 2009) which are not available for non-Gaussian likelihoods?  Line 266: “In Figure 5, we explore the effect of increasing the number of inducing points for three datasets” Can you please elaborate on the details?  --------------------------------------------------------------------- After author feedback --------------------------------------------------------------------- I've read the rebuttal and the authors addressed all my concerns properly. Therefore, I have updated my overall score from 6 to 7.