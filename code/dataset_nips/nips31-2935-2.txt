This paper is concerned with multi-class classification using GPs, placing particular emphasis on producing well calibrated probability estimates. Inference in multi-class classification for GPs is in general a hard task, involving multiple intractabilities (matrix factorisations).  One of the attractions of using GPs for classification is that they aught to be well calibrated, which is not always the case for non-probabilistic classifiers. The paper here proceeds in two directions: how to make an efficient multi-class GP (with comparisons to existing methods); and analysis of the calibration properties of such methods.  GP regression is a much simpler task than GP classification (due to the Gaussian likelihood). This immediately suggest that perhaps by treating the (binary) class labels as regression targets, and ignoring the fact that they are discrete variables (this can be extended to multiple classes through a 1vs rest approach). Predictions from such a model may lie outside of the [0, 1] range, so one can either clip the outputs or apply post-calibration. This forms a baseline in the paper. Another idea is to use separate regression models for each class, and then combine these through the use of a categorical/Dirichlet model. I note that this idea is not new - it has been used extensively in the context of Bayesian linear models, and I found several references to this kind of model for GPs [1-4]. The idea of constructing these  as separate Gamma likelihoods is novel as far as I am aware, and the neat thing here is that the Gamma likelihood (which would be intractable) can be quite well approximated by the lognormal distribution, allowing inference on log scaled versions of the targets. Note here that the model is heteroskedastic, although there are only two possible values for the noise per instance ($\alpha_i$), which seems somewhat limiting.   The analysis of the behaviour of $\alpha$ (section 4.3), shows that it it’s well behaved in testing scenarios. I would conjecture here that the between class variance must be similar in train and test for this to be the case.   The experiments are appropriate, and Figure 4 shows that the method is competitive (although not obviously superior) to the competing methods. When combined with the speedup, the comparison with GPC is certainly favourable. It is perhaps harder to argue that it dominates GPR (Platt) and NKRR, although on balance the uplift in performance is probably enough to accept the additional complexity in certain situations.   In sum, on the positive side this paper is well written, and contains a (fairly) simple idea that is well executed. The negative side is that there is limited depth to the analysis.  Specific comments: 51 “It is established that binary classifiers are calibrated when they employ the logistic loss”. This is under the (strong) assumption that the true conditional distributions of the scores given the class label are Gaussian with equal variance. This of course is often violated in practice. 126 “The Bayes’ rule minimising the expected least squares is the regression function … which in binary classification is proportional to the conditional probability of the two classes”. I guess you mean “The Bayes optimal decision function”? And do you mean the ratio of the conditional probabilities? As in the point above, this being calibrated relies on a conditionally Gaussian assumption 132 breifly -> briefly Figure 1 - are these in fact two-class problems? Table 1 - how are the numbers of inducing points chosen? There’s no obvious pattern Appendix Figure 1 - HTRU2 -  GRP (Platt) - I believe this is a case where Platt scaling cannot help  [1] Cho, Wanhyun, et al. "Multinomial Dirichlet Gaussian Process Model for Classification of Multidimensional Data." World Academy of Science, Engineering and Technology, International Journal of Computer, Electrical, Automation, Control and Information Engineering9.12 (2015): 2453-2457.  [2] Chan, Antoni B. "Multivariate generalized Gaussian process models." arXiv preprint arXiv:1311.0360 (2013).  [3] Cho, Wanhyun, et al. "Variational Bayesian Inference for Multinomial Dirichlet Gaussian Process Classification Model." Advances in Computer Science and Ubiquitous Computing. Springer, Singapore, 2015. 835-  [4] Salimbeni & Deisenroth. Gaussian process multiclass classification with Dirichlet priors for imbalanced data. Workshop on Practical Bayesian Nonparametrics, NIPS 2016.  Note: Following author feedback my concerns about related work have been addressed. I feel that this is a solid if not remarkable paper.