Added after author response:  I'm now happier with the simulations after  reading the author response.  I'm still concerned that the actual proof is 30 pages, and hence not in the paper, and not checkable in the time frame of a NIPS review.  ----  This paper (2949) proves results (in supplementary information, with summaries in the paper) on how the computation time of Hamiltonian Monte Carlo sampling grows with increasing dimensionality, d.  There have long been heuristic arguments that the number of gradient evaluations required to sample a nearly independent point should usually scale as d^1/4. (So assuming time growing as d per gradient evaluation, total run time scales as d^5/4.) This paper contributes to putting this on a firmer foundation, for a wider class of distributions, including some of practical interest.  The main focus of the paper is the unadjusted HMC algorithm, in which the accept/reject test is omitted. When the dynamics is simulated with the leapfrog method, the distribution sampled from when there is no accept/reject test is not the one desired, but becomes closer to the desired distribution as the leapfrog stepsize is reduced. The point of the results in this paper is to investigate how fast, in order to maintain some desired accuracy bound. the stepsize needs to be made smaller (with correspondingly longer computation time) as d is made bigger.  Some more discussion of how it might (or might not) be possible to connect this result to a similar result for standard HMC with an accept/reject test would be interesting.  This is presently only briefly hinted at at the end of the paper.  Some simulations are presented of HMC and other methods for Bayesian logistic regression.  These could be improved, as discussed below.  Overall, this is a useful contribution, which however does not fit entirely comfortably within NIPS. The supplementary information with the actual proof is 30 pages long. A final NIPS paper would presumably be able to reference this as archived somewhere else. It is of interest to have a summary paper to expose progress in this area to a wider audience.  Remarks on Section 6 (Simulations):    Some aspects of the simulations are not adequately described.  What was the value of r?  What values for the regression coefficients were used in generating the data?  (Or were many values drawn from the prior used, with the results shown being averages?)  What were start states for the chains?  With a standard normal prior on 1000 coefficients, and standard normal inputs, the input to the logistic function is very likely to be far from zero, so the probability of the response being 1 will be very close to 0 or 1. This is not typical of most real logistic regression problems.  It is odd to say things like (lines #335-336), "the autocorrelation time of UHMC was fastest over the interval [0.15,0.5], while ULA was fastest at eta=0.55", as if a value of eta was somehow externally imposed, rather than being freely choosable.  Detailed comments:  #25:  I think i times eta^2 should be i^2 times eta.  #44, #72-74: HMC (with leapfrog integration) applied to Gaussian distributions was also analysed pretty completely in the following paper:      Kennedy, A. D. and Pendleton, B. (1991) "Acceptances and       autocorrelations in hybrid Mone Carlo", Nuclear       Physics B (Proc. Suppl.), vol 20, pp. 118-121.  Note that a leapfrog step with a Gaussian target distribution is a linear map, making the analysis not too difficult.  #99:  "[34,2]" should probably be "[35,2]".  Reference [41] is a garbled combination of two unrelated references.  