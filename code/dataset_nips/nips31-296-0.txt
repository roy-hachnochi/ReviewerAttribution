This paper followed the work of nonlocal neural network to discuss the properties of the diffusion and damping effect by analyzing the spectrum. The trained nonlocal network for image classification on CIFSR-10 data by incorporating nonlocal blocks into the 20-layer PreResNet presented most eigenvalues to be negative and convergence challenges when more blocks were added under certain learning rate and epochs. A rough look at the nonlocal operator representation under steady-state shed light that the output signals of the original nonlocal blocks tend to be damped out (diffused) along iterations by design.  A new nonlocal network with namely nonlocal stage component was proposed to help overcome the aforementioned damped out problem by essentially replacing the residual part from weighted sum of the neighboring features to the difference between the neighboring signals and computed signals. Another proposed change is replacing the pairwise affinity function based on updated output to the input feature, which stays the same along the propagation with a stage. The new proposed nonlocal network outperforms the original nonlocal network in the previous mentioned data example in the sense of providing smaller validation errors under various block cases (in the same/different residual blocks). We also see about 2/3 of eigenvalues are positive. Meanwhile, we see the validation error rate first decrease and then increase again when more nonlocal blocks are added. A light touch on the nonlocal diffusion process representation and discrete-time Markov chain with nonlocal jump representation help understand the stability benefit of the proposed nonlocal network.          The structure and idea of the paper is clear, would be a plus if the connection to nonlocal modeling part be more rigorously written and a little more introduction/comparison with closely related/cited method.  A few minor detailed comments are  1) Full name of the method when first appear like ResNet in Line 38 and ReLU in Line 61.  2) Might help reader to better follow if briefly point out the relation of Z^{k} in (2) and Z_{i} in (1) when first appear,  without having to digest based on later (3).  3) Define weight matrix W_{z} in (1), clarify the h of \mathcal{L}^{h} in (14) is discretization parameter, make it clear in (16) z_{t} is \partial{z}/\partial{t}.        