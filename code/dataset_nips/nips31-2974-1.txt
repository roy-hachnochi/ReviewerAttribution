= Summary  The paper presents a two-stage method to infer programs that generate input drawings. Initially, a CNN architecture is applied to translate the drawing into a sequence of primitive draw commands (lines, rectangles, circles, etc). Then, a program synthesizer is used to collapse the straight-line program into a proper program with conditionals and loops.  First, let me say that I absolutely love the results of this paper. It is a wonderful combination of relatively simple tools to achieve something unexpected. However, the paper itself has a bunch of problems, mainly because it has to cover a lot of content in little space. Maybe 8 pages are just not the right format for it and it should go straight to a journal, or maybe splitting the two contributions into two papers would make sense. I should say that I have seen this paper under submission before (at ICLR'18). I think the paper can be accepted as it is, but its long version (including much of the content in the supplementary material in the main text) would always be easier and more interesting to read.  = Quality  The results are great and the experiments supporting the evaluation are thoughtfully selected, answering most of my questions. Most of the remaining questions have to do with baselines that appeared after the initial version of this paper appeared. For example, in Sect. 2, a comparison to the very recent SPIRAL [A] is an obvious question. However, as [A] appeared on arxiv in April, this can hardly be expected for a NIPS submission, but it would be an interesting point for a future (final?) version of this paper. In Sect. 3, a comparison to more recent work building on top of DeepCoder (e.g. [B]) would have been interesting, as these approaches use the partial program generated so far to drive the search for the remaining program; something that seems especially helpful in the graphical domain.  = Clarity  This is by far the weakest point of the paper; many finer points only become clear with the supplement in hand. I understand that this is due to the page limit, but it makes the 8 page submission hard to read. One part that I found unclear (even with supplement) was the precise search method in Sect. 3, i.e., how is the search policy integrated with the backend search?  = Originality  To the best of my knowledge, both the graphics-to-statements method as well as the biased statements-to-program method are new contributions that pull together ideas from a wide range of different fields.  = Significance  Can I please have this thing to generate tikz from my awful drawings? The presented task is very cute and could be useful to translate pen inputs to vector graphics or LaTeX. The combination of perceptual inputs and program synthesis shows the way for future integration of the two fields, for example for learning repetitive tasks from demonstration. The extrapolation idea in Sect. 4.2 seems nice, but is probably just a toy. The error correct idea from Sect. 4.1 feels underexplored, and a tighter integration of program synthesis method with the graphical component could further improve on this.   References:  [A] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami      and Oriol Vinyals. Synthesizing Programs for Images using      Reinforced Adversarial Learning.  [B] Ashwin Kalyan, Abhishek Mohta, Alex Polozov, Dhruv Batra, Prateek      Jain and Sumit Gulwani. Neural-Guided Deductive Search for      Real-Time Program Synthesis from Examples.