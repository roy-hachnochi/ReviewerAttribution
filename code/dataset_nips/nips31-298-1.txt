=== added after response === I appreciate the authors' honesty in acknowledging some of the weaknesses of the obtained results. I also tend to agree with the authors that the obtained results, relatively speaking, are significant and do shed new insights in understanding ResNet. As such I voted for acceptance (without strong opinion) although the outcome could be either... === end ===  The main goal of this work is to understand the effect of skip-connections in ResNet, through the lens of optimization. Although ResNet is strictly more powerful than simple linear regression (in the sense that linear regression is a special case of ResNet, if the weights follow a trivial pattern), its optimization may be more challenging than the linear regression special case. The authors formally ruled out this possibility by proving that any local minima of a particular ResNet architecture, or more generally any approximate stationary point, has objective value no larger than that of linear regression. However, finding such a local minima, as the authors showed through a simple example, may still be challenging. Instead, the authors turned to online learning and give a straightforward regret-type guarantee for using SGD to optimize a slightly tweaked ResNet architecture.   The paper is very well-written and the technical results, as far as I can tell, are solid. The authors did a terrific job in motivating their problem, which is undoubtedly intricating and significant. The idea to restrict the analysis to stationary points that are sufficiently good (e.g., with objective above a certain value) is not new but nevertheless insightful. However, the analysis also largely overlooked the actual inside structure of the network. Essentially, the authors treated ResNet as a bi-affine function and compared to a linear section. The results, while of some theoretical interest, perhaps are not very relevant in practice.   My biggest concern for this work is the following. Take Section 5 for example. First of all, it is misleading to say SGD "converges" for the tweaked ResNet. Secondly, in practice, we can always solve the linear regression problem and then initialize the ResNet weights correspondingly. Then we trivially obtain the regret bound in Theorem 4. (If you do not like the idea of solving linear regression offline first, then run a concurrent online learning algorithm, like SGD, that solves linear regression along with a different algorithm that solves ResNet.) How does this give any insight about the superior empirical performance of ResNet? My point is, Theorem 4, itself really does not tell us much, other than linear regression is a special ResNet.   The results in Section 3 are more interesting but again my feeling about it is mixed: on one hand it is certainly encouraging to be able to say something concrete about the loss surface. The conclusion that any local minima is at least as good as the best linear predictor is certainly very nice. But on the other hand, from a practical perspective, to achieve an objective value that is as good as the best linear predictor (for any architecture) is not really difficult. Not to mention that we do not know how to actually get to any of those local minima (in polytime).  More technical comments: Theorem 3: are all the Lipschitz constants independent of theta? this is actually where the inside network structure would have an impact and it is perhaps better to put more effort into analyzing it.  Corollary 1: why do you need the Lipschitz continuity assumptions here?   Does the actual form of the objective function F in Eq (2) matter? If not can we translate the objective so that the linear predictor is a local minima and conclude that all local minima are equally good? (probably not, but why?)  Proof of Lemma 2 and Thm 2 can be sharpened: it is easy to see that the minimum eigenvalue of the Hessian after line 416 (appendix) is  .5 * [ lambda_{min}(B) - sqrt{ lambda_{min}^2(B) + 4|r|^2 } ],  where B is the top-left submatrix (and is positive semidefinite). 