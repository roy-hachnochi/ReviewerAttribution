The paper introduce a idea to combine graphical models under GAN framework. The proposed idea is to match distributions locally, which is similar to conditional distribution matching, instead of learning whole joint distribution together. The generally idea is interesting to NIPS, but the evaluation is relatively weak, which may need more justification to prove the idea of the proposed algorithm.   1. In line 234, it says "GMGAN-L has less meaningless samples compared with GAN-G". This comparison is very subjective. For ambiguity I can see in Fig 2(b) is some 8 similar to 1. However, MNIST does have these distorted digits in the data. On the other hands, it seems 2(c) has more confusion on 4 and 9. The arguments above can also be challenged. The point is, it is not convincing to me to judge the performance based on Fig 2, given they also fail to correctly cluster 10 digits. For quantitative results on MNIST (table), the gap is not significant by taking std into account, so maybe MNIST is not a good example to convince people GMGAN-L is superior than GMGAN-G.   2. In "Infogan", it also learns latent code for digits. The results they reported in the paper seems correctly cluster 10 digits. I don't see any discussion about that in the draft, could you comment on that?  3. For clustering results, what's the clustering ACC on cifar10? Many columns reported in Fig4(b) and 4(c) are based on color tones, which can also be achieved by clustering based on pixel values. Then a fundamental question is, does the proposed algorithm really learn interesting clustering results? A simple two-stage baseline is we do clustering first, then train GANs.   4. For SSGAN, the baselines seem to be too weak. For example, what's the architecture of 3DCNN (I apologize if you mentioned somewhere but I missed it). If the model capacity is enough, MNIST should not be a difficult task to it. Also, instead of those proposed baselines, there are many state space models like [13], I would like to see the comparison with those. At least I expect to see the competitive performance with those VAE based models.   5. Minor comments: the figures are all too small in the paper. It is hard to check if you print it out.   ==== The rebuttal addressed my concern in mixture models, so I raised my score from 4 to 5. However, there is still concern about the bad performance of 3DCNN for fair comparison. The reason mentioned in the paper is [44] use a two-stream architecture. However, in practice, although two streaming architecture brings improvement, using one stream won't results in "significantly" worse performance. For example, in [44]'s table 1. Based on human evaluation, one stream network is evaluated to be better than two-stream case in 47% testing cases. Also, the follow up work of [44] ("Generating the Future with Adversarial Transformers") is using one stream architecture. Therefore, I strongly encourage the author to investigate this issue and address this issue in the revision. 