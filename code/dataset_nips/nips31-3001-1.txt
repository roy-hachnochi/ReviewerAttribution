  This paper studies whether a pre-trained machine comprehension model can transfer to sequence-to-sequence models for machine translation and abstractive text summarization.    The knowledge transfer is achieved by 1) augmenting the encoder with additional word representations which re-used pre-trained RNN weights from machine comprehension; 2) adding the RNN weights from the MC’s modeling layer to the decoder.  Overall, I think the presented ideas could be somewhat interesting, but the current results are not convincing enough. The authors failed to give a clear explanation on why this knowledge transfer should work and what is the main intuition behind this.  According to Table 2, sharing the encoding layer gives ~1 point gain, while sharing the modeling layer really helps very little. The focal loss seems to help 0.7 ~ 0.9 points; however, this gain shouldn’t be merged with the other two components as the main theme of this paper is to do knowledge transferring from machine comprehension. Therefore, the improvement from re-using the pre-trained weights is indeed quite limited. Moreover, it is unclear why sharing the encoding layer can actually help -- especially it encodes a full paragraph for SQuAD while it only encodes a single sentence for WMT. Would a pre-trained language model help in the same way (see Peters et al, 2018, Deep contextualized word representations)?  For the summarization result (Table 4), the absolute improvement is also small. Can you give a similar ablation analysis to show that how much of the gains come from sharing the encoding layer, the modeling layer and the focal loss?  Some other comments:  - The citation format needs to be fixed, for example, line 15, there is no space before [Seo et al, 2017].  - Cui 2017a and Cui 2017b are the same paper.  - 21: I think SQuAD cannot be called the “one of the toughest machine comprehension tests”. It is better to call it “the most commonly used MC test” or “one of the most important MC tests so far”.  - 109: in the follows -> as follows  - 199-200: the presented results on SQuAD is already quite lower than the state-of-the-art (see the SQuAD leaderboard).  - In introduction, there a few places which cite the literature of MC work but the citations are sort of arbitrary:  1) Line 15:  [Seo et al., 2017; Shen et al., 2017a]  2) Line 19: end-to-end neural networks [Wang et al., 2017; Xiong et al., 2017a; Cui et al., 2017a]  3) Line 34: many methods[Seo et al., 2017; Wang et al., 2016; Xiong et al., 2018].  - Additionally, the paper describes the proposed approach as a “machine comprehension augmented encoder-decoder supplementary architecture”. Not sure what “supplementary architecture” means here but it is more like doing transfer learning from an MC task to a language generation (seq2seq) task.  ========= I have carefully read all other reviews and the authors' rebuttal. My score remains the same. 