The paper proposes a transfer learning approach of re-using both the encoder and the modeling layer from a pre-trained machine comprehension model in sequence-to-sequence models.  The augmented sequence-to-sequence model shows nice gains on both NMT and summarization.  The experiments also show some ablations indicating that both the encoder and the modeling layer provide some gains.  The experiments also show that just adding randomly initiatized (not pretrained) parameters does not improve, so the improvements are not just an artifact of increased model capacity.  This work is part of a growing sequence of works showing the effectiveness of pretraining on one task and transferring to another (McCann et al, 2017; Peters et al, 2018).  Prior work has looked at transferring MT and/or language modeling.  This work differs somewhat both in a) what is transferred and b) the source task.  I have not seen machine comprehension as a source task before.  Strengths: Shows the efficacy of transferring knowledge from machine comprehension to two difference tasks.   Includes ablation experiments showing the effect of subcomponents and random initialization vs. pretraining. Figure 1 is very helpful for understanding the proposed model.  Weaknesses: There appear to be 3 components of "MacNet": 1. The Encoding Layer 2. The Modeling Layer, 3. Using the focal loss (Lin et al, 2017).  My impression is that +/- the focal loss is the difference between the "Baseline+Encoding Layer + Modeling Layer" row and the "Baseline + MacNet" row.  This row actually has the larger differences compared with adding either of the pretrained components, which is somewhat disappointing.  Are ~50% of the gains due to adjusting the objective?  How well does this do on its own (without the pretraining parts)?  It would be nice to clarify the contributions of both: a) using this task versus previously proposed tasks for pretraining and b) using this approach vs. other pretraining transfer learning approaches.  Given that both changed simultaneously, it's not clear how much to credit the task vs. model.  It could be that the task is so good that other existing transfer learning approaches are even better, or that the method is so good it would help with other source tasks even more, or both or neither.  Thanks for the response.  It would be good to include the comparisons to Cove and the summarization ablations in a final or subsequent version.