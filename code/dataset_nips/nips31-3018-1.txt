The authors propose a new method to enforce independence in the approximate posterior learned via auto-encoding variational Bayes. Specifically, this is achieved by maximizing a new lower bound that is obtained as the sum of the standard ELBO and a penalization term. The penalization term enforces independence through a kernel-based measure of independence, the Hilbert-Schmidt Information Criterion (HSIC). The authors showcase the advantages of their method over state-of-the-art alternatives in three applications: (i) learning an interpretable latent representation in a case study for which exact posterior inference is tractable; (ii) learning a representation of a face dataset (the extended Yale B dataset) that is invariant to lighting; (iii) denoising of single-cell RNA sequencing data.  The paper is well written, the presented experiments are convincing, and the method has the potential of being used to tackle problems in computational biology and other fields. I have only (very) minor comments:  1) the authors could motivate their work a bit more in the introduction, maybe by extending on the concept introduced in section 3: “Independence between certain components of the representation aids in interpretability.”;  2) the authors could add some sentence to give the intuition of the background/methods before proceeding to a formal description;  While this is not crucial, I think it would make the paper accessible to a larger audience, which could be important to encourage the use of the method.  -------------------------  The authors have addressed my (very) minor comments in their response. 