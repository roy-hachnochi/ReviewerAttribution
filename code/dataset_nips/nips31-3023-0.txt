Paper considers fairness in the context of machine learning systems that can refuse to make a prediction; and pass difficult instances to humans for them to label instead. Fairness in this new context is an interesting problem that requires understanding the biases and limitations of the human working with the classifier.   The authors make a good case for their new approach being a useful generalisation of the existing learning to reject methods; that considers the accuracy and bias of the human response on particular instances. I like this idea, and I think it may become an important part of HCI in the future.   The derivations are straightforward and flow appropriately from the definitions; and clearly show learning to reject as a special case of this approach.  Experiments are understandably synthetic, using ML algorithms with additional information as a replacement for humans , but clearly make the case for the approach.  In conclusion, this ticks all the boxes for a nips paper for me. Good, relevant idea, clear to follow, and sufficient synthetic experimental support.  Using actual people in the experiments would make it even better, but this would introduce a whole host of new challenges and by no means is it necessary to show the merits of the approach.    Minor errors: Typo cam-> can in abstract and wrong citation style used in paper. 