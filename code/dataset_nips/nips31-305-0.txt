The paper proposes a predictive model for video that decomposes the video into multiple components and probabilistically models the temporal dynamics of each component individually. Variational autoencoder is used to infer the approximate posterior of the latents. Temporal dynamics is modeled at the level of the latents (ie p(future frames latents | past frames latents)). Experimental results are nice, particularly on the bouncing balls data, showing that the model can capture the interactions between the components.   There are some aspects that are unclear to me: (i) how is the final frame generated (sec 3.2)? by adding all components x^i_{1:k} for i=1,..n? (ii) what encourages the model to tease apart different components? How are z^i and x^i encouraged to be different for various 'i'?  (iii) Fig 2: In the two dimensional RNN, one dimension runs over the components (vertical direction) which seems to impose arbitrary ordering over the components (ie 'i-1' comes before 'i'). Does it affect which components are captured in i < j? (iv) Line 182: what is the form of dynamical model 'f'?  Minor: Line 218: "automatically" appearing twice  ===============  I have looked at the author response and am willing to maintain my original score. 