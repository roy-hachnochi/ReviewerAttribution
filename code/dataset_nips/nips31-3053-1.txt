This paper proposes a minimum hyperspherical energy objective as a regularizer for removing the redundancy among the neurons for image classification. The idea is motivated by the Thomson problem in physics. Extensive experiments validate its effectiveness for different types of neural networks.  However, I still have some concerns: i) The idea of drop out actually makes the idea robust to occlusion, and and it may help learn similar neurons for classification. In your experiments, if we block the image with some black blocks of different sizes, say10%\times, 30%\times, 50%\times, even70%\times of image width, will the performance of the proposed method still be good? In other words, is the proposed solution robust to the occlusion? ii) I would like to see the performance of the proposed method to other network architecture, like densenet, and GoogleNet.. iii) Please evaluate the proposed method on ImageNet. ImageNet is a more real and challenging dataset, it contains lots of occlusion. If the proposed method cannot be shown its effectiveness on ImageNet, I would like to doubt the effectivness for real data. Further, the improvement of the proposed solution is marginal on many datasets reported in the paper.