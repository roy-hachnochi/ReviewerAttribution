The paper tackles an important problem of learning a policy from historical data. In its setting, the data may be generated by an unknown policy, and have complex action space, thus making existing methods hard to apply. The authors propose a novel method which successfully solves these problems. The effectiveness is validated with theoretical justifications and experimental results.  In a nutshell, the authors provide an elegant solution for the problem considered, where previous methods are likely to fail. Related works are discussed in Section 3, with the difference from their work, as well as the reasons the existing methods cannot be directly applied in their problem. From the analysis and the experiments, I think the proposed method is simple and welly suited for their problems of deep learning in complex games.    Clarity: The writing is mostly clear and rigorous. Most notations and symbols used in this paper are defined in Section 2. Personally, I think formula (7) - (10) in Section 6 can be written more strictly without omitting s and a, although I get what the authors mean.  Significance: I think the authors tackled an interesting and important problem in practice. I like the solution the authors proposed, which is both simple and effective. From the results in their experiments, the improvement over baseline methods is significant. In my opinion the work is likely to inspire a few future works.   Questions: In Figure 1, I understand that a fair comparison should be made between EWIL and IL, which clearly shows EWIL a winner. However I am a little troubled by the gap between IL and the behavior policy. Do you have any analysis about why the performance of the cloned policy is better than the behavior policy? Since in IL you do not use any information from reward r, I expect their performance should be comparable. I guess this gap is related to the function approximation adopted?  Overall, I think the paper tackles an important and interesting problem in a creative and effective way. The proposed method is analytic justifiable and empirical validated. The writing is clear and correct. For the above reasons, I think the paper ought to be accepted. ######################After Rebuttal ################################ After reading the feedback from the authors and the comments from other reviewers, I want to append some extra opinions in my comments. 1) I agree with the authors that the state distribution does not change the optimal discriminative classifier, which is the optimal policy conditioned on each state. 2) In Sec 5.1, the authors show the connection of Alg 1 and imitating a better policy. The approximation error may not be averaged on the state distribution of the improved policy. However the equivalence still holds with their definition of KL divergence with specific state distribution \rho(s). 3) For the function approximation part, personally, I think it is not important whether a policy is better than another in some states with zero state distribution probability. To me, it is a common assumption that every state considered in the state space will be visited many times, as in an ergodic MDP with infinite horizon.