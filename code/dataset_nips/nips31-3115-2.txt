This paper proposes to use small world graphs as data structures to embed words vectors into for a fast lookup of potentially relevant words that are then put into a softmax.  It's a useful datastructure. Not quite machine learning... but useful for softmax classifications in MT and language modeling.   Wish it was trained on a fast and competitive language model and actually mention perplexity numbers. Ideally you can use: https://github.com/salesforce/awd-lstm-lm  On NMT, it would have been better to use a parallelizable transformer network also.  In spite of the large number of words in a vocabulary, human brain is capable --> In spite of the large number of words in a vocabulary, THE human brain is capable  speedup while attaining the accuracy --> speedup while maintaining the accuracy