This paper introduces new sparsity inducing design paradigm for probability distributions that generalizes existing transformations and have desirable properties such as (approx) scale/translate invariance, lipschitz continuity, monotonicity, full domain etc. The paper is well-motivated and written, with ample examples explaining the behavior of the loss functions and experiments. The paper also describes an application to multilabel classification, and provides closed form functional updates to train based on the proposed framework. The resulting loss function is also convex; the authors also draw comparison to hinge loss.   I think this paper should draw many discussions at nips. There is no theoretical consistency studies, and while the empirical evaluations could have been expanded to more applications, nevertheless this is a strong paper imho.   For completeness, can the authors also include the derivation of the Lipschitz continuity of 1+1/Kq.   I would suggest reporting of time taken to train, since constraint violation type losses are slower to train on. Also, it should be easy to fashion a smoothened version that can be formulated that is not as "strict" but at the same time does not require to worry about subgradients.   Is there an intuitive explanation of why sparsehg does not seem to perform as well ? It trades off for more area of the domain to have a sparse representation, is it over-regularizing ? 