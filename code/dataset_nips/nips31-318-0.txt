This work is interesting. I would like to ask the authors to compare their approach to GradNorm. Comparison to Kendall et. al is not sufficient due to caveats of uncertainty weighting.  Also, the authors are strongly encourage on NYC V2 data set.  This works lacks a stronger experimental section.  I have re-read the paper and the rebuttal posted by the authors. I thank them for performing the additional experiments and comparison they did with GradNorm. They did, however, ignore to do demonstrate the performance of their approach on NYU V2 dataset. Furthermore, here is a more detailed reviewed i chose to omit initially.  A large portion of the introduction in spent on discussing why assuming a linear combined loss function L = sum(w_iL_i) is not ideal for multitask learning, yet their method is choosing this linear combination of gradients based on their numerical \alpha vector. My high-level opinion is: I think it's a neat idea that makes a valiant effort to take a principle approach on MTL. The results are also promising, The main contribution however, is the approximation of the single backward pass, which was the chain rule plus an application of the triangle inequality.  A few more specific comments: (1) I believ from the Desideri paper, that there is a 1d search for the learning rate at each step - they set learning rate to the maximum learning rate possible such that all task losses are monotonically decreasing at that learning rate. In this work, however, the 1d search is not present, despite the paper borrowing heavily from this paper. Is there a justification for this?  (2) How does the algorithm interact with more complex update rules? E.G. Adam, momentum?  (3) I am still confused by the approximation performing better than the non-approximated version. How is the approximation a form of regularization?  