 Let me explain why I think the algorithm is the same as Andrychowicz et.al., with "the new interpretation of such algorithm and the parametrization in terms of machine teaching scenario" as in my review.   Following the same notations used in this paper, let's check the update rule in L2T-DLF:  w_{t+1} = w_t - \eta_t g(w_t; \theta)  where g(w_t;\theta) = \nabla_\theta L(w; \theta).  Conduct this update T times, we obtain w_T which can be understood as a RNN with \theta as the parameters. Plug this into the Eq (2), \theta is then optimized by the (stochastic) gradient descent. Essentially, the L2T-DLF is learning an update rule in the optimizer, which is the same as Andrychowicz et.al..   There might be two differences: 1, in L2T-DLF, the update rule is derived from loss function perspective, and in Andrychowicz et.al., they directly parametrized the update rule as a RNN.  2, the objectives of the optimizer (as clarified by the authors in the reply)  Andrychowicz et.al. use LSTM and working on multiple datasets, while L2T-DLF use a traditional RNN and working on training and development dataset, which makes these two different. However, these are the parametrization and loss function, which I think should be known as "model", while I am talking about the algorithm to obtain the learned optimizer.   I think these connections should be discussed in the paper.    Regarding the machine teaching protocol, the author assumes the teacher not only know the students updates rule and stepsize, but also can *modify* these in students, i.e., the students will use the first-order of the teacher's output for future learning. This is different from the existing literature [30].  The teaching protocol is important in machine teaching. With different ability of the teacher, the teaching efficiency will be different. For an extreme example, if the teacher can directly manipulate the student's model, then, no sample will be needed. As a machine teaching paper, the teaching protocol should be clearly discussed.   In sum, I think at least this interpretation is interesting. However, the authors should discuss the connections to the existing work, especially Andrychowicz et.al., and the teaching protocol.   ============================================= In this paper, the authors raise an interesting teaching scheme, i.e., the teacher adapts the loss functions for the students, which leads to the changes in the update rule of the students. By parametrization of the loss functions, with the reverse-model differentiation trick, the loss function of teacher and the model of the student can be updated together. The authors evaluate the proposed algorithm on several practical applications, including image classification and neural machine translation, and demonstrate the benefits of the proposed method.   However, there are several issues in current version need to be addressed.  1, The proposed algorithm is **exactly the same** algorithm proposed in [1]. The novelty part is the new interpretation of such algorithm and the parametrization in terms of machine teaching scenario. However, such important reference and other related work [2, 3] has never be appropriately cited and discussed.   Please add the discussion to the related work [1] and its extensions [3] and emphasize your contribution comparing to [1,2,3].   2, The machine teaching scheme is not clearly explained. Specifically, based on the algorithm proposed in the paper, it seems the teacher can access to the zero- and first-order of the model of the students, and moreover, the students can access to the first-order of the teachers loss function. The differences communication scheme between the proposed teaching scenario and the existing teaching scenario should be discussed clearly.   3, The reverse-model differentiation has a well-known drawback. The memory cost are increasing with the steps of the unrolling of the first-order updates, i.e., T in the Algorithm 1. Such drawback limits the RMD usage in practice and should be discussed in the paper.  In sum, I think the interpretation of the learning to optimization idea in terms of machine teaching is novel and inspiring. However, the existing work should be correctly acknowledged, and the drawback should be explicitly discussed.   [1] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. arXiv preprint arXiv:1606.04474, 2016.  [2] Li, Ke and Malik, Jitendra. Learning to optimize. International Conference on Learning Representations (ICLR), 2017.  [3] Chen, Y., M. W. Hoffman, S. G. Colmenarejo, M. Denil, T. P. Lillicrap, M. Botvinick, and N. Freitas (2017). Learning to learn without gradient descent by gradient descent. In: International Conference on Machine Learning, pp. 748â€“756 