The universality for smooth and non-smooth problems, and ability to converge on stochastic problems are very appealing properties of the algorithm. Not having to tune the smoothness parameter seems especially attractive since it makes practical deployment of the algorithm much simpler. To the best of my knowledge, no other algorithm combines all of these properties in one package.  Weaknesses: The algorithm suffers an extra log factor that would ideally be removable. The stochastic guarantee for AcceleGrad is dissapointingly non-adaptive. One would hope for an adagrad-like guarantee to complement the adagrad-like step-sizes.   Clarity: The paper is clearly written.  Originality: The main results seem original. I believe the stochastic result for adagrad adapting to smoothness is unofficially known among some others in the community, but there is some value in stating it formally here.   Significance:  As a deterministic algorithm, this algorithm seems to have significant advantages over previous universal algorithms in that it avoids line searches and does not require pre-specifying a tolerance for the final solution. The stated stochastic guarantee seems less good than the those obtained in other adaptive algorithms, but it is aesthetically appealing to have at least some guarantee.  I think these kinds of adaptivity results are very interesting, and I think there is hope that the basic techniques used in the proof can form a basis for future work in the intersection of adaptivity and acceleration.