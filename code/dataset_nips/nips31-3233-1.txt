The authors introduce a training algorithm (back-matching propagation) which optimises each layer of a neural network using a separate matching loss. This is inspired by the work of Carreira-Perpinan and Wang and their Method of Auxiliary Coordinates. This approach allows the authors to study the Hessian of the loss at each layer, with respect to the parameters at that layer. These are the local Hessians. Efficiency of Deep Learning tricks such as skip connections and batch normalization can then be studied using these local Hessians. The authors also propose a variant of SGD, called scale amended SGD, and test this variant in an experiment.  Some issues with the paper: * Comparison of vanilla SGD and scale amended SGD in section 4 is done using a single run of training with each algorithm. There is no confidence interval. * How where the hyper-parameters chosen for Figure 1 in section 4? Since this compares two different algorithms, there is no guarantee that a set of optimal parameters for one algorithm are also optimal for the other. How do we know that different hyper-parameters would not show better results for vanilla SGD?  Other comments: * In section 2, neither Procedure 1 nor formula (5) define the update rule for $z^{k+1}$. This needs to be clarified. * If $k$ is an index for the various elements in the input data $X$, what is the meaning of the notation $z_b^{k+\frac{1}{2}$? * Footnote 3 on page 4 should be moved earlier in the paper since bias terms have already been removed in previous formulas.  ===== Thank you to the authors for their reply.  Their response do address the points I have raised. I am happy to raise my review to 'marginally above' provided the authors do the following:  1) make an effort to better explain what their algorithm exactly is (in particular, properly define all variables, with clear update rules);  2) replace their single run curve with the multiple run graph they put in their rebuttal.