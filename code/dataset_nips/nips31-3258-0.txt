The paper presents a new strategy for training networks that are robust to adversarial attacks. The approach is intuitive and well explained:  (1) The authors how that given a perturbation epsilon, the network will return the correct answer providing that the margin (difference between correct class and highest scoring incorrect class) is higher than a certain function that depends on the lipschitz constant and the norm of the pertubration.  Note that a function with high margin is highly flexible/discriminative, while that with a low lipschitz constant is smooth and more robust.  Based on this above, the authors introduce a regularization term (auxiliary loss) during training which encourages classifiers to be robust (i.e. increasing the "guarded area" by either increasing the margin or decreasing their lipschitz constant  The authors propose a new technique for computing the Lipschitz constant that is efficient and a tighter upper bound than previous approaches.   Empirical results show that the authors' work better withstands C&W style attacks compared to existing approaches.   