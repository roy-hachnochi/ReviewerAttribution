The paper introduce a proximal algorithm for solving composite optimization problem with sparse l1 norm regularizer in a distributed fashion. It extends a previously introduced method call SCOPE (S-Y Zhao etal 2017) in order to handle sparse regularization. As a main contribution, the paper highlight the dependence of the partition of the data on the convergence rate (which is linear  given strongly convex loss part). Numerical experiments demonstrate practical efficiency of the proposed method.  As stated by the authors, the pSCOPE is limited to l1 norm regularizer whereas this restriction does not seem very legitimate. It would be interesting to keep a general framework that applies to any regularization R with a computable proximal mapping.  According to (M. Jaggi, 2013) the Lasso and SVM are equivalent in an optimization point of view. While SCOPE applies to SVM, the authors claim that it does not apply to the lasso. So the authors should justify properly the assertions on the limitation of SCOPE.  The assumptions on theoretical results are pretty confusing and sometimes does not apply for the examples given in the paper. For instance the Lipschitz continuous assumption on f_i in lemma 2 does not hold for the Lasso. Also the strong convexity assumptions on F_k in Theorem 2 does not hold if the ridge regularization is not added as in the elastic net case. So the convergence rate presented does not holds for the vanilla lasso and logistic regression with l1 regularizer. So the second point in line 73 should be clarified or weakened.  M in line 106 is not defined in line 12 of algorithm 1, is there a missing nabla? The definition 5 of a partition seems independent to P, so why it should be wrt P()? Lemma 2 in line 180 eta is not defined in theorem 2.  I am not an expert in distributed optimization and I can not evaluate the significance of the contribution. This seems to me to be a rather direct extension.