 Title: Contamination Attacks in Multi-Party Machine Learning  Summary: The paper considers an attack scenario in multiparty learning and present a defense method based on adversarial training to mitigate contamination and membership attacks. The results show that the method can provide such protections with little cost of model performance.   Strengths: The paper presents an interesting application of adversarial training in the multiparty setting.   Weaknesses: Contributions are specific to the presented applications.  Quality: The proposal is straightforward, and the performance is evaluated appropriately although not comprehensively.  Clarity: The paper is mostly well-written. The abstract is too nondescript ("...there exists attacks...").  Originality: While the data poisoning is a well known problem, the idea of inducing "malicious correlation" in a multiparty setting seems new. There are missing references.  Significance: The impact of the this paper as a theoretical or algorithmic work is likely limited, but the paper demonstrates new applications of adversarial learning.   <Detailed comments>  Can the idea of "malicious correlation" be explored more fully and formally? The goal of an attacker is to influence the globally-learned f_\ast in desired ways, and as such "malicious correlations" seems to be just one example of possible data-poisoning attack at training time in the multiparty setting.  The algorithm in table 1 doesn't seems to be the most general approach for influencing the trained model.  Can the optimal attributes/labels of the malicious party be found through optimization, e.g., as in "Understanding black-box predictions via influence functions" by Koh et al, 2017?  lines 117-120: For a non-adversarial party, this policy seems to encourage the party from sending meaningful data, since the party will receive only the local f_i if the sent data (D_train_i,D_val_i) are not from the same distribution. However, how does it affect the adversarial party?  Using adversarial learning to preserve privacy through the key equation (2) has more related papers than [20], such as "Censoring representations with an adversary" by Edwards et al., 2015, and "Preserving privacy of high-dimensional data with minimax filters" by Hamm, 2015. In particular, the latter presents an optimization problem in a multiparty setting somewhat similar to this paper.  In Proposition 1, it would help to remind the readers of interpretation of the proposition for completeness.  line 208: membership  Evaluation: Are there any other comparable method that can used for the same purpose? How effective is the cross-validation based defense?  For the three datasets, what is the largest contamination percentage that can be successfully prevented by the proposed method? Should c be fine-tuned for this?