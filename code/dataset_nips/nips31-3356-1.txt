The paper addresses the actual problem of efficient training of very deep neural network architectures using back-propagation. The authors propose an extension of the back-propagation training algorithm, called features replay, that aims to overcome sequential propagation of the error gradients (backward locking).  Backward locking disables updating network layers in parallel and makes the standard back-propagation computationally expensive.   The principle of the new method is to divide NN-layers into successive modules, while each module is time-shifted with respect to the following one. Such modules can be processed in parallel without accuracy loss and with relatively small additional memory requirements. The method comprises a novel parallel-objective formulation of the objective function.  The authors also present: - a theoretical proof, that the algorithm is guaranteed to converge under certain conditions, and  - a thorough empirical analysis of the new method (for convolutional NNs and image data).  Clarity: The presentation is comprehensible and well-organized, the new method is described and analyzed adequately (I appreciate the pseudo-code).  (-) The graphs in Figure 4 are too small and therefore hardly comprehensible, the lines overlap.  Quality: The paper has a good technical quality. The description, formalization, theoretical analysis and also experimental evaluation of the new method are detailed and comprehensible.  The experimental results indicate that the new method induces NNs that generalize slightly better than NNs induced by the standard back-propagation algorithm. Such property is surprising and deserves deeper investigation.  Novelty and significance: The paper presents an efficient mechanism to parallelize the back-propagation algorithm and increase its efficiency without great memory requirements.  (+) The principle of the method is simple and easy to implement (despite a relative complexity of its formalization). (+) The experimental results for deep convolutional NNs are very promising: slightly better generalization than standard back-propagation algorithm, fast convergence, low additional memory requirements. The method seems to substantially overcome the previous approaches that usually suffer from at least one of the following problems: low efficiency, high space complexity, poor convergence or worsened generalization.  ____________________________________________________________  I thank the authors for their rebuttal, I think, the method is worth of publication.