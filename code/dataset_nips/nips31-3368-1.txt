The paper proposes using different likelihoods for each of the outputs of a multi-output Gaussian process. The paper addresses most areas: a model, an approximation (with variational bounds), stochastic inference for large data sets, hyper-parameter tuning, how prediction is done in the approximate model, experiments on synthetic data, and experiments on real-world data. Hence, the paper is very thorough. However, there a a number of shortcomings which will be highlighted below.  While I have not seen any paper prior to this that explicitly places different likelihoods on the outputs of a multi-output Gaussian process, it is not hard imagine this --- especially when variational inference is used which reduces the (log-)likelihood terms to a summation. The difficulty is to find convincing applications/data for which such a model is useful. I think the Human Behaviour Data is a good fit for this model, except that the presence/absence-at-home output is somewhat redundant and contrived --- isn't this output a threshold version of the distance-from-home output? I also find the choice of the London House Price Data rather inappropriate --- the authors have used the property-type as an output when it is best used as an input. For the High Dimensional Input Data, the authors have chosen to predict the gender and age, which are attributes/covariates/inputs in the data set, while side-stepping the more interesting and important and original intended task of distinguish between "the presence and absence of cardiac arrhythmia and to classify it in one of the 16 groups." Instead of these last two data sets, I recommend that the authors concentrate on the Human Behavior Data --- for example, using a counts model for the number of Whatsapp messages sent; and/or analyzing the effect of the present/absence-at-home output. In addition, I also think a covariance function that incorporate periodicity is a better fit than a vanilla RBF kernel for this data.  L65: A better characterization of the method may be to say that it *combines* MOGP with Chained GP, and that it *generalises*   L154 and L157: Here, the concepts of independence in the model and independence in the approximation are not clearly stated.  In section 4, it may be clearer to state that Chained GP with Bernoulli is simply the classical binary-classification Gaussian processes.  Minor: The two [Valera el al. 2017] references need to be given different labels.  [Quality] This submission is technically sound. However, I feel that the experiments are lacking to fully evaluation the benefits of this model. In addition, the authors should compare with the work of Yang et al. 2009 using the data sets therein (both simulated and the asthma data); or the work of Valera et al. 2017 using the data sets therein.  [Clarity] This submission can be clearer by addressing some of the points above.  [Originality] This paper is a new but rather obvious combination of previous work.  [Significance] I think the work, especially with the released code, will be widely used in data sets of this nature.  [Comments after author rebuttal] The reply has addressed my reservations on "convincing applications and data", and I have revised my score upwards. 