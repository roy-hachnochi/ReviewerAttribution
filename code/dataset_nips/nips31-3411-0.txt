This paper presents a technique for adapting from multiple sources to a target. The main goal of this paper is to mitigate negative transfer from unrelated domains. The authors use the now popular adversarial neural networks-style framework where the feature layer is trained to minimize the label loss + a loss function to measure how close the domains are in the feature space.   The main contribution of the paper is to use a Wasserstein like distance between the source distributions and the target distribution for the second part. The authors further add a weighted importance to each of the source domains to represent the relevance of that domain. The weights for the domains are the output of the softmax over the Wasserstein distances of target domain to each of source domains.   The authors also show an error bound over classifier discrepancies using a Wasserstein-like distance measure. The key idea about this bound is that it bounds the error on target domain by a weighted sum of errors on source domains and not with the max -- this implies that reducing the weight over a potentially unrelated source domain can mitigate its impact.  Pros: 1. Well written and straightforward to understand. 2. Theorem 3.3 is a nice extension to previous results on multi domain adaptation.  Cons: 1. Lack of experimental analyses. E.g. it needs ablation analysis to disentangle the contributions of Wasserstein distribution matching and weighted source matching.   Suggestions: 1. Authors should motivate the results in section 3 in a better way in light of the proposed setup. It is not clear how theorem 3.3 relates to the proposed technique. 2. Section 2 will be much clearer if the authors put the algorithm in the main paper instead of the appendix. E.g. there is a circular dependency between source weights w_s and the parameters \theta_D and it is not entirely clear that this is handled iteratively until one looks at the algorithm.  Read author response and satisfied.