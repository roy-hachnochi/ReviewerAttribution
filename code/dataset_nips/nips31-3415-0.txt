This paper describes a tractable gradient computation for structured prediction problems, in which the authors proposed an approach to efficiently compute the expectation of feature function term that appears in the gradients using WFA. Although the proposed approach is sound, it has very restrictive assumptions: features should have the Markovian property which makes it not advantageous with respect to existing approaches. Specifically, I believe that if features are decomposable to set of local features with the Markovian property of order q and the loss is decomposable, then there exists a junction tree with tree-width of q that represents those feature. Therefore, an efficient inference in that tree is O(|\Delta|^q), which is similar to what proposed here (without considering the constant which should be a function of l ). In general, I am not convinced about the impact of the proposed algorithm since the authors compare the gradient computation with a naive algorithm that enumerates every structure; definitely dynamic programming such as inference in junction tree would end-up with very similar results based on the construction of the problem, so I would expect that sort of comparison as proof of concept instead of comparing to naive computation.  Moreover, I cannot follow how the proposed approach can be used for computation of gradient in a deep neural network except the weights of the last layer. I would appreciate if the authors elaborate more on that.   