Summary: The paper proposes a probabilistic approach to detect whether a new data point is an outlier or an inlier with respect to the training data manifold. The proposed approach, called Generative Probabilistic Novelty Detection (GPND), utilizes an adversarially trained autoencoder. The decoder network defines a manifold in the observation space and the encoder network projects the data onto the manifold. An adversarial loss is defined to shape the latent space to be a known parametric distribution. A reconstruction error loss and an adversarial loss are defined to ensure the reconstruction quality of the projected inlier data.  The training data (inlier) is modeled as being sampled from the manifold with additive noise and the distribution is modeled via the product of (a) the distribution in the tangent space which is a function of the target distribution in the latent space, and, (b) The distribution of the residuals in the normal space which is considered stationary. A sample is considered an output if it’s probability w.r.t the inlier distribution is below a threshold.   The paper validates the approach by setting up (artificial) outlier detection tasks using the MINST, COIL-100 and Fashion-MNIST datasets. The experiments on these datasets show that this probabilistic estimation of outliers works better than the existing approaches.    Strengths: The paper addresses an important problem – how to detect an outlier or an example atypical of the training data. This problem is significant and important for deploying any ML system in an open-world scenario. The authors present an incrementally novel approach – using adversarially trained (both latent space and output) autoencoders with a probabilistic formulation to calculate the probability that any sample is from the inlying distribution. I haven’t seen the specific derivations before although they seem straightforward. The exposition is mostly clear and the approach is technically sound. Results on MNIST, COIL-100 and Fashion-MNIST datasets seem satisfactory. Ablation studies are also performed to quantify the impact of the factors in p(w).   Weaknesses: I have a few points below which I hope can be addressed in the rebuttal. 1. Choice of certain hyperparameters – how is the size of the latent space chosen?  2. Experimental results:  Ablation study: needed to understand the impact of different components of the loss function – reconstruction loss and the two adversarial losses – one for the latent space, other for the output as also the impact of the probabilistic model. The authors allude to the fact that adversarial training improves the model, this is not experimentally validated anywhere.  Manifold projection and the residual: The authors model the two probability distributions – one on the manifold via the latent space distribution and the other for the residual of the projection onto the manifold. Independence and stationarity are assumed to compute the two and use the product as the probability for generating an inlier data point. Since outliers are expected to be away from the manifold, the projection residual is expected to play a bigger role in identifying outliers. This is not the case as shown in Figure 5. Why? Please discuss. Comparison with other methods: There is also no comparative discussion of their results vis-à-vis the state of the art.  