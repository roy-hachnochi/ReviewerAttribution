The authors argue that accelerating/decelerating return constraints can help model interpretability.  To this end, they propose two methods: (i) an extension of input convex neural networks that supports monotonicity constraints; (ii) an extension of partial monotonic lattice models that supports concavity/convexity constraints.  The proposed methods are compared to several constrained and unconstrained baselines on different datasets.  Pros: - The paper is very well written, and definitely relevant to NIPS - Constraining the model to have accelerating returns w.r.t. chosen variables allows to better control the model, which can be useful in real applications - The technical contribution seems sound - The empirical evaluation is reasonably thorough (several datasets and relevant competitors)  Cons: - The term "interpretability" is unqualified, little evidence for it is provided - The improvement in terms of pure performance is minor or negligible - All datasets have a small number of features; scalability and stability of learning are not evaluated   Major remarks:  - The only points that I am really concerned about are the first two.    It seems that the term "interpretability" here is used as a synonym of "visualizability".  I suggest the authors to clarify this in the text.    It also seems to me that visualizability only applies to the calibrator functions.  Indeed, for higher-dimensional models the interactions between variables may be very complicated, making it difficult or impossible to show the effect of inputs on the output.    In other words, I think that visualizability does not really apply to the constrained SCNN and RTL models as a whole.  This seems to mine the idea that these constraints can improve visualizability (at least in higher dimensional models).    It is still the case that the constraints help in controlling the model, and this is good; but this is *different* from interpretability.  As a matter of fact, if the learned functions can not be visualized, the interpretability argument becomes moot: the user already knows that the curves will obey the constraints that she specified -- but she won't be able to see what the curves actually look like, i.e., she won't be able to really analyze and understand them.    The authors should qualify their claims, evaluate them empirically (e.g. with a user study), or drop them.   - I agree that MSE is not the only useful property of the model.  In some applications it makes sense to sacrifice performance in exchange for more control.    However, in the four datasets (with the caveat that MSE is non-trivial to interpret...):    - Cal Sales is essentially a toy dataset (it's 1D), it's useful for debugging but not really representative of most ML applications.    - Puzzle Sales (3 features): all models do "much worse" than the DNN baseline; constraints have some positive impact, but nothing major.  Note that the test set is non-IID, so these numbers may be biased.    - Domain Pricing (18 features): performance is better than the baseline (which however overfits horribly), but the additional constraints have little impact.    - Wine Quality (61 features): performance of SCNN improves slightly, lattice performance is unaffected.    - Ranking (15 features): considering only MSE on the IID test set [the non-IID numbers are much harder to analyze], SCNN and RTL performance worsens slightly for the full train data case.    It seems that the more features there are, the less evidence there is for the additional constraints to really help performance.    In a way, the paper is missing a "killer application" where shape constraints can really make performance better.  However, as I said, the contribution makes sense anyway, as it allows to better control the learned model.    I think that the authors should clarify that (ultimately) the contribution is not really about performance.   Minor remarks:  - The observation in Appendix D seems quite important to me (it is a limitation of the model, after all), and should be moved to the main text for visibility.  - What is the impact on run-time of adding the constraints?  What is the impact on learning stability?  - (It would be nice to check what is the effect of these constraints on adversarial examples.)