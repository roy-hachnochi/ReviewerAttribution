Incremental approach to learn SPNs based on new observations by adapting the structure (using expansions and collapsing of nodes) and parameters accordingly. The main result is that one can do so while improving the model quality measure, which leads to an interesting updating procedure for SPNs (though one may debate about the term "online" for that). The approach to learn SPNs expands and collapses the model according to incoming observations. The idea is somehow ad-hoc, but it is interesting nevertheless. Experiments show good accuracy in a collection of cases. The drawback is that SPNs are arguably more suitable for discriminative instead of generative analysis. This is acknowledged in the conclusions. The paper is well-written and the approach is interesting. Learning SPNs in an "online" fashion is important and worth pursuing.  Some points: - Abstract: "inference is always tractable" - this is not true, full MAP is already NP-hard.  - "SPNs are equivalent to Bayesian and Markov networks" - this might be misleading, since to represent the same domain, one may need an exponentially large SPN in order to represent the same as a BN or MRF. So we should be careful with the "equivalence" usage.  - Hard EM, which is mentioned for learning, is an NP-hard task in SPNs, since it requires (full) MAP inferences (see Peharz et al PAMI 2016, Conaty et al UAI 2017, etc). Obviously one cannot claim that an approach is tractable if part of the task is NP-hard and an approximate algorithm is used. It would be better to clarify that.  - Some references in the list are incomplete.  After response: The issue with respect to MAP and hard EM has been clarified.