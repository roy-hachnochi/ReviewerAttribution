This paper shows that under technical conditions, as the size of the "minipool" approaches infinity, uncertainty sampling is moving in the descent directions of the expected 0-1 loss in expectation. Therefore, uncertainty sampling may be interpreted as performing SGD with respect to the expected 0-1 loss. Based on this interpretation, the authors tried explaining why uncertainty sampling sometimes yields a smaller risk than the standard random sampling approach and why different initialization gives different outputs, as these are common behaviors of the SGD.   The authors' perspective is quite fresh and inspiring, but there are some weakness.   First, the main result, Corollary 10, is not very strong. It is asymptotic, and requires the iterates to lie in a "good" set of regular parameters; the condition on the iterates was not checked. Corollary 10 only requires a lower bound on the regularization parameter; however, if the parameter is set too large such that the regularization term is dominating, then the output will be statistically meaningless.   Second, there is an obvious gap between the interpretation and what has been proved. Even if Corollary 10 holds under more general and acceptable conditions, it only says that uncertainty sampling iterates along the descent directions of the expected 0-1 loss. I don't think that one may claim that uncertainty sampling is SGD merely based on Corollary 10. Furthermore, existing results for SGD require some regularity conditions on the objective function, and the learning rate should be chosen properly with respect to the conditions; as the conditions were not checked for the expected 0-1 loss and the "learning rate" in uncertainty sampling was not specified, it seems not very rigorous to explain empirical observations based on existing results of SGD.   The paper is overall well-structured. I appreciate the authors' trying providing some intuitive explanations of the proofs, though there are some over-simplifications in my view. The writing looks very hasty; there are many typos and minor grammar mistakes.   I would say that this work is a good starting point for an interesting research direction, but currently not very sufficient for publication.   Other comments:  1. ln. 52: Not all convex programs can be efficiently solved. See, e.g. "Gradient methods for minimizing composite functions" by Yu. Nesterov.  2. ln. 55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter. 3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate.    4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties.  5. The proofs in the supplementary material are too terse. 