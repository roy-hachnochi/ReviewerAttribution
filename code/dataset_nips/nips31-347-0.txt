This paper proposes contrastive explanations to explain predictions from machine learning models. The idea is very elegant: this paper observes that most current work in interpretable machine learning only use features that are present in the instance, but human reasoning also relies on minimal differences from the closest class. In particular, I like the argument based on the notion of pertinent positives and pertinent negatives in medicine and criminology.  The mathematical formulation of pertinent negatives is close to perturbation analysis and it is clever to use autoencoders to make the generated instance close to real examples. But the formulation of the pertinent positives may require a bit of more discussion. It seems straightforward and an intuitive adaptation of the pertinent negatives. But there is no requirement that the chosen parts are meaningful. This is indeed be a hard problem, but this may be related to the relative worse performance of the pertinent positives in the experiments too. It seems that the pertinent positives are much harder to interpret, at least for me. Another implicit requirement in the proposed approach is that x_0 falls in discrete space, which may be an issue in some settings.  The main weakness of this paper lies in the evaluation. Although it is a great thing that this paper uses more datasets than MNIST, the evaluation can be much improved.  1) The statements in the MNIST experiment such as "While results without an CAE are quite convincing, the CAE clearly improves the pertinent positives and negatives in many cases. Regarding pertinent positives, the cyan highlighted pixels in the column with CAE (CAE CEM PP) are a superset to the cyan-highlighted pixels in column without (CEM PP). While these explanations are at the same level of confidence regarding the classifier, explanations using an AE are visually more interpretable." are problematic. These are quite subjective statements, and some form of quantitative evaluation across subjects is required for such claims.  2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant.   3) This paper did not provide enough information regarding how the evaluation was done in the brain functional imaging experiment. It seems that the only sentence is "With the help of domain experts".  4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case.  Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid.  Minor presentation-related suggestions:     I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as "Explanations as such are used frequently by people" are questionable and at least requires better evidence.     line 218: an CAE -> a CAE     line 252: spend -> spending  I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.