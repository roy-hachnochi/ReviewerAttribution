The authors propose an approach for non-linear time alignment of sequences in a shared latent space. More specifically they explain the observed time series data via a generative process: (i) first the inputs are passed through an alignment function, a, (ii) a shared latent process, f, is applied to the aligned signal, and, (iii) a warping function is responsible to introduce output specific attributes to the observe the final signal. In all three steps the authors impose Gaussian process (GP) priors to model the unknown functions, forming a set of deep GPs, one for each observed sequence. Additional structure is imposed on the function f by allowing it to model correlations between multiple outputs via a convolution with a number of independent processes. The key part of the paper is that the set of deep GPs is enforced to interact with each other via sharing information through the common middle layer, i.e., the function f is shared across the different Deep GPs. The model is learned via optimising the lower bound to the likelihood, which is obtained following a nested variational compression approach. Experimental results on toy and real world data demonstrate the advantages of the proposed model.  The paper is nicely written and I really enjoyed the fact that it targets a specific problem/application from the very beginning. It may seem a bit weird at first, however, the reader really knows what is expected to follow and can definitely make the connection between the modelling choices and the targeted challenges. I really liked the idea of sharing the middle layer and the inducing variables u_{f,d} across the multiple sequences. However, the authors should be a bit more careful with the used notation. From my understanding *boldfaced* f is basically the collection of the function f evaluated on the data a(x). If this is correct then when the authors refer to a shared *boldfaced* f throughout the manuscript can be very misleading, especially when we talk about sequences with different cardinality.  My biggest concern has to do with my understanding of the Psi statistics in Section 3.2 - First of all indices in this section are overloading the indices already defined. i is now used to denote the inducing point and n the point in the sequence, while i has been already used to denote the elements of X, i.e. X={x_i}_i=1:N. Can you please clean up this mess? - In both equations 6 and 7 we can see the Psi and Phi to be computed under some u. What is that u? Normally I would expect that the inducing input Z would be in the place of u. Have we assumed that the inducing inputs at the second layer are the inducing outputs of the previous layer? Then that would make sense, although, in that case we would need to integrate out also over u from the previous layer. In general I am very confused since in the same page in line 105 we have already defined different Z and u for f. Am I missing something? Please explain and correct me if I am wrong.  The evaluation procedure is targeted to highlight the advantages of the proposed model, although the captions accompanying the figures do not always fully explain what is happening: - For example in Figure 3 there is no description about the continuous lines that are draws from the model; the reader has to wait till the next experiment and specifically Figure 5 in order to acquire that information. - Furthermore, in Figure 4 there is no description whatsoever of what the middle row (purple) line demonstrates. Is this the recovered latent function? In general I do not understand how is it possible to associate points in that plot with points in the two turbines since the two sets have different cardinalities. Why this is not the case in the toy experiment in Figure 3?  Minor comments: - Figure 3 c & d, y labels for the plots in the third row should be g instead of y. Am I correct? - Same in figure 2 - References need update since you cite the arxiv version of many published papers.  ---After rebuttal--- The authors have addressed my comments. I enjoyed the paper. I believe it can get improved by including an additional experiment with more than two signals to align or a more general non-linear warping function on real data. Since I have not suggested that in my original review I trust the authors would include such an experiment in the final version, if the paper goes through. I am still positive about the submission.