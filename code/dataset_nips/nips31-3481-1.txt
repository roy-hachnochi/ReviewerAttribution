The paper presents a new approach for training spiking neural networks (SNNs) via backpropagation. The main contribution is a decomposition of the error gradient into a "macro" component that directly optimizes the rate-coded error, and a "micro" component that takes into account effects due to spike timing. The approach therefore creates a bridge between approaches that treat SNNs mainly like standard ANNs (optimizing the rate-coded error), and algorithms that work on the spike level such as SpikeProp, which have previously not been able to scale to larger problems. There are similarities to previously published methods such as from Lee et al. 2016 or Wu et al. 2017, which are discussed briefly,  but there are novel contributions. Furthermore, the results on two tasks (MNIST and N-MNIST) show improvements over the state-of-the-art, although those improvements are small. Code for the algorithm is released in the supplementary material, which should allow reproduction of the results.  I think the paper is a nice contribution to the field of SNN training, but neither very original nor a major breakthrough. Furthermore I think that the tasks are not chosen to allow highlighting the real strengths or limitations of the method, and that apart from the good empirical results there is no more insight why this method works better (e.g. on the MNIST task with Poisson rates, where spike timing should not matter much). This explains my vote for score 6 (barely above threshold). I am very familiar with the related literature, and I think the authors have cited the most relevant work, so I chose confidence score 5.  The derivations are not too complex and seem to be correct and properly described.   Regarding clarity, my main criticism are the figures, which are far from self-explanatory. One reason is the small size and the short captions, but also e.g. the arrows in Fig. 3 are barely visible.  The paper is of relevance to the NIPS community, especially to the subcommunities interested in neuromorphic computing and computational neuroscience. I would however vote for a poster presentation, since these are not the biggest crowds at NIPS. The results advance the state of the art on relatively simple benchmark tasks by small margins.   Detailed comments: The problem formulation starts from an error function which only makes sense for rate codes, so the approach seems to be limited to classical classification (or regression) tasks, but not for tasks where reproducing a spike pattern is the goal. The approach is thus not fundamentally more powerful than conversion methods, but achieves its advantage from treating timing effects at intermediate levels. It is interesting that this is an advantage even for tasks like static MNIST where timing plays almost no role, but no insight into how real spiking networks should learn or operate.  In Section 2.2 the assumption is made that the total firing count can simply be calculated as the total post-synaptic potential divided by the threshold. This is only correct if there are no refractory periods or synchronous spikes that drive the potential way above threshold. In those cases, due to the reset of the membrane potential to zero, it seems that some of the potential is simply lost.  In section 3 it is not precisely clear how the thresholds are set "depending on the layer". Also the conversion into Poisson spike trains does not describe the maximum input firing rate, which is known to be crucial for the success of SNN classifiers. A time window of 400 ms for the spike sequence is also pretty long, and it is not described how stimulus length influences performance.  The simulation time step for N-MNIST is described as 0.55ms, which seems unusual. Please check if this is the correct value.  The evaluation is not quite satisfactory, because the compared methods differ in architecture, training method, and training epochs, and no attempt is made to separate these hyperparameters. E.g. it would be interesting to see the performance of the macro/micro method with 200 epochs, to have a precise comparison with STBP. In general the improvements are small and no error bars are provided, which makes the comparison not quite conclusive.   What the evaluation does not show is how much the micro vs. macro approach contribute to the success of the method. This would have required different tasks, where the timing of spikes (also on output level) plays a bigger role.  Although the paper is generally well written, there are occasional grammar errors ("We particularly concern...") that should be checked and corrected.  -------  Update after author response and discussion: While I appreciate the effort of the authors in running additional experiments during the rebuttal phase, I am not sure how much we should read into the comparison in 1.) in which the authors "quickly" implemented competing methods and show pretty terrible errors of the competing methods. It is of course impossible to judge whether the other methods were implemented correctly, but such a big difference is a big suspicious.  Fig 1.b from author response reveals that the method does not improve after about 25 epochs and the error is consistently higher than for a CNN, which is a bit alarming as well.  I also feel that my other detailed comments regarding the mathematical basis and details of the models were not at all addressed in the response. The figures would also definitely need some upgrade, but I understand that this cannot be handled in the feedback.  Overall, I think this paper is still right around threshold, so I think it is justifiable to either accept or reject. I will keep my score at 6 and certainly not be upset with either decision.