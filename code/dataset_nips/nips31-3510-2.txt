It is a good work because the authors  have proposed a new optimization problem for approximate policy iteration methods and achieved a better policy improvement than the conservative policy iteration.  However, there is one problem confusing me. In the practical instance of DPI, LQR works because cost function is designed to be approximated so that the approximation error would be low. But the cost function would not always be so smooth. In other words, the practical instance of DPI may fail if the cost function could not be approximated by quadratic function. Besides, why \eta is designed to be time-varying while \pi is a single policy as these two are both policies.  And please show the scores of the continuous settings. Although the experiments of continuous settings shows a better performance of the proposed algorithm than TRPO-GAE under the cumulative cost  defined in the paper, it lacks the average scores comparison of these experiments by default reward settings. 