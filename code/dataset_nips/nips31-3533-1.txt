Update: After reading the author feedback, I think the paper provides a good insight on the properties of regularized OT in the context of learning and the proofs seem correct. The paper would be stronger if the results also included continuous distributions.    -----------------------------------------------------------------    ------------------------------------------------------------------------------------------------- The paper provides theoretical guarantees for the convergence of GANs based on regularized Wasserstein distance. The approach itself was already proposed in (Seguy et. al 2017), so the main contribution of the paper is really the theory. The three key results are the following: the authors show that the regularized Wasserstein distance is smooth with respect to the parameters of the generator. They provide a bound on the error when approximately computing the gradient of the distance,  where the error comes from solving the dual problem up to some precision. They finally show that the expected gradient averaged over the iterations remains within a ball which radius depends on the accuracy of the approximation of the distance. The paper is globally well written although some notations are not properly introduced.  Overall, the contribution seems rather marginal as the results are somehow expected, especially theorems 4.1 and 4.2. They rely on the fact that the dual problem can be solved up to some accuracy epsilon. However, there is no guarantee this precision is ever achieved when using a fixed size neural network to approximate the dual solution.  On the other hand, theorem 3.1 could be more useful, but there are some points that need to be clarified:   - In the proof of theorem 3.1, it is not straightforward to me how Danskin's theorem applies to this case. The most common version of the theorem requires that the set under which the optimization occurs to be compact which seems not to be the case for the set of all possible couplings \pi. Since theorem 3.1 seems to be one of the main contributions of this work, it would be good to know precisely how the assumptions are used to prove the differentiability and which version of Danskin's theorem is used with proper references.  -Also, the set of joint distributions seems to be endowed with some norm, is it the total variation norm on signed measures? I'm guessing it is rather the L_{1} norm under dp(x)dp(y)? which would make sense since the optimal coupling is known to have a density with respect to dp(x)dq(y) by lemma 2.1, however, the notation is rather confusing at first sight.   - In theorem 4.1: The statement is a priory for arbitrary probability distribution p and q, however the proof in the appendix assumes that p and q are discrete and relies on extra assumptions stated in Remark E.1.1. It seems however that a more general proof could be obtained using only assumptions of theorem 3.1: inequalities similar to 31 and 35 could be obtained for any p and q under an appropriate norm. 