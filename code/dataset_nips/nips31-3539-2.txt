This paper aims to develop a cheap sub-differential principle for nonsmooth functions. They provide a randomized algorithm and show that under certain restrictions on their library of non-smooth functions, the randomized algorithm can generate sub-derivatives at a computational cost that is within a constant factor of 6 of the cost of computing the scalar function itself.   The paper is not within the reviewer's expertise though the reviewer is very familiar with optimization and gradient-based methods. So the comments are mostly from the perspective of an optimization person outside of the field of "automatic differentiation methods".   Strength: + Developing a cheap sub-differential principle is important for achieving computational efficient gradient-based optimization methods for non-smooth functions.  + It looks like the paper has a method that can handle it.  Weakness:  - The paper is very hard to read and understand for a person who is not working on "automatic differentiation methods". The organization of the paper and the mathematical writing of the paper are not reader-friendly.  For instance, almost all assumptions read like theorems. This may be common in the field of "automatic differentiation methods". But it looks confusing to the reviewer.   - Continuing with the assumptions: It is hard to know how practical the assumptions are. The method relies on  "overloading the library with ASD subroutines" (assumption 3.2). How hard is it?  - The randomized algorithm needs to sample uniformly at random from the unit sphere. What is the computational cost of this procedure? I think this step won't scale up freely with the dimension of x. Therefore, the contribution of the method is questionable.   - The paper needs to have numerical tests that report real run time on popular-used non-smooth functions where x is preferred to be high dimension.  Typo: Theorem 3.1: "is allowed use" to "is allowed to use"