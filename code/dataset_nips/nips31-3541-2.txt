In this paper, the authors formally establish connections between arguably popular, unsupervised learning models, e.g., hidden Markov models (HMM), tensorial mixture models (TMM), latent tree graphical models (LTM). Further, more explicit relationship between expressive power and model parameter (i.e., depth of model) was explored. Finally, an efficient algorithm for approximating "deep models" as a shallow model was described. Empirical validation confirms that this algorithm is indeed useful.   In overall, it was very interesting to see probabilistic models from different domains to be formally connected in a single framework of SPNs. The connection itself seems to provide a significant connection and has potential to be a stepping stone for more exciting developments from interchanging ideas of different domains.   While I fully understand the page constraints of the paper, I would like to suggest more detailed explanations on the models that are dealt in the paper. Especially, it would help the readers greatly if there is derivation of how each models are "homogeneous", i.e., can be reformulated into (1) with explicit description of corresponding density tensor.   Finally, from a curious reader perspective, it would be very interesting to see possibility of extensions to the 'intractable' case. Especially, the authors already mentioned that there is an equivalence between tensor-train and HMMs, while tensor-trains can be generalized to tensor networks and HMMs to general Markov (or graphical) models.   Minor comments: - The paper's description of latent tree models were constrained to hidden variables with finite values since it indices of tensor can only be finite. However I would like to point out that there is also an interesting work on using tensor with inifinite indices, called "Function-Train: A continuous analogue of the tensor-train decomposition". I suspect that this would correspond to HMM with continuous-valued hidden variables. - typo in line 263: "the accuracy \epsilon there is exponentially small"