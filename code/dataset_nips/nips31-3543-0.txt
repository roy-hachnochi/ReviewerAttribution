This paper adopts a perspective that has become popular recently, which views parameters in an optimization problem as defining measures in the scaling limit. The benefit of this approach is that non-convex optimization with the squared loss becomes a much simpler looking convex optimization problem over the space of measures. This paper uses this approach to derive based limit theorems for the large parameter regime.  I am not an expert in this area, so while I find the perspective and results interesting and worth highlighting, I found the paper difficult to parse in places. I believe significantly more could be done to elucidate the content of the results, propositions 3.2 and 4.2 in particular.  I will list some confusions I had in the course of reading the paper here, in the hopes that they will help the authors revise their work for a wider audience.  The role of convexity: After (10), the authors note that the objective function becomes convex in the limit. But what is written in (10) is already a convex function on the space of measures, which has linear structure. Is what is gained in the limit the fact that the measure possesses a density with respect to the Lebesgue measure? (Is that why the authors emphasize the term density?) Or is some other base measure intended?  Self-quenching for SGD: the authors highlight the particular self-quenching property of SGD, and argue in the introduction that this gives theoretical justification to the common belief that adding noise improves generalization. However, examining Prop 3.2 and its proof, I observe what I understood the authors to mean by "self-quenching" present in this setting as wellâ€”in the n \to \infty limit, the fluctuations of order n^(-1/2) grow smaller in the limit, so that in fact no fluctuations remain at order n^(-\xi) for \xi < 1. Is this indeed what is called self-quenching? (I.e., does it present the same phenomenon as is highlighted in Prop 4.2 and in the introduction?) If so, I am confused why this is highlighted as a virtue of adding noise, since the purely deterministic dynamics of GD also evince this behavior.  Numerical experiments: These are slightly hard to interpret. First, which plots show SGD dynamics, and which are for GD? Second, I'm puzzled by how to interpret the dotted lines in each plot. In the case of RBF, how are we to make sense of the empirical n^{-2} decay? Is this somehow predicted in the analysis of the GD, or is it an empirical phenomenon which is not theoretically addressed in this work. Please clarify. In general, I do not find that the numerical experiments clearly support the story presented earlier in the work, except with respect to the broad qualitative fact of convergence in the n -> infty limit.  ADDED AFTER FEEDBACK: I thank the authors for their response. As they revise this work, I encourage the authors to expand their discussion of self-quenching for SGD and clarify the experimental results. I would also encourage them to further discuss the connection to Dean's equation (the phrase "McKean-Vlasov equation" does not appear in the initial submission; this should be added if necessary). This would go a long way to making the methods used in the proofs clearer to the NIPS audience.