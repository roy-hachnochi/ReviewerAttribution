This article maps weights of a neural net that are being learned to particles, and views the learning procedure and dynamics of the particles. The article derives in an algorithmically constructive way the universal approximation theorem, and scaling of the error with the number of hidden units. The results are then demonstrated on learning of the energy function of the p-spin model. It is worth noting that the conclusions of the article are insensitive to details of the network architecture.  The general direction this papers takes is quite interesting. However, the paper is not very clear in what is actually the main contribution and hos is it related to existing work. I note some potentially questionable points below:   ** The abstract states: "From our insights, we extract several practical guidelines for large scale applications of neural networks, emphasizing the importance of both noise and quenching, in particular." It was not really clear to me from the paper what are these "practical guidelines".   ** The article analyzes online gradient descent, where a new mini-batch is sampled every iteration and is never reused in the learning afterwards. While this is an interesting learning dynamics that attracted attention, this is not the same as SGD that usually reuses samples. This assumption seems well hidden in the paper on line 163. Instead it should be stated from the beginning in order not to confuse the reader.  ** Maybe a word or two could be spend on the sample complexity of the procedure in the paper. It seems that we need very many samples, but how many? Do we know how does their number and the training time scale with n and d?   ** The analysis of the dynamics that maps weights on particules and described then the dynamics via an analog of the Dean's equation (relatively well known in statistical physics) is interesting and will be particularly appealing to physics audience.   ** The work of F. Bach "Breaking the curse of dimensionality with convex neural networks." seems to be related to the present paper, especially concerning the result about convexity of learning when the hidden layer is large. However, this relation (and novelty wrt that work) is not discussed in the present paper.  ** I found that the example of the p-spin model on which the authors demonstrate their results in not very well motivated. I see that it is a complicated function to fit, but does learning of this function arise in some applications?  ** The plot in Fig. 1 should be better explained. In the first pannel the authors plot 1/n and 1/n^2, but it is not clear why the 1/n is a better fit than the other. The data seems rather noisy and only about one decade is plotted. Should this be considered as a convincing evidence for the derived scaling? Page 8 refers to Fig. 5, should this be Fig 1?  ADDED AFTER FEEDBACK: My comments n. 3,4,5 were well taken into account in the rebuttal.   As for n. 1: Scaling in n is still not a practical guideline since usually one deals with a system of a given size and one does not have any access to scaling. Thus I would recommend the claims about practical guidelines to be tuned down. This paper is far away from any *practical guidelines*.  As for n. 2: This comment about SGD versus online was ignored in the rebuttal, while I think this assumption is strong and should be stressed.   As for n. 6 & 7: The rebuttal repeats what is already written in the paper. To me the interest of this particular example as well as the presentation of the numerical results remains obscure.     Overall while the presentation of this paper is far from ideal, yet the approach is interesting as well as the theoretical results. The above drawbacks prevent me from raising the score to more than 5, but I decided to raise to 5.         