The authors study the problem of quantizing recurrent neural networks. While extreme low bit quantization (2 bits quantization) has achieved strong results for CNN, so far, such quantization performed poorly for recurrent neural network. The goal of this paper is thus to identify the reason for this observation, and to propose extreme quantization scheme better suited for RNNs. First, the authors compare different weight quantization: 2-bits uniform quantization, thresholded ternary quantization (TTQ) and Bernoulli ternary quantization (BTQ). This comparison is performed using a RNN trained on Penn TreeBank. The authors also report the distribution of the full precision and quantized weights of the network, as well as activations. Based on these experiments, they conclude that different quantization scheme should be used for weights and activation, as they have different distribution (gaussian for the weights, bimodal for the activation). Additionally, they propose to a "slop factor" to make the activation distribution more peaky, and thus further reduce the quantization error. They finally compare these approach, improving the perplexity on PTB from 126 to 110 (for ternary quantized RNN), while a full precision model gets 97. They also report results on wikitext-2 and text8, without comparison to previous quantization techniques.  I liked the way that this paper is organized, starting from empirical observations to motivate the technical choices of the method. The problem studied in this paper is well motivated, and the paper is well written and easy to follow. My main concern regarding the writing of the paper is Figure 1: why is the memory used by activations so large, compared to the weights of the model? During inference, it is often not necessary to store all the activation, and since the motivation is to run the models on small devices, it probably does not make sense to use minibatch. Could the authors give more details about how these numbers were obtained? Regarding the training of quantized networks, how are the quantities for uniform and threshold quantizations (e.g. max, mean values) computed? Are they updated after each gradient step? Similarly, in the experiments reported in Table 2, is the slop factor applied at train time or only at test time?  This paper is technically sound, and the proposed method is simple. However, I feel that the technical contributions are a bit incremental, as they are a fairly straightforward combination of existing techniques (threshold/Bernoulli quantization, and slop factor). For this reason, I believe that this paper is borderline for acceptance to the NIPS conference.