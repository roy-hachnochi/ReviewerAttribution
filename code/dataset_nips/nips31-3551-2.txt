Thie paper proposes a method, for detecting abnormal examples including OOD and adversarial ones. The authors propose to calculate the mean and covariance matrix of the input representations using the representations from the penultimate layer of the model. Using the above representations the authors can calculate the Mahalanobis distance as a scoring function. In addition, the authors demonstrated their approach on the Incremental Learning task, where they add new labels as outputs for the model.  Overall this study is well written and the experiments are adequate, however, there are a few concerns; Do the authors evaluate their method on other distance metrics for OOD detection such as Euclidian distance? if so, do the authors also combine the representations from all the different layers and train an LR model for that?  In general, I'm wondering if the results are due to the Mahalanobis distance/due to the use of the input representation/to the due to the use of an ensemble of representations/due to the use of LR classifier. The authors mentioned they train the LR model using adversarial examples as OOD samples ("we also apply another validation strategy where adversarial samples are used as OOD samples"). From which dataset did you get the adv examples? From the in-distribution examples?  Moreover, it is not clear to me how did the authors generate and calc the results for the "Validation on adversarial samples" part in Table 2. What model did they use to generate these examples? It seems like the results for the baseline are not changing and for ODIN they stay the same for many of the cases. The authors mentioned, "we measure the detection performance when we train ResNet by varying the number of training data and assigning random label to training data on CIFAR-10 dataset." This is not exactly a fair comparison between the proposed method to the others since the other two methods are based on the softmax prediction. In cases, where we do not have enough training data, the classifiers will probably perform poorly and then the confidence won't be as good either. Lastly, the results are surprisingly good and improve recent SOTA by a huge gap. I would expect the authors to provide some explanation for such an improvement besides visualizing of the outputs using t-sne (which is kind of obvious).   Minor comments:  Line 171: "In the case, we expect" -> "In that case..." Line 267: "we keep 5 exemplars" -> "we keep 5 exempels" Line 227: "... and assigning random label to training data on CIFAR-10 dataset." - > "..and assigning random labels to training data on CIFAR-10 dataset."  ---- RESPONSE ---- I would like to thanks the authors for putting in the effort in their response.  The authors made some things clearer for me in their response, but I still think they could do a better job performing controlled experiments and properly evaluate the different components of their method. Esspatioaly after gaining such massive inprovment over previous SOTA. 