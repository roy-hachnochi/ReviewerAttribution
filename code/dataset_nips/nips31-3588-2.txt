This paper considers the distributed multi-player bandits problem without assuming any communication between the players. It gives a distributed algorithm and proves that it attains poly-logarithmic regret.  - The theoretical result holds only asymptotically and even there it is not clear that it is optimal.  - On the practical side, the algorithm is complicated, introduces additional hyper-parameters without any intuition on how to set them. The lack of systematic experiments does not convince the reader about the usefulness of this approach.  1. Is it possible to extend this theory to the case when K < N.  2. The statement of the main theorem is not clear. Please quantify what you mean by small enough \epsilon and large enough c_2.  3. The algorithm is of the epsilon-greedy style where we do some initial exploration to obtain good estimates of the rewards, which is not optimal in the standard MAB case.  Is the log^2(T) regret bound tight for this particular problem? 4. It seems that it is not possible to extend this theory to the case where the rewards are Bernoulli since it is difficult to disambiguate between collisions and zero rewards. Please clarify this.  5. For the GoT phase, please explain Lemma 3 better and make it self-contained. Also, please better connect the algorithm to the descriptions in the theorems.  6. Lemma 5 is a statement of large enough k. This is quite dissatisfying. Is there a way to bound this and say that after this phase, the regret scales as \log^2(T).  7. The algorithm introduces additional hyper-parameters c1, c2, c3. How do you set these in the experiments and what are guidelines for doing so? 8. Is it possible to prove something stronger assuming some minimal amount of  communication?  *** After Rebuttal  *** I have gone through the other reviews and the author response. And my opinion remains unchanged. 