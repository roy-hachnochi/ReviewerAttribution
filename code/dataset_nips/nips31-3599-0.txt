This paper proposes an algorithm to solve extreme multi-class classification problems using Error Correcting Output Coding. During training, the algorithm simply learns l (logK) independent binary classifiers. Main contribution of the paper is in the inference algorithm. It reduces the costly loss based decoding framework for ECOC to that of finding shortest path on a weighted trellis graph.  The paper is well written and easy to understand.  The proposed algorithm is quite interesting and novel. However, I have a few reservations -  1) Authors should have also compared their algorithm against a recently(WWW2018) proposed extreme classification algorithm called Parabel [1]. Parabel gives same accuracy as one-vs-rest(OVR) but is orders of magnitude faster than OVR both during training and prediction. Moreover, its model size is also much smaller as compared to FastXML. 2) As a baseline comparison it might be better to include the results with Hamming Decoding as well (keeping the training procedure same as LTLS) 3) As shown in DiSMEC paper[2] the model size of OVR can be significantly reduced by simple hard thresholding. So, authors should have reported the model size of OVR after hard thresholding. Often this model size comes out to be much less than FastXML(ref. [1][2]) and hence might beat the proposed algorithm in terms of model size. Simply dismissing the comparison with methods like DiSMEC and PPDSparse by arguing that “line of research is orthogonal to the ECOC approach, which can also employ sparse models to reduce the model size ” does not seem fair, particularly because all the datasets considered in the paper have hight dimensional sparse bag-of-words features. 4) To improve readability of the paper, instead of just providing the comparison graphs, it might be better if results are also reported in a table.  5) In Accuracy vs prediction time/model size graphs for fastXML just a single point is plotted. For fastXML prediction time/model size can be easily varied by changing the number of trees. So, authors should have presented the entire curve as is done for W-LTLS 6) It might be useful to include some discussion on training time of W-LTLS. I am assuming it would be orders of magnitude faster to train as compared to OVR which gives the best prediction accuracy.   [1]Y. Prabhu, A. Kag, S. Harsola, R. Agrawal and M. Varma. Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising. In Proceedings of the ACM International World Wide Web Conference, Lyon, France, April 2018.  [2]R. Babbar, and B. Schölkopf, DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification in WSDM, 2017