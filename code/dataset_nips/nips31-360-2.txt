Quality: The work focuses on feature correlation effect of dropout in deep neural networks (DNN).The authors propose a method to reduce the increasing feature correlation effect that occurs as one tries to increase the signal-to-noise ratio (SNR) of representations. The propose feature decorrelation techniques exploit the batch normalization which effectively controls the tendency of increasing feature magnitude. The work seems to connect well with the existing literature and provide a effective way of improving the performance of multiplicative noise in image classification tasks.  Clarity: The objective of the problem and the theoretical results by and large have been presented in a clear way. However, I didn't quite understand the unified view in section 3.2 that presents the multiple variants of the FDMN-0, FDMN-1 etc. I agree that the authors presented well in the previous section how FDMN could be used to effectively reduce the feature correlation effect using batch normalization. But the presentation of the different variants of FDMN was not quite clear to me. More specifically, the equation representing FDMN-0 in (15) and how the noise components are generated in FDMN -2 in equation (19) may need some more remarks or discussions.  Novelty: I feel the work has potential and the authors present a simple but effective approach of reducing the feature correlation in dropout. Certainly, it's a novel contribution to the existing literature of DNN. However, I feel the proposed technique could have been presented with a bit more clarity to show how it is more effective than the existing regularization techniques in DNN.  Significance: This area of work is significant to the literature discussing regularization in DNNs. I have my reservation with the multiple variants of FDMN. I am not entirely convinced how the noise component is generated in those different versions. Hence, I can't judge the actual importance of the current techniques in comparison to the existing regularization methods.  ---------------------------------------------------------------------------------------  I read the author response. I am happy with the author feedback for my queries. I think they have done well enough in justifying the motivations behind different variants of FDMN (which was my primary concern). Further, the comparison with shake-shake regularizer has been explained well therein. Overall, I am happy with the detailed response of the author(s).