This paper develops chaining methods for the mutual-information-based generalization bounds in Russo and Zou'15. For an increasing sequence of partitions of the metric space, the paper can provide upper bounds on the expectation of loss function at algorithmic output point, using the sum of mutual information terms along a chain of hierarchical partition, as in classical chaining upper bounds in empirical process theory.  It is very interesting to see the mutual information bound working in a multi-resolution way under discretization, and being combined with classical chaining techniques. However, this paper is perhaps not strong enough for NIPS:  1. The techniques are standard and simply adapting the classical chaining arguments to this new setting. It follows standard framework in chaining proofs, and applies the Donsker-Varadhan's variational form of KL divergence that has been used in e.g. Xu and Raginsky'17.  2. If the authors are going to convince readers about the usefulness of the discretization method and hierarchical partitioning, they should at least provide some examples where mutual information type bounds are really useful, and see if they get improved. The example provided in this paper, however, is far from satisfactory and actually very artificial. It's basically adding some atom to mess up the original mutual information bound without discretization, and showing that the chaining still works. It provides very little intuition about what we can get by chaining mutual information.  Detailed comments: 1. The authors should have noted existing works on PAC-Bayes chaining results (Audibert and Bousquet, JMLR 2007, Combining PAC-Bayesian and Generic Chaining Bounds). Though the mutual information bounds are different from PAC-Bayes, they are derived from similar techniques. 2. The statement about connection to Zhang et al., (2017) and generalization error of SGD is vacuous and deviates from the theme of this paper. I suggest the authors to remove this part.