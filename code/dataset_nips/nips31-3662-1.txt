The paper presents the dillema that current implicit models are continuous push-forwards of single mode distributions with connected support, thus by the mean value theorem the support of the generated data distribution is connected as well. While the universal approximation theorem allows these kinds of distribution to be approximated arbitrarily well in a weak (e.g. Wasserstein) sense, this is unlikely given the capacity of the models, and the training procedure (namely their inductive bias).   The authors study this problem in a few toy and large scale experiments, and propose an alternative where the generated distribution is a particular learned mixture of implicit models. They add a variational regularization term so that different generators learn different manifolds (approximately they increase the mutual information between the sample and the id of the generator, so knowing which generator is maximally informative of the sample). Furthermore, since having a different number of mixture components than real manifolds or a nonuniform weighting of the components in the real manifold can lead to problems well elaborated in the text, they explore a simple alternative to learn a good prior distribution over the mixture components.  There are some carefully designed experiments (and even metrics) to study the problem, as opposed to just showcasing performance. I find this sufficient for NIPS.  The paper is extremely well written, and easy to read. My only concern is that some smaller claims are not validated by experiments, citations or theorems, and they should be fixed or removed. They are not central to the paper nonetheless, thus my good score:  - 83-84: 'One should push the generator to learn a higher frequency function, the learning of which becomes more and more unstable' Figure 2 showcases that current methods fail at learning these kinds of data distributions well, not that the training becomes unstable, If the authors want to claim that training of high frequency functions becomes unstable, they should provide an experiment or a theorem to back up that claim.  - 110: "... is more robust to bad initialization": why? Increasing the number of generators increases the probability of a bad initialization in one of the components of the mixture, and there's no experiment showing robustness to a bad initialization of one or some of the mixture components.  - 133: 'leaving the loss intact': the mutual information term actually changes the loss, and potential problems are reflected in 143-145 (before the prior is learned). The regularization in the MI component and the loss for the prior are thus part of the generator's loss, and then the argument in 131-133 doesn't apply.   Minor things: - Two points in line 11. - 87: This is commonly referred to as 'mode dropping', not 'mode collapse'. Mode collapse is usually referred to the case where all or almost all the samples look almost exactly the same (e.g. instead of generating only cats in a cat and dog dataset, generating only the same picture of one cat or one cat and one dog). - Question: is the distance in 183-184 in pixel space? If so, why is this a meaningful metric? - 160: typo in corss -> cross