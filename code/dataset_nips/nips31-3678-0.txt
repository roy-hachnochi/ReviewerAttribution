The problem of the study is out-of-distribution detection; we would like to detect samples that do not belong to the train distribution. To solve this problem, the authors propose reframing the prediction problem as a regression in which the sparse one-hot encoding of the labels in the prediction space is replaced by word embeddings. The presented method also has a flavour of ensemble methods in the sense that a base feature representation (Resnet for image prediction, Lenet for speech recognition) is used and then the feature branches off to multiple fully connected networks each of which generates an embedding vector as the prediction. Each fully connected network is responsible for generating vectors in a specific embedding space. A total of five embedding spaces are used (Skip-gram, GloVe, and FastText trained on independent text datasets). To detect out-of-distribution samples, they learn a lowerbound on the sum of the squared norms of the embedding predictions. If the sum falls below a threshold \alpha they classify it as an outlier. They provide a strong evaluation on the out-of-distribution problem using the literature measures and evaluations on several datasets comparing against the previous state-of-the-art (ODIN, ICLR 2018) and Deep Ensembles (NIPS, 2017). However, in the adversarial examples, they only provide a broad assessment and preliminary results within a limited testing environment. The authors also provide empirical results on the behaviour of wrongly classified examples within their pipeline, which indicates there may be a way to characterize uncertainty in the prediction in a reliable way (left for future work).  Quality. The submission is technically sound, well-motivated and well-connected to the previous work. There are a few things that could be improved. i) For evaluation of ODIN, the authors fixed T=1000 and only find the optimal epsilon. Technically, they must grid search over all the temperatures (1, 2, 5, 10, 20, 50, 100, 200, 500, 1000) in the paper as well. ii) For evaluation of ODIN and DeepEnsemble, the authors compare against the network architectures that were used in the original papers, while the presented method is relying on Resnet-18 and Resnet-34. Both of the previous works do not strictly depend on a specific network architecture, therefore, their performance could be assessed using the same architectures as the presented method. While this difference in the base network seems insignificant, it is an important one for the evaluation since the topology of the network dictates different inductive biases and therefore different behaviour on the out-of-distribution samples. iii) For the adversarial examples, the authors extract adversarial examples from a VGG-19 network, while the base network architectures under study are DenseNet-121. Again, these two architectures are so drastically different in terms of the inductive biases that the performance of the networks under study will not provide any useful information for robustness against adversarial examples. However, the main selling point of the paper is around out-of-distribution detection and I'm not particularly worried about these experiments.  I would be happy to increase my rating if the authors address (i) and (ii) properly.  Clarity. The paper is well-written and easy to follow. However, the Related Work does not do justice to the literature. Novelty detection, open set recognition, prediction with abstention are the other keywords under which this problem is studied.   Originality. As far as I'm familiar with the literature, the idea is novel and interesting. The connection to the previous work is well-justified and the work is a nice addition to the literature on this problem.  Significance. The work is a clear improvement over the previous state-of-the-art (factoring the two issues that I mentioned earlier). I think the future work based on this idea could be a fruitful direction of research. ============================ Update after rebuttal. The authors adequately addressed my concerns. I have increased my overall score.