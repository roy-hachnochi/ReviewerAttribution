Imitation learning in robotics faces several challenges, including the ill-posedness of IRL, as well as sample efficiency.  Building on Generative Adversarial Imitation Learning (GAIL), this paper addresses these issues by placing GAIL in a Bayesian framework (BGAIL).  Like GAIL, BGAIL does not require an MDP solver in the inner loop, and can learn from a significantly smaller number of samples than GAIL due to the superiority of using an expressive posterior over discriminators, rather than point estimate.  Strengths:  + Imitation learning generalizes much better than behavioral cloning, but is currently under-utilized in robotics due to data efficiency issues.  Thus, this is a highly relevant problem.  + The Bayesian setting is well-motivated, both from a theoretical standpoint and evidence from prior work such as Bayesian GAN.  The theory for implementing this is not just gluing together BIRL and GAIL, but is quite technical and seeming sound.  + It is satisfying to see that BGAIL was derived in such a way that GAIL is a special-case in which point estimates are used, rather than a different algorithm entirely.  + The experiments are high-quality.  It is nice to see that the paper compares BGAIL not only to GAIL, but to a more highly tuned version of GAIL that makes the comparison more fair.  Although BGAILâ€™s asymptotic improvement over GAIL is very minor, the efficiency gains in the low-data setting (Fig 1) are quite nice and make this a worthwhile improvement that may make imitation learning on robots significantly more tractable.    Weaknesses:   - It would have been nice to see experiments that showed the effect of using different numbers of particles.  It is currently unclear how important this choice is.  - The writing is occasionally uneven and could use additional editing (more from a language-usage and type perspective than a technical perspective).  Also, in the equations, the notation is exact, but gets quite unwieldy at times. It may be worth considering dropping some of the sub/superscripts for convenience where possible (and noting that you are doing so).   - This is a minor detail, but in section 2.2 when discussing the ill-posedness of IRL, the paper focuses on degenerate solutions.  But the main ill-posedness problem comes more from the infinite number of reward functions that can describe identical policies / equally explain demonstrations.  Overall, the author response addressed several of the concerns of myself and the other reviews, reinforcing my positive outlook on this work. 