This paper deals with a safe reinforcement learning problem, where constraints are defined as an expected cost over finite-length trajectories, the goal being to maximize an expected gain (also over finite-length trajectories, so more general than cumulative rewards), while the constraint is greater than a given lower bound. To do so, the authors extend the cross-entropy method applied to classic RL. If sampled policies are feasible (satisfy the constraints), they are sorted according to their gain (as in classic cross-entropy). If not, how much the constraints are violated is also taken into account (by selecting the closer to feasible ones). The proposed contribution is supported by a theoretical analysis and experimental results.  Overall, the paper is well written, deals with an important subject, and provides interesting theoretical and experimental results. However, it could be improved with some clarifications, regarding the approach, as well as theoretical and empirical results. Also, the scalability of the proposed approach could be discussed (a possible work about this is “Evolution strategies as a scalable alternative to reinforcement learning”).  Many (usually small) changes could help clarify the proposed approach, for example: * It would be better to say already in l.110 that J>0 (it is said after, but can be unnoticed, and Eq.1 makes no sense with negative J). * In Alg.1, what are N_l (n_l?), lambda_l? Also, in "ensure" part, there should be NEF and Pi * sec. 4.2 could me made clearer, notably by clarifying how one goes from distributions g to the etas, and especially how lines 11 and 12 in Alg.1 are derived (it is not that clear, even with the appendix). It could maybe also help to instantiate it (say in the appendix), for example for the Gaussian with diagonal matrix case considered practically. * l.230, "by definition of eta", this is really not clear. The definition is eta(t) = m(v(t)), but it is never clearly stated in the paper (can be infered from alg.1, or line 200)  * In the main text (eg l.115), a policy is said to be feasible if H_Z(pi)>d. In Fig. 1, feasible policies are below the dashed lines, that is H_Z(pi)<d. This is very misleading.  Regarding the theoretical analysis: * convergence is shown, but it would be good to give more intuition to what it converges to. As I understand, this is covered by th. 4.2 and 4.3, but more hints on the quality of the obtained solution (if anything can be said) would be welcome. * It is nice to provide complete proofs, but they look very similar to the ones in [11], this being never clearly stated in the paper ([11] being not even cited in the appendix). Please clarify, explain in what they differ, what are the additional difficulties for considering constraints. It is nice to provide complete proofs, even if close to [11], but what comes from [11] should be clearly stated.  Regarding the empirical results: * the problem considered is a kind of variant of the puddle world problem, that can be well solved with classic RL. This can seem somehow against the considered benchmark, it could be disambiguated (eg, the different considered setting might not be adapted to the puddle world) * an important thing not said is how is evaluated the performance of CCE. For PPO, it is (implicitly) clear, as only one policy is learned. But for CCE, 40 policies are sampled at each iteration. Is the reported performance the one of the best policy? Of the quantile kept? Of all sampled policies? This is an important question, as it is said that for other approaches "feasibility is rarely guaranteed in practice" (l.89). So does Fig. 1 reflect the behavior of all sampled policies, or just the (few) best one(s)? * it would have been interesting to keep the same policy spaces for all algorithms (it is a plus of CCE, not being constrained to stochastic policies, but it could be a factor in the empirical results) * Fig. 1.c and 1.d are not consistent with table 1 (switched value of d_i, c:-5/3:-.5, d:-.5/4:-5) * the x-axis inf Fig.1 is not clear. Is it the total number of sampled trajectories consummed? For example, 1 iteration of CCE (40 policies, each 10 runs) corresponds to 12000 transitions/400 trajectories fed to TRPO? * some empirical (important) details are missing: alpha, lambda, etc.  =======  Thanks for the rebuttal. I think this paper can be a good contribution to NIPS.