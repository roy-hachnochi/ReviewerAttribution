This paper proposed a novel framework for multiagent imitation learning for general Markov games. The main contribution the proposal of a general multiagent Inverse RL framework, MAGAIL that bridges the gap between existing multiagent RL methods and implicit generative models, such as GANs. MAGAIL is rigorous extension of a previous work on generalized single agent inverse reinforcement learning, GAIL. MAGAIL allows incorporation of prior knowledge into the discriminators, including the presence of cooperative or competitive agents. The paper also proposed a novel multiagent natural policy gradient algorithm that addresses the high variance issue. MAGAIL is evaluated on both cooperative and competitive tasks from the Particle environments, and a cooperative control task where only sub-optimal expert demonstrations are available. Three variants of MAGIAL (centralized, decentralized, zero-sum) are evaluated and outperform several baselines, including behavior cloning and GAIL IRL baseline that operates on each agent separately.  This paper is well-written and reasonably accessible (given the mathematical rigor), and the topic is of importance to the machine learning community (both multiagent systems and reinforcement learning). The contributions are nicely laid out, and the structure of the paper leads to a good flow for the reader. The contributions are clear. The experiments are convincing.  Minor points: The advantage function in equation (11) is wrongly specified. There is a minus sign missing.  In the definition of occupancy measures, it might be better to change T to P to avoid the confusion of transition function. 