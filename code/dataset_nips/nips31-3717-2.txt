This paper proposes a general multi-agent IRL framework or multi-agent imitation learning for general Markov games, which generalizes Generative Adversarial Imitation Learning to multi-agent cases. The learning procedure corresponds to a two-player game between a generator and a discriminator. The generator controls the policies of agents in a distributive fashion, and the discriminator contains a classifier for each agent that is trained to distinguish that agentâ€™s behavior from that of the corresponding expert. Experimental results on a particle environment and a cooperative control task demonstrate that it is able to imitate complex behaviors in high-dimensional environments with both cooperative and adversarial interactions.  Unlike most existing work in multi-agent imitation learning where the agents have very specific reward structures, the proposed framework can handle both cooperative and competitive scenarios. However, the most difficult part is the case when the agents are self-interested and the payoffs are general sum. In this case, there may be multiple Nash equilibriums. When the agents learn in a decentralized manner, they may not be converging and jump between equilibriums. It is not clear how such cases are handled in the proposed method.  Update: Thanks for the rebuttal. I have read it carefully.