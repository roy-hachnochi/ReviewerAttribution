Update after rebuttal:  I think the rebuttal is fair.  It is very reassuring that pseudocode will be provided to the readers.  I therefore keep my decision unchanged.   Original review:  In the paper "Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo" the author(s) consider the problem of inference for deep gaussian processes (DGPs).   Given the large number of layers and width of each layer, direct inference is computaitonal infeasible, which has motivated numerous variational inference methods to approximate the posterior distribution, for example doubly stochastic variational inference (DSVI) of [Salimbeni and Deisenroth, 2017]   The authors argue that these unimodal approximations are typically poor given the multimodal and non-Gaussian nature of the posterior.  To demonstrate these characteristics, the author(s) using an MCMC approach (specifically SGHMC of [Chen et al, 2016]) to sample from the posterior, thus providing evidence of the non-Gaussianity.  Following this approach, they demonstrate the multimodality of a DGP posterior arising   The disadvantage of the SGHMC algorithm is the large number of hyper-parameters to be tuned.  The author(s) adopt a BNN approach and furthermore a Monte Carlo Expectation Maximisation approach (Wei, Tanner, 1990).  The authors introduce a variant where instead of choosing a new set of samples each M step, the authors recycle the previous, replacing the oldest sample with a newly generated one (calling it moving window MCEM).      To demonstrate their algorithm, they compare a number of non-linear regression algorithms on a set of standard UCI benchmark datasets, MNIST (using robust-max likelihood for classification), and Harvard Clean Energy Project dataset.  I first qualify this review by saying that I am not an expert in deep gaussian processes, or methods for variational inference of DGPs, and so I cannot comment on the relative merits of these with any confidence.  It is not surprising that VI are not able to capture the multimodality present in DGP posteriors, and it is good that this important feature has been identified.  Certainly, if the DGP is a subcomponent of a larger Bayesian model, then failure to capture all modes of the posterior could be disastrous.  Therefore, I definitely see the value of this proposed approach, and so I commend the authors for producing a well presented case for this.     Two major concerns with this work relate to: (1) the lack of acknowledgement of similar works which apply MCMC for inference for DGPs , (2) the lack of clarity on certain aspects of the implementation of the SGHMC and the MW-MCEM method.   I detail the issues in the following:  1. The paper [Hoffman, Matthew D. "Learning deep latent Gaussian models with Markov chain Monte Carlo." International Conference on Machine Learning. 2017.]  also presents a stochastic gradient approach based on HMC.  Although the context is slightly different, I feel it is similar enough that this should have been heavily referenced in this work, and formed the basis of the numerical comparison.   Also related, to a lesser extent are [Han, Tian, et al. "Alternating Back-Propagation for Generator Network." AAAI. Vol. 3. 2017] and [Wolf, Christopher, Maximilian Karl, and Patrick van der Smagt. "Variational Inference with Hamiltonian Monte Carlo.", 2016].   2 .  Another major issue is the lack of details of the MCMC algorithm.  Mainly it is absolutely not clear what form the subsampling takes.   The author(s) refer to a "reparametrization trick to the layer outputs" citing Chen at al. 2014 without explaining further.  Firstly, I'm not sure in what context [Chen et al 2014] use the parametrization trick.  I usually refer to   [D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. International Conference on Machine Learning, 2014.]  and   [D. P. Kingma, T. Salimans, and M. Welling. Variational Dropout and the Local Reparameterization Trick. 2015.]  In particular, given that exact sampling is possible with cubic cost wrt N, it is important to understand the relative cost of iterating the SGHMC algorithm,  i.e. the computational cost associated with producing a stochastic gradient as a function of N, minibatch size, etc.  This has not been addressed in the paper.  3.  The author(s) are correct that choosing m in the MCEM is a challenge.  Too small risks overfitting of parameters, too large is expensive.  I feel that the author(s) might not be aware of the well established literature on this problem.  Indeed, this problem has been addressed in the statistics literature:  One classical paper (Levine, Casella 2001) addresses the problem of recycling samples for the M step in some detail,  proposing a further importance sampling step which would appear to be highly relevant to this situation.    