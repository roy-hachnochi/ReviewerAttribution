This paper presents a method for online distillation of deep neural networks. Online distillation methods avoid the complexity of distillation with two-stage training using a larger teacher network. This is done by training multiple base models in parallel and setting a loss function that moves the output of each individual base model toward the aggregate/consensus output.  The main idea of this work compared to other online distillation techniques is that the proposed ensemble, base networks share a part of the architecture, and a gating mechanism is used to combine the outputs into aggregate logits. This technique keeps the aggregate network small, making it possible to train the full aggregate model in one stage and without a need for synchronization (compared to [16]).  The proposed method is empirically tested against both offline two-stage (KD) and online (DML) distillation methods. The results, somewhat surprisingly, show that the proposed method outperforms the offline method in test accuracy. The required computation is also considerably smaller than the KD method. The paper also shows that obtained final network with the proposed method is in a wider local optima by showing that it is less sensitive to random perturbation of the parameters. Other experiments in the paper show how online distillation and sharing of the layers can act as regularizers when we use the full ensemble at test time.  Quality and clarity: the paper is mainly an empirical analysis of a new distillation method. There is no concrete theoretical justification for the proposed method. The paper lightly touches on the idea that the proposed aggregate network architecture is benefiting from regularization effects of sharing network layers and the gating mechanism. Part of the paper are hard to read with a few broken sentences in different sections.   Originality and significance: The idea of online distillation is not new (see [15][16]), but the proposal to share layers when co-distilling base networks is interesting and to the best of my knowledge original. Although the idea is marginal, the result seem interesting and significant.  Question and comments: How do you decide which layers to share in the aggregate model? Why not take the base network with index = argmax g_i? The choice of scaling the KL loss by T^2 is not well justified. This could be a hyper parameter that you can optimize for with cross validation.  Notes: Line 17: necessary -> necessity (maybe just revise the sentence) Line 54-56: Broken sentence Line 70-71: Broken sentence Line 116: While sharing most layers â€¦ Line 240: Comparing thetas is not meaningful. Maybe revise the sentence.  I have read the authors' feedback.