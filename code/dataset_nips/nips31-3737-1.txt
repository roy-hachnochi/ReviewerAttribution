       Summary:          This paper introduces an AE into NAM to produce the latent code for target image, thus avoids the complex optimization procedure to find the latent code involved in NAM. Then the authors extend the AE-NAM to VAE-NAM, to provide one-to-many ability to the inference procedure. The results reveals the performance of VAE-NAM is much better than NAM and various GAN methods.           Comments:   1.     Strengths: a)      The paper gives quantitative result for NAM-family methods, which make up the shortcomings of the original NAM paper in ICLR workshop 2018; b)      The AE trick, although not novel,  did solve the problem in original NAM, and the performance is much better in the MNIST->SVHN case. 2.     Weakness: a)      There is no quantitative comparison  between AE-NAM and VAE-NAM. It is necessary to answer that when one-to-many is not concerned, which one, AE-NAM or VAE-NAM should be used. In another word, the superior of VAE-NAM comes from V or AE? b)      It contains full of little mistakes or missing references. For example:                           i.          Line 31, and Line 35, mix use 1) and ii);                          ii.          Line 51, 54, 300, 301, missing reference;                         iii.          Equation 2): use E() but the context is discussing C();                         iv.          Line 174: what is mu_y? missing \ in latex?                          v.          Line 104: gramma error? 3.     Overall evaluation: a)       I think the quality of this paper is marginally above the acceptance line. But there are too many small errors in the paper