The authors address the problem of unsupervised mapping across domains. They summarized two constraints that are usually applied in related work and their drawbacks: 1) Adversarial domain confusion, which is a saddle point problem and is difficult to train and 2) Circularity, which forces the transformation function to become 1-1. They then argue NAM can solve the problem without the above two constraints with some weakness. 1) number of codes depends on number of samples 2) inference by optimization 3) multiple solutions are obtained by multiple restart. The proposed method use an variational autoencoder in place of the directly founding code for each sample, an idea that is easy to understand and makes sense.  strength: VAE-NAM is easy to understand and easy to implement. This framework fits any generative model. With this small modification, the 1-1 mapping constraint can be addressed.  weakness: The impact is marginal. The major contribution of this paper lies on a tweak of the encoder function, i.e., z_y = C(y), although the final improvement is significant.  I do not see the strength from the pictures (in Figure 1) of VAE-NAM in the qualitative research. They just look similar to others. In the the quantitative evaluation, I don't see why AE-NAM is not compared? How does AE-NAM work?  The authors mentioned that there is a negative results in SVHN2MNIST line 256, but it is not clear what this refers too. More elaboration is needed.  quality: There are some typos. Most annoying is the exchange of C() and E() in the formula.  clarity: This paper is reasonably clear  originality: This paper is original. The solution makes sense, solves the problems, and is easier to train.  significance: Good