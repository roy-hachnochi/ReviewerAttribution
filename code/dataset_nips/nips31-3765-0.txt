This paper considers the problem of textual network embedding. The authors adopt a diffusion map type method to incorporate both the graph structure information and the textual semantic information. The paper is clearly written and sophisticated with appropriate experiments following the demonstration of the methods. In both the multi-label classification task and the link prediction task, the proposed methods (with different variations) outperform the baselines in general. The choice of the datasets (Zhihu, Cora, and DBL) is also valid and representative. The parameter sensitivity analysis in the experiment section is also helpful and sometimes missing from similar works.  I found the tuning of weights for the four objectives at (9)-(12) interesting. For others working with combined losses, usually the weighs are not essential. But in this work, I think the weights have some concrete meanings since each objective represents a particular part of the similarity between the original and embedded spaces. What does the fine-tuned weights look like? Any intuition?  I have some technical questions for this work. Is there any intuition why the direct average of word embedding is better than Bi-LSTM embedding for most cases? Is there any particular reason why the authors include the concept of tensor in section 4.2? I am not able to find the necessity of doing so but it complicates the expression.  === After rebuttal === Thanks for the rebuttal. I think the authors have answered most of my questions. 