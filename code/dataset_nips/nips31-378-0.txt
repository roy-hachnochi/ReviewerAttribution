The paper proposes a compositional method to generate image captions. First, a classifier is trained to generate a list of none-phrases for an image. Then two none-phrases are connected by a connecting sequence to form a longer phrase. The algorithm stops when a long phrase is evaluated to be a complete caption. The proposed method achieves comparable SPICE score with state-of-the-art methods.  Pros: 1. The compositional paradigm is able to model the hierarchical structure of natural language. 2. The compositional procedure consists of parametric modular nets, instead of hand-crafted procedures.  Cons: 1. The authors claim that the sequential model cannot reflect the hierarchical structure of natural languages. However, sequential models are used to encode the phrases and determine whether a phrase is a complete caption. If a sequential model is unable to model the hierarchical structure of languages, how can it know whether a compositional phrase is a complete caption?  2. In Eq. (1), the left phrase and right phrase representation are added. How to determine the order of two phrases in the longer phrase. For example, how to decide it is "a dog playing a football" instead of "a football playing a dog".  3. In the second study of the generalization analysis, the none-phrase classifiers are trained with in-domain data. Are in-domain data used by other methods?  4. As building and clock occur frequently in the training set, is it possible the none-phase classifier misclassify a building to a clock? If so, will the proposed method have the same error illustrated in Figure 1? 