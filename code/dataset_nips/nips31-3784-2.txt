In this paper, the authors present a novel framework, Sparse Attentive Backtracking (SAB), which can be used to capture long term dependencies in sequential data as an alternative to Back Propagation Through Time (BPTT). The inspiration for the framework comes from the intuition of how humans do credit assignment â€” the relevant memories are retrieved based on the current stimulus and accredited based on their association with the stimulus, which is quite different from BPTT which rolls out the past states in reverse order.   The paper describes the biological motivation and details how certain memories (past hidden states) can be aggregated into a summary with weights based on their association with the current state. The paper is clearly written and the workings of the framework aptly exposed. There is a slight disconnect between the motivation which talks about attending to salient memories within a long horizon, and the proposal which limits to hand coded connections to a set of memories. However, even with this limitation, an LSTM augmented with SAB performs comparably to state of the art methods often outperforming them which hints at its utility and the potential for improved results with better curated memories. Furthermore, the authors talk about these limitations in venues for future work alleviating my main concern to some extent.  Overall, I think the paper makes a novel contribution which would be of interest to the wider community even though some more work on addressing the shortcomings of the presented approach would make for an even stronger submission.   -- After rebuttal -- Thanks for addressing the different concerns raised by the reviewers, and doing the extra work to argue the relevance of the proposed method. I maintain that this is a good paper which would be interesting to the scientific community. 