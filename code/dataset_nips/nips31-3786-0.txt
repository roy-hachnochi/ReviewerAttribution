This paper presents a method for finding approximate Nash equilibria in two player zero-sum games with imperfect information (using poker as the motivating example) using a depth-limited solving technique. Depth-limited techniques avoid the need to traverse an entire game tree by replacing nodes in the tree with an estimate of the expected value of their corresponding subtree (for given strategies). This approach doesn’t work in imperfect information games because optimal strategies can’t be reconstructed from values (this idea is clearly demonstrated in a simple rock-paper-scissors example in Section 2 of the paper). This paper presents a technique for avoiding this problem by building player 1’s strategy in a depth-limited sub-game with respect to a set of candidate player 2 strategies (and their corresponding values) that define how player 2 may play in the subgame below the depth limit. It addresses the lack of depth-limited solving in Libratus [Brown & Sandholm 2017] which necessitated a pre-computated strategy that was extremely computationally expensive and presents an alternative to DeepStack’s [Moravčík et al 2017] depth-limited solving technique.  # Quality  The key idea of this paper is that you can reconstruct player 1’s Nash strategy with depth-limited solving if at the leaves of a subgame you store player 1’s value for every pure player 2 strategy (as summarized by Proposition 1). This proposition doesn’t appear immediately useful: it assumes we already have player 1’s strategy and that the set of all pure strategies for player 2 grows at the same rate as the full search space, so it doesn’t seem helpful to use depth limiting if you need to store an exponentially large object as every node, but the authors find that it is sufficient to instead use only a constant number of well-chosen strategies with an approximation of player 1’s Nash strategy and still get strong performance.   Pros  - empirically they’re able to get very good performance with very constrained resources. This is the strongest support of the claim that a small number of strategies is sufficient  - there is potentially a large design space in choosing P2’s strategies. I suspect there will be a lot of future work in this area  Cons  - despite presenting two approaches for building the set of P2 strategies (bias and self-generating) the paper gives very little guidance on what constitutes a good set of strategies. I would expect performance to be very sensitive to your choice of strategies, but this isn’t evaluated. The implementation used in the experiments use the self generative approach for the first betting round and the biased approach thereafter. Why? What happens if you just used the self-generated approach or the biased approach?  - The computational restriction to a 4-core CPU and 16 GB of memory makes for a flashy abstract; but I really would have liked to see what happens with a less arbitrarily constrained model. Do improvements in performance saturate as you keep increasing the size of the set of P2 strategies? What does the DNN performance look like if you used the largest model that you could fit on the GPU it was trained on instead of limiting it for fast CPU performance? How does the quality of the blueprint strategy affect performance? All of these questions could easily have been answered without needing massive compute resources, so it seems weird to limit oneself to the resources of a MacBook Air.   # Clarity Overall I thought the paper was very clearly written and I enjoyed the simple examples shown in section 2 to describe the problems associated with depth limited search.   There were a couple of ambiguous / misleading / etc. statements that could be improved:  - lines 147 - 150: the claim around 10^100 strategies seems very misleading. Surely what matters is not the size of the set of the potential number of independent choices, but the coverage of the subset you choose? Unless you can prove something about how the number of independent strategies relates to the quality of the solution I would remove this line.  - line 166 “Thus \sigma_2’ certainly qualifies as an intelligent strategy to play” -  by that definition any arbitrary strategies is “intelligent” because it has expected value zero against a Nash strategy. I’d argue that’s just a property of a Nash equilibrium not about how intelligent the strategy is…   - line 358-359: I don’t think there’s a relationship between the computational complexity of computing a function (such as best response or equilibrium) and the difficulty in learning it (given the same amount of training examples)? Function approximation quality depends on the properties of the function (Lipschitz constants, etc.) rather than how expensive it is to collect the data points.   # Significance Despite my reservations about the empirical evaluation of this work, I do this that it is significant. I suspect a number of authors will explore the space of heuristics and learning algorithms for generating a useful set of P2 strategies. It is for this reason that I am recommending its acceptance. But I do think it would have been a far stronger paper if the implementation details were more explicitly evaluated.  Typos:  - line 190 - should be “\sigma_1^*” not “\sigma_2^*”  - line 118 - delete “so” in “so clearly P1 cannot do better”