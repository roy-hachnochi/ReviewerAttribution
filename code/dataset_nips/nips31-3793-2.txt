The authors introduce DS-PSRL for reinforcement learning in 1-dimensional continuously parametrized MDPs. The algorithm is computationally efficient because the number of policy switches is fewer than O(log T) (where the major computational cost of a policy switch is solving an MDP). The authors provide a Bayesian regret bound that characterizes how the algorithm scales with the smoothness of the parametrization and how quickly the posterior distribution concentrates. The experiments suggest that DS-PSRL competes favorably against TSDE and works well even in the case where the task is parametrized by multiple parameters. The paper is clearly written and provides both theoretical insights and strong theoretical results.  Post-Author-Response: I've read the response and I believe this is paper should be accepted. I view the scalar parametrization as a starting point that future work can extend. I would prefer that the authors compare to performance against multiple algorithms.