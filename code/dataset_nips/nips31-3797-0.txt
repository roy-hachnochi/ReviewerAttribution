  The main argument is that:  1. Sec 2: convergence and generalization in NNs are separate issues. Convergence is necessary for generalizing well, but if a network converges without normalization, BN does not add further improvement in generalization. 2. Sec. 3 & 4: convergence is governed by random initialization and gradient magnitudes (e.g. exploding gradient problem, the importance of condition number not growing exponentially w.r.t. depth). Interestingly, the distribution of gradient magnitudes between layer units in an unnormalized network has high variance (i.e. poor conditioning) but the variance of gradients *between* minibatch inputs is small, suggesting that the gradient is not depending on the inputs at all and has been overwhelmed by biases introduced by poor initialization. 3. Sec 3 & 4 experiments: BN fixes convergence problems for higher learning rate settings, by keeping the condition number roughly unitary. In contrast, unnormalized networks cannot train with large learning rates because gradients would scale exponentially.  4. Under the "low noise == sharp minima == poor generalization" hypothesis, using larger learning rates scales up the "SGD noise" (Eq 2), which leads to better generalization.   Strengths:   - The authors approach an important area of study in theoretical understanding of Deep Learning: why does batch normalization improve training times, convergence on training set, and generalization to test set? - Reproducibility of experiments via code included in supplemental material. - Empirical variance analysis of gradients w.r.t. intra-layer and intra-batch. - Showing that the unnormalized network has a low-rank gradients was intruiiging.  - Empirical rigour  Weaknesses: - My general reservation about this paper is that while it was helpful in clarifying my own understanding of BN, a lot of the conclusions are consistent with folk wisdom understanding of BN (e.g. well-conditioned optimization), and the experimental results were not particularly surprising.  Questions: - Taking Sharp Minima Hypothesis at face value, Eq 2 suggests that increasing gradient variance improves generalization. This is consistent with the theme that decreasing LR or decreasing minibatch size make generalization worse. Can you comment on how to reconcile this claim with the body of work in black-box optimization (REBAR, RELAX, VIMCO, Reinforcement Learning) suggesting that *reducing* variance of gradient estimation improves generalization & final performance?   - Let's suppose that higher SGD variance (eq 2) == better generalization. BN decreases intra-unit gradient variance (Fig 2, left) but increases intra-minibatch variance (Fig 4, right). When it is applied to a network that converges for some pair \alpha and B, it seems to generalize slightly worse (Fig 1, right). According to the explanations presented by this paper, this would imply that BN decreased M slightly. For what unnormalized architectures does the application of BN increase SGD variance, and for what unnormalized architectures does BN actually decrease SGD variance? (requiring LR to be increased to compensate?) How do inter-layer gradient variance and inter-minibatch gradient variance impact on generalization?  - For an unnormalized network, is it possible to converge AND generalize well by simply using a small learning rate with a small batch size? Does this perform comparably to using batch norm?  - While Section 4 was well-written, the argument that BN decreases exponential condition number is not new; this situation has been analyzed in the case of training deep networks using orthogonal weights (https://arxiv.org/pdf/1511.06464.pdf, https://arxiv.org/pdf/1806.05393.pdf), and the exploding / vanishing gradient problem (Hochreiter et al.).    On the subject of novelty, does this paper make a stronger claim than existing folk wisdom that BN makes optimization well-conditioned?   