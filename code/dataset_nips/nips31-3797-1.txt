In this submission, the authors undertake an empirical study of batch normalization, in service of providing a more solid foundation for why the technique works. The authors study a resnet trained on CIFAR-10, with and without batch norm (BN) to draw their conclusions. They first point out that BN enables training with faster learning rates, and then argue that this allows SGD to have a greater regularizing effect (via reference to the flat minima results). The authors then put forward a sequence of measurements that argue that without normalization, the optimization problem is poorly conditioned. This includes showing that: the gradient distribution is heavy tailed (Fig 2); the output at initialization is biased to a single class (and so are the gradients; Fig 4); the gradients in the convolutions spatially align at initialization, producing a strong input-independent term (Table 1); the gradients in the convolutions align across channels at initialization, producing low-rank structure (Fig 5). In each case, BN ameliorates the effect. These effects combine with a final appeal to random matrix theory that the condition number of the unnormalized network (ignoring nonlinearities) is almost certainly very poor in expectation, and that BN does something to help.   I like this paper: it's topical (given the Rahimi appeal), generally well-written (though there's some room for improvement), suggestive, and well explored. I don't think it's airtight, and there are a few errors scattered about that I think are fixable. Even if it doesn't turn out to be the full explanation for BN success, it's a step in the right direction, and likely to have a significant impact on the field -- both in terms of its content, and its general approach.  Some corrections or clarifications required: - L60: "BN has a lower training loss indicating overfitting". It has neither a lower train or test loss from what I can tell in Fig 1, and not sure why lower train => overfitting. - Fig 1: legends are hard to parse, put in actual learning rates in better order - some mistakes in Section 2.1 equations. L73: there should be an summation in the expectation. Eqn (2): should be a square in the denominator (||B||^2). The expression in L74 of the "noise of the gradient estimate" would be better expressed as the noise in the gradient step, since the gradient estimate itself won't have the factor of \alpha in it. - L115: I think this should refer to Fig 4, not Fig 3. But then Fig 4 is showing the gradients, not the activations. I found Fig 4 confusing as the x-axis says "class activation", but this is really "class". Given that the text talks about classes being favoured, it might be worthwhile to separately show the class activations (showing that one class is favoured), and the gradients (showing that these are imbalanced).  - One other point on Fig 4: it seems like an obvious solution is just to immediately adjust the class biases at initialization to equalize average class assignments. I think the authors aren't doing this because they're making the claim in the next section that this is a general phenomenon throughout the network (where it can't be fixed by adjusting biases). It would be worth pointing out that this is an illustration. - Table 1 legend, last line "We also see that...". I cannot see this trend within the data in the table. - L196: equation (7)? - L216: "Result Theorem 1"? - L218: "changing entry distribution"?