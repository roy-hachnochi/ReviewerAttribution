The paper tries to demystify the success of BatchNorm, which despite widespread use does not have a good explanation why or how it works. The manuscript separates into two somewhat unrelated parts: 1. The authors claim that most or all of the benefits of BN are mediated via enabling the use of a larger learning rate. This is backed up experimentally by showing BN with a small LR does not perform appreciably better than no Normalization.  2. The authors try empirically to identify the mechanism by which BN enables the larger learning rates.   The authors mostly succeed on arguing the first point, but fail to cite the work of Leslie Smith, who has done rather in depth work on why, when it comes to learning rate, larger is generally better, and how various forms of regularization such as L2 interact with it. Similarly, Smith and Le recently analyzed learning rate in a Bayesian framework. Note that this analysis contradicts the earlier statement from Keskar about "large batch sizes lead to sharp minima" which is cited here, so please include a more balanced discussion of previous work.  Further, this observation does not help to explain why existing alternatives to BN, such as LayerNorm, GroupNorm, InstanceNorm etc. don't work as well.  Investigating these alternatives is an active field given limitations of BN to deal with small batches, and being computationally expensive. Would methods like Batch Renormalization or GroupNorm be expected to work equally well if they enable and equally large LR? In my experience they work without changes to learning rate, so why don’t they?  In the second part, the authors go beyond the standard approach of analyzing convergence via Eigenvalues of the Jacobian (which don't scale beyond small toy examples), by showing that there is a data independent component dominating the gradient in the "no normalization" case. This is an intriguing finding that I think they authors did not study in enough depth.  First of all, it would be useful to study where this gradient direction is pointing us. E.g. aggregating and averaging over a few batches should give a clean estimate of this term, and it could be tested if it always points in the radial direction rather than along the hypersphere of constant radius. If we follow this direction, does it lead to a reduction in cost, or is it a very narrow minimum with regions of high cost lurking closely behind it? Since this is only a single dimension in a high dimensional space, it could be insightful to provide visualizations along this direction, compared to e.g. a random orthogonal direction.  This leads to the testable hypothesis, that gradients in the radial direction are large and constrain the learning rate, while gradients tangential to the hypersphere are much smaller.   Minor comments:  - 72, typo, "let define" should be "let us define" - line 75, equation 2: Please provide a bit more detail what linear algebra goes into deriving this equation. See the similar derivation in Smith and Le, "A Bayesian Perspective on Generalization and Stochastic Gradient Descent", section 5, which arrives at a linear scaling rule.  - 79, typo, "lead" should be "lead to" - line 105, worded a bit ambiguously, please make clear it's a single BN layer, not on top of  each residual block. - Table 1 is hard to parse, might be better as a bar plot - 175: Are you aware of the related work by Schönholz and Sohl-Dickstein, e.g. https://arxiv.org/abs/1611.01232  and https://arxiv.org/abs/1806.05393 about initalization in deep linear networks? The latter introduces the tf.ConvolutionDeltaOrthogonal, which is similar to the ideas here.  - 186 typo: "largest singular" missing "value"