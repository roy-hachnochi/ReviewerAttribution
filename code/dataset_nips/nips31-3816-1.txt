I have read the other reviews and the author rebuttal and am still in favor of accepting the paper. The rebuttal is impressively thorough, addressing all of my concerns rather decisively with new experimental evidence. ------------------ This paper proposes a new problem, interaction-free object removal, and a neural architecture for solving it. In the problem setup, the system is given an image and an object class label and must remove all instances of that class from the image. The system must thus learn to both localize the object, remove it, and in-paint the removed region. The paper proposes to accomplish this by training a generator network to fool an object classifier into thinking the object class is no longer present in the image. To prevent the generator from doing this by synthesizing adversarial noise patterns, it is divided into two stages: (1) a mask generator and (2) an in-painter. Critically, only the mask generator is trained to fool the classifier; the in-painter is trained only to fool a real/fake image classifier, preventing it from learning to create adversarial noise patterns. The system also regularizes the shape of the generated masks by sampling them from a prior (in the form of a GAN) learned from a separate, unpaired dataset of mask shapes. The system is evaluated by via classification performance and perceptual quality on removing objects from COCO images, and also on removing logos from Flickr Logos images.  This paper presents an interesting research direction. I was at first unconvinced about the utility of the 'unsupervised object removal' problem setup, but the logo removal use case turned me around--I can definitely see how there can be scenarios where one wants to remove something from large collections of images (or streams of images) en masse, without any human intervention. The two-stage generator scheme the authors use to avoid adversarial noise patterns is quite clever. I do find the qualitative results of the method to be a bit underwhelming, especially when compared with recently published work on 'supervised' inpainting (and I have some questions about the reported favorable comparison to those methods in the supplement; see below). That said, this is the first work on a new problem, so that could be considered acceptable. Overall, I'm leaning towards wanting to accept this paper.  - - - - - - - - -  I'm glad to see that supplemental includes comparison against current state-of-the-art inpainting methods; this was something that I felt was missing from the main paper. I would like to see qualitative results for this comparison, as well. Qualitatively, the results from the proposed method just don't look as good as results presented in recent inpainting work--yet the proposed method numerically performs best according to Table 1 in the supplement. I'm having a hard time reconciling this discrepancy. Are the quantitative metrics not very reflective of perceptual quality? Were the prior methods insufficiently trained? Do the authors have any insights, here?