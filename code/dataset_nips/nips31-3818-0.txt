The paper presented a novel way of detecting adversarial examples. Although the discussions were mainly centered around the specific domain of face recognition, I find the underlying ideas to be insightful and potentially more generally applicable to a wider range of tasks/DNN models as well. The efficacy of the identified witness neurons was also demonstrated through empirical studies and I think this in its own right makes it an interesting enough contribution.  * That said, the biggest concern I have is on how much of this idea can be easily extended to other tasks because it seems the notion of this "attribute" is very much task-specific and, if not defined properly (e.g. being too broad or too specific), might either render the resulting attribute witness neurons non-existent or useless for interpretability purposes. Have the authors considered testing the robustness of this detection algorithm itself w.r.t. the set of "attributes" by conducting experiments with different sets of attributes (e.g. by adding/removing certain attribute from the set, comparing "attributes" across various granularities (e.g. from edgelets with different orientations to simply chunking the face images into upper/middle/lower regions, etc.) or types (e.g. geometry metrics of the face organs, etc.))?  * How consistent were the identified witness neurons across the different inputs? And why just use 10 inputs (presumably the more the better)?  * A slight notation issue: index i only appears on the l.h.s. of Eq.(7) and thus doesn't link well with anything on the r.h.s.  * Please explain what you mean by "attribute conserving transformation" in more details.