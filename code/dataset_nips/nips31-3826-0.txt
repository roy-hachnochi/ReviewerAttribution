Summary:  The authors provide a collection of results regarding the capacity of different polynomial threshold gates of N variables with d-degree.  The key result is the closing of a multiplicative gap between upper and lower bounds on the capacity of polynomial threshold functions (e.g. Thm. 3.1).  They leverage these ideas to provide capacity bounds for neural network architectures.  Positive:  The manuscript is exceptionally clear, thorough, and of obvious practical relevance.  The new lower bound proof closes an old gap in our understanding of capacities of polynomial functions.  The techniques in the appendix on random tensors are also clearly powerful, and, if the open questions posed in section 10 aren’t already a good indication, this work opens many avenues for further research.   Negative:  While this paper is excellent, as a bit of sociological commentary, the lengths of supplementary materials for NIPS submissions are starting to get…out of hand.  I understand the author’s likely desire to broadcast this work to the machine learning community, but this really might’ve been better served as a journal article, both because the review process would be of higher quality, and because the full meat of the work could all be in one place (instead of shoving a ton of really interesting stuff into Ch. 10 of the supplementary material (!!!)).  Regardless, I, of course, strongly suggest acceptance publishing in NIPS.  Here are some minor typos that I spotted:  6: feedfoard -> feedforward  55: synpatic -> synaptic  64: fedforward -> feedforward   85: computatble -> computable  Eq. 10: leq -> \leq   Supplementary material:  pg. 8: betterunderstood -> better understood  3.41: arrangments -> arrangements  10.4: Poission -> Poisson (unless there’s a Poission I don’t know about…)