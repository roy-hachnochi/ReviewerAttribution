The authors propose Adaptive Parameterization, a network architecture that largely replaces nonlinear activation functions with learnable input-dependent transformations. The parameters of these transformations are proposed to be produced by another neural network. The authors describe several examples of this architecture and present results on standard benchmarks using an RNN whose recurrent function implements the proposed Adaptive Parameterization.  The work is clearly presented and well motivated, and the results are compelling. As far as I know, the core concept is original, although indeed closely related to some previous work, as noted by the authors.   The work could be strengthened by streamlining the mathematical notation and conceptual flow in sections 2 and 3. The concepts presented here are rather simple, and readers would benefit if the authors clarified this presentation as possible (e.g., there seem to be many auxiliary variables and functions floating around--eliminating these might reduce the working memory burden placed on readers). Further, some of the links made to the "Example Policies" in section 2.2 are rather imprecise and might be worth striking or rephrasing (e.g.., "partial adaptation" only standardizes inputs / outputs in the unlikely event that the parameters learned end up z-scoring the inputs or outputs; "singular value adaptation" only relates to SVD in the unlikely event that the learned parameters have the appropriate orthogonal structure).