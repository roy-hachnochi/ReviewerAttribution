This paper proposes a novel approach to interactive visual content retrieval.  It formulates the task as a reinforcement learning problem, and rewards the dialog system for improving the rank of the target image during each dialog turn.  To alleviate the cost of collecting human-machine conversations as the dialog system learns, the dialog system is trained with a user simulator, which is itself trained to describe the differences between target and retrieved images.  Experiments show the advantage of the proposed RL-based dialog approach over alternative baselines.   [strengths]  1. To my knowledge, this is the first dialog-based interactive image retrieval system where the system and user communicate using natural language.  The proposed framework and its components are appropriate for the problem.  2. The idea of using a user simulator for relative image captioning is interesting.  This helps avoid the costly alternative which would be to collect human-machine dialogs as the system learns.  3. A new dataset for the task of relative image captioning is introduced.  However, the usefulness of the dataset may be limited (see below, weaknesses  point 2).  [weaknesses]  1. The paper's conclusion that "user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results" is too strong given the weakness of the pre-specified attributes baseline (Sec. 5.2). To me, this is the biggest weakness of the paper. Specifically, if my understanding is correct, that baseline uses a 10-D attribute vector, which is computed using hand-crafted features like GIST and color histograms [13].  Thus, it is not clear whether the improvement is due to natural language feedback (as the paper claims) or if it's due to better features.  This aspect needs to be studied in more detail.   2. For the relative image captioning dataset to be truly useful, a more systematic and automatic way of evaluation should be presented that goes beyond user studies (as presented in Appendix C). This is important so that new methods that try to tackle this problem can be properly compared.    3.  It'd be helpful if the paper could provide some typical failure cases so that the reader can have a better understanding of the system.   [summary]  Overall, due to the novelty of the problem and sound approach, my initial rating is to accept the paper. However, there are also some weaknesses (as mentioned above) that I would like to see addressed in the rebuttal.  [post-rebuttal final comments] Overall, the rebuttal has addressed to some degree the weaknesses stated in my initial review. I am, however, not completely satisfied with the authors' response regarding my point that the paper's claim "user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results" is too strong. Any comparison with hand-crafted feature based approaches needs to be stated clearly as so, as it is difficult to draw other conclusions out of such a comparison; i.e., if the baseline were to use a deep representation, it may perform just as well as the proposed approach. This needs to be empirically verified. Although this weakness is not enough for me to lower my initial rating, I feel that the paper either needs to (1) perform the appropriate experiment (i.e. the baseline should use a 10-D attribute vector derived from a deep neural network) or (2) clearly state that the baselines' features are hand-crafted and thus much weaker and may be the key cause for its lower performance. The rebuttal states that the revised paper will have something along the lines of (2), but I think it's important to explicitly state that the baseline is using "hand-crafted features".