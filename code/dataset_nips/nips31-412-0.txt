The paper combines existing ideas of disentangled representation learning (Higgins et al, 2017) and categorical latent variable learning (Maddison et al, 2016) in VAEs to create an architecture capable of learning both continuous and discreet disentangled latent variables.   The paper is very well written. It produces compelling results demonstrating disentangling on a variety of datasets. It compares the approach to the relevant baselines and does further analysis to build the readers' intuition. While the novelty is arguably limited (the paper combines two existing approaches in a rather straightforward manner), it is the first demonstration of an unsupervised approach to learning disentangled continuous and discrete latent variables that works well. Hence, I believe the paper has merit to be published at NIPS.  Suggestions for improvement:  -- I think the paper would be stronger if more discussion was provided around the choice of the number of discrete latent variables. For example, in the MNIST experiment, it would be useful to show how the representation learning is affected by over- or under-specifying the number of discrete dimensions (e.g. <10 or >10).  -- It would be good to compare the performance of the model to that of InfoGAN (Chen et al, 2016), since InfoGAN can also learn both discrete and continuous disentangled representations  -- How does the model perform when a dataset contains more than 1 discrete latent variable? Is it able to discover that?   -- Do the authors have any intuition or metric to measure whether the discrete dimensions are used in a way that is disentangled and improves the model's performance (this is relevant to the discussion point around the Chairs dataset)?  -- Why were 10 discrete dimensions used in the CelebA experiment? Presumably there are more than 10 identities in the dataset. Have the authors attempted using >10 discrete dimensions? It would also be useful to see the identities learnt by the model in a plot similar to Fig. 2c.   Minor points:  -- The reference to Chen et al (2018) on line 15 is misleading, since that paper did in fact address discrete disentangled representation learning  -- The Desjardins et al (2012) reference on line 28 is misplaced  -- What is the schedule for setting the capacities C_z and C_c? It is worth including these in the main text  