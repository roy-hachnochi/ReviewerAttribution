Update following rebuttal:  I thank the authors for their thoughtful reply to my questions. I maintain my assessment that this is a a very good paper.  -----  The authors present an interesting exploration of combining joint and discrete distributions into a fully factorized latent representation in the context of a beta-VAE framework.  The concept itself is only moderately novel, as all of the components (the beta-VAE framework, the forced capacity technique, Gumbel-softmax distributions, etc.) are based on previous work, and the authors merely combine them, but the results are nevertheless convincing and interesting. It's good to know that assuming an appropriate prior for the data (e.g., a discrete variable with 10 dimensions for MNIST combined with a set of continuous degrees of freedom) actually helps in disentangling the data distribution, compared with assuming a strictly Gaussian prior.  One question that I would like to see addressed, which the authors apparently missed is: is the forced capacity technique (i.e. annealing the values of C_z and C_c during training) still necessary to achieve disentanglement when the joint prior is used, or could the same level of disentanglement be achieved without it, just by virtue of having a more appropriate model for the data?  A minor improvement to the paper would be to give Figure 4a its own number, as it is a bit confusing to have it in the same figure as 4b and 4c, which show a different dataset. 