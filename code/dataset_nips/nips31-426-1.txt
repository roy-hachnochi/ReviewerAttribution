This paper proposes a novel interpolating function to replace the output layer of deep neural nets, for example the softmax function. The authors use ideas from manifold learning and study a weighted nonlocal Laplacian layer. This results in DNNs with better generalization and more robust for problems with a small number of training examples. For experiments, WNLL is tested on image recognition tasks, such as CIFAR, MNIST, SVHN. Impressive test error reduction results are reported.  Pros: This paper is clearly written and easy to follow. The authors did an especially good job on literature reviews.  Cons: While the achieved error reduction results on MNIST and SVHN are quite impressive, stronger baselines with more supervision do exist and can do a better job. Is it possible for the authors to apply the same algorithm on a standard transfer learning task? Furthermore, the proposed neural network layer seems general enough for a wide range of applications, it would be of interest to a greater audience if the authors also experiment with, e.g. NLP and speech tasks.  As the authors also mentioned in Section 5.1, i.e. limitation and future work, the proposed algorithm poses memory challenges for the ImageNet dataset. I wonder whether the authors could provide specific quantitative results of train/test speed and memory comparison for w/ and w/o WNLL layer.  The paper has several typos. For example, the authors mixed up x and y in several places. Line 232, "tTis" should be "this". Reference [12] and [13] are duplicated. 