In this paper, the authors propose to replace the softmax output activation with a surrogate activation based on weighted nonlocal Laplacian (WNLL) interpolating function. The overall results seem encouraging. The approach shows superiority of generalization accuracy on a wide variety of networks, and reduces CIFAR10 error rates by more than 1.5% in most architectures.   Cons: The author didn’t provide run time analysis using WNLL vs the vanilla softmax activation. What’s the extra computational overhead caused by WNLL? In particular, solving the linear system in Equation (2) or Equation (3) can be quadratic, which inevitably increases the time and memory cost. As the authors mention, this also causes the training difficulty for larger dataset such as ImageNet, which raises the concern of its practical usefulness.   Minors: There are a few typos throughout the paper. For example, in Section 5.1, “tTis” should be “This”. Also in Figure 1(b), the testing time input should be X solely without the need for Y.