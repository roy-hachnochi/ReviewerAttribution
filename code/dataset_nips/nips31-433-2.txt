The authors propose a novel framework composed of three players, a classifier, a teacher, and discriminator. With this work, this paper effectively combines distillation and adversarial networks in one frame. The classifier is not allowed to privileged provision (such as larger capacity model) but train from the teacher with distillation loss. Both teacher and classifier is trained on adversarial loss against the discriminator, following standard GAN setting. The authors proposed KDGAN and showed it is effective and efficient in two interesting problems, model compression and image tag recommendation task.  Strength:  - This paper is well-written in general. It summarizes existing work (especailly Naive GAN) clearly, so the readers can easily understand the background. KDGAN formulation was also clearly motivated and explained.  - It is especially good to have a section 3.3, trying to improve training speed with theoretical motivation.  - Both experiments are relevant and important applications in ML field.  - I believe this paper is worth to be presented at NIPS in terms of originality and clarity.  Weakness/Suggestion:  - In section 3.2, the authors mentioned that the teacher model also learns from classifier (student) model, motivating from a general thought that teachers can also learn from their students in society. It'd be nicer to see some experimental evidence to show this.  - In section 4.1, the experiment is about deep model compression, aiming at reducing the storage and runtime complexity. I see experiments about faster training, while I couldn't see discussion about the size of the model. It will be interesting to compare smallest model sizes accomplishing similar level of accuracy, though this experiment may be harder to conduct.  - Image tag recommendation may be easily applied to video annotation dataset (e.g, YouTube 8M) for your future work.   [Additional comment after author response] Thanks for additional experiment and clear explanations. I adjusted the score, as it cleared my concerns.