This paper describes a method to integrate knowledge distillation and adversarial network to get the best of both worlds. The proposed architecture has three components: a classifier, a teacher and a discriminator. A distillation loss regulates  the relation between the classifier and the teacher so that the produced pseudo-labels agree with each other.  The adversarial loss  connects the teacher and the discriminator: the last one tries to recognize pseudo-labels from true labels.  The difference of the proposed strategy with respect to pre-existing methods with three similar components are well  discussed in the related work section. Besides that, two novelties are intruduced: (1) mutual learning of T and C (not only  C from T but also T from C); (2) the Gumbel-Max trick to reduce the variance of gradients.  Overall the paper introduces some new ideas and the experiments show the effectiveness of the proposed strategy.  I have only few doubts:  - I find it a bit confusing the discussion about  the availability of 'priviledged information' at training time as basic motivation of the work. Indeed when passing to the experimental sections we are only dealing with standard samples x and standard labels y, there is no mention of extra information which are generally defined as priviledged (e.g. attributes).   - the first set of experiments is about model compression, however details about the obtained compression are not provided (number of parameters of T and C?). It is not fully clear to me if the considered competing methods are actually obtaining the same model compression, or  wheather KDGAN is more/less effective in this sense (more compression) besides being better in terms of produced accuracy.  - Although the introduction of the two novelties mentioned above surely helps increasing the overall KDGAN performance  it is not explicitly discussed their role. An ablation analysis would be beneficial to understand quantitativaly their contribution.