Summary  Recurrent networks of leaky integrate-and-fire neurons with (spike frequency) adaptation are trained with backpropagation-through-time (adapted to spiking neurons) to perform digit recognition (temporal MNIST), speech recognition (TIMIT), learning to learn simple regression tasks and learning to find a goal location in simple navigation tasks. The performances on temporal MNIST and TIMIT are similar to the one of LSTM-networks. The simple regression and navigation task demonstrate that connection weights exist that allow to solve simple tasks using the short-term memory of spiking neurons with adaptation, without the need of ongoing synaptic plasticity.  Quality  The selection of tasks is interesting, the results are convincing and the supplementary information seems to provide sufficient details to reproduce them.  Clarity  The paper communicates the main messages clearly. But the writing could be improved significantly.   The introduction is not well structured. Why are the contributions in paragraphs 2 and 4 separated by the long paragraph on L2L? What is meant by the two levels of learning and representation (line 43)? Why is the discussion of the PFC and STDP relevant for the rest of the paper (lines 48 to 56)?  I did not understand the sentence that starts on line 123. Fig 1B seems to indicate that the LSNN performs similarly as an LSTM, not significantly better.  line 128: It would be helpful say here already, how the inputs are encoded for the spiking networks such that the reader does not need to go to the supplementary material to understand this aspect.  line 180: I did not understand how the network could learn to cheat.  Originality  The work seems original. But a section on related work is missing. There are also other works that try to solve (temporal) MNIST and TIMIT with networks of spiking neurons (e.g. https://doi.org/10.1016/j.neucom.2013.06.052). How is this work related to other approaches?  Significance  I enjoyed reading this work. It is nice to see networks of adaptive spiking neurons perform well on temporal MNIST and TIMIT and solve the simple regression and navigation tasks without synaptic plasticity (after having learned the domain knowledge). If the approach scales, it may also be interesting for neuromorphic hardware. What we learn about biological neural networks is less clear. Activity or short-term plasticity dependent models of working memory are well known in computational neuroscience (see e.g. http://dx.doi.org/10.1016/j.conb.2013.10.008) and the present work does not seem to make clear testable predictions beyond the observation that these networks are sufficient to solve the studied tasks.    Minor Points: - Why was the same adaptation time constant used for all neurons in the first two tasks, but a distributions of time constants for the second two tasks?  ==== After reading the author's response and the other reviews I am more convinced that this is exciting and novel work. But its presentation and embedding in the literature can clearly be improved. Regarding biological neural networks: I agree with the authors that the work demonstrates the power of adaptation in recurrent networks of spiking neurons. But the hypothesis that adaptation could be used as a form of activity-silent working memory is very straightforward and it is not clear to me, why nature would have favored this over e.g. activity-silent short-term-plasticity based working memory [Mongillo et al. 2008].