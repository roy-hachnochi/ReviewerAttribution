Summary The authors trained spiking neural networks to perform sequential MNIST, TIMIT, learning to learn families of non-linear transformations, and meta-reinforcement learning of water mazes. To accomplish this, they trained a population of LIF neurons using BPTT with smooth pseudo-derivatives around the spikes and the deep rewiring algorithm for tuning network connectivity. Some of the LIF neurons had an adapting property whereby their excitability was reduced after firing.  Positives The idea of doing sequence classification, L2L, and meta-rl problems in spiking networks is very appealing, because of the applications in modeling of biological neural activity and neuromorphic computing. The tasks are compelling and the results seem strong. Even though the critical learning algorithm, BPTT, doesn't have a biological interpretation, the fact that the authors are able to solve these tasks with RSNNs at all appears to be a technical feat and a useful demonstration.   Areas for Improvement However, the paper doesn't provide a historical summary of the state of the art for RSNNs applied to these sorts of problems, so as someone without a working knowledge of that literature, I'm not able to evaluate how large of an advance the paper makes beyond prior work.  Further, the paper doesn't clearly explain the motivation behind the architecture and learning algorithm design. What's the role of the adaptive LIF units? Do they help with long-term dependencies somehow? Why does the architecture have "Long short term memory" in its name? Is it to do with the adaptive units, rewiring algorithm, something else? What are the essential differences between this architecture and, say, a homogenous population of LIF neurons, which make it better suited to tasks with long-term dependencies? Does deep rewiring matter -- what happens without it? In order to understand the implications of this work, these issues need to be made clear.  Summary I think the quality of the work behind this paper is probably strong, but the lack of clarity makes it difficult to understand the implications of the work and to assess its originality and significance. I think an improved draft on this work could be quite a strong paper, but I'll have to recommend rejection for now.  Edit after author feedback and reviewer discussion: In their rebuttal the authors explained the novelty of the work and justified some design/naming choices I asked about in my review. However, the rebuttal didn't indicate exactly how they would fix the clarity problems that led me to need to ask about those two points. Overall this seems to me like valuable work written in a somewhat unsatisfactory manner. I increased my rating to 6 to reflect the apparent quality of the underlying work, and hope the authors improve the clarity for the camera ready. 