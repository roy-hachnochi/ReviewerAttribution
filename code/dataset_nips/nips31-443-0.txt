The paper introduces a greedy approach to tackle the NP-hard problem of hashing with deep learning. Although the main idea of “Discrete Proximal Linearized Minimization” is borrowed from “A Fast Optimization Method for General Binary Code Learning”, the paper shows how to use this approach in combination with neural networks and SGD. Also, the reported results beat state of art methods.  About the experimental setup, there is no comparison with other methods on widely used datasets such as NUS-WIDE, which has an important distinction that samples can have multiple labels.  It would be very good to see a comparison against such state of art methods as DVSQ (Deep visual-semantic quantization for efficient image retrieval) and DMDH (Deep hashing via discrepancy minimization).  Post-rebuttal comments: After reading the rebuttal and all the reviews, the Authors have addressed convincingly my concerns. They have provided results for additional experiments and have shown that their method also works under a multilabel setup. The third reviewer pointed out that similar ideas were proposed for the case of stochastic neurons. However, this paper still is the first one that applies these ideas to hashing. Although what pointed out by the third reviewer diminishes the novelty aspect of the paper, I have not changed my initial overall score.