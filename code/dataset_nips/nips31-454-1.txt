The paper proposes to regularize the CTC classification loss by increasing the entropy of the probability assigned to alignment paths, rather than to individual outputs of the network. The paper is well motivated, provides clean intuitions and good empirical results.  I only have minor comments about the paper: - The quality of Fig. 2 can be improved - i assume the plots show the posterior of different symbols, can a legend be added to make this more explicit?  Possible addition to the paper's related work section: The authors consider bidirectional networks and therefore do not have the problem of the network delaying its outputs until enough future context is seen. This is often impractical in ASR systems and constraints to the path taken by CTC were considered to alleviate this issue - this is in line with the proposed equal spacing prior (see [1]).   It is also possible to sample CTC paths and optimize the network outputs for them, rather than doing the full forward backward algorithm [2]. Path sampling has a side-effect of also improving the accuracy of the model, possibly by introducing a smoothing behavior.   I believe referring to the two above works will strengthen the motivation behind the proposed entropy regularization.  [1] https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44269.pdf [2] https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b0ffb206352c9f77a03d6a1df224a8240bce974a.pdf