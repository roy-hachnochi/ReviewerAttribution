 In this paper, the authors presented a framework to obtain interpretable deep networks. Inspired by the interpretability of a linear model, they 1) generalize the coefficients to be dependent on data 2) use interpretable basis concept (IBC) instead of raw pixel. The final network is similar to a two-tower model used in the recommendation system, with one tower responsible for feature extraction (regularized with reconstruction loss), the other tower responsible for supplying the coefficient (regularized with smoothness). As a result, the model approximates a linear model w.r.t. IBC locally. The authors showed their results on simple datasets.   The paper is well-written and the underlying intuition is brilliant. The main drawback is the lack of a performance metric in the experimental results, e.g., the accuracy for MNIST classification.   score(out of 5): quality(4) clarity(5), originality(4), and significance(4)  My main concern is whether we can achieve a state-of-art performance on some of the AI task with the proposed network architecture. Since many modern networks have a linear layer as the last layer, I think the results should not be limited by the relevance tower. However, the reconstruction loss on the concept-tower might cause difficulties. It will be great if the authors can take the additional effort on obtaining a competitive result since interpretation should not just serve the purpose of interpretation: an interpretation on the best performing model can be much more meaningful. It is understandable that such tasks can be resource-demanding, so I think it should not be a deciding factor for this submission.   Minor comments: Line 179: corrupted reference.