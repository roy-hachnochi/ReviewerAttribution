After reading the author rebuttal: I am pleased with the new experiments, and have increased my score accoedingly. -------------------------------------------------------------------------------  This paper proposes an architecture that uses an RNN to parameterize the matrices that define a linear state space model with Gaussian noise. The model can be trained end-to-end on multiple related time series. Exact inference in the resulting SSM for each individual time series can be done using Kalman filtering.  This is a well written paper, that clearly motivates and introduces the proposed deep state space model. As the authors discuss, this paper is closely related to the KVAE model of [9], but instead of using the RNN to compute the weights of a mixture of globally learned SSM matrices, the RNN is now used to define the matrices themselves. Time-series forecasting is however a novel application for this type of models, and I found it interesting to see that this combination of deep learning and probabilistic modelling techniques works well in the experiments.  While there are some initial promising results, I would have expected a stronger experimental section to support your claims, with additional experiments in the large-scale setting and possibly with non-gaussian likelihoods. Some important experimental details are missing. For example, how big are the RNNs/SSMs in each of the experiments? Which are the covariates in the electricity and traffic experiments?  The authors claim that by incorporating prior structural assumptions the model becomes more interpretable. Wouldn't the RNN learn a complex non-linear mapping of the input that makes it difficult to understand how the inputs to the model affect the final prediction?  A related work you did not cite is: Karl et al. Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data. ICLR 2017 Also, citations [8] and [9] are duplicates.