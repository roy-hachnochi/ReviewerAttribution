Clustering large datasets with outliers is a fundamental problem.  In the distributed setting, it becomes quite challenging and only recently [1,2], there has been a substantial progress on the upper and lower bounds on the approximation ratio and communication complexity.  The present paper essentially resolves both questions in the case when there are a known number/fraction of outliers. This is a major advance, clearly worth acceptance to NIPS.    The discussion of the related work omits [2], which it should not. In particular, the lower bound for the blackboard model is features an O(m + k) term, for m machines and k cluster, rather than O(mk)  or O(mk log mk) in the upper bound by the present authors. Some  table or discussion would be very welcome.   There is another NIPS submission on the same problem [3]. It would be nice if the authors could comment on the relative practicality (if not present an empirical comparison). UPDATED AFTER REBUTTA: I appreciate the authors promise to do the empirical comparison.   [1] https://dl.acm.org/citation.cfm?id=3087568 [2] http://papers.nips.cc/paper/6560-communication-optimal-distributed-clustering [3] https://arxiv.org/abs/1805.09495