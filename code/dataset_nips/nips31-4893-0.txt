RenderNet is a neural network that can approximate a 3D render. The network is trained in a supervised way using pairs of inputs (3D model, camera pose and lights) and outputs (3D rendering using an of the shelf standard render). The network has several modules: rigid body transformation (World2camera coordinates), CNN with 3D convolutions, projection Unit (3d to 2D) and finally a CNN for computing the shading. The method is trained in chairs from ShapeNet. However it seams to generalize well for other classes. RenderNet has been used in an application example of inverse graphics (Recovering the 3D structure from a 2D image).  There are several works that perform 3D rendering in a differentiable way. The most important ones I knew are already cited in the paper: OpenDR, and the NN based ones. This new method is a more powerful network. It allows 512x512 rendering which is bigger than the previous SOTA. It also allows render texture. And it is differentiable, so it can be used as a module for other networks.  As was points, it is difficult to fully understand the paper due to the lack of details. It is not clear to me how the textures are used. I miss details in the method section. Most of the evaluation is qualitative with the figures from the paper. There is only one table with numerical results but it is limited and difficult to understand: What are these methods trained on? What is the testing set? How does it compare to other methods but EC? Why EC-Deep is worse than EC? PSNR for EC and RenderNet are almost the same but visually EC is much worse... Qualitative results of EC-DEEP? What is RenderNet Face? fig 5.c what is that? Regarding the texture it has been only used for faces. Can it generalize to other classes?  Is the code going to be available? Either will be very difficult to reproduce.