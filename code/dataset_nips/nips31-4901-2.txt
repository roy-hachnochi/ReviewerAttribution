This manuscript studied the problem of robust hypothesis testing, i.e., the minimax hypothesis testing between two hypothesis classes which are assumed to be Wasserstein-1 balls around the respective empirical distributions. To tackle this problem, the authors first applied the idea of using convex optimization to solve hypothesis testing (i.e., replace the 0-1 loss by a convex surrogate loss), and then write the dual form of the resulting convex optimization problem to arrive at a finite-dimensional convex optimization problem, which becomes tractable. In other words, the authors provided an efficient and approximate approach to solve the robust hypothesis testing problem.   Conceptually speaking, the ideas used in both steps are not new:   1. Replace 0-1 loss by convex surrogate loss for hypothesis testing: this idea has been appeared before in GJN'15 ([8] cited in the manuscript). Although only the exponential function was considered at that time, the extension to general convex functions is quite straightforward.   2. Use strong duality to reduce the infinite-dimensional Wasserstein ball into a finite-dimensional convex program: Jose Blanchet had a series of papers on distributionally robust optimization which covers exactly the same idea and lots of related tools.   Although the fact that both ideas are not new may be the main weakness of the current manuscript, in my opinion, it is still really nice in the sense that this manuscript combined both ideas to solve the robust hypothesis testing problem, which is still novel to the best of my knowledge. This combination leads directly to an efficient way to solve the robust hypothesis testing problem, which has considerable applications in practice such as change point detection. This approach also sheds light on solving general minimax hypothesis testing problems efficiently (where the Wasserstein balls can be changed).   Edit: I appreciate the authors' feedback and would like to vote for accepting it. I also increased my score from 6 to 7. 