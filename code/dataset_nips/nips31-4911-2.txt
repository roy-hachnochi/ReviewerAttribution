The authors explore and develop an approach to code correction, or repairing software vulnerabilities, (in general sequence correction) via a (Wasserstein) GAN and Neural Model Translation (NMT) approach, that does not require having paired examples for training.  The approach only requires having ample examples of each domain (faulty codes and correct codes, or more generally, good and bad sentences/sequences). The authors have to overcome a number of challenges, and show that they get favorable accuracy compared to seq2seq techniques. The experiments are performed on two synthetic/controlled domains, and one with C++ and software vulnerabilities (10s of thousands of functions).  Recent work on NMT with monolingual corpora and unsupervised cypher cracking are perhaps closest in nature to this work, but the author explain a major difference.  The authors explore a two regularizations and as well as curriculum learning approach to (potentially) improve results.   I didn't get an idea of how far from practical utility, in fixing vulnerabilities, the approach is.. even for seq2seq which has a better accuracy of 96% (vs 90 for proposed GAN).  For instance, do the approaches output good/reliable probabilities, and can one achieve reasonable recall with say 99% confidence?  It would help to further motivate the problem.  It appears that even detecting a (nontrivial) vulnerability is a challenging task, and manually fixing such (vs automated) may not be the main problem.  Update after author rebuttal: The authors promise to improve the motivation & address questions raised. The rebuttal hasn't changed my review.  