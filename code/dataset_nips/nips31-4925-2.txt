The paper introduces a deep belief net architecture for re-expressing the probability distribution of words in a topic hierarchically for deep topic model architectures. For each belief layer, the authors use the product of a sample from a gamma distribution to reweight the topic probabilities from the previous layer as a Dirichlet prior for the new layer's topic probabilities - hence the name, Dirichlet Belief Net (DirBN). The authors incorporate this system with Gibbs sampling for inference into a variety of existing models to demonstrate its flexibility and the improvement in both perplexity and coherence it provides to topic models.  The authors do a good job of motivating this model, grounding it in existing work, providing implementation information and showing DirBN improves on these models. I think the perplexity, coherence, and document classification evaluations are appropriate, as are the choice of models to evaluate. I have a bunch of smaller points to discuss for the paper, but most of them have to do with clarity.  One question lingering after reading this paper, though, relates to the "hierarchy" of topics being learned. The model consistently uses the same number of topics per layer, so it is unclear to me that each layer is offering higher-level intuitions so much as just offering alternative ways of organizing the information. The qualitative analysis provided by the authors did not do much to convince me otherwise; I am not sure that preventing overlap in top topic terms in some examples maps to a meaningful improvement in final topic quality. I also was less interested in which model was best across all options than whether DirBN improved over the baseline model in a statistically significant way for each of PFA, MetaLDA, and GBN, which seems to be true in several cases but is hard to read off of Tables 1(a-c) right now.  I was also a little curious about whether the authors would expect this work to scale well to larger vocabularies, as most of the corpora tested are modest compared to some topic models of e.g. Wikipedia.  My last technical concern relates to the perplexity measurement, as line 182 suggests it is deviating a bit from what I would expect for left-to-right estimation for topic model perplexity via "Evaluation Methods for Topic Models" (Wallach et al. 2009). Is there a particular reason for the deviation?  Moving towards my clarity concerns: some of the structure of the paper gets confusing. My suggestion would be to better streamline the discussion in the paper by placing the discussion of inference for DirBN (section 3) between the original description of the model and the descriptions of inference for models incorporating DirBN (both section 2). It might also be good to explicitly separate out the final inference steps people would use (the "To summarize" section) into an algorithm or an enumerated list of sampling steps for different variables; the quantity of inline math in this explanation makes it hard to navigate. More of the math could probably also be moved to the appendix, aside from mentioning the auxiliary variables used and the sampling distributions for x, y, and z.  I have a bunch of more minor corrections as well: - line 46: drawn from Dirichlet distributions - line 55: to a document, whereas our model (the sentence beginning with Whereas isn't a complete sentence right now) - line 64: applying DirBN to several well-developed models, including Poisson Factor Analysis - line #? (just above equation 1) - probably "vanilla LDA", not "a vanilla LDA" - rather than providing the ellipsis version of the initializations in equation 1, just putting the first two lines without ellipses would probably suffice) - line 159: you only list three datasets - line 162: tokens are instances of words, probably better to say "There are 13,370 types in the vocabulary"  I think this paper is meaningful, well-balanced work that contributes to the deep learning for topic modeling literature. I would highly recommend modifications to improve clarity and address some of my outstanding technical questions for a final version.  AFTER REVIEW: The authors did a good job of addressing my outstanding questions, particularly about the topic hierarchy and the vocabulary scaling. I hope the authors will include some explanation of that and the perplexity evaluation in the paper. 