The paper proves a very interesting result: For maximizing Deep Submodular Functions (DSF) with matroid constraints, one can provide efficient algorithms that, under mild assumptions on the singleton marginals, have  approximation factor better than 1-1/e (and potentially approaching 1 when the rank of the matroid becomes large).  This is given in Theorem 1 which I think is the main result of the paper. The basic idea behind the algorithm is that for DSFs there is a natural concave extension, equations (4), (5) that can be maximized by projected gradient ascent (this result has been proved in [3]).  The authors show in Proposition 1 that this concave extension is close to the multilinear extension, and in section 4 they show that the projected gradient ascent algorithm can be implemented efficiently (e.g. the subgradient of the concave extension can be computed efficiently due to the deep structure). The paper is written well and the results are novel. However, I have a few comments for which I would appreciate a response from the authors:  1) How is the projection step in Algorithm 1 done? E.g. what is the complexity for general matroids? Since you prefer k being large, it would be useful to provide the exact runtime of the algorithm in terms of n and k. Also, in Figure 3 (c), is the projection step considered? (I guess for partition matroid the projection step is easier than general matroids)  2) Can the authors explain how the bounds of Theorem 1 compare with 1-1/e if k is not so large? (lets assume that the modular term m_{+-} in equation (1) is zero to make the overall objective monotone). Also, it would be beneficial if the authors could explain how the bound in Theorem 1 would be in the worst case. I.e. given a fixed k and perhaps negative modular terms, how bad the bounds of Theorem 1 could be? In particular, are they better than 1/2?   3) In your experiments, I think it would be useful in include settings with negative modular terms (m_{+-} in equation (1)).   Some minor comments:  - Line 40: befit -> benefit - Regarding the discussion in lines 74-78, there is now an efficient and practical algorithm for maximizing the multilinear extension over matroids (see the stochastic continuous greedy algorithm by Mokhtari et al, 2017). Basically, you don't need to compute the multilinear extension at each step (unlike [6]) and only an unbiased estimator (that can be computed very efficiently) is enough to maximize the function using stochastic optimization algorithms.  