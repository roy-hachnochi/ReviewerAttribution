Summary: Deep submodular functions are a class of submodular functions that may be modeled using a neural network. This work focuses on maximising deep submodular functions with matroid constraints.  This is done using multilinear extension that can be optimized using gradient asnet. Further, pipage rounding is used to find an approximate optimal solution to the original problem.  Pros: - It is first novel attempt to maximize deep submodular funcions under matroid constraints. - The paper is well structured and well written - Theorem 1 where they prove that multilinear extensive of DSF need not be calculated to optimize the function is the major result - Theorem 3 gives an approximation due to pipage rounding  negatives/clarifications: One of the main drawbacks of the paper is experimental evaluation. One of the issue with deep neural networks is the amount it takes to optimize the function in the parameter. I believe, the goal of deep submodular functions is to leverage the underlying function structure to be able to speed up the optimize algorithms yet as express as DNNs. However, this paper does not demosnstrate this.   Update after authors response: I do agree with the authors that NIPS should have space for theoretical contribution and that is precisely the reason why I tend to vote towards accepting and upgrading to 7. However, I believe, a good experimental evaluation would have further strengthened the paper.