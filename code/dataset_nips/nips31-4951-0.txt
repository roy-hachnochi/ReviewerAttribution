The paper presents a learning scheme for deep policies which is based on sequentially combining existing methods of learning from demonstration and learning from human preferences. The experiments validate the approach with simulated and real human teachers, obtaining interesting results but still remaining some open questions.  It is said "Expert demonstrations are always kept in the buffer", so in cases wherein the demonstrations are not good, but the preferences of the teacher can lead to a high performance policy, there would be a problem, because the Demos are pulling the policy to a wrong area in the solution space. How to deal with this problem?   Is it possible to draw from the experiments, in which kind of problems the preferences have a higher impact for the policy improvement. It would be interesting to see comments in a generalized way, about what kind of problems are more problematic for learning with human preferences, and which are prone to be easily trained (e.g. the kind of problems that can get superhuman performance).  Results in Fig 3 (right) show something that can be really problematic in this presented approach. It is shown that the learned reward model is not aiming for the same objectives of the real reward function of the environment. So The RL agent always will try to maximize the reward function, however if it is wrong, obtaining high returns might be senseless (the method is not learning what the human teacher wants to teach, or the human understanding of the task is not complete). In those learning curves, the real return obtained at the end of the process is still an acceptable performance? It would be necessary more   Why in these experiments the learned model r only takes as input the observations? is it for simplifying the model? or because it is known that the real score might be based only in states and not in actions? what would happen when considering the actions along with the observations in these experiments?