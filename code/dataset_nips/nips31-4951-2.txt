This paper combines a number of existing methods and performs an in-depth empirical evaluation.  This is a very well written paper with high-quality scholarship.  ----  I take issue with the ALE being "difficult enough to require nonlinear function approximation," i.e., "State of the Art Control of Atari Games Using Shallow Reinforcement Learning" https://arxiv.org/abs/1512.01563. I also think that the claim that it is "simple enough to run many of [sic] experiments on" is questionable, unless you're a large industrial research lab.  The citations tend to heavily favor recent work, which is generally OK. But in the related work section, the citations imply that intrinsic RL was only invented in 2015 - it's much older than this.  "clips" are discussed in Algorithm 1 on page 3 before being defined.  It's not clear to me why expert transitions are always kept in the replay buffer (page 4) since the expert's demonstrations could be sub-optimal.  In the majority of experiments a "synthetic oracle" that knows the true reward is going to select between clips. It's not clear to me whether this is a reasonable approximation of a human. Are there many clips where the reward is 23 vs. 24, or 2000 vs. 2003? Or are the clips normally qualitatively very different, and therefore easy for a human to compare without carefully estimating the reward on every action?  ---------------  After reading the authors' rebuttal and the other reviews, I have increased my score. Even though the assumption of an oracle (instead of a human) is strong, the work is still interesting. Additionally, I decided to weight the high quality of the paper's scholarship and writing more highly in my overall evaluation. 