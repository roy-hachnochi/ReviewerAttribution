This paper presents novel arithmetic units called NAC and NALU for neural networks. The proposed NAC unit is used for calculating additions and subtractions of the input vector; NALU calculates multiplications by applying a logarithm before the NAC unit. The experimental results show that the NAC/NALU units worked well on numerical addition and counting tasks--even when the lengths of numbers in the test data are different than those of training data.  This work is motivated by the problem of learning the identity function, Figure 1 shows that many activation functions do not work in this problem. What is the performance of NAC/NALU on learning the identity function? Would it be a straight line as expected? Figure 1 also shows that some activation functions perform well on this problem (although hard to identify on the figure because of the overlapping lines and colors). It would be better if the authors can show their performance on the same tasks in the experiment section.  Another problem is that the illustration and caption of Figure 2 are not very clear, e.g., what is the three blue dots on the left of the tanh matrix? How do the gates work? Why are they designed like this? Please provide a more detailed description of the NALU unit.   The experiment on reinforcement learning shows that although NAC did not work on extrapolation tasks as expected, it degenerates slower than the compared method. How about NALU's performance on this task? The reward of A3C can lower than 0, why A3C+NAC did not get lower than 0?  Some other questions: 1. What is the \hat{M} in line 87? 2. I would prefer to see NAC/NALU applied to more complex tasks, such as the object counting tasks introduced in section 1.