This paper conducts an in-depth analysis of the optimal dimension for word embeddings. It starts with proposing a loss, based on a very straightforward motivation, and then decomposes the loss into variance and bias components in a very nice way (which is Theorem 1 in the paper). Then the theorem gets extended to other cases, without changing the main motivation behind it, and reveals its fundamental relation to SNR in information theory and signal processing. Thus it gives a satisfying good theoretical inspection and explanation to [Arora, 2016]. Plus the support of previous research [Levy and Goldberg, 2014] on the connection between skip-gram model and matrix factorization, the whole set of theory becomes applicable to a set of widely used models like skip-gram.   I would say reading through the main part of the paper is enlightening and rather an enjoyable adventure.   The experimental results regarding the robustness of word2vec/glove models w.r.t. dimensions (Section 5.1) are consistent with what many people are experiencing when using these word embeddings. Which the paper has well explained that in its previous sections.   I have to say it is a very good presentation of an in-depth explanation of the well-known word embedding tasks. And the theoretical results do have its practical applications. 