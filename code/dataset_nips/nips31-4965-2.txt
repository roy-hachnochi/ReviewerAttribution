The paper develops a timing model for distributed DNN training based on AllReduce and, based on this, proposes a pipelined training strategy that allows to mask the faster of computation and communication times. The paper proves convergence in the convex and strongly convex case and evaluates PipeSGD empirically on a variety of benchmarks (MNIST, CIFAR, ImageNet) using a GPU cluster with 4 nodes. Empirically, PipeSGD leads to a significant speed-up relative to decentralized synchronous SGD and synchronous SGD.   The paper is clearly written and as far as I can tell the timing model makes sense. The empirical results are quite favorable but I would encourage the authors to attempt to scale this framework to larger clusters: 4 GPUs is quite small for distributed deep learning and the paper is likely to have a bigger impact if PipeSGD can be shown to work well for larger cluster sizes.