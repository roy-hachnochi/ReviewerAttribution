[Remarks following on the authors' response are included below in brackets.]  This paper presents D-Nets, an architecture loosely inspired by the dendrites of biological neurons. In a D-Net, each neuron receives input from the previous layer as the maxpool of linear combinations of disjoint random subsets of that layer’s outputs.  The authors show that this approach outperforms self-normalizing neural networks and other advanced approaches on the UCI collection of datasets (as well as outperforming simple non-convolutional approaches to MNIST and CIFAR).  They provide an intuition that greater fan-in to non-linearities leads to a greater number of linear regions and thus, perhaps, greater expressibility.  I am still quite surprised that such a simple method performs so well, but the experimental setup seems sound.  I would ideally like a further analysis of the behavior of D-Nets.  For example, how does the optimal number of branches grow with the size of the layer?  [The authors note in their response that no pattern was observed in preliminary experiments on this question.  This negative result would be interesting to note in the paper.]  It would also be interesting to see an analysis of other nonlinear functions than maxpool in combining the contributions from different dendritic branches.  [The authors note in their response that maxpool was the most effective nonlinearity tested. Other nonlinearities tested should perhaps be noted in the paper, even if they gave weaker results.]  The main theoretical result, Theorem 3, shows that D-Nets are universal approximators.  This follows from a simple proof using the facts that (i) any piecewise linear (PWL) function is the difference of two convex PWL functions and (ii) any convex PWL function can be expressed as the max of line segments, and is thus expressible as a D-Net.  D-Nets are a type of feedforward neural net (FNN) (with sparsity and max-pooling), but in various places, the paper compares FNNs to D-Nets as if they were two separate things.  For example, the definition reads: “In comparison, a dendritic neural network…”   This terminology should be changed - for example, to MLP in place of FNN, or simply by saying “standard FNN”.  p. 3: The definition of k seems unnecessarily complicated and also slightly contradictory, since it can vary by 1 between different branches.  One might simply write k = n_{l – 1} / d, eliding the discussion of floors (which probably don’t matter in general unless k is very small).  Figure 2 could be more extensively labeled, showing d_1 = 1, n_1, etc.  Lemma 1 should be presented in such a way that it is clearer it was proven in [32].  E.g. “Lemma 1 (Wang [32]).”  Regarding compositionality and expressibility, references could be added also to Mhaskar, Liao, Poggio (2017) and Rolnick, Tegmark (2018).  Line 171: The function T_A is never actually defined rigorously, and this notation should perhaps simply be omitted since it is not used again. The “expressivity” of a network is not a quantity – or rather, it can be measured by many quantities.  It seems that here the authors are interested in the number of linear regions of a piecewise linear function.  It should be made clearer that no statements about T_A are actually proven – merely tested empirically.  There are numerous typos and small grammar issues throughout the paper, which detract from its readability.  These should be corrected by careful proofreading.  Some examples from just the first page: - ln 5: “accumulator” should be “accumulators”. - ln 9: “non-linearity” should be “non-linearities”.  - ln 12: “significant boost” should be “a significant boost”. - ln 12: “its generalization ability” should be “their generalization ability”. - ln 13: “in comparison” should be omitted. - ln 18: “made into” should be omitted. - ln 20: “as simple weighted sum” should be “as a simple weighted sum”.