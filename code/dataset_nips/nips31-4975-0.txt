— This paper introduces an algorithm for learning specialized models in a multiple choice learning framework for visual question answering.  — The authors make the observation that naively applying MCL to VQA leads to poor generalization because of data deficiency — each model looks at fewer training examples  than a single model trained on the whole dataset — and more crucially, they miss out on compositional information. For example, one model might be specialized for ‘what color is the umbrella?’ and another for ‘how many people are wearing glasses?’ while at test time they question may be ‘what color are the glasses?’.  — To work around this, the authors augment MCL with distillation (MCL-KD). Specifically, they train independently ensembled base VQA models on the entire dataset, and then while training using MCL, subset of models are trained using oracle assignments (as in usual MCL) while the rest are trained to imitate the base models’ activations.  — The authors demonstrate impressive results on CLEVR and CIFAR-100 using the proposed approach, wherein they compare against MCL and Confident MCL.  Strengths  — The paper is very nicely written. It starts with a clear description of the problem, the observations made by the authors, and then the proposed solution — positioning it appropriately with respect to prior work — and then experiments.  — Results overall, and especially on CIFAR are quite impressive. Given the small dataset, MCL and CMCL perform worse than independent ensembling, while MCL-KD performs better.  Weaknesses  — How does feature sharing affect performance in case of VQA? That might obviate the need for knowledge distillation to some extent as all models till a specific layer will now get to see the entire dataset, thus having access to compositional information. If there are complementary gains from FS and KD, do the authors have intuitions on what they are?  — I have a small confusion in table 1. Maybe I’m misinterpreting something, but why are top-1 accuracies for MCL at k = 1 so low? Especially since these are initialized with IE base models.  Evaluation  Overall, this is good work, and I’m happy to recommend this for publication.  The authors study an interesting and useful extension to MCL / CMCL. In terms of quantitative results, the proposed approach outperforms prior work on CLEVR and CIFAR, and qualitative results on CLEVR do show that some amount of specialization to question families is induced in these models through this training process.