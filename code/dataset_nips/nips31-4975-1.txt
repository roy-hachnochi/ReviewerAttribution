Post-rebuttal Update ================= Given the authors response and the inclusion of VQAv2 experiments, I'm more convinced of the method's efficacy and recommend acceptance.  ---------- Summary: ----------  This submission puts forth a knowledge-distillation based loss term to the Multiple Choice Learning framework of [9]. The propose approach is a somewhat minor (though impactful) deviation from the Confident Multiple Choice Learning approach of [18]. Specifically, the KL divergence to a uniform distribution for non-minimal loss models is targeted instead at the output distribution of some pretrained base model. This tweak seems to stabilize the performance of the ensemble as a whole -- leading to significantly improved top-1 accuracy, albeit at somewhat lower oracle performance when sufficient data is present.   ---------- Clarity: ---------- Overall the paper is clearly written. I do have a couple of minor quesitons/concerns:   - It is unclear how the question clusters were determined for CLEVR?  - Some comments in the paper seem overly harsh to past methods (see L155 for an example). I'm sure these are oversights that sneak into submissions as deadlines loom, but it is worth restating some borrowed wisdom: "Existing approaches need not be bad for your method to be good."   - The swap from accuracy in Table 1 to error rate in Table 2 is a little disruptive.  ---------- Quality: ----------  Experimental Setting:    The title and introduction of the paper argues quite well for the need for specialization in the context of visual question answering, but the experimental setting is less convincing. I've listed some concerns/questions below:    - Why was CLEVR chosen as the evaluation setting for VQA? Does it have some property that regular VQA datasets don't? CLEVR is meant as an diagnostic dataset for relational reason and furthermore is essentially solved. As such, it is somewhat unclear whether ensemble improvements in this setting extend to real VQA datasets. I would like to see experiments on the VQA 2.0 dataset.    - How was the balancing term \beta set? L155 says it was set empirically, but results on CLEVR are reported on the validation set. Was an additional set withheld from the training set to evaluate this parameter or was it tuned by hand for validation accuracy? I would like to see a chart of oracle and top-1 performance as \beta is varied.    I'm conflicted on the CIFAR100 experiments. On one hand, it isn't clear how well they fit with the title / motivation of the paper as addressing a VQA problem. On the other, they do establish generality of the method and explore a very data sparse setting for MCL methods.     ---------- Originality: ---------- As noted in the summary, the technical novelty in this submission is somewhat limited over prior work. While the contribution is simple, if it can be shown to robustly improve over existing approaches, then I consider its simplicity a virtue rather than a cause for alarm.   ---------- Significance: ---------- If the requested results come back positive, I think the paper could be a significant work -- offering a simple approach to improving deep ensembles.   