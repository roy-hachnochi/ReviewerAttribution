In this paper, the authors propose a new algorithm, inspired by actor-critic methods, for multi-agents setting in partially observable environments under a collective global reward objective function. Moreover, the authors provide a credit assignment method in order to boost up the learning. In this work, the agents are considered to be equal (exchangeable), therefore follow the same policy.  The paper is well written and has a clear overview of the previous works.    Comments:  If the proposed method is for just planing and not for RL, I would suggest changing the title, the proposition, and also the motivation. It might mislead the readers. Moreover, comparison to Monte Carlo search is also motivated.   The authors start to discuss the decentralized learning, but the method developed seems to be centralized since a single Q function is learned. Also, it would be better to clarify whether the agents observe the latent state "d". If yes, please elaborate on Dec-POMDP discussion, if not, please elaborate more on how the Q function is learned in a decentralized fashion. Also does each agent observes others action? Moreover is the policy shared? If no, how the updates occur and if yes, is it really a multi-agent setting?  Generally, the optimization here does not happen in the decentralized fashion (at least I could not see the decentralized part), therefore please provide a more clear motivation for your solution and clarify why is not solving a single MDP.   Also, more motivation over two provided utility functions is useful.   Also, Apex F is missing. 