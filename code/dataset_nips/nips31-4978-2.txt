Summary: The paper considers the convergence properties of multiple-passes averaged SGD under more natural (and optimal) choices of step sizes. The authors address a regime of parameters where performing more than #data iterations is provably necessary to obtain optimal statistical efficiency. Moreover, the upper bound provided in the paper is matched by known lower bounds (at least, for a significant fraction of the identified regime). The theoretical findings are then claimed to be supported by experiments conducted on synthetic data and MNIST.  Evaluation: Bridging the gap between the ML practice of performing multiple-passes averaged SGD (which is beneficial in many practical applications) and our theoretical understanding is a fundamental and intriguing question. The paper makes an important contribution in addressing this gap by identifying a (non distribution-free) regime where various regularity assumptions on the data provably justify multiple-passes. The analysis mainly revolves around on an extension of a previously-established connection between full-gradient descent and the averaged SGD iterates (which could also potentially form a weakness of this technique). The predictions follow by the analysis are claimed to be confirmed by experiments conducted on synthetic data and MNIST, but I'm missing some details here which lowered my overall score (see below). Lastly, I find this paper relatively accessible and easy-to-follow.   Comments: P1, L35 - can you elaborate more on 'using these tools are the only way to obtain non-vacuous' or provide a reference (or forward-ref for the minimax section if that was your attention)? P2, L52 - Can you provide a pointer for 'best prediction performance... as..'?  P2, L53 - Do you have more intuition as to *what* makes the 'easy problems' easy (and vice versa) in terms of the interplay between alpha and r? P2, L65 - 'parametric models larger dimensions' seems gramtically incorrect. P3, L87 - The way the paragraph right after assumption 3 is pharsed seems to render this assumption irrelevant. More in this context, assumption 4+5 always hold for finite-dimensional feature spaces. In general, the split into the finite- and infinite-dimensional cases seems to be referred to by the paper incoherently. P6, L213 - 'lead' P6, L218 - 'let' P7, L257 - by 'misspecified', do you mean underspecified? P7, L270 - Not sure I completely understand how you estimate the optimal t_*(n) (this made it somewhat tricky for me to judge the soundness of the results). Did you use validation-set based tests? Also, what is the behavior of the excess risk when t's are taken to be larger than t_*(n)? 