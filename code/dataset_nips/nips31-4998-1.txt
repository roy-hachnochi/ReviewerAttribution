     The authors present a framework for MDP planning in a situation where 1) interaction with the system is limited, 2) the dynamics are uncertain but the reward function is known, and 3) the state space is continuous or smooth in some known way. The approach is to compute Q^* for a collection of transition dynamics θ drawn from a prior over transition dynamics. Each time the agent interacts with the system, a collection of θ are drawn from the posterior over transition dynamics. Rather than explicitly plan in each of these posterior transition models, the authors use a GP to estimate Q-values for the posterior θ from the Qs computed for the prior θ. These Qs can then be used to create a behaviour policy.          The idea is essentially to build one uber-q-function parameterized by \theta that conditions on different transition models. This makes sense if the Q-functions vary mostly smoothly with \theta, which seems plausible.          Quality:          I believe that the technical aspects of the paper are well thought-through, and I think the experiments are adequate.             Clarity:          Overall I felt the paper was very well-presented. I have a few specific comments/questions about clarity.          "The accuracy of the proposed framework is demonstrated using a small example as well as noisy synthetic gene expression data from a melanoma gene regulatory network model." - It might help to make it more clear what the control problems are here rather than just the subject area.          "Let the uncertainty in the dynamics be encoded into a finite dimensional vector θ, where θ takes its value in a parameter space..." - I find this difficult to interpret, since later on θ seems to represent just a transition model. Could it be "...encoded into a random variable \Theta"?          In the algorithm: "Draw N sample points" - Always try to be as specific as possible. "Draw N transition models"?          Originality:          The paper is a well thought-through combination of existing techniques, applied in a useful way.          Significance:          The work has clear interest within the RL community at NIPS, and will be of interest to people with applications that are more data-poor (as opposed to large-scale deep learning methods.) I think it makes a significant contribution to the ability to solve these kinds of problems.      === Update  I thank the authors for their response and clarification.       