Summary: This paper analyzes SGD on cross-entropy objective for a 2 layer neural network with ReLU activations. They assume a separability condition on the input data.  Pros: 1. This paper analyzes a very interesting version of the problem I believe. That is, they stick to the cross-entropy loss and they study SGD the algorithm used most often in practice.  2. The techniques seems novel and innovative which is different from past work. I think this is an important contribution and adds strength to this work.  Questions: 1. The assumption A1 seems very similar to linear separability. I don't understand why one cannot club D_{i,j} for all j \in l into a single distribution D_i. Then assumption A1 is the same as separability. Am I missing something? It would be helpful to explain the assumption A1 pictorially.  2. The rates in Theorem 4.1 should be made explicit. Currently it is extremely opaque and hard to interpret the result and verify its correctness.  3. The idea of a pseudo loss and coupling that to the true loss is interesting. I think it would improve the presentation of the paper to add a bit more intuition of the technique of page 5. 