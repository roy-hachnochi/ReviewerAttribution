Summary: This paper proposes a way to deal with "catastrophic forgetting" by assigning the parts of the model specifically for particular tasks that are being learned and learn to grow the network by adding more filters as more and more task are introduced. In this way parts of the model become more task-specific. The controller which decides how many filters or units to add to each layer is an LSTM network over the layers that learns the sequential relation between consecutive layers. They train the controller via reinforcement learning using REINFORCE with learned actor and critic. They have done a smart reward shaping in order to train the controller.  Overall Comments: The idea of adaptively expanding the network according to the new task that is being added and the capacity that is assigned by the controller is task dependent is very neat.  However, I am quite surprised that the REINFORCE objective actually worked in this setup. Because in the similar settings for instance in adaptive computation time, REINFORCE has not really worked that well. I think the main reason that it worked in this setup might be the reward shaping.  I think overall the paper is well-written besides some small typos. For example "s/matche/matches/" in line 24.   I think one main part that this paper lacks is the experiments section: 1) Experiments are only limited to CIFAR10 and MNIST. These are very toyish tasks. I would be in particular interested in seeing results on some NLP or reinforcement learning task. For example on one of the tasks that Progressive neural networks were tested on.  2) More ablations are needed. In particular, ablations on the controller would be super useful. For example, one ablation that I would like to see would be a fixed controller or a random controller would be interesting as well.  Overall, I like the paper and the idea, but I don't buy the experiments part of the paper completely.