== After author feedback == Thank you for your feedback!   Regarding the discussion on model gradients to approximate p(z|x). My main point here is that standard VAE methods does not require access to the model gradients, \grad_z log p(x,z) , at test time. VAE:s (using reparameterization) however do require it at training time. This is not necessarily a weakness of the proposed method, it is just something that I believe can be useful for readers to know.   == Original review == The authors propose Hamiltonian VAE, a new variational approximation building on the Hamiltonian importance sampler by Radford Neal. The paper is generally well written, and was easy to follow. The authors show improvements on a Gaussian synthetic example as well as on benchmark real data.  The authors derive a lower bound on the log-marginal likelihood based on the unbiased approximation provided by a Hamiltonian importance sampler. This follows a similar line of topics in the literature (e.g. also [1,2,3] which could be combined with the current approach) that derive new variational inference procedures synthesizing ideas from the Monte Carlo literature.  I had a question regarding the optimal backward kernel for HIS discussed in the paragraph between line 130-135. I was a bit confused about in what sense this is optimal? You don't actually need the backward kernel on the extended space because you can evaluate the density exactly (which is the optimal thing to use on the extended space of momentum and position).  Also the authors claim on line 175-180 that evaluating parts of the ELBO analytically leads to a reduction in variance. I do not think this is true in general, for example evaluating the gradient of the entropy analytically in standard mean-field stochastic gradient VI can give higher variance when close to the optimal variational parameters compared to estimating this using Monte Carlo.   Another point that I would like to see discussed is that HVAE requires access to the model-gradients when computing the approximate posterior. This is distinct from standard VAEs which does not need access to the model for computing q(z|x).  Minor comments: - Regarding HVI and "arbitrary reverse Markov kernels": The reverse kernels to me don't seem to be more arbitrary than any other method that uses auxiliary variables for learning more flexible variational approximations. They are systematically learnt using a global coherent objective, which with a flexible enough reverse kernel enables learning the optimal. - Line 240-241, I was a bit confused about this sentence. Perhaps restructure a bit to make sure that the model parameters theta are not independently generated for each i \in [N]?