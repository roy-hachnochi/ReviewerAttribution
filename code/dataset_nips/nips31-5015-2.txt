[Post-response update]  I have read and am satisfied with the author response. My score has been adjusted.  [Original review]  Paper overview:  - The authors propose to combine variational inference with Bayesian Monte Carlo to perform joint inference of the posterior and evaluation of the log marginal likelihood. The use case is expensive-to-evaluate likelihood functions. Active sampling is used to select query points for model parameters. Experiments demonstrate the effectiveness of the approach over recent BQ-type methods.  Review summary:  - The idea of combining VI and BMC is interesting, and the experiments show that the proposed method improves upon recent methods. There is some intuitive exposition missing from the paper (example below). Adding would help in understanding the significance of the contribution. The experiments are extensive which dampens some of the concern arising from the heuristics involved in the method. Unfortunately, there is no discussion or even comment on the computational considerations involved for the proposed method. Some discussion on the methods' limitations would have been nice.  Clarity:  - The algorithm and experimental procedure are generally well described. But, there seems to be a lack of intuitive explanation for design choices. For example, if the joint posterior is modeled as a GP with unimodal mean function, why is the variational approximation multi-modal?  - Why do BMC and WSABI-L share the same color coding in Figures 1 and 2?  Originality:  - The idea of combining variational inference and Bayesian Monte Carlo is novel as far as I know.  Quality:  - The experimental procedure is extensive in consideration of likelihood types. However, comparison wrt/ the real-world datasets of Gunter et al. (2014) would have been nice. Also, since there is no discussion of computational considerations for the proposed method, performance wrt/ wall-clock time would have been preferred.  - Since the symmetrized KL is computed with moment-matched Gaussians, it is not clear how valuable a metric it is in assessing the posterior approximation quality.   - Can the authors speak to the performance of the proposed method beyond D=10?  - What are the weight distributions of the variational posterior mixture components like at convergence/stopping? In the synthetic experiments, how do the mixture components compare to the real components? How does the method perform with a fixed no. of components, e.g., 1,5?  - There are some heuristics involved in the algorithm - increase of mixture components w/ no. of samples, no. of active points to sample, no. of GP samples to perform approximate hyperparameter marginalization, definition of plausible regions - which create some concern regarding the generalization performance of the method. 