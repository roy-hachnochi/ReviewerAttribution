The paper presents an approach, from an optimization perspective, to integrate both the controller and the model. I am glad to see that this work has abundant citations from both classical control theory community and reinforcement learning community. And the idea of formulating the problem is nice, however I have concerns about clarity and significance.  First, the formulation of the problem does not take into account any randomness (those from environment) and does not even mention Markov decision process. In the equation 2, there is no expectation operator available. At least for people from reinforcement learning community, the clarity can be improved if you start the formulation from some expected performance measure (return). With the current formulation and proposed method, I am doubting the applicability of this method. The paper emphasized some previous work requires an "unrolling" process for optimization, however, I think those works are used for other purposes instead of imitation learning? The motivation of this work should be better addressed.   Second, although it is fine to use the proposed method for imitation learning, given the comparison criterion imitation loss and model loss, why not compare with a naive method such as learn the model and imitation loss simultaneously from expert demonstrations? Importantly, the paper should clearly state what are available in the imitation learning setting ( the expert itself available, or only demonstrations, or sth else)? From the paper, I think there is a somewhat strong assumption that the expert itself should be available, and I cannot intuitively think of how the proposed method can learn a policy which is better than the experts'.   Third, the paper claims the proposed method has a lower computation and space complexity. It should be better to clearly identify the complexity in asymptotic notation (per iteration). Or at least add some explanation about why it can save space complexity.   Last, the paper should include an appendix, with more implementation and experimental details.  Update: the response addresses part of my concern. I upgraded my rating. I am still concerning about its applicability. 