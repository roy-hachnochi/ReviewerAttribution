The contributions of this paper consist in presenting a formulation for learning groups of variables in multitask linear prediction problems, and developing an optimization algorithm to solve the bilevel problem.   The idea of learning group structure is not new. The authors mention other approaches in the literature, but do not compare with them. The formulation introduced by the authors is missing some discussion to make clear what is the meaning of the groups obtained in cases where there are few or only one task.  Are all the zero variables put together in the same group? In addition, the group lasso penalty usually includes a factor on each group that is proportional to its size (see for example Meier et al. (2008) "The group lasso for logistic regression" JRSSSB). Missing this factor, it seems that the method might give preference to equal sized groups?   The optimization algorithm and theoretical results are clearly written and sound. However, several optimization strategies have been used for solving problems like 3.1. The authors discuss some other approaches but I believe the motivation of why this new method is needed can be improved. Some of the details in the proof of Theorem 3.1 are referred to [2] but should be included for completeness.  Numerical simulations and real data show the effectiveness of their method, but including other approaches for learning groups and prediction in multi-task learning can be included to make the contribution clearer. The real data example is brief.  UPDATE: Thanks to the authors for their response. After reading the rebuttal letter, my questions have been clarified.