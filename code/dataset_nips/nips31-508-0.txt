On the very high level, I am not convinced with the motivation. It seems the motivation is solely to focus “more” on the confusing classes for each sample. - Isn’t this already happening when using softmax?  That is, the backpropagation gradient of softmax-cross entropy is proportional to the difference of true and predicted probabilities. In the case of this work, the extent of focus is obtained by “another predictor” which acts the same as a class predictor as far as I understand. This first brings me to the second question,  - why do we need another affine transform to obtain \rho and not use the class probabilities in the first place? Especially in light of added cross entropy loss in (11).   The experimental results and the comparison to baselines show improved performance on multiple datasets. However, since most state-of-the-art results on benchmark datasets use softmax, it would have been more convincing if better results could be achieved using those architectures and the set of hyper parameters. For instance on ImageNet and one of the top recent architectures.  Also, other quantitative and qualitative results do not show any insight beyond that of a standard softmax and the confusion matrix and predictive uncertainty obtained from it.  The rebuttal provides more baselines which better shows the significance of the approach.  