Main idea: This paper studies the adversarial defense for the deep learning models. Compare with the traditional deep learning defense model that suffers from the problems of constrained architecture and not linearly scalable. This paper proposes a linearly yet adaptive defense model that shows good performance compare with the state-of-the-art frameworks.   Strengths:  The written is good and logic is easy to follow. Introduction, as well as the related work, are thorough and complete. The derivation of the dual network is novel and intuitively more efficient in computation. The further bound analysis using random project with mean estimator can further speed up the calculation and cascading ensemble could reduce the potential bias  The code with the submission serves good as a supplementary for the understanding of general audiences.   Weakness: One of the claimed contributions of this framework is improved scalability. It would be reasonable to theoretical analysis what is the converging speed improvement compared with the former approach. The corresponding experiments regarding this linear scalability are not conducted and this should be added in the final version. How good is the median estimation in Theorem 2, is there any degenerate cases caused by this approximation? The corresponding analysis is missing. Regarding experimental results, the performance in terms of the error rate looks promising. However, the plots shown in Fig 2 need the explanation to highlights the advantage of using random projections. It looks like the curves are struggling with converging at the early stages and various projection dimensions are not that different in terms of convergence.  The appendix has quite a few typos that need to be fixed. e.g. missing references, inconsistent notations (regarding dual norm), etc  Quality: Overall, the writing quality is very good, the main idea is clearly presented. Clarity: The notations are simple and well-organized. The presentation has a few issues, but the appendix could be improved by fixing typos. Originality: It is an incremental work with very considerable novelty. Significance: Theoretical proofs are novel, experiments still have space for improvement. 