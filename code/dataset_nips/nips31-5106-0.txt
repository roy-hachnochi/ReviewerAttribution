This paper studies linear stochastic bandits with heavy-tailed payoffs. Heavy-tailed means that the distributions have finite moments of order 1+epsilon for some 0<epsilon<=1. When epsilon =1, the problem degenerates to the light-tailed case with finite variances, which is the well-studied case. The standard stochastic bandits with heavy-tailed payoffs have been well addressed by Bubeck et al. 2013. The regrets of Bubeck's algorithms can match the bounds in the light-tailed case in standard stochastic bandits. However, the existing algorithms for linear stochastic bandits with heavy-tailed payoffs are suboptimal, because their regrets do not match the bounds in the light-tailed case in linear bandits. So this paper fills the theoretical gap by providing a lower bound and two algorithms that match the lower bound within logarithmic factors. I think this is a very interesting work to our community.  Though the designs of algorithm are based on the standard ideas, median of means and truncation, that are well-studied in the literature, the authors find a more efficient way to explore than the baseline algorithms. However, I have some general questions.  1. How do you compare your algorithms? MENU vs TOFU? In terms of regret and computation complexity. 2. Why not compare all the 4 algorithms in all the cases?  3. Why there are only 10 independent repetitions in the experiment? Because of the heavy computation? 4. MENU and TOFU seem to take more memory cost and computation time than MoM and CRT. Can you provide some bounds on the computational complexity?  ====== after rebuttal ===== My concerns are addressed.