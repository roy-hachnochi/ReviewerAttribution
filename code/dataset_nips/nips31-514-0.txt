The paper presents few results on the concentration rates of the posterior distribution of the parameters of ReLU networks, along the line of the ones already available in a non-Bayesian setting.   My overall score is due to the fact that, in my view, the organisation of the paper is not clear. The new results are jut mentioned at the end of the paper, with few comments, the prior distributions are chosen without cross-references to any known rule. The introduction present a description of the literature, without a presentation of the problem, which is given only in Section 2 (i.e. after the introduction). The authors would like to present the results as applicable to more deep learning techniques, but lack in showing why they are generalisable to cases different from ReLU networks.  From the point of view of the clarity, there are a lot of acronyms and mathematical symbols which are never defined, making the paper difficult to read for someone who is not familiar with the subject: since the focus of the paper is between deep-learning and Bayesian nonparametrics the reader possibly interested in it may be not familiar with part of the literature/notation. This point is particularly important given my previous comment on the fact that the definitions are given in a way which is not linear. 