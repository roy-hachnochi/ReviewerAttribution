This paper is about a novel method to robustly train deep neural networks when datasets contains extreme noise in the labels. The idea is quite simple and elegant. Based on the fact that neural networks have the "memorization effect" (i.e. the ability to predict correctly to some extend before being losing accuracy due to the memorisation of the noise), the approach exploits it to teach two neural networks concurrently by selecting only R(T) samples on each mini batch, which have a small loss and are probably well classified. The choice of selecting only R(T) istances is well motivated, as it is the need for using two networks. The idea of using two networks is similar in spirit to the classic co-training and has connection to the Decoupling approach [19] but it is applied to the task of training networks with noisy labels. Experiments are performed on MNIST, CIFAR10, CIFAR100 and show that the approach is superior to several state of the art approaches.  Strengths: + Real data is typically noisy, so this line of research is of high interest and related to many applications of deep neural networks. + The paper is very well written, very ease to read, complete. I found it to be very interesting. + The method works in general better than the compared state of the art works, especially when high level of noise is present. + Experiments are consistent and well performed.  Weakness: - It would have been interesting a test with a bigger, state of the art network for classification on ImageNet dataset. Even AlexNet would have been interesting to see. - The hypothesis that the noise parameter tau is fixed and known is limited. It would have been interesting to have at least an experiment where the value is tested with "wrong" values such as too low or too high.  Easy fixable issues: - Table 1: CIFAR100 has 100 classes, not 1k. - Table 2: "large noise" -> "large class" - row 139: NIVIDIA -> NVIDIA - row 183: 87.53 % when in table 3 is 87.63 %. - row 189: pair 20% while Fig 3 (a) is pair 45%. - row 189: "higher accuracy MentorNet" -> "higher accuracy than MentorNet" - row 192-3: as they are methods *that* do instance *selection* during training.