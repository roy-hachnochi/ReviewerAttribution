UPDATE AFTER READING THE AUTHORS' RESPONSE: I'm pretty happy with the authors' response -- they provide some reasons why the proof technique is novel, and why the image processing example is noteworthy. Unfortunately, the author response is also necessarily brief, and to be really confident, I'd have to see the revised paper. So, I'm not increasing my score. But subjectively I feel better about the paper now.  Summary of the paper: This paper addresses the problem of recovering a pair of sparse vectors from a bilinear measurement. The authors propose a convex programming algorithm for this task. In the case where the signals are sparse with respect to random dictionaries, they prove that their algorithm succeeds with a near-optimal number of measurements. They develop a fast ADMM-type solver for their convex program, and apply it to an image processing problem (removing image distortions).   Overall, I think this paper is interesting, and technically impressive. However, note that this paper is building on a line of recent work on bilinear measurements and phase retrieval, and it appears to be more of an incremental improvement. (For instance, it appears that the convex program is a variant of the recently-proposed BranchHull program, augmented with L1-minimization to recover sparse signals.)  Some specific strengths and weaknesses of the paper: The recovery guarantee uses some very nice probabilistic techniques, such as Mendelson's small-ball method. However, it only addresses the case of random dictionaries, which is mainly of theoretical rather than practical interest.   On the practical side, the paper makes a nice contribution by developing robust versions of the convex program that can handle noisy data, developing a fast ADMM solver, and applying it to an image processing task. One weakness is that there is not much explanation of that image processing task (for instance, what class of image distortions does one want to remove in the real world? and how does the authors' method compare to other methods for doing the same task?).  Suggestions for improvements to the presentation: In Theorem 1, can one say more about how the constant C_t depends on t? Section 4 (the proof outline) needs some polishing. In particular, the \oplus notation is not explained when it appears in Lemma 1; it is only explained much later, in a footnote on page 8. The sentence beginning on line 217 is confusing; a better explanation is on line 222. In Lemma 2, the inequality involving L has an extra \tau, compared to the inequality that appears in the proof on page 13 of the Supplemental Material. Also, in line 258, when discussing the bounds on the Rademacher complexity C(D) and the tail probability p_\tau(D), it would be helpful to state the bounds in the main paper, even if the proofs are deferred to the Supplemental Material. Finally, there are several typos and minor grammatical errors throughout Section 4. 