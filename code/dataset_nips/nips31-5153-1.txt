Summary:  The authors show that an artificial neural network (ANN) with infinite width can  be interpreted using Neural Tangent Kernels (NTK). Using the NTK, they show that ANN converges faster on a few large principal components of the NTK  which they argue is a motivation for early stopping.   Quality:  Quite frankly I had a hard time understanding all the mathematical definitions related to kernels. I cannot do better than make an educated guess.  Clarity:   The motivation, theoretical explanations and experiments were well explained in general. Unfortunately I could not understand it in details.  Originality:   I am not familiar enough with the subject to evaluate the originality.  Significance:   The final results and assumptions related to skewed distribution of eigenvalues of the input data with respect to the NTK seems related to similar observations made with respect to the Hessian [1]. Due to my lack of understanding of the  theory behind the NTK I cannot tell whether those are truly related or not.  [1] Karakida, Ryo, Shotaro Akaho, and Shun-ichi Amari. "Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach." arXiv preprint arXiv:1806.01316 (2018).