Update: I am getting a better understanding of this paper after a second look, with the help of the rebuttal and the discussions. I think this is a good paper and am changing the evaluation accordingly.  The phenomenon this paper pointed out is rather surprising. In particular in the square loss case, it says gradient descent acts like a linear regression, in which all bust the last layer barely moves, and the last layer tries to learn the best linear combination of the (almost fixed) hidden representation.   I did some experiments and find that this phenomenon is indeed true -- fixing a time horizon T and letting the dimension of intermediate hidden layers be large, the last layer changes more significantly than all other layers. This is really interesting and maybe the authors can bring this fact in a stronger tone.  About the relationship with Mei et al., I agree with the rebuttal -- the views are largely complementary. While both focuses on wide neural networks, Mei et al. identifies the exact dynamics for one-hidden-layer nets with a fixed size, while this work assumes the size goes to infinity but is able to characterize arbitrary deep nets. ---------------------- This paper proposes a Neural Tangent Kernel, a model of the gradient descent dynamics of neural networks in the function space. In the case of linear parametrization, this is a kernel gradient descent with a constant kernel (the tangent kernel). In the non-linear case, this result cease to hold, but this paper shows that the neural tangent kernel converges, in the infinite width limit, to a fixed kernel over any finite time horizon. Consequently, the training dynamics of a very wide neural net can be similar to that of a least-squares when it uses the square loss.  I feel like this is a solid piece of theoretical work but am not quite sure about its connection to (and significant among) concurrent results. In particular, does the limiting invariant kernel say something similar to "no spurious local minima" or "no bad critical point / essential convexity" in the infinite-n limit? One possibility is to compare with Mei et al. (2018), which builds on the observation that one-hidden-layer net is a linear function of the empirical distributions of weights, and thereby the square loss is convex. This empirical distribution is a good "function space" for one-hidden-layer nets. As both papers shows some sort of "linear regression"-like scenario in the infinite-width limit, I wonder if any interesting interpretations / connections can pop up from here.  I also find it hard to pin down some technical details about kernels, especially Section 3.0 about functional derivatives and kernel gradients (perhaps not easy anyway due to the space constraint). But it would still be great if this part can be made more approachable to non-kernel-experts.   Reference: Mei, Song, Andrea Montanari, and Phan-Minh Nguyen. "A Mean Field View of the Landscape of Two-Layers Neural Networks." arXiv preprint arXiv:1804.06561 (2018). 