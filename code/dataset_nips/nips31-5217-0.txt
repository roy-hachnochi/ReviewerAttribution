Summary: This paper argues for the use of randomized prior functions for exploration in deep reinforcement learning. The authors show that some of the existing approaches do not satisfy some basic sanity checks even in a linear setting. They then show that randomized prior functions do satisfy these sanity checks in a linear setting, and present experimental results on simple simulated domains to argue that the intuition behind these results extends to the non-linear setting.  Quality: My main concern with this paper is not very self contained. Many important details are pushed to the appendix (including a fully spelled out algorithm), and/or the reader is pointed to referenced papers to understand this paper.  Clarity: While the paper is well structured and easy to read on a macro level, many details have been omitted and/or pushed to the appendix which breaks the logical flow and required me to re-read many paragraphs.  Originality: Although randomized prior functions are not new, this paper presents new arguments regarding the logical inconsistencies of some competing methods, and results on the success of randomized prior functions for exploration in RL.  Significance: I believe that the algorithms and arguments presented in this paper will inform future research, not only theoretical but also RL applications.