The author(s) present a multilingual topic modeling approach based on anchor word based topic models with provable guarantees (see Arora et al 2013). The author(s) make two main contributions, first, they propose a greedy approach to find multilingual anchor words to match topics over different languages. As a second step, the author(s) study the effect of using humans to interactively improve the model using a human-in-the-loop approach (see Hu et al 2014).   The paper is good and interesting. The experiments are reasonable and are well motivated. This is also the case for the proposed approach to identify multi-language anchor words, an approach that seems to beat previous approaches to dictionary-based multilingual topic models. The paper is clearly written and easy to follow. The only question is how big the contribution is.  Some minor issues: a) The author(s) motivate decision and the use of anchor words for increased speed. At multiple places, this is motivated by natural disasters where time is of the essence (see line 2-3, 23-24, and 214-215). I agree with the author(s) that speed is important, especially in an interactive situation, but I can’t see the connections to disasters or similar situations. Nothing in the experimental evaluation connects to this situation (where Wikipedia and Amazon are analyzed). I think the author(s) need to motivate this with a reference where this kinds of models actually have been used in those situations or remove/tone down that argument in the paper. b) At line 10-11 the author(s) argues that the approach is used to correct errors. This is not shown in the experiments in any way, what is shown is rather that the users can improve the models. c) At line 26-27 the authors propose the anchor words as a way of incorporating knowledge into models. The paper cited (10) focus, to my knowledge, on anchor word for the purpose of separability. Especially since the identifying the anchor words is done using a greedy algorithm rather than using human knowledge. But I may be wrong. d) At line 123: I think the definition of d should be in the text, not as a footnote. Without a definition of d, equation 4 does not make any sense. e) One of the main evaluation metrics used in the paper is the classification accuracy for a 6 label classification task using an SVD classifier (see for example line 226-229). This evaluation metric will actually measure how well the model can learn topics that can represent these labels. This is not the same thing as how good the model is. I think it would be good to have perplexity or some similar measure that explain how well the different model fit the data. f) The issue of classification accuracy also affect the experimentation results with the 20 users. It is not clear from the paper if the users knew the 6 labels that they were classifying. If they did know, that would be a different task than if the didn’t know, so I think this needs to be stated. I think this is partly mentioned by the discussion on "adventure" at line 258-261. g) One issue with the experimental result is that it is not clear how often the MCTA topic model did not converge. The author(s) used models that had not converge for 30 minutes. That make it difficult to assess how good the baseline actually is. I think it would be good to know how long it would have taken for a MCTA model to converge for the given data to assess how fair the comparisons are. h) The sentence “We could train a neural network to predict labels for Chinese documents” at line 256 is a little difficult to understand. It is not obvious what the author(s) want to say.   Typos: i) In equation 3 I guess it should be \mathcal(G) under the summation sign. Also S_k is not defined anywhere. ii) At line 113, “inperfection” should maybe be “imperfection”. iii) Reference 4 is missing the publication year in the bibliography.   Quality:  The paper is in general of good quality with the experiments showing positive results. The main questions concern issue e, f and g above, that concern the choice of classification accuracy as the main evaluation metric and how the time limit of 30 minutes for the MCTA model affect the experimental results.  Clarity: The paper is very well written. It is clear what the authors do, the approach/method and how the experiments has been conducted. There are some minor issues that can further improve the clarity of the paper.  Originality:  I may have missed some reference, but I do think the idea of multilingual anchor words are new.  The basic ideas are built upon known techniques (anchor word topic models and interactive topic models). I generally think that the originality is the weakest part of the paper, since the additional contribution in the form of identifying multilingual anchor word is not a very large contribution as such, even though it is nice.  Significance: The significance of the paper is a little hard to judge. The paper is well written and the results from the experiments are good. The idea of multilingual anchor-words are also straightforward to use so that approach may very well be used in further work using topic models with provable guarantees.  UPDATE AFTER REBUTTAL: I think the rebuttal clarified some of the issues with the paper. But I'm still critical with the focus on classification accuracy. In the rebuttal, the authors clearly state that this is the goal: "However, ideally we want topics that are interpretable and can improve classification accuracy." and "Our goal is to create interpretable topics that can also act as additional features to improve accuracy in classification tasks.". But at the end of the rebuttal, the authors conclude that classification is not the best approach to for topic model usage: "We want to clarify that MTAnchor is not a tool solely for finding features that can optimize classification performance. In that case, a neural network should be used."  So this makes it a little confusing what the purpose of the paper is. Even though I think the basic idea of multilingual anchor-words is good (as an example to explore multilingual corpora), the focus on classification as the goal weakens the paper.  Also, the criticism of the argument for disaster relief is not discussed in the rebuttal, although raised by me and reviewer 3. This affects the question regarding the convergence of the MCTA. If it just takes 4-6 h to get the model to converge, the question is how well a converged model would behave as a baseline. Without knowing this in Table 1, I'm not sure Table 1 is a fair comparison, especially if there is no motivation why the models only have a computational budget of 30 minutes.  Finally, the authors point to Chang et al (2009) as an argument for not using perplexity. But perplexity still shows how well the model fits the data, even though this is done by the model in a, from a human perspective, incoherent way. So including perplexity (or similar measures) would strengthen the paper even though perplexity won't say how coherent the model is. 