The paper focuses on an important problem in multiagent learning - non-stationarity introduced by other agents. It proposes a novel rectified belief model to overcome the problem of indistinguishability with miscoordinated policies and combines a few ideas made popular by neural networks - sharing weights and distillation. This results in an extension of the idea of Bayesian Policy reuse, originally formulated for transfer learning and later extended into BPR+ for online learning, which the paper terms Deep BPR+. The paper tests the efficacy of their approach on relatively small tasks and finds that the proposed method can perform quite close to an omniscient one.  The paper clearly traces the origin of its ideas to BPR and BPR+ algorithms and the limitations it's trying to overcome. It comes up with a novel approach to a belief model about other agents using both the reward signals and the opponent models based on their observations. The paper notices that in the original formulation, the belief model and the performance model are highly correlated and any miscoordination will therefore result in 0 reward and remain indistinguishable from other failures. Although it introduces another assumption which the paper should clarify - the agents have access to other agents' observations and actions (past sequence of moves) and is not trying to reason from _their_ observations of the other agent's behavior. It's slightly unclear how the thresholding is determined for switching behavior. Moreover, there are still quite a few moving parts, so hopefully the source code to replicate the experiments will be made available. The experiments, however clearly show that reasonable policies are being learned with this method and glad that the paper includes error bars in the figures.  There are a few relatively minor things that are not clearly stated in the paper. For example what's the importance of entropy regularizer in opponent modeling. Or why exactly Deep BPR+ gets higher accuracy rate with faster switching. Is it just because of the distillation? Moreover it's unclear from Figure 4, how the error bars look for these methods, how many runs and seeds were used to compare. Since all experiments were done on gridworlds with only 2 agents, it's hard to say how well the proposed methods generalize to other scenarios. Also it's not clear what's meant by "coexisting agents that can adaptively change their behaviors based on their observations". I assume it's the general multi-agent learning scenario where the agent behaviors are continuously changing over time.