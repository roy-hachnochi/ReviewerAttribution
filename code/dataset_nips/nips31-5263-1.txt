## [Updated after author feedback] Thank you for your feedback. Your discussion about the uncertainties on the experiments makes sense and you have a good point. Standard deviations on the mean and median would indeed not make much sense. I would still have preferred if you had performed a cross-validation for each dataset-method pair in Appendix F, but I acknowledge that this will be computationally very demanding.  Thank you for including a discussion of Eq. (9). It is insightful and I hope you will include it in the paper. It does, however, not provide the motivation I was hoping for and it is still not clear to me how you came up with the proposed basis in the first place. The choice clearly works, but I see the lack of motivation as the weakest point of the paper. Including a discussion on how you found the particular form of the basis will make the paper significantly stronger and it might spark further developments. I genuinely hope you will add it.  Based on the reviewer discussions, I have decided to reduce my score from 9 to 8. The reason is alone the missing motivation for the proposed basis. The origin of the key contribution of the paper is a mystery, and it should not be so. Still, I strongly recommend an accept of the paper.  ## Summary The paper presents an extension to the Decoupled Gaussian Processes formulation of Cheng & Boots (2017), which addresses optimisation difficulties with the original method. The authors propose a new decoupled basis for the mean and covariance that extends the original basis with an orthogonal part, capturing the residues missed by the original method. The authors go on to show that the proposed basis leads to the existence of natural parameters and derive gradient update rules for these. This allows for using natural gradient descent to optimise the model, which is shown to outperform the original method, both in terms of convergence rate and performance on regression and classification datasets.  ## Quality The paper is of high technical quality, with adequate details in the derivations to follow them.  It is very good to see that the performance has been evaluated on many (12) datasets. However, I urge you to also perform a cross-validation and report mean and standard deviation across the folds. At the very least, you should report the spread of the performance metrics across the datasets in table 1 and 2. The values are all very similar, and it is difficult to judge whether your method is significantly better. It is a shame that such an otherwise strong paper misses these key metrics.  Since the proposed method can represent the mean and variance with different amounts of inducing points, it would be interesting to see the experiments evaluated with a metric that also uses the variance, e.g. NLPD.  ## Clarity The paper is very clearly written with an excellent structure. The idea and story are easy to follow, and motivations and comparisons to related work are provided along the way. I very much enjoyed reading it.  Some motivation for the exact form of Eq. (9) would, however, be nice to see. From what follows, it is clear that it works, but how did you come up with it in the first place? Such motivation would be valuable to readers at the beginning of their research careers.  ## Originality As the proposed methods is an extension of a previous method it is, by nature, somewhat incremental. However, the proposed new, decoupled basis and the following derivations of natural parameters and natural gradient descent update rules makes for a solid and novel work.  ## Significance The paper provides a solid and significant extension to a recently proposed method. The work is of high quality and a significant contribution to the field of scalable GP methods.  ## Minor comments Line 285: Table 6 -> Table 1 