The paper provides upper bounds on sample complexity at which the gradients of the empirical loss uniformly converge to the gradients of the population loss.  With uniform convergence of empirical gradients to population gradients, one can now argue that for non-convex optimization problems where the population objective satisfies KL (gradient dominance) condition, any algorithm that reaches first order stationary points of the empirical objective indeed returns a solution that has near optimum population loss.   The results are applied to get sample complexity results for minimizing population loss in the special cases of generalized linear models and robust regression in high dimensional regime and for one hidden unit ReLU network (under additional margin conditions).   The main technical novelty here is that the authors directly manipulate vector valued Rademacher complexities to provide dimension independent bounds on sample complexity. The tools and the results are potentially applicable to wider class of problems.   Comments: - The complexity of the model class \mathcal{W} here is controlled by the norm bounds of the predictor B (and associated upper bound  R on dual norm of inputs x). In this sense, G and R  are key quants of interest in reasoning about sample complexity and explicit dependence on them should be included in the main equations in the theorem and in discussions.  - (comment on future work) In order to bound population loss of solutions returned by first order critical points of empirical loss, we only need uniform convergence of gradients at critical points of the empirical loss and not in the entire parameter space. In particular, for regions of the parameter space where the gradients of the empirical loss are large, we do not necessarily need convergence to population gradients. I wonder if this can be exploited to get stronger upper bounds by allowing the upper bound to explicitly depend on ||\nabla L(w)||.   ----- Read author response