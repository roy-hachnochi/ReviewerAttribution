Summary  A method for learning a classifier robust to label noise is proposed. A new class of robust loss functions is considered to tackle the problem. The contribution is based a truncated and Box-Cox transformed version of the mean absolute loss, which was shown to be theoretically robust to uniform label noise. The proposed method (unlike most previous work) works for both closed and open-set noisy labels. Several experiments support the approach.  Detailed comments  I liked the insight given in formula (5), which can be considered the starting point of the paper argument. The theoretically-backed derivation of the truncated loss is nice -- in contrast with several heuristics proposed in this area. The straightforward application to open set noise is also interesting. I think this is a solid contribution and deserves publication.  A weakness is the need of two new hyperparameters, i.e. k and q. Although, it seems that a default choice worked well throughout all experiments.  It would be helpful to know more about the additional computational cost due to the pruning step. How slower is training? Moreover, I think that some details about the proposed alternating optimisation are omitted. How do you guarantee that the v_i are bounded in [0,1] by using SGD? Do you use a proximal method?  Minors  * line 113: why the fact that CCE is unbounded matter here? And isnâ€™t MAE unbounded as well? * line 143: could you at least show on a footnote the derivation of CCE from Lq when q -> 0 ?