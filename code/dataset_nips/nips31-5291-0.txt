The paper presents an ensemble of deep neural networks (DNN) where the members of the ensemble are obtained by moving away (in weight space) from a pretrained DNN. The process works in a exploration/exploitation fashion by increasing/decreasing the learning rate in cycles. The DNN is trained in minibatches for a few epochs during these cycles and a "snapshot" of the weights is kept when the learning rate is set to its lowest value. Each of the snapshot constitute one member of the ensemble. In addition the paper shows that the minimum loss found by two DNN trained using different random initializations of the weights can be connected by paths of low loss. These paths are not straight in the weight space but the paper shows that they can follow simple routes with a single bend.  I found the paper very clear, easy to follow and with compelling ideas, specially the ones related to the paths between modes. However, I still have some doubts/concerns about it: * The first part of the article is very interesting but raises many doubts about its generality. In this sense, Can the paths between modes always be found? Are the paths always simple and single-bended? How far where the modes of the two networks found? In the case of having many modes, can a path be found joining any of them? I find that there is missing evidence/theory to answer these questions in the article. * Related to the second part, I found that the proposed search, which is marginally different from [9], is not really related to "sec 3 finding path between modes". To see this relation, it would be interesting to plot the path in a similar way to fig2 or to show that a straight path would work differently. In addition, as Fig 3(left, middle line) suggests the followed path is not really through a low cost region as in sec3. * Finally, the following reference, which is very related, is missing: Lee, Stefan et al. Stochastic multiple choice learning for training diverse deep ensembles. NIPS-2016. 