This paper presents a non-negative tensor decomposition method, called Legendre decomposition. The objective function of Lengendre decomposition is convex and therefore, always guarantees global convergence. The relationships between lengendre decomposition to exponential family and to Boltzmann Machines are presented. Experimental results show that with the same number of parameters, Lengendre decomposition achieves smaller reconstruction error compared to existing tensor decomposition methods.   This paper is generally well organized and is not difficult to follow. I consider the proposed method as novel. One important advantage of Lengendre decomposition over existing methods is that its objective function is convex and therefore always converges to global optimum. The experimental results show that the proposed method outperforms the existing methods in terms of reconstruction error.  I have difficulties in understanding the concept of e-flat submanifold, m-flat submanifold, m-projection and e-projection. Therefore, I am not certain whether the formulas in Section 2.2 are correct. Technical content in the remaining parts appears to be correct.  Some detailed comments are provided as follows:  It looks to me that point (1) in line 34-36 is redundant provided point (2) in 36-38, because convex formulation and global convergence already imply unique solution.   In line 64, it should be $v = (i_1, \ldots, i_N)$.  In line 217, why do you choose the decomposition basis to be this specific form? Is it due to the structure of the datasets? Provided a dataset, how should we choose the basis?   In the experiments, could you also present how the proposed method compared to other methods, in terms of running time?  The proposed method consistently outperforms Nonnegative Tucker except for the experiment on digit “1” in MNIST, as shown in the supplementary material. Do you know why this happens? Is there something special in this dataset? 