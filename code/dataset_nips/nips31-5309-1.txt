The paper uses reinforcement learning for automated theorem proving. This is a timely and important topic. The NIPS community probably does not know much about theorem proving and hence the paper may be hard to follow for the average NIPS participant. However, I've long waited for someone to try and use the recent progress we have seen in RL for ATP (which I find much more interesting and important as an application compared to games) and hence I think this paper will make a nice contribution for NIPS.  Having said that I'm still a little puzzled by the description of the features and the state space. I would have liked more details here and an explanation for how these features are used in standard ATP systems. If you're bootstrapping on features that have been proven useful in non-ML/non-RL ATP systems with hand-crafted selection heuristics, this should be explained very clearly. Also, it would have been nice to at least mention in passing results for other standard theorem provers for this standard data set (e.g. E or Vampire) for proper calibration as to whether this relative progress that you report is likely to translate into absolute progress for the field of ATP.  Update: The authors have commented on my two main concerns and if they add the relevant discussions/results to the paper it will be a stronger paper.