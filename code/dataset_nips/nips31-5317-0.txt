The work analyzes fixed-point solutions of the Stein Variational Gradient Descent (SVGD) objective for finite number of particles. It shows that SVGD exactly estimates expectations for a set of functions, the "Stein matching set", determined by the kernel. For linear kernels applied to Gaussian distributions, this set includes all second-order polynomials; thus linear-kernel SVGD estimates the mean and variance of Gaussians exactly. More generally, random features allow probabilistic bounds on distribution approximation.  The results appear both theoretically interesting and of potential practical value in designing inference algorithms. The questions asked are natural (I remember wondering myself about connections to sigma points when reading one the first SVGD papers), and the answers are neat and slightly counterintuitive -- one might have expected that an RBF kernel rather than a linear kernel would estimate Gaussian moments well.   The writing is generally clear, though there are minor typos and grammatical errors that might benefit from proofreading by a native English speaker. I found no obvious technical mistakes, though I did not have time to verify the math in any depth. I appreciate that the authors include basic experimental verification in the supplemental. Overall, this is a solid theory paper that I think many NIPS attendees will find interesting.  nits (incomplete): line 6: 'image of Stein operator' -> 'image of the Stein operator' line 21: 'losses' -> 'loses'