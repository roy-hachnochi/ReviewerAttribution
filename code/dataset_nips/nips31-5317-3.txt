In this paper, the authors study the fixed point properties of Stein Variational Gradient Descent, an nonparametric inference algorithm which updates the positions of a set of particles in the direction of steepest descent of the KL functional, subject to a max norm in a RKHS. Until this paper, properties of SVGD were understood in the limit of a large number of particles; here, the authors study instead the properties of any fixed point of the SVGD updates with a fixed number of particles. They identify that fixed points define an atomic distribution which matches the moment of the target distribution on a set of function they call the Stein Matching Set - more precisely, the images of the elements of the RKHS (evaluated at the atoms of the fixed point) through the Stein Operator. That definition being implicit (understanding the set of functions whose moments are matched requires knowing the values of the atoms of the fixed point), they characterize more precisely for degenerate kernels. They show that for linear features and Gaussian kernels, this correspond to matching the first two moments. Finally, they study kernels defined through random features and find that the fixed points have an error in sqrt(1/n) in kernelized stein discrepancy.  This paper is a very nice read; it is self-contained, excellently written, and provides interesting insights on the behavior of SVGD. While most of the results (barring section 3.4, more complex) are perhaps not entirely surprising, they are novel to me and shed light both on what we can expect of SVGD inference schemes, and more importantly, how to design the features used so that good approximations are obtained on the desired test functions (particularly in the case of degenerate kernels, though they are less powerful approximators). Minor criticism: section 3.4 seems like it contains important results, but is fairly condensed. It makes it both harder to read and, as a result of not being 'contextualized' enough, harder to appreciate: how do theorem 3.5 and 3.6 compare to similar frameworks in statistical learning theory? What was the state of knowledge regarding uniform bounds using these types of features? Was the 1/sqrt(n) rate achieved by other methods, can it be improved upon, etc.? Maybe tightening the section by clearly presenting the main result and its broader significance, and pushing the rest of the section in the appendix would .have been preferable, leaving some room for simple numerical experiments (only found in the appendix so far).  Nevertheless, I think this paper is a valuable addition to the body of work on SVGD, with clear presentation and intuitive results.