The submission #5332, entitled "Statistical Recurrent Models on Manifold valued Data", presents a framework for the fit of Recurrent Neural networks on SPD matrices.  Much of the efforts are spent on deriving the framework for these computations. Particular attention is paid to the estimation of Fr√©chet mean for this type of data, in order to obtain a fast yet reliable estimators.  the the authors consider frameworks where data are motion sequences, the modeling of which requires such SPD matrices, or diffusion MRI, where so-called diffusion tensors are manipulated.  All these data are very low-dimensional, so that the reported algorithmic optimization unlikely to be useful in practical settings.  The experiments show that the proposed approach is accurate in the problems considered and requires fewer iterations for convergence.  My main impression is that the paper introduces such sophistication because it is cool, even in the absence of compelling needs.  The value of the contribution is thus confined to the computational and algorithmic developments The proposed approach seems rather elegant, but it certainly scales poorly with data dimensionality. For instance, taking Cholesky factor has a cubic complexity, which is ignored in the present context where the dimensionalities are small.    The derivations seem sound. Personally, I do not see the point is dealing with deep learning as an aesthetic game, and am rather skeptical about the value of this contribution.  The experiments are not strongly motivated. For instance in neuroimaging, people stopped dealing with diffusion tensors about 10 years ago. Developing in 2018 a testing framework for this type data seems anecdotal at best. Some comment on moving mnist.  Overall, the experimental framework is thus full of weird choices, and seems in general quite unnatural. It is a pity, because there exist real use cases for SPD matrices in some domains.  The paper is somewhat hard to understand. The authors overemphasize some details and do not give the big picture. I had a hard time figuring out what is x_t in eq (3), and o_t in eq (4). One needs to read further to understand that.  Finally, the paper is unpleasant to read because of overlapping lines. The authors have played with negative \vspaces, which is simply bad. 