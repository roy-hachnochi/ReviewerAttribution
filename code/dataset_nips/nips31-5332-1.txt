Overall, I found this work interesting and the direction is good. The paper, especially the introduction, is well written and most of the arguments are reasonable. If my concerns can be reasonably addressed, this could be considered as a paper.   Summary of the work: Geometric perspective and theoretical understanding are both increasingly important in the deep learning field. Since 2017, there has been a lot of interests in a subfield called geometric deep learning. In this work, the authors first identified that: previous works have been focusing on applying these geometric ideas to CNN trained with functions (data) defined on manifold and graphs, there has been a lack of work on applying this geometric perspective to RNN. Then author applied this geometric perspective to a specific type of RNN models called statistical recurrent units (SRU), which was proposed relatively recently and was trained with data from Euclidean space.With a manifold assumption, the authors need to generalized different operations to respect the geometry. The proposed model achieved both training speeding up and a better performance on several tasks.  Comments: 1. A reader may easily find the abstract is too general, which is almost misleading. It doesn't reflect well what the authors actually do in this paper. It promised a general manifold perspective in RNN models, however, all what the authors did was generalizing SRU to SPD matrix manifold. SPD matrices are both interesting and important, but by no means the authors' concrete work can support the claim in the abstract. In fact, for readers familiar with Bronstein et. al. 2017, SPD matrice manifold would not even be the first choice. I highly suggest the authors to make the claim more concrete and reflect what they actually do to make sure the readers' expectation is not set too high. As a result, I was wondering "why SPD?" in the section 2 and then I was wondering "why SRU?" in section 3. In the beginning section 2, the authors claim "... our development is not limited to SPD(n) ...", I haven't found any support. If the authors insist to keep such big claims, please provide at least two types of important and representative manifolds or convey the generality better.   2. Please discuss more about the design principles. In section 2, the authors' choice of the metric and "geometric means" are purely based on the computational cost. A reader may have questions like "What about other properties?". A minimal comparison of different choices is desirable, or please point the interested reader to the right references. Here is a concrete example of the questions: E.g. there are many different metrics for SPD(n), Log Euclidean, Cholesky, power-Euclidean, Affine-invariant, Root Stein Divergence etc. For a computational efficiency, Root Stein Divergence does seem to be a good once. But it doesn't give a geodesic, however, Log Euclidean does. It's very important to discuss these since the actual model proposed in this paper is not very general, the design principles might be more important to the readers so that we can learn something easier applicable to their works. And in return, the impact of the work can be larger.  Another reference should be cited in the section 2 to fill some gaps in the references already cited: Sra 2012, A new metric on the manifold of kernel matrices with application to matrix geometric means.  3. In section 3, this is the place where the authors first mention their work is actually to general SRU to SPD matrix manifold. As I mentioned before, this is too late, the authors should discuss it earlier.  4. Also at the end of section 3, it might be good to have a discuss on how to in general generalize the operations in RNN to manifold valued data.  5. What's the unique challenge in applying the geometric deep learning perspective to RNN models? And what new tools should be introduced to take extra care when using this perspective to RNN rather than CNN. A very important insight in geometric deep learning is to assume the signal is a function defined on some non-Euclidean domain, specifically a function defined on Riemannian manifold or an undirected graph. Also a manifold itself is very different from a function defined on a manifold.  Update after reading authors' reply: I think the response is reasonable and I slightly increased my evaluation score. But, again, I still can not evaluate the choice of SRU + SPD and further the authors should make an effort to show the applicability of the work to the other RNN models. Also the authors should make the claims more concrete in the final revision.