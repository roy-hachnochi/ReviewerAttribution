The Author addresses the tasks of policy evaluation and learning from observational data by directly deriving weights that optimize for balance between the weighted data and the target policy. The proposed approach avoids the shortcomings of the widely-used inverse propensity weighting approach – namely, high variance and inefficient learning – and obtains, on a few toy examples, better results; theoretical consistency guarantees and regret bounds presumably underlie this improved performance.   While the proposed approach seems novel, the paper is very condensed – both conceptually and textually – and hard to read.   The method is said to be “computationally intensive” and, indeed, even using a commercial optimizer, the analyzed examples are very small, both in terms of the number of samples (up to a few hundreds) and features (<10), much smaller than typical real-world datasets, e.g., derived from electronic health records, the paper’s motivating example. Please report run times to demonstrate the extent of this challenge.   Table 2, can you explain the better performance of Balanced vs Balanced-DR? 