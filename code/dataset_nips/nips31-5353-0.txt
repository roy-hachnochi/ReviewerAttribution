I had a hard time understanding the contributions of this paper. This might be partially due to the partial use of SDE notation that I’m not very familiar with, but it is also due to not very clear writing. I’m unsure about the originality of the content, and I am not convinced by the experiments (if I understand correctly what’s going on). I’m assigning a high degree of uncertainty to my score, as it could easily be that I’m missing some essential contributions.  Detailed comments:  How does the proposed approach differ from the prior work of [1,2] etc? Iterative refinement as been proposed as eg Posterior Policy Iteration (as has been in SOC). Just using a continuous time representation (that get discretized later anyways) would not convince me of sufficient novelty. Do the authors want to emphasize that they backpropagate through the refinement?  I do not understand the Mocap experiments. Do they use the inferred controls to actually control the mujoco simulator with humanoid body? Also, I do not know how diverse the data set is. Are there test planning problem that are not in the training data. What is the model that is used? The appendix mentions a locally linear model. How is it parameterized exactly? I am unhappy about the introduction of the model in section 3.1. What’s the prior over z exactly? Is it the uncontrolled (ie u=0) version of the SDE from eqn 9? The videos of the pendulum problem confuse me. What does the left panel show during the swing-up task? If I see it correctly, the rendering on the right does a swing-up, but not the “ground truth” on the right.   [1] On stochastic optimal control and reinforcement learning by approximate inference. Rawlik et al [2] Learning neural network policies with guided policy search under unknown dynamics. Levine, Abbeel. 