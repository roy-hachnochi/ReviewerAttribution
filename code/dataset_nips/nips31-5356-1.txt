Summary: The authors propose a robust MDP formulation where the uncertainty set contains all models that satisfy a constraint based on features extracted through sample trajectories of a reference policy. The formulation allows coupling across state-action pairs such that it can address some of the shortcomings in the rectangular model. An algorithm for finding the optimal robust policy is provided and is evaluated on a synthetic and a real-world problem.  This formulation seems interesting and novel to me, but I have several main concerns, mostly about the clarity of the presentation:  1. Since the major claim is that the formulation allows coupling across state-action pairs, the authors should at least illustrate and motivate how kappa and the features can be used in general, perhaps by expanding section 3.1  2. The author claims that Eq.(5) is more general than rectangular constraints. This implies that all rectangular constraints can be expressed through kappa. This doesn't seem obvious to me, perhaps the authors can show how a general s,a-rectangular constraints (say, uncertainty sets defined with max-norm balls or  non-convex sets) can be translated to (5) through the features.  3. I notice that \bar{\pi} in the regularizer in (5) disappears in (6). It is not obvious why, a little remark on that would be helpful.  4. In the gridworld experiment, it is not clear to me how generalization to unseen state/actions happens through the chosen features. The authors mention 10 features in section B.1. It seems that these features only encode whether an action succeeds but not the next state itself. What is the dimension of phi here? Are the 10 features replicated for every state-action-next-state combination or a nominal model is required? What is the reward function? I assume -1 per step, but then, with N=10 and T=2N, this cannot generate a return of -200.  Overall, the work seems promising but it lacks clarity in its present form. 