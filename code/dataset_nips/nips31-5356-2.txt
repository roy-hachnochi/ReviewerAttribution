This paper presents a procedure for computing robust policies in MDPs where the state transition dynamics are unknown, but can be estimated only through a set of training trajectories.  Unlike prior work, which creates bounds the true dynamics within uncertainty sets on a state-by-state basic, this approach uses a global feature matching approach to constrain the true dynamics.  The paper presents a casaul entropy optimization problem to find a robust policy and a dynamic programming procedure to compute its gradient.  Great paper, pleasure to read, well written and easy to follow.  There are a few important issues that have not been addressed, but I believe the paper stands well on its own as-is.  There is little discussion about how one might go about selecting the state transitition features.  This is obviously important for anyone who wishes to use this approach, though those familiar with inverse optimal control will have some understanding of this.  Of more importance, is the sample complexity both preceived and theoretical.  \tilde c should converge like 1/sqrt(N), but there should come a point (for a particular set of features) where the local uncertainty sets are more accurate.  This is alluded to in the experiments, which are sufficient, but not overwhelmingly compelling.  Belief states (and their discretization) seem like a detail that can somehow be polished or removed as they only appear in computing the gradient, not the final policy.  Perhaps this can be accomplished by solving the saddle-point problem directly.  Minor detail: Some care needs to be given to ensure gradient descent converges for maximum entropy problems.  Without, e.g., L2 regularization on the parameter vector, the weights may not converge (as no solution may exist) even though the policy will usually end up being sensible.