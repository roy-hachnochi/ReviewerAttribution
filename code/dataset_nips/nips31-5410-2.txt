SUMMARY  The paper extends recent works on reversible networks (e.g. RevNets) to RNNs/GRUs/LSTMs. The paper is more interesting than an incremental extension because it explores a fundamental tradeoff between the model's ability to forget using the gating mechanism and the reversibility of its computation. The main contributions of the paper are: 1. variants of GRUs and LSTMs that allow for reversible computing (similar to RevNets), 2. an explicit discussion of the tradeoff mentioned above, 3. an adaptation of the reversible multiplication of Maclaurin et al. for this problem, and 4. empirically matching the performance of standard GRUs/LSTMs with the proposed reversible counterparts while using 5-15 times less memory.    STRENGTHS  - The paper is very clear and well-written. It does a great job of motivating the problem, explaining why it is nontrivial, and presents a sensible solution that works.  - The discussion on the forgetting-reversibility tradeoff, albeit hampered by the confusing example (see below), is insightful and may inspire additional research directions.  - The scheme for storing lost information is effective in practice. For instance, the validation perplexity in PTB LM by a standard LSTM is 73 (Table 3) whereas the one by a reversible version is 72.9 (Table 1) with close to 5 times less memory usage.    WEAKNESSES  - I wasn't fully clear about the repeat/remember example in Section 4. I understand that the unrolled reverse computation of a TBPTT of an exactly reversible model for the repeat task is equivalent to the forward pass of a regular model for the remember task, but aren't they still quite different in major ways? First, are they really equivalent in terms of their gradient updates? In the end, they draw two different computation graphs? Second, at *test time*, the former is not auto-regressive (i.e., it uses the given input sequence) whereas the latter is.   Maybe I'm missing something simple, but a more careful explanation of the example would be helpful.   Also a minor issue: why are an NF-RevGRU and an LSTM compared in Appendix A? Shouldn't an NF-RevLSTM be used for a fairer comparison?   - I'm not familiar with the algorithm of Maclaurin et al., so it's difficult to get much out of the description of Algorithm 1 other than its mechanics. A review/justification of the algorithm may make the paper more self-contained.   - As the paper acknowledges, the reversible version has a much higher computational cost during training (2-3 times slower). Given how cheap memory is, it remains to be seen how actually practical this approach is.    OTHER COMMENTS  - It'd still be useful to include the perplexity/BLEU scores of a NF-Rev{GRU, LSTM} just  to verify that the gating mechanism is indeed necessary.  - More details on using attention would be useful, perhaps as an extra appendix.  