This paper proposed a method to reduce the number of parameters in deep neural networks with low displacement rank (LDR) matrices. It proposed a group of LDR matrices and several general displacement operators. Also, it proposed a method  to learn the low-rank component/operator at same time and gave a bound on VC dimensions of multi-layer neural networks for structured matrices.   The paper is well written although the terminology might be hard to follow for some readers.  The method is a generalization of several previous works on LDR framework for DNNs and the proposed algorithm of learning low-rank component together is somewhat intuitive. However the theoretical analysis on complexity is original to my knowledge and the empirical study is extensive.  Overall it is above average and I suggest for accept.   Comments:  Authors proposed many kinds of LDR matrices and gave a bound on VC dimensions with LDR matrices on neural networks, however, what is the advantage of learning both operator and the low-rank component at same time? Can you show the advantage with VC dimension?   To my understanding, most training algorithms with LDR weight matrices are not scalable as they are at most up to BLAS level 2 operation. What's the computational efficiency with GPU of these different displacement operators?  Detailed comments:  -- Figure(2) sub-diagonal should be [x_1, x_2, ...]