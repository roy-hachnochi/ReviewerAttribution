The paper introduces a rich class of LDR matrices and more general displacement operators enabling learning over both operator and low rank component with joint learning of operators and rank competent.  The paper proves bounds on the VC dimension and sample complexity accompanied by empirical results on compression FC, convolutions and language modeling with significant performance improvements or matching performance on parameter budgets/fixed budget respectively. The authors also show the structured classes have lower generalization error.  The paper clearly shows consistent higher test accuracy/matched accuracy than baselines replacing layers with the compressed operators. They show that learned operators have better accuracy over fixed operator  They show by increasing rank of the unconstrained layers of the neural networks .  Low  sample complexity without compromising accuracy and sometimes outperforming with far fewer parameters.  I think the originality lies in the bounds that the authors prove on VC dimension and sample complexity and the extensive evaluation on the significance and effect of the approximately equivariant linear map A,B that model high level structure and invariance while the low complexity remainder R control standard model capacity.   The theory seems to work although I have not verified all the proofs in detail. It would be interesting to see empirical comparisons  between the structured class of jointly learnable operator and rank LDRs versus other other compression techniques Hashnets, FastFood etc.   In general, the paper is very well written and structured. The authors clearly state their important contributions and justify the claims with adequate theoretical guarantees and empirical results. The work is explained clearly and without ambiguity. A bit more background on related work on LDRs in the context of compression would be good. 