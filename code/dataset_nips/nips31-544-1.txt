This work proposes a deep generative model for learning semantic representations of text. They propose two models, the first can be trained on raw text and is used to initialize the second for which supervision can be used.  The paper is a extension of previous work where their sentence representations are Gaussians and not just vectors.  They evaluate on the Quora dataset and do achieve "state-of-the-art" results, though they do overstate these. They achieve these only when sentences are modeled separately. Also this dataset is not as widely used evaluating sentence similarity - related work often evaluates on the STS tasks, though this dataset is another nice addition to evaluations of semantic similarity. This is also a source of missed references/comparisons in the work (see "Towards universal paraphrastic sentence embeddings" and "Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations"). Therefore it is hard to see how strong their numbers are in comparison to other methods that also use pre-taining and learn representations instead of just a scoring function.  Overall, I like the paper, it is difficult to get these VAE methods to work and it is a contribution to see how strong they can perform on these tasks given they have some advantages (like an interpretation) over earlier methods. I would've liked stronger comparisons and more evaluations (to be comparable with related work) - it would strengthen the paper a lot if they were competitive or better. It would also be nice to see how their unsupervised approach works on its own and how their results are better than other VAE methods that do not represent sentences as Gaussian distributions.