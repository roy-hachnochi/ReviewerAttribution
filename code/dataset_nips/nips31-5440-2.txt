This paper proposes IterVAML, an iterative version of value-function aware model learning for RL where a dynamics model is being fitted to minimize the bellman error during every iteration of an approximate batch value-iteration step.  I think the idea of fitting a model in order to minimize bellman error is a neat idea, and this iterative version makes it a much easier algorithm to implement in practice.   I'm not very familiar with RKHS and metric entropy so I only skimmed the proofs.  It would've been nice to see some toy experiments, perhaps in simple discrete MDPs with linear function approximation, comparing this approach with just learning an MLE model.  On a broader note of discussion, if ultimately you will be using the learned, approximate value function to get a policy, I'm not sure if there's benefit to learning a model and performing model-based RL as opposed to something model-free like fitted Q-iteration. While fitted Q-iteration could be seen in some ways as using the empirical MLE model, it is also more directly fitting a value function. This approach ends up using your data twice, once to fit an approximate model, and then again to fit a value function. Again, perhaps some empirical experiments in toy domains could illustrate the difference.  Overall I liked the idea, and this could potentially result in improved value iteration especially when scaled up in complex domains where complex function approximators are used.  === After Author Rebuttal === I do like the theoretical contribution of the tighter bound. If the authors can provide simple toy experiments, basically to show that their method is not completely impractical, I would be fully convinced. Still, I do think just based on the theory I would still accept this paper.