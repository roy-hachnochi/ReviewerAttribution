Summary: This paper addresses the topical problem of domain generalisation. Given multiple source datasets for training, it aims to meta-learn a regulariser that helps the model generalise to unseen domains.  More specifically, the regularizer to learn is a parameterised function, of which a simple form can be \sum (\phi_i * \theta_i), where \theta is the classifier-model parameter, and \phi is the regularizer-model (or meta-model) parameter. In this way, the regularizer learns to punish the model complexity in order to improve the performance on a held-out domain (domain-(b) in author’s notation) after several SGD updates based on domain-(a), so the trained model is expected to generalize well on unseen domain. Results show good DG performance on PACS object recognition and amazon review sentiment-classification benchmarks.  Strengths: + A technically sound method for achieving DG, with a well motivated, and intuitive underlying mechanism. + The key advantage of the proposed method is that, since the meta-learning part happens in regularization terms, the updating for classifier model is not limited to one-step only. In contrast to [8,20], such multi-step updates will lead to higher-order gradients. + The design of the shared feature+task-specific networks improves scalability. (However it is simple to realise the same modification for [8,20].) + Good that positive results are obtained on two very distinct kinds problems of image and text. And also with distinct architectures (AlexNet, ResNet-18/50, MLPs). + Analysis of weights in Fig2 is interesting.  Weaknesses: 0. Novelty: While the motivation is quite different, the key schema of this submission is still quite close to [8]/[20], i.e., in each iteration, two domains ((a) and (b)) are sampled as meta-train (train set) and meta-test (validation set). This somewhat caps the novelty. 1. Presentation/Explanation: — Eq(2) seems like un-necessary “Mathiness”. Makes it look fancily Bayesian, but then the paper doesn’t then proceed to actually do anything Bayesian. Sec 3.2 could be removed without affecting the contribution. — The key details of the method is explained in L134-168. However particularly Sec 3.3 L139-168 are rather densely/unclearly written and very hard to parse. If I wasn’t very familiar with this topic and methodology, I would have little hope to understand what is going on. Fig 1(b) is too vague to be of any help deciphering this paragraph. Better to spend the extra space of Sec 3.2/Eq 2  explaining everything better. 2. Meta-Train/Train mismatch: Meta-learning setups normally strive to mimic the way in which the method will be used in practice. But here it seems there is a mis-alignment of train/test conditions for episodic meta-learning. The meta-learning is trained “1-1”: working  on pairs of domains (a,b) \in Source Domains by training on domain a, and evaluating on domain b. However, the way in which the regulariser is actually used is “Many-1”, training on *all* the sources. IE: the Meta-Reg parameters are tuned to work with much less data than they are actually exposed to when used in practice. This may lead to sub-optimal outcomes. 3. Competitors: - It would have been good to have some other DG baselines to put the results, particularly on the sentiment dataset, in context. EG: Domain-adversarial, [A] CrossGrad (ICLR’18), [B] CCSA, Doretto, ICCV’17, [C] DICA, Muandet, ICML’13. - For  Tab4, it would be helpful to have a baseline with conventional (\phi_i=constant)  l1 regularisation.  Randomly weighted l1 seems like a weird, and possibly bad baseline.  4. Intuition. While the method is intuitive and appealing at a high level, it would be nice with some more analysis about exactly how it works (i.e. beyond Fig 2), and what are the assumptions it needs. - EG: How works: For example is its effect via reducing overfitting per-se, or via leading the optimizer to find an equally good or better fit that is a more robust minima? (EG: Something like worse train loss vs same or better train loss compared to baseline?).  - EG: Assumptions: Are there any assumptions? For example does it only make sense if all train+test domains used are independently sampled from the same distribution over domains, or is no such assumption necessary? Could one imagine a specific group of source vs target domains that are different in some (possibly pathological) way such that this method definitely goes wrong and exhibits "negative transfer" to the new domain? ( Providing a  synthetic example to break it should not be a reason to reject, but rather a positive to clarify to readers the conditions under which the method is/isn’t expected to work). 5. Related work. Although this paper is focused on DG. The mechanism is to learn a regularisation hyper parameter. It would therefore be good to explain how it relates to the growing body of related work on gradient-based hyper-parameter learning so readers can understand the connections. EG: [Gradient-based hyperparameter optimization through reversible learning. ICML’15;  Forward and Reverse Gradient-Based Hyperparameter Optimization, NIPS’17].   Other minor: - L155: “It is important to note that dependence of \Phi of \theta comes from gradient steps in Eq 4”. Should it say dependence of \theta on \phi?  Typo: Line 158, pictoral -> pictorial Fig.2 Description: regularizaton/ regualrizer -> regularization/regularizer Line 249, intial ->  initial Fig 1 (b) “l steps” is visually similar to “1 steps”, maybe authors can use another symbol. 