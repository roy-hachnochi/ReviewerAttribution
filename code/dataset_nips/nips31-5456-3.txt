This paper considers a type of safe RL where 1) the rewards are unknown + not seen 2) there are expert demonstrations The setting assumes nature can set the weights on reward features such that they are consistent with the expert demos, but that the selected policy will do as poorly as possible. The goal is therefore to maximize the minimum return of the policy.  This also results in the agent being able to learn in environments different from the demo environments: any states that correspond to new reward features, since they could be arbitrarily bad, are avoided.  The paper's contributions are primarily theoretical, but I do like that it shows at least some experimental results in two toy domains.   There are some details missing from the experimental section making the results impossible to replicate. For example, how many experts were used in the experiments? How many demonstrations were given? How close to optimal were they?  Section 2 defines an MDP as having a finite number of states, but experiments include a continuous state task. This should be clarified.  Similarly, when talking about an algorithm that "can find the optimal policy for an MDP in polynomial time" it would help to say exactly what it is polynomial in.   Overall the paper is well written. The sentence on line 120 appears to be missing a verb.  