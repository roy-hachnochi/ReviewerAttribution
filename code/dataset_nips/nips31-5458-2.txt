The paper deals with a very important problem, of having to work with samples that have a small space. The authors formulate a Bayesian Multi-domain learning framework where domain-specific and shared latent representations from multiple domains are learnt to enable learning a target domain with a small number of samples. The application area is cancer subtyping.  The paper is well written and the modeling assumptions are sound.   Comments:  >>>A concern I have is how the model scales - 1) since inference is based on MCMC. How long did the simulated experiments take to run. 2) The simulated and real-world experiments only had 2 domains. Have the authors explored with more than 2 domains. Does the model scale in this setting. 3) How did you handle 14K genes in your MCMC framework - as in did you have to infer variables that had dimensions equal to the number of genes?   >>>In the plate figure, you could color code the blocks that are 'your contribution', given it heavily relies upon Zhou et al (2018). It will also be helpful to place the distributions in the Figure. \alpha_0 and \beta_0 are not given on the plate model.    UPDATE after rebuttal: Thank you for the response and clarifications. It is a nice neat work. You should consider placing your explanations in a supplementary, if accepted/allowed.  