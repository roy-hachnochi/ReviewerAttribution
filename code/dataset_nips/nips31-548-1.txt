This paper examines the problem of tensor regression and proposes a boosted sparse low-rank model that produces interpretable results. In their low-rank tensor regression model, unit-rank tensors from the CP decomposition of the coefficient tensor is assumed to be sparse. This assumption allows for an interpretable model where the outcome is related to only a subset of features.  For model estimation, the authors use a divide-and-conquer strategy to learn the sparse CP decomposition, based on an existing sequential extraction method, where sparse unit-rank problems are sequentially solved. Instead of using an alternating convex search (ACS) approach, the authors use a stage-wise unit-rank tensor factorization algorithm to learn the model. A convergence analysis of the stage-wise algorithm is provided.  The novel contributions of this paper appear to be the the imposition of sparsity on the CP decomposition of the low-rank tensor regression model and the boosting unit-rank tensor factorization algorithm.   Experimentally, the paper demonstrates that the boosting algorithm performs equivalently as an alternating convex search approach and that the boosting algorithm appears to be advantageous in performance time only when the number of features is high. This is further illustrated on a medial data set, where the boosting + ACS approaches both perform similarity in terms of predictive performance + sparsity (with ACS requiring more computational time); comparing the proposed approach to other low-rank tensor methods + sparse regression methods, it appears that there is not always a significant improvement offered in predictive performance + sparsity. If there is one clear benefit offered by the proposed boosting, it is faster computational time.  I read the authors' feedback and thank them for their comments. The t-tests to check for significance in performance on the 5 experimental data sets are helpful to take into consideration. I also do appreciate the big-O computational complexity analysis per iteration.