The paper proposes a new algorithm for the training of multi-prototype binary classifier by solving its convex-concave saddle-point relaxation. Both strong theoretical result and comprehensive empirical study are provided.  More detailed comments:  (i) model size (timing result): the title of the paper is "SMaLL", but the model sizes or timing results are not provided as related works that also experiment on the multi-prototype model (Gupta et al, 2017) did. Only number of selected features is reported, which is not enough when comparing with other types of models like sparse logistic regression (since it only has one vector while the proposed model has multiple vectors).  (ii) algorithmic improvement vs. model improvement: it would be good to have comparison to other optimization algorithms for the multi-prototype classifier, as for now from the results we only see the joint effect of algorithm and model. Is the improved accuracy due to a better algorithm? or a better model?  Is there existing work that learns the same (or similar) model with a different algorithm?  (iii) The data sets are from openML and do not contain common datasets with some previous works such as (Gupta et al, 2017). This makes it difficult to check consistency of the accuracy results with other papers.   Could the authors provide links to some papers that also use the OpenML datasets during the author feedback? And also why those datasets are chosen?  Also, for high-dimensional experiments, it seems only one data set of different settings is used. Why is it the case?