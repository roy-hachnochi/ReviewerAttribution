Standard RL framework often focuses on learning policies that are specific to individual tasks with hand-specified reward functions. This paper proposes an RL framework that jointly learns representations of inputs and policies by practicing to reach self-specified random goals during training. It incorporates unsupervised representation learning into sample-efficient off-policy goal-conditioned reinforcement learning. The proposed model is extensively evaluated on vision-based tasks without access to any ground truth state or reward functions, successfully demonstrating the advantages of the proposed model.  The paper is self-contained, well written and easy to read. The proposed methods simply trains a Î²-VAE over raw sensory inputs and used for 1) to embed the state and goals using the encoder, 2) to sample goals for exploration from the prior, 3) to sample latent representations to retroactively relabel goals and rewards and 4) to estimate distances in the latent space for rewards to train a goal-conditioned value function. Despite its simplicity, models seem to be very effective in various ways: it provides a more structured representation of complex inputs such as images for RL; it allows for the sampling of new states, which can be used to set synthetic goals; it allows relabelling the goals and rewards facilitates the training of the value function with data augmentation; and it provides a well-shaped reward functions over latent representation then pixel-wise Euclidean distance for images.   Evaluation is another strength of the paper. The model is compared against prior model-free RL algorithms over standard benchmarks. The paper also reports on ablation studies and scaling issues.     Line 291-292: "In Figure 5, we see that sampling from the VAE is significantly better than HER or not relabeling at all. Lastly, a mixture of the the VAE and HER sampling performs the best, which we use in GRiLL." Should VAE and HER be swapped? In figure 5, HER seems to be performing better than VAE. Also, restate your intuitions behind why GriLL performs the best here. This should be considered at other places in the Experiment section.  Typos and Other comments:  Figure 3 Caption: Footnote 4 missing. Line 102: E[] Remove spaces before footnote pointers. Footnotes 3 and 4 are missing.  