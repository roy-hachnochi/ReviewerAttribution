         **Summary and main remarks**          The manuscript investigates binary classification and proposes PAC-Bayesian bounds relying on a notion of stability. Stability has attracted a lot of efforts in the past two decades and loosely can be interpreted as "when the input training data changes by a small amount, the resulting algorithm should also change at most by a small amount". The notion used in the manuscript (Definition 1, lines 79--81) is of similar flavor as to what is known as hypothesis stability in the literature (see Bousquet and Elisseeff, 2002), but rather on the learned hypotheses. The authors call an algorithm $A$ $\beta_m$-stable iff the sup over index $i$ of the difference between the algorithm trained on an $m$ sample and the algorithm trained on the exact same $m$ sample but where data point $i$ is an independent copy, is bounded by $\beta_m$ in the sense of the norm induced by the Hilbert structure. This is the strongest notion of stability.          As it is well-known in the literature and also noted by the authors (this is the exact title of subsection 4.3), stability implies concentration. Therefore, requiring that an algorithm $A$ is $\beta_m$-stable straightforwardly yields a concentration result on its empirical risk towards its risk. Such a concentration result is a cornerstone of PAC-Bayes and the authors derive a PAC-Bayesian bound (Theorem 2, page 3) which upper-bounds the discretized Kullback-Leibler (KL) divergence between the empirical risk of a randomized classifier and its risk, by a term of magnitude $\beta_m^2 / \sigma^2+\log(m)/m$. This bound is vacuous whenever $\beta_m$ does not decay as $m$ grows -- the authors remark that for SVM, Bousquet and Elisseeff proved in 2002 that $\beta_m$ is upper-bounded by $1/n$, and the bound in Theorem 2 becomes non-vacuous with a rate of order $\log(m)/m$. I feel the manuscript might be lacking a discussion on which other algorithm satisfies this regime. A single example (SVM) might not be convincing enough.          The manuscript then compares its bound to three others in the literature, and discusses at lenghth its merits. I have found this discussion very interesting. The manuscript closes with the proofs of claimed results and a small numerical experiments section. I have checked the proofs and as far as I can tell they appear to have no flaw.          **Overall assessment**          The paper is well-written and contains interesting contributions. Past years have seen a resurgence of PAC-Bayesian bounds, and stability is a common framework for deriving generalization bounds; however it seems that the manuscript is the first to combine both approaches. My main concern is that the scope of the manuscript might be limited, as only one example of learning algorithm which complies with the stability notion introduced is given (SVM).          **Specific remarks**          - typo: repeated "that" line 33         - Some improper capitalization of words such as Bayes, PAC in the references.         - Lines 292--293: I agree that the bound proposed in the manuscript seems to be the first PAC-Bayesian bound relying on stability. However it seems that Celisse and Guedj (2016) derive PAC (not Bayes) generalization bounds using a notion of stability ($L^q$) which is weaker than the notion used in the manuscript. I would suggest to rephrase the last sentence (and other such claims elsewhere in the manuscript) to avoid confusion, and explicitly state that the manuscript contains "the first nontrivial PAC-Bayesian bound that uses stability".          [Rebuttal acknowledged.]       