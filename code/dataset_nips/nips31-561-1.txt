The paper proposes to solve a new convex optimization problem termed quadratic decomposable submodular function minimization (QDSFM) where they first derive a dual formulation for the same and solve it using Random Coordinate Descent (RCD) establishing linear convergence. The conic projection step in RCD is computed via a modified Frank-Wolfe and min-norm point methods.  The authors claim that their work is the first in solving QDSFM in its most general form. While I agree with their assertion, a lot of significant portion of their work follows from the previous established results in references [11], [13] and [14]. For example, the linear convergence of RCD is a known result. The key contribution of this work appears to the observation in equation (5) on which the dual formulation hinges upon.  The authors cite references [4], [16] and [17] and state that employing regularization with quadratic terms offers improved predictive performance compared to p=1. On reading these citations it is not clear whether it is true only for the specific Lovasz extension considered in equation (10) or for the more general objective in equation (3) assuming the general convex f_r(x).   Few minor typos: 1. Definition of y(S) in equation 2 is missing 2. The definition of f(x) in line 69 should be f(x) = max_{y \in B} <y,x>  ---------------------------------------------------------------------- The authors feedback addresses my concern regarding the contributions of this work. I agree with the authors that the weak convexity result established in Lemma 4.2 is new and an essential ingredient for their convergence results. However, the authors feedback do not provide examples of other decomposable submodular functions where their set up can be used beyond those considered in equation 10 which I believe is also a concern raised by Reviewer 1. I also do not see any comments on the quality of the rounded solution if we treat the problem as a continuous relaxation of the discrete problem.