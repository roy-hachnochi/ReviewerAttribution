Summary: This work proposes a generic class of optimization problems called quadratic decomposable submodular function minimization(QDSFM). They are helpful in solving page ranking, transductive learning problems. A dual is derived, which is solved using coordinated descent algorithms such as Alternating Projection or Random Co-ordinate Descent and the inner loop involves projection onto a cone. They have shown convergence guarantees and support their claim with experiments on synthetic and real datasets.   Positives: -  Overall, the paper is very well structured - The QDSFM is a novel contribution to literature, in my view. The algorithms, however are similar to the ones used to solve DSFM problems. -  I have checked the proofs and the proofs seem correct. - They support the claim with synthetic and real data.  Weaknesses/Requires Clarification: - Typos in the proof of lemma 4.1. They forgot the replace s by a. Similary, in the rest of the paper, it would be nice if the variables are used consistently. - In my practical experience, even for DSFM, the convergence is guarenteed only if the inner loops are exact projections. I believe, FW and MNP might not be useful in this case. There can be problem instances the approximate projection in innerloop does allow convergence to optimal. Could you add some comment/results for the QDSFM case. - It would be really helpful if a spell checker is run because there are some typos in the paper. (eg: Muchroom in line 253)     Update after Author response: I agree that FW is approximate and MNP is exact. I apologize to have mentioned MNP as approximate. However, what I meant was FW is less expensive but approximate while MNP is exact but its complexity depends very much on the function. Generic MNP oracles are expensive. However, it does make sense that we need exact oracles in the inner loop for convergence of the outerloop.