The paper proposes a nice algorithm for risk-aware reinforcement learning. The original objective function is reformulated through Fenchel dual. This formulation gets rid of the two-sample issue in the original formulation. Finite sample analysis is also proposed. I think it is a nice work.   In RL, the independent sampling issue arises in different settings. For example, in policy evaluation, when using MSPBE as an objective, the gradient method would also derive sth like the product of two expectation terms. The most common strategy so far is to use two time scale updating, where usually two updating rules need to be updated at different speeds and could be very sensitive. This work is an important first step, though the Block coordinate method still requires two step-sizes. I think it should be easier to tune (i.e. maybe same step sizes for both). From theoretical perspective, at least the work is able to provide a finite sample analysis, which is not common in the big RL area.   And, this work provides a new perspective to resolve the two time scale problem, though it is in risk-aware RL, it could be a good reference for other settings.  However, I still think careful designed experiments in classical RL domains can be added to make more people feel interesting. Particularly, for a new algorithm, it is better to use simpler domain to let first-time-readers know what the domain's difficulties are and make it clear about how well the algorithm work with a thorough parameter sweep.  