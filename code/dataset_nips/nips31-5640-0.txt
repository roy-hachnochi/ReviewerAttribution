The paper shows the importance of the used training setup for MAML and RL^2. A setup can include "exploratory episodes" and measure the loss only on the next "reporting" episodes.  The paper presents interesting results. The introduced E-MAML and E-RL^2 variants clearly help.  The main problem with the paper: The paper does not define well the objective. I only deduced from the Appendix C that the setup is: After starting in a new environment, do 3 exploratory episodes and report the collected reward on the next 2 episodes. This is not explained by the Equation (1). The paper would be much clearer if mentioning the exploratory episodes.  The paper has a potential to help people to design the right training objective.  Comments to improve the paper: 1) Mention early the dependency of the update operator on the exploratory episodes. 2) If you explicitly define the objective, the RL^2 on the right objective will become the same as E-RL^2. 3) You should discuss the important design choices, when designing the objective or its approximation: 3.1) The small number of "exploratory" and "reporting" episodes may be just an approximation for the large number of episodes seen during training and testing. Some high-hanging exploration would appear beneficial only if having many reporting episodes. 3.2) The reporting phase should start with a new episode to prevent the agent placing a bag of gold on the ground before the end of the exploratory phase. 3.3) The length of the exploratory phase should not affect the discount used during the reporting phase. Otherwise the agent would like to end the exploratory phase early. Reconsider Equation (11). 3.4) The length of the exploratory phase should be limited to avoid exploring forever.  Update: I have read the rebuttal. Thank you for considering the clarification improvements. I hope that you will specify the loss function as a function of the exploratory and the reporting episodes. Only then it makes sense to show that the other approaches are not minimizing the loss function directly.