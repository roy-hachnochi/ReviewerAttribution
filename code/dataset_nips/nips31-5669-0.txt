This paper proposes a decomposition of the pre-activation prediction (the values of the last intermediate layer in a NN) into a linear combination of activations of the training points. The weights in this linear combination are called representer values. Positive representer values represent excitatory signals that contribute to the prediction of the particular sample in the corresponding class, while negative representer values inhibit the prediction to that particular class. The representer values can be used to better understand the prediction of the model.  The experimental section shows how this technique can be used in several explanatory analyzes, such as:   * Data debugging: for a MNIST dataset, consider some of the labels   being interchanged in the dataset (in the example used some 1s   become 7s). With this corrupted data, the binary prediction   (distinguishing 1s from 7s) has a low accuracy of 85%. Using the   representer values, a simulated user checks a fraction of the labels   of the dataset and flips back the labels accordingly. The   experimental results shows that when using representer values to   guide the flipping of the label, the accuracy of the classification   recovers faster (i.e., a smaller fraction of the data being checked   leads to higher classification) than when using other methods for   guiding the flipping (loss-based, random and influence functions).  * Positive and negative examples: This section compares the positive   and negative examples given by the proposed method and the influence   functions. The proposed method seems more consistent in providing   the right type of examples.  * Understanding misclassified examples: For a few misclassified   examples of antelopes, the representer values are used to inspect   the training samples that inhibit the correct prediction. An   interesting discovery is made: the highly inhibitory training points   contain an antelope together with other (larger) animals and the   true label associated with these points is the other animal.  Overall I really enjoyed reading this paper, specially the experimental section. The technique introduced seems to be useful in all kind of debugging experiments. My favorite one was the understanding of misclassified examples. In addition, it seems to perform slightly better than influence functions and it's significantly faster to compute.  I am curious how this method could be extended to tasks that are not classification tasks. For example, to NLP tasks that detect spans in a sequence of text. Also, is it feasible to include the computation of the representer values in some type of library or within some of the popular frameworks such that it becomes a tool readily available?  Nit-picks: * Are citations 13 and 14 swapped? * Line 91: do you mean a generic yi or only the yi the predicted label   of xi (I think it's the former and maybe use a different index for y   to denote that it's a generic - let's say - yt)  UPDATE: After rebuttal and discussion, I'm lowering my score. Some more feedback: Quantitative results in Fig 3 When I looked again at the results presented in Fig 3, I'm not sure I understand the correlation between the two graphs in that figure. Let's look at 0.15 on x axis. Loss (green) and Influence (blue) manage to flip a higher percentage of the mislabeled samples than random (red) (graph on the right); however, retraining leads to higher accuracy for random (red) than loss (green) and influence (blue) (left graph). Counterintuitive to me. What am I missing? In particular, random being better than influence is counterintuitive. In general, as a reader, I expect the authors to discuss the results obtained, not only stating: here it is, ours is better.   I was enthusiastic when reading this paper as I think we need more work in AI that addresses explainability, model debugging and data set debugging. This paper introduces such a method. After reading the other reviewers' comments (in particular R2), I agree that the interpretation of the negative vs. positive examples is subjective and neither the paper nor the rebuttal manage to quantify the importance of the positive/negative representers.   I was thinking about the following experiment: pick a test sample (predicted correctly by the model), sort representer points (both negative and positive), sort them on absolute value, and start removing the top 5%, 10%, ... retrain the model and see when the test sample is misclassified. This would be an indirect measure for the importance of the representer points in correctly classifying the test sample.