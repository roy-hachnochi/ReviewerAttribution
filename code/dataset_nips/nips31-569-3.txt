Paper summary:  This paper studies linear regression with the absolute loss. The goal is to develop estimators that work well when the moments of the data beyond the second-order could possibly not exist. This is the "heavy tail" situation studied in many previous works, but not for linear regression with absolute loss. In this paper, an estimator based on solving a non-convex optimization problem is proposed and analyzed. The optimization problem uses Catoni's truncation technique, which has been used in many previous works on learning under "heavy tail" situations. A risk bound is developed for the estimator. Another setting where the covariates are also bounded is studied, and it is shown that empirical risk minimization succeeds in that case.  Review:  The estimator proposed is very similar to one proposed by Brownlees, Joly, and Lugosi in their 2015 paper. In the context of regression with absolute loss, the analysis of Brownlees et al require the prediction functions be $L_\infty$-bounded. So the contribution of the present paper is, for linear regression, a modification of the estimator and accompanying analysis that work when the covariates could be unbounded. Technically, in the present paper, the Catoni risk estimate  $\mu$ used in the Brownlees et al objective is (optimistically) dropped from the objective function. This is a simple idea that works out nicely, using fairly simple/standard techniques. I checked the proofs of the main result; they seem to be correct.)  Throughout the paper, the authors make some rather inappropriate comparisons between $\ell_1$- and $\ell_2$-regression, because the goals are rather different. The absolute loss is motivated with the goal of estimating conditional median. The squared loss is motivated with the goal of estimating conditional mean. If robustness was the main concern, but the goal was to estimate conditional mean, then absolute loss would not really be appropriate unless the noise was symmetric. So I think the authors should use some care when discussing the absolute loss and why it is used, and also when making comparisons to other works that are motivated by estimating conditional mean.  The total boundedness of the predictor domain $\mathcal{W}$ is a restriction not present in some of the previous work. It is similar to regularization. The analysis by Audibert and Catoni was very interesting because (among other reasons) it could be applied to ordinary least squares, where there is no regularization.  Theorem 3 is essentially a special case of standard Rademacher complexity results for Lipschitz losses and linear predictors. The only subtlety is that the loss is not bounded, but difference of losses (relative to any fixed predictor) are, under the assumption of bounded $x$'s and bounded $w$'s. Anyway, that is only needed for the concentration part of the generalization bound; the Rademacher part is already known. Note that for $\ell_2$-regression, the same techniques would not apply (and instead one must use smoothness and other properties). Anyway, I don't think this is a major contribution of the paper, but it was nice that the authors pointed it out.  The final comment regarding Lipschitz losses seems to come too late! I think throughout reading the paper, it was clear that the Lipschitz-property of the loss would be the key to making the estimator work. The focus on $\ell_1$-regression felt a bit odd, partly because of the aforementioned issues regarding motivation. I think the paper would have felt stronger if the details would have been worked out for Lipschitz losses.  Overall:    - The main result is interesting and new.   - There is a nice idea in the paper.   - The paper is a bit "light" in terms of the strength of the main result, and the other claimed major result is a bit of an oversell.   - The paper needs a major revision in terms of how the problem and results are discussed.  More comments:    - Display below line 79: I suggest you clarify what norm is being used here (and in the rest of the paper). Also, I think there is an extraneous $|$ around there.   - Line 160: Typo, "converging numbers".   - Remark 3: This is an inappropriate comparison, as discussed above. I suggest either clarifying this point or removing the remark.   - Line 176: Typo, "$\epsilon$".   - Corollary 2 and Remark 4: The big-$O$ used here is hiding some moments. I think if you only care about the rate, then $d$ need not appear; if you care about other things beyond the rate, then you should also care about these moments.