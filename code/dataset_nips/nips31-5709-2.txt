This well-written paper explores and extends “Difference Target Propagation” (DTP), a non-backprop deep learning algorithm for artificial neural networks. The authors suggest two modifications to DTP to make it more biologically plausible, and another modification to improve performance when the targets are class labels. Experiments compare DTP and the modified versions to BP and another algorithm, DFA, on MNIST, CIFAR-10, and ImageNet.  All three of the proposed modifications to the DTP algorithm seem incremental, but they are very reasonable and the experimental results support the hypotheses. The analysis and results are nicely presented.  The other contribution is the experimental results on large data sets. The authors falsely claim that this is the first study to apply biologically-motivated learning methods to CIFAR-10 (Lines 243-245), they should cite Nokland “Direct feedback alignment provides learning in deep neural networks” and Baldi et al. “Learning in the machine: Random backpropagation and the deep learning channel,” both of which experiment with FA and DFA on CIFAR-10. As far as I know, this is the first attempt using such algorithms on ImageNet, but as the authors report, it is difficult get good performance on large problems using new learning algorithms, as hyperparameters need to be carefully re-tuned and the network might need to be larger.