This paper proposes a generative model which builds on ideas from dynamical systems and previous deep learning work like the Kanerva Machine. The main idea is to design and train an architecture that, when unrolled as a dynamical system, has points from the target distribution as attractors.  I found the presentation of the model reasonably clear, but thought it suffered from excessive formality. E.g., the description of p(M) could just say that the rows of M are isotropic Gaussian distributions with each row having its own mean and scaled-identity covariance. The references to matrix-variate Gaussians, Kronecker products, vectorization operators, etc. don't contribute to clarity. Similarly, talk about exchangeability doesn't add much. One could just say that the model treats all observations in an episode as independent, conditioned on the memory M. It's a matter of personal taste, but it seems to me that much of the technical presentation in the paper creeps into this sort of over-formalized style.  The empirical results are difficult to evaluate, since the model kind of operates in its own domain, on tasks for which there aren't obviously useful benchmarks and baselines. Some quantitative results are presented, and the model seems to outperform the Kanerva Machine and Differentiable Neural Computer in terms of memory capacity. Qualitative results on a couple of denoising tasks are also presented, but it's not clear how we are supposed to interpret them.  I think the general direction of this paper, i.e. generative models incorporating memory and aspects of dynamical systems, is interesting. But, I don't think the motivation provided by the presented experiments is strong enough for other researchers to go through the effort of reproducing this work. The paper could be improved by simplifying presentation of the technical content and by experiments exhibiting the benefits of the model in more natural tasks, e.g. as a component in an RL agent. --- I have read the author rebuttal. 