    This paper present a module in neural networks called Gather-Scatter (GS), which is used to provide context propagation in representation learning.    The GS module is very simple, in this paper, average pooling and additional convolutional layers are used to coonstruct the GS module.         The draft is clearly written and well organized. The paper is very clear and the author provided enough training detailes to reproduce the results.    Besides, a series of baseline models and datasets are used to show the effectiveness of proposed module, including ResNet-50, ResNet-101, ShuffleNet, ImageNet, CIFAR-10 and CIFAR-100. Useful ablation studies are presented to help understand the proposed method.           However, the main drawback of this paper is on the novelty. There are strong similarities to recently proposed Squeeze-and-Excitation Networks, and there is no comparison to this work.    Besides, the some analysis is not convincing enough, and it still needs more studies to lead a stronger paper. As a conclusion, I rate this paper as "Marginally below the acceptance threshold".       In the following, the detailed weakness of this paper are listed:       1) The proposed method is very similar to Squeeze-and-Excitation Networks [1], but there is no comparison to the related work quantitatively.    2) There is only the results on image classification task. However, one of success for deep learning is that it allows people leverage pretrained representation.    To show the effectiveness of this approach that learns better representation, more tasks are needed, such as semantic segmentation.     Especially, the key idea of this method is on the context propagation, and context information plays an important role in semantic segmentation, and thus it is important to know.    3) GS module is used to propagate the context information over different spatial locations. Is the effective receptive field improved, which can be computed from [2]?    It is interesting to know how the effective receptive field changed after applying GS module.    4) The analysis from line 128 to 149 is not convincing enough. From the histogram as shown in Fig 3, the GS-P-50 model has smaller class selectivity score, which means GS-P-50 shares more features and ResNet-50 learns more class specific features.       And authors hypothesize that additional context may allow the network to reduce its dependency.    What is the reason such an observation can indicate GS-P-50 learns better representation?            Reference:    [1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, CVPR, 2018.    [2] W. Luo et al., Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS, 2016.      