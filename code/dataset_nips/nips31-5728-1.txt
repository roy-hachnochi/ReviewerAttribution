In this work authors introduce a method for quantifying linguistics shifts on a corpus level which they refer to as  the global anchor method (GAM). This method is based on the already proposed local anchor method (LAM). Unlike LAM which is used for adaptation detection on word level, the GAM generalizes to corpus level adaptation.   Unlike LAM where anchor words are selected using certain heuristics or NN search in the GAM all common words across the two collections are used as anchor words.  In addition GAM also allows for embeddings of different dimensions to be compared which is not possible with the alignment method.   Lastly, unlike LAM, GAM is more optimized and scalable to compute as it uses only matrix multiplication.  Authors also theoretically show that the GAM and the existing alignment method are equivalent.   Overall I think that the work presented in this paper is interesting and the presentation follows a relatively concise and clear path. On the other hand it's a small addition to a very narrow domain. As such I think the the work presented here may be more suitable for an NLP conference or a workshop.   It would be good to perform the empirical analysis of the constant factor relationship using embeddings of different dimensions. It would also be interesting to see a plot as to how the ration changes with the number of dimensions.   What embedding dimensions were used for this study?  For better clarity Figure 2 should have a legend.  “The metrics the two methods” -> “the metric which the two methods” ?