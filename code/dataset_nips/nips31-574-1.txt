The authors propose a differentiable k-nearest neighbors selection rule and, subsequently, a neural nearest neighbors block (N^3 block) to exploit self-similarity in the CNN architectures for restoration.  The paper reads mostly well.  The strengths of the paper are the proposed N^3 block and the improvements over the baseline methods achieved when using N^3.  The comments/questions I have are as follows: a) In the denoising case, the authors use DnCNN as baseline and an interleaved architecture. I wonder how much from the gains over DnCNN are due to the use of more parameters (layers), of a larger receptive field, and larger cropped images (80x80) than DnCNN. DnCNN's performance in the high noise levels is mainly constrained by the 50x50 size patches employed in training and receptive field. b) The validation for image super-resolution is even less convincing. Partly because of the small gains over the baseline and partly because of the choice of the methods, none matching the current state-of-the-art performance level. Again, I'm wondering how much from the gains are due to the use of more parameters/layers and other training choices.  Relevant literature missing: Timofte et al., Ntire 2017 challenge on single image super-resolution: Methods and results, 2017 Ledig et al., Photo-realistic single image super-resolution using a generative adversarial network, 2017 Bae et al., Beyond deep residual learning for image restoration: Persistent homology-guided manifold simplification, 2017 Lim et al., Enhanced deep residual networks for single image super-resolution, 2017