This submission presents an RNN based construction for channel codes with feedback, a classic code-design problem from information theory. The goal is to design error correcting codes with feedback for the AWGN channel where each transmitted real value is received with added Gaussian noise. In feedback codes, we assume that before transmitting each symbol the transmitter has access to all noisy symbols seen by the receiver, and it can use this information to design the next symbol. Furthermore, codes for an AWGN need to be designed so that the average power (two-norm square/length) of each codeword must be below a threshold.   This is a well-studied problem in information theory along with the seminal Schalkwijk-Kailath (S-K) known to be asymptotically rate optimal. However, at fixed blocklengths, the performance of S-K scheme seems quite suboptimal. Moreover, S-K scheme falls short of acceptable performance when the feedback is noisy or delayed. The current submission presents a code design scheme based on RNNs that shows promise in simulations to overcome these shortcomings. The proposed construction is developed in steps, leading to the final construction. We summarise the features of the final construction below:  1. Uncoded bits b_1, ..., b_K are sent (using real values +-A of fixed amplitude A) followed by 2K transmissions with the pair (c_{k,1}, c_{k,2}) chosen to depend on (i) input c_k corresponding to b_k; (ii) the estimated noise y_k - c_k for this transmission; (iii) noise estimates of the previous two transmissions in the second phase y_{k-1,1}-c_{k-1,1} and y_{k-1,2}-c_{k-1,2}. Specifically, a single directional RNN is chosen with the k-th RNN cell generating the input (c_{k,1}, c_{k,2}) based on the observations mentioned above. For the case when the feedback itself is noisy, y_k is replaced by its noisy version \tilde{y}_{k}. Finally, the average power constraint is maintained by normalising the RNN outputs (this is not very clear to me -- do the constraints need to be maintained on average or in the worst-case?)  2. For decoding, the k-th input bit is decoded using the three receptions (y_k, y_{k,1}, y_{k,2}) containing information about it. This decoder comprises a Gated Recurrent Unit (GRU) which is training jointly with the RNN encoder of the previous step, with cross-entropy as the loss function.   3. The first enhancement done to the architecture above is including a zero padding after the message to be transmitted. This is a standard technique in sequential code design that allows one to reduce the error in the final message bits.    4. Next, to combat the unequal error protection offered to the uncoded transmission c_k and the coded transmission (c_{k,1}, c_{k,2}) , the inputs for these information symbols are weighed differently with weights trained empirically.  5. Finally, even the weights (input amplitudes) assigned across different information symbols are set to be different and the weight vector is trained empirically.  This final construction is termed a "neural code construction" and  numerical evidence is provided to show that this outperforms the S-K scheme in error performance by orders of magnitude at blocklength ~150 (corresponding to sending 50 message bits). Furthermore, the gains are enhanced even more when practical variants such as noisy and delayed feedback are included. The numerical results are interesting and add to the belief that neural network based code designs can compete with the classic code designs, which has been pushed by a recent line of work (see Kim et. al. "COMMUNICATION ALGORITHMS VIA DEEP LEARNING" at ICLR 2018).   While the results are interesting and should attract attention from communication industry, who will be happy to focus on a single architectural design that can capture everything from communication to intelligence, I find the contribution incremental. The machine learning tools of RNN used are standard and have already been explored in the past (see for example the ICLR work mentioned above). Further explorations along this theme must be reported in the communication and code design literature where the audience can weigh-in on the nuances arising from considering different use-cases of the same basic idea. Also, from an information-theory point of view, the results presented are unsatisfactory for the following two reasons:  1. Since the paper is presenting results for finite blocklength, they should benchmark their performance against the finite blocklength rate bounds for feedback codes which are available in the literature. S-K code is an asymptotically optimal scheme presented 50 years ago, there is no point in benchmarking against it at lengths as short as 150.  2. The paper extends their proposed codes to longer lengths simply by concatenating it with the optimal S-K scheme. This offers nothing more than the classic S-K scheme; I suspect that many simple list decoding schemes when concatenated with S-K will yield the same performance.  Overall, I recommend rejection since there is already a lot of published work on this topic over the past one year and the current submission offers nothing much over those works, and also, since the performance has not been compared with the information theoretically optimal benchmark available in the literature. On the other hand, I believe that studies such as this will be apt for publication in communication conferences where this theme can now be picked up.   ====  I thank the authors for their response. Other reviewers have convinced me that the observed performance of RNN based feedback codes proposed in this paper is promising and may lead to further advances in this direction. I have upgraded my rating to 6. 