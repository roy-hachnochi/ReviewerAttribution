The paper under review proves implicit regularization properties  of gradient descent applied to multi-layer linear networks. The main results establish that the gradient descent algorithm applied to a multilayer  linear network converges towards a solution which is characterized by the structure of the network.  I think that the results are timely and interesting for the NIPS community.   The presentation of the paper needs to be improved. There are several typos and many sequences  which are not clear. Moreover, some parts of the proofs and the theorems are not sufficiently detailed in my opinion. Some examples: line 374: the following assumption is made in almost every statement: "the incremental updates w(t+1) − w(t) converges in direction", but the authors never discuss when this property holds in practice. It would be interesting to know whether there are e.g. choices of the stepsizes ensuring that this assumption holds.  line 378: \Delta w_t is not defined line 383: why lim t→\infty of \gamma^\nu y_n <x_n, P(w^(t))>/||w^(t)||^\nu > 0?  line 409: in order to understand equation (17), in my opinion it would be helpful to anticipate  the formula after line 409 before (17)