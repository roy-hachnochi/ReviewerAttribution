The paper proposes to learn the structure of Bayesian networks via continuous optimization, where the key contribution is to use the trace of the matrix exponential of the element-wise squared adjacency matrix as acyclicity measure, which can be exploited in numeric optimization. For optimization, a simple and effective augmented Lagrangian approach is proposed.   Strengths: + While the used characterisation of acyclicity is widely known, to the best of my knowledge it has not been exploited for structure learning. Thus the paper is original. + the optimization scheme appears to be natural and effective. + The experimental evaluation shows the efficacy of the approach, delivering convincing results.  Weaknesses: - the d^3 computational cost hinders scalability of the approach, and will likely squeeze a tear out of a practitioner's eye, when applying the method to a large problem. - it is not clear, whether the method could be applied to more classical cost functions, e.g. BDe.  Quality: The key contribution is clearly the continuous characterization of DAG acyclicity via the matrix exponential, which is a very natural approach. It is actually surprising that this has not been done before, so that this paper fills a gap with a convincing approach. One reason why this approach has not been published before might be certain practical difficulties concerning optimization, when using the matrix exponential. Perhaps the authors could comment on this, in particular whether they have tried other, less successful approaches.  Clarity: The paper is very clear and reproducible.  Originality: While the characterization of acyclicity was definitely around in folk wisdom, it seems that this paper is the first to actually successfully use it. Thus, an original contribution.  Significance: While, as mentioned above, the approach probably does not scale well (d^3 computational costs), the stage is open to improve on that with various approximations. It is highly likely that this work will stimulate further research in this direction.    *** EDIT *** I read the authors' response and they have adequately addressed (my anyway minor) concerns. 