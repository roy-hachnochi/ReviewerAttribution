Content: In the field of deep learning a large number of "saliency methods" has been proposed over the last years, that is, methods that try to explain which parts of an input image drive the network's decision for some label. The authors of the present submission propose a few sanity checks that saliency methods should pass as a necessary condition for them to explain anything about the model decisions.  The propose two classes of tests: Model parameter randomization tests and data randomization tests. The first class asserts that replacing all or some model parameters with randomly initialized parameters (and therefore degrading model performance) should change the saliency method's output. The second class asserts that randomly permuting labels in the training data (and therefore forcing the model to memorize the training samples) should change the saliency output.  They apply these tests to a variety of commonly used saliency methods and show that many methods fail in some or all of the tests. Finally they offer potential explanations for some of the failures with the example of simple linear or one-layer convolutional models.  Quality:  In general I think the presented submission is of high quality. The proposed tests are mainly well-motived (but see below), the experiments seem reproducible.  With respect to the data randomization test, the authors claim that an insensitivity to the permuted labels shows that the method doesnot depend on the relationship between instances and labels in the original data. I'm not completely convinced by this argument: Randomization of the labels forces the network to memorize the input images together with their labels. It could be the case that the features that help classifying an image are the same that help memorizing the image. But I agree with the authors that one would hope to see a difference.    In the discussion, the authors first focus on methods that approximate an element-wise product of input and gradient and argue that in these cases the input "structure" dominates the gradient (line 179). This is shown for MNIST in Figure 5. In general I agree with the authors, but I have some issues with this statement and the presented evidence for it:  * The shown effect mainly holds for MNIST data where large areas of the image are black. In my opinion the actual issue here is that all versions of "input times gradient" assign a special role to the value zero in the input. In MNIST that even makes some sense (since here zero is the absense of a stroke), but for natural images in general I'm not convinced by given zero (i.e. black) a special role compared to gray or white. For me this is the main reason to avoid input-times-gradient methods and the observed effect is just a side-effect of that. * On a related note, in Figure 5d: Is scale of the noise on the x-axis relative to the scale of the input data? If so, at least for imagenet (which is, unlike MNIST, not sparse) I would expect that for sufficiently large gradient noise the noise significantly changes the rank correlation.  In section 5.2 the authors analyze two simple models: a linear model and a one-layer convolutional network. They show that the convolutional model mainly produces saliency maps that look like the output of edge detectors and argue that this is a problem for the saliency methods. I don't agree completely with this: a simple one-layer convolutional network in the end is actually a combination of local pooling and edge detection. So seeing edges in the model explanation might not be a failure of the explanation method after all  Clarity:  The submission is presented very clearly and structured very well. I enjoyed reading it. I have a few minor concerns:  * Figure 2 is never referenced in the main text and I'm not sure which model is used in this figure. * I first wondered how the models in the reparametrization test are (re)-initialized, then I found the detail in the supplement. A short reference to the supplement might be helpful if the space allows.   Originality:  The proposed sanity checks are original. Randomizing model weights or training labels is something that has been done often before, but utilizing these methods as sanity checks for saliency methods is a new contribution for the field of saliency methods.  The submission mentions other related methods like masking salient areas of the image to change the model decisions (but see below).  Significance:  I like the proposed sanity checks and think they can be very helpful as a necessary condition for estimating the explaining power of existing saliency methods. That makes this work a relevant contribution.  However, I'm missing a few things that I think would increase the relevance and significance of this submission.  My main concern is that all included saliency methods are different versions of gradients or gradients times input, which are known to have weaknesses (See e.g. Kindermans et al, Learning how to explain neural networks: PatternNet and PatternAttribution, ICLR 2018). Therefore I would like to see methods that explicitly try to overcome these limitations. This includes that aforementioned PatternNet which is nicely theoretically founded and also Zintgraf et al, Visualizing Deep Neural Network Decisions: Prediction Difference Analysis, ICLR 2017.  Another easy-to-fix issue that in my opinion degrades the high potential impact of this submission: Right now there is no summary which of the included saliency methods the authors would recommend for future use given their results. I think this would be an important conclusion for the paper  If the two above points are fixed, I would argue that the presentd work is of very high relevance to the community and definitely should be published. In the current state the work is still relevant because the proposed sanity checks are very well motivated but the significance is somewhat limited.  One additional point that is less important in my opinion, but would help underline the relevance of the work: the submission mentions other related methods like masking salient areas of the image to change the model decisions (l. 107). I think it would be interesting to see whether the results of the submission are consistent with these other quality measures. This would show whether the proposed sanity checks offers new evidence over those methods that should be more than just sanity checks.   Minor:  l 17: "Cite several saliency methods": I guess that's a leftover todo  l 254" I guess the reference to Figure 14 is actually supposed to refer to Figure 11?   Update after the rebuttal:  I'm pretty happy with the authors response. I like especially that they now include more methods and some results on ImageNet. I'm still missing a thorough discussion of the label permutation test (and would love to see more ImageNet results due to the special structure of MNIST). Nevertheless I think this paper has an important contribution to make and therefore I'm upgrading my rating to an 8.