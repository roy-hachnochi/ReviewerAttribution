Response to the rebuttal: Thanks for the helpful clarifications and new experiments that directly address some of my original comments. I have updated my score from a 6 to a 7.  ---  This paper proposes a way of modifying MAML to be probabilistic by injecting noise into gradient descent. The method is empirically validated on some synthetic domains and in ambiguous image classification with celebA.  I like that the proposed method is also model-agnostic way of adding stochasticity by adding the noise in the parameter space instead of a model-specific way of adding stochasticity. This lets the technique be applied to classification and regression tasks without any modification. I think it would be interesting to further quantitatively study the distributions that this method induces for specific tasks, and to compare it to model-specific approaches.  Because this method is straightforward to apply in all of the domains that MAML captures, I am surprised that there are not more quantitative comparisons, especially on the standard few-shot classification tasks such as Omniglot and MiniImagenet. Did you try ProMAML in these domains?