Summary: Reward shaping can speed up learning, and this paper considers the setting where experts propose functions of unknown quality. The algorithm combines the information of the experts to find an optimal ensemble without computational cost with respect to standard TD methods. This idea can be incorporated on any training procedure based on state-action-reward sequences. Simulations show the effectiveness of the approach.  Specific comments: 1) It would be useful to number individual lines in equation (7) that are refered to in the lines below.  2) There is still quite a bit of noise in the outcomes of simulations (Fig 1 and 2). If feasible it might be helpful to average over more trials.  3) It is unclear whether code to replicate the simulations will be made available. Making high quality code available for others would be a big plus.  4) A discussion on where experts can come from would be a useful addition, as it might not always be obvious how to do this. In particular, I am wondering if this algorithm can be useful in the following scenario: consider a complicated RL task, which requires a rich model to be able to succeed. However, training this model from scratch would take a long time due to its complexity. Instead, smaller models are first trained to perform smaller tasks. By combining these as experts, the richer model is able to learn much faster in its initial stages.  5) The peaper and simulations seem to focus on experts that are either uniformly good or uniformly bad. One can imagine two experts that are useful in different parts of the state space. The authors might want to comment on this scenario.  Overall: The paper is well written, easy to follow, and provides a clean algorithm. It is self-contained, and the simulations are convincing. If high quality code is made available alongside the submission that would make a big difference (please comment).  -- Update after author response: Thanks for the detailed response, which only reinforces my opinion that the paper should be accepted. I agree with other reviewers that the simulations are meager and look forward to seeing the additional simulations in the final version. Good luck!