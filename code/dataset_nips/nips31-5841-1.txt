This works explores a novel matching procedure between activations in neural networks, that can be used to determine whether two identical networks initialized from a different seed converge to the same intermediate representations. With a carful laid theory, a suitable and provably efficient algorithms and empirical experiments, the authors give non-trivial insights into the representations learned by networks. The authors establish in an eloquent and concise way the necessary concepts for their exact and approximately matching activations metric. I enjoyed both the convincing motivation given and the carful treatment all the way to a practical algorithm that can be easily used. I was, however, a bit disappointed from the empirical experiments. I feel that this work could benefit from a wider range of networks tested and variety of activations. By doing a more comprehensive comparison we might be able to learn how different architectures behave and get more practical insights which a currently a bit lacking to my taste.  Strengths - A novel approach with thorough survey on past attempts - Good theoretical and justifiable argument, with efficient algorithms - Interesting results regarding convolutional vs fully-connected layers  Weakness - Empirical results could benefit from a wider range of architectures and tasks (currently only VGG on cifar10). Some could argue that they are currently not convincing.  Edit: Following rebuttal, I've decided to update my review and recommend acceptance. I recommend that additional empirical results be referred to in main text.