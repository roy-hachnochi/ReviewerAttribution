Update following the author rebuttal:  Thank you to the authors for a thoughtful rebuttal. The updated results, experiments and clarity of details give much more insight into the paper. Though, the solution is not completely unique, I agree with the authors that understanding and doing a systematic study of adding positional encodings across different tasks, is really helpful. _____________________________________ The authors propose a new operation called the CoordConv, that explicitly adds co-ordinate information to the convolution operation to be more robust towards translation invariance and dependence. The solution is adding the (x,y) coordinate representation before doing the conv operation. It is a simple yet effective operation. By adding position information, the CoordConv performs much better and faster on the Not-so-Clevr dataset for both Supervised rendering and Coordinate classification task.  The performance gain on ImageNet for the CoordConv op seems really small. Applying the CoordConv solution to MNIST and checking performance again perturbations of translation, scale, rotation, etc will show the op's generalization property. This will also help to compare with Spatial Transformation Networks.   There has also been other related work to add position embeddings, either learned or using sine and cosine of different frequencies. So, I am not sure how novel the technique is. https://arxiv.org/pdf/1802.05751.pdf  other comments: In the Supervised Coordinate Classification, why does the test accuracy have so much variance?