The authors propose a manner of weak self-supervised learning, with a novel loss function, which allows the learning of an embedding that is robust to transformations of the inputs. In turn, this embedding facilitates downstream (classification) tasks, allowing for a low sample complexity.   The paper presents a few very interesting findings. For one, the proposed embedding, learned with the joint loss (OJ), performs better at the even/odd MNIST transfer learning task. Additionally, and perhaps most importantly, the authors show a trade-off on the Late Night faces dataset between the number of orbits used to learn an embedding, and the number of samples needed for learning. This finding connects very well to the intuition that more visual experience would better prepare a learner for any specific downstream tasks, hence requiring fewer samples for learning.  Perhaps my main point of criticism is based on the same figure 4, but then on the left. The number of samples for learning is on the x-axis, the classification accuracy on the y-axis. If we select a reasonable classification accuracy (e.g., over 90%?), the curves of the different learning losses come very close together. Notably, having the joint loss (OJ) instead of its components does not seem to matter at such a point anymore. It thus raises the question: does the loss then actually matter so much? A related remark can be made about the trade-off graph: the interesting part may be the yellow part. The x-axis of the right figure stops before we can see that increasing the number of orbits actually would give us such a performance. It would be good if the authors clarified this matter.  Furthermore, the statistical tests in Table 1 are hard to believe. The caption mentions that bold signifies a significant difference from OJ. However, how can ST on MINST with a performance of 0.97 +- 0.02 be not statistically different from OJ with 0.67 +- 0.05? Perhaps ST was not included in the tests? Still, a similar strange result then is the comparison with OE in the bottom 3 rows. Especially the result of OE on the M-PIE set with 0.61 +- 0.01 as compared to OJ’s 0.95 +- 0.01. I would urge the authors to reconsider their statistical test, or convincingly show how distributions with such a different mean and small standard deviation can still be regarded as the same…  In general, I think this is an original and well-written paper, so I would recommend an accept.  Small remarks: 1. Definition 3: should you not also add = x to the formula xc = g0 x [ = x], given that g0 is supposed to be the identity transformation? 2. “Note that the loss is independent of the parameterization of Phi, Phi~”. What do you mean with parameterization, exactly? The loss function clearly depends on the parameter instantiation of Phi, Phi~. The way in which Phi is parameterized? But why then say that it does depend on the actual selection of triplets? 3. Proof of proposition 1. Frankly, I do not really see why this proposition is of real interest… It seems rather circular. Also, is the condition of boundedness easily (provably) satisfied, and is it then still meaningful? Why does the sqrt(alpha) go away? Is the ||…||2 the square, and not the 2-norm? 4. I think the remark on a generalization of de-noising auto-encoders is interesting. 5. Exemplar (EX): is it possible in one phrase to give an intuition how this method works?  6. “consisted from” -> “consisted of”    