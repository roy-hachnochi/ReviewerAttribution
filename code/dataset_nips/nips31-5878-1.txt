After reading the rebuttal, I favor acceptance but would strongly encourage the authors to carefully edit for clarity.  As it is currently written, the paper is awash in undefined terms, poorly defined terms, and verbose notation. My list of notational comments is not intended to be an edit list. Some of the specific suggestions may well be incorrect, but my comments should hopefully direct your attention to portions of the document which need work.  The paper presents a finding that while Convolutional GANs are theoretically hard to optimize due to non-convex loss surfaces, in practice, and with a few assumptions that do not seem to be too onerous, convolutional GANs can be minimized in their latent variable with respect to a partial output (sample).  The result seems to largely be a result that strided convolutions can be written as a Toeplitz Matrix, which can be rewritten to be block diagonal with a permutation matrix, and then is provably invertible as long as the filter weights are gaussian or sub-gaussian.   Positives:   1. The theoretical contributions seem to be mature 2. figure 4 a,b appear to validate the fact that 0 is a maximum, and -rho z and rho z are minima for a two layer relu network. 3. Results on "inpainting" images is currently interesting. A recent paper, published around the time of submission, Deep Image Prior Ulyanov et al CVPR 2018 fixes the latent variables and trains CNN parameters to recover an image.  They show that doing this allows for lossy image recovery but do not provide any theoretical findings.  Negatives:  1. While I think your proof is mature, I'm worried that it's too constrained.  Two layer networks are only so interesting, and it's unclear to me how badly the loss surface under your analysis would blow up as more layers are added.   2. I'm not sure if this is a large issue for your proof, you seem to rely on the effective weights with relu taken into account being Gaussian, but your experimental validation of weights (sec 4.10) is on the raw weights.   3. Significant portions of the paper are difficult to follow due to typos or failure to define terms in the main text. (list below)    footnote 2: "Collpase"  line 95: "sub-sapling"  line 105: "n2=n" should be n2=n1?  line 111: "yAx" should be y=Ax?  eq 3: this should be z {diamond} rather than z {hat}  eq 4: this should be |Ay - AG(z)| right?  You aren't trying to reconstruct the 0s  section 2.1 you seem to be describing in detail the construction of a Toeplitz matrix, but never say so.  eq 5: I don't think you define \mathit{B} in the main text.