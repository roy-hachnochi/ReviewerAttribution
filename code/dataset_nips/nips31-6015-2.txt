This paper proposed a multi-agent reinforcment learning framework by a double averaging scheme where each agent performs averaging over both space (neighbors) and time to incorporate gradient and local reward.  The basic idea to achieve this is to reformulate the task in a primal-dual way to a decentralized convex-concave saddle-point problem.  Authors also prove that the proposed algorithm achieve fast finite-time convergence.  The paper is technically sound but the presentation of the paper need improvement. Some notations are ambitious and over complicated, making the equations difficult to follow.  e.g.  "s" is used as the state variable in the early sections of the paper but in Eq. 14, "s" is used as the surrogate for the gradient w.r.t. \theta. Are they the same variable? If so, why one can use the state as the surrogates for the gradient w.r.t. \theta.   The experiment is weak where a network with only 10 agents are simulated.  Can authors include more experiments with a large scale?