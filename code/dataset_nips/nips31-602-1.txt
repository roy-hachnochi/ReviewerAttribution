This paper studies the optimization landscape of multichannel sparse blind deconvolution, and uncovers it has the desirable “all local minima are global” and “strict saddle properties“ that have been shared to a few other problems such as dictionary learning and low-rank matrix estimation. This is an important message and allows the use of manifold optimization algorithms to solve the nonconvex problem in a global manner.   Further comments: -the sample complexity of the analysis seems to be very high compared to what was indicated possible from the simulations.  it will be useful if the authors can comment on why there is such a big gap in the sample complexity requirement; -can the Bernoulli-Radamacher model be extended to Bernoulli-subGaussian model? The former is quite restrictive and rarely holds in practice. -though first-order methods are desirable as the per-iteration cost is low, Theorem 4.1 has a rather slow convergence rate. In particular, the algorithm should converge geometrically in the vicinity of the ground truth. It may still be beneficial to use second-order methods to reduce the high number of iterations required in Theorem 4.1 for first order methods; -the proof roadmap is similar to what was proposed in the paper “the landscape of non-convex losses” by Mei et.al. The author should compare their approach with this paper and discuss the differences. -the comparisons in Figure 4 seem quite unfair and misleading since the latter off-the-grid algorithms do not use the fact that the signal is sparse in the Fourier basis. A more fair comparison should take all direction-of-arrivals off the grid, and then compare the performance of the algorithms. -to handle miscalibrated PSF in super-res fluorescence microscopy, it might be useful to use total least squares to partially learn the PSF. 