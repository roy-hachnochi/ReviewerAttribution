Abstract: Would remove the capitalized (ENV, TASK) from the text.   Introduction: Line 39-46, the actual terminology used - policy basis, embeddings, etc are not at all described so the description here is quite confusing.  Related work: Other things to mention Multi-task multi-robot transfer -(Devin et al, (although mentioned later, good to mention in related work)), Invariant feature spaces transfer (Gupta et al), work by bou ammar on transfer learning, progressive nets (rusu et al).   Section 3.1 - very nice problem definition in general. A careful consideration of the three scenarios mentioned in the introduction would be good to have here.   In Fig 2, what is a task descriptor and what is an environment descriptor .  Section 3.2 - Key idea seems to be a parameterization of the policy via a bilinear mapping with state features, action features and a metric U which depends on e_env, e_task. Reward is also predicted but with different coefficients. This is similar to successor features.   Section 3.3 In order to impose more structure since parameters of alpha and inputs are all being optimized, a disentanglement loss is being optimized. The idea is very simple - discriminability of env or task from the state-action representation. It has been used effectively in a number of other scenarios - eg unsupervised exploration and is a good choice here.   Section 3.5  A slightly more in-depth consideration of the fine-tuning setup would be helpful here.   Experiments are interesting and helpful, the ablation study is well done and a number of meaningful baselines are compared with. Figure 6 is especially nice in understanding transfer capabilities.   Table 3 empirical performance seems a bit weak overall. Any reasoning behind this?  Overall I think this work is clearly motivated, builds nicely on prior work and does a good job of experiments. I would recommend an accept. 