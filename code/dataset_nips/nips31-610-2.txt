The paper presents a framework for learning to transfer across environments as well as task. The framework consists of separate environment and task descriptor which generate embeddings and a policy network and reward function that takes environment, task, state and action embedding as input and outputs policy and reward through a look up table. In order to transfer to unseen environment and task the look up table for the environment and task is updated which enables fine tuning with just a few demonstration. The look up table acts as an attention mechanism which is updated for new environment and task.  It is difficult to interpret the results of visualization from t-SNE of environment embedding. Additionally, it would be interesting to see saliency or activation maximization based method applied to the task descriptor to see whether the saliency of the network is consistent. Instead of average reward, it would be interesting to see the reward mean and variance for different algorithms. It would be interesting to see how the method compares to the meta-learning approach.  Minor Corrections: Line 148: typo "relies" Line 171: typo "state"  Update: The authors did a good job in their response and also cleared some misunderstandings I had with the paper. I've changed my score to an accept. One of the problems I feel that is still not addressed properly in the paper is the interpretability (t-SNE analysis) but overall experiments and ablation studies are thorough.