The authors focus on the behavior of EM when model is misspecified, obtaining insights form a number of restricted cases. This is an important topic, first because EM is a very popular tool, and second because misspecification must be the rule rather than the exception. The author do have some interesting derivations and results, and they do contribution with some novel insights. However, there are some questions here concerning significance. The main criticism here is that the studied cases are really special ones; mostly one is looking at mixtures of two Gaussians with known variance and expectations theta and -theta (and then corrupted versions of this model). Of course one must study the easy cases first, but here the cases are so specific that I wonder what can be extrapolated from them. This is the main difficulty.   Also, I find that the meaning of "misspecified" is not always clear. Misspecification means that a class of models is selected, say belonging to set  Theta containing values for a parameter theta, and the true parameter is not in Theta. This seems to be used in the paper, but sometimes the authors write as if an estimated value is misspecified just because it is not equal to the true value. Is it really a case of misspecification? I guess not. But perhaps the authors do not think that either; perhaps it is just the text that seems to pose things this way. The problem seems to be that the authors do not clarify what exactly is supposed to be known and what is estimated, hence the reader sees "variance is misspecified" and thinks "well, but variance is estimated, how can it be misspecified" only to find that variance is supposed known (a rather strong assumption!!).  In any case, most of the time I was comfortable with the definition of misspecification I just gave; my suggestion is that the definition of misspecification should be formally given, together with more details on what is known and what is estimated.   Now the thing is that results concerning the behavior of maximum likelihood under misspecification are rather old; for example there is classic work by Berk from 1966. The idea there is that by looking for maximum likelihood one finds the closest projection via Kullback-Leibler divergence; this is a *consequence* of maximum likelihood. But in this paper the closest projection appears suddenly, as if it were an obvious goal in itself. But the reader must wonder why one is looking at the projection via Kullback-Leibler; more explanation should be provided.  Question: why is the condition \eta>=1 necessary? What does it guarantee?  Also, the constant \gamma should be defined and explained in the statement of Theorem 1, not after it.  I had trouble understanding the meaning of Figures 1,2,3: what exactly is depicted there? What do they show? Please explain better.   Concerning the text, a few points: - Abstract, line -3: "implies". - Order references when a set of them appears together. - Section 3: first paragraph is not clear; sentence "Consequently, the..." is really hard to parse. - There are problems with capitalization in the references (em, gaussian).