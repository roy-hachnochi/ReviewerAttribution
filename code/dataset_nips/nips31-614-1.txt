 This paper introduces several results regarding the robustness of classifiers in terms of misclassification under adversarial examples. Providing a straightforward definition of robustness, the work shows that there exists an upper bound on robustness for any classifier, which is worsened by the dimensionality of the latent space or the classification task. This upper bound can be attained, achieving maximal robustness, by having classifiers inducing linearly separable regions in the latent space. Systematic modification of classifiers is also proposed to achieve unconstrained robustness close to in-distribution robustness. These results are supported by theoretical analysis and some empirical studies.  The work brings some enlightening results involving the robustness of classifiers under adversarial examples. The paper is well written, the results are novel, and their exposition is of high quality. I propose to accept this paper as it would contribute to the community significantly.  I consider the work to be showing a series of negative results diminishing the current optimism around generative models and bringing some realism to such endeavors. As the theory suggests some ways of achieving the upper bound, it would have been encouraging to show some empirical demonstration of that. For example, it might be shown that a state-of-the-art method is severely affected by adversarial examples on a real dataset due to not achieving the upper bound of robustness, but a modification to the classification as suggested by the work alleviates this issue.  Additionally, it would be useful to know if there exists an example on a real dataset where the upper bound of robustness is highly reduced due to a large number of classes and getting around this would be hopeless. Or, is it the case that we can always be hopeful of getting around it by low-dimensional latent space with non-smooth generative models?  *** After author rebuttal ***  Excellent paper and excellent replies. Thanks!