The paper proposes a simple, convincing approach to combine evolutionary reinforcement learning (RL) and more "classical" RL.  The manuscript  title is much too general. Evolutionary algorithms have been used for RL for a long time, and many refer to this approach as "evolutionary RL".  The references ignore some basic work on evolutionary RL. One may argue that most classic work is not applied to very deep architectures with many layers. However, recurrent neural networks were considered and RNNs are typically considered to be deep architectures theses days regardless of how many layers they have. For example, see  Verena Heidrich-Meisner and Christian Igel. Neuroevolution Strategies for Episodic Reinforcement Learning. Journal of Algorithms 64(4), pp. 152-168, 2009  and references therein.  "The use of a fitness metric that consolidates returns across an entire episode makes EAs invariant to sparse rewards with long time horizons": I think this statement may be a bit misleading. Evolutionary RL methods are Monte Carlo RL methods. Thus, they do not exploit intermediate rewards beyond the contribution to the final return. That is, they have a conceptual drawback compared to, say, temporal difference methods in this respect. This should not be sold as an advantage for tasks without meaningful intermediate information.  "The quantitative results obtained for ERL also compare very favorable to results reported for prior methods [12, 21, 22, 59], indicating that ERL achieves state-of-the-art performance on these benchmarks1." This should be quantified. The results of the alternative methods should be reported, at least in the supplement.  ---  I read the authors' reply. Regarding the comment in the rebuttal 'However, the low-fidelity signal that EAs use (fitness that compiles episode-wide reward into one number) does have the property of being "indifferent to the  sparsity of reward distribution" [42] ...': Of course, EAs are indifferent to the reward distribution, because as Monte Carlo methods the just consider the final return. This means, they do not exploit any additional information contained in the timing/distribution of rewards. This in a drawback - and not a feature.  