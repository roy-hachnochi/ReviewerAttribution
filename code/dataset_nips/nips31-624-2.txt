This manuscript explores how to select training data (given limited budget) for achieving good performance for deep learning systems. Inspired by the learning process of human toddlers, the authors conduct a series of experiments and show that the size, quality, and diversity of the training data plays important role for obtaining better performance of an object classification task. The presented idea is clear, and the the experiments are conducted comprehensively. However I do have several concerns which are outlined below:  * The key result presented in this manuscript is qualitative rather than quantitative. I learn that we should use bigger, high quality, and diverse images to train a deep network, but I am less clear on what are the combinations of these factors to get the best performance, given limited budget. Should I use all very big images and a bit diverse, or I can make the dataset pretty diverse but with a portion of small images in it? Since the authors do have a large dataset, I believe some ablation studies with these factor combined are needed to get better insight of how to achieve the best performance.  * I am curious to see the generalization power of the key findings. That is to say, besides the toddler dataset the authors use, I would like to see the same idea tested on a public benchmark, say ImageNet. Of course not all 1000 categories are needed, a proof of concept on a small subset with sufficient number of training images should be sufficient.  * I would also like to see the effect of blurring the training data. Does the blurring have some effect on the performance compared with using original images? Sometimes the peripheral information also helps visual recognition [1]. Also as the authors suggest that blurring is used to model the contrast sensitivity function of human visual system, has the authors considered using log-polar transformed images, as in Wang & Cottrell [2]? It would also be interesting to see the feature learned in the VGG network, as compared with [2].  * Related work on building models to connect visual object recognition in computer vision and developmental psychology: [2], [3], [4]  Evaluation: Quality: Overall quality is good, but we need detailed quantitative analysis in order to better demonstrate the central idea.  Clarity: The manuscript is clearly written and is easy to follow. Originality: This is an original work to my best knowledge. Significance: Significance can be better justified by conducting more ablation studies and testing the generalization power.  [1] Larson, A. M., & Loschky, L. C. (2009). The contributions of central versus peripheral vision to scene gist recognition. Journal of Vision, 9(10), 6-6. [2] Wang, P., & Cottrell, G. W. (2017). Central and peripheral vision for scene recognition: a neurocomputational modeling exploration. Journal of vision, 17(4), 9-9. [3] Smith, L. B., & Slone, L. K. (2017). A developmental approach to machine learning?. Frontiers in psychology, 8, 2124. [4] Wang, P., & Cottrell, G. (2013). A computational model of the development of hemispheric asymmetry of face processing. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 35, No. 35).  ===================post rebuttal=========== On one hand, I agree that the authors have done substantial amount of work for data collection and the work is highly original; on the other hand, I expect the authors to put more emphasis on their core idea, that is, the properties of visual data that lead to better visual object learning (as I mentioned in my first concern). Unfortunately, this issue is not addressed in the rebuttal. So my current take-away on this core idea is - we can use large images with some diversity to build our training set, but I sill do not know how to combine these factors. This is also why I raised my ImageNet concern, as the authors can potentially build a small subset to verify the idea. If the idea can be verified on ImageNet, I believe it will make significant contribution to the computer vision society. However, I do think R2 made a reasonable comment on the amount of work it takes to run such experiment, so it will not be a major point that bothers me.  Still, the authors have not satisfactorily addressed my biggest concern (and one concern raise by R2 about the control experiment of size). and I think it impairs the significance of the submission. However, consider the work is highly original and the high quality of the presented experiments, I bump my score to 6. 