In this paper, the authors propose a new algorithm for optimizing tree. Given an input tree, the proposed algorithm produces a new tee with the same structure but new parameter values. Furthermore, it can learn sparse oblique trees, having a structure that is a subset of the original tree and few nonzero parameters  Major issues: 1. The algorithm arms to optimize eq. (1) but assuming a given, fixed tree structure. So, its performance highly depends on the initial tree structure, and it is essentially a postprocessing algorithm. Since there exist techniques for pruning a given tree, it is unclear to me why the proposed algorithm is necessary.  2. The algorithm is heuristic, and lacks theoretical guarantees. Theorem 3.1 relies on a heavy assumption. Theorem 3.2 is about the classification error, instead of generalization error. Furthermore, the convergence behavior of TAO is unclear.  3. In the experiments, the authors did not compare with other methods.  I am ok with acceptance,  provided the authors could address the following concerns.  1. Since the algorithm is a postprocessing method. The authors should examine the impact of the initial tree structure on the performance. I notice that in the rebuttal the authors have provided more experiments.  2. I agree that it could be too difficult to provide theoretical guarantees of the proposed method. But the authors should try some methods, such as regularization, to avoid overfitting. In the current version, the issue of overfitting is left to cross validation.  3. For the binary misclassification loss in (4), does the selection of surrogate loss important?