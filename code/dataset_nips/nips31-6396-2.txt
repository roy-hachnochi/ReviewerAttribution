The paper establishes two improved generalization bounds for uniformly stable algorithms. Using the approach introduced in [2,21] for differentially private algorithms, the authors establish 1) an upper bound on the second moment of the generalization error that matches the lower bound on the first moment (up to constants), and 2) an upper bound on the tail of the generalization error that improves previously known results (the tightness of this results is left for further investigation). The results are shown to imply stronger guarantees for well-studied algorithms, specifically in the common situation when the stability parameter scales with the inverse of the square-root of the sample size. Examples are developed in the supplementary materials and include  1) ERM for 1-Lipschitz convex functions bounded in [0,1], improving the low-probability results in [25] and establishing high-probability bounds; 2) gradient descent on smooth convex functions, establishing low- and high- probability bounds for the setting explored in [14];  3) learning algorithms with differentially private prediction, establishing bounds that are stronger than the (multiplicative) bounds established in [10] in some parametric regimes.  The paper is well-written, with the main contributions clearly emphasized and commented. Connection with previous literature is detailed, and the proofs are well-commented and easy to follow. The elements of novelty that add to the approach in [2,21] are also well-described (see Lemma 3.5, for instance).  The paper offers new insights on algorithmic stability, which are particularly relevant to the field of machine learning these days, given the key role that stability has played in the analysis of widely-used algorithms (currently, stability seems the only tool to understand the generalization properties of multiple-pass stochastic gradient descent, as in [14]) and given its connection with differential privacy, another main topic of research in the field.