This paper proposes a new method for the clustering of persistence diagrams using recent techniques in optimal transport. The problem is quite important; clustering provides a sensible way to group data according to their topological characterizations. It is also very challenging due to the Wasserstein distance between the persistence diagrams. This paper proposes to (1) approximate the Wasserstein distance between diagrams using the regularized optimal transport, and (2) treat the computation of the Frechet means as another optimal transport problem, and find the optimal solution using gradient descent. Several major technical challenges are addressed, include:  1) the Wasserstein distance may involve matching points with the a diagonal line.  2) estimating tighter bounds for the convergence stopping criterion.  3) a convolution based algorithm to improve the computation efficiency.   The proposed method is compared with the state-of-the-art (Hera) and is shown to be more efficient.   The problem addressed here is to compute the average of a set of diagrams. It is different from computing pairwise distance or kernel. A good tool for this task will lead to efficient clustering of persistence diagrams. This can be the crucial first step for analysis when direct supervised training on persistence diagrams is not effective (which is often the case in practice).   The main advantage of the proposed method is it can be implemented in a series of matrix multiplications, which can be efficiently executed using tensorflow + GPU (for the first time, I believe). Other methods, such as B-Munkres and Hera, solve the problem as a discrete optimization task; there is no (obvious) way to implement them in GPU/multi-core. (Hope this answers a question of R4.) I personally think the GPU implementation is extremely important. To really incorporate topological information in deep learning, we need to run topological computations repeatedly during the training (after every epoch, or even every batch). For this level of computational demand, an algorithm that finishes in seconds is already too slow.   The paper is well written. A few additional suggestions/comments for improvements:  * The technical part is a bit dense. To make it more accessible to readers, it might be necessary to provide some overview of the method in the beginning, especially regarding how the two parts (sec 3 and 4) are connected.   * It is not clear to me whether the method allows multiple smoothed barycenters, as Turner’s algorithm does. If so, how?  * In Fig 4, it might be necessary to also report the error. For distance computation, Hera is supposed to produce the true distance. But Sinkhorn will be stoped when the error is tolerable, right?  * In Fig 5, it will be useful to report the variance achieved by B-Munkres and Sinkhorn respectively, for reference. It is also necessary to show the smeared and the spiked (post processed) barycenter diagrams, as well as the output of B-Munkres, for quantitative evaluations.  * Typos: L138: “even but one”. L272: GPGPU.  --  Final suggestions:  This paper has solid contribution and worth publishing in NIPS. But to be better appreciated by the general learning community, and even many people familiar with TDA, there are certain additional improvements (in terms of presentation) that could be added to the final version.  1) Different computational problems for TDA have been addressed in the past. It would be helpful to provide more contextual information of the addressed problem (clustering of PDs), e.g., how it is related but different from the computation of persistence diagrams, bottleneck/Wasserstein distance, kernels & simple vectorization techniques for supervised learning. A diagram could indeed be very illustrative.  During discussion, we (reviewers and AC) feel that to better demonstrate the necessity of the proposed method. It will be very useful to also refer to and (qualitatively) compare with simple vectorization (persistence images)+average or the average of landscapes. These methods definitely have computational advantages, but they will fail to produce meaningful ("sharp") average diagrams (for better interpretations), compared with Wasserstein-distance-based methods.