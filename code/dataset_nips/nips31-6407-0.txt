## Summary: The authors analyze an existing adaptive stochastic optimization method, ADAM, and propose/analyze a new adaptive stochastic optimization method, YOGI, for solving non-convex stochastic optimization problems. Bounds are given for the expected gradient of an ergodic average of the iterates produced by the algorithms applied to an L-smooth function, and these bounds converge to zero with time.  The authors give several numerical results showing that their algorithm has state-of-the-art performance for different problems. In addition, they achieve this performance with little tuning, unlike in the classical SGD.  A motivation behind their work is a paper [27] that shows that a recent adaptive algorithm, ADAM, can fail to converge even for simple convex problems, when the batch size is kept fix.   Their analysis points to the idea of using growing batch sizes. An idea which they show does translates into good performance.   ## Quality: Over all good.   The definition of SFO complexity depends on the definition of $\delta$-accuracy for a solution $x$, which was defined as $x$ satisfying  $\|\nabla f(x)\|^2 < \delta$. However, the theorems only prove bounds on the expected $\|\nabla f(x_a)\|^2$ where $a$ is uniformly sampled in [T] and $x_a$ is a random object that depends on $\{s_t\}$ sampled from $\mathbb{P}$. It would be good to say a few words about how, given a sequence $\{x_a\}$ that satisfies the bounds in the theorems, we can find *one* solution that is $\delta$-accurate without much computational effort.  If the expected value of the squared norm of the gradient of f at x_a is <= O (1/ T) doesn't this imply that the complexity to get an error of 1/T is T ? Where does the $^2$ in $O(1/\delta^2)$ come from? What am I missing?  I would suggest that in the expectation in Th1. the authors are explicit that the expectation is being taken with respect with to $a$ and $\{s_t\}$ sampled from $\mathbb{P}$. The same goes with the expectations in Sec. 2.  I would suggest that in Alg. 1, the authors write the v_t update equation in the form v_t = v_{t-1} - (1 - \beta_2)(v_{t-1} - g_t^2) g_t^2, to facilitate comparing YOGI with ADAM.  ## Clarity: The paper is very well written and easy to understand. Great job !  ## Originality: The algorithm, even if a simple modification of ADAM, is new, and the proofs seem to be new as well.   ## Significance: Empirical performance gains are not consistently amazing, but the fact that their algorithm requires little tuning is a great thing. It is push in the right direction. SGD sometimes outperforms YOGI. Is it the case than if we invest an equal effort in tuning YOGI, maybe after introducing some extra parameters into it, that YOGI can beat SGD in these cases as well?  It is rather unfortunate that the authors do not include any ideas behind the proofs in the main text, and have the reviewer read 6 extra pages of proofs. There is a myriad of paper that show method XYZ gives better empirical performance on different ML than the previous state-of-the-art. What distinguishes one paper from the other, is the support from theory. In this case, I would suggest passing many of the details about the different numerical experiments to the appendix, and bringing some "juice" to the main document. In particular, the authors should emphasize what are the key technical steps in their proof techniques that distinguish their work from previous analysis of similar algorithms.  This prevents the reader to appreciate the full significance and originality of their work.  It would be good if the authors could write in the paper if they will, or not, release code after and if the paper gets published. A written commitment to reproducibility, no links, would be enough.