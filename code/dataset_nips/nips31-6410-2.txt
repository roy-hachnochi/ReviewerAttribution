Update after rebuttal:  After reading the rebuttal / other reviews, my conclusion is not changed significantly.   This paper shows an end-to-end approach that works well for a specific set of MDPs (static background, dynamics objects) with <= 2 objects. Showing that the composition of modules can work in a simple setting is interesting. However, the paper can be compared with previous work along multiple angles, but each one feels unsatisfactory.  - The dynamics aspects used are not comparable to previous work on learning dynamics (as authors cited, see e.g., https://arxiv.org/pdf/1612.00341.pdf that demonstrates their approach on 7 objects and that shows they can predict "the future" and interesting dynamical information e.g., mass).   Also see https://arxiv.org/pdf/1705.10915.pdf, which predicts many frames into the future for *flexible* objects, and with high fidelity. Note that foreground/background setting is the same as here. This problem is actually much harder than learning object representations. To be convincing compared relative to this, the authors would have to evaluate their method on at least more than 1,2 objects.   - As a work on unsupervised learning of object representations from sequential data, see e.g., https://www.di.ens.fr/willow/pdfscurrent/kwak2015.pdf (note that this doesn't use DNNs, but is easily combined with deep features).   Hence, I stand by my previous conclusion: in the current state, the work would benefit greatly from  - showing e.g., the merits of this method on a (challenging) RL task, as their introduction suggests as the motivation for this work.  - or, comparing more challenging / richer dynamics aspects with previous work (see above)  I do strongly encourage the authors to flesh out these aspects of their work.   These would be additions that would definitely raise the delta significantly.  ---                  Summary:           The authors propose to learn environment models at the object-level rather than pixel-level (i.e., predict what the next state of a collection of pixels ("object") is). They train a neural network (OODP) end-to-end for this purpose. At the core is a Dynamics-net, which is action-conditioned; other information of the MDP is not used. Objects are divided into static and dynamic ones; the number of objects is fixed before training. A separate background splitter network is trained to predict all non-objects. An additional region proposal loss (OODP+p model) is used as well to focus dynamic object networks on "good" regions that likely contain a dynamic object.             The authors also interpret the model by inspecting the scene parts predicted by the static / dynamic object models.          Pro:           - Using the structure of object-ness is a good idea to learn dynamics and potentially speeding up model-based methods.           - Quantitative results of OODP suggest the method works well (lower state-prediction "accuracy" than other methods).           - Authors evaluate on k-to-m generalization (train on k envs, test on m envs) for different backgrounds and layouts.            Con:            - The authors only evaluate on data from a single MDP, which seems quite limited. Does the performance gains appear for other games as well?            - How do we choose n_O beforehand?            - How does the model deal with a variable number of objects at test-time? What if objects disappear (e.g., an enemy is killed)? How would we extend the proposed architecture? It would be nice to include a discussion of this aspect.             - The authors do not evaluate on any real RL task (i.e., with exploration) / integrate OODP with model-based RL approaches; the only MDP information seems to come from the action-conditioning and the fixed sequence of states for training. The contribution of this paper thus seems to mostly be in the supervised learning domain (although the motivation mentions RL). Therefore, I'm not so sure that L289 is warranted -- "We make one of the first steps in investigating whether object-oriented modeling is beneficial for generalization and interpretability in ***deep reinforcement learning***".             Clarity:           - What are the units of "accuracy"?            Reproducibility:           - ODDP is a complicated model with many moving parts and a complicated total loss function (involves 6 losses!). It thus seems hard to fully replicate the authors' results. It would thus be very beneficial to release code and trained models.         