The paper is very well written. 1) it explains well the challenge of communication 2) it provides  a simple yet powerful combined variance/communication objective 3) Shows how to minimize this objective, with connection to two SOTA methods for reducing the communication requirements of NN distributed training 4) Proposes a new method based on SVD that improves communication by performing a smart rotation of the axis on which the gradients are sparsified  The experiments are convincing, although it would have been god to have the performance on a few other datasets.  I have several slight concerns/questions: - Would it make sense to use more advanced matrix factorization technique (eg dictionary learning) to compress the gradients even more than using SVD ? The problem I foresee is the complexity of running a matrix factorization algorithm at every time step but in theory it should perform better. - The connection between the average sparsity s and the bucket size b is not clear. How are they comparable ?  Minor comments -------------- It may be clearer to explicitly state a call to a sort function in Algorithm 1 Lemma 4: \exists should be in english  l203: typo "we s and suppose". Why do you suppose that X is s-balanced entry wise ? For simplification ? l214: in to -> into