The authors present an algorithm for lifelong representation learning that adapts variational autoencoders to the lifelong learning setting. The framework is presented as a full generative process, where a set of latent factors are (selectively) shared across tasks, and the tasks themselves are generated by an unknown distribution. The algorithm optimizes for the reconstruction error with a regularization based on the MDL principle that has been studied for learning disentangled representations. The algorithm automatically detects distribution shifts (i.e., task changes) and avoids catastrophic forgetting by "hallucinating" data for previous tasks while training on a new one. The authors show empirically that their algorithm is able to extract relevant semantic knowledge from one task and transfer it to the next.   Overall, I think this is a very good paper. The topic of lifelong (deep) representation learning addressed in the paper is very underdeveloped (which the authors rightfully point out), which I believe makes this a valuable contribution. My main concern with this paper is the lack of baseline comparisons--[40] is mentioned as a related approach, and there are also multiple other non-deep representation learning approaches (e.g., online dictionary learning) and non-representation learning approaches to lifelong learning that could address some of the problems in the experiments section.  The algorithm selectively shares latent components via a boolean mask, which is inferred by keeping a threshold on how much the posterior of the component given the data diverges from the prior on the component. There seems to be an important connection between this masking and sparse coding, a popular mechanism in lifelong learning, and so I believe the authors should discuss this relationship and provide a qualitative comparison.  The  algorithm infers the task/environment by comparing the reconstruction error of the most likely environment on the new data and on its own data, and by comparing the boolean masks. The authors justify why merely comparing the errors is not enough, but not why merely comparing the masks is not. The latter seems like a viable option, so it seems that the authors should either attempt using it or explain why they don't.  Finally, the algorithm avoids catastrophic forgetting by generating samples from the partially learned generative process and imposing a penalty for changing the encoder and decoder too much for previous datasets. This approach, instead of using actual data from the previous tasks, is substantially more data efficient, which is key in a lifelong learning context. One concern I have is that for this penalty, the authors use different distribution distances for two distributions, but do not justify this choice.   The experimental setting is very good, first providing  general performance measures and then adding other experiments that are useful for seeing the effect of each of the separate parts of the framework. The ablation study addresses one big potential concern when presenting an algorithm with so many components, which was good. I also thought the semantic transfer experiment was helpful to show the potentila of the algorithm for transfering knowledge across highly varied tasks. However, the authors compare only to a single-task learning baseline in the main results. They point out [40] as the only other lifelong variational autoencoder, but limit their comparison to the SD condition in Table 1 claiming that it is similar to [40], without providing any explanation about how it is similar. In fact, there is no description or qualitative comparison to [40] in the paper. Since there is apparently no direct comparison, the author's claim that their method outperforms the only related approach seems invalid.  The readability of the paper could be improved by providing an intuitive explanation of the terms in the equations before introducing the equations themselves.  Typos - Line 126: "... ignore informative z_n irrelevant..." - Equation 4: x_b^s is not introduced --> what does subscript b stand for?