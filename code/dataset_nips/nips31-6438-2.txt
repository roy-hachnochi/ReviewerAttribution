The paper demonstrates the convergence analysis of (stochastic) gradient descent methods under Markovian sample inputs. However, the experiment section is rather weak; it is from [1] but didn't compare with the method in [1]. Further, although the paper claim that their theoretical result for non-reversible Markovian chain would be substantially better at sampling, the experiment is not provided. Thus, I would recommend a rejection.  Major comments: 1. (line 85) The loss may be undefined when the input to log < 0.  Minor comments: 1. (Appendix eq 54) I do not understand why P^{J_k}_{i,j} converges to 1/M.    Shouldn't it converge to \Pi_{ij}? 2. (Appendix ineq 61d) I do not see how eq (54) works here. Where is M and H?  [1] Ergodic Mirror Descent. John Duchi et al. SIAM 2012.  ==== POST REBUTTAL ==== I accept the explanation in the author feedback and bump the result to weak accept.