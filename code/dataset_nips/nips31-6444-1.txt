Summary: This work considers the problem of learning a robust policy in a scenario where state inputs to the policy are subject to intermittent periods of adversarial attack. The authors propose a meta-learning based approach, whereby separate sub-policies are learned for the nominal and adversarial conditions, and at each time step a master policy selects one of these sub-policies to execute on the basis of their respective advantage estimates for the current observed state.  Qualitative assessment: The idea of using advantage estimates to detect adversarial attacks is quite appealing, since by definition an adversarial attack should decrease the advantage of the policy regardless of the attack method used. However, a disadvantage is that the attack can only be detected after a delay, since the agent must first experience an unexpectedly low reward. This seems like it would be especially problematic in domains with long time horizons and sparse rewards, where the consequences of selecting a suboptimal action may not become apparent until much later on. The authors acknowledge this limitation, although I think it deserves more than a sentence of discussion. I’d be interested to see how well their method performs on tasks where the reward is sparse.  The adversarial attacks simply consisted of additive uniform noise, which seem rather crude. The authors state that gradient-based attacks are not well-studied in the context of continuous action policies. I think this is fair, although at least one gradient-based attack against DDPG has been described in the literature (Pattanaik et al., 2017). One of the purported advantages of the method described in this paper is that it is agnostic to the attack model, so it would be good if the authors could demonstrate this by showing that it is also effective at mitigating a different type of adversarial attack.  Figure 4 shows a single example where the MLAH master agent has learned to distinguish between the nominal and adversarial conditions. However the more salient thing to show is the overall performance of the non-oracle MLAH agent. In particular I’d like to see learning curves like those in Figure 3 plotted on the same axes as oracle-MLAH so that we can see the performance impact of learning the master policy rather than using an oracle to select the sub-policy.  I'd also like to get a sense of what coping strategies are learned by π_adv. For example, can π_adv learn to correct for biases introduced by adversarial perturbations, or does it simply learn an open-loop policy that ignores the input states? On a similar note it would be good to see some videos showing the behavior of π_nom and π_adv under nominal and adversarial conditions.  Clarity: The main ideas in the paper are presented quite clearly, although there are a few details that I think ought to be included: - Table 2 and Figure 3 show 1σ uncertainty bounds, but the authors don’t state how many random seeds were used per experimental condition. - The magnitudes of the adversarial perturbations aren’t stated explicitly. The caption of Figure 3 refers to an “extreme bias attack spanning the entire state space” and a “weaker bias attack”, but it’s unclear what exactly this mean in terms of the scale of the observations. - The text mentions that the PPO parameters are given in the supplementary material, but this information seems to be missing.  Minor comments: - Line 55: “We compare our results with the state-of-the-art PPO that is sufficiently robust to uncertainties to understand the gain from multi-policy mapping.” - this sentence doesn't make sense to me. - Line 115: apposed → opposed - Line 256: executes deterministic actions - Line 265: optimize its attack - Line 281: singly → single 