In this manuscript the authors present a scheme for ranking and refining invariance transformations within the Bayesian approach to supervised learning, thereby offering a fully Bayesian alternative to data augmentation.  The implementation presented also introduces a scheme for unbiased estimation of the evidence lower bound for transformation+representation models having a Normal likelihood (or writeable in an equivalent form as per the Polya-Gamma example).  Moreover, the authors have been careful to ensure that their implementation is structured for efficiency within the contemporary framework for stochastic optimisation of sparse approximations to the GP (variational inference with mini-batching, etc.).  Although limited by space, the examples presented are convincing of the potential of this approach.  It is my view that this is a valuable and substantial contribution to the field, although I would be prepared to concede in relation to the NIPS reviewer guidelines that in some sense the progress is incremental rather than revolutionary.  The primary concerns I have regarding the proposed method are common to: (i) the entire family of variational methods: namely, the limited justification for use of the evidence lower bound as approximation to the marginal likelihood for model selection purposes; and  (ii) the entire process of model selection by marginal likelihood: the role of prior-sensitivity. That is to say I would be very interested to see a comparison between complete and variational treatments of this problem one a class of models over which the complete Bayesian approach is computationally feasible.  Likewise, to see a connection against existing methods for complete marginal likelihood estimation over group-structured spaces (e.g Kong et al. JRSSB, 2003, 65:585-618).  But this is well beyond the scope of a single NIPS contribution.  Typos: p5: "stems less the bound" ? p8: "double-sum over the of the" ?  Minor: p2. "We believe this work to be exciting, as it simultaneously provides a justification for data augmentation from a Bayesian point of view" ... seems to me more like this manuscript is an argument *against* doing data augmentation ?