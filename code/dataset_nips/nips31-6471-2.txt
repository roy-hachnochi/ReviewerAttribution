The authors propose three-dimensional Tensor Product Representations combined with RNNs for natural language reasoning tasks. The architecture is evaluated on the bAbI task, and it outperforms DNC.  Pros:  1) The proposed architecture achieves state-of-the-art results on one natural language reasoning task with a  rather simple structure.  2) The authors provide ablation study on most important components of the model.  3) In Section 7 they provide detailed analysis of the representations, to show that their reasoning model is interpretable.  Cons:  1) Notation in Section 2 and 3 are confusing. There are \mathbf{f, r, a, b, u} in Section 2, and \mathbf{f, r, t, l, u} in Section 3, to represent entities. It is really hard to capture the basic idea from first reading. Please make the notations as succinct as possible.  2) As I understand, this architecture is inspired by Paul Smolensky's tensor memory model. My question is that why the sentence have to be mapped to a representation as in Eq.5; then representations of entities and relations are obtained from it via MLP? Why not just segment the sentence into entities and relations as motivated in Section 3, and combine them via tensor product? 3) My other concern is whether this model can be applied to more complicated NL reasoning tasks, since the authors compare their architecture with DNC.