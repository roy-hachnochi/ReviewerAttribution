This paper proposes a modified form of policy gradient which can be used for RL tasks with deterministic dynamics, discrete states, and discrete actions. The basic idea is to combine gradients which marginalize over past behaviour using the current policy's probabilities, and gradients based on the usual stochastic roll-out-based approach. This produces an unbiased gradient estimator with reduced variance relative to using only stochastic roll-outs (e.g. plain REINFORCE). A couple tricks, which introduce some bias, are added to make the model perform well.  The model is applied to the task of inferring programs from natural language specifications, and generally outperforms previous methods. The technical novelty of the method isn't great, but it seems to perform well when the required assumptions are met. The method is presented clearly, in a way that doesn't seem to exaggerate its technical sophistication. I appreciate the straightforward approach.  The paper would be stronger if it included experiments on a reasonably distinct domain, so that we can have some confidence that the method's benefits are more broadly applicable. --- I have read the author rebuttal.