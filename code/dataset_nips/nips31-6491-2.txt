This paper presents a creative new approach to structured prediction. Instead of using a parametric model to directly form a distribution on output given input, the proposed model is comprised of a retriever and editor: the retriever finds a training point that is most easily edited to form the correct output, and the editor performs the editing. This work is based on [13] where a non-conditional generative retrieve and edit model was proposed, but adds substantial and important new ideas in adapting this approach to conditional generation.  Also interesting is the novel training technique and the component approximations, which add intuition and motivation. Instead of attempting to jointly learn a parameterized retriever and editor, this paper takes what appears at first to be a simple pipelined approach. However, the authors demonstrate that the VAE-style objective for training the retriever is a formal lower bound on an objective that corresponds to training the retriever in the context of an optimal editor. In the end, the pipelined nature of the overall approach means it is extremely simple to plug in additional editors -- this whole technique is easily ported to many tasks and models.  Overall, these ideas are new and interesting and will go on to inspire lots of future work. Further, because of its simplicity, I expect this approach will be very useful in practice. Even without positive results I would advocate for this paper -- and the results are strong on two difficult tasks!   Misc: -line 65: What about stochastic EM? Wouldn't this be tractable alternative for joint learning? -Even if we could tractably sum over training data, might we still see better performance from using oracle training for the retriever?   -line 142: But the true data generating distribution doesn't depend on (x', y'). Have I misunderstood something? Or are we assuming our dataset was produced by an editing process? This seems like a critical point in understanding how the VAE objective is a lower bound on the optimal scenario for retrieval. Probably worth expanding a bit here.