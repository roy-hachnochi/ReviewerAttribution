Last review  The paper presents a new approach for Tucker decomposition based on sketching.  As  classiclal Tucker Decomposition can be solved using Alternating Least Square,  where each of the successive minimization problem related to one element of the decompositions (loading factors and core tensor) can be written as a matrcial LS problem, each of this successive problem can be  solved approximatively using sketching.  The authors propose the use of the Tensorsketch operator, defined for sketching matricial problem  where some matrices have a decomposition based on kronecker product, as encountered in the successive problems of the Alternating LS. As sketching can also be used on the inner side of a matricial product, the authors describe a second algorithm where the Tensor time matrix product is sketched. Theoretical proposition on the quality of the sketched minization are also given. Last, numerical experiments on both real and synthetic data sets are presented on large datasets. In summary : the Tensorsketch operator is inserted in the Tucker-ALS algotithm which enables the decomposition of larger tensor.  This is a good idea but the work itself may be too incremental as it lacks some global analysis of the algorithms presented : the impact of the sketching relatively to the global ALS scheme is not studied in theory whereas the algorithms show a relatively important relative error compared to ALS  in the experiments.  While it is still an alternating algorithm on the outside, the decreasing nature of ALS algorithm at each minimization is lost when the sketches are inserted, because the problem solved is not the same for each minization. How does the algorithm incurs for case when the objective function increases after solving a sketch ? The same comment can be made concerning the multipass vs one-pass. In the one pass, the sketching operators are stable, so there should be decrease between two occurrence of the same problem whereas with changing operators the problems always differs.  Moreover, claims notably on streaming (in both introduction and conclusion) should be further developed as in the algorithms presented Y is a completely known input.    Comments The proposition 3.1 is false in the sense that it is presented. The author pretends its conditioning is small because it is bounded by (1 +epsilon)^2/(1âˆ’epsilon)^2 for epsilon in (0,1). This is erroneous since the bound tends to infinity when epsilon tends to 1. But, the problem can be easily fixed by specifying a suitable value range for epsilon  In the experiments, large but sparse tensor are generated, which can be somehow contrary to the orthogonality constraints on the factors. As the sketching do not rely on keeping the factors othogonal, a non-orthogonal version of the algorithm could be a better comparison (with maybe some Tychonov regularization of the factors) 