This paper focuses on adversarially robust machine learning. As existing literature struggles to develop adversarially robust models, the authors suggest to focus on building adversarially robust features. The authors present a method to build adversarially robust features, leveraging on the eigenvectors of the laplacian of a graph G obtained from the distances between the points in the training set.  As a validation for their approach, the authors present a theoretical example where traditional methods (neural nets and nearest neighbors) fail to provide robust classifiers, while the proposed method provably provides robust features, and present experimental comparisons on MNIST data.  Furthermore, the authors show that if there exists a robust function on the training data, then the spectral approach provides features whose robustness can be related to that of the robust function, which suggests that the spectral properties of the training data are related to the adversarial robustness. This intuition is also validated experimentally at the end of the paper.  The paper reads well and the writing is insightful. The explanation of the method gives a clear intuition on why the proposed method makes sense to find adversarially robust features. Furthermore, I really appreciated the honesty of the authors in highlighting in several points the shortcomings of their approach. This paper does not solve the problem of adversarially robust learning, and the method may be computationally expensive. However, to the best of my knowledge, it is original and poses the basis of interesting future works.  That said I have a concern: while the method to build adversarially robust features makes sense, it would be useful to better articulate why adversarially robust models/classifiers can be obtained using adversarially robust features. Is there an obvious implication stating that "if we use robust features, then we will get a robust model"? Are there cases where this implication fails? While this is is somewhat obvious for linear models (which explains the success of the experiments in sec. 5.1) I would like to see some theory - or at least some intuition - explaining why robust features make sense in general, also for non linear models, like neural nets.  