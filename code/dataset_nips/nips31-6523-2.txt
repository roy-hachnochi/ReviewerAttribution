The authors present a novel implicit pruning methodology that works by calculating importance of filter connections between layers.  The paper is written very well and touches an important topic. The methodology seems sound and the experiments make sense given the method. A couple of questions & comments:  1.  "Variance which is also a uncertainty measurements could be regarded as a acceptable compromise. " (134)  The authors could use the ln(2*pi*e*variance) which would actually be the entropy under a Gaussian approximation. This would result in a higher entropy in the distribution over  Synaptic Strenght across the synapses.  2. Experiments:  a) Further comparison with CondenseNet and other simple pruning strategies are needed at the level of model inspection. Is there a difference in what connections are classified as important?   b)  How (or is)  the distribution of Synaptic Strength affected by  how deep the synapses are in the network? (how uniform/peaked on the lower/higher layers)? c) Is the distribution of Synaptic Strength similar across different task? (CIFAR/Iimagenet)?  d) Why only top x% performance is reported and not average performance, or performance after convergence? e) How many repetitions where done on the task? 