The paper presents an approach to interpretability, where model optimization involves the estimation of an interpretability score by local proxies evaluated by humans.  The integration of cognitive aspects with machine learning algorithms is an important aspect of the emerging field of interpretability. The objective is to include the interpretability requirement in the learning process in spite of the vagueness of the notion of interpretability.  The present paper describes an interesting approach, which, as far as I know, is a new way of building cognitive aspects into a learning algorithm. Papers such as this one are important as they represent a response to the challenge raised by several recent position papers to address interpretability in a systematic manner.  The difficulties described in the last section (like large variance in certain situations) give valuable information, as they point to phenomena which may be inherent in the human-in-the-loop approach in general and thus deserve to be better understood. Perhaps more of this discussion could be moved to the paper from the supplement. 