== Update after author response == Thank you for the clarifications and answers. They all sound reasonable to me. I just have one comment, about this statement: "In this light, our work can be viewed as providing evidence for the use of these proxies in the future." I agree that your work provides evidence for these three proxies, but it seems like one could more directly (and probably more easily) test the proxies by giving humans different models that vary along those proxy scores and seeing how interpretable the models are. One would hope that human-in-the-loop optimization could give us a model, or tell us some principle about interpretability, that is harder to get from the proxy scores.   At any rate, thanks again for the good paper.  == Original review ==  The authors tackle the problem of learning interpretable models by directly querying humans about which models are more or less interpretable (as measured by the time taken by humans to perform forward simulation). In my opinion, this is a conceptually interesting and underexplored direction: research in interpretability often starts by declaring that some mathematical property equates to interpretability and then trying to calculate or optimize for that property without actually running any studies with actual humans. Studies like the present one therefore fill a much-needed gap in interpretability research.   User studies take a lot of work. The strength of this paper is that it includes a broad variety of useful experimental investigations, e.g., comparing different proxies for interpretability, examining the feasibility of doing zeroth-order optimization with humans in the loop, and so on. The paper is well-written and it is clear that the authors have taken pains to be careful and scientific about their experiments.  The main weakness of this paper, in my opinion, is that it lacks strong empirical results. In particular, from what I can tell, none of the human-optimized models actually beat models that are optimized by some easily-computable proxy for interpretability (e.g., mean path length in a decision tree). Indeed, the bulk of the paper talks about optimizing for interpretability proxies, though the title and rhetoric focus on the human-in-the-loop component. What, then, do we get out of human-optimization? The authors argue in line 249 that “Human response times suggest preferences between proxies that rank models differently.” This statement comes from Figure 3a, but the variance in the results shown in that figure seems so large that it’s hard to make any conclusive claims. Do you have some measure of statistical significance? Looking at Fig 4a, it seems like models that optimize for path length do as well as models that optimize for the number of non-zero features.  Despite the above shortcomings, I think that this paper contains interesting ideas and is a solid contribution to human-in-the-loop ML research. It is hard to criticize the paper too much for not being able to show state-of-the-art results on such a challenging task, and this paper certainly stands out in terms of trying to involve humans in model building in a meaningful way.  Other comments: 1) Is the likelihood formulation necessary? It complicates notation, and it’s not obvious to me that the SILF likelihood or a prior based on mean-response-time is particularly probabilistically meaningful. I think it might simplify the discourse to just say that we want accuracy to be above some threshold, and then we also want mean response time to be high, without dealing with all of the probabilistic notions. (e.g., as a reader, I didn’t understand the SILF measure or its motivation). 2) Does mean response time take into account correctness? A model that users can simulate quickly but incorrectly seems like it should be counted as uninterpretable. 3) Also, why is HIS proportional to the *negative* inverse of mean response time? 4) The local explanations part of the paper is the least convincing. It seems like a model that is locally explainable can be globally opaque; and none of the human results seem to have been run on the CoverType dataset anyway? Given that the paper is bursting at the seams in terms of content (and overflowing into the supplement :), perhaps leaving that part out might help the overall organization. 5) On local explanations, line 154 says:  “We note that these local models will only be nontrivial if the data point x is in the vicinity of a decision boundary; if not, we will not succeed in fitting a local model.” Why is this true? Why can’t we measure differences in probabilities, e.g., “if feature 1 were higher, the model would be less confident in class y”?