The paper considers a learning to learn problem (LTL) where the tasks are linear regression and ridge regression around a common mean as a family of algorithms. Unlike in the previous works where the empirical risk of the new task is considered, the author(s) proposed to learn the common mean by directly minimizing the transfer risk. The paper uses SGD to incrementally learn the new tasks in LTL setting using the online splitting approach and studied its statistical guarantees (under certain assumptions).  The problem is interesting and as the author mentioned in the paper there are only very few papers with the theoretical guarantees for LTL. The paper is well organized and easy to follow.  Even though the underlying algorithm has been considered already (Evgeniou et. Al., KDD 2004), the application of this approach to LTL where the common mean (h) is learned by the meta algorithm using simple transformation while the inner algorithm (that depends on h) works well on new (but similar) tasks.   Under the assumption of example 1, the optimal splitting (r=0) results in using the common mean (h) for the new tasks. It will be interesting to see how this can be related to the lifelong learning with weighted majority voting (Pentina and Urner NIPS 2016) where the weights will be learned by the meta algorithm.  Usage of dataset splitting for learning this common mean is novel and the transfer risk bound shows how the splitting of the dataset is affected under the strong assumption E[w|p]=\bar{w}.  It is unclear from the bound in Prop 3 for Example 1, that we can get a consistent estimator as T-> \infty even when the sample size is kept constant. This doesnâ€™t seem directly applicable for the environment and assumptions considered in the Example 2.   This paper has several important theoretical contributions to the LTL problem setting and I recommend the acceptance of this paper.  