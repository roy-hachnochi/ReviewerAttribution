The authors consider the learning-to-learn setting under the assumption that all the tasks are linear regression problems and their solutions are close to each other in l2 norm. First, the authors observe that the transfer risk in case of squared loss and ridge regression is basically a normal expected squared loss in a modified space. Second, the authors propose to split the data for every task in two subsets to imitate the train/test split and re-write the objective function according to this data processing. Lastly, they observe that receiving one task at a time allows them to do SGD on that modified objective.  Implications of Proposition 3 are quite interesting and, I think, deserve a more elaborate discussion in the manuscript. First, it suggests to not perform any task-specific training (r=0) and implies that learning is possible even when the learner observes only one sample per task(n=1) . This feature of the considered setting is quite interesting. May be it is connected to the fact that there is no clear advantage of having more data in learning algorithm (3) (even in case of infinite training sets it will not find the optimal weight vector). Also, the number of samples per task n influences only the term corresponding to the noise-variance, while all other terms decay only with T->infinity.