This article deals with the problem of estimating the gradient of the expectation of a function of a random variable whose distribution is parameterized, an important problem in machine learning. It briefly reviews existing approaches (mainly reparametrization and likelihood ratio) and introduces the notion of probabilistic computation graph (PCG), as well as a way to compute partial derivatives in it based on the total derivative rule. Based on this, the authors discuss how to use this to estimate the gradient on such a graph. A relationship to policy gradient in reinforcement learning (RL) is then discussed. Novel algorithms for this specific case are presented, and experimented on variations of the cart-pole problem, with comparison to PILCO (the likely motivation for this paper).  I think this paper makes valuable contributions, but it is pretty hard to follow. Besides heavily relying on [1] (which is also quite hard to follow), the link between the proposed PCG and the original problem (estimating the gradient of the expectation of a function of a random variable whose distribution is parameterized) is unclear. It becomes clearer with the relationship to RL, but I wonder what more generality it could have. The proposed algorithms are also unclear, and it is hard to draw conclusions from the experiments.  More detailed comments, in the reading order: * bold p and p are not the same, but look very similar (especially as indices), would be better to choose another notation * l.61-65: how is BIW-LR linked to the original problem? We have one xi, then many xis, this is not motivated nor explained * l.70 (and other places): what means that a computation can be performed locally? * PCG: could explain briefly how it differs from a stochastic computation graph (even if clearly not the same thing). More importantly, how is this linked to the original problem? It really looks like the problem changes between 2.1 and 3, without being explained or motivated. For example, how computing the quantity in Eq.1-4 helps solving the original problem? * Eqs 1-4: would be good to give a hint of how these different equations are useful (somehow clearer after, but much later) * Sec. 3.3 is not clear. The link to the original problem is still unclear. There is no longer the phi function in the expectations, it disappeared. Why the true zera are intractable, and how to choose the hat zeta is also unclear. Therefore, the following is also unclear. How to choose the nodes IN could be explained. * Sec. 5 is not clear. In 5.1, how are sampled the particles, in what space, and why? What density is approximated as a Gaussian and why? Same problem in sec. 5.2, what are the xs, according to what distribution is mu the expectation, and so on. * Sec. 6 is hard to follow, even knowing PILCO and having been through [1]. It is also hard to draw conclusions about the proposed approach, what does it brings? * l.192, about the accuracy of the estimated gradients, are there any clues that the proposed approach improves accuracy?  =======  Thanks for the rebuttal. I think that this paper has valuable contributions, but also that the required revision regarding clarity is too significant. So, I'm still on the weak reject side. Whatever the outcome, I think that the paper could have broader impact and audience with a strong revision.