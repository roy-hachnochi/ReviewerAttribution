The main idea of the paper is to use continuity properties of the gradient of a function to "reuse" gradients i.e. for nearby points, one may reuse gradients that have been already computed. The paper develops a conceptual and algorithmic framework to exploit this idea.    Pros: 1) The idea of lingering gradients is an interesting concept 2) nice algorithm/data structure to exploit lingering gradients 3) extension of 2) to SVRG and the discussion on implementation details nicely done. Promising numerical results.   However, some major issues:  Larger problems: With the assumptions they make this is a solid paper in my opinion. However, I'm not so about the validity and relevance of these assumptions.   In abstract they claim that the algorithm has very good performance under certain conditions (alpha = 0 in their analysis if I understod correctly). To me it seems like that the only cases where this condition applies is when all component functions are constants.  When motivating assumption 2 they show a plots of |B(x,r)| for two different x and say that the plot shows that indicates that assumption 2 is true. The hard part about this assumption is that |B(x,r)| should be uniformly bounded for all x.  On the contrary it seems to me that the plot indicates that as we approach the optimum to bound |B(x,r)| with r^(b)/C either b or C has to decrease. It should be said that the method performs very well on this example so it should be possible to verify the assumptions in this case.  They have two examples in the introduction where they discuss when this method might be relevant. It seems to me that the first example (svm) is of a lot more interest to the ml community than a resource allocation problem so the choice of numerical experiment is a bit strange.   Some minor things:   In theorem 3.5 I believe that D should be an upper bound of ||x0 - x*|| and not ||x0 - x*||^2.  Figure 1 uses the '0' label for two different things. Should probably be a 10 instead.  Some inconsistency in how they present performance with regards to step lengths in figure 3a (seems weird to only present different step lengths for some of the algorithms).   In the end, the value of this paper to me really comes down to whether assumption 1 and 2 are relevant in ml applications. After reading the paper I'm not convinced that they are. 