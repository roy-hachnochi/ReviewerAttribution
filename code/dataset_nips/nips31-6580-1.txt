This article builds on previous work on signalling in multi-agent games, and incorporates mutual information in trajectories into a regularisation term to learn policies that do or do not reveal information in cooperative and competitive domains respectively. It presents an interesting algorithmic extension but does not convey the motivating significance of learning this new capacity.       While the ability to learn either behaviour is clearly beneficial and interesting from a system-designer point of view, it is unclear why an agent would sacrifice reward in the given empirical evaluation. The method resembles reward shaping, but the newly introduced rewards do not diminish, and actually change asymptotically optimal behaviour. It would seem much more interesting to understand whether this method helps in games where the reward actually drops if the opponent infers the goal well enough.       In general, the article is well written, and the presentation is rather clear. Related work is cited and the method is contrasted against alternative approaches. Minor comments, on the first page 'whether' should be 'where' and 'composes' might better be 'combines'. 