The authors present a framework, in the context of 2 player stochastic games, that enables an agent to (hide) reveal intentions to a second agent by executing (non-) informative actions or visiting (non-) informative states. The revealed information can be used by the second agent to infer what is the goal of the stochastic game. This can be achieved by extending the usual RL objective in two possible ways. On one hand,  one can add a mutual information bonus (or penalty) between states and goals, and on the other hand, between actions and goals given states. The paper presents how to estimate the gradients of those mutual informations in order to apply policy gradient updates. Some experiments in a grid world domain demonstrate the agent’s behaviour when hiding/revealing such information to the second agent.  The paper is overall a good paper. The parts of the paper with no math is well written but for the mathy sections it could benefit from a more accurate exposition (defining all terms properly), and in particular from a  more formal exposition of the true objective function that algorithm is optimizing (I will elaborate more into that later). In general, though, it is an interesting and well executed idea. It seems an original idea to use the mutual information in this way and clearly one can see the effect on the agent behaviour to hide/reveal intentions. It would have been very nice to extend the results to more complex / or high dimensional problems, since at this current state the applicability is rather limited. A part from this, it is a nice work.  I have two main concerns:  1) On the exposition: It is clear that the authors want to  maximize the sum of future discounted rewards (and KL terms). However, for example, as noted at the end of page 2, the authors mention “… we will train the agent to maximize the objective J_action[\pi] = E[r] + \beta I_action …”  where it seems that  the expectation of rewards is computed for a single time-step ( and also the expectation inside I_action). Then later after all the gradients derivation, one reads in Eq 6 that the gradient (now,  suddenly depending on time t) depends actually on the sum of rewards (that are inside A_action(t) ).  Similarly, the variables (such as the state and actions) in the I_action term that were time-independent according to Eq 1, suddenly they are time-dependent in Eq 6.    I think (might be wrong here) that the authors define the problem formulation with a mutual-information for a single-time step, but then really want to optimize a sum of mutual-informations (as one can see from Equation 7). They, somehow, achieve their final goal by doing the n-th order approximations to the densities \hat p_n (s_t |g)  and \hat p_n (s_t) and then re-arranging terms in a certain way (for example when going from Eq 21 to Eq 22 in the appendix). In general, I believe that the paper and the derivations would be clearer if the authors would properly define the objective that they are optimizing.   2) In the tables 1 and 2 in the appendix one can see that the n-th order approximation of  \hat p_n (s_t |g)  and \hat p_n (s_t) is made with n being 0 and 1 for the action and state mutual informations respectively. I expected to see bigger number here. Why did the authors choose the the zeroth order for the action mutual information, since they actually can compute a better approximation?   Minor comments:  -  I_action in line 85 is already defined in line 60 -  p(s) in Eq 1 is not properly defined. What distribution over states, the stationary? The time-dependent distribution? - Eq 5 of the appendix, If I am not wrong,  there is an  s_{t-u} that should be s_{t-n} - Eq 6 of the appendix, If I am not wrong,  the sum is up to “n” not up to “u”