Update:   The authors do a good job of answering my questions. So I am raising my score.  ****** This paper describes a method for learning a conditional GAN when there is significant corruption or noise in the labels. The paper proposes additional regularizers for training the generator and discriminator to achieve this.   These regularizers that resemble standard auxiliary conditional GAN involve learning two additional classifiers. The first one is called the permutation regularizer and aims to reduce the mismatch between the generated label and true label. The other regularizer seems to encourage high confidence label predictions. How does the later relate to conditional entropy minimization [1]?     The experiments are based on CIFAR10 and MNIST, showing the benefits of RCGAN over the original conditional-GAN.   The discussions on the experiments could be expanded. Some of the image generation plots can be brought into the main paper for completeness.     It will also be nice to have some comparative results on the related Ambient GAN method. It is not clear what the authors mean by ‘sharp analysis for both the population loss and the finite sample loss’.       The organization and writing of the paper needs to improve a bit. Line 38 needs to be reworked. At several places symbols are used much before defining them. The theoretical results seems disconnected from the main text body, it will be useful to make them crisper and move less important details can be to the appendix.    [1] Miyato, Takeru, et al. "Virtual adversarial training: a regularization method for supervised and semi-supervised learning." arXiv preprint arXiv:1704.03976 (2017).