Motivated by the problem of visualizing the loss landscape of a deep neural networks during training, and the heuristic consisting of performing PCA on the set of parameters output by stochastic gradient descent, this paper considers a simplified model where the data comes from a simple random walk in Euclidean space instead of a NN.   The authors attempt to justify this heuristic by asking what the projection of this walk on its first few principal components should look like.  An asymptotic analysis is performed and the conclusion that most of the variance of the walk is captured by its first few components is reached. Furthermore, the projection of the walk in the first two components converges to a Lissajous curve, a parametric curve in the plane of the form t -> (cos(a t),cos(b t)).   This reasoning is then extended to the case of a discrete Ornstein-Uhlenbeck process. Then the authors show that their findings are reasonably accurate compared to data coming from a NN on real-world datasets.    The idea of performing PCA on the output of a NN for purposes of visualization is an interesting one, and this paper makes a first step towards understanding this proposal through the analysis of a very simple model.     However, the analysis is conducted at an extremely precarious level of rigor. Limits from the discrete to the continuous are taken without care, and a number of approximations are made without making precise the meaning of the approximation (e.g. replacing Toeplitz by circular matrices, replacing discrete matrices by integral operators,â€¦) while still writing equality symbols in the process.    Also, the justifications related to the case of the OU process are more of qualitative discussion rather than a quantitative argument.   Therefore the conclusions can only be trusted on a very qualitative level.    To summarize, the idea is interesting but it seems that the authors have not spent a sufficient amount of effort into making their arguments quantitative and mathematically convincing.   