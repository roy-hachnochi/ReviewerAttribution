This paper presents a method for improving Variational Auto-encoder performance when additional information (such a view angle or image time) is available. The solution proposed is simply to place a GP prior over the latent representations.  The main technical difficulty the authors faced was in reducing the computational complexity of the GP and in particular allowing batch stochastic gradient descent methods to be applied, which is complicated by the non-parametric nature of the GP.  I wonder if the authors considered using a sparse GP with the fully independent conditional (FIC) assumption [1]. Using this approach, when we conditioning on the inducing variables, the likelihood factorises over data points and we can apply batch SGD directly without any problems. I thought failure to consider this approach was a weak part of the paper.  I thought the experiments were adequate and show the method does 1) improve performance and 2) out perform the benchmark algorithm. I think we would be very disappointed if the first of these had not been true however.  Overall I thought this was an OK paper. I don’t think the paper provides any great insights, although the method of pre-training the VAE then learning the GP parameters, then joint training was appreciated. I would also have liked to know how well the system worked when used in GPLVM mode, with unobserved feature vectors. However the general approach is reasonable and the exposition is fine.  Typos etc.  Line 40 word missing: A natural solution to this problem is [to?] replace the … Line 194 doesn’t quite make sense: …we found that convergence was improved by first doing inference only the VAE Line 204 word missing: …where we [omit?] some parameters for notational compactness. [1] Quinonero-Candela, Rasmussen, A Unifying View of Sparse Approximate Gaussian Process Regression, JMRL 2005. 