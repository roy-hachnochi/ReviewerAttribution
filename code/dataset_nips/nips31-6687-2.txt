I'm happy with authors rebuttal response. I'm improving the score under the assumption that the extended empirical analysis will be included in the final version  ---------------- The paper proposes a generalization of Option-Critic (OC) to more than two levels. It derives respective policy gradient updates for termination and intra-option policy.   The paper is quite clear, given the exhaustive supplementary material, can be well reproduced/followed both theoretically and empirically.  I have found the experimental evaluation to be lacking, I hope that authors can elucidate the following:  -  What are the options that have actually been learnt on each of the levels? What is their duration? Are they non-trivial? From the presented experiments it is hard to see whether learnt options amount to anything significant or simply re-learn action-repeat heuristic or even collapse completely to micromanagement (option duration of 1).  - Authors are right to point out the inconsistencies in ATARI evaluations. However, they make a very particular choice of using ATARI environment without action repeat (NoFrameskip). This is a non-standard setup and although there is nor reason not to use it, a discussion on why this choice was made is required. If proposed method only achieves improvement in this setup, this is fair enough, but it should be explicitly stated and discussed. Also, reporting results on a more standard (action repeat = 4) setup is then required. Going back to the first question, so what options does the method learn in the chosen (NoFrameskip) setup? Does it go beyond just learning the action repeat heuristic? What are the actual policies that it learns?  - It is important to notice that the reported scores for the ATARI games are, overall, extremely low. For example, on the game where most improvement is observed (Zaxxon) scores reported in the literature ([30,1,18]) are on the order of tens of thousands, compared to 300-400 in this paper. This, surely, has to be discussed! The learning curve on Berzerk looks odd, is it contains no variance. What policy did it learn? I believe this discrepancy with the literature is due to not using action repeat, as discussed above, but it is unsatisfactory that authors don't address this explicitly in the manuscript at all.  Given the comments above, I find the experimental section of the paper significantly below the standards of NIPS. I do appreciate the theoretical contribution of the paper and would've voted accept if the experimental section was done up to the standard. The experiments have to clearly answer: i) what options are actually learnt and are they meaningful? ii) do the help in a more standard setup? iii) how do they help in the setup chosen by the authors (did they re-learn action repeat?). 