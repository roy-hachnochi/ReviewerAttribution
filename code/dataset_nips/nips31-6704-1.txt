After reading the rebuttal and other reviews, I am convinced that this paper is a welcome contribution to NIPS. Thank you for sharing this idea!  -------  The authors propose DIRECT, a method which computes the variational objective for learning discrete latent variable models exactly and efficiently according to Kronecker matrix algebra. They demonstrate accurate inference on latent variables discretized up to 4-bit values, and train on millions of data points.  The paper is well-written and concise. I grealy enjoyed reading the motivation, main ideas, and experiments.  The core idea comes from Proposition 3.1, in which inner products of Kronecker product vectors can be computed in linear rather than exponential time (i.e., exponential in the number of discrete latent variables b according to the number of possible outcomes they take m). By utilizing probabilistic models and variational approximations where ELBO optimization only results in inner products of this form, they can perform optimization of the exact ELBO.  It would be great if the authors could comment on how this might extend to hybrid cases. For example, certain variables may be correlated (in the prior or variational posterior) and in those cases, it may be okay to spend the exponential complexity or even use Monte Carlo sampling.  I enjoyed the derivations for the GLM as well as preliminary experiments on discretely relaxing Gaussian latent variables.  Note the abstract, intro, and conclusion greatly overclaim significance. The idea is restricted to a specific class of models that factorizes across dimensions in both probability model and variational approximation. This is far less general than REINFORCE and it would be useful if the authors are clearer about this.