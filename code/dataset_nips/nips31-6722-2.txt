This paper is about exploration strategies for deep reinforcement learning. They present approach based on encouraging the agent's policy to be different from its recent policies. This approach is simple and easy to implement, and they show variations of it implemented within DQN, A2C, and DDPG. The show nice results, particularly in the sparse reward versions of the mujoco tasks.  This work is related to previous works on exploration in deep RL such as the work on noisy nets or Bellemare's work on using density estimators to drive exploration. However this approach is simpler to implement and has better empirical results.   The strengths of the paper are its originality and significance. The ideas in the paper are original, and since they show good results and are easy to implement across a set of deep RL algorithms, I believe they have the potential to be signfiicant and taken up by many deep RL practitioners.  The paper is well-motivated and the experiments are detailed (multiple variations of the idea applied to 3 different algorithms across many different domains). There are a few things in the implementation and experiments that are unclear, however. The paper would be much improved with these issues clarified.  Line 139 says that the agent is ensured to act consistently in the states its familiar with, as pi and pi' will yield the same outcomes for those states. I'm not sure what would ensure this to be the case. For A2C, the authors state that they maintain the n most recent policies. What does this mean exactly? They keep copies of the last n networks? The n most recent means the n networks after the last n updates? For the distance-based scaling, it seems that as the policy converges towards optimal, the D distance will go towards 0 and the alphas will continually increase. This doesn't result in instability in the algorithm? For the performance based policies, you measure the performance P(pi') over five episodes. How does this evaluation happen? Do you run this separately or was each pi' already kept unchanged for 5 episodes?  There's a few minor errors in the paper: - Line 238 is the first time Div-DQN is introduced with out explanation of what it stands for. - None of the text explains or compares to the Curiosity-DQN results in the plots. - It's not obvious what the Div-DQN algorithms in the plot legends refer to (e.g. what is Div-DQN(Linear)?)  - The refernece list format is broken, many of the papers show up with the authors as {first initial} et al. {last name}  