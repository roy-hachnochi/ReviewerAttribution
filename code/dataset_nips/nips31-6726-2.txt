The paper introduces connection between posterior regularization and entropy/KL regularized RL. Through this connection, it proposes to adapt the posterior constraint functions on real data, and shows that the with learned constraints it leads to significantly better generative models compared to the baseline or fixed constraint. It also provides a variant for implicit generative models, where the KL is reversed when fitting the generative model. The results are promising, and the method seems to be applicable for different generative datasets.  A question is if adapting constraints on real data could lose its original benefits of constraining the data through high-level feature matching. If the neural networks are very powerful, even if they are initialized with pre-trained parsers etc, could they unlearn them and learn arbitrary statistics that aren’t semantically meaningful? Through constraining specific network architectures and regularization with respect to other tasks, this may be avoided, but it would be good to include some discussions. For example, can you just initialize a specific network architecture with some random weights, and see if it will learn good constraints purely from observed data (without pre-training on external dataset/task)?   Comments: -in section 4.1, another benefit of using p_\theta as proposal is that it doesn’t require likelihood evaluation, which is not tractable for implicit models. It’s also good to comment it’s a consistent but biased estimator.  -Given it’s not sequential decision making problem, such close connection to RL isn’t very necessary. You may just discuss it from the perspectives of KL minimization/calculus of variations.  