Summary ------- This paper presents a framework which makes use of manifold learning to add structure into sparse coding models. By adding a manifold structure in the space of the space of the dictionary atoms, it permits to represent each data points with sparser coding signals as the dictionary can be adapted to the specific data point inside the manifold. This idea is combined in section 3 with a temporal regularization for the learned representation which ensure that the sparse code in a video are not changing too fast.   Overall assessment -----------------  Overall, while this paper takes a pedagogical approach to the general concepts of sparse coding and manifold learning, it lacks of proper formalism. The objects in equation (1) to (5) are not defined at all. The spaces in which x, phi, alpha and other variables are defined are not given, making it very hard to follow in the following. Also, the notion of manifold embedding, central in this work is never properly explained. This let the reader guess what are each variable.  Moreover, the evaluation of this technique is weak, as only qualitative results are shown on limited dataset (20x20 video with a 100 frames). Also, the reconstructed video displayed in the one used to learn the model. More extensive evaluation of this technique is needed, on larger video and with more quantitative results.   Questions --------- - Eq(2-3): If I get it correctly, phi_data =x? - Could you clarify the difference between the phi_LM and the P_LM? - Figure1: If I understand correctly, k should be f in the legend to match the text? -l140: I don't see where the sparsity intervene in section2.1. Could you clarify? -Figure3: The obtain representation beta in figure3B is dense. Isn't your goal to derive a "sparse" transform?  Typos and nitpicks ------------------ -l40/43: need citations -l110: a m-dimensional manifold in which space? -l111: a generalized eigenvalue decomposition of what? -l227: 'affinity groupe'  Post-rebuttal comments --------------  I have carefully read your rebuttal and the other reviewer comments.  Overall, I agree that the idea is interesting and novel but this paper intention is to bring together manifold learning and sparse coding and I think it lacks pedagogy to do so. It is very vague, as also stated by R1 and as the different spaces are not clearly stated, it is hard to perceive which object is a vector or a function and in which dimension. Personally, I find it really confusing and hard to follow. The rebuttal does not seem to acknowledge this as stated with "Sparse coding is pretty standard stuff" and "the notion of manifold embedding is pretty clear in the manifold learning literature" and does not clarify the concepts questioned by R1. I am also confused at the fact that the transform is looking for h-sparse function, which are not expected to be sparse according to the rebuttal.  I still recommend rejection, but mainly based on the narrative aspect of the paper. It might be biased by my lack of knowledge in manifold learning but I think the pedagogical part is very important for such a novel approach. 