Paper 6729: The Sparse Manifold Transform  OVERVIEW: The authors present an unsupervised learning algorithm that combines (1) sparse representation, with (2) steerable nature through linear combinations of landmarks, and (3) includes temporal smoothness constraints in the basis of the representation. The authors show that the transform is (a) quasi-invertible for sparse signals,  (b) it finds sensible groups in the dictionary, (c) it has interesting nonlinear interpolation properties, and (d) stacking several of these layers complicates the identified features as one goes deeper into the structure.  Overall, I find the work very interesting because the proposed method combines sensible assumptions and sensible properties emerge from natural datasets. Results are promising and the properties of this transform should be studied in more detail in the future.  RECOMMENDATION: If the authors improve some comments on the methods modelling the relations among sparse coefficients, and make the code available for the scientific community (the last is mandatory) I think the work should be accepted in the conference.   MAJOR COMMENTS:  The interested reader could find technical details and experimental illustrations too schematic. That is why I think the authors have to make the code available. However, presentation and experiments  are fair enough given the length restrictions of a conference paper. In follow up works authors should focus on comparisons to alternative representations (which may not be mandatory  in this first report). For instance:  Does the proposed transform have consequences in neuroscience?. Does it explain better than other unsupervised learning methods the properties of natural neurons (feature tuning) and their interactions? On the engineering side, how can this representation improve pattern recognition tasks? (as opposed to others).  Bibliographic note. In sections 1.1 and 4 the authors mention some literature that identified remaining relations between the coefficients of sparse representations (or linear ICA representations) and criticise that these relations are not learnt but typically imposed (Hyvarinen et al. 01, Osindero et al. 06). The authors also cite the literature that captures these relations through statistical criteria but citation  is not consistent in both places L52 and L264 (please cite all references in both places).  However, they miss some references about the relation between wavelet-like sensors in V1. In this literature the relation between coeffcients sometimes is learnt statistically as in some of the references already included and also in (Schwartz&Simoncelli Nat. Neurosci. 01, Laparra&Malo Front.Human Neurosci. 15), but other times they emerge from psychophysics and have interesting connections to the PDF of images as in Malo&Laparra Neural Comp. 10.  Specifically, Schwartz&Simoncelli01 point out that the referred relations between wavelet-like filters do exist and show that divisive normalization with local interaction between neighbors of the representation may reduce this statistical relations. They learn the neighborhoods and interactions to reduce redundancy.  Information maximization after linear transforms also gives rise to the same kind of adaptive nonlinearities where data leads to stronger interaction between coefficients which are closer not only in scale and orientation as reported in [28], but also in temporal frequency or speed (Laparra & Malo 15). Actually, the prinicpal curves method in [28] and (Laparra & Malo 15) heavily relies on the manifold flattening concept referred in [10] and [17]. Interestingly, the psychophysical tuning of these interaction neighborhoods between wavelet-like neurons using divisive normalization also gives rise to bigger redundancy reduction (Malo&Laparra10).     MINOR COMMENTS  - L227 grouop -> group - I would appreciate more detailed information on how Fig.4 is computed (even though I understand space restrictions).  REFERENCES:  Schwartz & Simoncelli 2001. Nature Neuroscience  4, 819â€“825. doi: 10.1038/90526  Laparra & Malo 2015. Frontiers in Human Neurosci. https://doi.org/10.3389/fnhum.2015.00557   Malo & Laparra 2010. Neural Computation 22(12): 3179-3206 