In this paper, the authors propose a recurrent neural network architecture which is based on the GRU but uses complex valued hidden states (but real-valued gates) and unitary transition matrices. The model is compared to relevant baselines on two toy tasks and a human motion prediction problem.  The work is mostly well presented and easy to understand, and the design choices well motivated. Some improvements could include: dropping some of the repetitions (e.g. about the restrictions of holomorphic functions), motivating Stiefel manifold optimization over e.g. projected gradient descent, or better motivating the choice of having real-valued gates.  The main exception to the above comment is the experimental section, which leaves important questions open. The motion dataset needs to be better presented (in more details). Baseline numbers taken from previous publications would also be welcome. As it is, it looks like this paper's implementation of the GRU baseline does significantly worse than the one presented in Martinez et al. for smaller seeds (80 and 160). Why is that?  Finally, while I would not consider the following paper a required citation as it is relatively recent, the authors might find it relevant: QuaterNet: A Quaternion-based Recurrent Model for Human Motion, Pavllo et al., 2018  Some typos: l. 1     -- Complex numbers l. 21   -- Norm-preserving l. 80   -- z = x + iy l. 110 -- forward-pass instabilities l. 184 -- s^b and s^d 