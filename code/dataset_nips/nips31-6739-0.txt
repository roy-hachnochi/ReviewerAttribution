In this paper, authors proposed a metric called warping distance to measure the distance between raw sequence. BetaCV is optimized to learn the parameters in the metric and the robustness of this metric to initial guess of clustering is proven. Compared with using Euclidean distance between sequences’ latent representation, the proposed method shows some potentials to get better clustering results.  My main concerns include: 1. I think authors may underestimate the power of autoencoder. The LSTM-Autoencoder used in this paper is too simple. If a VAE structure is introduced to represent latent variable h, a better reconstruction result may be available and the latent variable h may have better clustering structure. In other words, without trying more powerful autoencoder, the comparison between the proposed method and latent representation+Euclidean distance is unfair to some degree. 2. Additionally, when raw data “t” are high-dimensional observations, the distance defined in original high-dimensional space may not work well. Is it a potential risk of proposed method? 3. The scalability of the proposed method is a problem. Even if applying SGD, the complexity of the proposed method is O(N^2). When each sequence has a lot of samples (i.e., large N), which is very common in practice, the proposed method is inapplicable.  4. In the experimental section, authors seem to ignore the comparison between the proposed method with more fancy “latent representation+Euclidean distance” strategies. 5. The notations in the Proposition 1 is incomplete. The x is not well-defined.  --------------------- After rebuttal ----------------------- I agree with Reviewer 2 and 3 that this work has some interesting points, e.g., learning a distance metric in an unsupervised way and unifying some distance metrics with a common parameterization. My main concern is the solidness of the experiments --- 1) I still think that VAE, especially its variant with GMM prior, should be considered as a baseline. 2) 10-dimensional data actually is not real high-dimensional data in machine learning and data science. In summary, the curse of dimensionality should be taken more seriously and more baselines should be considered.