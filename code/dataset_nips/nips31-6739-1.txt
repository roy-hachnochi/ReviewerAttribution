summary: 6/10 This paper proposes a warping procedure in an unsupervised manner. Specifically, it has two separate stages. First it uses an auto encoder to learn a latent embedding for trajectories. Second, it uses a conventional metric betaCV and optimizes the average distance of trajectories within the same cluster determined by their latent representations. A few experiments were conducted on both synthetic and real data. The results can be more convincing.  quality: 5/10 There are places which are a bit hand wavy. For example, in the `Warping distances` section, it is mentioned `distances are generally well-suited to trajectories, this serves to regularize the process of distance metric learning, and generally produces better distances than using the latent representations directly `. It is nice to investigate/elaborate this as one natural questions is why not simply use the Euclidian distance induced by the learned latent representation as the distance metric but still using the second stage?  A proposition is provided suggesting the latent betaCV may be a reliable metric for the ground truth. But it would be nice if this proposition has any practical indications, e.g., how long the trajectory length needs to be for a set of trajectories with C clusters. As in the current work, there’s not much insight from the proposition.  3 experiments were conducted: 1 synthetic, 1 real, 1 semi-real (real data injected with noise). However, there are some questionable parts about each of the experiment. In the synthetic data, only additive noise and outliers are added; sampling rate is not mentioned but this is a main motivating point for this work. Quantitively addressing the robustness to sampling rate should be an important justification. In the real data, the metric is a bit heuristic, i.e., the closeness within same clusters varying with number of clusters. There are only 500 trajectories in total and tens of clusters. It is not convincing how strong this metric correlates with the quality of the warping procedure. Not to mention SSPD is better than Autowarp when number of neighbors is relatively small, which is a more reasonable regime to care about (e.g., <50).  In the semi-real data where noise is injected. However, clearly those injected noise follow the model assumptions used in the proposed procedure, which is an unfair comparison for other procedures. This makes this ‘real data’ experiment much less convincing.   clarity: 8/10 This paper is in general clearly presented.   originality: 7/10 It is nice that the distance metrics are summarized with a common parametrization. The tools (auto encoders, optimization method) are conventional, but they are used in this context in an interesting way.  significance: 7/10 Comparing time series is an important problem. If the proposed method is investigated in a more rigorous way, it can be more impactful.  Update: The authors addressed my concerns reasonably well, though the experiments can be much stronger than presented in the paper.