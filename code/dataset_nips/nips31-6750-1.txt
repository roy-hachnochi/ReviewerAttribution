This paper presents a method for improving the performance of DQN by mixing the standard model-based value estimates with a locally-adapted "non-parametric" value estimate. The basic idea is to combine the main Q network with trajectories from the replay buffer that pass near the current state, and fit a value estimate that is (hopefully) more precise/accurate around the current state. Selecting actions based on improved value estimates in the current state (hopefully) leads to higher rewards than using the main Q network.  I thought the idea presented in this paper was reasonable, and the empirical support provided was alright. Using standard DQN as a baseline for measuring the value of the approach is questionable. A stronger baseline should be used, e.g. Double DQN with prioritized experience replay. And maybe some distributional Q learning too. Deciding whether the proposed algorithm, in its current form, makes a useful contribution would require showing that it offers more benefits than alternate approaches to improving standard DQN. --- I have read the author rebuttal.