In this paper, the authors studied a connection between optimization and regularization paths. The main results show that the iterates derived by gradient descent are close to the solution of regularization paths if the time index and regularization parameters are appropriately selected. This relationship is then used to study generalization analysis of gradient descent.  (1) It seems that the derived bound $O(e^{-mt}-e^{-Mt})$ shows not essential advantages over the bound $O(e^{-mt}+e^{-Mt})$ based on traditional approaches. Indeed, both the bounds are of the order $O(e^{-mt})$ if $m\neq M$.  (2) In Theorem 4, it is required that the empirical risk $R_n(\theta)$ is strongly convex, which is a bit restrict. In my opinion, the empirical risk generally can not be strongly convex. Also, the assumption $1/v\geq 2\|\nabla R_n(\theta)^*\|/\|\theta^*\|$ is not natural enough since the right-hand side is a random variable.  (3) Corollary 5 requires $t$ to be upper bounded by a random variable. Perhaps we are more interested in the case of $t$ large than a constant. Moreover, the derived bound on $\|\theta(t)-\theta^*\|$ in the last inequality of Section 3.1 is of the same order of the term based on traditional error decomposition, as stated in the last paragraph of page 7. If this is the case, the analysis in this paper shows no advantage over traditional analysis.  (4) In the comparison with stability, the authors claimed that their approach can obtain high-probability statements. However, this seems not to be true. Indeed, with a restatement, the bound grows as a square-root function of $1/\delta$ rather than a logarithmic function of $1/\delta$, where $\delta$ is the confidence parameter. Furthermore, the authors claim the bound based on stability does not provide an early stopping rule. I am afraid this is not the case. The bound in line 176 suggests that one can stop the optimization if $e^{-mt}=O(1/n)$.  (5) In the experiments with classification, the sample size is a bit too small.