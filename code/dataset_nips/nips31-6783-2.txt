Summary: This paper presents a general form of an algorithm for learning with beam search. This general form includes in it various learning to search methods in the literature -- early stopping perceptron, LaSo and Dagger. The paper also presents a regret analysis for this algorithm and shows no-regret guarantees for learning with beam search.  Review:  The generalization of various algorithms presented in the paper is interesting. Perhaps the primary contribution of the paper, however, is the no regret guarantee in section 5 -- previous guarantees for this setting are perceptron-style mistake bounds. This is a nice result.  It is slightly surprising that the continue strategy (i.e. Dagger style imitation learning) has better guarantees than LaSo. I wonder if this makes a difference in practice though.  However, the paper is notationally very dense and not easy to read. Somewhat frustratingly, a lot of the heavy lifting in the proofs have been moved to the appendix. For example, the paper talks about theorem 4, but even the statement of the theorem is in the appendix. It would have been good to see at least proof sketches in the body of the paper.  Given that the primary contribution is a theoretical one, this part of the paper needs better explanation.  