This paper gives a complete analysis of how many iterations are required for a Krylov subspace method to approximately solve the “trust region” and “cubic-regularized” quadratic minimization problems. These problems take the form:  min x^T A x + b^Tx   subject to ||x|| <= R or with an additional regularization term of + param*||x||^3. A is a symmetric, but not necessarily PSD matrix (i.e. it can have negative eigenvalues). The objective function is not necessarily convex.  Problems of this form are important in a number of applications, especially as subroutines for regularized Newton methods. In addition to their practical importance, such methods have recently been used to give the fastest theoretical runtimes for finding stationary points and local minima of general non-convex objective functions. The key ingredient in this recent work are provably fast approximation algorithms for the cubic-regularized quadratic minimization problem.   The goal of this paper is to analyze a significantly simpler approach for solving both regularized minimization problems approximately. In particular, either problem can be solved via a standard Krylov subspace method: compute b, Ab, A^2b, ..., A^tb for some relative small parameter t and optimize the objective function subject to the constraint that the solution lies in the span of these vectors. This can be done efficiently, although I would like to see a worst-case theoretical runtime stated for the subproblem (even if it’s slower than using Newton’s method as the authors suggest). Regardless, in most cases, the dominant cost of the algorithm is the computation of b, Ab, A^2b, …, A^tb. The author’s main contribution is provide upper bounds (and nearly matching lower bounds) on how many iterations t are required to obtain a good approximation solution. They provide bounds that depend polynomially on a natural condition number of the optimization problem and logarithmically on the desired accuracy epsilon, as well as bounds that depend polynomially on epsilon, but hold even for poorly conditioned problems. These later bounds show that the basic Krylov method matches what was established in previous work using more complex algorithms.  The main technical tools used are standard bounds from polynomial approximation theory, which the authors first use to analyze the trust region problem (with ||x|| constrained to be < R). This result then immediately yields a result for the cubic regularization problem, since an optimal solution to that problem is teh same as an optimal solution to the trust region problem with a specific setting of R. While the analysis is relatively straightforward and should not surprise those familiar with Krylov methods, the convergence results obtained haven’t appeared in the literature before (even while more complex algorithms have been studied).  For me, this makes the paper is a clear accept. It is also well written and easy to follow and I appreciated the matching lower bounds.     Small comments:  - Is it possible to state a worst-case runtime bound for solving the problem restricted to the Krylov subspace using Newton’s method? I think it would be nice to include. Alternatively, is it clear how to solve the problem using a slower direct method (i.e. eigendecomp of the tridiag Lancoz matrix)? If so, what’s the runtime?  - I like that most of the proofs pretty directly rely on basic polynomial approximation bounds. Is there some reason these technique don’t work for Lemma 2? It seems to me that it should be possible to prove directly, instead of through Tseng’s version of accelerated gradient descent. In particular, using your Lemma 4, it should be possible to directly argue that after t = \tilde{O}(sqrt(lambda_max/eps)) iterations, there is some x in the Krylov subspace such that ||x - s_*|| <= (epsilon/\lambda_max)*||s_*||. Then the claim would follow from the fact that x^TAx - s_*^TAs_* <= epsilon*||s_*|| and  b^T(s_* - x) <= eps/lambda_max*||s_*|| ||b|| <= epsilon*||s_*||^2.  Is the problem that this approach would lose a log factor? If so, can it be improved by using the standard accelerated gradient polynomial, instead of appealing to Tseng’s version? E.g. the polynomial from https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/? I find the reliance on Tseng’s analysis makes the paper feel somewhat incomplete since is seems like the reference work has never appeared in publication, nor been cited (correct me if this is wrong), so presumably it is not particularly well vetted.   - At line 585 if seems like a reference is missing. 