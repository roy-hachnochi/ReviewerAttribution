Summary: This paper presents a novel technique for reducing communication overheads in distributed optimization. Specifically, the method produces random sparsifications of stochastic gradients. Due to their sparsity, the gradients requires less communication cost. This work proposes an optimization problem that produces an unbiased gradient sparsification with a minimal number of non-zero entries, subject to some variance constraint. The authors derive a closed-form expression, as well as a greedy approximation algorithm that can be computed faster. Finally, the authors analyze the expected sparsity of the output of their method and empirical results demonstrating that their method can result in significantly reduced communication costs in practical settings.  I believe that the paper is good, but needs some thorough revisions before submitting the final version. In particular, some of the theory needs more rigor and the paper could use revisions for spelling, grammar, and mathematical typesetting.  Quality: The submission is generally sound. The proofs in the appendix are missing a few details and need some more elaboration, but can be made rigorous. Moreover, the empirical results are extensive and demonstrate the efficacy of the proposed method. The authors do a good job of discussing the pros and cons of their method, and discuss theoretically when the algorithm may be a good idea in Section 3. The authors make a few misleading claims about their work, however. In particular, they mention in the abstract that they introduce a convex optimization formulation. However, (3) is not clearly convex, so any claim to that effect needs justification. This issue is also in lines 38 and 293.  The primary issue I have with the theory is that there is an occasional lack of rigor, especially the proof of Theorem 1. The authors apply the KKT conditions to the optimization problem in (3). However, (3) involves the condition that the probabilities p_i satisfy 0 < p_i <= 1. The KKT conditions generally apply only to inequalities, not strict inequalities. While I believe that almost all of their proof can go through, the proof as stated is not fully rigorous. Additionally, the authors apply complementary slackness in section 2.2 which again would need to be justified. Similarly, the use complementary slackness to determine that they want to find an index k satisfying (4). However, they claim that it follows that they should find the smallest index k satisfying this, which again needs more justification.  One minor issue concerns Algorithm 3. While it is intended to be faster than Algorithm 2, it is not immediately clear that it is faster. In particular, Algorithm 2 requires O(klog(k)) computations, while each iteration of Algorithm 3 requires O(k) computations per iteration. Whether or not Algorithm 3 can be effective with fewer than log(k) iterations is not discussed. Moreover, the paper makes unsubstantiated claims that Algorithm 3 provides a good approximation to Algorithm 2 (as in Line 176-177). Either a theorem to this effect should be given or any such reference should be omitted or at least made more accurate.  Clarity: The paper is generally easy to follow. However, there are a decent number of typos, grammatical issues, and mathematical typesetting issues that should be corrected. Some example issues are listed below: -Line 27: "2) about how to use" should just be "2) how to use" -Line 33-34: "with minor sacrifice on the number of iterations" is grammatically odd. Maybe something like "with only a minor increase in the number of iterations." -Line 34: "of our" should just be "our" -Line 49: The typesetting of x_n collides with the line above it -The second math equation after line 58 should be broken up -Line 59: Lipschitzian should just be Lipschitz -The spacing in the math line after 72 is inconsistent. -Equation (4) should be separated in to two lines, as the condition on lambda is distinct from the following inequality, which is subsequently referenced. -Line 172 "is relatively less increased comparing to the communication costs we saved" is grammatically awkward. -Line 237 "CIFAR10dataset" should be CIFAR-10 dataset. -The caption of Figure 4 intersects the text below it.  Originality: The methods are, to the best of my knowledge, original. While the paper belongs to a growing body of literature on communication-efficient training methods, it fills a relatively unique role. Moreover, the paper does a good job of citing relevant papers. My only critique of the introduction is that descriptions of some of the most relevant work (such as [17] and [2]) are only briefly discussed, while some work (such as that on TernGrad, a competitor to QSGD) is omitted.  Significance: This paper is an interesting, useful contribution to the growing work on communicatin-efficient distributed machine learning. It analyzes the task at a high level and as a result is applicable to any algorithm that involves communicating vectors in a distributed setting. Moreover, it takes the time to actually solve its associated optimization problem instead of simply proposing a method. Moreover, it does the necessary legwork to compare to many other related methods, both sparsification methods and quantization methods. Moreover, the work gives experiments on multiple algorithms, including SVRG. It is significant and hollistic in its approach.  Again, the paper has a unique idea, good theory, and a good variety of experiments. Its only real flaws are a lack of rigor in some of the theory, some claims that are not fully substantiated (but whose omission does not detract significantly from the paper) and some grammatical and typesetting issues. These can all be remedied, and as long as this is done for the final version then I recommend acceptance.