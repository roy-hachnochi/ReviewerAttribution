Update: I thank the authors for their thorough response. I'm now further convinced of the significance of their work, especially with the addition of experiments varying network capacity and dataset size. I've increased my score accordingly. ------ The paper analyzes the behavior of generative models of images on synthetic datasets where latent factors like object count, size, and color are known. The authors observe that for each latent factor, the model distribution generally has greater spread than the original dataset. Another main result is that models tend not to learn complicated joint distributions over several latent factors when they are highly multimodal, instead partially reverting to modeling the factors independently. The findings are consistent across VAEs and GANs with fully-connected and convolutional architectures.  Despite tremendous recent advances in generative modeling, our best models remain poorly understood and have potentially-problematic behaviors which we're not aware of. This paper makes a solid contribution towards characterizing and understanding generative model behavior. Further, the empirical methods introduced (generative modeling on synthetic image datasets with known latent factors) can be useful to future work which seeks to evaluate generative models.  The experiments are thorough, well-described, and generally convincing. Some experiments are similar to previous work in generative model evaluation, though the ones in this paper are generally more thorough: - Many of the experiments are similar in spirit to the "visual hyperplane task" in [1], though that work was in a different context. The precision-recall experiments are similar to ones from [2] in the context of model evaluation. - The consistency of the results across VAE/GAN might be slightly overstated; Figure 8 shows that there is some gap in the extent of the effect, though not in the overall trend.  One potential empirical question is how much of the spread observed in e.g. Fig 5 results from estimation error in the measurements rather than modeling error? A note to clarify this would improve the overall argument.  All of the algorithms studied are asymptotically consistent; in the limit of infinite data and model capacity, they will exactly recover the data distribution. This paper is about generalization and inductive biases with finite data and model capacity; given that, an important question is precisely how large these networks are, and how big the datasets are (it's only mentioned in passing that they're trained on ``hundreds of thousands of distinct examples''). At the least, the authors should include these experimental details. The paper would be stronger with an experiment on how the observed effects vary with dataset size and model capacity -- do the effects drop off at all as we scale up, or should we expect them to hold for the largest models we can feasibly train?  The experiments are generally clearly described and the claims follow directly from the experiments. The authors do a good job of connecting their work to cognitive science literature without overstating the connection. The clarity of the presentation could be improved somewhat by explaining the indended meaning of ``generalization'' -- in this paper it means something slightly different from the typical sense (i.e. the model hasn't overfit to the training set).  Overall I recommend acceptance.  [1] "Parametric Adversarial Divergences are Good Task Losses for Generative Modeling", Huang et al. [2] "Are GANs Created Equal? A Large-Scale Study", Lucic et al.