It's been known that word embeddings trained by popular methods (e.g. word2vec) show a strong bias towards popular words, and that the frequency information is strongly encoded in the embedding vector at the expense of semantic similarity of rare words.  This paper is an attempt to alleviate this issue using adversarial training.  The authors borrow the DANN architecture to train 'frequency agnostic' word embeddings.  The resulting embeddings are shown to better encode semantically similar rare words and improve performance on downstream tasks (word similarity, LM training and machine translation).  The paper is clearly written.  Both the qualitative and quantitative evaluations are of high quality.  I'm convinced that the method really works to improve rare word representations.  It is also a noval application of adversarial training and has wide applicability.  One question I still have which is unanswered in the paper is this: - It's clear that the adversarial loss mixes the frequent and rare words together in the embedding space.  However, it is not clear why this method results in semantically similar rare words coming closer together.  There is nothing in the method that does this directly.  It could conceivably be the case that rare words are mixed together with popular words, while still being far from their semantic neighbors.    If the "why" question was better answered, I would be much more confident in my rating.  Other: it would be good to see fig 1 and the full table 7 in supplementary material be reproduced with the new model