The authors propose a learning framework to improve the word embedding. They start from studying a common phenomenon in popular word embedding models such as word2vec and Glove model, i.e., the rare words usually do not have semantically related neighbors because of their low frequency in the corpus. This phenomenon is also reported in the paper "All-but-the-top: Simple and effective post processing for word representations(ICLR 2018)." However, this paper (ICLR 2018) simply eliminates the common mean vector of all word vectors and a few top dominating directions from the word embedding.  To address this problem, the authors introduce a discriminator in the learning framework to categorize word embedding into two classes: rare words or popular words. For each specific NLP task, this learning framework includes two parts: a task-specific model and a discriminator. The learned word embedding not only minimizes the task-specific training loss but also fools the discriminator. The experimental results demonstrate their framework.   Itâ€™s not the first time that people introduce the adversarial training into NLP tasks. In "Professor forcing: A new algorithm for training recurrent networks(NIPS 2016)" and "Word translation without parallel data(ICLR 2018)", the authors both adopted the adversarial training, but from different aspects. This paper mainly focuses on the word embedding, which is different from the previous work.  The strength of this paper is that the experiments nicely demonstrate the effectiveness of the proposed approach. Also, the paper is well organized and well written.   However, my concern is whether the training process will converge and achieves an equilibrium effectively. This paper will be more convincing if the authors could analyze and conduct experiments on this issue.   I believe the contribution of this paper is important and I recommend this paper to be accepted.