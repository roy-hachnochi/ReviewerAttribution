This paper describes a latent variable approach to neural machine translation, where the generation of the source and target are conditioned on a latent layer which is claimed to learn the sentence's meaning. The idea is a natural extension of previous work, and also permits a multilingual extension where the source and target word embeddings are annotated with input languages, enabling so-called zero-shot translation and, more interestingly, allowing a more theoretically pleasing integration of monolingual data than prior work that provides a boost in translation quality, as measured by BLEU.   It's clear that this line of work is very much at the proof of concept stage, and I think the ideas presented in the paper are novel and interesting. I liked the "missing word" experiments as a means of testing whether the latent annotation learns anything. The authors broach (section 3 paragraph 1) what to me is the most interesting question addressed by this work: how is the latent variable approach, or other similar inductive biases, different from just increasingly the parameterization elsewhere in the model? I think these experiments go a step in the direction of answering that and corroborating the authors claims about what z is doing, but wish there were more experiments in this direction. Also, given how important these experiments are to the paper, it is unfortunate that the paper does not even describe how the deleted data was produced.  In general, the paper was very light on these kinds of experimental details that would help with reproducibility.  MINOR POINTS  Figure 2 is quite hard to read and the resolution provides very little information. Two rows might help.  Table 3 could benefit from some annotations calling the reader's attention to important differences and distinctions among the presented hypotheses.