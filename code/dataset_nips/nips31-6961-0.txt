This paper describes an approximate approach to learning a generalized mixed regression. First, the problem is approximated by empirical risk minimization. Then, a combinatorial constraint is relaxed to an atomic norm penalty. The algorithm proposed to solve the derived problem is a greedy one, where at each iteration a MAX-CUT-like subproblem is approximately solved (up to a constant-ratio approximation) by a semidefinite relaxation (similar to the one commonly associated with MAX-CUT). The paper includes a convergence analysis covering the algorithm, and a generalization analysis covering the original approximation/relaxation of the original problem formulation.  The result is relevant to a core tool in machine learning (mixed regression). The writing is remarkably clear and the exposition easy to follow. The end of Section 5 reconciles the gap between the guarantees of this paper and the difficulty of solving original problem, which is NP-hard. The experiments are thoroughly described and designed in a way that makes natural comparisons and illustrates the observed behavior of the algorithm.  One point of feedback, which I see as optional, but that may benefit the presentation, would be to motivate the generalized mixed regression problem by relating it to other problems. Are there any interesting special cases of generalized mixed regression other than mixed regression? Are other generalizations possible, and are they harder or easier?  Typo on line 119: "semidefinie" -> semidefinite Line 182: formatting issue with "argmin" expression.