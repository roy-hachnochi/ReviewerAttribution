This paper studies certified defenses against adversarial attacks on neural networks and proposes a Semidefinite Programming (SDP) relaxation for multi-layer networks with Rectified Linear Unit (Relu) activations and adversaries bounded by infinity norm ball. The paper builds on the linear programming approach of Kolter and Wong, where the authors express the relu constraint as an equivalent quadratic constraint. Writing the quadratic constraint as a SDP is then straight forward. The certification upper-bound is then obtained by considering a Lagrangian relaxation of this constrained problem.  My main issue with the paper is that while the SDP relaxation introduced may be novel, it is not clear that this will ever scale to anything beyond simple toy problems and toy architectures. Besides this, the SDP relaxation crucially relies on relu non-linearities and it is not clear how they can extend this to other non-linearities such as sigmoid, tanh etc or with other non-linearities typically used by practitioners such as batch normalization etc. The other issue is the usefulness of studying l_infinity bounded adversaries: no real world adversary is going to restrict themselves to a fixed epsilon ball, and even if they do, what choice of epsilon should one choose for the certificate?  In light of this, I think this paper is more suitable for a theory conference such as COLT and not very well suited for NIPS. 