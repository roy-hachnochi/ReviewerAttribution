This paper proposes a new method for certifying adversarial robustness of ReLU networks. Here "certified robustness" is achieved when the "worst-case margin" in Eq (2) is negative. Computing this "worst-case margin" is intractable and the authors instead upper bound this worst-case margin by formulating a quadratically constrained quadratic program, and then relaxing it to a convex semidefinite program. Experimental results, on small fully connected neural networks, show that this SDP-cert bound can provide tighter bounds on networks trained differently.  The explanation of the general idea of SDP-cert is fairly clear, and the authors also explained its relationships to previous work on LP-cert [11] and dual verification [5]. The new results on certifying PGD-NN is significant.  However, there are places where the presentation is not very clear and some of them actually affect understanding. In particular, I would like some clarifications for the following questions 1) If I'm understanding it correctly, non-convexity in Eq (4) is from z_j^2=z_j(W_j^T x). How does SDP relaxation make it convex? 2) Figure 2b and 2c, what're the coordinates of the bottom left corners? 3) Dimensionality of P in Eq (8), is it a n-by-n matrix where n=\sum_{i=1}^L m_i ? and how does the computation scales with the size the of the network? 4) around line 212~220 re: layerwise bounds l and u, does values of l and u only affect the optimization speed? or it actually affect the optimal value f_{SDP}? how much difference does it make for not having l and u, having looser l and u and having tighter l and u? 5) just to confirm 25 minutes (line 252) is a single example? and is there a big difference between time needed for PGD-NN, LP-NN and MaxGrad-NN?  Also I think the paper should include the following contents to be complete 1) line 278-282, there should be numbers/table/figures accompanied, the current form is not precise and not convincing 2) what is the main bottleneck to verify larger network? computation? memory? what are the computational and memory complexities? 3) what happens to the SDP if epsilon is larger? say 0.3? will the bound become more vacuous? or will be any computation issues? also how does it compare to other methods?  There are also many minor issues - line 68: non-negative should be non-positive? - line 129: P=V^T V, not consistent with previous text at line 117 where P=V V^T - line 136: The radius is probably (u_1-l_1)/2? instead of l_1*u_1? - Figure 2: colors are confusing because of the inconsistency between figure and the legend - line 146: “increases” should be “decreases” ? - line 274: tighte - a few failed latex \ref{}, e.g. line 46 - line 214~215, grammar - ref [11] [22] are the same paper - line 206 typo in the inequalities  I would recommend acceptance if these issues can be resolved. 