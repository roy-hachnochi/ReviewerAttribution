This paper describes a new learning algorithm for training Spiking Neural Networks which supports multiple layers in an analogous format as backpropagation for training deep neural networks. A unique contribution of this work is the ability to perform temporal credit assignment over past spikes from loss incurred at the current timepoint. This contrasts with previous work which focuses on credit assignment at the time point in which the loss is incurred and ignores the impact of earlier spikes.  This paper is very well written. Explanation of their current mathematical definition of a spiking neuron to how they stack and finally how to use the SLAYER algorithm to train those multiple layers flows very nicely.   There are a few clarifications that I think would improve the clarity of how the error signals are backpropagated through the network: Equations 9-11 would have better clarity for me if it started from the partial derivative terms necessary for computing the update and then describing the chain rule process for each layer (Equations 10 and 11).  While it is understandable that the current work on spiking neural networks does not currently compare favorably to artificial neural networks, such comparisons would be scientifically useful to understand the fundamental gaps in performance that still exists between these models. For example, the performance of the ANN counterpart for the convolutional architectures described on the datasets tested would give more information about how well such an architecture could potentially perform.  The spiking algorithms they do compare against use different architectures. It would be important to understand how the architecture changes in this work contribute to the observed differences in performance. Furthermore, does this algorithm continue to perform well networks with more than 10 layers? Are there similar improvements as more layers are added in a similar fashion to what has been shown in the deep learning community?  Minor Issues: Line 90: The other three categories 'aggregation' layers described is more commonly referred to as 'pooling' Many of the equations (ex: Eq 6,7,8) are definitions and should be annotated as := 