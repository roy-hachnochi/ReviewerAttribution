The authors propose a method of training spiking neural network models inspired by gradient descent via backpropagation. As with similar work, a surrogate function is used in place of partial derivatives through the neuron activation (the spike function). The main contribution of this work is to extend this approach to account for the temporal dependency between spikes --- i.e. the credit for the error signal is distributed to the entire activity history of a neuron, and not just the most recent time step. This seems computationally expensive, and the model is sophisticated, so it is good that the authors are releasing code to go along with the paper; the actual implementation seems non-trivial.   The paper details a new approach for training networks that could then be used on efficient neuromorphic hardware. It is not yet clear from the experiments whether the possibility of improving performance is worth the extra computation, but the experiments do demonstrate learning in networks of hundreds of thousands of parameters. In the experiments, how are the updates (Equations 18 and 19) actually computed?  The paper is well organized, and the algorithm and experiments are very clearly presented. 