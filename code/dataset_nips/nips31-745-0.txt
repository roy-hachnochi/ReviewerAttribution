After author response: thanks for your response to my comments. I still think the paper should be accepted.  ====================  Reinforcement learning (RL) has started gaining huge popularity in recent years due to impressive empirical success (e.g., AI defeated the world champion in Go). Unfortunately, this success is still unexplainable theoretically.  Specifically, there is no mathematical explanation to RL in the case that there are many observations, as in real life RL problems.  The paper presents an algorithm that is proven to be successful in the rich (i.e., many) observations setting, under some assumptions. Their algorithm, VALOR, is based on an algorithm, LSVEE, from a preceding paper [17]. They prove, under some assumptions (like realizability, a small number of hidden states and other assumptions that will be discussed later) that VALOR is PAC-RL  (see e.g., Reinforcement Learning in Finite MDPs: PAC Analysis).  This paper is very well written.  The paper addresses the important mission of gaining a theoretical understanding of RL with rich observations and the paper's perspective is an interesting one. On the downside, there are many assumptions that the paper is making that somewhat weaken their result. Some of the assumptions are highlighted (e.g, the transitions between the hidden states are deterministic), and some are more subtle but important nonetheless and should be explained further: 1. For Theorem 3 to be meaningful, the value function class and the policy class should not be too large.  2. In line 98, it is assumed that the observations obey the Markovian assumption.  It will be interesting to see experiments proving the empirical success of this algorithm.