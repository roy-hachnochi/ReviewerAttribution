# Summary  The work proposes a novel differentiable and fast network architecture to learn high-dimensional filtering operations.  # Paper Strengths  - Important problem: high-dimensional convolutions have applications for inference in CRF, computer graphics and CNN operators, among others. Computational efficiency is a big problem in this domain. - The paper provides a solid contribution towards generically accelerating these filtering operations: it applies the generalised canonical decomposition from [Cohen and Shashua, 2016] to convolutions and suggests an architecture to mimic the behaviour of established high dimensional filtering techniques. - The paper is well written; however pretty condensed. - The presented experiments cover a large range of possible questions:  - comparison to related state-of-the-art: filtering defined on the bilateral grid or the permutohedral lattice  - comparison to baselines: a CNN that implements splat/blur/slice with canonical polyadic decompositions  - applications to inference in dense CRFs: semantic segmentation and stereo  - a computer graphics application  # Paper Weaknesses  - Some parts of the work are harder to follow and it helps to have checked [Cohen and Shashua, 2016] for background information.  # Typos and Presentation  - The citation of Kraehenbuehl and Koltun: it seems that the first and last name of the first author, i.e. Philipp, are swapped. - The paper seems to be using a different citation style than the rest of the NIPS submission. Is this intended? - line 111: it might make sense to not call g activation function, but rather a binary operator; similar to Cohen and Shashua, 2016. They do introduce the activation-pooling operator though that fulfils the required conditions. - line 111: I believe that the weight w_i is missing in the sum. - line 114: Why not mention that the operator has to be associative and commutative? - eq 6 and related equations: I believe that the operator after w_i should be the multiplication of the underlying vector space and not \cross_g: It is an operator between a scalar and a tensor, and not just between two scalars. - line 126: by the black *line* in the input  # Further Questions  - Would it make sense to include and learn AccNet as part of a larger predictor, e.g., for semantic segmentation, that make use of similar operators? - Do you plan to publish their implementation of the proposed AccNet?  # Conclusion  The work shows that the proposed method is expressive enough to approximate high-dimensional filtering operations while being fast. I think the paper makes an interesting contribution and I would like to see this work being published.