Summary:  This paper considers the problem of machine teaching when the learner has a general type of preference function over hypotheses, where the amount a learner prefers a particular hypothesis depends on its current hypothesis. Specifically, they are interested in understanding when an iterative/adaptive teaching protocol fairs better than a non-adaptive approach.  They demonstrate that when a learner only has global preferences (i.e. their preferences do not depend on their current hypothesis), there is no gain in an adaptive strategy. They also provide two example settings in which one can show adaptivity can lead to improvement over the non-adaptive setting. They also present conditions under which a certain greedy strategy is within some logarithmic factor of optimal.   Quality:  This paper appears to be technically sound, although I have not checked all of the proofs rigorously.    Clarity:  This paper is reasonably clear, but I did have difficulty following the discussions surrounding the example classes (2-Rec and Lattice). For example, the discussion in the first paragraph of Section 5.2 introduces an oracle S(), a subroutine Teacher(), and a term “sub-task” that all seem to be interrelated but don’t seem to be formally defined.   Originality + significance:  The local preference model of a learner in the machine teaching setting appears to be an interesting contribution to the field. It is well-motivated and very general.  Unfortunately, the algorithms considered here only seem to be feasible in relatively toy settings. In particular, to implement Algorithm 1, we need to find the hypothesis which maximizes \tilde{D}(h). However, this appears to require enumerating over the entire candidate set (which may require enumerating the entire version space to construct). This paper would be strengthened significantly by a reasonable example where (say approximately) maximizing \tilde{D}(h) can be done efficiently.    ————————————————————————————————————  Revision after author response:  After viewing the author response and the other reviews, I have decided to revise my score upwards. My main objection to the current work was that the proposed algorithms only work in toy settings due to the difficulty of the optimization problems. However, as pointed out by another reviewer, this paper has several important contributions, and the formalisms presented here open the door for others to work in this area.