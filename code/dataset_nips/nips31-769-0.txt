Summary :   While Generative Adversarial Networks (GANs) have become the desired choice for generative tasks in the community, they also suffer from a nagging issue of mode collapse (cf. [1, 2, 3, 4] in the paper). The current literature also has some empirical ways to handle this issue (cf. [1-3, 16, 19, 22] in the paper). The paper showcases a significant stride in presenting for the first time a theoretical consolidation and treatment of the mode collapse problem. They present the technique of packing, in which the discriminator now uses multiple samples in its task. They also validate their formulation on synthetic datasets and real datasets (Stacked MNIST and CelebA).  Detailed Comments:  Clarity : The paper is very well written, both rigor and intuitive expositions are presented.   Originality : As explained above in summary, perhaps this is the first time a framework of mode collapse is constructed and its theoretical underpinnings are discussed. While packing is in a similar vein to the already known, minibatch discrimination (as also agreed by author), their unique standing point is the easier implementation and their theoretical analysis.   Quality and Significance : The paper presents the exposition very systematically and rigorously, as they mention in the introduction, their significance is conceptual, analytical and algorithmic as well. The survey of the related work is also quite comprehensive and comparative, boosting the quality of the presentation. The theoretical analysis is tied to how the total variation distance would change on packing. The authors argue (rigorously) that with packing, the total variation distance for mode collapsed distributions is amplified and hence penalized in the GAN training - thus guaranteeing resolution of the mode collapse issue. This they corroborate with experiments.   I think this is a strong contribution to NIPS this year. Paper should be accepted. Some of the concerns mention below must be addressed, though they don't point to the paper's rejection:  (a)  Insufficient Comparison with Minibatch Discrimination : The authors do mention their advantage over the related technique of minimatch discrimination, however this is treated quite sparsely. Readers must be interested in knowing the comparison in sufficient detail especially in their implementations which is claimed to be superior for PacGAN on Page 3.   (b) JS Divergence : The authors mention (in supplementary material) that their methodology breaks for JS Divergence giving loose bounds. Nonetheless given the importance of JS Divergence (which is obtained after using cross-entropy loss, which is actually how GAN is trained), it will be worthwhile to present how these bounds look like. In short, somewhat more treatment (heuristic or rigorous) to bridge the analysis between 0-1 and cross entropy loss is desired.   (c) More Experimental Evidence : Given the importance and the significance of the mode collapse problem, it would be advisable to run the algorithm on several more real datasets to pronounce its robustness. 