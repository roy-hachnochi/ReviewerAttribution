The authors proposed a network composing of VAE and memory-augmented neural network (MANN), named Variational Memory Encoder-Decoder(VMED) to model sequential properties and inject variability at each step in sequence generation tasks. The authors use a Mixture of Gaussians (MoG) to represent the latent space, where each mode associates with some memory slots. The idea of training MoG prior, but a unimodal Gaussian posterior for each training data is nice. The authors have nice mathematical walk through. And, demonstrated better performance in BLEU-1, 4 and A-Glove scores in several different language modeling datasets. The example results in Table 2 are also quite impressive, showing diverse, high-quality sentences generated by the VMED model.    The work is a step upon the variational RNN model (VRNN) (cited as [8] in the paper). I wonder why the authors did not try to run VRNN as one of the baseline models in Table 2. As discussed around L94-98, VRNN use hidden values of RNN to model the latent distribution as a Gaussian, and would be an important baseline to compare to VMED to see how much gain does the external memory provided beyond the LSTM-based model.   While using the external memory in a VRNN setup is interesting. This paper, however, is not the first one suggesting this idea. Generative Temporal Models with Memory by Gemici et. al.,[1], is the first one I know and unfortunately, is missed from citation in this manuscript. Though, the methods in these two manuscripts are different in detail. (for example, using MoG prior in this work.) Also, the results in language modeling is quite impressive.  [1] Gemici, M., Hung, C.C., Santoro, A., Wayne, G., Mohamed, S., Rezende, D.J., Amos, D., Lillicrap, T.: Generative Temporal Models with Memory. arXiv preprint arXiv:1702.04649 (2017)