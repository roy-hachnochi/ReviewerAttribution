This paper studies the problem of handling the langauge/text pariors in the task visual question answering. The great performance achieved by many state-of-the-art VQA systems are accomplished by heavily learning a better question encoding to better capture the correlations between the questions and answers, but ignore the image information. So the problem is important to the VQA research community. In general, the paper is well-written and easy to follow. And some concerns and sugggestions can be found as the following:  1) The major concern is the basic intuition of the question-only adversary: The question encoding q_i from the question encoder is not necessarily the same bias that lead the VQA model f to ignore the visual content. Since f can be a deep neutral network, for example, deep RNN or deep RNN-CNN to leverage both the question embedding and visual embedding, thus the non-linearity in f would make the question embedding as a image-aware represention to generate the answer distribution. Thus if the f_Q and f is different as stated in the paper (since the VQA models f are SAN and UpDn, the f_Q is a 2-layer dense network), then the question embedding equals to q_i (since it is linear transformation throught through the 2-layer dense network) to generate the answer distribution for the question-only adversary. Then the question embedding for question-only mdoel and the VQA model are different, which make the question-only model in the adversarial scheme doesn't hold.  2) The mutual information reguarlizer seems needs to be re-designed. The current optimization direction will let question encoding contains more information about the image. However this intuition doesn't mean the question encoding contain letter language priors. Adding anther mutual information regularizier to minimize the H(A|q_i) could help.  3) In the experiment, the question-only model can be better by introducing non-linearity to make the quesiton encoding behave similarity compared to the VQA models.  4) How to train the question-only model? It is trained based on equation (4) or (9)?  5) How to tune the lambda_q and lambda_mi? As the numbers in Table1 seems quite effective, but not very intuitive.  6) In table 1, why the gain of the VQA+MI performs the best on the VQA v2 dataset amony the versions with regularizers? Since by adding the mutual information regualrizer, the answers could better leverage the image information, it could generate slightly better result compared to the base models.