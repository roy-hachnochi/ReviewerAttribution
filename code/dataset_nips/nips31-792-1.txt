This paper proposes bilinear attention networks which extends co-attention network to consider every pair of multimodal channels. The authors propose bilear attention networks to enable bilinear attention distribution atop low-rank bilinear pooling. Besides, the paper also introduces a variant of multimodal residual network to efficiently integrate the joint representations from the multiple bilinear attention maps. By doing this, in the experiments, the bilinear attention network can allow up to 8-head attention, and shows the effectiveness of the network on both Visual QA task and visual grounding by comparing with some state-of-the-art approaches. The paper seems cite a good amount of recent significant publications on attention networks and bilinear pooling. The paper is in general well-written and easy to follow. Please find some of the minor suggestions or issues as below:  1) In the experiments, what the impact of adding the nonlinearity is? It would be good if incorporating the discussion or result in the paper.  2) Instead of using learning rate decay, two alternatives would be either using NT-SGD : Polyak, B. and Juditsky, A. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838â€“855, 1992. or using a learning scheduler (similarity based etc.).  3) In Table 2, it would be good if using nparams=1280 to be the same as for co-attention for BAN to show the improvements.  4) It will be also helpful to show the running time performance compared with co-attention.  5) typo: in the BAN section, the j-th channel (*column*) of input Y.