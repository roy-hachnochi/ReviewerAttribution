This paper considers problem of learning pairwise measurements when the underlying distribution follows a natural exponential family distribution. Specifically, the paper creates virtual examples and optimize the contrastive loss over them.  From Eq. (3.4), my understanding is: if y_p and y_q are closer, the corresponding loss should be smaller. If we have a multi-class problem where y_p > y_k > y_q, hence y_p – y_q > y_k – y_q; for this scenario, is the loss function Eq. (3.4) still reasonable?  The paper focuses on optimization problems with simple parameter space C(\alpha) in Eq. (3.3) as a constraint. Will the derivation still hold if complex constraints are involved?   On Line 44, the paper considers to use nuclear norm as a regularization to ensure low-rank structure. Is this a necessary part for the derivation? 