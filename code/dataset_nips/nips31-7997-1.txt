The paper suggests a general framework for implementing sparse PCA based on sparse regression. The focus of the paper is theoretical, with the main emphasis on provable guarantees for the spiked covariance model. The paper shows that using black-box lasso solver, their method performs better than "covariance thresholding" (NIPS 2014). The paper is clearly written and appears to be sound. I vote for accepting.  Caveat: I did not study the mathematical details of the proofs.  MAJOR ISSUES  1. There appears to be some disconnect in the sparse PCA literature between "more applied papers" focusing on possible applications (e.g. Zou et al. 2006) and "more theoretical papers" focusing on theoretical guarantees using the spiked covariance model (e.g. diagonal thresholding, covariance thresholding etc.). Personally, I am not very familiar with the latter line of work. Despite a decent literature overview in Section 1.1, I am left a bit wondering about the relationship.  (i) Given that Zou et al. is by far the most cited and well-known paper on sparse PCA (almost 2k citations on Google Scholar), would it make sense to add it to the comparison in Figure 1? I have literally no idea how it will perform in comparison to the methods analyzed here. I do understand that Zou et al. did not provide any theoretical guarantees. But one can surely still assess how their method works in this particular experiment?  (ii) In a purely applied setting, when working with p>>n dataset (e.g. gene expression data) that is clearly very far away from spiked covariance model, what would the authors recommend? Using their method? Or using something like Zou et al.? Why? I would like to see some brief discussion of these questions. And what about p<n setting, like the pitprops dataset discussed in Zou et al. and elsewhere?  MINOR ISSUES  1. The authors spend a lot of space motivating the Q_i measure that they use (lines 240-255), but isn't it just R^2 (up to the scaling by the total variance)? Total variation minus unexplained variation is the standard formula for explained variation. Maybe at least mention R^2 here. Lines 241-246 were to me more confusing than helpful.  2. Algorithm 2 will not necessarily yield $k$ features, correct? Even though it has $k$ as the input? I'd like to see some comment on this.  3. Constant 13 appearing in Algorithms 1/2 is very mysterious. I have initially suspected that it's a typo, before I looked in the supplementaries. Maybe the authors can comment on this constant in the main text?  4. When using lasso as SLR(y,X,k), how exactly is it supposed to be set up? For lasso, one usually needs to choose lambda parameter, not k. Is the idea to do a binary search on lambda for desired k? Line 298 mentions "thresholded lasso with lambda=0.1". What does "thresholded lasso" mean here? If I understand correctly what it means, then it is *not* lasso. Does this "thresholded lasso" have any provable guarantees? Please discuss in more detail. Also, why lambda=0.1??  5. The formatting of line 2 in Algorithms 1 and 2 is a bit different ("output").  6. Some abbreviations are not deabbreviated the first time they appear. Line 68, line 218  7. Typos: line 62  8. Broken citations: footnote 3  9. Capitalization in the titles in references is broken ("pca", "gaussian", etc.)