The paper proposes a method termed TransPlan, to use Graph Convolutional Networks to learn the relations defined by an RDDL description to learn neural network policies that can "transfer" to different MDP planning domain instances. The architecture combines several components including a state encoder, an action decoder, a transition transfer module and a problem instance classifier. Only the action decoder requires retraining for transfer and the paper shows how a different component in the architecture (transition transfer module) can be used to quickly retrain and get substantial gains in transfer to a new domain without any "real" interactions (zero-shot). Authors evaluate their performance on benchmark domains from IPPC 2014 and show substantial improvements over standard algorithms which do not leverage the structure offered by an RDDL description of the problem. The authors also a do a few ablations studies to find the relative importance of different components in their system.   # Strengths  The paper proposes a method termed TransPlan, to use Graph Convolutional Networks to learn the relations defined by an RDDL description to learn neural network policies that can "transfer" to different MDP planning domain instances. The architecture combines several components including a state encoder, an action decoder, a transition transfer module and a problem instance classifier. Only the action decoder requires retraining for transfer and the paper shows how a different component in the architecture (transition transfer module) can be used to quickly retrain and get substantial gains in transfer to a new domain without any "real" interactions (zero-shot). Authors evaluate their performance on benchmark domains from IPPC 2014 and show substantial improvements over standard algorithms which do not leverage the structure offered by an RDDL description of the problem. The authors also a do a few ablations studies to find the relative importance of different components in their system.   # Weakness: Which brings us to various unclear parts in the paper. First of all, there are key claims that are hard to justify. For instance: "a key strength of neural models is their effectiveness at efficient transfer". I am sure you'll find a lot of disagreement here especially when you are not working in the vision community where ImageNet models transfer fairly well. This is not the case with models trained with RL (DQN or policy gradients) because the gradients are a lot noisier and the representations learning is more difficult. Another example being, "A3C  algorithm [â€¦], because it is simple, robust and stable" is hard to swallow given the term "asynchronous" in the algorithm. The paper's treatment of "transfer" is quite unclear too. Transfer learning has a long history and would refer to multiple surveys on transfer in RL [1,2] to better place their objective. Moreover, we can expect that the NIPS audience wouldn't know as much about symbolic AI and RDDL description, so use of terms like fluents without defining them first, leaves things unclear to the reader. Similarly, even thought the components of the architecture are clearly explained individually, their exact combination and how exactly the losses are setup is quite unclear. I hope the authors are atleast planning on releasing their code for easier replication.  There are quite a few components in the proposed method. Whether they are warranted can only be checked by experimental verification. The paper is quite unclear about the exact nature of these problem domains - what's the observation space like, what are the possible number of problems that are generated in a domain, etc. (One can look it up on IPPC but making the paper clear would be better). Moreover, since these are planning problems it's hard to say if DeepRL algorithms like A3C are right baselines to benchmark against. The paper _is_ using the model after all (transition). Wouldn't it be useful to at least show the standard methods used in IPPC and their efficiency. Do we even gain on anything at all by showing transfer abilities if the total time taken by standard planning algorithms for each problem domain is less than learning via DeepRL? Moreover it's unclear what were the parameters of A3C algorithms itself - number of workers/batch size etc. It doesn't look like Figure 2 and Figure 3 show averaged runs over multiple seeds (maybe fill color to show standard error?) nor is there any standard deviation for Table 2 results. So although the inclusion of "SAD" seems pretty big as an improvement, I can't make a strong claim given how much variance there can be with DeepRL experiments.   # Minor things:   - Line 85: Missing citation?  - Line 329: Not sure if Table 2 suggests that. Possibly meant to be Figure 3?  [1]: http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf [2]: https://hal.inria.fr/hal-00772626/document 