The authors propose a method for considering fairness across groups defined by protected attributes when doing PCA.    I like how the authors don't stop at the obvious objective with an equal number of dimensions per group, but think deeper about the problem and come to the formulation they come to.  I also think the contextualization to the recent literature, including allocative/representational is good.  The math, which results in a semidefinite program appears to be correct.    In the related work section, it would be nice to expand beyond Zemel et al. (2013) to newer pieces of work in the same vein: H. Edwards and A. Storkey, “Censoring representations with an adversary,” in Proceedings of the International Conference on Learning Representations, San Juan, Puerto Rico, May 2016.  A. Beutel,  J. Chen,  Z. Zhao,  and E. H. Chi,  “Data decisions and theoretical implications when adversarially learning fair representations,” in Proceedings of the Workshop on Fairness, Accountability, and Transparency in Machine Learning, Halifax, Canada, Aug. 2017.  F. P. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney, “Optimized pre-processing for discrimination prevention,” in Advances in Neural Information Processing Systems 30, Long Beach, USA, Dec. 2017, pp. 3992–4001.  B. H. Zhang, B. Lemoine, and M. Mitchell, “Mitigating unwanted biases with adversarial learning,” in Proceedings of the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, New Orleans, USA, Feb. 2018.  D. Madras, E. Creager, T. Pitassi, and R. Zemel, “Learning adversarially fair and transferable representations,” arXiv:1802.06309, Feb. 2018.   Remark 4.1 can be accompanied by reference to Z. C. Lipton, A. Chouldechova, and J. McAuley, “Does mitigating ML’s impact disparity require treatment disparity?” arXiv:1711.07076, Feb. 2018.  --- I would have liked the "practical side" future work done in this paper itself.  --- Has many typing, typesetting, etc. problems that should be fixed.