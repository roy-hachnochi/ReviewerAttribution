The paper formulates and proves a policy gradient theorem in the off-policy setting. The derivation is based on emphatic weighting of the states. Based on the introduced theorem, an actor-critic algorithm, termed ACE, is further proposed. The algorithm requires computing policy gradient updates that depend on the emphatic weights. Computing low-variance estimates of the weights is non-trivial, and the authors introduce a relaxed version of the weights that interpolate between the off-policy actor-critic (Degris et al., 2012) and the unbiased (but high variance) estimator; the introduced estimator can be computed incrementally.  The paper is well written and makes an interesting contribution. The authors might further improve the paper by emphasizing about what are the specific novel contributions of their paper (both on theory and algorithmic side), as it is unclear in some parts whether a certain proposal is novel or has been adapted from related/previous work and combined with other concepts in a novel way. (E.g., Thm. 1 seems straightforward so long as the notion of emphatic weights is introduced; would be helpful if the authors can clarify whether/why it hasn't been derived before or whether it was studied in a different context.)  Additional comments and questions: - Even thought it is stated at the beginning of section 2, I would recommend making it clear in the statements of the theorems that the state and action spaces are assumed to be finite. Eq. 7 implicitly makes use of that. In case of continuous state space, one would have to solve the Fredholm integral equation of second kind instead, which I believe is not as straightforward. - Eq. 11 proposes a very specific adjustment to the spectrum of the (I - P_{\pi, \gamma}). The authors analyze empirically the learning behavior of ACE with different values of lambda_a. I am curious whether the spectrum of (I - P_{\pi, \gamma}), which should be based on the properties of the MDP and the policy, has any effect on the choice of the hyperparameter? - (On-policy) policy gradients are widely used beyond discrete and finite-state MDPs. It is unclear whether ACE is as widely applicable. If not what are the key challenges that need to be addressed? I would like the authors to comment on that.