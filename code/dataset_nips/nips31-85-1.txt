This paper derives the exact gradient for off-policy policy gradient with emphatic weights, and use an iterative algorithm to provide an unbiased estimator of the weight. The new algorithm corrects the gradient used in the previous OffPAC algorithm and showed significant improvement in the experiments.  I think the paper is interesting and well written, and the algorithm is novel based on my knowledge.  Some comments:  1. I have questions about the objective function (3). Why use this objective for off-policy policy gradient? Based on the equation, this objective seems to take behavior policy for time t->\infty, and then take model policy for the rest. I cannot get the intuition why this objective is useful and how it related to the online policy gradient objective.   2. How to prove that the limiting distribution d_\mu(s) exists?   3. What is \rho and \delta_t in Line 131?  4. The intuition seems that larger \lambda_a increases the variance of the iterative estimate of emphatic weights. It would be great to derive the relation between the variance and lambda_a. It would also be good to show that in the experiments.   5. In proposition 1, what is the expectation over in the LHS of equation. 