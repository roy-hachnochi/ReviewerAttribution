This work proposes a Gaussian process on graphs. Given node features, the idea is to first sample a Gaussian process on graph nodes, and then average the outputs of 1-hop neighborhood as the likelihood parameters for each node, to sample the observations. The likelihood parameters can be treated as another Gaussian process, with graph Laplacian incorporated.  The paper gives some interesting views of the modeling of the likelihood parameters --- another GP or Bayesian nonlinear model specific feature mappings. The experiments on active learning on graphs and prediction on graphs shows prediction performance comparable to or better than graph convolutional CNN.   This is a piece of interesting work that incorporates the graph structure in Gaussian process modeling. The application is interesting as well. The active learning case resembles a little bit Bayesian optimization process and I am not surprised it works well.   The paper claims that is a semi-supervised learning work --- however, I am not very convinced. So where does the information of the test data come from? Is it from the graph structure? I hope the authors should explain this more clearly.  It seems the experiments do not compare with standard GP, why? The paper lacks a very important baseline --- doing so we can see how the graph structure takes effect.  Why do you just use 1-hop average? Is it for simple or for connecting the graph Laplacian? Why donâ€™t use weighted average? Have you considered 2-hop average or even 3-hop average? What will be the benefit or trade-off? I think it will be much better than the paper has discussed or compared with these choices.  