This paper proposed a new framework to jointly learn "word" embedding and "topical" mixture of documents. The proposed approach is based on Wasserstein topic model build on the word-embedding space. The proposed approach was applied to several tasks related to medical records data.  The paper is overall solid and well organized.   I have a few concerns regarding the experiment validation - only very simple baselines (word2vec like embeddings, or LDA) are demonstrated. As author pointed out in related work, there have been recently seleval attempts in applying NLP tools for very similar dataset/tasks. There are also other attempts to combine LDA and Word2vec in the same framework. Considering a few based lines in each of these aspects would be much more convincing.   - since are generative model were proposed, some analysis/metrics like likelihood and per-plexity would be good to demonstrate how the proposed model fits the real-world observations  - it would be good to consider a few different datasets in this domain to demonstrate the descriptive power of the proposed model. (e.g., in Citation[11])