The main idea: this paper proposes a novel Wasserstein method based on the underlying word (code) embedding. The Wasserstein method can measure the distance between topics. The topic word distributions, their optimal transport to the word distributions of documents, and word embeddings are jointly learnt. The paper also proposes a distilled ground-distance matrix when updating the topic distributions and smoothly calculate the optimal transports. The method achieves strong performance on the task of mortality predictionï¼Œadmission-type prediction, and procedure recommendation.   Compared with prior work, this paper proposes a unified framework that could jointly learn topics and word embeddings. Also, this work updates the pretrained embeddings rather than freeze them, and have a hierarchical architecture for topic modeling. No prior work has combined Wasserstein learning with knowledge distillation.   Strength: this paper is fairly well written and the idea is very clear. The method is novel in that it proposes a Wasserstein topic model based on word embedding. It jointly learns topic distribution and word embeddings. And it incorporates knowledge distillation in the update of topic models. The experimental results show improvement over baseline models.   Weakness: It'd be great if the author could be more explicit on the motivation of using Wasserstein method to model distance between admissions. It's not that straightforward from the current writing. The author only describes the limitation of previous methods.    The author mentions that their method outperforms state-of-art methods. It's unclear to me which method in table 1 is the state-of-art. Word2vec/Glove and AvePooling and Doc2Vec seems a bit weak baseline.   Related to the above question, have you tried other classifier other than KNN? For example, it could a simple neural network that fine-tunes the pre-trained word2vec/Glove. This could be a better baseline than using fixed pre-trained embedding in KNN.    