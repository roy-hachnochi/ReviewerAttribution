The authors present several modified GAN architectures and losses for tackling response generation in a single-turn conversational setting. In particular, the authors propose a particular architecture and loss for the discriminator, an additional mutual information component to the loss (using a variational approximation to MI and an approximate backward model), and finally a "dual" objective, which aims to treat the forward and backward models symmetrically. The authors apply their approach to two social-media style datasets, and evaluate in terms of BLEU, ROUGE, some embedding-based metrics from previous work, some diversity-targeting metrics, as well as with a human user study. Their approaches improve over baseline seq2seq, conditional GAN, and the MMI model of Li et al. (2016) in essentially all metrics.  Strengths: -  The authors carry out a large number of experiments with a large number of evaluation metrics (including human evaluation), and their approach generally outperforms the baselines.  - The experimental comparison between cGAN, AIM, and DAIM effectively ablates at least some of the novel architectural and loss components introduced by the authors. - The authors suggest several modifications to GAN-style generation models that appear to be helpful.  Weaknesses: - It would have been much better if the authors had compared their approaches with previously published results on established datasets. For instance the datasets of Li et al. (2016) could presumably have used. Even if, as in footnote 1, there are concerns about some of the other datasets, some results on these datasets could have at least been included in an appendix in order to better establish how the proposed approaches measure against previously published results. - While some of the novel architectural and loss components are effectively ablated (as above), the authors claim additional benefits of their approach that are not backed up theoretically or empirically, such as that the cosine-similarity based discriminator is superior to other discriminator architectures, and that learning with DPG is preferable to other approaches.  Finally, there are some relatively small presentation-related issues: - Line 81: if Z is added to H0 element-wise this isn't a Hadamard product (which would multiply element-wise) - Line 83: it's a bit strange to claim you aren't using a standard LSTM unit just because you take the (soft) argmax of the output distribution rather than sample; as far as I can tell the LSTM architecture hasn't changed at all. -  The claims throughout of proposing a "learning framework" seem overblown; this isn't a new framework, but a modified loss and architecture.  