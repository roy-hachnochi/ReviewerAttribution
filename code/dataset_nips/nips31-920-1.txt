This paper add to an ongoing discussion about the information bottleneck theory of deep learning. The main contributions of this submission is a theory of the entropy and the mutual information in deep networks that is derived leveraging the replica trick in statistical physics. These results are corroborated by careful experiments. Finally, an application to investigate the contentious compressive phase of learning is performed. Overall the exposition is exceptionally clear and whatever ones opinion about the information bottleneck theory, tools to gain theoretical insight into the mutual information seem like they ought to be extremely valuable. Overall, it is also great to see an application of the replica trick in the context of deep learning.  As an aside, there is a lot going on in this paper and I think there really isn't sufficient time to do a proper job of going through the details of the theory. I therefore list my overall confidence as low. I look forward to taking some time to do so in the future.