Contributions of the paper:  The authors consider a stylized statistical model for data that respects neural network architecture, i.e. a Markov structure of the type T_\ell = \varphi(W_\ell*T_{\ell-1}, \xi_\ell ) where T_0 = X is the input, T_L = y is the output label, $W_\ell$ are random, independent weight matrices, \varphi is a nonlinearity applied elementwise on its first argument, possibly using  external randomness \xi_\ell. For data generated from this specific model, they make the following contributions.   1. They show that under this stylized model, one can obtain a simple formula for the (normalized i.e. per unit) entropy  H(T_\ell)/n and mutual information I(T_\ell; X)/n between the input data and each successive layer, in the high-dimensional limit. This formula is, in general, derived using the non-rigorous  replica method from statistical physics.   2. For a two-layer network and a class of nonlinearities, they rigorously establish this formula, using a technique of `adaptive' interpolation.   3. Based on this model, they provide some experimental results for two synthetic scenarios: the first based on the teacher-student model,  and the second for generative models, such as variational autoencoders. The experimental results are multi-faceted and include a comparison with entropy/mutual information estimators, validation of the replica formula, and some applications to the recent information bottleneck proposal of Tishby et al.    Summary:  The paper is a solid contribution, and I would argue that it is a clear accept. It would make a good addition to the NIPS program  and is of potential interest to communities possibly unfamiliar with the approaches in the paper.   Comments for the authors:  There are multiple aims that you have in the experiments section: which point to interesting applications for your results, like comparing entropy estimators, validating the bottleneck theory, potential training innovations. Further, some of your experiments aim to validate the replica formula where it is not known to hold,  e.g. during training (thanks to dependence). It would be good to add an experiments summary that clarifies these aims and points the readers to the relevant sections. You might have to move some of the  discussion due to space constraint.    I have read the author response. My scores are unchanged, I look forward to the final version of the article. 