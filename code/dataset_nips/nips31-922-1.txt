The paper presents a novel method of exactly recovering a vector of coefficients in high-dimensional linear regression, with high probability as the dimension goes to infinity. The method assumes that the correct coefficients come from a finite discrete set of bounded rational values, but it does not - as is commonplace - assume that the coefficient vector is sparse. To achieve this, the authors extend a classical algorithm for lattice basis reduction. Crucially, this approach does not require the sample size to grow with the dimension, thus in certain cases the algorithm is able to recover the exact coefficient vector from just a single sample (with the dimension sufficiently large).  A novel connection between high-dimensional linear regression and lattice basis reduction is the main strength of the paper. The ideas and algorithms in the paper are presented clearly. The main limitation of the algorithm is the fact that all of the guarantees hold „with high probability as dimension p goes to infinity”. No finite dimension guarantees are offered, which raises the question of whether the proposed algorithm only achieves the desired behavior for combinatorially large dimensions. That would make the „polynomial runtime” guarantee somewhat misleading (since the runtime depends on p). It is worth noting that despite the authors’ claim that taking dimension p to infinity is „very popular in the literature”, after looking at some of the cited works I noticed that typically at least some finite dimension guarantees are offered in these works. Thus, to address this, in addtion to „w.h.p. when p tends to infinity”, the authors could - if at all possible - state the guarantees as „w.p. at least 1-delta for p larger than…”, for example.   Minor comments: - line 76: „so that to” -> „so as to” - line 212: „can be applied,.” -> „can be applied.”