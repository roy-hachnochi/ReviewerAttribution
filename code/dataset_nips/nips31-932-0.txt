This paper proposes to teach image captioning models new visual concepts with partial supervision, i.e. image level labels. Based on the image labels, a complete caption is first estimated using constrained decoding. Then the image caption model is trained using both supervised data and the estimated image-sentence data. The proposed method achieves the state-of-the-art result on novel object captioning task.  Strengths: 1. It is novel to apply constrained beam search [19] in the training process. 2. The experimental results are good.  Weaknesses: 1. The writing is not clear. The descriptions of the techniqual part can not be easily followed by the reviewer, which make it very hard to reimplement the techniques.  2. An incomplete sequence is represented by a finite state automaton. In this paper, only a two out of three finite state automation is used. Is it possible/computational feasible to use more complicated finite state automaton to select more image labels? As there are 12 image labels per image on average, only selecting two labels seems insufficient.  3. The authors describe an online version of the algorithm because it is impractical to train multiple iterations/epochs with large models and datasets. Is it true that the proposed method requires much more computation than other methods? Please compare the computational complexity with other methods.  4. How many estimated complete captions could be obtained for one image? Is it possible to generate over C(12, 3) * C(3, 2) = 660 different captions for one image? 