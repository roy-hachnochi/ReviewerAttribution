The authors tackle the problem of image captioning in the setting where captions may not be available for `novel' categories. This setting is well motivated by the authors and is a practical one - it is easier to scale image-level annotations than it is to get image captions. They present an approach that uses some paired image captions and visual labels of novel categories to caption novel categories. The visual labels of novel categories are combined using an automata to only accept captions that contain some mention of the new classes. The method is evaluated on the novel class split proposed by Hendricks et al. The authors show numerical improvement over baselines.  Strengths - The paper is clearly written and it was fun to read. I like the way the authors have motivated the problem, their setup and solution. The use of an automata is a nice idea. - The authors have done a good job of evaluating against recent state-of-the-art baselines, including papers published at CVPR 2018 (NBT). - The qualitative examples in Figure 2 and 4 are useful to see what this method can achieve and where it fails. - Table 1 does a good job of highlighting the upper bound of this method, and showing how close one can get to it using the partial supervision approach.  Weaknesses - L198-205: It is not clear how exactly the FSA is constructed. Do you select 3 labels out of all the visual labels (novel ones) for the image? Is this selection ever changed? How are synonyms determined? How is the FSA constructed to handle synonyms - do you add synonyms to all the disjunctions and create separate sets? The paper spends little time explaining details of it's major idea/contribution. This makes reproducibility hard, although the authors do say they will release code (L83). - A minor observation from Table 1: the in-domain score is highest when not using any out-of-domain data (row 1). Does this indicate that model capacity is limited because using out-of-domain data reduces in-domain (especially for CIDEr) performance. - A missing comparison from Table 2 is using LRCN+CBS with a ResNet-101 CNN. Moving from VGG16 to ResNet101 gives significant improvement for the NBT baseline, and I suspect it should do the same for LRCN. A comprehensive comparison would benefit the authors and future research.  After rebuttal ------------------ I do not agree with the authors' assertion that ResNet-50 results for LRCN "should be similar to ResNet-101". I strongly suggest running the experiment instead of suggesting similarity. I will keep my rating unchanged.