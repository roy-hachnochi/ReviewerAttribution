This paper examines the Rademacher complexity of L_{p,q} normalized ReLU neural networks. Results are given that imply that generalization and approximation error can be controlled by normalization. These results are interesting and represent a meaningful contribution to the understanding of ReLU networks.  Some improvements do need to be made to the writing. In particular, the results are simply listed in the main text with all accompanying motivation or intuition deferred to the proofs in the supplement. Since the paper appears to have too much white space between lines, there should be plenty of room to provide reasonably detailed intuition and/or proof sketches for the main results in the main text. This addition would greatly improve the readability of the paper for a larger audience.  Edit: The rebuttal addressed all issues I had, and the new proof they indicate will be in the final version is interesting. I therefore vote for accept.