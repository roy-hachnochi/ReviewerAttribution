Edit: I increased the score because the authors answered my question about the derivation of sample complexity of M and the experimental verification and I'm satisfied with it.  Summary  The paper proposed a novel discrepancies between probabilistic distributions called \Phi Stein discrepancies (\PhiSD) and random \Phi Stein discrepancies (R\PhiSD) for the fast computation of Stein discrepancies.  Existing kernel Stein discrepancies (KSD) requires O(N^2) computation that is time-consuming when N becomes large.  To reduce the computation, the paper first proposed \PhiSD that is the generalization of KSD to Lr norm. And then replace the computation of Lr norm with importance sampling to construct R\PhiSD. They first prove that the proposed \PhiSD can upper bound the KSD for some kernel and then introduce \PhiSD for the kernel with enough non-convergence detecting power. Then they prove that R\PhiSD with M=\Omega(N^(\gammar/2))<=\Omega(N) samples can upper bound \PhiSD with high probability. Thus, we can use R\PhiSD to test the goodness of fit. Experimental results using synthesized data demonstrate that R\PhiSD with IMQ kernel shows good empirical performance.   Qualitative Assessment  The idea that generalization of KSD to Lr norm combined with importance sampling seems interesting. The proposed L1 IMQ is fast to compute and show good performance for Gaussian distribution.  However, there seems some types and lack of explanations. Also, I have some question about main results.  As for Proposition 1, the proof should be more detailed because this is one of the main results. I think the proof use the fact that F and QT are commutative. It is more understandable to write so. In the proof, do J mean D? Also, it should be noted that \rho \in Lt.  In Proposition 3.4, I suspect that the righthand should be (1-\epsilon)^{2/r}\PhiSD^2 because R\PhiSD^2 is sum of 2/r power of w_d. Also, how is M=\Omega(N^{\gammar/2} derived? I think the author assumed that E[Y_d]=\Omega(N^-1), but I think this is not proved in the paper.  As for the experiment, the proposed L1 IMQ requires much importance sample than L2 SechExp for Gaussian mixture in Fig 2. Also, the differences of discrepancy measures are much smaller than those of other methods. Though L1 IMQ show good performance for Gaussian dist in Figure 3, it is questionable if L1 IMQ is effective for complex distributions. I suggest to match the experimental setting to the previous works such as the paper of Kernel Stein Discrepancy.  The paper will be more readable if ‘Proposition’ and ‘Assumption’ is written in bold. Also, the definition of KSD should be written in main section rather than in special cases.  Overall, the idea seems novel and promising, but the theoretical and experimental explanations are not convincing enough.