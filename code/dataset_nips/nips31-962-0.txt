Update: I keep my initial rating. As a potential improvement for the paper, I see the authors only tackle the non strictly convex case. I am curious how the result would be modified if the authors assumed strong convexity.  Original review: In this paper the authors introduce SVRG OL, a distributed stochastic optimization method for convex optimization. Inspired by SVRG, the authors first compute a high precision estimate of a gradient at an anchor point v using a large number of samples. This part of the computation can be trivially distributed as a map reduce job. Then, the authors perform a relatively small number of serial adaptative SGD updates (i.e. adagrad) on a single machine but with modified gradients similarly to what is done in SVRG but with the high precision estimate of the full gradient at point v. They then use the average of the iterates of the adaptative SGD as a new starting point and anchor point v. For a given budget N, roughly sqrt(N) iterations of adaptative SGD are performed. The computation of the gradient is requires close to N computation of gradients which are dispatched on m machines. As long as m < sqrt(N), the SGD part is not limitant thus allowing for a linear speedup when increasing m.  I am wondering what is the critical contribution of this paper. In particular the number of samples used to compute the full gradient estimate seems to be of the order N, the dataset size, so there seem to be a limited gain on that side. The other aspect is the adaptative SGD inner iterations. But again I did not get a feeling of how this method would compare to simply using SVRG with a distributed computation of the full gradient and then serial updates. My understanding is that SVRG OL is some interpolation between SGD and SVRG, carefully done to obtain a final convergence rate of 1/sqrt(N).  I also have some doubt about practical applications. My own experience with SVRG was that it was not significantly outperforming SGD for complex task and it seems that this algorithm uses a very few number of samples for the SGD iterations. My intuition is that typically in deep learning, small batch sizes works better and in order to learn efficiently from a training example, one should ideally compute its gradient individually. In the present algorithm, the vast majority of the data points will only be used to reduce the variance of other data points gradients but will never themselves drive an update. I'm interested in any comment from the authors on this surprising affect.  The experimental part is using large datasets, which is a good thing for distributed optimization. There seem to be a gap between the theory and practice. The results looks promising but the presentation as a table is not so easy to read, it seems that the different algorithms are run for different number of epochs making comparison impossible in the table. For instance if algorithm A as run for twice less time and has a worst accuracy then model B that has run for twice as long as A, then no conclusion can be drawn. In my opinion a table is only useful if the same budget is given for all methods for instance: - accuracy after same run time, - accuracy after same number of training examples, - time to reach fixed accuracy etc. I would have enjoyed a plot of the time to reach a given accuracy, plotting this for a varying number of workers is also a very good way to verify the linear speed up.  Overall I found the paper interesting. I did not manage to get a good understanding of why this approach was working while the number of SGD updates seems surprisingly small, I tried to understand the proofs techniques for that but did not have enough time to really dive into them. I am in favor of acceptance  A few remarks: - in the proof of Lemma 1, Prob(||...|| >= epsilon for all k), shouldn't it be Prob(there exist k such as ||...|| > epsilon), as there exist would be the union while for all is the intersection of the probabilistic events? - in proof of Lemma 1, looking around it did not seem that a vector space version of Hoeffding was so easy to find. If the authors would have some reference for it, it would be nice.