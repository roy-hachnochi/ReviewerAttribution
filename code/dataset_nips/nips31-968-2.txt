The paper discusses a new approach to evaluating range based anomaly detection algorithms. It provides unable recall and precision evaluation metrics.  1. quality  Overall the paper is well written and a good discussions on precision and recall philosophy and why the need to update the evaluation method for range based anomalies.   The range based recall discussion is good. Balancing between existence and overlap is welcome but also introduces a number of tunable parameters that raise some questions on how best these would be setup for different use cases. This increases the complexity of comparisons between methods.    - How will learning algorithms be tuned given the different evaluation tuning parameters. Other methods aren't optimising for this (including LSTM AD)? - What happens when during evaluation when the anomaly distribution changes, necessitating different tuning parameters for evaluation?  Would have been great to also have ARIMA, or simpler time series anomaly detection model to start hinting on how such models would be augmented to be able to best fit the metrics.  Small typo, page 6 line 263. Should be single-point  2. clarity  Overall the writing is clear. I however, even if it is for space reasons, feels that both figure 4 and figure 5 are not readable on printed paper. I had to use the pdf and zoom on. There should be a limit on how small the font should be. This is a challenge because you refer to the figures for the comparisons in the experimental section.   The development of recall and precision ranged based metrics are well explained.  3. originality   This is a good tun-able evaluation method that takes into account the challenges of limiting the scope of what precision and recall are in the case or ranged based anomalies. The work shows why ranged based anomaly evaluation is important and how the typical recall and precision measures are a special case of the ranged based evaluation methods developed. Further benchmarking and comparisons against other anomaly benchmarks are shown and a discussion on limitations both of this evaluation model and the other benchmarks discussed.  4. and significance  The paper does address an important challenge in the anomaly detection domain. Dealing with non-point based anomalies is important. Think of doing conditioning monitoring of different processes. It is not sufficient in all cases to identify that there was an anomaly, but being able to find how long it lasted for and exactly which period will assist heavily with troubleshooting. This models provides an approach that can be used to capture such requirements while allowing flexibility to reduce to a classical recall or precision evaluation.  The experiments cover a number of properties that are important and highlight properties of both the developed evaluation measures and the benchmarks metrics.   I appreciate the discussion on the computational cost of finding the anomalies and how there could be some optimisations.  - How will this impact the performance in a streaming anomaly detection setting if one has to use the middle and flat anomaly detection? 