This paper proposes a new solution for visual tracking in the tracking-by-detection framework. The key idea is to use the derivative of the prediction scoring functions with respect to the image input as an attention map and regularize its mean and variance together with the classification loss. Traditional tracking-by-detection methods use separate attention module to generate feature weights, which may not well generalize. The proposed method could generalize well since the attention is naturally integrated into the detection model.  Strong points:  (1) The paper is well-written. (2) The idea of exploiting derivative w.r.t. the input image is interesting and seems significant. (3) The authors provide many experimental results and also video demo.  However, I still have some major questions about it. The first one is that the reasoning in Section 3.1 is not rigorous. The first-order Taylor expansion of f_c(I) is A_c^T (I-I_0) + f_c(I_0), which means when you take derivative at I_0, and you want to evaluate f_c(I_0), then the first term disappears. Considering this, the reasoning after Eq.2 seems to be invalid. If we use another image I as input, and expand f_c(I) at I_0, then the computation of the derivative for all images should be w.r.t. I_0. The second question is, how would the regularizer (Eqs.3 and 4) behave if \mu_{A_p} and \mu_{A_n} are negative? For example, if \mu_{A_p} is negative, its absolute value would tend to zero if you are minimizing Eq.3. This is not what we want.  Based on the rebuttal, I feel the paper is not clearly presented. 