This paper proposes an interesting idea of using “Virtual Softmax” to improve the discriminative power of features learned in DNNs. The method works by injecting a dynamic virtual negative class into the original softmax, extending the softmax dimension from original C to C+1. The weight vector associated with the additional class is constructed to align with the current feature vector X_i, which effectively pushes the decision boundary of y_i to overlap with the class anchor weight vector.   The author also provides some theoretical intuition w.r.t why adding the extra negative class helps the network to learn more separable and compact features. They verify the approaches experimentally on several tasks, and have shown superior performance. The proposed technique is easy to use and can be practically useful. 