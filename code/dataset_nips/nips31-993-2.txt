===Update:======= I downgrade my review to 5. The main concern is  1)  Some more extensive simulations will make the results more convincing, as the numerical experiment is the only way to assess the performance of the proposed priors.   2)  I would recommend more theoretical clarifications on the usage of Bayes factor, which from my understanding will only be selection-consistent under M-closed assumption. It might take a major revision to reflect such comprehensive comparisons.   With that being said, I believe the paper does contain interesting results that are novel and useful to the community. In particular, the theoretical results seem sound, and the paper is fairly readable. But I think there is also room for improvement.  ===Original============ This paper applies Bayesian model selection in the context of boundary detection.  Non-local prior is proved to have a higher order convergence rate of Bayes factor BF(1|0) in hypothesis-testing when the null is true. As a consequence, the proposed algorithm will enjoy a quicker rate towards consistency in boundary detection. Theorem 2 mathematically proves the consistency. The experiments show advantages over all other existing methods.   The structure of the paper is clear and easy to follow. However, I am not extremely convinced why Bayesian model selection is the most valid approach in the context of change point detection. Particularly in the case of multiple change points (the non-trivial situation), the proposed method is more like Bayesian model averaging (BMA), rather than model selection in the sense that multiple models are selected and combined in the end. Why is the marginal evidence a good indicator for the joint?  (Sure, I understand it works in empirical experiments)  Furthermore, I am wondering if it would be worth comparing the proposed methods with other model selection methods other than marginal likelihood (for example, leave-one-out log predictive density, which is not sensitive to the construction of priors, though you probably have to modify LOO a little bit for time series). Or equivalently, others model averaging methods (Bayesian leave-one-out stacking, for instance, has shown advantage over BMA in M-open situation).  