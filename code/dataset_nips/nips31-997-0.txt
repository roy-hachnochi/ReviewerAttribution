The paper deals with the convergence properties of stochastic incremental hybrid gradient hard thresholding methods. It proves  sample-size-independent gradient evaluation and hard thresholding complexity bounds similar to full gradient methods.As the per-iteration hard thresholding can be as expensive as SVD in rank-constrained problems,  it is important for iterations to converge quickly while using a minimal number of hard thresholding operations. The paper gives a rigorous analysis of convergence in "big data" settings and also shows the heavy ball accelerations also yield expected benefits.