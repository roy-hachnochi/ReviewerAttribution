This work tackles the problem of sample efficiency through improved exploration. Off-policy Actor-Critic methods use the estimate of the critic to improve the actor. As they ignore the uncertainty in the estimation of the Q values, this may lead to sub-optimal behavior. This work suggests to focus exploration on actions with higher uncertainty (i.e., those which the critic is less confident about).  The authors suggest of Optimistic Actor Critic approach, which is an extension to the Soft Actor Critic and evaluate their approach across various MuJoCo domains.  This approach seems like a promising direction, the paper is well written and the motivation is clear, however I have several issues/questions:  1) Can this approach be applied to Actor-Critic schemes in discrete action spaces? The exploration scheme wouldn't end up as a shifted Gaussian, as in the continuous case, but rather a probabilistic distribution over the simplex which is bounded by distance \delta from the original policy. 2) Extension to non-convex action-value functions. This approach focuses on Gaussian policies, however, as the action-value function may be non-convex [1] considering Gaussian policies may result in convergence to local sub-optimal extremum. Can this approach be extended to more general policy distributions, such as a mixture of Gaussians (exists in the SAC source code and in a previous revision of their work)? 3) This work builds on-top of SAC, which is a maximum entropy approach. Since this is a general approach that can be applied to any off-policy Actor-Critic scheme, you should also show experiments with OAC applied to DDPG/TD3. 4) Evaluation: The results are nice, but aren't groundbreaking. The main improvement lies within the Hopper domain and the Humanoid 4-step version (in the rest it is unclear due to the variance in OAC and SAC). 5) Results for TD3 seem very low. Based on my experience (with TD3 in the v2 domains), it attains ~10k average on HalfCheetah, 4k on Ant and 2.5k on Hopper - with the official implementation (average over 10 seeds).  [1] Actor-Expert: A Framework for using Q-learning in Continuous Action Spaces  Small issues: 1) I wouldn't exactly call what TD3 do a lower bound. As they show, the basic approach results in over-estimation due to the self-bootstrap and their minimum approach simply overcomes this issue (similar to Double Q learning). This is different than an LCB which takes the estimated mean minus an uncertainty estimate. 2) line 162 - you refer to Eq. (12) which is defined in the appendix. Should refer to (5) instead. 3) At each step, computing the exploration policy requires calculating the gradient. It's OK that an approach is more computationally demanding, but it should be stated clearly.  Note to the authors: I think this work identifies a real problem and the approach looks promising. However, I believe it there is additional work to do in order for this to become a full conference paper and this paper should definitely be accepted in the future.  ------------- POST FEEDBACK ---------------  The authors response satisfied most of my "complaints". Thank you for taking the time to provide full answers and additional experiments. I have updated my score accordingly.   I urge the authors to continue working on this direction, expanding to discrete action spaces and other areas of interest which may benefit greatly from this approach and those similar to it. 