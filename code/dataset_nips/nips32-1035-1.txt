Originality: off-course, exploration and actor-critic architectures are not new in reinforcement learning, but here the authors suggest to start from an original point of view (which is the combination of two main drawbacks or RL approaches (i) pessimistic under-exploration and (ii) directionally uninformedness).  Quality: intuitions and ideas are both well developed and illustrated. The proposed algorithm is interesting as well, and definitely reaches good performances. However, experimental results show that, despite improvements regarding other methods, millions of environmental interactions are still required. So, even if the proposed approach is able to reach state of the art performances, the main problem, which is stated by the author in the first paragraph of the introduction, still remains (I quote) : "millions of environment interactions are needed to obtain a reasonably performant policy for control problems with moderate complexity". Of course, this remains one of the biggest challenges of (deep) RL.  The paper is well positioned regarding the literature.  Clarity: The paper is very well written and organized. Intuitions and ideas are well presented and illustrated.  Significance: the topic is indeed interesting, and I think other researchers in the field will definitely be interested in the results of the paper.  *** Post feedback *** Thanks to the authors for the feedback and additional explanations. I have updated the confidence score accordingly.