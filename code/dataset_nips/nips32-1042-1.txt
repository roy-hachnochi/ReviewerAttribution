The paper provides with a method for off-policy prediction in RL, where instead of using Importance Sampling weights in order to learn the value of one policy given samples from another policy, the authors propose a resampling scheme, where first a sub-sample of experiences from an experience buffer are sampled according to these importance sampling weights, and then an update is done without multiplying with the importance weight ratio itself. This allows reducing the variance since these ratios might have extreme values in case the behavior and target policy are very different.  I found the paper interesting, however, I am not convinced how this differs from the Sampling Importance Resampling technique that is mentioned in the paper, which seems to already perform the same idea. In addition, I found some of the assumptions used in theorems quite restrictive and it is unclear to me why should they hold (see general comments below for details). The paper is generally well-written and clear, although some explanations are missing in my view, specifically in discussing the assumption used in various places, and in the experiments section where many details are missing.  In summary, this is an interesting technique that seem to perform well in practice and have benefits over existing methods, but I feel the presentation should be improved and some additional explanations should be added.   General Comments: - It is unclear to me how Importance Resampling (IR) is different from the Sampling Importance Resampling (SIR) method. You should explain this point. - You compare the bias of IR and BC-IR to the bias of WIS, but in the variance you compare to IS and not to WIS. why is this the case? why not compare to WIS? - line 126 - "For reasonably large buffers, \bar{\rho} will be close to 1" - why is this true?? this is unclear. - Assumption 1 is quite restrictive... assuming independence between transition tuples is not realistic since all transitions come from a behavior policy in the MDP, thus transitions are dependent in the sense that they come from a markov process - it is unclear why this assumption holds. - The conditions in Theorem 3.3 are quite restrictive, and the relaxation you propose still doesn't have to hold. Is there a reason for the conditions required for the low variance to hold in practice? also, you write once median and once mean, what is correct (line 189)? - line 203 - your last statement leaves the equation without giving the actual result - what is the expected difference between the two variances, where expectation is over buffers?   - In the experiment section, you choose a target policy that chooses in a deterministic way the down action, which leads to extreme ratios of importance sampling. What about a softer comparison, when the target policy is not as extreme and still has some probability to perform other actions?  - Figure 1 is very cluttered and it is hard to see where each algorithm is. Maybe consider enlarging it or somehow make it clearer. - In the experiments discussing the variance of the updates, you say that other policies are found in the appendix. Can you write whether the results are qualitatively similar for these other policies or are they different from the ones you present?  - line 287, line 291 - you are referring to Figure 5- there is no figure 5 in the main text. Is this supposed to be figure 3? if so, what is a and b (in line 291)? - Figure 3 - what is the error specified of on the y-axis? how is the sensitivity to learning rate measured here? - Figure 3 - rightmost graph - what is the iteration number on the x-axis? It is not specified and so it is hard to see when does IS and IR converge to the same variance. - Car simulator experiment - you show the learning rate sensitivity, but not the convergence curve- which is the more important plot to display. Also, in figure 4, what is measured on the y-axis? how is this measured? It is unclear to me what are we seeing in this figure. It seems that the lowest error for IR is when the learning rate is higher, not lower as written in the text. It also seems that IR is either worse or better than IS, depending on the learning rate, i.e. it is not consistent. - line 319 - "we showed that IR is consistent" - but you only showed that BC-IR is consistent, not that IR is consistent.   Technical Comments: - line 189 - "meanx" should be "mean" - Figure 2 - what is MARE on the y-axis? should it be MAVE?   --- Added after author feedback --- I have read the author feedback. My two main concerns regarding this work were the novelty of it and the usage of assumption 1. Regarding the novelty - the author mention in their feedback that the novelty is by applying the method (which is not different from SIR) to the RL setting where the buffer is not fixed but online. However, once they assume assumption 1 and assume in all analysis that the buffer is a sample of n i.i.d samples from some fixed distribution, then this is not a valid argument... it does not capture the online nature of RL and the online nature of the content of the buffer. Thus, this becomes similar to previous works using this in domains other than RL. I am also not quite convinced that the assumptions in theorem 3.3 are easily met, and the explanation given in the  feedback isn't very formal but more 'intuitive', which weakens the result. My second main concern was the use of assumption 1, however, since it seems other reviewers advocate that this assumption is common, and given that the authors will also clarify and revise the experimental section, I am ready to increase my score.