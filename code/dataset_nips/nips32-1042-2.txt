Thank you for addressing my comments. In particular Q3 was addressed and I recommend (1) making the explanation re relationship between learning rate and variance in updates obvious;  and (2) mentioning that error bars are in the caption for Figure 3.  Given that two reviewers were confused about C1f I recommend explaining it in a footnote.  I will maintain my score after taking into account that I might have been over enthusiastic about the theoretical contribution. Thanks!  =========  Originality: Though there exist other works that weight samples from a replay buffer this paper presents a concrete alternative to one of off-policy learnings biggest woes: importance sampling. The paper provides extensive theoretical treatment and empirical validation that are original as far as i know. Here are few ways to make the originality clear:  O1: Some discussion of other approaches to non-uniform sampling of the replay buffer and how your technique varies (my understanding is that you are doing GVF learning compared to something like Q-learning which does not require the sampling distribution to match the policy distribution) O2: some discussion, perhaps in the conclusion, about extensions to control would be useful.  Quality: The theoretical contributions are of high quality. There are a few clarity issues i found which are specified in the next (clarity) section. I especially liked the detailed experimental setup and the motivations for using each of the domains. I was confused about one thing:  Q1: What Is the difference between “Step” and “Number of updates” in Figure 1? I think this should be clarified.  I think the experiments have adequately verified the improved sample efficiency and reduced variance claims. However, I find that some of the conclusions might be better supported by a more in-depth discussion of the results. I outline these below:  Q2a: L238-249: Though this paragraph makes sense, it can be improved significantly if the figures are  referred to more often in the text. (For example L240-243 which figure? Which part of it?) Q2b: I think L287 is referring to the wrong figure. Q3: L290-292: I think the kind of variance referred to here is different from what we care about. In particular, I think it means “variance in random restarts” and not necessarily “variance in the updates” (as shown in Fig 3 right). I think its fine if this was clarified. However, I should point out that Fig 3 left and center do not have error bars so we cant really say anything about “variance in random restarts” but only regarding “learning rate sensitivity”   Clarity The paper is overall well motivated and written. There are a few issues I have with the clarity of the mathematic claims and proofs:  C1a: The authors refer to the average update direction as X. This seems to be also mixed with the transition tuple. I strongly recommend decoupling the two. This is particularly problematic in Theorem 3.2 where X refers to both transitions in the buffer as well as the importance (re)sampling updates. C1b: There are some steps of the proofs in Appendix B that are somewhat confusing. Though i was able to follow most of it and didn’t find any obvious errors, I believe this paper could almost serve as a cornerstone paper if each of the steps are made explicit. For example, I don’t know how the equations after L500 progress. Additionally, it seems that mu_B refers to two different quantities in this proof and might cause unnecessary confusion.  C1c: L514 missing a >= 0 on the second line. C1d: Theorem 3.4 please restate the math statement after the text statement in L196 similar to L185. C1e: L169-170: “For high magnitude ratios…” I am not sure why this sentence is here. It sounds important but how does the work get around this issue. Is L171-173 an empirical observation about it? C1f: L126-127: I understand that for small buffers rho \approx 1 and therefore \bar{\rho} \approx 1 but why is it true for large buffers?   Additionally, there are certain parts of the text that were confusing to me: C2a: Minor typo L189 “meanx” C2b: L193 “and so potentially...” is incorrectly worded.   C2c: L100-101: why is it not straightforward? Perhaps you mean to say, “Since it is not straightforward…” C2d: L53-55 I have no idea what this sentence is trying to say. C2e: I think L60-62 should come earlier in the text as a very clear motivating reason for not wanting to use IS. C2f: L284 confusing as to what this means “simulating the many possible updates …”  I personally enjoyed the colour consistency and the clarify of the figures. There were a few figures that could use some clarification: C3a: Figure 7 and 8 should have “sample efficiency plots” come to the start of the figure. C3b: Figure 4: are the arrows for BCIR in the wrong place? C3c: Is it worth converting the exponents in the log scale plot to integers? 