This paper studies the low-rank tensor completion problem. Authors propose a two-stage nonconvex algorithm, where the first stage is initialization and the second stage is gradient descent. Authors prove that sample complexity is same as existing works, but this algorithm enjoys linear convergence.   Overall, the paper is readable and has good quality, but I suspect whether it is suitable for conference paper. One reason is that many key intuitions and backgrounds, especially for initialization step, are deferred to supplementary material.  So the story is incomplete and has no clear explanations. Besides this, I have some technical concerns. 1, the most pressing concern is assumption A3. Can we relax A3 since that's too strong. It is not usually required in other related methods. Is it because of A3 that you don't need regularization in GD? Under A3, the least square-like problem has no design.  2, Q1 and Q2 in Sec 1.2 are not well proposed. It would be better if authors clarify contributions more clearly. To me, the main challenge is sample complexity, which is not improved. It's not surprising that two-stage algorithm achieves linear rate with certain near-optimal statistical rate, since in principle linear rate comes from applying GD on square objective. This nonconvex objective appears in low-rank matrix completion and robust estimation quite often, and has been studied in [33].  3, there is no clear dependence with kappa in main theorem 1.4. Please clarify how the result depends on kappa. What's the power of kappa in step size.   4, why GD has no projection step such that the incoherence condition keeps holding along the iteration. Also, why we have an upper bound for the iteration times? This is not common for two-stage algorithms when it is applied to other problems, e.g. robust estimation.  5, A separate result for initialization step is needed. Using this initialization for other methods, e.g. [33], will the performance of other methods get improved?  6, for experiments, authors should provide comparisons with other advanced methods, especially [33, 63], to show the superiority. 