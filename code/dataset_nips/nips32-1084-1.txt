--------- post rebuttal comments: After carefully reading the comments from the other two reviewers and the responses provided by the authors on all the comments, I am willing to upgrade my score from 6 to 7. My concern about the paper being "highly specialized" remains, but it will be of interest to some within NeurIPS. --------- a) summary of the content: In this paper, the authors consider an online convex optimization variant where an online learner plays a series of rounds against an adaptive adversary. In each round $t$, the adversary picks a convex cost function $f_t$ and reveals it to the learner. The learner chooses a point $x_t$ and incurs a hitting cost $f_t(x_t)$ and a movement cost $c(x_{t-1},x_t)$ associated with the change of point over the previous round. The objective of the learner is to minimize the sum over $T$ rounds of per-round hitting costs and movement costs, and design online algorithms that perform well against the adversary, as measured by competitive ratios. Concentrating on the case of the $\ell_2$-squared norm $c(x_{t-1},x_t)=\frac{1}{2} \|x_t-x_{t-1} \|_2^2$ and $m$-strongly convex functions $f_t$, the authors show a lower bound of $\Omega(m^{-1/2})$ on any online algorithms, show that an existing algorithm called Online Balanced Decent (OBD) has a lower bound of $\Omega(m^{-2/3})$, and propose two optimal variants, Greedy OBD (G-OBD) and Regularized OBD (R-OBD) with an $O(m^{-1/2})$ competitive ratio. They also show that assuming that under additional smoothness for $f_t$, R-OBD can simultaneously achieve a constant, dimension-free competitive ratio and sublinear ($L$-constrained dynamic) regret.  b) strengths and weaknesses of the submission.  * originality: This is a highly specialized contribution building up novel results on two main fronts: The derivation of the lower bound on the competitive ratio of any online algorithm and the introduction of two variants of an existing algorithm so as to meet this lower bound. Most of the proofs and techniques are natural and not surprising. In my view the main contribution is the introduction of the regularized version which brings a different, and arguably more modern interpretation, about the conditions under which these online algorithms perform well in these adversarial settings.  * quality: The technical content of the paper is sound and rigorous  * clarity: The paper is in general very well-written, and should be easy to follow for expert readers.  * significance: As mentioned above this is a very specialized paper likely to interest some experts in the online convex optimization communities. Although narrow in scope, it contains interesting theoretical results advancing the state of the art in dealing with these specific problems.  * minor details/comments:  - p.1, line 6-7: I would rewrite the sentence to simply express that the lower bound is $\Omega(m^{-1/2})$ \- p.3, line 141: cost an algorithm => cost of an algorithm \- p.4, Algorithm 1, step 3: mention somewhere that this is the projection operator (not every reader will be familiar with this notation \- p.5, Theorem 2: remind the reader that the $\gamma$ in the statement is the parameter of OBD as defined in Algorithm 1 \- p.8, line 314: why surprisingly?  