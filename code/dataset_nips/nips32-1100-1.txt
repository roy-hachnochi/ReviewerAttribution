The main contributions of this paper are to propose an algorithm to learn a pooling architecture and one to grow the architecture with only self-organization principles. The developmental algorithm is evaluated on a different input geometry and on experiments with faults in the first layer. A last experiment evaluates the proposed algorithms on a MNIST classification task.  I like the originality of the work, as the authors propose the principle of a growing machine, that is able to yield a functional architecture from a limited set of rules. The principles to follow for building such self-organized network are clearly exposed.   The concerns expressed below have been answered by the authors' rebuttal, I appreciate that network is implemented with spiking neurons relying on bio-inspired hardware, as it has nice properties for processing temporal input stream. The results could be more convincing with a task relying on coincidence detection or time-structured events, like audio processing, echolocation or event-based vision.  The authors argued in the rebuttal that the global inhibition scheme is limited in space, indicating that it does not scale with the size of the network which was my main concern. This is thus a more biologically feasible approach that is compatible with hardware implementation.  The "flexibility" of the network allows the network to address the so-called packing problem, that is how to efficiently cover the sensor space. The authors shown in their response convincing experimental results with a hyperbolic geometry that have non-uniform density.  -- Original comments -- I could not understand why the authors rely on a spiking model for the layer-I and ReLU units for layer-II. Model of sensory inputs with intrinsic noise could be modeled with neural masses or discrete neural fields. What is the benefit of spiking neurons in this contribution?  On the layer-I, the local-excitation and global-inhibition scheme is encountered in the literature but it is limited by the inhibition range. Biological observations are seldom in favor of a global inhibition, except in organisms with a limited brain size (mainly insects) as a truly global inhibition for all neurons require a complex network of connections.  In my opinion, the proposed self-organization for arbitrary input-layer geometry of Sect. 4 is not convincing enough, mainly because the tiling of the input layer is uniform. I think that variation in the density of input sensors could better demonstrate the approach, for example following with hyperbolic distribution (like Poincar√© Disk).  In Sect. 6, I find that the random networks are performing very well on MNIST classification, how a network with random connectivity fed by spiking neurons is performing around 88% accuracy?   As a small note, I did not find the movie of Fig. 2 in the supplementary material.  This approach is highly original and clearly exposed. The quality and significance are less clear as this approach is difficult to compare to existing methods.