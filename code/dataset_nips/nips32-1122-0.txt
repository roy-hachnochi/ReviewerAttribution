The paper presents a semi-parameteric model for long-term planning in a general space of problems. It works by training parametric goal-conditioned policies accurate only on local distances (i.e. when current state and goal are within some distance threshold) and leveraging the replay buffer to non-parametrically sample a graph of landmarks which the local goal-conditioned policy can accurately produce paths between. Moving to any goal state is then accomplished by (1) moving to the closest landmark using the goal-conditioned policy, (2) planning a path to the landmark closest to the goal using value-iteration on the (low-dimensional) graph of landmarks, (3) using the goal-conditioned policy to get to the goal state from the closest landmark.  The paper essentially tackles the problem that goal-conditioned policies, or Universal Value Function Approximators (UVFA), degrade substantially in performance as the planning horizon increases. By leveraging the replay buffer to provide way-points for the algorithm to plan locally along, accuracy over longer ranges is maintained. The method is close to a heuristic in the sense that there is no learning in this higher-level graph structure, it is largely agent designer prior knowledge to get UVFAs working at longer ranges. Despite this, I still think the method can be impactful as a first step to solving this challenging long-range planning problem.    The evaluation settings demonstrate that the method is viable and improves upon baselines for a variety of continuous control experiments. Unfortunately, most of the improvements are in navigation domains, and in this case I believe a very similar method for navigation has already been done (Semi-parametric Topological Memory [1], which I believe should be cited given its close similarity to this method). Therefore a more informative evaluation setting would be in tasks further away from navigation, and more towards general MDPs which is where I think this paper's contribution is. For example, seeing this implemented on Atari games would be a far more interesting result. It would also be great seeing this method used to drive frontier exploration (doing random exploration near states which have been visited the least number of times).  In conclusion, I believe the paper's idea of leveraging the replay buffer to build a set of non-parametric waypoints is an impactful contribution. Unfortunately, a relatively uninformative evaluation setting with too much focus on navigation (bringing it a bit too close to previous work [1]), makes me suggest a score of 6.  Some other points: - Figure text is extremely small and difficult to read. - It is hard to contextualize Figure 4(b) without knowing the lower-bound on average steps to goal. It would be easier to read this graph if you added the lower-bound as a dotted line. - How are the curves in Figure 3 generated? Are these the same HER model but in one evaluation you use planning and one you use the HER policy directly? Or is the red curve also using the planner at training time? - Also, what replay buffer do you use at each point along the curves? The one encompassing all states up to that step? Or the replay buffer at the end of training? - I believe the word "unanimous" on line 22 means things are in agreement, which doesn't seem to fit when describing tasks.  [1] Savinov, Nikolay, Alexey Dosovitskiy, and Vladlen Koltun. "Semi-parametric topological memory for navigation." arXiv preprint arXiv:1803.00653 (2018).  