This paper is different from most works focus on reducing the domain shift from perspective of loss functions, contributing to network design by developing a novel transferable normalization (TranNorm) layer. TranNorm is well motivated, separately normalizing source and target features in a minibatch and meanwhile weighting each channel in terms of transferability. It is clear different from and meanwhile significantly outperformes related methods, e.g., AdaBN [15] and AutoDIAL [21]. The TranNorm layer is simple and free of parameters, which can be conveniently plugged in mainstream networks. I think that this work will have a non-trivial impact: the proposed TranNorm can be used as backbone layer improving other state-of-the-art methods. The experiments are extensively evaluated both qualitatively and quantitively, demonstrating the effectiveness of the proposed TranNorm.   The TranNorm layer is key contribution of this paper. This layer consists of separately normalizing the source and target features, followed by weighting with $\alpha$ with respect to transferability, which is empirically defined. I would like to see ablation analysis on $\alpha$:  what will the performance be if one sets $\alpha=1$, and what will the performance be if the transferability is defined as softmax or Gaussian with tunable variance over discrepancy of statistics, i.e., $\mu/\sqrt{\sigma^2+\epsilon}$ (rather than only $\mu$).  --------------------------------------------------------- One of my comments is that what the performance will be if the two probabilities build upon $\mu/\sqrt{\sigma^2+\epsilon}$, rather than only $\mu$. However, after reading the rebuttal, I still cannot see what kind of distances are used in the probabilities of softmax and Gaussian, from the second table and analysis therein. Note that I check the submitted code in trans_norm.py (lines 152 and 156), finding that the two probabilities build upon only $\mu$. I wish the authors make further clarification and perform corresponding experiments in the final version.  I keep my original recommendation unchanged. 