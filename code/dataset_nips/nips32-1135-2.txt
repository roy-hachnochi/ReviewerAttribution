Strengths: + Proposes a new normalisation statistics-based method for DA. This line of attack against the domain-adaptation problem is rather under-studied compared to other approaches. + A good range of benchmarks. Evaluation on multiple base DA methods and network backbones. + Analysis Sec 4.4 is interesting.   Weaknesses:  1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi,  2017,”Universal representations: The missing link between faces, text, planktons, and cat breeds”. Although this paper may have made some better design decisions about exactly how to do it.  2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific.  3. Positioning wrt AutoDial, etc: The paper claims “parameter-free” as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. It’s not clear that removing  a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. "Seldom previous works", etc.  ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.   