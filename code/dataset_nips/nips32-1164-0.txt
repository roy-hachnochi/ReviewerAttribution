 Pro: +It provides a better theoretical guidance for learning the hyper parameters for model selection using the PEA theory, which accelerates DARTS .         +The results are impressive, though conjunction with a bunch of data augmentation methods.          + Theoretical proofs are valid.   Cons:  - The paper is somehow difficult to read, check the comments below. It needs clearer description. One thing for CIFAR10 is the author adopted many tricks (275 page 7) for learning. However, some of the works such as DARTS do not use part of it such as auto augmentation etc. Baseline algorithms should add same tricks for fair comparison.   There are two proposals that the author proposed, 1) regret for evaluation 2) wipeout. I think it also worths to see how different components affects the results in ablation rather than simply showing the final results. Also ablation about the model FLOPs or inference time are valuable    Comments of the paper.  The author us (#) as citation, which save space, but raise confusion between equation and citation (especially in checking the proof at supplementary). Change () to [] for better distinguish the two.   In PEA setting, there are p_t for prediction from the network. p^(j, k) as prediction from edge. I found this is confusing, better clarify the two p are predicting different things.   Bigger problem is at Fig.1, the visualization is not clear, it should have better notation with correspondence to PEA setting and the fonts are too small for a clear understanding of the figure. Make it better and clearer is a must.  Please relocate Fig.1 and All.1 to Page 3 for better readability.   ================================================== The author's feedback solved my concern, I keep the score the same. However, it also needs better clarification about the holding of assumptions.  