Post-rebuttal update:  I have read the rebuttal. Thanks for the clarification regarding they type of experiments where there is a larger gap between DAC and the baselines, as well as the clarification on PPO+OC/IOPG.   The paper proposes a new method for learning options in a hierarchical reinforcement learning set-up. The method works by decomposing the original problem into two MDPs, that can each be solved using conventional policy-based methods. This allows new state-of-the-art methods to easily be 'dropped in' to improve HRL.   Originality. The paper clearly builds off of earlier results like the option-critic framework. Nevertheless, the paper proposes new original ideas that are contribute to this line of work and that admit further extension.   Quality: The technical quality is high. I did not find inaccuracies in the main material, although I didn't read the supplementary material in detail. The experiments provide appropriate support for the claimed benefits of the proposed method. The experiments seem reproducible and reliable (based on multiple random seed, with standard errors provided for all results). From the experimental results, it would be interesting to analyze on what kind of tasks DAC clearly performs best, and on what types of tasks it is comparable to other methods like AHP+ PPO. I was surprised to see the relative performance of OC and IOPG looking quite different compared to the reported results in Smith et al, but that doesn't have a bearing on the conclusions of the paper. It would also be interesting to see how easy or hard it is to modify other approaches (OC or IOPG) to use a method like PPO rather than vanilla gradients.   Clarity. The paper is in general well-written and well-structured. Equations and experimental results are presented in a clear and interpretable fashion. Some minor comments are provided below.   Significance: I think the proposed method can significantly impact HRL research. The ability to drop in state of the art optimizers is very interesting and make it easy to further improve results in the future.   Minor comments: l. 27 "there are no policy-based intra-option algorithms for learning a master policy". These seems to contradict the overview in table 1. Perhaps this statement could be made more specific.  l. 45. The definitions seem to be specific to discrete MDPs (eg specification of the co-domain of p as [0,1]). However, later on continuous MDPs are considered as well.  l. 62-63. In the equation for u_pi, on the RHS, I think the second s should be s'? l 155-156. Maybe the statement here can be made more practical. I don't know any practical policy optimization algorithm that can be guaranteed go improve the policy, due the stochastic nature of the gradients. E.g. in case gradients of the base algorithm are unbiased, is there anything we can say about the combined pi^H and pi^L updates? l 173-174. Polling can also be seen as a special case of call-and-return (where B is always equal to 'stop'). Maybe this statement can be made more clear.  