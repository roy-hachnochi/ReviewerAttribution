Post rebuttal update:  I have read the rebuttal prepared by the authors and believe that they have made a good job in clarifying some of the points that I brought up in my original review. I also believe that the authors have the opportunity to significantly improve their paper by incorporating some of the comments made in their rebuttal; e.g., to clarify some the points that were not clear in their original submission and some of the statements which were, perhaps, too strong, in particular regarding the empirical performance/advantages of the proposed method.  ------  This paper introduces a method to learn option policies and their termination conditions by re-expressing the corresponding resulting SMDP as two MDPs defined over augmented state and action spaces. The authors introduce a mathematical formulation involving two MDPs that model the different challenges of learning hierarchical policies: a high-level MDP that models the problem of learning a policy over options and their corresponding termination conditions; and a low-level MDP that models the problem of learning an individual policy for each option. The authors also show that the value function of one MDP can be expressed in terms of the value function of the other, so that a single critic is required.  This is an interesting paper based on a variation of an existing method/formulation called Augmented Hierarchical Policy (AHP). It is relatively well written and the experiments demonstrate (1) that using the proposed method either improves performance (when used to solve a single task) or that it doesn't hurt performance; and (2) that, when evaluated on transfer learning settings, the learned options are often reusable across tasks.  I have few comments and questions:  1) I suggest toning down the sentences in the abstract and introduction that describe the empirical performance/advantages of the method (e.g. when the authors say, in line 8, that "DAC outperforms previous gradient-based option learning learning algorithms by a large margin and significantly outperforms its hierarchy-free counterparts in a transfer learning setting"). This claim is inconsistent with, e.g. line 220 ("The performance of DAC+PPO is similar to vanilla PPO in 3 out of 4 tasks"), line 247 ("[DAC] maintains a similar performance to PPO, PPOC, and AHP+PPO"), and line 250 ("[DAC works best] in 3 out of 6 tasks").  2) in the Background section, you introduce two different probabilities: p(s_{t+1}, o_{t+1} | s_t, o_t) and p(o_t, s_{t+1} | o_{t-1}, s_t). Why is the 2nd definition needed, if (assuming that I understand this correctly) only the 1st one is used to express the equations that follow those definitions? Where is the latter probability used/required?  3) to make notation more consistent, I suggest making the dependence of q(s,o,a) on the policy pi explicit, just like you did when defining q_pi(s,o) and v_pi(s).  4) in the equations immediately after line 84, the variables S0, O0 and S1 were used but were never defined/introduced. What are they?  5) it is not immediately clear to me why, in the AHP formulation, the policy is defined over an action space given by (B x O x A). In particular, if the policy pi^{AHP} already specifies what option o_t should be selected at time t, why does it need to also specify the primitive action a_t that should be executed? Shouldn't the primitive action a_t be picked stochastically by the policy of the selected option?  6) the introduction of an option "#" that is never executed (line 124) is not well-motivated. Why is it necessary to include it in the action space of the high-level MDP? And what is O_{-1}?  7) in Table 1, what do you mean by the "Compatibility" column? In particular, what does it mean for an algorithm to be compatible?  8) I am not sure I understand the decision of using an on-policy method (vs an off-policy method) "[given that] O is not fixed [and that therefore] the MDP M^H for learning pi becomes non-stationary" (lines 187-189). What is the relation between an MDP being non-stationary and an on-policy method being more appropriate?  9) I am curious about the possible relation between (a) the observation that when using DAC the termination probability often becomes higher as training progresses (line 296); and (b) the option occupancies shown in Fig3. In particular, why is it that during the first task, a single option is selected most of the time (the blue one), while during the execution of the second task, options alternate very fast even during the execution of each "part" of the Cheetah movement? Is this related to the fact that the termination probabilities are already very close to 1 when the second task begins?