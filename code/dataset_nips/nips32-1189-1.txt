The paper touches an issue that is very important and most likely the reason why hyperbolic embeddings have not been adopted widely. From my experience, hyperbolic embeddings sometimes have catastrophic results compared with competing methods. This is because of numerical instabilities. The paper is very well written with a lot of theoretical and empirical results. The solutions the authors provide is theoretically proven and very well documented. The experiments are also sufficient and realistic and they prove the point. The only problem of this paper is that it has too much information that most of the NeurIPS audience would not be able to follow, as it is not familiar with a lot of interesting mathematical terms. I spent significant time to go through the references. Given the complexity of the solution, the authors have done a good job explaining some mathematical concepts, but they definitely needed more space. The supplementary material is 16 pages!       line 45: A schematic would explain the idea better reference [4] : “In In Flavors of geometry “ repeated “In” line 71:” Since  for hierarchical data such as a tree with branching factor b, the number of leaf nodes increases exponentially as the number of levels increases”. I have seen this explanation in several papers but I think it is not correct. Also, this argument is fundamentaly wrong as it compares an infinite space with a finite space. The number of the leafs of a hierarchical space grows exponentially but remains finite. The area of a disk contains infinite points. Technically speaking the unit circle can fit the universe. The reason why euclidean space is different and it is described very well on this paper: As described in this paper https://papers.nips.cc/paper/5971-space-time-local-embeddings.pdf “...The maximum number of points which can share a common nearest neighbor is limited 2 for 1-dimensional spaces,  5 for 2-dimensional spaces and so on.While such centralized structures do exist in real data d-dimensional spaces can at most embed (d + 1) points with uniform pair-wise similarities.” Also the triangular inequality imposes more constraints.  See the references:  K. Zeger and A. Gersho. How many points in Euclidean space can have a common nearest neighbor? In International Symposium on Information Theory, page 109, 1994. L. van der Maaten and G. E. Hinton. Visualizing non-metric similarities in multiple maps. Machine Learning, 87(1):33–55, 2012.  I suggest reading this blog https://networkscience.wordpress.com/2011/08/09/dating-sites-and-the-split-complex-numbers/ and also this paper: https://dl.acm.org/citation.cfm?id=2365942   line 86: “This suggests that the  numerical model used for learning an embedding can have significant impact on its performance.” To me that doesn’t come as a logical consequence. The reference talks about the importance of curvature, flat, negative, positive. At least that is the main point. The title doesn’t address the numerical issues. line 91:” However, those models suffer from the precision and accuracy problem” We need a little bit of clarification here. Where the models compared with other embeddings and didn’t prove to be competitive in performance? Do the authors believe that the reason was numerical instability? If yes it would be nice to repeat these experiments with their solution and prove the point. line 131: The NeurIPS audience is not very familiar with Fuchsian groups. I had to do a lot of reading to understand how the tiling is done. That paragraph is too condensed line 146:”plane as a pair consists of “ change to “plane as a pair that consists of” line 162: “Importantly, any element of G can be represented in the form “ Is this a fact for all Fuchsian matrices? line 248:”Construct tilings from a set of isometries that is not a group.” This is an interesting idea. Can we have a reference for that? An example of isometries that are not groups would be useful here. 