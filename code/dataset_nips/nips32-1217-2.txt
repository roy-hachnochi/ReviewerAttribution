The work describes the proof for establishing the milder over parameterization and better iteration complexity using GD or SGD under the assumptions of Gaussian random initialization for each layer, similar to 2 that results in great improvements to both.   The assumptions and setup is similar to 2 (as referenced), with the main difference being the exploitation of the gradient regions for all the data points, that allow for larger area, tighter bounds and linear trajectory length. The complexity increase from this increased computation is stated to be O(n) for n data points. This needs to be factored into the complexity calculations and it is not apparent if this treatment was done justly under 4.1. Further, the remark under 3.11 sounds like it could be better explained for the case of 2 layer ReLU networks (than referring to appendix). It would also been great to include practical results for implementation using e.g. data sets to support the theory.  Overall, this paper sounds like a positive contribution to advancing state of the art with above caveats.  