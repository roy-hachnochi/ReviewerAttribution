- How does the channel wise attention module help in addition to the word-spatial attention? An ablation here would be useful to validate this architecture choice. - In equation 4, what is the interpretation of \gamma_i. Since the word embeddings come from a pre-trained bidirectional RNN, it is not clear why this number should correspond to a notion of the “importance” of a word in the sentence, and it is not clear to me what importance means here. Is my understanding correct that the objective in equation 5 is for the model to maximize the correlation between words depicted in the image and the important words in the sentence? - When the semantics preservation model is used, how does this affect the diversity of the model samples?  To my knowledge the proposed word-level discriminator and channel-wise attention are novel. The paper is well written despite a few unclear points mentioned above. However, the significance is difficult to judge because the CUB dataset is somewhat saturated, MS-COCO samples are not realistic enough to easily assess the controllability, and the text queries do not seem out of sample enough to test the limits of the text controllability.