 This paper uses the formalism of autoencoders to model the joint probability distribution of the text attributes of a _ pair of nodes _. The correlation/dependence is encapsulated by the corresponding pair of latent variables, in the case where the two nodes are connected by an edge. In a first derivation (7), the edge is assumed visible. Later (8), it is assumed latent and marginalized.  The manifest advantage of this approach is that the information about edges and the text attributes of the vertices are encoded simultaneosuly, unlike previous approaches as reviewed by the authors.  The parameters of the posterior distributions are estimated using an original textual embedding which takes into account the text of the other nodes (Section 3.2). Even just this embedding alone leads to significant accuracy improvements for an existing SOTA method (CANE; the integrated method is noted as PWA in the tables).  I only have a somehow major remark about the presented approach. The authors claim that this method is able to encode "structural information" per node. This means, create an embedding of its connectivity to other nodes. In reality, the embedding (h) used in the approach is just an arbitrary vector per node that is learned by minimizing the training objective. How could this be proved to be connectivity information?  The authors also state that they are able to mollify the computational complexity deriving from modelling pairs of vertices. It would be very useful if they added actual computational times for both training and inference (they are missing also from the Supplementary).  ==== AFTER THE AUTHOR RESPONSE: I am overall satisfied with the authors' response. About H, I had no doubt that it is useful (see Fig. 4(d)): whether it comes into correspondence with actual structural information is still to be proved.