This paper proposes to use a mixture of generators to solve the mode collapse problem. Specifically, for a certain generator, it mainly focuses on the training data which is not covered by previously trained generators. By training a sequence of generators, this paper provides a theoretical analysis that the mixture of generators can cover the whole data distribution.  There are three concerns. The first concern lies in the second paragraph. It claims that most divergences are global and are not ideal for promoting mode coverage. It lacks supports, such as related works and experimental results. Besides, several existing works claims that KL divergence can largely solve the mode collapse problem [1][2]. I think these related works should be discussed in the main part. The second part is about speed and scalability. Since the mixture of generators is trained sequentially, what is the time complexity compared to training a simple DCGAN for all these experimental settings, such as in fashion-mnist and toy-data. The third concern is about the experimental setting. For the first toy data in Figure 2, whatâ€™s the weight of the minor mode compared to the major mode? In real-world application, will this mode just be treated as noise? On the other hand, I am curious about the likelihood of samples in the minor mode estimated using the kernel density estimator with sufficient enough samples and sufficient small variance. Besides, this paper does not provide any large scale results, such as cifar10 and celebA.   [1] Nguyen, et al. "Dual discriminator generative adversarial nets." Advances in Neural Information Processing Systems. 2017. [2] Du, et al. "Learning Implicit Generative Models by Teaching Explicit Ones." arXiv preprint arXiv:1807.03870.  # Post-rebuttal comment  I have mixed feelings for this paper. On one hand, this paper proposes a new method (i.e., the point-wise convergence) to define the mode collapse problem, which is novel and interesting. And a corresponding algorithm is provided to train a mixture of generators to address the mode collapse problem. On the other hand, the experiments are the main limitation: no experiment on large scale natural images is provided, such as the Cifar10 and celebA. Indeed, the authors address most of my concerns. However, the claim of the Figure 6 in Appendix B is incorrect. If we use the KL divergence, i.e., KL(P||G), rather than the inverse KL, i.e., KL(G||P), the green distribution will be chosen rather than the red one. According to my numerical results, KL(P||N(0, 1)) = 4.6, whereas KL(P||0.25N(-10, 1) + 0.5N(0, 1) + 0.25N(10, 1)) = 0.36. It verifies the zero-avoiding properties of KL divergence[2].   To summarize, I still have an inclination to reject this paper in its current state, because of the limitation of experiments and incorrect claims for the KL divergence. However, based on its novel contribution, I will also be happy if this paper will be accepted.