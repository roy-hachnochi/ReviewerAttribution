The paper describes a new algorithm for training a mixture of generator that can, with some theoretical guarantee, capture multiple modes in the data distribution. One of the main contributions of the paper is a new measure of distance between distributions that explicitly considers point-wise coverage (rather than the traditional global notions of coverage, e.g. f-divergence), optimizing for this metric allows one to construct mixtures that aim to capture multiple modes (even minor ones) in the target distribution. It appears that existing work training generators with modified objective functions and those that ensemble multiple generators do not explicitly train for this type of "point-wise" or "local" coverage, but rather train for some proxy that may indirectly encourage coverage of multiple modes. In this light, the contribution of both the distance measure and the algorithm is significant. Furthermore, the proposed algorithm comes with theoretical guarantees of certain degrees of point-wise coverage (given appropriate choices of the number of expressive generators in the mixtures and a covering threshold). This is valuable as the author demonstrate that theoretical analysis lends insight to how to choose hyper parameters in practice. Finally, the paper presents evidence, in cases where the number of ground truth modes are known, for the effectiveness of the proposed method on synthetic and image datasets, and comparison results with respect to other methods that aim to capture multiple modes. Overall the paper is very well written. The technical exposition is clear and easy to follow. The theoretical analysis is thorough and the central idea is intuitive and appealing. 