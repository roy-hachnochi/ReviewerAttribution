The paper is well written and organized, the appendix is extensive. A new heuristics, called episodic backward update (EBU) is presented. The heuristics aims at increasing data-efficiency in sparse reward environments. To mitigate the instability, that this less random approach introduces, a diffusion parameter is proposed together with EBU. A method for adaptive selection of the diffusion parameter is presented. This is replacing a more general approach by a heuristic, that even needs another parameter to be robust. This is not a good idea in general, but in the domain of sparse rewards the paper shows, that the heuristic has its advantages. So it might well be a method that will be used in these domains.  The plots in Figure 5 are very interesting, showing some problems of the methods (which I believe are common in Q-function based methods, but not so often discussed).   A few suggestions/comments: The comparison of "39 days" with "a couple of hours" is quite unfair, as in the former case, vision has to be learned as well. In subsection 3.1 the Q is not bold, you could use \boldmath{Q} The font of Figure 2, 4, 5, and 6 is very small, practically unreadable.  