The main point of conduction to me is the main objective of this paper (not the objective they introduce). The authors write about  “diversified representations”. Why is this useful? What does this mean? How is this different from disentangled representations? How did they show their own definition empirically.  This main argument is hard for me to distill from the current version.  That being said the paper is well written and easy to follow.   Next I would like to ask for further clarifications on the objective that is been introduced, the method, itself. a) In section 3.3.2 , could you expand your explanations ? Why is (8) a bound on (7)? b)  The optimisation seems very involved, can u say more about the hyper-parameter space you have been looking at/ sensitivities? c) Can you extend on the kind of networks that you have been using ?  aka the backbone networks? d) What would happen if you let parts of your objective go? E.g. do you need  term one Eq 3? e) What form does the discriminator network have? If I was to say that the classification performance is probably so good because the discriminator is pretty close to a what a classifier does, what would you say? Similarly like a GAN would have better FID scores than a VAE because the discriminator has such a similar functional form. And thus in that way you are somewhat cheating?  In this submission, I would have enjoyed reading the code.  The experiments seem competitive to other methods, even though error bars have not been provided. However again I can not find a clear hypothesis and respective experiments on what diversified means.  In what context other than classification is it useful? 