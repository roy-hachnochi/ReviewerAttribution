The authors examine the ability of mutual information constraints to diversify the information extracted to the latent representation.  They study this by splitting the latent representation into two parts which are learned in competition using a mutual information constraint between them and opposing (min/max)  mutual information constraints between the latent representations and the data. To train the model, the authors use a two-step competition and synergy training regime. The authors demonstrate the improved performance resulting from learning diversified representations in both the supervised and self-supervised setting. In the supervised setting, the authors show that the competing representations differ in their sensitivity to class-wise differences in the data.  The difference is proposed to arise from the separation of high-level and low-level information learned by the two representations. In the self-supervised setting, the authors examine the role of the diversified representations in disentanglement and show increased performance compared to B-VAE and ICP.  Major Comments: - The idea is well-motivated by the information bottleneck literature and the although the mutual information derivations present in this work are not novel (1), the role of diversified, competing representations in this context is not so well studied.   Minor Comments: - an x-axis label stating which dimensions are from z and which are from y should be included.   [1] Chen, T. Q., Li, X., Grosse, R. B., and Duvenaud, D. K. Isolating sources of disentanglement in variational autoencoders. In Advances in Neural Information Processing Systems, pp. 2610â€“2620, 2018 [2] Kim, H. and Mnih, A. Disentangling by factorising. In Proceedings of the 35th International Conference on Machine Learning, 2018.