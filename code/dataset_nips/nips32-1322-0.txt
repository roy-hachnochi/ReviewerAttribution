The paper proposes a tensorized version of the transformer to reduce the number of parameters in the neural network architecture. The idea is original,  and the empirical results are convincing on language modeling as well as machine translation. The writing however is not very clear (e.g. para 2 in intro, section 2.2)  - please have your paper proofread for language and grammar. The results however are quite significant and will help scale the transformer to larger problems.   Comments: 1. In theorem 3.1, the relation of the basis vectors to Q, K, V is confusing in the current notation. Might be useful to clearly specify it. 2. Did you try using more than 2 cores or a bigger model with more layers (esp. since the original transformer was limited by GPU memory)?  3. The paper is missing discussion on some very related work: a) Generating Long Sequences with Sparse Transformers Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever  **Edit**: Thanks to the authors for their response. Please add in the discussion of the sparse transformer in the related work and important details such as your definition of basis vectors to the main paper to improve the clarity.