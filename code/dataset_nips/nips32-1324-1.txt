*** UPDATE *** Thank you for your response to the points raised. The modifications to the presentation that have been discussed should lead to an improvement in the manuscript, so I have increased my score by one level.  This paper derives the distribution of test statistics based on maximum mean discrepancy (MMD) and kernel Stein discrepancy (KSD). The focus is on testing for the "best" model among a range of candidates, as opposed to goodness-of-fit testing. The use of these test statistics is empirically illustrated.  My impression of this manuscript is that the idea and the analysis is probably fine - testing is not something in which I am an expert - but that the idea is somewhat incremental and the writing style comes across as sloppy and imprecise, to the overall detriment of the manuscript. Thus the comments below are an extensive list of presentational issues which need to be fixed:  l1. The first sentence of the abstract, frustratingly, isn't clear. I think the way that the positive and negative labels are being assigned is the opposite to what seems natural, and that causes confusion. To me "positive" should naturally be associated with "joint best model".  l58. "for data" -> "from data"  l95. The authors write "(assumed to exist under mild conditions)". If interpreted literally, this would be ridiculous, so rephrasing is needed. Moreover, the authors should actually spell out the "mild conditions" and provide a reference.  l131. The authors write "J = argmin" which is extremely confusing given that one of the central premises of the manuscript is that the "best" model need not be unique. The "\in" symbol should be used instead. Similarly on l150.  l158. The symbol \mu is undefined. I would guess it is meant to be z.  l231. The statement of Theorem 4.1 is unclear to me - I did not understand it. For example, TPR is a function of the random sample, so how can it be true that TPR_PSI >= TPR_Split holds deterministically for all N? Could there not be some realisation of the data such that TPR_PSI < TPR_Split?  l234. Moreover, the statement of Theorem 4.1 is unacceptably imprecise. For example, \mu is defined in words as "the population difference of two discrepancy measures". Which discrepancy measures? The authors should be precise and use mathematical formula instead.  l240. There are grammatical issues with Lemma 4.2 which preclude a precise mathematical interpretation of what is being said. These include, but are not limited to, l241 where "best. We" should be "best, we". (Also, see lA458.)  l244. The authors reference equation 4.2, but there is no equation 4.2...  l271. The authors write that "In this case, H0 is true". However, in this case P_1 and P_2 seem equally close to R, so why is H0 true? It would have thought it was false, since H0 would prefer P_1 to P_2.  l287. The line "linear time estimators perform the worse than the complete estimators counterpart" has a grammatical issue, but also the issue that linear time estimators do not seem to be mentioned beforehand in the main text.  l288. The results in Fig.1 do not seem to account for the sampling variability of the dataset, and seem to be based on a single realisation of the dataset. Some assessment of the sampling randomness on these results is needed.  l444. "stein" -> "Stein"  lA474. The authors call \nabla_x \log p_i(x) the "log density", but it is the gradient of the log density.  lA477. One of the \nabla_x should be a \nabla_y. See also lA481.  lA478. Is writing "m_2 = m(m-1) / 1" really necessary? (i.e. remove the "/ 1"). 