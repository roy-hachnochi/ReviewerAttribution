After rebuttal: I focused my comments and attention on the utility of this method on providing behavioral representations, whereas R3 and the authors drew my attention to the novelty of the separation loss, and their specific intent to primarily model _individual decision-making processes_, and not behavior more generally. The text could use clarification on this point in several places. I still do think that existing PGM-based approaches with subject-level random variables are a fair baseline to compare against, since they do create latent embeddings of behavior on a per-subject basis (with e.g. per-subject transition matrices in HDP-HMM models of behavior), but want to recognize the novelty of the architecture and approach.   -------------------------------------------------- Originality: There is a large body of work for machine learning modeling of time-series behavior with interpretable latent spaces. I find the originality low in the context of that prior work, including Emily Fox's work on speaker diarization and behavioral modeling, Matthew Johnson's recent work on structured latent space variational autoencoders, and others. Given the input of the model is a bag of low-dimensional sequences, probabilistic graphical models are an appropriate baseline here.   Quality: The writing, construction of the model, and evaluation of the model are sound. However, the model is not compared to any alternatives. It is difficult for me to place an absolute significance on the work if it is not compared to even a naive or strawman baseline. For instance, if you chunked the input sequences, did PCA on all chunks, and averaged the embedding, how close would that get you to the properties of the model being proposed? Is an RNN even necessary? If the ultimate goal (or at least stringent evaluation) of this model is to tell apart different treatment groups, then set up a classification task.  Clarity: The work is clearly written and the figures are well-constructed and present information clearly.   Significance: The work is if low to moderate significance, given the context the work is in. Directly comparing to alternate ways of capturing an interpretable latent space would help lend significance, if this method was indeed better, faster or easier to use than alternate methods. The model is clearly capturing task-related information in figure 4, but I have no idea if an RNN is required to do this, or if a much simpler method could do the same. Without this backstop, I don't know if this is an interesting method. 