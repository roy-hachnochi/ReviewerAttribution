Summary: This paper explores control with modular limbs which can change dynamically change their configurations as a part of their action space.  This paper defines a limb, which can connect itself to other limbs and exert torques. It is a basic building block from which the agents construct their bodies. This paper limits the morphologies to tree like structures but could be generalized. A single controller neural network is shared across all of these limbs and therefore decisions about their actions are done in a distributed fashion. These limbs communicate in 1 direction when they are connected with a setup similar to an RNN. This agents successfully learn to connect into bigger bodies and solve tasks such as standing and locomotion. As baseline the paper chooses an agents which can the fully observe and act on all of the links.  The experiments are aimed at showing a better generalization to novel scenarios than the baseline. However every algorithm is only run once and details of the task are not clearly described. The baseline algorithm fails to learn properly with the given number of limbs, which is unexpected given other work in the field on high-dof control systems. The results also do prove that this approach generalizes well across number of limbs. Specifically during locomotion the algorithm learns a policy which is very robust with respect to loosing limbs.   Overall the idea is novel, the paper is largely well-written, and the positive results are promising.  In particular the zero-shot generalization to a different number of limbs is a compelling result, which is not possible for non-modular architectures.  The main caveat is that the baselines failed on all 3 tasks, which suggest a poor choice of evaluation domains or lack of effort on baseline performance.    Comments: The idea and approach presented in this paper is very nice. These agents are  modular and learn reassemble themselves according to the context. Also the application of Dynamic Graph Networks is nice as they are well matched to the problem. The choice of baseline also seems appropriate as it well represents the currently used approaches. In the explanation however I found several parts which I would have prefered to be clearer:  Action space: The linking and unlinking is described at several places within the paper but it is not clear to me how exactly it works and which limbs is responsible for what. My understanding is that: linking is initiated by the parent, unlinking by the child and torques are determined by the parent. Does that mean that each child node connected to the same parent experiences the same torque? What happens when a node does not have any children? In classical control tasks (e.g. cheetah, humanoid) such torques would not be possible but the video shows a controllable single limb bodies. My only interpretation is that the torque is between the limb and the world which is not mentioned anywhere though. Also there is no explanation what coordinate frames are these torques in (ie. body or world). It would also be good to include a description of the range of the magnetic force and torque ranges. In second 43 of the video we see 2 limbs flying directly up which cannot be achieved by torques alone. Is the magnetic range longer than 1 link?  Observation space: I miss a more detailed description of the observations. I assume the position 3-dim cartesian, but is unclear what is the representation for the rotation and whether there were any convolution layers applied to the 9*9 height grid or how big the grid was.   Communication: You describe DNN as a comunication structure. However later it is described that the information only flows in 1 direction and accumulates along the links. This setup is more similar to a RNN than a GNN. It is not clear whether more communication architectures were tried and what was the reason to choose this one. Also it is not clear how the links manage to find each other at the beginning. The information is only shared across connected bodies and so they can dynamically decide how to meet. In stand up case they can "agree" on specific x-y coordinates but it is not clear how it can work in the locomotion case.  Baselines and evaluation: This is very similar to a multi-agent setup which can be unstable to optimize and often have big fluctuations in their performance based on the random seed. The experiments as they are described are only ran once for each algorithm and seed which makes it hard to judge how big the gaps in the performance really are. The modularity provides a very clear advantages in terms of generalizations across number of lines but at the top of table 2 all the generalizations fall to >90% of their baseline performance. This would suggest that the biggest benefit might be the trainability of these systems. It is also not clear why the 6 link baselines failed to learn. The reasoning presented in the paper and supplementary material is that the action space was too big but 6 links * 3 actions = 18 dimensional action space which is comparable to humanoids where these agents still work. It was also not clear to me what the water environment is as both buoyancy and drag are mentioned.