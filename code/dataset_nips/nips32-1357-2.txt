The paper looked into an interesting problem of training self-assembling modularized robots and investigated the generalization of different training methods. Though there has been work in self-assembling robots or training modular robots with graph neural networks, training modular robots that can change its morphology through self-assembling is novel in my knowledge. The paper is well written and easy to follow. The experiments shown in this work consists of two tasks: stand up and locomotion, and the resulting policies are tested to generalize to different perturbations and morphologies.  One question I have about the method is that itâ€™s not completely clear to me how important the self-assembling part is to the generalization of the resulting policy? For example, if during training, one uses the graph neural networks representation and trains on a set of manually designed morphologies, is it also going to generalize to other morphologies?  Another question about the implementation of the self-assembling robot. From the video it seems that the robot sometimes breaks apart into multiple parts. Are those cases all due to the individual agent producing the unlink action, or the robot might break in other conditions (like due to large external force, or being taken by another link). In general it might be interesting to visualize the actions of linking and unlinking during the rollout.  Overall, I think this is an interesting paper with impressive results.  