Originality: To the best of my knowledge, conducting human psychophysics and comparing their performance to computational models has not been done for the precise problem formulation examined by the authors. However, the authors did not describe previous work on anorthoscopic perception, which has examined similar question (how do people perceive a figure that is revealed to them through a slit moving over it over time? Is the constructed perception equivalent in each order?) and has a long history dating back to Helmholtz. For a good review, see Rock, I. (1981). Anorthoscopic Perception. Scientific American, 244(3), 145-153 (https://www.researchgate.net/profile/Marc_TESSERA/post/Is_mathematics_a_human_contrivance_or_is_it_innate_to_nature/attachment/59d62f0ec49f478072e9f719/AS%3A273518934069250%401442223403218/download/1981AnorthoscopicPerceptionRock.pdf).   Quality: The human experiments and computational modeling are well conducted. Very high quality. My main criticism of the computational modeling is that the neural network that was tested gets different evidence than that given to participants. I would have preferred to see a comparison to a simple recurrent neural network that only gets subsequences in an online fashion rather than one that gets the entire sequence at once.  Clarity: Given the amount of material and the page limits, the paper is clear. It is well-written. There are some, minor experimental details that I think should be in the article itself. I will list them here for the authors (they did not affect my score, although I would expect some of them to be updated).    Line 117: Units given in pixels. This is device-dependent. Please provide a device-independent measurement. You could add in resolution and screen size or inches.   Lines 136-137: How were they blocked. Did they have to be \geq 67.5% accuracy for each block?   Lines: 142-144: Were there learning effects? Was it the same noise per trial randomly sampled once and then the same precise sequence is given to all participants/use the same seed or was the noise randomly generated for each trial and subject? Former is preferred, but regardless, please make clear.    More general: Were participants told the noise distribution? What cover story were they given?  Significance: Generally, causal perception and reasoning is a very, important problem of high interest to the NeurIPS community. I am sympathetic to the authors for examining a tractable problem that is similar in difficulty for people and models. However, it has low ecological validity. This would be less of an issue if the authors explained human performance using one or more of the computational models. However, it seems like people act differently from all of the models presented.   **Author feedback response: I thank the authors for responding to my criticism. I agree with them that "toy" stimuli are important for uncovering basic mechanisms. However, even these toy stimuli typically have some resemblance to a real-world task (e.g., direction in a random dot motion task is similar to what small noisy observations of a moving surface might look like to a perceptual system). I believe the connection in the task mentioned by the authors in their response is weak.   I still think the paper should be accepted and published. My hope is that the authors take the feedback to guide future work and perhaps that it may be worthwhile to revise some of the motivation for the manuscript.