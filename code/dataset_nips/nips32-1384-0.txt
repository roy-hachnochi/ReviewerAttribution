-- The rebuttal answered all my theoretical questions. I still feel that the work would be greatly improved by adding numerical experiments. --  This paper considers regret minimization in Markov decision processes (MDP). In particular, the authors refer to a specific setting called 'online MDP', where the dynamics, that is, the transition probabilities, are known while the reward is not. Regret minimization then refers to the idea to minimize the ``regret'' given that rewards could be chosen/observed in an adversarial manner. The authors start with a (rather technical) introduction, pose related work, and explain the main ideas based on concise preliminaries. Afterwards, an extension to large state spaces by using approximate occupancy measures and thereby avoiding concrete state-mappings is provided.   In general, I found the paper very well written and good to follow. The topic seems relevant - however, I would have liked a more thorough motivation for the setting where the transitions probabilities are already known while the reward is not. The authors name a few applications (on page 2), but no further explanation is given. That leads to the second problem: No experimental evaluation is provided. While I believe that the linear programming approach will work nicely, it would be good to see how the work performs in practice. That is even more true for the approximate approach for large state spaces.  Finally, the authors are not aware of some related work, see for instance   Chatterjee, Krishnendu. "Markov decision processes with multiple long-run average objectives." International Conference on Foundations of Software Technology and Theoretical Computer Science. Springer, Berlin, Heidelberg, 2007.  Křetínský, Jan, Guillermo A. Pérez, and Jean-François Raskin. "Learning-based mean-payoff optimization in an unknown MDP under omega-regular constraints." arXiv preprint arXiv:1804.08924 (2018).  A similar setting has also been considered in  Junges, Sebastian, et al. "Safety-constrained reinforcement learning for MDPs." International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, Berlin, Heidelberg, 2016.  I also have some technical questions:   1. Is an upper bound on the reward required? In that case, also approaches (expliting dual MDP formulations) from robust/uncertain MDPs could be interesting to compare with, see for instance:  Wiesemann, Wolfram, Daniel Kuhn, and Berç Rustem. "Robust Markov decision processes." Mathematics of Operations Research 38.1 (2013): 153-183.  2. Algorithm 1 basically just performs online updates based on the occupancy measure so far, that is on the history of the current policy. Is there not more information that could be exploited?  3. As far as I see a discount factor is not incorporated. Can the approaches be easily adapted?        