Summary:  This work is a continuation of a line of works on online Markov decision processes (Online MDPs). The main focus of this work is to tackle Online MDPs in very large state spaces. It considers the full-information setting where the dynamics of the MDP is known, while the reward function for each time-step may change adversarially, but is revealed to the learner after each decision. The underlying MDP dynamic has a fast mixing property. The goal of the learner is to maximize the total reward of horizon T. The authors design two algorithms, Alg. MDP-RFTL and Alg. Large-MDP-RFTL.  The paper is written clearly. The quality of the analysis is above average in my opinion. On the other hand, the topic of Online MDPs has been considered by several previous works, eg., [15] (Even-Dar et al. 2009), [28] (Neu et al. 2014), [13] (Dick et al. 2014), etc. In terms of the scope, while several of the works above also considered the bandit setting, this paper only deals with the full-information setting. The analysis and the algorithms follow several techniques from [15] (Even-Dar et al. 2009), [3] (Abbasi-Yadkori), etc.  The first algorithm Alg. MDP-RFTL has regret bound of order \tilde{O}(\sqrt{\tau(ln|S|+ln|A|)T}ln(T)). The idea is to use Regularized-Follow-The-Leader algorithm on the feasible set stationary distributions of the MDP, as in [13] (Dick et al. 2014). The regret bound for Alg. MDP-RFTL is \tilde{O}(\sqrt{\tau(ln|S|+ln|A|)T}ln(T)), which has an extra dependence on |S|, but in the same time has a better dependence on \tau than the SOTA regret bound in [15]. This algorithm requires the knowledge of the mixing time \tau.  The authors design the approximate algorithm, Alg. Large-MDP-RFTL, applicable for large state spaces with computational complexity independent of the state space size, with regret \tilde{O}(\sqrt{T}) compared against the best hindsight in a specified subspace of policies/stationary distributions. Alg. Large-MDP-FRTL also requires the knowledge of the mixing time \tau. The approximate algorithm uses projected stochastic gradient ascent (PSGA) sub-routine to avoid the computational complexity to be dependent on the size of the state and action space. However the number of iterations K(t) needed for the sub-routine PSGA for each time-step t has to be large enough. K(t) is of the order \tilde{O}(t^4\tau^8 T^3) (for K(t) defined in line 621), which may be large, when t, \tau, T are large. Could the authors give some discussion on this? Thanks. (Also the K(t) specified in line 254 in Theorem 2 is different from that in the proof in line 621.)  Another issue is about the unknown constant c, which bounds the infinity-norm of the optimal dual variable \lambda^* for the dual problem of Primal 1. The c’s dependence on |S| and |A| is not very clear, which may affect the regret bound \tilde{O}(c\sqrt{T}). (Only a few special cases were discussed in Appendix B, including an example with c\leq 2|S|.) Perhaps more concrete examples may be needed to better assess the quality of the regret bound in terms of c.  Considering the real-world applicability of the Alg. Large-MDP-RFTL: 1) First, the theoretical mixing time assumption of the MDPs may not be easily imposed on real-world problems. (While we also assume the dynamics of the MDP is given.) This mixing time \tau in addition appears in the Large-MDP-RFTL hyper-parameters H(t) and K(t). Thus, in practice, one must be able to estimate the mixing time accurately. Hence, if merely estimated approximately, a discussion  should be preferable to see how this might affect the algorithm. (More real-world examples in this online MDP setting may help to enhance the paper's impact.) 2) Second, the choice of \Phi should have an impact on the algorithm Large-MDP-RFTL, since the comparator in hindsight is restricted on the subspace determined by \Phi. (Small regret may not eliminate possible poor performance when compared against ill-picked policy linear subspace.) In retrospect, it may be nice to try to include some discussion on choosing the best linear subspace. 3) Third, the constant c in the regret bound \tilde{O}(c\sqrt{T}) for Large-MDP-RFTL is not clear, at the moment, in its dependence on the size of state and action spaces. Based on these concerns, perhaps, it is preferable to supplement the theoretical results with some experiments for this online MDP setting.  About the constraint set for \theta, there is inconsistency for the definition of the constant W, shown below: 1) (W as 1-norm bound for \theta): The constraint set \Theta for \theta is defined in line 204-205, where the constant W>0 bounds the 1-norm of \theta. (||\theta||_1\leq W). Then W can also serve as 2-norm bound for \theta. In line 555, ||\theta||_2 \leq W is used, for applying Theorem 3. 2) (W defined as such that 0\leq \theta(i) \leq W \forall i\in [d]): In the proof of Lemma 11, by recasting the 1-norm minimization as a LP problem Primal 1, the condition \theta \in \Theta is rewritten as -\theta(i) \geq –W \forall i \in [d]; \theta(i) \geq 0 \forall i \in [d]. Also in line 571 and 578, it says that “since all entries of \Phi and \theta are nonnegative…” In the proof of Lemma 1, W is also used in the same way.  There is an error in Lemma 16: In the proof for the upper bounds for R^\delta, , under line 580, the proof has a mistake: although ln(x) \leq x, we are bounding |ln(x)|=-ln(x). Perhaps we may argue that since R^\delta \leq R (as stated in line 579), where R is the negative entropy, both R^\delta and R are upper bounded by 0.  In the proof of Lemma 13 for bounding the Lipschitz constant G_F: |ln(dW)| is replaced by dW in the end. However, |ln(dW)| is not necessarily less than dW, unless dW \geq 1 (although we can usually assume it, for large d and W). Suppose dW \geq 1. Keeping the log may be beneficial: ln(dW) is smaller, which may help in the main theorem to reduce dependencies on d and W.  Experiments may further help evaluate the algorithms as in [3].