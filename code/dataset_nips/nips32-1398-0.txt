***Additional comments after author response:***  I expressed no major concerns in the review which needed addressing in the author response.  The response did elaborate on the relationship between the approaches to ReLU initialization considered and the earlier portion of the paper - this should be made clearer in the paper.  However, as pointed out by the other reviewers, the structure in the proposed Gaussian submatrix initalization has previously been proposed in Balduzzi et al. [2].  ---  Paper overview:  This paper considers the problem of neural network initialization. It analyzes how signals are transformed through the layers of a feedforward neural network, assuming weights are initialized from Gaussian distributions.  Previous work used a mean-field assumption to study these dynamics, and used the results to identify parameters for the Gaussians to ensure stable propagation of the mean of the signal variance through the layers, a necessary condition for training deep networks.  This work considers how the distribution of the initial signal variance is transformed through the layers of the network.  This is done by introducing an integral operator with an activation-dependent kernel to model the transformation.  This operator can be viewed as a generalization of the input-output Jacobian considered in other work, and so its spectrum is of interest for understanding stable signal propagation. Having established these results, the authors consider ReLU networks specifically for the remainder of the paper.  They derive the integral operator kernel for ReLU, and analyze its spectrum.  The analysis shows that there is no way to guarantee stable signal propagation with regular Gaussian initialization in general (i.e. not just in expectation).  Further study of the correlation dynamics illustrates that the correlations increase as one moves through the network - as was previously known -  but also that narrower networks exhibit higher variance in the correlation distribution.  Thus it is possible that networks with narrower layers might be able to more easily capture and maintain small correlations, yielding better trainability. (On the other hand, the authors also point out that broader layers protect against operator eigenvalues larger than one which are problematic for stable gradient propagation.)   The authors next propose a novel initialization scheme for deep ReLU feedforward networks.  The approach uses parameter sharing to essentially set up the initialized network to behave like a suitably initialized linear network with half as many neurons in each layer.  This allows negative correlations to be implicitly captured and propagated using the “duplicated” neurons.  The required parameters can now be initialized either through suitable Gaussian initialization or through orthogonal initialization.  Empirical results on MNIST and (clipped) CIFAR-10 give preliminary indications the proposed initializations train better at moderate depth than the standard He initialization.  Originality: The paper is the first I have encountered that explicitly models the dynamics of the finite-width signal variance distribution.  The resulting integral operator and kernel that are studied in the context of ReLU are thus in some sense new objects being studied for the first time.  The proposed new ReLU initializations combines a neat parameter-sharing idea with existing approaches to initializing neural networks.  Quality: This is high quality work, and I did not detect any noteworthy technical issues. One minor criticism is that the ReLU initialization proposed does not really use the machinery from the earlier part of the paper - even if it might have been inspired by it(?).  Furthermore, due to the weight sharing violating the iid assumption on the weights, the traditional theory for analyzing signal propagation is not strictly applicable.  As a result, the paper feels a bit like a mix of two papers: a development of some fascinating theory (which could probably do with more details provided), and an empirical study of a proposed initialization scheme which performs well, but arguably could do with a little more theoretical underpinning.  The authors also do not mention the (non-)applicability of existing theory in the weight sharing setting, which I think should be discussed.  Clarity: The paper is generally well-written; I list some minor corrections at the end of my review.  Significance: I think the idea of analyzing the dynamics of the signal distribution could be quite influential, and is likely to be developed further.  The proposed initialization scheme certainly seems to warrant larger-scale investigation; if it holds up it may become a new standard for initializing ReLU networks.  Other corrections/suggestions: Thm 1: “as transformation by” -> “as transformed by”; l.109: k_l is referred to simply as k L.127: correct “give rise of the signal propagation” L.132 and elsewhere: use \langle and \rangle for inner product L.139: x_1 and x_1 -> x_1 and x_2 Theorem 4: define \delta_0 and p_{\chi_k^2}. L.178 and below: x^m in text vs. y^m in Theorem 4 is somewhat confusing; similarly \lambda_{l,m} vs \lambda_{m,l} - tidy up notation consistency. L.190: it is unclear to me why the eigenfunction converges to the given \delta_0, which seems different to f_{-1} as specified in Thm 4. L.198: this is the inner product, not cosine similarity - as you point it, it is unnormalized Figure 2 caption: diamonds, not squares, would be a better description L.241: Note that the He initialization does not yield stable signal propagation in conjunction with dropout.  An alternative initialization scheme [1] indicates one should take the dropout rate into consideration.  How does that interact with this conjecture? L.259 : why include \sigma_{w,l} here? l. 271: “reduce a number” -> “reduce the number” Please specify the  batch size used for SGD in your experiments? L.278: “He Initialization” -> “He initialization” Ll.291-292: “more parallel” does not make sense. Perhaps: Closer to parallel, more aligned or more correlated. Right hand plot in Figure 4 does not add much, unlike Figure 3 - it may be more valuable using this space for some more details in the text. L.287: “but is not” -> “but it is not” Please improve the references section (Correct capitalization, missing sources such as for reference [4], cite published versions instead of preprints such as for reference [8]).  [1] A. Pretorius, E. van Biljon, S. Kroon, H. Kamper.  Critical initialisation for deep signal propagation in noisy rectifier neural networks.  NeurIPS 2018. [2] D. Balduzzi, M. Frean, L. Leary, J.P. Lewis, K.W. Ma, B. McWilliams. The Shattered Gradients Problem: If resnets are the answer, then what is the question? ICML 2017