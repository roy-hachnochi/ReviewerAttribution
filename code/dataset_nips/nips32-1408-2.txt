ORIGINALITY: high  QUALITY: high  CLARITY: medium SIGNIFICANCE: medium  The paper defines, analyzes, and applies new information theoretic quantities called gradient information. As we obtain Shannon entropy when we use the true distribution in cross entropy, we obtain gradient information entropy when we use the true distribution in the Hyvarinen formulation of Fisher divergence. Gradient information entropy is derived for common distributions (Table 1). We can further define conditional entropy and mutual information in similarly analogous ways and show that they together satisfy the chain rules. There are other nice properties: Gaussian is the maximal entropy distribution with a fixed variance (as in the classical case) and we can derive a Cramer-Rao bound-like result for mean squared error (Proposition 3). Because the Hyvarinen formulation bypasses normalization, we enjoy exactly the same computational benefits by using gradient information as in using the Hyvarinen loss. The authors demonstrate this by replacing the KL divergence with Fisher information in the Chow-Liu algorithm, yielding a substantial speed-up in runtime. Other applications on classification and community discovery are discussed.  (An extension of the Hyvarinen formulation of Fisher divergence for bounded RVs is also presented, which is a nice complementary contribution.)  One way the paper can improve is a more thorough discussion of 'why' these new quantities exhibit incongruences wrt the KL-based quantities. Is there a notion of cross entropy? Given how gradient information entropy is motivated, it may seem natural to define H_nabla(p,q) = E_{y~p}[s_nabla(y,q)]. Will it follow that H_nabla(Y) = min_q H_nabla(p,q)? I'm guessing that this is not included because it doesn't work out, but a discussion would be useful.   Differential entropy is basically useless on its own (Marsh, 2013). Can we draw analogies with gradient information entropy, which isn't even positive? The stability interpretation is not sufficient for me to feel I know how to interpret these quantities. In the information bottleneck setting, learning p_{Z|Y} to maximize I(X,Z) where (x,y)~p_{XY} has a clear information theoretic meaning. Does gradient information mutual information have any application in this scenario?   Due to such incongruences, we have yet to see if these new information theoretic measures will have significant impact in the future. Experiments suggest some promising directions.   There are numerous minor errors in the manuscript which get in the way of reading. Please fix: they include  - Change "log q(y)" to "log p(y)" in (2) - Change w to alpha in line 120.  - In Definition 1, why say Y = [Y1, Y2]?  - Include expectation when defining J(Y) in line 156.  - What are the actual values of alpha and beta used in Table 1?  - Section 3 can be written more clearly.  UPDATE: Thanks for the response. Please do include either results or a discussion on cross entropy, which is by far the most practical quantity in the classical case. 