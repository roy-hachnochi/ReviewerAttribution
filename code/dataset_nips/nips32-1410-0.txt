The paper contains technically novel contributions and comprehensive theoretical analysis that cover many important scenarios. It is also well-written. Thus, I recommend to accept the paper for publication in NeurIPS.   Below are my related questions:  1. SpiderBoost uses sampling with replacement. As it is known that sampling without replacement induces lower sampling variance, can similar analysis be applied to handle such a sampling scheme and obtain better complexity result?   2. The final output is randomly selected among all past iterations. Does this require to store all the generated variables?