Summary ======= This paper proposes a method to reduce the activation memory requirements needed to train (convolutional) neural networks, by using full-precision floating point numbers to compute exact activations in the forward pass, but storing quantized, low-precision versions of the activations in memory for use in the backward pass. This approach is evaluated by training ResNet models on CIFAR-10, CIFAR-100, and ImageNet, and comparing against the naive approach of using 8-bit activations for both forward and backward passes. The experiments demonstrate that it performs on par with using exact activations, while requiring ~8x less memory for activations.   Originality =========== + While the method itself is very simple, the paper does a good job contrasting it to previous approaches that trade off memory savings for additional computation (e.g., reversible models) and approaches that train low-precision/quantized networks where both the forward and backward passes use 16-bit/8-bit floats.   Quality ======= + The experimental results are convincing: the paper shows that using approximate activations in the backward pass closely matches both the training loss and test accuracy of exact activations, while saving ~8x activation memory.     + I appreciate that the experiments are all presented with error bars.     + In addition, Table 2 is very nice, as it demonstrates how this method can allow the use of significantly larger mini-batches on a single GPU.  * It may be worthwhile to have a plot like Figure 3 where the x-axis is wall-clock time, to underscore the idea that the proposed method is not computationally expensive.  * It is strange that the paper focuses only on convolutional neural nets, when the approach should in theory be applicable to other architectures/tasks as well. * Should include some diversity in the experiments, for example using VGG or DenseNet architectures.     - Is it the case that this approach would not reduce the memory requirements of training a DenseNet, because the "outstanding layer activations that remain to be used" include all preceding layers in the network? * Most importantly, it would be good to include a discussion and evaluation of different nonlinearities like sigmoid or tanh, beyond what is already discussed in Section 3.4. Would this method work to reduce memory requirements for training RNNs as well, where the LSTM equations make use of both sigmoid and tanh activations? This would be an interesting experiment to show, or perhaps to illuminate what the failure modes of the method are. * Similarly, experiments involving Transformer networks would be useful to demonstrate broader applicability. * All the current experiments use SGD with momentum as the optimizer; it would be nice for completeness to have results using other optimizers (such as RMSprop or Adam), to understand how they perform with gradients computed using approximate activations.   Clarity ======= + The paper is generally well-written. + Figure 1 is nice and conveys the method well. - However, Figure 2 is very crowded and hard to read.   Significance ============ + The method has negligible computational overhead while providing a constant factor reduction in memory usage for CNNs. I think this could be a useful trick to incorporate into deep learning packages.   Minor Points ============ * An interesting optional analysis would be to use the approximate activations in the forward pass, and then use exact activations in the backward pass (by storing the exact activations anyway), to verify that the exact activations are more important for computing the loss than for computing the gradient.   Post-rebuttal ========== I thank the authors for their clarifications in the rebuttal. While the proposed approach is targeted towards a specific class of architectures (those with ReLU activations), I think it would be a useful trick to incorporate into deep learning frameworks. I encourage the authors to add a discussion of its limitations to the paper (i.e., that due to the sigmoid/tanh activations, it is not suited for RNNs), but I think the paper presents an interesting approach for reducing activation memory requirements of modern CNNs. As R1 mentioned, the authors can also reference and discuss Banner et al. 2018. I raised my score from 5 to 6.