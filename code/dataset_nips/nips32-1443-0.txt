The authors present a methodology for a deep learning model for the nuisance parameters for causal estimation (mean outcome and propensity score functions), which is based around learning a shared low-dimensional representation of the confounders regularized for good finite-sample performance, and a learning algorithm for training this model based on their presented concept of "targeted regularization" (which is a regularization scheme inspire by TMLE). Using their novel methodologies they are able to achieve state of the art performance on standard datasets for high-dimensional causal inference. Their methodology combines multiple different ideas in causal inference (multi-headed deep learning models and targeted learning) in a novel way, and their empirical evaluation seems very strong, so I would recommend the paper for acceptance.  Some issues with the paper are as follows: - They claim that their methodology is stable because it does not involve any propensity terms in denominators. However as far as I can understand this is false because their prediction is based on their learnt \tilde{Q} function, which involves propensities in denominators in its second term (which is weighted by \hat{epsilon}. - The baselines in their evaluations are not completely clear. In particular it seems like the "baseline (TARNET)" method should be the same as "TARNET (Sha+16)" in Table 1, however they report different numbers in each. It seems like maybe the second is the number reported in past work, and the first is using the author's code possibly with different details in exact model architecture and learning hyperparameters, but this is not made explicit. - The authors claim that part of strength of model is insensitivity to very low/high propensity scores due to lack of propensity scores in denominators. However in their evaluations they exclude data with extreme propensity scores which makes this claim difficult to verify. In addition since different methods will estimate propensity scores differently it is not made clear what data points are removed, and whether different methods are being evaluated on the same data - It seems weird that Equation 2.2 has no hyperparameter for how much the two loss terms are weighted, is there a reason why no such term is included? - They claim that the third head in their model regularizes the model such that finite-sample performance should be improved. However no part of their experiments evaluates this claim. It would be good to see an experiment, even if it's just with synthetic data, that tests this (could be done by using a synthetic data distribution and comparing model performance with 2 vs 3 heads with small n versus very large n)