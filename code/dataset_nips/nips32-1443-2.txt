Summary: In this paper, the authors address the problem of estimating treatment effects from observational data when all covariates are measured (the ‘no-confounding’ assumption). The estimation proceeds in two stages: in the first step a model for the expected outcome, i.e., Q(t, x) = E[Y | t, x], and one for the propensity score, i.e., g(x) = p(T = 1 | x), are fitted; in the second stage, the average treatment effect is derived from the previously computed fits of Q(t, x) and g(x). The authors focus on improving the models estimated in the first stage with the ultimate goal of improving the treatment effect estimation in the second stage. For this purpose, they propose a neural network architecture called Dragonnet, in which the outcome models Q(0, x) and Q(1, x) are tightly coupled with the propensity score g(x). The authors then propose a procedure called targeted regularization for improving the asymptotic properties of the neural network-estimated functions in terms of estimating the average treatment effect, at the expense of predictive performance.  Detailed comments:  • Originality: The two methodological contributions appear to be original. Relevant related work is adequately cited in a separate section on page 5, but more references could be added. For instance, the paper “Representation Learning for Treatment Effect Estimation from Observational Data” by Yao et al. is probably highly relevant for this work.  • Quality: The ideas proposed appear sound, but are validated only through a limited number of experiments. I have doubts that asymptotic properties like double-robustness are achieved so easily for targeted regularization. It seems that coupling the estimators for Q and g will in general lead to loss of consistency and of the double-robustness property. The authors claim that “consistency is plausible – even with the addition of the targeted regularization term” because “the model can choose to set epsilon to zero”. However, the non-parametric estimating equation, which is needed to achieve the good asymptotic properties, is satisfied only for the value of epsilon that minimizes the modified objective (locally), and this value will not be zero in general. I also did not find the empirical study particularly convincing. For instance, the authors fail to explain how they combined targeted regularization with TMLE in for the experiments described in Table 2 (page 7) and Table 4 (page 8).  • Clarity: The submission is well-structured and easy to read for the most part. However, most of the figure and table captions are extremely bare and are not self-contained. What’s more, The authors tend to overuse hedge words and rhetorical questions in their argumentation.  • Significance: The two methodological contributions, Dragonnet and targeted regularization, improve on state-of-the-art approaches like TARNET and TMLE, respectively, but only incrementally. It is hard to accurately evaluate how significant these contributions are based on the limited number of experiments. A theoretical analysis of the newly-proposed estimators couple with a more comprehensive experimental section would go a long way towards shedding light on the significance of these ideas.  • Minor comments:  • The legend is missing in Figure 2. The x-axis scale should be removed.  • Is equation (3.5) missing a factor of (-2) on the right side?  • On page 7, the authors claim that “targeted regularization essentially never hurts”, yet in Table 3 the targeted regularization degrades the performance of the simple baseline estimator (row 1) in half of the cases. Could there be a typo in the table?  • Have the authors verified that “in cases where the targeted regularization loss term is large” the model responds “by setting the parameter epsilon to 0 and recovering the baseline”?  • The terms ‘semi-parametric’ and ‘non-parametric’ are used throughout the paper as if interchangeable. For example, at the beginning of Section 3 (page 3): “This modified objective is based on non-parametric estimation theory.” and later “We review some necessary results from semi-parametric estimation theory”.  • The footnote explanation for calling the architecture “Dragonnet” is curious as dragons typically have just one head (country to hydras). 