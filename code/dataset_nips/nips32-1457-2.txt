The paper provides interesting insights about the existence of asymmetric valleys in deep neural networks and claims that asymmetric valleys can lead to better generalization. The authors impose slightly non-standard strong assumptions, but empirically demonstrate that these assumptions are in fact practical and not difficult to achieve in practice. The paper is extremely well written and easy to read.  However, there are few issues which concern me: 1) The authors state that the biased solution at an asymmetric valley is better than the biased solution at an asymmetric valley. How does it compare to a unbiased solution at a symmetric valley? It is not clear how often do these algorithms end up in asymmetric valleys and if using a biased solution is a good idea when we land up at symmetric valleys  2) Do the authors run optimize for learning rates for SGD? Additionally, what decay schedules have the authors experimented with?   3) Why do authors run SGD from the SWA solutions? While this provides evidence that if are at an asymmetric valley then SWA performs better than SGD, however how often does SGD end up in the same neighborhood (valley) as SWA solution and the relative generalization guarantees are unclear?  Post-rebuttal: Having read the author's rebuttal, I would like to change my review from weak accept to an accept