Since 2017, there has been a considerable effort in improving confidence modeling with classifiers, with 2 majors goals: rejection when uncertain, and detecting out-of-distribution examples. In a work that has been mostly empirical and focused on DNNs, this line of work stands out by being mostly theoretical, taking its seeds from work with boosting with abstention.  There seems two main contributions in this work, using excellent theoretical derivations. However, their significance may be limited as the authors do not make any effort to connect them to the deep learning literature: 1) Negative result: In some multiclass setting using rejection, it is pointless to train a separate rejector. Solutions that converges towards the Bayes optimal solution requires the rejector to be a function of the Bayes-optimal scoring function, that is it should not be trained separately. 2) New Bound: The excess bound loss for CE (theorem 8) which clearly states that one can train with the cross-entropy loss (usual loss  for Softmax DNNs) without taking the rejection threshold 'C' into account. This is a very nice result, which confirms theoretically what most people were already doing empirically.   I am not sure is this is what the authors mean by "novel rejection criteria" in the abstract, but the main lessons I am taking from this paper is that they confirm me in what I was doing as a DNN practitioner.   This paper would greatly gain in significance with better connection to the Deep Learning literature.  1) The scope of the negative result is unclear. The example given is over over losses (MPC/APC with binary exponential loss) I have never seen used and can only imagine in the boosting literature. There has been recent attempts to train separate rejectors in DNN settings. See for instance "Learning Confidence for Out-of-Distribution Detection in Neural Networks" (https://arxiv.org/pdf/1802.04865.pdf), where the authors spend enormous effort taming a very unstable training procedure over the loss in their Eq.(5) with hacks, in particular in determining a hyper parameter lambda. Could we apply the conditions expressed in Eq.(6) to their work?  2) The new bound that justifies training using Softmax and cross-entropy should be better publicized. Which leaves us facing the same mystery: why does SGD fail so badly to converge towards the Bayes-optimal, producing over-confident outputs (see https://arxiv.org/abs/1706.04599 "on calibration of modern neural network").  One key part of the paper which would greatly benefit from more intuitive explanations is Section 3. The results are first presented without justification and I could not understand the explanations given from line 144-158.  While the paper is well written, the English could be improved and some explanation are sometimes unclear or confusing. A few detailed comments by line: 74 "It is well known": The Bayes-optimal rejector is not trivial, and some do not agree with it. It should be traced to Chow. The fact that the threshold should be (1-c) is not 'well known'. 128 typo "seperation"  145 Is the "objective function" W or dW/dr ? 171 MPS -> MPC 174 State that MPC and APC are the same with exp loss! Actually, this part is very hard to read: the distinction between MPC and APC does not add anything to the understanding of the paper and could be moved to the appendix. 227 Do you mean? This enables us to derive the same bound in a considerably simpleR way This enables us to derive a more general bound in a simple way 277 "can no more" ... "unlike" is a strange construction. 