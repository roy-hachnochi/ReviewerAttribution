The paper analyzes algorithm 1, which is very similar to the SGD with piecewise constant learning rates that people use for deep learning in practice.  The theorems result from separately bounding optimization and generalization error.  The optimization bound depends on one-point weak quasi-convexity (or alternatively, weak convexity), and the generalization bound is a uniform stability result.  Empirical results provide evidence that these assumptions approximately hold when training ResNet models on CIFAR-10.  One question I have here is why \mu (a curvature variable) is plotted as only an average \mu, unlike \theta.  In cases that the model converges to approximately zero loss (most cases), the assumptions appear to be holding along the optimization trajectory.  I think that at times, the paper overstates its conclusions, starting with the title.  As a theory paper, it should be more careful to state the limitations in practice.  Eq. 5 is an especially strong assumption for deep learning.  Comparing generalization performance using uniform stability upper bounds, which are very loose, can potentially result in misleading explanations as well.  Overall, I find this to be a useful step forward for theoretical understanding of deep learning optimization.  Update after author response period: the authors have agreed to add more discussion of the limitations of their results.  I have read through the other reviews and responses, and I will keep my review the same.