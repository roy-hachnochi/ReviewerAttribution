Recent theoretical work on analyzing Deep Neural Network training has focused on the evolutions of the over-parametrized neural network prediction errors under Gradient Descent. These evolutions can be neatly described in a matrix form. This paper found that the matrices can approximate an integral operator this is determined by the feature vector distribution only. Consequently, Gradient Method can be viewed as approximately apply the powers of this integral operator on the underlying function that generates the labels. Moreover, this paper also derives a new linear convergence rate on the new assumption of the underlying function. This analysis is derived from the integral operator approximation analysis above.  This paper is well-organized and well-written. The novelty, clarity, and originality of this paper are quite great. This paper provides a new tool to understand why the basic Gradient Descent methods work very well on Deep Learing models and showed new convergence rates on the new assumptions of the underlying function and feature vector generation, which might be fit to situations in the real world.