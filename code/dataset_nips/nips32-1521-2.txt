This paper considers gradient descent as an operator on functions, and thereby obtains convergence guarantees in terms of the eigenvalues of that operator. A similar perspective, in a somewhat simpler setting, was studied in Vempala--Wilmes (2017). Convergence guarantees for empirical risk minimization where also obtained in the overparameterized setting in Arora et. al. (2019), among others.  The authors observe that some previous convergence guarantees give rates that tend to zero as the number of examples increases. To avoid this problem, the authors introduce the assumption that the target function is well approximated by its projection to a small number of eigenspaces of the gradient operator. For example, the function could be a harmonic polynomial over the sphere with respect to the uniform distribution. In this case, the authors obtain linear convergence rate as long as the number of hidden units is at least quadratic in the number of examples.  Overall, this is a pleasant paper, mathematically. However, its improvements over the existing literature are not dramatic.  --- I thank the authors for their very careful response. I continue to believe this paper is worth accepting to NeurIPS and have slightly improved my score.