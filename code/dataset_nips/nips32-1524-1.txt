The approach is an reasonable approach that allows a user to interactively "drill down" to find a desired image.  The evaluation is fairly strong, including both simulated queries based on Visual Genome annotations and a user study that asks users to iteratively construct queries to focus in on a desired image, and compares to reasonable baselines representing the state of the art on dialog-based image retrieval  The approach is new, but relatively simple and straightforward, using multiple user query encodings and a learned similarity that allows it to retrieve images that match all of these queries  The language encoding uses uni-directional GRU.  Why not bidirectional?  Why not use a more recent transformer-based language encoding such as BERT?  These have been shown to produce better language embeddings for most NL problems.  The user experiments give users an image and then have them iteratively search for it in the corpus.   Does this accurately model the real world problem where people are searching for an image that they are not actually looking at while they are constructing queries?  It is a reasonable experimental methodology that allows effectively quantitative evaluation but I wonder if it truly models a realistic scenario.