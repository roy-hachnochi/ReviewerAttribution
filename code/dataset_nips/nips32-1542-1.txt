The core value of the submission is in the idea and formulation. This is perhaps somewhat subjective, and so hard it is hard to describe the supporting evidence, but it is an especially elegant description of the network design problem. The unification of core parts of the sparse neural network literature combined with the neural architecture search problem is very nice, and will mean that the paper of interest to a very large cross-section of the NeurIPS community.  The setup of the comparisons is limited in some ways, but smaller choices were clearly made well. For instance, in the negative, only a few sample networks are presented. However, the authors clearly did make some (admittedly small) effort to present multiple points along the cost/accuracy tradeoff curve for their and comparison methods. The comparison methods include a nice cross-section of near-state-of-the-art networks, though the baseline method the submission builds on for its own method is only MobileNetV1. Overall, the completeness and quality of the experiments are in the range where they are more than sufficient for validating the new and interesting idea that is in the submission, but would be only slightly insufficient for "SOTA" results on a more incremental contribution.  One of the most difficult complications that prevent the kind of strategy in the submission from being effective is the handling of stride/resolution/scale between different feature maps. The submission acknowledges around line 135 that this is "not obvious." The description of how this is solved is very cursory, though all of the experiments do mention partial details, including for example Table 4 in the supplement listing describing the striding in comparison and result architectures. However, I am still uncertain on some details. How exactly are different strides included among the possible edges? It is simply the cartesian product of some set of integer strides and the possible set of edges varying on other parameters? Or some subset?  The best qualities of the submission are overall a little on the "soft" side: in that it is focused on presenting a formulation/problem/idea and, generally speaking, has relatively little experiments or formal theory. For instance, I suspect there is more justification and substance to some very key design decisions in the the edge update rule in line 5 of Algorithm 1 than is described by the authors, but I do believe the choice is correct even though the reason why is not fully described in the submission. By contrast, though, there maybe need to be more description of how the edge swapping described in section 2 relates to the particular formulation of that step in the proposed method. However, despite this "softness" the idea is very good, and the experiments are at least sufficient for a proof of concept. I'll therefore assert that the submission would be interesting and valuable to NeurIPS proceedings readers if accepted.  == Updates after rebuttal/discussion ==  The authors gave some further explanation of the lower-level details of their method, and these look reasonable. The planned code release will very thoroughly address these and is likely the best approach: since the paper introduces very new methods and concepts it most likely would otherwise be difficult to fully specify in a short conference-paper format. The submission to the MicroNet challenge, that is going to be held with NeurIPS, is also likely to help other researchers in understanding the particulars of the work. My overall score is unchanged, but I am somewhat more confident in recommending acceptance. 