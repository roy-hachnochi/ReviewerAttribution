The problem presented in the paper is very interesting and challenging, as it considers the case where no observable data is present, and the approach proposed is in some sense very reasonable, generating artificial data points. The theoretical explanation for this approach follows a dependence assumption and simple approximations and de-compositions of the probability p_x.   The weakest part in this paper is the experimental part. Although the framework is new and lacks of fair benchmarks it seems that  several details and experiments are lacking in order to understand the benefits of using this approach and its limitations:  1. Why the size of z was chosen to be 10? How does it affect the results? 2. Why z is assumed to be low dimensional? 3. How your method scales to more complex and challenging datasets?  