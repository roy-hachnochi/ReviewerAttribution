To my knowledge, this is original research and the method is new. The authors make crystal clear comparison with previous work, and related papers are adequately cited (maybe except at the end of first paragraph, where only a review paper about variational approximations of the Gibbs posterior is cited, missing many papers that derive learning algorithms directly from minimizing PAC-Bayesian bounds, e.g. in Germain et al. (JMLR 2015), Roy et al. (AISTATS 2016)).  All claims in the paper are thoroughly discussed demonstrated (in appendix). I did not check the detail of all the proofs in appendices, but as far as I checked I found no mistakes.  The paper is very well written and organized. I appreciated the fact that the authors take the time (and space) to motivate the results, even by providing a so-called "pre-theorem" that is less general than their main one, but helps the reader understand the significance of the new bound.  Finally, I have no doubt that the result is significant: the new PAC-Bayesian bound provided in this paper tackles a limitation of many PAC-Bayesian bounds from the luterature, contributing in opening the way to applying this theory to a broader family of problems. 