Regarding the first contribution: using visual features for Chinese characters (which are visually inspired) is a long-standing idea, but as the authors pointed out, few previous works were able to achieve a significantly better performance than purely embedding driven approaches. In this regard, the paper does a good revisit to the problem and provides an interesting hypothesis.  Regarding the second contribution: a dedicated CNN structure with diverse training corpus (historical scripts) and multi-task learning scenario is novel and seems to be effective, as demonstrated through several benchmark datasets. It is especially encouraging to see that this gives non-trivial additional improvement over BERT, a very strong baseline.  Regarding the lack of analysis: this is a rather disappointing because this kind of paper would benefit from a qualtitative evidence that backs how the visual features help the downstream task-specific model to better understand the language.  Appropriateness: I am also worried about the appropriateness of the paper to NeurIPS. The paper seems to be more appropriate for NLP conferences than NeurIPS. I am not sure if NeurIPS has a large-enough audience for this kind of work.  After rebuttal: the rebuttal was helpful but more detailed analysis on why/how a visual model helps to learn better character embedding would be desired. Increased my score from 5 to 6.