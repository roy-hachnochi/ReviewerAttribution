Post rebuttal: Thank you very much for your response it has addressed my questions and happy to increase my score.  The paper proposes a new model for VQA which computes bi-directional attention over the image + answers as well as over question words + answers. Related ideas have mainly been explored in reading comprehension (e.g. BiDAF [1]) but also in VQA [2,3,4]. The paper discusses the model in the context of a new dataset called VCR, for visual commonsense reasoning. The paper is well-structured, although there is room for further improvement of writing and clarity, especially of the model section. Good visualizations and ablations studies are provided. My main concern however is the fact that results are only provided for VCR, without experimenting on more common VQA tasks such as VQA1/2. Since the key aspect of the model is the bidirectional attention between the question/image and the words in the answer sentences in VCR, it is not clear to me whether the model will work well compared to existing approaches for standard VQA tasks where the answers are usually 1-2 words.  Model - The description of the Vision-to-answer heterogeneous graph module (section 3.2.1) was a bit hard for me to follow - I didn’t fully understand the structure of that module from the description, especially the part about the guidance mechanism and the word-level attention. It might help to give a more intuitive / higher-level explanation of the goal of each computation. - I like the high-level idea of the model as discussed in the introduction, of computing graphs both over the image and the question to find more semantic relations between the objects, as illustrated in figure 2 (e.g. person - (pours) -> bottle). However, I don’t fully understand where the visual or question modules infer these graphs in practice? If I understand correctly, it seems that the adjacency matrix used (end of page 4) simply captures the similarity in vector space between each of the objects/question words and each of the answers, rather than computing relations between the different objects themselves. Is this covered by a different module? The visualizations in page 8 and supplementary indeed show only attention between objects and answers, or question words and answers, but don’t indicate spatial/semantic relations in between the objects / question words themselves. - I’m wondering about the necessity of having an independent contextual voted module (CVM) that is meant to capture global information about the image rather than specific objects. Wouldn’t it be possible to add additional “context” node/s to the same graph instead of having two separated modules? The “objects” in the graph don’t necessarily have to be physical concrete objects, but rather, any visual “entity” in the image (any visual features). Merging both modules into one graph would make the model in my opinion more unified.  - I also don’t understand some of the explanation about CVM. The paper says its goal is “to exploit long-range visual context for better global reasoning”. What does long-range visual context mean? The paper doesn’t explain that at any point - a concrete example may help clarify that point.  Experiments - The paper provides experiments only over a new dataset for visual commonsense reasoning called VCR. It would be good if experiments could be provided also for more standard datasets, such as VQA1/2, or the newer VQA-CP, to allow better comparison to many existing approaches for visual question answering. - I’m also wondering whether or not the model could perform well over standard VQA tasks - it seems that it mainly builds on the fact that the answers are sentences and can relate in different ways to each of the objects or question words. It therefore would be very important to see how the model performs on more standard VQA tasks where the answers are usually much shorter - about 1-2 words. - Most VQA models so far tend to use word embedding + LSTM to represent language - Could you please perform an ablation experiment using such approach? While it’s very likely that BERT helps improve performance, and indeed the baselines for VCR use BERT, I believe it is important to also have results based on the more currently common approach (LSTM) to allow comparison to most existing VQA approaches.  Clarity - (Page 1): In the first paragraph, the paper discusses vision-and-language tasks, and then provides a couple of examples: including (1) bottom-up attention and BERT. (“One type is to explore a powerful end-to-end network. Jacob et al. have introduced a powerful end-to-end network named BERT”). While I understand that BERT is mentioned here because the model later on uses it, I personally think it doesn’t make a lot of sense to give BERT as an example of a vision-and-language model. I think would be better instead to mention it in the Related Work section, briefly discussing BERT’s merits and how the new model uses it. - Some descriptions are a little bit repetitive (same sentences or explanations repeat verbatim in multiple places, e.g. “HGL integrate a contextual voting module / CVM to exploit long-range visual context for better global reasoning” which returns 3 times, and I noticed a couple more such cases). - (Page 4) It seems that the title “Task Definition” is probably not really describing the content of that subsection - maybe “graph construction” would be a better fit? - (Page 4) On line 131 it says that the “correlations” between the vectors are computed. I believe this means e.g. a dot product between them? It would probably be clearer to call it that way, or alternatively give a more precise definition of how the correlation is computed. - (Supplementary page 2) there are 5 duplicated sentences for each of the five examples - would be better to replace them with one paragraph that says something along the lines of “In the following, we provide multiple visualizations (figures 1-5) to demonstrate the effectiveness and interpretation of…” rather than repeating that same sentence five times. - Increasing the font size inside the figures in the paper and supplementary would be really helpful. - A few specific typos I noticed: *Page 2: boottle -> bottle *Page 2: “which consists a” -> consists of a  *Page 3:  “In multi-top reasoning” -> multi-hop *Page 3: “The CVM is to exploit” - missing word, maybe cvm’s goal is to  *Page 3: “and applied into” -> and been applied to *Page 4: “we divide guidance mechanism” -> the guidance *Page 5: “dot production” -> dot product *Page 5: “use a simple dot production to merging” -> to merge *Page 5: “to produce the for final” -> missing word after the *Page 6: “local visual local object feature” -> mabe “local visual feature” *Page 6: “cars” -> cards *Page 7: “we show the weighted connect of” -> probably connections? *Page 8: the HGL integrate -> integrates  [1] Seo, Minjoon, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. "Bidirectional attention flow for machine comprehension." arXiv preprint arXiv:1611.01603 (2016). [2] Lu, Jiasen, Jianwei Yang, Dhruv Batra, and Devi Parikh. "Hierarchical question-image co-attention for visual question answering." In Advances In Neural Information Processing Systems, pp. 289-297. 2016. [3] Hudson, Drew A., and Christopher D. Manning. "Compositional attention networks for machine reasoning." International Conference on Learning Representations, 2018 [4] Nam, Hyeonseob, Jung-Woo Ha, and Jeonghee Kim. "Dual attention networks for multimodal reasoning and matching." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 299-307. 2017.