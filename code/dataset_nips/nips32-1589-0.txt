Originality - this research is similar to prototype-based learning of neural networks, but it is the first to propose learning and detecting generic components that characterize object using three different types of reasoning (positive, negative and indefinite). Clarity - the paper is hard to read and follow. There are large chunks of text with no figures or equations to illustrate the concepts. In the supplementary material they provide a lot more information which was left out of the main paper. It does feel like the paper is not self-sufficient, as many important steps are only brushed over, such as the training procedure and how to generate the interpretations. Also, there are concepts such as "Siamese networks" which despite being known by most of the community deserve a citation or explanation for other readers. Same thing with the being more careful with definitions of variables (A and I are not defined). Also, the method used to generate the adversarial examples is not well explained. Quality: In terms of quality, my main issue was with the fact that the results do not seem to match the expectation set in the introduction that the model would be able to decompose objects into generic parts that operate as structural primitives (components). From the results it was not very clear to me what is the meaning of "component" and how to interpret the components that were extracted. In the case of digits it was easy to map each component to a digit and interpret the heatmaps as providing evidence for why an image would be classified or not as a digit. Even when the number of components is reduced to 9, we still interpret the components as digits but with one them missing, which forces the model to consider a digit as combination of other ones. However in the general case of images, the examples of visualization (figure 5) show "components" that look very similar to the training examples (or like "prototypes") and not like generic parts. The authors do not comment on this, which left me confused. Significance - a neural network architecture that can be interpreted through probabilistic reasoning (at least for the final layers) is likely to be built upon for further research, given the current interest in interpretable models.