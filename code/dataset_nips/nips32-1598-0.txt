------------------------------------------------ Comments after reading rebuttal:  I've read the rebuttal and appreciate that the authors created a new experiment demonstrating their technique, and which addresses my concern about there only being one demonstration of what is obviously a highly general method. Still, if you can do some version of the Mayan hieroglyphics, or work that example into the introduction, it would improve the paper even more. My score has been raised from 6 to 7.  ------------------------------------------------  The paper proposes jointly learning a "perception model" (a neural network), which outputs discrete symbols, along with a "logic model", whose job it is to explain those discrete symbols. They restrict themselves to classification problems, i.e., a mapping from perceptual input to {0,1}; the discrete symbols output by the perception model act as latent variables sitting in between the input and the binary decision. Their approach is to alternate between (1) inferring a logic program consistent with the training examples, conditioned on the output of the perception model, and (2) training the perception model to predict the latent discrete symbols. Because the perception model may be unreliable, particularly early on in training, the logic program is allowed to revise or abduce the outputs of perception.  The problem they pose -- integrating learned perception with learned symbolic reasoning -- is eminently important. The approach is novel and intuitive, while still having enough technical ingenuity to be nonobvious. The papers is mostly well-written (although see at the end for a list of recommended revisions). However, they demonstrate their approach on only one very small and artificial feeling problem, when the framework is obviously so generically applicable (for example, could you teach the system to count how many ducks vs geese are in a picture? How about detecting whether a pair of images contain the same object? Or, decoding a sound wave into phonemes and then a word; or playing tic-tac-toe from pixels; or inferring a program from a hand-drawn diagram, or a motor program from a hand-drawn character?). Despite the small-scale nature of the experimental domain, the experiments themselves are excellently chosen, for example they investigate a transfer-learning regime where either the perceptual model for the logical knowledge model is transferred between two variations of their task, and they compare against strong baselines. For these reasons, I weakly recommend that this paper be accepted.  Although this paper gets many things right, the one problem is the experiment domain, which feels simple and "toy", and should be ideally supplemented with either another simple domain or a more "real" problem. The supplement does an above-and-beyond job of motivating your domain using Mayan hieroglyphics - definitely put this in the main paper. Can your system solve a version of those Mayan hieroglyphics?  Figure 6 suggests that the learned logical theories could be human interpretable. This is great. Can you show some examples of the learned logical rules?  Questions for the authors that were unclear in the paper (these should be addressed in a revision):  How much of the logical theory can you learn? I browsed through your supplement, and it seems to show that you give a lot, but not all, of the symbolic concepts needed. The paper (lines 200-206) actually seem to imply that even more prior knowledge is given, but the supplement (and the source code to the extent that I browsed it) actually show that what you learned was more impressive (for example, you learned logical definitions of xor, while the text says you give it "a definition of bitwise operations", which I had incorrectly assumed included AND/OR/etc.). What if you were to replace the logical theory learner with something more like metagol - would you get an even more powerful symbolic learner by piggybacking on a system like that?  Why do you need the MLP on top of the relational features? Why can't you just use the logical theory and p() at the final iteration?  How is equation (3) not equivalent to maximizing the number of examples covered by $B\cup\Delta_C\cup p$ ?  Line 148 makes it sound like not satisfying equation 1 means that Con=0, but if I understand it correctly you can both not satisfy equation 1 *and* have Con>0.  In equation 5, you have a hard constraint on |\delta|. Why not have a soft constraint, and instead maximize $Con - \lambda |\delta|$, where $\lambda$ would be a hyper parameter? This would get rid of the $M$ hyperparameter. Also, what do you set $M$ to?  Can you say something about RACOS? I and many readers will not be familiar with it. Just one qualitative sentence would suffice.  Why is RBA so much harder than DBA? Is it only because of the perceptual front-end, i.e. they have the exact same logical structure but different images?  Small comments: Figure 7 would benefit from titles on the graphs I wouldn't call your new approach "abductive learning", because "abductive reasoning" is already a widespread term and is easily confused with what you have named your approach. Something like Neurosymbolic Abduction seems both more accurate and has less of a namespace collision with existing terms.