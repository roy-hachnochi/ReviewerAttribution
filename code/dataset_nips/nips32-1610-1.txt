This paper focuses on incremental EM methods for large datasets. The authors discuss the relationships between some previous incremental EM methods and propose a new incremental EM algorithm, the fiEM algorithm. The authors analyze the non-asymptotic global convergence rates of these algorithms, and compare them empirically.  Originality:  1. The new algorithm fiEM is an interesting extension of the original algorithm iEM [1]. It is a good extension since it is not complicated but efficient.  Quality:  1. The global convergence non-asymptotic rate analysis for the incremental EM algorithms is interesting. It showed us that the sEM-VR algorithm [3] and the new algorithm fiEM are faster than the previous algorithm iEM [1] (in expectation), which is an interesting result. It would be more interesting if the authors can provide a more detailed comparison between the sEM-VR algorithm and the fiEM algorithm since they all require O(n^{2/3}/\epsilon) number of iterations (in expectation).  2. Most of the assumptions that the authors make for the proof are reasonable, except for that I am not sure if H1 is satisfied in many settings. The sets S and Z may not be compact in many settings.  3. Empirically, the fiEM algorithm and the sEM-VR algorithm work well compared to other methods. However, on one of the real datasets in Section 4.2, the fiEM algorithm was outperformed by the previous algorithm sEM-VR.  Clarity:  1. The notation of this paper is clear, but not very easy to follow since the authors used some notations before definition. It would be better if the authors can adjust the text for easier understanding.  Significance:  1. The global convergence rate analysis is interesting and useful, although the experiment results may need to be improved. Also, it will be better if the authors can work on more models and datasets on the experiments.  References:  [1] Neal, Radford M., and Geoffrey E. Hinton. "A view of the EM algorithm that justifies incremental, sparse, and other variants." Learning in graphical models. Springer, Dordrecht, 1998. 355-368.  [2] Mairal, Julien. "Incremental majorization-minimization optimization with application to large-scale machine learning." SIAM Journal on Optimization 25.2 (2015): 829-855.  [3] Chen, Jianfei, et al. "Stochastic Expectation Maximization with variance reduction." Advances in Neural Information Processing Systems. 2018. 