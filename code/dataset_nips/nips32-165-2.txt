Definitions 1-2 and Lemmas 1-4 seem clear.   The paper studies upper bounds on the number of linear regions of the functions represented by ReLU neural networks over a box of the input space, given certain conditions on the gradients of the neurons and the biases. The idea is to draw a distinction between the theoretically possible number and the number that might be observed in practice.    The main result discusses how under certain conditions on parameters and gradients, the number of linear regions of the function represented by the network is usually small.   Although the paper interprets the result as `depth independent', it is presented in a way that includes constants that may well be influenced by depth.   The theorem could be interpreted as saying that there are regions of parameter space where the number of regions of the represented functions is small. This is not surprising, as claimed in the title of the paper. Nonetheless, the result is interesting in that it gives some details on the regions of the parameter space where this happens, even if this description includes somewhat less explicit conditions on gradients of neurons.   The paper also seeks to explain how the requirements of the theorem might hold true at initialization and during training. It presents some experiments evaluating the linear regions observed along one dimensional paths on input space.  