This paper discussed how to enhance the existing methods in which designed prior could over regularize the posteriori, so it will try to find a way to learn a complex prior which can learn  the latent pattern of data manifold more efficiently. To learn such prior, paper adopted and modified one dual optimization technique and introduced an efficient algorithm on how to update the hierarchical prior and posteriori parameters. The combination of complex priori with the introduced algorithm have learned a posterior which has more informative latent representation and avoids posteriori collapse. In addition, paper introduced a graph search method to interpolate the states and showed how effective algorithm can discover a meaningful posteriori over the experiment section. So we can summarize the contribution of this paper as following  - Introduce a hierarchical prior which can avoid over regularization of the posterior while learning latent variables manifold  - Adopting and expanding an optimization technique and an algorithm to learn hierarchical prior and  hierarchical posterior parameters. Authors used importance weight techniques to model the prior and hierarchical posterior  - Defined an interpolation techniques in latent state and demonstrated its success using different experiments  In experiments section, authors demonstrated application of their method using synthetic and real data and showed the proposed method outperformed competing algorithms. Appendix contains very useful information about how changing pieces of algorithm or prior will change the outcome  and is convincing why such setup has been picked.   Quality  Motivation, claims and and supporting material in main paper and supplementary material are explained well and I could not find any significant technical issues with the details of claims made in this paper. The quality of experimental results is very good and many possible variation has been experimented and choice of the authors have been justified. Just one point is to show whether equation 9 which is objective function of our optimization is less that marginal log-likelihood of data under certain circumferences for \lambda and \kappa.   Clarity: I think the paper objectives and explanation are pretty clear and flow of material is very smooth.   Originality: As mentioned in summary the main contribution of this paper could be summarized as bellow  - Introduce a hierarchical prior which can avoid over regularization of the posterior while learning latent variables manifold  - Adopting and expanding an optimization technique and an algorithm to learn hierarchical prior and  hierarchical posterior parameters. Authors used importance weight techniques to model the prior and hierarchical posterior  - Defined an interpolation techniques in latent state and demonstrated its success using different experiments  The paper has solid original contribution. Authors have done a detailed literature review and most of related works have been mentioned and contribution of this paper have been compared and highlighted clearly.  My only suggestion is that authors can take a look at the paper Molchanov, Dmitry, et al. "Doubly semi-implicit variational inference." arXiv preprint arXiv:1810.02789 (2018). which use semi-implicit hierarchical prior and hierarchical posteriori distribution which makes it very similar to this paper. I would belive that optimization of parameters is more efficient than doubly semi-implicit due to how the inner-outer loop has been handled in this paper.  Significance:  The method is new and original.  I think the experiments section has answered many question that I had while I was reading paper and extend of experiments are higher than average NIPS submission. Just as mentioned in originality section, I would like to refer the authors to Molchanov, Dmitry, et al. "Doubly semi-implicit variational inference." arXiv preprint arXiv:1810.02789 (2018) which they potentially can compare their performance to the method purposed in that paper.