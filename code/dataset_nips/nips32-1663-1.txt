This paper considers the problem where we want to identify an \eps-best-arm in the fixed-confidence multi-armed bandits setting. Unlike the traditional setting, it assumes each arm pull incurs a non-unit cost which equals to the expected future reward of that arm. This setup has potential applications in ad placement: a retailer may want to do some experiments before deciding under which search queries the ad is shown.  If a placement incurs a click, the retailer may need to pay for this event. The cost per placement paid to the advertiser is related to the click rate. So the retailer may want to use the minimum cost to identify an \eps-best arm. To achieve this goal, this paper introduces an elimination-based algorithm which is similar to that in [14], but it saves a log(K) factor compared to the uniform sampling or the algorithm straightforwardly derived from [14].  The high-level idea of the proposed algorithm is that instead of removing half arms in each round, it removes arms such that the total weight is decreased by a constant factor with high probability. Because of this the estimation scheme as well as the analysis becomes more involved. Theoretically, this setting is very interesting and the proofs look solid.  The paper is well-written.  However, I have some concerns about the practical usage of the algorithm. 0) About the motivation that the cost of arm pull is equal to the expected future reward of that arm, I am not sure this is actually the case used in ad placement.  I would like to see some concrete real world examples. 1) The authors did not show explicitly the constants in the analysis, which may be very large since the algorithm needs to invoke Median-of-Means procedure repeatedly.  While I understand that this is for the convenience of the analysis, large constants may be unaccepted in real world applications despite the fact that the algorithm saves a log(K) factor. 2) I think the fixed-budget setting makes more sense for this problem, since a retailer usually offers a limited budget to do some survey. 3) It would be better if the authors can add some experiments to convince the readers the superior performance of the proposed algorithm. 4) The assumption that "pull all arms in finite time" for manipulating an unlimited number of arms is not practical.  Minors: -- It seems that \theta was not used in the proof of Theorem 3.1. However, \theta was specified as a parameter in the main algorithm.  Is \theta = \phi_{M}?  Please clarify. -- Line 17, “... observes their instantaneous reward”: reward -> rewards -- Line 21. “... arm pull ...”: usage of “arm-pull” should be consistent; there are some other places that this issue exists -- Line 54, “Related work” should be put together with line 55  -- Line 73, “Let K be total the number ...” -> “Let K be the total number ...” -- Line 157, “... in more detail in as Theorem 5.6” -> “ … in more details in Theorem 5.6” -- Line 178, “... is also used in for the median-elimination algorithm ...”: “in” is redundant -- Line 219: "\eps-good arm” -> \eps-best arm -- Appendix A:  no “2” factor in the first equality and the first inequality Lemma B.1: p should be at most ½ otherwise the last sentence “... more than 1 - p fraction of the empirical means satisfy \omega - \hat{\omega}^j \leq \sqrt{a \sigma^2 / \ell}, hence the p-of-means estimate also satisfies this inequality” does not hold  ------------------------------- [After response] The authors have tried to address the motivation issue.  However, I still feel that this problem is a bit artificial, and it is still not clear what real applications this problem will have.  Other than this, the technical details of this paper look interesting and the paper is a pleasure to read.