I've read the author response and increased my score to a 7 - I vote for acceptance, conditional on the authors including coverage accuracy curves in the final version as they agreed to in the response, and coverage-accuracy numbers (something like table 2 in https://arxiv.org/pdf/1901.09192.pdf would suffice) in the supplementary. I think these are important mainly because past work on selective classification use these, and it would be very helpful for future researchers to compare these numbers.  The author response addresses my concerns. It would be good to add intuition about why TCP does better than BCE as a conjecture (future work can check this). The new comparisons against baselines, and TCP on the validation set, were helpful. It would be good to add a note that there could be better ways of using the validation set that maintain accuracy and do even better on your metrics.  Suggestions for improvement: - Run on more datasets. BCE is the obvious baseline (train a classifier to predict correct vs wrong examples). The supplementary shows that TCP does better than BCE on 3 datasets, and about 0-2% better. This isn't statistically significant even at p = 0.1. It would be nice if the authors had a chance to include at least one more dataset (even if TCP turns out to be worse than BCE there, that would be good to know), and mention these results in the main paper. - To make the paper even more compelling, I would advocate seeing if this idea still helps when train/test are different domains e.g. MNIST -> SVHN or for out of domain detection.  ----------------------------------  Novelty: Medium. Their goal is to identify incorrectly classified examples. The naive baseline is to directly predict whether an example is correctly or incorrectly classified. This is similar to their BCE loss, and has been done before (e.g. Blatz et al 2004, Devries and Taylor 2018). Their specific novelty is to predict the softmax probability of the true label. Also they apply it to in-domain selective classification. The novelty is satisfactory, but not particularly high.  Clarity: High. The paper is very clearly written.   Quality: Medium.  - In practice, neural networks are often trained until they achieve 0 training error. In that case, max-confidence and true-confidence on the training set are identical, because the model gets all the examples correct. Did you stop training your models before they reached 0 training error? My concern is that the method may be less applicable for modern neural networks where we typically train until they get 0 error. At the very least, this caveat should be discussed in the paper, since the method seems sensitive to the training procedure.  - It seems like you train the model and the true confidence predictor on the same training set? What if you train the true confidence predictor on the validation set? Could this improve the results, and make it less sensitive to training procedure? My intuition is that this would be better - at least for the naive baseline/BCE where we try and classify an example as being predicted correctly or incorrectly.  - What were the test set accuracies of your models? It could potentially be easier to predict incorrect examples for a model that’s less accurate, so it would be good to see these.  - The improvement of the proposed method over BCE seems fairly small 1% - 2.5% improvement in AUPR, and BCE seems to be the obvious baseline. While BCE isn’t exactly what DeVries and Taylor did, it’s fairly close.   - I think the experiments were fairly extensive. One could always ask for more experiments, for example the method could be compared to DeVries and Taylor, or other selective prediction methods like (Geifman and El-Yaniv, 2017). However, it seems satisfactory to me. It would be nice to see coverage/accuracy curves/scores though (El-Yaniv and Wiener, 2010).  - I could not find the GitHub link to the code, even though the reproducibility checklist says it was provided. The appendix says 'all models are trained in a standard way' - more details are needed for reproducibility.  Significance: Medium. The problem of identifying incorrectly classified examples, also known as selective prediction, is important. They show that a simple method can work well. The improvements are modest, but the method is a lot simpler than the alternatives so future work may build off of it.  References: Confidence Estimation for Machine translation. John Blatz et al. COLING 2004. Learning Confidence for Out-of-Distribution Detection in Neural Networks. DeVries and Taylor. Arxiv 2018.  Selective Classification for Deep Neural Networks. Geifman and El-Yaniv. NIPS 2017. On the Foundations of Noise-free Selective Classification. El-Yaniv and Wiener. JMLR 2010.