Update after author rebuttal: The authors have addressed concerns over use of held-out set for confidence estimator training and it would definitely be useful if this discussion is added to paper. The authors mention challenges in adopting such an approach with small dataset tasks. However I would recommend a focus on large scale datasets where even preliminary experiments by the author show relatively more interesting observations. 2. Over calibration issues in the confidence estimation branch the authors have reported better calibration performance compared to MCP. However as these details are not clear from the rebuttal, I cannot comment further. I would encourage the readers to not limit their discussion on calibration to a comparison with MCP and provide a discussion on calibration issues observed if any.  ------------------------------------------------ In this paper the authors propose a confidence estimation network which is trained for a classifier and shares parameters with the classifier network.   The authors are interested in confidence estimation when the model is used matched conditions. Hence they train even the confidence estimator using the same training data.  However models typically perform very well on the training data, even when compared to matched test data (i.e., test data drawn from the same true distribution). Hence it is not clear if choosing a held-out set for training the confidence estimator might result in better confidence estimation on test data.  It would be very interesting to see if even the confidence estimation network suffers from the same calibration issues as the primary classification neural network. Related analysis would be very informative.  Further analyzing the behavior of these confidence estimation models as the test data deviates from the training data would be useful.  It is not clear why the confidence estimator needs to share the parameters with the primary classification network. Is it for reducing the computational complexity ?  