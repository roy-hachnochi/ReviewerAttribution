The paper provided some interesting understanding, but is not significant enough to explain interesting issues in deep learning.  The paper showed that lazy training can be caused by parameter scaling, not special to overparameterization of neural networks. What does this tell us about the overparameterized neural networks? Does this result imply that lazy regime of overparameterized neural networks is necessarily due to parameter scaling? If not, lazy regime of overparameterized neural networks cannot be explained simply by parameter scaling. I would like to understand the logic of the paper here. What exactly does the paper want to convey?  The paper provided experiments to demonstrate that lazy training does not necessarily yield good performance. This is a good observation. However, beyond this, does this tell us anything about overparameterized neural networks? I feel this does not imply that lazy training regime that overparameterized neural networks enter does not provide good performance. I found it is more meaningful to compare the lazy training of overparameterized neural networks and lazying training by large scaling parameter of underparameterized neural networks. I wonder if any of the experiments in the paper can imply any property of such a comparison.   ---------- After authors' response  The authors answered my questions satisfactorily. I appreciate their efforts into running extra experiments to provide further understanding. Thus, I improve my score to 6.