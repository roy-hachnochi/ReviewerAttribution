-- In Equation 2, it encourages the distribution of styles of all domains to be as close as possible to a prior distribution. However, I am a little confused about how it disentangle different domain styles (E_s_{x_i}).  In other words, is there any possibility that E_s_{x_i} and E_s_{x_j}, where i is different from j, are very close? I will appreciate it if the authors can provide more explanations here.  -- domain label encoder Ed     I suspect the necessity of introducing a domain label encoder here. I think the information extracted from E_{d} can be handled by style encoder E_{s}. I hope the authors can illustrate this in the rebuttal.  -- Figure 2 (d) only utilize one generator for multi-modal data, but in previous works for multi-modal translation, which is illustrated in Figure 2 (b), different modal data needs different generators. Why in multi-mapping translation only one generator is enough, as shown in subfigure (d)?  -- In Table 2, why DMIT w/o D-Path achieves the best LPIPS score on all the cases (while DMIT outperforms other settings in FID)?