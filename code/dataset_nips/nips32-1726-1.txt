The paper proposes to view MT as a sequence labeling problem modeled with a linear chain CRF, noting that such an approach becomes feasible if the length of the target output is fixed or predicted (as is common in non-autoregressive MT). The authors use a more or less standard transformer encoder-decoder architecture, but the decoder is non-autogressive and simply consumes a fixed number of padding tokens, and the log probability of the sequence is modeled with a CRF, which makes use of the transformer outputs at each time-step.  Experimentally, the authors show that they can outperform many recent non-autoregressive MT baselines, while attaining comparable speedups.  Originality: as noted above, this does appear to be an interesting and rather original idea, at least for neural MT. I think the main promise of this approach is in exact decoding, though the authors do not investigate this much.  Quality and Clarity: Though the paper is easy to follow, I think the presentation could be improved in several respects: - I think it's a little strange to refer to the proposed method as non-autoregressive; it is autoregressive, though it uses only the previous label/token as its history. - Equations (2) and (3) should be corrected so there is no p(z|x), since z (which is the input of padding tokens) is not random and is not modeled. Similarly, if p(T'|x) is random (which it doesn't appear to be) the left-hand-sides should be changed to p(y, T' |x).  - I think the discussion of the proposed method's runtime on lines 178-181 needs to be longer and perhaps formalized a bit more: in particular, the authors should justify why they view their proposed approach as being O(n k^2), and more pressingly, what they view the complexity to be of decoding under the models with which they compare. For instance, what do they view as the decoding complexity of an RNN-based decoder (perhaps with no attention)?  Experimentally, the authors compare with a large number of recent baselines, which is very impressive. However, I believe some of the baselines could be improved. In particular, the Transformer baseline appears to use a beam size of 4, which will slow it down. It would be good to see its performance with a beam of size 1. Even better, training a Transformer model on the beam-searched outputs of a teacher Transformer model (i.e., with knowledge distilliation) can often lead to improved performance even with greedy decoding; note that this is the most fair comparison, since the non-autoregressive models are also trained from a teacher Transformer model. Furthermore, the authors do not include timing information for the RNN decoder, which should also be linear in the length of the output sequence. (Attention to the source complicates this a bit, though there are recent models (e.g., Press and Smith (2018)) that get good performance without it).   Update: Thanks for your response. I'm increasing my score to a 7 in light of the response, especially given the distilled greedy Transformer results.