The results seem to improve over ProtoNet which is basically the only baseline used in the paper despite mentioning lots of data augmentation papers in related work. However, I couldn't quickly equally comparable works using ImageNet data to do meta-learning on CUB (although I am not working in this field).  Reproducibility looks okay barring the fact that BigGAN is a conditional GAN and requires class label. Strictly speaking it is not mandatory if the goal is to find BN parameters anyway (which depend on class label in BigGAN), but I imagine the initialization matters. I'd appreciate clarifications about this aspect of the implementation.  The writing is fairly clear, although section 4 contains quite verbose notation.  The idea is not particularly original, many tried using GANs for data augmentation. But this implementation does work, even though the improvement is not ground-breaking.