************ Quality ************  The technical details in the paper are sound to me. There's some space to improve for the experiments.  I think the main contribution of this paper is proposing a method to transform the complicated neural network structure to a nonlinear feature mapping function, so that they can linearly separate the weight and feature mapping. Given the feature mapping, kernels/correlations and posterior distributions over output functions can be explicitly built for BNN (or DNN).   Therefore, I would expect to see  1. What does this feature mapping look like? I think the authors show the kernel instead of the mapping itself. I think this kernel is NTK kernel, right? It looks very much like the correlation matrix calculated from the output of each data example. What about comparing to other kernels (as mentioned in the paper), or kernels in standard GPs? What's the influence of increasing or decreasing the number of parameters (dimensionality of the feature map)? If you just plot the correlation matrix of the output layer, do you recover similar "kernel matrices"? Without showing some comparisons, it's a bit vague to see the contribution of the connection from DNN to GP.  2. Showing the GP posterior mean only seems to be an overuse of GP since what is more interesting from GP compared with deterministic DNN is the uncertainty.   3. What's the quantitative performance (e.g. accuracy), compared with DNN, BNN, NTK, or GPs with other kernel? Better or worse, why? Figure 2 seems to be a quite obvious thing. GP with an NTK kernel but finite dimensional feature map is worse than an infinite feature map (infinitely wide NN). It's good to have it there, but more inspections for different kernels and comparisons with other BNN-GP type of works might helpful to reveal the strength or weakness of the method.   I like sec. 5.3. This shows one advantage of casting DNN in GP, which helps tuning hyper parameters in a Bayesian way.  ************ Clarity ************  The paper is pretty clear and well written. They did a good job establishing the connection from DNN to GP in math.   ************ Originality ************  There are a few emerging works recently showing the connection between DNN and GP when the structure is infinitely wide. They get rid of the constraint and make the connection for finite hidden units, in which case the feature map in a GP has finite dimensions. They derive GPs from approximations of Hessians, which I think is pretty original.  ************ Significance ************  I appreciate the attempt to understand DNN with GP posteriors and kernels. Theoretically, this helps people understand DNN. The significance would be better evaluated if the authors could add more experimental results (as mentioned in the quality part). My current evaluation is better than medium, but below high.   ################### Thanks for the response from the authors which has addressed most of my questions. I think this is an interesting paper establishing the theoretical connection. What I feel missing is the experimental result to show why establishing such a connection is beneficial. Why people try to formulate BNN into GP? Tuning the hyper parameters is a good reason but the authors took a small sip of it. What about posterior inference, accuracy performance on large real datasets? The authors spent a large portion of the experiment section showing the GP kernels, which is a bit less interesting. Therefore, I would still keep my original decision.              