    Summary    The manuscript discusses a loose relationship between GPs and nonlinear parametric models. Using a spherical Gaussian prior (i.e. l2 regularizer) on the parametric weights and a Hessian approximation in a parametric model: inference in the parametric model can be interpreted as GP inference with a degenerate GP.        Originality    The interpretation seems somewhat novel, but not very surprising, though. Making a local Gaussian approximation (during Laplace approximation) can certainly be interpreted as a Gaussian process in some sense.        Significance    The interplay between parametric models such as DNNs (and their limits) and GPs in terms of architecture and algorithms is interesting. The proposed methodology is -- however -- just a starting point and provides only little value.     Quality    The manuscript gives some insights but the empirical part can be strengthened. In particular, the direction of the optimization experiment in Section 5.3 is very interesting but unfortunately very short. A non-trivial demonstration or a discussion of limits, pitfalls etc. needs to be added. I'm not sure of the message that should be conveyed by Figures 3 and 4. For the paper as a whole, it does not seem that the core results are specific to DNNs but should hold for any parametric model. Also the spherical Gaussian prior seems not to be crucial. Shouldn't other smooth priors work as well?        Clarity    The paper is reasonably well written with some typos and room for improvement in terms of language.        Details    - Intro, paragraph 1: real-world problem is an important problem    - Section 3, last paragraph: formulation is    - Section 4, line 143: estimate Gaussian    - Discussion, line 258: indicate this to certain degree    - References: Capitalization (gaussian, bayesian, neural information ...)     ########################### The new experiments show that the marginal likelihood can guide parameter optimization in a non-synthetic dataset and also beyond the "artificially introduced" parameter $delta$ i.e. the width of a network. However, only follow up work will have to show whether the proposed connecton between BNNs and GPs is useful beyond small datasets and and for other parameters.