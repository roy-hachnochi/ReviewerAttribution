Update: Thank you for the feedback. While it cleared some of my concerns, the most important one regarding the GGN approximation is not fully addressed: see my original comments, (ii) and (iii). Like the authors said, the fact that the derived models have data-dependent likelihood is helpful for understanding the behavior of approximate inference methods, but it's not so helpful in justifying their use in posterior inference. Therefore I would keep my original decision.  Minor comment: it's not clear what you meant by ``the NTK appears for Laplace, but for VI the kernel is different''. (12) clearly looks like an NTK, evaluated at a location different from the optima.  ========  This paper uses neural tangent kernel (NTK) to study BNN posterior approximations. To connect NTK to BNN posteriors, the authors consider simple Gaussian posterior approximations (Laplace and mean field VI) and further employs the GGN approximation to Hessian.  This simplified setup is interesting, and should be useful for future research. However, its limitations should not be overlooked, and I wish the authors could be clearer about them.  The first issue is about the justification of GGN. The authors claim (L102) that it is a good approximation because DNN fits data well. But  (i) for classification problems, having a good fit does not imply the residual vanishes, e.g. when the likelihood p(y|f(x)) is parameterized by sigmoid or softmax; (ii) the neglected term is the product of residual and Hessian (\nabla^2_{ww} f); most modern network architectures use ReLU or similar activations, which leads to very large (in theory, infinity for ReLU) Hessian, so a small residual does not necessarily lead to small approximation errors; (iii) in the analysis of VI (Eq (11)), it is not even clear if models in the high probability region in the variational distribution all have a good fit.  Therefore, the authors should be clear at the beginning that they are considering the GGN approximation, and either provide empirical evidence that GGN is a good approximation to the original Hessian (especially on classification problems and for ReLU-like activations), or show that posterior approximations with GGN still have good performance.  Secondly, all derived GP models have data-dependent likelihood models (\epsilon \sim N(0, \Lambda^{-1})). The only exception is in least-square regression problems, in which \Lambda is independent of data and model. This is not similar to standard Bayesian modeling practice, and makes the results a lot less appealing when the original BNN problem has non-Gaussian likelihood. While it is not deal-breaking, I believe it is necessary that the authors acknowledge this limitation, or provide justifications that the derived GP models are still suitable in statistical modeling.  Apart from the aforementioned issues, the paper is well written and the derivations appear correct. 