Overall, the paper is solving an important problem, and provides compelling convergence guarantees.  However, my main question is exactly why the constrained RL setting is more challenging (from a theoretical perspective) compared to constrained optimization. In particular, the authorsâ€™ approach seems to be based heavily on [34], but they do not clearly distinguish why [34] cannot be directly applied to solving a CMDP problem. I would be much more convinced about the novelty of this paper if such a discussion is provided.  Furthermore, the proposed algorithm does not appear to guarantee safety during the training phase. Assuming I am understanding correctly, the authors should make it clear that their goal is to identify a safe policy rather than ensure safety during learning.