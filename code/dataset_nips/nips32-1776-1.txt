The paper proposes a numerically stable and back propagation compatible eigendecomposition for deep neural networks. It addresses the instability issue in analytic derivatives, as well as the convergence issue for power iterations. The authors give the theoretical justification behind their approach, while showing the numerical evidence behind the choice of parameters. The algorithm is robust when applied in ZCA and PCA denoising, which marginally improved the performance deep networks on CIFAR_10/100 and ResNet18/50. The paper is sound, though I am not familiar enough to comment on the originality. The algorithm should be useful in practice for handling the corner cases.  Major comments:  1. The author mentioned a few times in the paper that the method is intended for large matrices, yet most matrices in the experiments have relatively small size. I am curious if the failure cases become more or less common, when the matrices become large enough.  Minor comments:  1. The improvement in the experiments seem rather small. Most of the time, the convergence behavior looks very similar whether using the existing methods or the proposed one.  Post author feedback:  Thank the authors for addressing my questions. I think the problem this paper is trying to tackle is important. Unfortunately, I am still not convinced whether using a power series approximation to the derivative when the eigendecomposition is not differentiable is the best approach in this case, and whether it leads to meaningful improvement in applications. As I am remain mostly neutral on this paper, I decide to leave my score unchanged.