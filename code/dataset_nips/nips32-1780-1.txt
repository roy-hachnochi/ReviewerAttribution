Authors propose a non auto-regressive parallel text 2 mel-spectrogram model that allows a significant speed up in  text to speech generation. The underlying model is based on feed forward transformer model extended with two auxiliary tasks for predicting length and duration of the underlying phonemes (i.e the input is phoneme, not word-based). To appropriately train the whole system the approach still requires auto regressive teacher model to properly work out phoneme durations. The model does not seem sensitive to spurious generative errors like repetitions or omissions.  For waveform generation another non-autoregressive waveglow vocoder is used (not a contribution).  Overall the study seems fair and reproducible, proposes several solutions for existing shortcomings in  e2e TTS systems, offers large speedup while preserves accuracy of auto-regressive models. It's a good paper.  Would it make a difference if you assumed access to pronunciation dictionary? G2p may introduce some errors along the way (though good it works with it regardless).  You could probably enforce monotonicity in the attention aligner, rather than score them based on which one behaves as you hope.  I would definitely cite [1] as you borrow a number of blocks and ideas from that paper (like 1d convolutions in tts context, etc.)  [1] FFTNET: A REAL-TIME SPEAKER-DEPENDENT NEURAL VOCODER, 2018  Minor:  Several unnecessary repetitions.  ==== Update: Thanks for answering my concerns. Wrt  g2p thing - you should make it explicit in the paper your system requires a pronunciation dictionary.