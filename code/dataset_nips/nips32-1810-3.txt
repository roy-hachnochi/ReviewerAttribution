The authors propose a methodology for training deep neural networks to predict on discrete sets of objects.  This can be useful for a variety of tasks such as matching or sorting.  The authors accomplish this by mapping from a latent representation (z) to a set (Y) by incorporating a representation loss (for representing the set as z) that is iteratively refined through gradient descent.    The paper is well written and easy to follow.  The proposed method seems reasonable and simple to implement.  However, the authors do not compare to any existing literature on deep learning for sets including [20], [23] and Mena et al. (Learning Latent Permutations with Gumbel Sinkhorn Networks).  How does this work improve over those methods?  In particular, Mena et al. learn a mapping from arbitrary feature vectors to a latent representation over sets using deep networks that is then decoded into a permutation.  This would seem like a much more reasonable baseline than the simple feedforward network provided in the experiments.  In Section 5.3, Mena et al. conduct an experiment to map scrambled pixels from MNIST digits into position assignments to reconstruct the digits.  This seems like a very similar experiment to the first one of this paper.  As such, why isn't that work compared to?  Why can't [20] or [23] be applied to this problem as well, to provide stronger baselines?  The authors repeatedly reference (5 times) an unpublished paper [2] (that is presumably from the authors) as well for motivation and comparison.  It seems somewhat dubious to self-cite an un-peer reviewed article provided in the supplementary since it abuses the page limit.  The reviewers need to either assume it is correct and backs up the citations or they need to review another additional paper to verify.  In all the experiments, it seems like at some point running more iterations of the inner loop causes significantly worse results.  Is this due to some kind of overfitting?  Is there some sort of regularization that can be done here to make the algorithm more robust.  Is there some guidance as to how this can be overcome?