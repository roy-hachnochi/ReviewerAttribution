-------------------------------- Post-rebuttal comments -------------------------------- I want to thank the authors for their rebuttal. I am glad that we agree that implying that the presented architecture is intrinsically particularly well-suited to address the challenges presented by continual learning is a stretch, and I appreciate and agree with the clarification that the NSMs are on the other hand a good fit for online learning. I also want to thank them for the useful comparison using the STE on the NSM architecture, demonstrating that their proposed training procedure is indeed more effective.  ------------------------------ Review before rebuttal ------------------------------ - Originality:  The connection between binary stochastic activation and normalization seems novel. - Quality:  The work seems of good quality - Clarity:  The paper is generally well-written and clearly presented. One aspect though that is not motivated at all is the connection between continual learning, which they mention several times, including in the abstract. The authors do not explicitly address continual learning in the main body of the paper. They do not test their architecture on continual learning benchmarks, neither they seriously motivate what type of mechanism in their architecture would be useful for addressing the challenges of continual learning, such as catastrophic forgetting or transfer learning. - Significance:  This is an interesting contribution proposing that binary stochastic activations confer an architecture self-normalization properties akin to weight and batch normalization. However, the statements implying that this mechanism might be inherently useful for continual learning is confusing and calls into question parts of this work and its presentation.