This submission presents a range of interesting connections between deep and kernel learning. I find the presentation however rather unusual. In particular, the factorisation of final layer matrix U into A and E is not "convenient" as the authors put it but critical for their exposition. In addition, the choice of e_i in equation (2) and \tilde{z}_i needs to be properly argued for or at least discussed. Furthermore, the lack of overview that would succinctly put the overall approach for linking deep and kernel learning impedes the flow. A diagram or something to this sort would have been immensely helpful. Given how much notation you are using it would have been very helpful again to have a diagram or summary of some sort to help the reader to absorb it. Why the memory cell, though a vector, is capitalised (reserved for matrices) in your work? "Tilded" and regular versions of variables is an important aspect in your work and it should be properly introduced.   Overall I believe the submission is sufficiently original, lacks in some respects regarding its quality and clarity and is sufficiently significant to a wide audience in deep and kernel learning communities.   After reading the authors response and discussion with other reviewers I am hoping that authors would not only add one diagram but make other changes that would make this very dense submission easier to follow and understand. Therefore I am adding +1 to my previous recommendation. 