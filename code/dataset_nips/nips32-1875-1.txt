Section 3 & 4 feels mostly independent of kernels, it reads as a train-of-thought discussion on how to motivate and build useful RNN architectures. It fails as a clear description of the actual proposed models, as it takes quite a bit of re-reading to detail and understand what the RKM-LSTM is, for example.  For the language modelling experiments, I feel Table 3 should probably acknowledge what is state-of-the-art for WikiText-2 and PTB, as there's an insinuation that the proposed models are SOTA (which they are not). For example, [24] is cited however the numbers in their paper are actually better than what is presented in Table 3. E.g. for [24] PTB is 60.0 valid, 57.3 test; WT2 is 68.6 valid, 65.8 test. But there are more recent variants of the awd-lstm, e.g. "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model" Yang et al. that achieve 63.88 valid and 61.45 test. It is worth noting what state-of-the-art is in results tables, if one is to claim that the proposed model is sota.  For the document classification tasks, the gains are extremely minimal over an LSTM. I would heavily suspect a transformer would obliterate these models at these tasks nowadays, and given the lack of appropriate citation for the language modelling task - I am not 100% sure that there are not better performing works on these tasks.  The paper could mention Quasi-Recurrent Neural Networks, which appear to be very similar to the n-gram LSTM.  I think the general clarity of the paper could be improved, the introduction, Section 2, and the results sections were quite nicely written but S 3, 4, &  5 I found dry and a little arbitrary. As said, it's difficult to even extract what the proposed models are going to be exactly from S4.  Unfortunately I don't find the connections between kernels and recurrent neural networks very enlightening, but I think many people do and this could motivate new work, and in sum would consider accepting this paper if it were re-written.  ===== Thank you for your response, in light of it I have changed my review to an accept.