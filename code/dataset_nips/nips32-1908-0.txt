This paper proposes a human based benchmark system in order to rate GANs. While readers might argue this is too costly, the authors nicely illustrates the costs of their system and motivate the need for using human raters as the true gold standard. While the system is introduced as being general and applicable to any type generative model, the author missed a (very recent) related work from the NLP community. The system HUSE from the paper Unifying Human and Statistical Evaluation for Natural Language Generation, Hashimoto et. al. 2019 utilizes a similar turing test like method to rate the accuracy of a translation model. In addition to this fact, the Hashimoto et. al. claim that, while humans are the gold standard for rating the accuracy, they miss to quantify the diversity capacity of an underlying generative model.