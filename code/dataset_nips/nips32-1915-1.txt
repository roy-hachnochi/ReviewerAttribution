Update: I have read the author's response and am keeping my score  ===========  Originality:  The idea that SGD biases networks towards simple classifiers has been discussed in the community extensively, but lacked a crisp formalization. This paper proposes a formal description for this conjecture (and also extends it -- Conjecture 1 and Claim 2) using a rich and simple information-theoretic framework.  This work is roughly related to recent work on understanding the inductive bias of SGD in function space, e.g. Valle-Perez et al, Savarese et al: both only analyze the solution returned by SGD (under different assumptions), and not the intermediate iterates like this paper does. By defining the 'simplicity' of a neural network solely on the mutual information between its predictions and another classifier leads to objects which are invariant to reparameterization of the network, and only depend on the function that it implements. Such measure is highly novel and might prove to be a good candidate for the study of the complexity of networks in function space. The characterization of the two regimes (linear and 'post-linear') is also novel and interesting.  --------------------  Quality:  The claims are clear and backed up by experimental results. Since the paper aims at supporting the conjecture/claims with experiments, these could be more extensive. For example, plots of \mu_Y(F_t, L), I(F_t, L), I(F_t, Y_S) over time (as in Fig 4) in settings where the conjectured behavior does not manifest itself:   - Bad initialization, such as in Figure 8b in the appendix  - Different optimizer, which can be simulated with SGD + L2 regularization with small negative penalty lambda, as by enforcing that the weights grow any potential implicit L2 regularization of SGD is cancelled  - Non-standard network parameterization  In other words, it would be interesting to have some idea on what exactly is necessary for the conjectured behavior to happen.  --------------------  Clarity:  The paper is very well written, with clear definitions, experiments and discussion.  --------------------  Significance:  The formalization of the conjecture, the clear experimental results to back it up, and the proposed complexity measure \mu_Y(F, L) are all strong contributions, and very timely when considering the recent discussion on the inductive bias of SGD in the function space of neural nets. The proposed framework is very suited for this discussion as it is completely agnostic to how the network is parameterized.     [1] Guillermo Valle-PÃ©rez, Chico Q. Camargo, Ard A. Louis - Deep learning generalizes because the parameter-function map is biased towards simple functions [2] Pedro Savarese, Itay Evron, Daniel Soudry, Nathan Srebro - How do infinite width bounded norm networks look in function space?