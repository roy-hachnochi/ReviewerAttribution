Learning to generate dance according to a given piece of music is an interesting task, and could be benificial to artists in related areas. Both adversarial learning and reconstruction loss are widely used in various generaiton tasks, they are never applied to this new task before this work. Therefore, I recognize the innovation in terms of methodology made by this application work. Evaluation include both quantitative results and qualitative results. From the quantitative results (on automatic metrics and human judgment),  it looks like the improvement over the selected baselines is significant. The authors also provide a video in supplementary material and show how the dance generated visually.    Overall, I think the paper makes decent contributions to AI research and industry, however, I have several concerns (suggestions):  1. The authors hilghlight their innovation on decomposition of dance session to dance units. However, from their descriptions in the supplementary material, they just divide the dance session to small pieces with each 32 frames (2 seconds). Thus my understanding is that the dance unit is independent with kinematic beat or onset strength. Then what's special for the dance unit?  2 Dance generation is not totally new. The following work studies the same problem with deep learning techniques, but is ignored by the authors:  a. Generative Choreography using Deep Learning  b. Dance with Melody: An LSTM-autoencoder Approach to Music oriented Dance Synthesis  I suggest the authors to compare their method with these existing ones.   3. Long sequence generation is a big challenge for DL based models due to exposure bias. It is common that the model will output similar units (e.g., poses in the context of dance generation) after a few steps. Therefore, I doubt about if the proposed method can really generate long sequences, since 20 seconds is not long.   4. Poses in the selected dance styles are relatively simple. Have you tried generation of any pop dances that with complicated poses? 