Quantitative evaluation of mode coverage using experiments on augmented MNIST, similar to Metz et al. (2017, Unrolled Generative Adversarial Networks), better shows the quality of the trained energy function (rather than qualitative evaluations).  Why in the inpainting and salt and pepper experiments, some of the recovered images are completely different from the given images. For example, in the last row, the recovered plane is a different plane.  Moreover, it seems that the recovered images have saturated colors (white background on row three and five, and saturated blue on the last row). Although the code is not provided with the submission, I saw the repository online and played with provided trained model and inference procedure. I think the saturated colors are an important problem of the proposed algorithm. I would like to hear the authors' comments on that.  The previous methods by Xie et al. (2016, A Theory of Generative ConvNet) and Ingraham et al. (2019, Learning Protein Structure with a Differentiable Simulator), which use similar Langevin dynamics for sampling from energy-based models should be addressed in the paper.   Formatting issue/typo:  -Some figures and tables do not have a figure or table number associated with them, while they are referred to in the text with their numbers. -The column's title says "Salt and paper"   === I've read and considered the author feedback and other reviews. 