Originality: While lots of works have studied the property of the endpoint found by SGDs, the literature looking at the SGD training dynamics in the context of deep neural networks is sparser, and the loss contribution metric appears novel to me. The paper is therefore original from that aspect.   Quality: The paper is in general of good quality. However, few specific points could be improved: - It would be nice to characterize the approximation errors introduced by the first order taylor expension - Authors claim that the Loss contribution is grounded while other Fisher information-based metrics heavily depends on the parametrization chosen. Could the authors expend on this point  and provided a more detailed comparison between LC and the metrics introduced in [1] and [13] - In the introduction, authors claim that entire layers drift on the wrong direction during training. However, in the section, this observation only seems to apply to CIFAR/MNIST Resnet. It would be nice to characterize how robust is this observation.  Clarity: The paper is clear and enjoyable to read.  Significance. Looking at the training dynamics is an important topic. The authors propose a new metric to study the dynamics and provide nice empirical observation (learning is noisy, some layers sometime drift in wrong direction, learning is synchronized between layers).  However, it is not clear how significant the loss contribution metric is, my main concerns are: - How does the LC compare with previously define metrics [1,13]. In which case the LC is more informative? - Is LC informative of the quality of the final point found by the optimizers? LC performs the dot products between the update and the batch gradient. It is not clear to me why using the batch gradient is a sensible thing to do. In particular, would networks training using batch-gradient descent using higher LC? We know that in practice, large-batch gradient find solutions that exhibit worse generalization - Would it make sense to use the gradient computed on the validation set to have a better estimate of the expected loss gradient instead of the empirical one?   Update: Thank you for your rebuttal that did  a convincing job about the utility of their metrics wrt to the FIM.   On the other hand, after discussion, I also agree with R2 that further empirical validation is required to ensure the the metric can find neuron that  are "hurting/helping" in training, or more generally why LC is informative of the quality of the final point found by the optimizers.   In light of the second review, I find that the paper is borderline and I decided to keep my score. I do think that the idea explored in this paper is an interesting one and I encourage the authors to continue working in that direction.  