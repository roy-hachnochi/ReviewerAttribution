Efforts to shine light on the black-box optimization process of large over-parameterized networks is an interesting and challenging topic. The authors investigate this problem by analyzing the per-parameter contribution to any loss function reduction. The paper is well-written overall and makes a few new and interesting observations. There are however, some major concerns I have with the paper that I detail next.   - I think that the "help" or "hurt" heuristic is too simplistic and myopic. The authors claim that if the contribution of a parameter at any given iteration is negative, then the parameter is said to have hurt the loss reduction. While this is true in the mathematical sense, I feel that this is too simplistic. A local increase in loss function may lead to an eventual (greater) decrease contribution of the same parameter. By only looking at individual iteration snapshots, the authors have no way of accounting for such an effect. If the authors instead choose, say, an RMS value of the contribution, the interpretation of the term will be less myopic. Along the same lines, it might also be interesting to investigate the nodes that are maximally hurtful/helpful. Problems such as dead ReLUs (stemming from poor initialization) or connection to infrequent features might disable neurons (and hence, paths) and it would be interesting to see if the author's approach could discover these.    - As a thought experiment to verify the above claim, I suggest the following experiment. At any given iteration, take a learning step and compute the sets of helpful, hurtful and indifferent parameters. Then, undo this iteration and take the same step but only for the variables which were helped. If second-order effects are not present and this myopic view is true, then such an experiment should yield similar outcomes to the original training trajectory (or better).   - The approach ignores curvature from the discussion. Rather that using simply the first-order terms, maybe the authors could try using higher-order terms too? This may be expensive in some circumstances but an approximation of this RMS values of the moments of the gradients (similar to what Adam/Adagrad maintain) might be worthwhile too.   - "Freezing the last layer results in significant improvement." is a known phenomenon, see "FIX YOUR CLASSIFIER: THE MARGINAL VALUE OF TRAINING THE LAST WEIGHT LAYER" from ICLR 2018.   - The paper is missing actionable uses from the analysis. While a detailed analysis is enough for publication to NeuRIPS, I feel that this paper is incomplete without some sample uses of the analysis. The authors discuss some possibilities in the last section (using LC for identifying over-fitting, or for architecture search) but stay short of presenting them. I highly encourage the authors to have exemplar uses of their analysis in the paper.     -- UPDATE -- I read the rebuttal and other reviews and I increase my score from 4 to a 5. I feel that, especially in the light of their rebuttal, a score of 4 is unfair to the authors.  I wish to note that the author's rebuttal was very well presented.  However, I am still concerned about the metric. While it has benefits over FIM, I feel that there is inadequate validation of whether this metric highlights what is intended. The authors do not discuss the experiment I suggested in my review to tease this effect. Further, the claim about myopic vs aggregate effects (Rebuttal #5) is not convincing. If the myopic and aggregate views are different, that requires a reconciliation with some of the other claims. 