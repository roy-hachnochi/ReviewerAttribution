Originality:  There have been other works that look into visual bias for things like advertisements as well as other papers that consider political bias in natural language.  The paper does a good job of outlining these works and showing where their model is different.  Understanding bias from images seems unique and interesting.  Quality:  The paper is well cited and put into proper context.  There do not appear to be any technical errors in terms of how the model is presented/trained.  When splitting the dataset into train/test splits, are individual sources placed into either train or test?  E.g., are Breitbart images found in both train and test?  If not, could there be a source specific bias which is learned (and not helpful for understanding political bias)?  Clarity:  The prose is clear and the paper is quite enjoyable to read.  There are some important details missing though.  In particular, I did not understand how the fusion layer was implemented.  I also do not understand what Ours (GT) is in Table 1.  It would also be helpful to understand what kinds of errors the bias model is making; the best model in Table 2 is at 62% (chance would be at 50%).  What kinds of things does the current model not understand?  Results in Table 1 of the supplemental are somewhat helpful, but it would be helpful to know if things like better pose understanding or sentiment analysis would improve results.  I see the data collected as a primary contribution of the paper.  It might also be helpful for a discussion on how this data could be used by others in the community.  Is the intention to have a benchmark task on bias prediction, or are there other aspects of the dataset that would be useful to researchers?  It seems like an interesting set of data, but it would be helpful to have this a bit more explicitly outlined.  It could also be helpful consider datasheets for datasets for this dataset (Gebru et al. Datasheets for Datasets. Arxiv).  It is unclear if the data is biased in such a way that it is not learning about useful/interesting visual bias; in particular, if more right leaning articles discuss gun laws, perhaps the model can learn that any image related to gun laws is right leaning.  This is somewhat shown in Figure 3 of the supplemental where (for example) a picture of a person on a red carpet is considered ``left''.  Finally, it would be interesting to also consider politically neutral images.  It seems that when collecting the dataset these images were just thrown out.  Is this mainly because finding truly neutral images is hard?  Understanding if an image is neutral seems important as well.  Significance:  The significance of this paper comes from the question being asked (can we learn political bias from images?). The collected dataset could be useful for other people interesting in studying bias in images.  Additionally, authors include a variety of annotations types (e.g., explanations from humans about decisions) which could be helpful for different types of analysis.  The experiments are plentiful.  Authors not only consider bias prediction but also image editing to make an image more "left" or "right", accuracy breakdown across different kinds of images, image-text alignment, etc.  It would be interesting to see if any sort of interpretability methods could be used to shed light on the results.  E.g., when making a prediction what is the most important part of the image for the model to consider?  There are no strong claims about their model (is it not novel in comparison to other methods for learning with privileged information?).  The paper could be more significant if the model was run on another similar task with good results.  I lean towards accept because I think the kinds of questions being asked in this paper are important and would like to encourage more work like this at ML conferences.  UPDATE:  After reading other reviews, I increased my score to 7.