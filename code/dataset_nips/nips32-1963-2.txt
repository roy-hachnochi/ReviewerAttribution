The paper  addressed the quotient image based image editing with a single GNN framework. After describing the formulation, the experiment went on with the system with multiple different applications in image editing, including face relighting, swapping, transfiguring, image enhancement, etc. However, the paper are missing many experimental details. The most critical components of the experiments that are missing: (1) Are there any learning-based components in the system? If not, how much handcrafting is needed for different tasks? If yes, how are they trained? (2) What are the roles of each module (FQIA-GNN-L1, L2,L3 etc) in the various experiments? It might be helpful to show the illumination map along with the   results. (3) What are the outputs of each module in each experiment? How are they obtained by these modules in different tasks?  Some other questions/comments: (4) How does this method derive the two relighting results in Figure 2? What are the  experimental settings that generate these two results? (5) There are weaknesses in the experiments. For example, in relighting, it would be beneficial to show relighting of multiple subjects with a single reference to demonstrate consistency. (6) On image enhancement, it would be better to show a quantitative evaluation, similar to the baseline [28]. Â  In conclusion, although the paper is has an interesting and ambitious goal: a unified GNN framework for image editing, the delivery is not convincing. The experiments are weak and many details are missing.  POST REBUTTAL comments: The authors seem to have tried to address most of my comments in the rebuttal regarding the level of details in the experiments and additional quantitative evaluations. The a uthors also included a small user study. I would increase the score from 4 to 6 after the rebuttal, and suggest that the authors include all additional experiments and visualizations in the final manuscript. 