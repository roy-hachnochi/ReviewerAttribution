Overview: - This paper studies the conditions for exact recovery of ground-truth labels in structured prediction under some data generation assumptions. In particular, the analysis generalizes the one in Globerson et al. (2015) from grid graphs to general connected graphs, providing high-probability guarantees for exact label recovery which depend on structural properties of the graph. - On the one hand, extending the results of Globerson et al. (2015) to general graphs seems like a valid contribution. On the other hand, the assumed generative process (lines 89-101, proposed in Globerson et al., 2015) is somewhat toyish which might make the results less interesting. Therefore, I am inclined towards acceptance but not strongly. - I went over most of the proofs in detail and did not find obvious errors (see more minor comments below though).  Comments: - I feel like the presentation can be greatly improved by including an overview of the main result at the beginning of Section 3. In particular, you can state the main result, which is actually given in Remark 2 (!), and then provide some high-level intuition on the path to prove it. This may mean that some derivations will have to be deferred to the appendix, which I think is fine. - Lack of empirical evaluation is another obvious shortcoming of this paper. - Perhaps you can come up with a title that is more specific to this paper (e.g., “Exact Recovery of Labels in Structured Prediction with General Graphs”)?   Additional related work: - Very recent paper (published after NeurIPS submission): Approximate Inference in Structured Instances with Noisy Categorical Observations, Heidari et al., to appear in UAI 2019. Generalizes Globerson et al. (2015) to multiclass variables. I am wondering if you can use their result to generalize your approach from binary to multiclass variables. - Result on tractable models (line 41): Tightness of LP relaxations for almost balanced models, Weller et al., AISTATS (2016). - Result on approximate inference (line 47): Train and test tightness of LP relaxations in structured prediction, Meshi et al., ICML (2016). - Very recent result on label recovery (published after NeurIPS submission): Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation, Jain et al., COLT (2019). The generative process seems very different and the focus is Belief Propagation, so this may be only remotely related.  Other minor comments: - Line 142: Let M be a signed Laplacian **with eigenvector y** as in Definition 5. Otherwise it’s not clear what y is in the statement of Lemma 1. - Line 163: missing superscript before \top - Line 167: did you mean “\beta w_i^+” instead of just “\beta w_i”? - Line 175: I am probably missing something, but why is the probability equal to |(w_i^+)^2 - (w_j^+)^2|? Don’t you also have to require that (w_i^+)^2 \geq (w_j^+)^2? - Line 197: consider adding n to the statement of Thm 2 as it appears in the expression for epsilon. This is actually done in Thm 3, so makes sense here as well. Also, maybe mention in Thm 3 that q is the node noise (as done in Thm 2 for p)? - Line 274: \mathcal{R} is overloaded, used earlier for Reals. - Lines 282-286: what does “bad expansion” mean? 