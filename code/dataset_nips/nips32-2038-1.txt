UPDATE: I've read the other reviews and the rebuttal. I am keeping my score - this is a good paper. ---------------------------------------------------  Originality. The study of Stochastic Gradient Descent in overparametrized setting is a popular and important trend in a recent development of huge-scale optimization for deep-learning. The authors propose a very basic and classical method, consisting from the well-known algorithmical blocks (SGD + Armijo-type line search) together with its first theoretical justification under "interpolation assumption".   The proof of convergence (for example, Theorem 2) mainly consists from the standard arguments (which are used for the proof of the classical non-stochastic Gradient Method under Lipschitz-continuous gradients).  Usually, if we try to repeat this analysis with SGD, the main struggle is how to take the expectation. Under the interpolation assumption, the authors are able to take this expectation easily and therefore propagate non-stochastic GD rates to the stochastic case. Despite the fact, that presented analysis is simple, up to my knowledge, these theoretical results and corresponding assumption are very novel.  The main concern about this would be to make this trick more transparent, probably, to provide some insight about the proof in the main paper. It would make the presentation more attractive, if to declare additionally the limitations of this proof, with a discussion of possible relaxation of the assumption. From now it is also not very clear, how restrictive is it.  In my opinion, more examples when interpolation assumption is satisfied, stated precisely and in the paper would help.   Quality. The submission contains a number of proven theorems with results about convergence of the proposed method.  The paper is self-contained. Numerical experiments are supported with the code. One more concern would be a lack of comparison of the results with that one,  given in works [5] and [77] (which are only briefly mentioned in the paper).    Clarity. The paper is written in a well-mannered way, and it is easy to follow.  It informs the reader adequately about the field and gives a comprehensive reference.  It would be interesting to see more examples and possible generalization of the presented approach.  For clarity of the text, I would also prefer to have the assumption ("interpolation" and "SGC")  stated in a kind of separated form, not between the main text.   Significance. Provided fast convergence rates appears to be the first strong theoretical result about adaptive SGD (to the best of my knowledge), making the results of this work significant. 