Summary:  Within the manuscript, the authors extend the continuous time Bayesian Networks  by incorporating a mixture prior over the conditional intensity matrices, thereby allowing for a larger class compared to a gamma prior usually employed over these. My main concerns are with clarity / quality as the manuscript is quite densely written with quite some material has either been omitted or shifted to the appendix. For a non-expert in continuous time bayesian networks, it is quite hard to read.  Additionally, there are quite a few minor mistakes (see below) that make understanding of the manuscript harder. As it stands,   Originality:  The authors combine variational inference method from Linzner et al [11], with the new prior over the dependency structure (mixture). By replacing sufficient statistics with expected (according to the variational distribution) sufficient statistics  the authors derive a gradient based scheme according to the approximation to the (marginal likelihood).   Quality/Clarity:  As said, my main concern is about clarity and to some degree therefore also quality.  My main confusion arises from section 4 (partly also 3), as the overall scheme is opaque to me. This is mainly due to the fact that part of the derivation is shifted to the appendix. As a result, it is unclear to me, how the expected moments can be computed from \rho_i, q_i. It is said, that this can be done from 7, but there I need \tau_i, how do I get this, this is not explained. Also, the final solution to (9) in (10,11) does not depend on the observations Y anymore, how is this possible?  Some minor things contributing to my confusion: - Line 75:  "a posteriori estimate": This is not a posterior over structures, but a maximum marginal likelihood estimate. - Eq (5), line 114: I was wondering about the 'normalization constant'. First, I think, it should be mentioned, that it is constant wrt to \pi. Second, Z is not necessarily the normalization constant of the true posterior but the approximation to the normalization constant that one would obtain, if the lower bound of line 105 would be used as likelihood, correct?  - Algorithm 1: is only mentioned two pages later and the references to equations don't make sense. Also this algorithm is not explained at all. - Line 127: ref [5] is actually EP not VI - Line 149: the shorthand is used later not there. - Line 161: psi (x,t): I guess this should depend on Y. As stated the overall inference scheme does not depend on the observations Y, that does not make sense.  - line 168: why should constraint ensure that incorporate noisy observations. The whole section is opaque to me. - Figure 1: subfigure labeling is wrong - Experiment british household: the authors report ROC scores, but do not mention the classification problem they are trying to solve, what was the ground truth? Also, it seems odd to me, that childcare is not linked to children.       Significance:  The proposed method does improve the scaling of inferring the dependency structure (reported from 4 nodes to 11). However, other approaches as in were discarded as not being sufficiently accurate or being too data hungry. The quality of the likelihood approximation for example could be evaluated on a small toy-example and compared against sampling based approaches, or [11]. 