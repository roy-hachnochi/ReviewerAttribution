originality, quality, clarity, and significance.  This paper proposes a method which integrates two new losses into an RNN autoencoder learning process for time series data. Specifically it adds a k-means loss to encourage k-means friendly clusters and a classification loss based on discriminating between shuffled input data and the true input data. It is evaluated on a number of standard benchmark datasets and shows an average improvement of 1% compared to the second best approach.  There is a number of benefits to this work.  a) I think the idea of using an RNN autoencoder, jointly trained with more cluster friendly losses, is good. Further, the 'fake data' discrimination process further strengthens this work. b) The evaluation process, while limited to benchmark datasets and (mostly) time-series algorithms, shows an improved performance, and the robustness of the approach. c) The ablation study helps clarify the individual contribution of each element of the proposed method.   However, I think there are also a number of significant improvements that could be made to this work to bring it up to the level required for significance. a) The integration of k-means into an autoencoder loss function has been successfully used before in non time series specific methods. I would be interested in understanding the motivation behind the use of the k-means loss in this setting. How does this hold-up compared to dynamic time warping approaches? b) Some parameters are hardcoded, such as T and lambda at lines 137 and 154. How robust is the method to these? I think this is important as in the unsupervised setting typical supervised methods of choosing these parameters is unavailable. Can these parameters be set based on some other knowledge? c) On line 163 it is mentioned that datasets contain a train test split. What is the significance of saying this? In the unsupervised setting is the entire dataset not used for testing?  d) I think each experiment should be run at least a few times (ideally 5 or 10) and the means and standard deviations reported. With approaches such as the proposed one it is difficult to understand how much the random initialization contributed to the final performance without this. e) The proposed method would also be much more convincing if larger more complex datasets were evaluated.  f) I would find it useful to have a comparison with some other state of the art deep clustering methods (such as DEC[1], IDEC[2]). While not designed for time-series datasets, a comparison with them will improve both the clarity and potentially significance of the proposed method. g) The choice of metric is rather limiting. I would like to see accuracy and NMI also included as an evaluation metric as they are both commonly used in the literature and provide additional insights into the performance.   Some minor suggestions to help improve the clarity of the paper Line 56 and 61 - 'a' should be 'an'. Line 192 - for / bracket placement. Line 185 - 'most best' is not grammatically correct, 'best' is fine.  [1] Xie, Junyuan, Ross Girshick, and Ali Farhadi. "Unsupervised deep embedding for clustering analysis." International conference on machine learning. 2016. [2] Guo, Xifeng, et al. "Improved deep embedded clustering with local structure preservation." IJCAI. 2017. 