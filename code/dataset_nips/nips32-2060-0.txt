This paper adresses the problem of calibration in predictive models in two ways: providing a method for obtaining sample efficient calibration models and showing some properties of estimates for calibration errors known in the literature.   Major issues:  - (Meaning of variance-reduced)  The denotation of your method as variance-reduced does not convince me, as the reduction of the variance refers only to the histogramm binning approach, if I understand it correctly. Which, on the other hand, is due to the fitting of the scaling function in the first step of your method. Thus, scaling methods could also be referred to as variance-reduced methods, couldn't they? Please correct me, if I am wrong.     - (Theorem 4.1)  Here, the statement is more or less: your method is almost as well-callibrated as the best possible recalibrator among the recalibrator set G after a certain number of samples. I understand your message that the calibration error of your technique can be estimated, while this is usually not the case for scaling methods. But wouldn't it nevertheless be better to use the scaling method for the calibration, while your variance-reduced method serves as a surrogate in order to check if the error is smaller than some threshold? Since if the estimated calibration error of the variance-reduced calibrator is with high probability smaller than some threshold, the calibration error of the underlying scaling method is smaller than this threshold as well. This would address the issue of scaling methods you mention in line 126.   - (Proof of Theorem 4.1) Fist, your n in line 575 must be of order 1/epsilon^4 in order to derive the statement in line 576 by means of Lemma D.1. But this would imply a sample size of a higher order than you state in the assertion of the theorem. Did you initially want to prove an epsilon bound instead of an epsilon^2 bound? This would be in accordance with your line 287. Second, you have in Lemma D.3 a high probability statement (also in Lemma D.1), but in your application of Lemma D.3 in lines 578-579 you do not mention this high probability statement anymore. Therefore, your statement of the theorem is a high probability statement as well, which, on the other hand, is missing in the formulation of the theorem.   In summary, the paper suggests an intuitive approach to the problem of calibrating prediction models by binning the output of a fitted scaling function. This approach seems reasonable, and in addition, the authors show that it comes with many benefits such as sample efficiency and the possibility of estimating the calibration error in contrast to other scaling methods. Furthermore, in the experimental studies the superiority of their suggested calibration technique is shown over the popular histogramm binning. However, as mentioned above, I believe that the method is better suited to access the calibration error of the underlying scaling method due to Theorem 4.1. Maybe it is possible to compare the method with a scaling method on some synthetic data scenario, where one has control over the distributions of X (resp. Z) and Y, and where one is able to derive the calibration error of the scaling method in closed form or approximate it appropriately. This could give some insights on the deviation of the method's calibration error to the calibration error of the scaling method. Nevertheless, I think the paper provides some valuable contributions to the considered field of research: First, the sample complexity for estimates of the calibration error and, second, revealing the issue of underestimation of the calibration error for scaling methods.  Minor issues:  - In each mathdisplay the period at the end is missing.  - line 74: You have to use the p-th power of the absolute value of the difference.  - lines 90 and line 98: It should be f instead of M respectively.  - lines 115 and 183: the class G should be specified.  - line 118,119: The transition is somehow abrupt. Maybe write this passage more coherent.  - line 119: It should be I_1,..,I_B instead of I_1,..,I_j.  - line 122: Here, g denotes the histogramm binning itself, doesn't it? Maybe be more specific here on g.  - Histogramm binning: Considering also the last point, I think it would be helpful to give a similar definition of the histogramm binning as in line 190 for the variance-reduced calibrator.  - line 189: "... mean of a set of values S."  - line 195: Here, you switch to a caligraphical G for the class of recalibrators.  - line 214: "In Section 3 we showed ..." is somehow exaggerated: you rather have argued that this is the case in lines 130 - 133.  - Definition 4.2: It should be \alpha >=1. Also you have not defined Z. I guess this is just Z=f(X).  - line 224: It should be Lemma 4.3 instead of Lemma D.4. And in the supplement it should be consequently Lemma 4.3 instead of Lemma D.4.  - line 260: "... measure the calibration error of binned model ...".  - line 265: \hat y_S instead of \hat y_i.  - line 270: you have not defined E^* before.  - Theorem 5.3 and 5.4: Is the upper bound of the 2-well-balanced property not needed?  - line 450: has 'a' density.  - line 479: a period is missing at the end.  - Proof of Lemma D.1: again you have not defined Z (only in the proof of Prop.3.3 in line 466).  - lines 509-510: it should be the difference in the calibration terms.  - Definition D.2: Why do you state the L_2 norm after the squared L_2 norm? This is clear that it is the square root of the squared L_2 norm.  After rebuttal:  The authors' response clarified the most important issues, in particular the way in which the theorem should be understood. Also, the adaptation of the theorem appears to be appropriate. Overall, I would now also opt for accepting the paper.  Minor: The effect of delta on the sample complexity is still a bit unclear. Moreover, I'm still not fully convinced by the name of the method.   