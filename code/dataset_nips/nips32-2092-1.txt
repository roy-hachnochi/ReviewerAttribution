Pros:  • The paper is well written and easy to follow. • The proposed approach uses only a single reference image to produce long range prediction in the future. • The model is object class agnostic, in the sense that it does not uses the class information of the foreground object to construct the key-point sequences. • The method is robust to moving background, as claimed by the authors and supported by few qualitative results. • The visual results are better than the compared approaches. • Key-point learning without any external labels appears to be a novel addition.  Cons/Questions: • Even though the authors claim that the method is robust to moving background, what happens if the background contains motion from similar objects but in different directions? For example, consider a street scene where the background may contain several cars and/or pedestrians moving in all directions. • Is this method applicable for scenes with multiple foreground objects as well? These objects can belong to same or different classes. • In lines 110-115, the method hallucinates the foreground in the future based on the single reference image given. Also, the background information is carried forward from this reference input. How does the model handles situations where the foreground as well as the background is moving (for example, a sports scene such as skating or a moving camera)? In these cases, the background will not be similar to the one in the reference input in case of long-term future. • The use of a small pool of AMT human workers to rank the models is not a reproducible method for future works. Even though this is an unbiased ranking, a quantitative study involving traditional metrics like PSNR/SSIM etc. would help other models to gauge their performance against the proposed one.  What is the rank of ground truth for such human evaluation ? That could have acted as a solid reference/benchmark to compare in relative scale.  • Is the model end-to-end trainable? • Failure cases not discussed  Also, strangely the sample videos in supple doc, have identical aliasing artifacts/degradation uniformly through all the frames. This is not consistent with more degradation as time progresses - which is evident ofcourse in frames of prediction given in paper & supple-doc.  Another noticeable fact is that your prediction is not in sync with the ground truth, as far as the pose of the moving object is concerned. Is this the result of disentangling motion and content ?  There has been lot of work on video frame prediction recently and many of them use the idea of disentangling motion and content. Also, it will be better to compare the model performance using auxiliary metrics such as action recognition accuracy on the produced output.  Also, what about cross-dataset evaluation? In Sec. 3.2, its not clear what you trained your system for Nemo and Mgif data.  Also the use of trivial adversarial loss functions used; This does not add value to CNN or GAN literature. Recent papers on video prediction have used other forms of objective functions. Why not try a few variants.  