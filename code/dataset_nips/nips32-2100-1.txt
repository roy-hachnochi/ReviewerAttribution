The idea of using reference models to guide query-based search is original and quite important in bridging the two lines of approaches to black-box attacks. The algorithm is explained clearly and its performance is evaluated with thorough experiments. The significance of the moving parts of the algorithm is also evaluated with carefully designed experiments.  However, I have one major concern with the current draft: Whenever we propose a method in adversarial ML or any security domain in general, it is of utmost importance to first define the threat model clearly. The threat model typically includes a number of components:  1. What's the attacker's goal? Targeted attack or untargeted attack? How many points does he need to attack (important)? 2. What information/resource does the attacker have? Is he given a reference model for free? Or does the reference model come in the cost of a certain number of queries, e.g. the number of training points used to train it? 3. What can the attacker do, e.g. what's the interaction protocol between the attacker and the victim learner? e.g. White-box or Black-box. 4. What's the cost of the attacker? e.g. Number of queries.  Depend on the choice of the threat model, the evaluation standard should be different. When comparing different algorithms, it is important to compare them under the same threat model, in which all attackers, though equipped with different attack methods, have access to the same resources. In this particular paper, it is not fair to compare the proposed method, which utilizes a valuable resource, the reference model, with methods that don't, e.g. NES and Bandits-TD. A fair comparison should take into consideration of the potential cost of this reference model.   For example, one can consider the cost of the reference model as spending a certain amount of queries as overhead training points for the reference model. Then, depending on the total number of examples the attacker needs to attack (specified in the threat model), this number can be divided between all examples. In the CIFAR10 experiment, the attacker attacks 1000 points, with 2000 training points for the reference model, so the mean and median number of queries should increase by 2 = 2000/1000 to account for this overhead. Then, there will be a trade-off and a choice to be made. When the number of points to be attacked is small, the attacker may not want to spend a large number of queries to train a reference model, whereas when the number of target points is enormous, it is a good strategy to be considered. This is the principled way of doing researches on problems in the adversarial setting, and is highly recommended.