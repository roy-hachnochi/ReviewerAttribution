UPDATE: The authors' rebuttal addressed some of my concerns, while this work still feels incremental, I'm inclined to accept it now.  Strength - A theoretical analysis of the convergence rate for the proposed algorithm. - A nice review of gradient complexity for existing variants of SG-HMC and SGLD.  - Experiments demonstrate improved mixing for the proposed algorithm over competing methods.  Weakness - The proposed algorithm is a relatively standard extension of SG-HMC and SGLD.  - While the proposed framework and analysis and bounds are a contribution, such results are fairly familiar in the literature.  References: The following articles might also be related:  Bardenet, R., Doucet, A., & Holmes, C. (2017). On Markov chain Monte Carlo methods for tall data. The Journal of Machine Learning Research, 18(1), 1515-1557.  Brosse, N., Durmus, A., & Moulines, E. (2018). The promises and pitfalls of stochastic gradient Langevin dynamics. In Advances in Neural Information Processing Systems (pp. 8268-8278).  Dang, K. D., Quiroz, M., Kohn, R., Tran, M. N., & Villani, M. (2018). Hamiltonian Monte Carlo with energy conserving subsampling. arXiv preprint arXiv:1708.00955v2. 