Update: The authors have helpfully pointed out that they do provide some guidelines on setting the hyperparameters.  This paper creatively combines underdamped Langevin MCMC work of Cheng et al. with the gradient estimator SPIDER of Fang et al. This allows the paper to use the theoretical result from Fang et al. to prove a better bound for achieving epsilon in 2-Wasserstein distance. Effectively it is the UL-MCMC algorithm with a better gradient estimator.  This isn't meant to imply that the work is trivial as adapting any insight from the optimisation literature for use in a HMC algorithm requires careful work to yield measurable improvements.  The paper was technically sound with a compelling theoretical analysis and adequate experimental results. The theory already suggests the algorithm works best on a small batch size, but it's unclear if there is any way to do anything other than roughly estimate how that hyperparameter should be set.  The paper is wonderfully organised though also a fairly dense read. This is likely unavoidable given the theoretical contributions. The supplementary section greatly helped in understanding the paper, but the key ideas needed were still in the main paper.  This paper is a great step forward in incorporating SPIDER and it will be nice to continue to see more variance reduction methods be introduced for Langevin methods, where they are sorely needed. 