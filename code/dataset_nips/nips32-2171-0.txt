Most existing neural network quantization methods use STE when performing extremely low-bit quantization tasks such as binary and ternary ones. They assume the full-precision reference and the quantized model have the same loss gradient for easy implementation. This paper proposes MetaQuant, a really novel method to calculate more accurate gradients from the training perspective. The proposed MetaQuant bridges the Gq w.r.t. quantized weights and r of full-precision weights as the inputs to a meta network, which outputs Gr and is trained jointly with the classification network (needs to be quantized) in an end-to-end manner. Three designs of meta quantizer are provided, and validated on DoReFa-Net and BWN using different image classification datasets and settings.  Generally, the paper is very well-written, including the motivation, the theoretical analysis, the proposed design, the practical implementation, and the experiment settings and results.  Here, I have some questions:   (1) In the current design, the meta quantizer is shared across all layers and each weight parameter is processed independently. As weight parameters in kernels/filters are correlated, have the author tried other design harnessing weight relations? By encoding weight correlations via better weights sharing designs for the meta quantizer, improved accuracy may be obtained.   (2)  The meta quantizer will introduce extra memory cost as partially described in the supp material, how about its impact to training time cost (not the number of iterations)? I suggest the author to put this part of experiments to the main paper.  (3) I would like to see a more comprehensive comparison on ImageNet, e.g., including more state-of-the-art results on binary/ternary networks.    ------------------------------------------------------------------------------------------------------ My questions are well addressed by the author responses. I think this paper is a decent submission, thus I retain the score of 7.