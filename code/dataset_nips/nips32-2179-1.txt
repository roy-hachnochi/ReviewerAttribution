2.1. Originality. The proposed method contains a lot of components. Some of them are well known already. In [16] a very similar architecture is proposed. The differences are that this work uses text, while [16] directly requires layout with bboxes and this one requires object crops, while the work in [16] generates objects from scratch. Overall, I think the originality of this method is only okay. Methodological novelty of the presented method, I believe, is marginal  2.2. Quality. The presented method seems to outperform both [16] and [4]. I would consider the numbers with caution, since the presented method uses ground truth image patches to generate images. Therefore, inception scores and diversity scores are much higher, especially the diversity score. Clearly, by sampling different patches you get these numbers higher, compared to works directly generating pixels. In section 4.4 the qualitative benefits of the presented method are discussed, while this simple advantage is omitted. Overall quality of the generated results is still far from realistic. Would it be possible to generate higher resolution samples, like 256x256? I don't think we should stick to 64x64 for comparison purposes.  2.3. The paper is well written, easy to follow. At least one reference/comparison is missing [Hong et al.]. Similarly to the presented work, they also first generate bounding boxes from text. They don't use any patches though.   2.4. Significance. The paper marginally moves the results in text-to-image synthesis forward. The paper proposes no groundbreaking contributions to the reader.  Hong et al. Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis, CVPR'2018.