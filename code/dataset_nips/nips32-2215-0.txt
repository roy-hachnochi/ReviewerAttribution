The paper is very clearly written, the litterature review and introduction are adequate the derivations presented in the main paper are sound and the experiments are insightful and relevant. I have the following comments and remarks:  - The proposed use of a Huber loss is in fact only there to simplify the theoretical analysis compared to using a classical L1 loss. The current presentation tends to suggest that the Huber loss may bring some specific algorithmic advantage, but it is not the case since the L1 norm (which is the limiting case of Huber as \mu goes to 0) always performs better or equivalently to Huber experimentally. This fact could be made more explicit by the authors in the introduction and problem formulation. A natural question is then: what would happen when using L1 minimization without LP rounding? Finally, it would be nice to discuss the relationship between the proposed approach and classical cross-relation methods penalized by an L1 term (e.g., Y Lin, J Chen, Y Kim, DD Lee (Neurips 2008), Benichoux, Vincent Gribonval (2013)).  - Line 120: "the nonconvex relaxation (5).." : the authors probably mean (4). However, while it is clear that solving (3) lead to exact recovery using the formula under line 121, it is less clear that it is also true for solving the convex relaxation (4). This statement should be justified.    - line 157: the definition below is not strictly speaking a partition, for any \xi>0.  - Below line 157: the definition is not valid for ||q_{-1}||_\infty=0, i.e., for basis vectors: the authors should explicitely mention whether or not they are part of the set.  - Line 160: "..they are closer to e_i.." : in what norm?  - The remark on the top of page 6 about the assumption that RQ^{-1} ~ I is not mathematically rigorous and makes one wonder how valid this approximation really is and how it affects all the propositions, which are only stated within this regime. More details would be welcome.  - Proposition 3.4. : I guess the assumptions \epsilon>1/5log(n) is needed for the proposition to hold.  -Footnote (11): the authors state that they use a Riemannian subgradient method similar to (15) for the L1 loss, but it is not clear how this can be done.  -Some implementation details are missing for the experiments, e.g., what values are chosen for \tau, \tau^(k) and \beta.  -The paper lacks a short conclusion outlining perspectives and future research directions.  - Unfortunately, I could not thoroughly check the 33 pages of involved mathematical proof in the supplementary material. I wonder whether it is reasonnable to expect NeurIPS reviewers to thoroughly check such large proofs, given that many reviewers have many other papers to review in a relatively short amount of time (in contrast, reviewing a long journal article may take several months).  Typos: - L99: similar -> similarly - footnote 6: of the standard Huber function - L105: a transpose sign is missing, i.e., C_a^TC_a - L113: amendable for -> amenable to - L117: In contrast, the sphere is ... - L130: by using a random initialization, provably recovers... - L133: only requires the kernel a to be invertible - L150 the problem (5) -> the problem (4) - L197: in stating -> to state - Eq. (14): u should be a q instead - Footnote 9: an equivalent problem to (14) - L214: in general subgradient method -> a general subgradient method - L217: we prove that \Zeta - L218: in the sense that - L219: we use the matrix-vector form of convolutions - L240: producing -> produces - L242: For fix n=500 and p=50 - L248: we repeat the simulation 15 times. - There are additional misuses of articles throughout the paper, please proofread.