Summary of the paper: The paper proposes an inference acceleration technique for Deep nets by having multiple shallow classifiers which feed on the intermediate outputs of a shared backbone neural network which has been trained for the specific task. For example, in Resnet50 the paper has 4 classifiers in total including the original FC layer which are placed at different depths and predict based on the representation till then. If the prediction score if more than the threshold set using a genetic algorithm, then the inference does an early stopping and gives out the prediction. However, if the shallow classifier is not completely sure, then the inference trickles down to the next cascades finally reaching the last classifier. If it reaches the last classifier the method proposed takes the sum of all the cascades into account for the prediction making it an ensemble sharing the backbone. The shallow classifiers are learnt using self-distillation from that point of the network where the shallow classifier is being attached to. The shallow classifiers also contain attention modules to enhance the performance of the shallow classifiers as well as help in understanding the attention maps getting activated. In a nutshell, this method helps in reducing the overall inference cost of an entire dataset based on what classifier each of the images trickles down to in the cascade.  Originality: Each of the components of this work are present in the literature for a good amount of time and as the paper puts it to the best of my knowledge, this is the first work which combined all the moving parts mentioned above and got it to work successfully with good results.  Quality: The paper is of really good quality be it in terms of writing or the experimental results and the real-world impact. I am sure this will have a positive impact on the resource-efficient ML community.  Clarity and writing: The paper was well written easy to follow.  However, I feel the there are some papers which have been missed in the reference. Like: 1) "Model Compression" -  Bucila et al., KDD 06 and "Do Do Deep Nets Really Need to be Deep?" - Ba et al., - While talking about knowledge distillation 2) The ensemble of the intermediate shallow classifier actually exists and is present in this paper "Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things" - Kumar et al., ICML 2017. I think this needs to be cited at the appropriate location of the paper. 3) I am sure there is a huge amount of literature on this, so I would ask the authors to revisit this once more before the next revision.  Positives: 1) The problem setting is very important in the current day and the experimental evaluation which is extensive shows the value of this proposed method. 2) Simple idea and lucid explanation make it easy for the reader.  Issues and questions: 1) In Algorithm 1, the ensemble is taken only if the prediction goes till the final classifier. What if the method does it for every level? Ie., if the datapoint goes till classifier 2 then why can't we add the scores from classifier 1 to make it more robust. And so on. I would like to see these numbers in Table 1 and Table 2 so as to show that the ensemble works in this case as well. This will make a stronger case for the method to be adapted given the much more higher accuracy at each level (this is what I expect, given my past experiences). 2) Why not find the threshold for the ensembles I have suggested above so that the threshold will be more robust ie., even if individually classifier 1 and classifier 2 are not confident and ensemble could be confident about that prediction. 3) I don't understand the (iv) and (v) of line 192-193. It would be good to explain in rebuttal if possible. 4) Figure 4 shows the parameter count for various networks with shallow classifiers. It would be good to just give a number in the text which shows the total memory and compute increment over the original model as this will have some overhead which the reader might want to know. (One can get it from Figure 1, but it would be good to make life easy).  5) It would also be good to report training time overhead after taking the pre-trained model.  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- I have gone through the rebuttal and other reviews. I am convinced with the answers in the rebuttal, however, I would agree with other reviewers regarding some experiments/justification for the design choices like self-attention, etc in the framework (simple ablation studies should help us show the obvious gains (if they are obvious)). You have shared the results on CIFAR with the intermediate ensembling, I would like to see the results for Imagenet as well in the next version as I have a feeling that this will make the 3/4th classifier cross the baseline. Please add all the things mentioned in the rebuttal to the main paper. 