The paper proposed a new family of GNNs that are more powerful than the existing ones (e.g., GCN, GAT, GraphSAGE). The notion of powerfulness is defined as the capability for model instances in a model class in solving combinatorial tasks over graphs. The key idea is to leverage port numbering of the graphs, and to take advantage of node coloring features (which seems necessary in order to go beyond trivial approximation ratios). The authors then analyzed the approximation ratio of the proposed CPNGNNs over several canonical combinatorial tasks.  Overall, the paper is well written and technically sound, and has offered a unified view of several seminal GNN model classes. The work can be viewed as an extension of Xu et al., 2018 [1] (in which the graph isomorphism task was the only focus).  Major concerns: 1. None of the theorems are backed up by empirical results. I believe experiments are crucial to verify the correctness of the theorems, and also to demonstrate the usefulness of CPNGNNs in practice. The authors would probably benefit from having at least one of the following: (1) CPNGNNs lead to better approximation ratio than existing machine learning solvers (e.g. methods in Bello et al., 2016, Vinyals et al., 2015) on synthetic graph combinatorial tasks analyzed in sect 6.2; and (2) CPNGNNs leads to improved/competitive performance on real-world benchmarks as compared to GIN, GCN, GraphSAGE etc. 2. As the authors pointed out, approximation ratios given by Theorem 4 and Theorem 8 are trivial. The improved ratio, i.e., (max_degree + 1) / 2 obtained by leveraging additional graph coloring features, also seems disappointedly loose. While it has been proven in the paper that no GNNs can do better, the paper can be strengthened if the authors could provide more insights/explanations for those unsatisfactory ratios. 3. Definition of "solving" a given combinatorial task seems tricky (L121-122). If my understanding is correct, a GNN model class is considered to be able to solve the task as long as it contains a single model instance that solves the task. However, there doesnâ€™t seem to be a straightforward mechanism to identify the right model instance in the given model class.  Minor questions: * Would k-coloring (k > 2) make any difference in the approximation ratio?  [1] Xu, Keyulu, et al. "How powerful are graph neural networks?." ICLR 2019.