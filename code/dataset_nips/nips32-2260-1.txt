Thank you to the authors for performing these experiments and addressing the concerns raised by the reviewers. I am pleased to see the performance of TRIP in the context of flows as well. I recommend that this paper be accepted.  ==  The authors present TRIP (Tensor Ring Induced Prior), a parametric family of distributions. These distributions are parameterized as a tensor ring decomposition (Zhao et al. 2016) by d "cores," which define a distribution over d discrete variables. A continuous distribution over R^n can be obtained by placing one Gaussian distribution for each value of the discrete variables, which corresponds to a mixture of a very large number of Gaussians (10^100 Gaussians in this paper). The authors then demonstrate the effectiveness of this parameterization as a learnable prior for VAEs and GANs. The authors justify this approach because the inherent multimodality of this parameterization may better suit the multimodal nature of natural images. The authors cite half-present glasses in the case of GANs trained on CelebA as a disadvantage of unimodal priors.  Originality: This work is builds on a wide body of work on learned priors. This approach seems novel as far as I'm aware, although I'm not familiar with the related work on tensor decompositions.   Quality: The authors carefully motivate, define, and experimentally test this approach in a wide variety of settings. One concern I have about the experimental setup is that the authors compare TRIP to a N(0, I) prior and a GMM prior. However these seem like unfair comparisons because TRIP has many more parameters than N(0, I) and GMM. It may be fairer to compare to a decoder with the same number of parameters as a TRIP-based decoder would have. I would also like to have gotten a better sense of how much slower a TRIP prior is to train compared to the standard approaches.  Clarity: I found this paper to be well-written and easy to follow. The logic flows well from section to section. I very much appreciated the visualizations, especially Figure 1, 4, and 5.  Significance: TRIP seems like a practical algorithm that can be used as a prior for VAEs and GANs, or more generally whenever a mixture of a large number of Gaussians is desired. 