The proposed technique (named SSE) applies to neural networks (NN) with embedding of categorical inputs. According to it, some random noise is introduced into the inputs: each categorical input is changed with some (low) probability. This change can be done with (SSE-Graph) or without (SSE-SE) prior knowledge. SSE comes along with a theoretical analysis: training a NN with SSE corresponds to a change of loss. This loss change is theoretically evaluated.  The paper is well written and clear. Just one thing: theta^* should have be defined on equation 7.  Although natural and simple, the main main idea seems to be original. The experimental results in the SSE-SE framework show that the proposed method works consistently. Moreover, replacing dropout by SSE-SE appears to improve the results.   The fact that the author are able to exhibit the optimized loss in their setup and compare it to the "true" loss, allows further development and study of SSE.  Questions: since SSE is close to drop-out, and drop-out can be seen as adding Gaussian noise, is there a similar interpretation for SSE?