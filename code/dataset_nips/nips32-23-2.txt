The paper presents a novel and interesting regularization method, theoretical analysis and good results, yet I fear its main contributions might be limited to recommendation systems or other fields where knowledge graphs are available, easily constructed, or in their absence, intuitively reasonable to assume a complete graph.   Outside those types of tasks, I find it presenting arguments which intuitively were not too compelling, as to why other fields or tasks would significantly benefit from such a method, despite showing improved results on some NLP tasks.  The simpler version of the regularizer, which in the absence of a knowledge graph assumes a complete graph, permutes embedding indices with a constant*U(1,N) probability. Despite its appealing theoretical properties, it also poses a risk of introducing a bias of its own.  The results on NLP tasks didn't show major improvements and lacked in explanation as to why this type of regularizer would be beneficial and effective for different NLP tasks.  As mentioned by the authors, training a language model is an instance of an NLP task for which this regularizer fits well, since this type of task aims to estimate a conditional probability distribution over sequences of words and benefits from word augmentation due to the large vocabulary size. Yet, many tasks don't have these properties and I fear that a task like NER might not benefit from this type of method, in the absence of a knowledge graph. The paper did a fairly good job with conveying the main ideas clearly, yet it contains some minor inaccuracies, mainly in the experiments section.