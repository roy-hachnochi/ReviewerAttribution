Originality: the paper lacks a sound and novel contribution. Theoretically, there is only one minor result as stated above. Technically, there is not a systematical experimental study on real deep networks. The main contribution is on discussing two different formulations of the Fisher matrix. The main trick on making these two formulations different (despite that the authors took a sophisticated approach going though GGN) is that the so called empirical Fisher relies on y_n (target of neural network output), and if one consider y_n to be randomly distributed with fixed variance based on the neural network output, the two formulations are equivalent, otherwise there is a scale parameter in eq.(3) which is shrinking making the two formulations different because of the shrinking and damping. In practice, for applying the exact natural gradient (without approximation) to deep neural networks, there is no reason to use the EF over the Fisher. For example, in the cited "revisiting natural gradient" paper, the Fisher matrix is given in eq.(26), (28), etc. and does not depend on the target y_n, and one has no reason to use the EF formulation.  Quality: the experiments are mainly performed on toy examples. It is not clear how these results can be generalized to real networks. The literature review can be enhanced, the authors used the term "empirical Fisher" which is used by the some machine learning papers (or is that the case the authors coined the term? please make clear on its first appearance) This should be connected to observed Fisher information which is a well-defined concept in statistics. There should be more discussions on information geometry and deep learning, where many literature are skipped.  Clarity: the paper is well written both in English and in math.  Significance: the paper can potentially broaden the audience of natural gradient methods to the deep learning community. However it has limited significance due to the lack of novel contributions. Overall I feel that it is below the bar of standard NeurIPS papers.