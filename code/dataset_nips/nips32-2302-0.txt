Original: the paper is original in the way it uses flow based generative models for 1-to-many image to image translation. The recent paper [21] also uses conditional flow based models, although there are differences.  Quality: The proposed approach is technically sound, and supported by good results in the experimental section. However it would have been nice to add into the comparisons the work of [21]. Was this omitted due to how recent that paper is? Also the way conditions were introduced into the CelebA dataset seems quite artificial: if only one couple of images were selected as prototypes for similing and not smiling, there might be many other differences between them that the model may pick up, and not only that of smiling. Which network was used to compute the FID in the CHC dataset? One last point in this regard, \mu(c)_i is supposed to be 0 when M_i is 0, but this is only so if b_\mu is also 0 (same for \sigma). Are the biases terms not used in practise?  Clarity: the paper is overall well written, and the loss functions are well motivated. Some parts could be made clearer, particularly when going through eq. 1 and 2, there should be a reference to Fig 2c. I would make Fig 2c a new figure on its own, as IMHO it is the diagram that provides the most information about the approach.  Significance: results look clearly better than competitors, although in an unconventional set of experiments. Still it may become a reference for other researchers working in similar problems. It also provides a new instance of that problem (CHC), which may be useful for the community.   Edit after rebuttal: Having read the other reviews, and given that the rebuttal addressed my concerns (except that regarding the use of Inception-V3 to calculate FID for CHC, as CHC images do not look anything like natural images used for pretraining Inception-V3), I agree this paper should be published, and hence I raise my assessment from 6 to 7.