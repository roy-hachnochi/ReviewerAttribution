Flows maximize the likelihood of observed images in a latent space and are formulated invertibly, such that at test time, latent space samples can be decoded into images. In their vanilla formulation, their setup does not allow to to condition on external attributed or images. This submission proposes an adaptation of flows for image-to-image translation problems and as such is an interesting and original work.  The paper is overall well structured. The general idea is somewhat hard to understand until Section 3.3 / 3.4. This is also in part due to the complex nature of Figures 2a) -2c), which don’t aid much in clarifying the idea that underpins the model. It would be beneficial to state the basic idea in simple terms early on in the paper. I would have liked to read something like `The model consists of a U-Net with multiple scales of latent variables between its encoder and its decoder. These latent spaces encode an image c to condition on, which is trained by reconstructing c. Simultaneously a flow based model is trained to associate possible outputs x with the given image c by maximizing the likelihood of the latents z that the flow produces given x under the U-Net’s latent space densities for c’.  Training the encoder-decoder is done using 3 terms, a L2-reconstruction term on the output space, an L1-term to encourage a sparse encoding of z|c by means of a mask that yields Normal and thus uninformative latent distributions and an entropy term on z|c that encourages the `unmasked’ latent distributions’ to have low variance. This setup seems a bit contrived and could potentially be avoided / done in a more principled way by training the encoder-decoder via variational inference? For improved clarity of the training objectives, the loss terms (left-hand sides of Eqs. 4, 5 and 6) should have the parameters with respect to which they are optimized as a subscript. It would further be interesting to have an analysis of how many of the latents end up being masked (surely a function of the weighting terms). Also, having latents at all scales, it appears the encoder has an incentive to go the easiest route and encode as much as possible in the scale with the highest resolution, which thus wouldn’t require finding a more abstract and semantically structured latent space? In this context the means (\mu) of e(z|c) of individual encoder scales could be fixed to investigate what individual scales encode, in case it can be teased apart.  It would generally be helpful to give the dimensions of the latent tensors at all scales, to better understand how the dimensionality d_x is split across scales.  The employed metrics test for the fidelity and the diversity of the generated images only, but do not seem to test for whether they are appropriate / plausible, given the image to condition on. Other datasets allow to do so more easily, e.g. image translation between street scene images on Cityscapes and its segmentation, which was considered by other image-to-image translation works. A way to quantify conditional plausibility on CelebA could be to pretrain a classifier to classifiy the ~40 attributes of each image and use this classifier to quantify whether they are conserved / altered in the generated images. This seemsan important analysis given that the proposed model does not have a deterministic path from c -> x, which means there could potentially be a very weak correlation only. Additional the likelihood of ground truth images x under the encoder could be reported, so e(f(x)|c).  The PU-Net is reported to produce blurry results, but it has not been stated what the exact architectural setup and training procedure for this baseline (and for the VU-Net) was.   There are various typos and errors in grammar such as `merginalizing’, missing white spaces, wrong/missing articles (`a’, `the’), `there is no oracle that [?] how large’, wrong usage of colons in datasets description.  [After Rebuttal] The rebuttal addressed the comments and questions I had raised. My score remains at 7.