The authors extensively cite the prior work that they are extending here and their work in context. The idea to prune nodes on basis of attention scores they get is novel compared to previous work, as is the case for supervising said attention (atleast in context of graph neural networks). The synthetic datasets provide reasonable test cases to test future algorithms in this domain.  I find a few questions that need to be resolved and/or explained in more detail. First, is the attention only used to prune the nodes in the graph or are the representation to the subsequent layers of the model weighted by the representation scores ? If it is the first case, then how is loss function of the final task backproped through the layer i.e if input to next layer is X[X.alpha < threshold], then I don't see a obvious way of passing the gradient through alpha values. If it is the second case, then I would assume the representation passed to subsequent layers are on a reduced scale (and depends on how peaky or uniform the attention is) .  In general, authors make a good job of evaluating their design decisions. Some of the examples are - 1) Generalisation performance - In Colors and Triangles task, they test on larger graphs than those appearing in the training set. On Colors and MNIST data, they test on augmenting the feature space to unseen colors. A question I have here why in colors task, GIN performs better while in triangles task , chebyGIN performs better ? Please also clarify how AUROC is being used to quantify attention correctness. Do we consider attention as providing us with binary decision about node's importance i.e if the node is important or not ? If ranking needs to be evaluated, authors might use direct measure of rank measurement like spearman rho or kendall tau.  2) Evaluating GIN and chebyGIN model on all 3 datasets with unsupervised, weakly supervised and supervised attention. The results here show that supervised attention is required for improving performance and that unsupervised attention doesn't give any reasonable improvement of previous baselines. A question I have here is how is weak supervision generated for the model. The authors mention that weak supervision is generated by removing a node from a graph and see how much model's output change . But which model are we talking about here ? Are the authors training a unsupervised attention model and generate weak supervision on basis of that ? In general, Section 3.4 needs to be expanded to detail how weak supervision is generated since in most cases, as author's mention, we don't have ground truth attention and from Table 1, we see that attention only helps if supervised or weakly supervised.  3) Multiple ablation studies on how attention init, thresholding and input dimensionality affect the model output. In section "How results differ depending on to which layer we apply the attention model?" , I am not sure which results/graphs are being used to back this claim (Please clarify that).  