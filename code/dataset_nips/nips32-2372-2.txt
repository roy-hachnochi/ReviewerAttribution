This paper studies the limitations of attention mechanism in GNNs when conducting graph classification tasks. Through empirical study on graph with ground truth attention, graph with Gaussian noise, and graphs with unseen node features,      Overall, it is an interesting work with empirical analysis, while the technical contribution is limited. It provides insights and interesting discussion on the capability of attention mechanism in GNNs;  for example, main strength of attention over nodes in GNNs is the ability to generalize to more complex or noisy graphs at test time. Also, the factors influencing performance of GNNs with attention are: initialization of the attention model,  strength of the main GNN model, and other hyperparameters of the attention and GNN models. Authors finally suggest that GNNs with supervised training of attention are significantly more accurate and robust, even with weakly-supervised training since ground truth attention is often not available. Their introduced way of supervised or weakly-supervised strategies require important hyperparameters setting, which was not discussed.  Most of the results are reported on datasets which have ground truth attention values. Only Table 2 shows evaluation on other real-world datasets, without comparison to GIN and GCN, which had good performance in Table 1.   