This paper introduces a method for incorporating prior knowledge encoded as logical rules to improve the performance of deep learning models. In particular, it takes logical rules which are in decomposable and deterministic negation normal form (d-DNNF), and proposes using an augmented graph convolution network to embed them into a vector space. This embedding is then regularised according to the logical constraints, allowing the addition of a "logic loss" term to train models obeying these logical rules.  Incorporating (symbolic) background knowledge to improve performance of deep learning methods is an interesting and valuable direction, and from the experiments using a d-DNNF rather than a CNF appears to be beneficial. However, for me the notion of using a d-DNNF as the source of background knowledge raises a few issues which I feel are not addressed in the paper.  In general, building d-DNNFs is a difficult problem - taking an arbitrary logical formula and compiling a d-DNNF from it is implicitly solving the #P-hard problem of doing model counting on the formula (because d-DNNFs are a "tractable form" and can do many difficult computations easily). This problem has been studied extensively ("A Compiler for Deterministic, Decomposable Negation Normal Form, Darwiche 2002", "New Advances in Compiling CNF into Decomposable Negation Normal Form, Darwiche 2004", "A Top-Down Compiler for Sentential Decision Diagrams. Oztok and Darwiche 2015" for a particular class of d-DNNFs), but this difficulty is completely hidden in the paper, and rather taken as a given that we can easily construct a d-DNNF.  In addition to being difficult to compile, having a built d-DNNF allows you to solve difficult problems easily, which brings me to my second concern. The experiments compare the results of using d-DNNFs vs. CNFs as the underlying embedding structure, but do not compare a more direct approach. That is, you could take something like semantic loss ("A Semantic Loss Function for Deep Learning with Symbolic Knowledge, Xu et al 2018"), and use the d-DNNF directly in the loss to enforce constraints without embedding the formula and having the inaccuracies that arise (contrary to the description in the paper, semantic loss requires solving exactly the same problem as this paper - building a d-DNNF, and does not require any sampling or approximation). To me this is the more meaningful comparison, since if the d-DNNF is already compiled there must be a justification for why the logical embedding is needed at all.  The paper is mostly quite clear: the technical content was understandable with the exception that I found it a bit difficult to understand exactly which part of the network these constraints addressed. This was mentioned later, but more explicitly clarifying it early on would be helpful. The setup and result of the experiments were easy to analyze, and I like that the authors used a real task and data set to validate their approach.  I found the discussion to be a little bit confusing - it seems like the claim is that embedding CNFs is harder because any d-DNNF can be written as a similar sized CNF (via tseytin encoding), but not every d-DNNF can be written as a similar sized CNF (due to weighted model counting being PP-hard), and therefore d-DNNFs may be easier to learn? While correct, this formalization does not make it any easier to understand, and more intuition is definitely required. The idea of why embedding d-DNNFs is more effective than CNFs is an interesting point to discuss (even if the discussion is inconclusive), but it could be made more intuitive, and also perhaps mention the inherent structure that is already in d-DNNFs, rather than just the sizes of the search spaces.  Post-rebuttal: The paper as is still has some motivational issues for me regarding the use of d-DNNFs, but I feel confident the authors can produce a good camera ready, which I would expect to include further discussion of the use of other embeddings and the effects on the technique (with some experimental ablations regarding the effect glove has), as well as a clearer motivation throughout the paper of why having a technique like this is important as compared to using d-DNNFs as is. 