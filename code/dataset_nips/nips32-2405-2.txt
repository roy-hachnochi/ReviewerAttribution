This paper proposes a perspective on training Bayesian Neural Networks (BNNs) that motivates how to best incorporate different tricks (such as batch normalization and momentum) into BNN training. The resulting algorithm scales to large inference problems like fitting a BNN to ImageNet and achieves well calibrated predictions. Starting point is an existing approach (VOGN) for fitting BNNs with natural gradients. The authors observe that the update equations of VOGN are similar to the update equations of popular SGD methods with adaptive learning rates. From this perspective, they can derive by analogy how to best incorporate different tricks for practical deep learning (batch normalization, data augmentation, distributed training). The extensive experimental study supports the claims of the authors.  Topic-wise, this work is a good fit to the Neurips community. There seem to be no 'new ideas' in this paper (VOGN comes from ref [22] and batch normalization, data augmentation, etc. come from the deep learning literature), so I would rate it lower on originality. Yet, I find it an important contribution to bridging the gap between Bayesian neural networks and practical deep learning.  The ideas and how they are connected are described clearly. This work is an interesting step into the direction of finding the right trade-off between computational efficiency and well calibrated predictions in Bayesian deep learning. 