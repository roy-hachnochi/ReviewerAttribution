- the text in the intro claims that MLE language models cannot perform one-shot language generation. this statement isn't exactly true; there has been a lot of recent work on non-autoregressive generation from conditional LMs (mostly within machine translation) that could be cited as a counterpoint here (e.g., Gu et al., ICLR 2018). this "contribution" should thus be toned down. - many typos throughout (e.g., "World level perplexity" in table 2, "model ground through" in line 244) - the evaluation results leave me unsatisfied. while ScratchGAN does seem better than other GAN-based alternatives, the perplexity difference between it and the MLE model on wikitext103 is immense. The authors attempt to explain this difference in lines 245-250, but I didn't quite catch the drift of their argument (isn't it bad that ScratchGAN does not favor diversity during training?). However, looking at the generated samples from the MLE model and ScratchGAN in the appendix, it is clear that the huge perplexity difference actually corresponds to noticeable differences in grammaticality and coherence.  - Why are there so few samples provided with the ScratchGAN after training? The supplementary material should have way more samples from each model so we can judge their relative quality, especially since the evaluation metrics used here are (outside of perplexity) hard to judge. - From my perspective, it is a stretch to say that ScratchGAN performs "comparably" to MLE trained models.