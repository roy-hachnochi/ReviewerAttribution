Originality: There's moderate novelty in the methodological contribution mentioned above. There's nice discussion of related work for language GANs but this field moves a bit fast and there are a couple of new papers that are not mentioned.   Quality: Besides the methodological contribution, the paper does a really good job trying to evaluate language GANs with many metrics to measure the diversity and quality of generated sentences. The results look great and I always appreciate a good ablation study, which the authors did, and with such great graphs to visualize the additional contributions for each technique (Fig 3).   Clarity: The paper is quite well written. The experimental details are provided in the supplementary materials which helps with the reproducibility. I wish there'd be a link, anonymous, to the code however. (why not?)  Significance: This paper certainly explores a missing gap of how to train GANs for natural text which is an interesting direction. To me, I'm still not entirely convinced about the superiority of language GANs for text generation over language models. As far as I know, they GANs are still substantially worse than LMs (Tevet et al, 2019).  In additions, can language GANs really scale to large neural nets such as using the Transformer? The balance between the discriminator and the generator power are quite delicate and it is progressively harder to tune once the discriminator and the generators are more powerful. There's hope however (for example, BigGANs for images) but this is still a somewhat uncharted territory for GANs. Can you please address the scalability? I am glad of the improvements made so far to use GANs for language, however. 