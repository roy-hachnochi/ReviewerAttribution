This paper studies the problem of learning useful representations for reinforcement learning through the lens of an adversarial framework. In particular, a good representation is identified as one that yields low linear value-function estimation error if an adversary is able to choose a value function (induced by a policy). The paper shows first that the the only policies that should be considered are deterministic, and then identifies a more narrowed set of adversarial values, though the number is still exponential. I really liked the theoretical insights of this paper, and because of this I tend to vote for acceptance, though I claim that experiments are too preliminary.  Some more comments below:  1- in (1) highlight more clearly that \phi is the only optimization knob. 2- in terms of readability, it is unclear why Lemma 1 is useful until after i read the proof of Theorem 1 from the Appendix. Maybe consider saying why this Lemma is useful, or move things around 3- isn't the first half of Lemma 1 (solution lies in the set of extreme points) a very well-known result in linear programming? If yes, then be more clear that this is not new. 4- this adversarial framework reminds me a lot of the use of Wasserstein distance in model-based RL, whereby a good model is defined as one that yields low error in the context of adversarial choice of value functions (that are Lipschitz). Do you see a synergy here? is there any deep connection? Also, can you clarify why you used model-based algorithms in experiments? There is no mention of model-based stuff until we get to experiments, so i am wondering if there is a connection. 5- for proof of Theorem 2 in appendix, maybe do define idempotent matrices and their properties. I checked the proofs of the first two theorems and otherwise they seem sound and clear. 6- the part that the paper falls short is experiments. It could still be OK if the authors showed a clear path towards extending the idea to function approximation, but this is lacking. Plus, the method cannot really beat the baseline even in the toy domain. Any comment on challenges when going to function approximation?  ---- post rebuttal: I am happy to see that the authors are willing to add a section that more seriously tackles/starts to think about challenges when going to arbitrary function approximators in practice.  As for the point about a potential model-based RL result, Farahmand and friends was indeed the paper that I had in mind. Also, because of the focus on linearity, Parr and friends 2008 on linear models shows a deeper connection/equivalence, and so could be useful. It would be very neat if there was a deeper connection. If one cannot be shown in this paper, can a conjecture still be made?