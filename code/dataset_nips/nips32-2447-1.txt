Post-rebuttal: I have read the rebuttal. I think the rebuttal has sufficiently addressed my questions. With the new experiments for supervised learning and reinforcement learning, I think the paper is much stronger now. So, I will vote for accepting this paper.  ---------  - Originality: I think the proposed method is novel.  - Quality:  1. I find the proposed regularization term a bit messy with 3 components added to the original objective function. However, the paper explains the reasons behind the modifications well. 2. On L206: did you sample only one sample weight once at the beginning of the optimization process? Or did you sample one sample weight at every iteration of the optimization?  From Eq 7, it seems that you took the first approach, which I find very strange that it could work at all since the log-likelihood term would not depend on the parameters of the model in this case.  - Clarity: The paper is mostly clear, except that the authors used too many "so-called" in Sec 3.1, which gives the impression that they don't agree with the names of the methods in literature. Please consider fixing them if it is not intentional.  - Significance: I think the contributions in this paper is reasonably significant.