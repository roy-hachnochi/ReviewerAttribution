This work deals with learning individual intrinsic rewards (IR) for muti-agent RL (MARL). Overall, the method provided is a straightforward application of a known IR method to MARL, the results are promising and the writing is clear. As such, this work has limited novelty but provides good empirical contributions, though these too could be improved by considering more domains. A more detailed review of the paper, along with feedback and clarifications required are provided below.   The work is motivated by the claim that providing individual IRs to different agents in a population (in a MARL setting) will allow diverse behaviours.  * Is it not possible that the IR policies learnt all look similar and the thus the behaviour that emerges is similar? The analysis at the end of the paper shows that a lot of the learned IR curves do overlap. Please provide more justification for this motivation.   The work clearly describes related work and how the approach here differs. The main contribution is to apply the meta-gradient based approach in “On learning intrinsic rewards for policy gradient methods” ([16] as per the paper) to the multi-agent setting.  * This looks to be a straightforward application where each agent has the LIRPG approach applied. Please provide succinct details of any modifications that are required to apply this and any differences in implementation.  The method section can be shortened, as most of the algorithm and objective are the same as the original LIRPG algorithm uses.   A range of methods are compared to the in the experimental section: independent q-learning/actor-critic, central critics, counterfactual critics, QMIX and the proposed approach (LIIR).  * Please clarify what is meant by “share the same policy”: do they share the same policy network weights or also the exact same policy output? Do all agents get the same observation? If so, what is the difference between IAC and central-v? Is the only change how the V is updated, whereas policy is the same? * How is the parameter \lambda tuned for the agent?  Lastly, the result sections show clear benefits of this approach. This method, along with several baseline is applied to a set of mini games for Starcraft. The analysis is promising and show that the method learns an interesting policy that captures the dynamics of the game. Overall this is a good contribution but for an empirical paper this could be strengthened by considering more domains or tasks and demonstrating the ability of this method to work across the board.  