Originality: Algorithms proposed are not completely new, though they are novel combinations of well-known techniques. The theoretical analysis is an important contribution, but it was unclear to me that there were technical breakthroughs instead of mainly building on approaches in prior work.   Update after reading author feedback: - Thanks for running / reporting the experiments that I requested. I was a little disappointed that the scalability was underwhelming -- doubling the number of workers from 4 to 8 only improved running time by ~20%. - After also taking into account the other reviewers' comments, I would still support accepting the paper, but at a slightly lower score. Quality: Presentation of multiple algorithms with proper theoretical analyses is pretty much complete. Empirical evaluations could be more extensive.  Clarity: Overall the writing was clear, with an easy-to-follow presentation of algorithms that build on top of the previous algorithm. I did get a little lost with some of the notations, but that is understandable for a theory-focused paper.  Significance: I rate the overall significance of this paper as high, for demonstrating both theoretically and empirically that asynchrony, quantization, sparsification, and acceleration can be combined together into a single distributed optimization algorithm.   My only complaints are with the empirical evaluations.  1. The greatest issue I have is that there is only 1 evaluation / plot (Figure 3b) on total training time. My understanding is that the motivation for all this work is to reduce the training time to epsilon loss, by reducing communication while not impacting convergence rate. With only 1 plot on total training time for 1 run using 1 dataset on 1 model / problem, I am unable to conclude that the proposed algorithms can robustly accomplish the goal of reducing total training time.  2. I also thought that the experiments did not address the question of scalability. Only one setting for number-of-workers was used. I do expect that the proposed algorithms will scale well; nevertheless, I would have expected this evaluation to be done for a paper on *distributed* optimization.  3. Finally, more extension experiments could have been done -- more runs to get error bars, more datasets, more problems, etc. -- to more convincingly establish the robustness of proposed approaches over competitor algorithms.  