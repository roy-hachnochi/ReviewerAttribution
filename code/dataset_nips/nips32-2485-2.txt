The paper addresses the communication bottleneck in distributed optimization and proposes double quantization for mitigating this. The approach is not particularly novel as many papers have proposed quantization or compression methods. Here are some detailed points:  - The technique is not new and indeed there are important references missing. For example, there should be comparison with the paper "signSGD: Compressed Optimisation for Non-Convex Problems" by Bernstein et al. which considers exactly the same problem.  - The main theoretical issue is that the results are based on an unbiased quantizer and this is indeed crucial for the proofs. The low-precision quantizer is only unbiased  in the domain dom(delta,b). - The motivation behind quantizing from master to workers is less clear. Open-MPI has a broadcast function and the cost is not the same as N unicasts. - The numerical results on 6 servers are quite limited and the achieved gain is not impressive. - To achieve convergence, model parameter quantization should have a vanishing error which means less and less quantization as iterations increase. What happens to total number of bits then for reaching a desired accuracy? The relationship of parameter mu and number of bits is unclear.  To conclude, after the rebuttal I am more convinced about the contributions of this paper. I still find the theoretical novelty of the work incremental; however, the algorithmic novelty and numerical results are quite good, so I increase my score to 6. 