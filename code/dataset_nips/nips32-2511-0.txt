The paper is concerned with link prediction in multi-relational datasets which is an important task in AI and ML and fits well into NeurIPS. For this purpose, the paper extends hyperbolic embeddings, which have shown promise on single-relational graphs to the multi-relational setting. As such, the paper makes also a promising contribution to advance hyperbolic representation learning by expanding its use-cases.  In general, the proposed method makes sense and is a technically relatively straightforward combination of recent advances in representation learning (e.g., hyperbolic, KG embeddings). However, there are some aspects in the paper that are currently not clear to me:  - The introduction seems to argue against hyperbolic embeddings that form the same hierarchy across different relations (which I agree with). However, it seems that the proposed method is doing exactly this by using the same hyperbolic embedding of an entity in all different relations.  - Additionally, it is not immediately clear to me why the model makes the specific choice to use both a linear transformation (R) and a ranslation (r) to model different relations. Similarly, what is the motivation for the modeling choice to transform the subject and object embeddings with different functions (diagonal linear transformation vs translation). Did the authors also try conceptually simpler transformations (e.g., only using a full R) and how did they perform?  - Regarding the evaluation: The results on WN18RR seem indeed promising. However, the results on FB15k-237 seem to indicate that the proposed model struggles more on classic KGs. It would be interesting to see if this holds up on further KG benchmarks like NELL and YAGO.  - It would also be interesting to analyze the the (relative) performance drop of MuRP on FB15k in more detail. While the Krackhardt analysis is informative, it doesn't seem to explain the difference, as directed pairs should also be easy to embed in the Poincare ball. For instance, a possible reason could be that a single embedding hierarchy is likely not a good model for this complex KG (whereas it seems more reasonable for WN18). This analysis could also provide valuable insights into next steps based on the proposed model.  - For WN18, I found the Krackhardt analysis to be a valuable contribution which might also get adopted in further papers on hierarchical embeddings.  - How does MuRP/E perform when using a comparable embedding to ComplEx-N3 d=2000? 