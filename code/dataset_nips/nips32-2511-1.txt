This paper defines a new model for knowledge base completion for both euclidean and hyperbolic geometries. Being an experimental paper, I believe the texts spends too much time on the background, and too little on the experiments. Especially since the performances of this model are not far beyond existing performances. Since the model can be discussed and heuristically justified in euclidean space, I would remove most discussions on poincar√© space and riemannian optimization as well as the specialization of the model in this space to the appendix.  The model defined in euclidean space is s(s, r, o) = -d(R e_s, e_o + r)^2 + b_s + b_o, and is justified as a multi-relational version of s(r, o) = <r, o>. The heuristic justification is that flexibility on the base space is added by replacing the norms with learned biases.  A thorough ablation study for this model is lacking.  Regarding the biases :  -What are the (euclidean or hyperbolic or both) performances of the model without one bias ? - Without both biases ? - What happens if the biases are not learned but are the non-transformed embedding norms ? - What happens if the biases are the transformed embedding norms ?  Regarding the multi-relational transforms : - What happens if we switch matrix transform and translation ? - Why only apply the coordinate rescaling R to one side ?  -What happens without one or the other ?   Results on all of these would help justify all the elements in this model, which otherwise seems completely arbitrary, beyond some heuristics justifications based on observations on word embeddings.  Regarding experiments, authors from [16] made available results as a function of rank that are slightly higher than what is reported in this paper (on par with TuckER). See https://github.com/facebookresearch/kbc for the results.  The low ranks performances for MuRP on WN18RR are interesting, and I understand that the use of hyperbolic space is motivated by this. The analysis in "Representation Tradeoffs for Hyperbolic Embeddings" suggests that dimension is not the right metric to compare euclidean and hyperbolic embeddings, since hyperbolic embeddings will make a more thorough use of their floating point representation. An analysis of MurE vs MurP with varying bits / entity (varying both dimensionality and floating points depth) would be very valuable and more convincing.  An analysis of the performances of MurE / MurP for higher ranks would also be interesting. As it stands in Figure 2, the performance of the model seems to plateau very quickly with the dimensionality, whereas methods such as [16] reach higher performances for higher ranks. Is the model overfitting beyond this point ? The regularization used is not discussed beyond initalization close to zero. Would an explicit penalty on the embedding norm (or the biases) help ? Or is the discrepancy coming from the learning objective ? (cross-entropy vs 50 sampled negatives and bernoulli neg log-likelihood).   Regarding Figure 4, the method for dimensionality reduction seems surprising. The biases are not included in the dimensionality reduction. In this case, why not do a PCA relative to the entity of reference (asia) ? The analysis of this figure seems very anecdotic. If the authors want to justify that current evaluation method under-evaluate the performance difference between MurE and MurP or MurP and other methods, a systematic study should be done, rather than one example.  Clarity : the method is clearly described. Significance: the experimental results are not detailed enough to assess the significance of this method. On one dataset, results are on par with the state of the art. On the second they are below the state of the art. Results at low dimensions are mitigated by the propension of hyperbolic methods to use the bits available more fully. The plateau-ing performances for higher dimensions puts in question all the experimental results in this paper and makes it very difficult to assess their significance. Is the limitation coming from the model ? from the loss ? the regularization ? the optimization ?  ===================== AFTER REBUTTAL ==========================  Thank you for all these new results and precisions, I have no further worries regarding performances for higher dimensions or lower bits of precisions.  I am however surprised by the results in the ablation study. Switching R and r seems to lead to a 2 point decrease in MRR for both models. This seems unintuitive, as the effect should be similar to reversing all relations in the dataset (subjects then become objects). Performance of the algorithm should be unaffected by such a reversal.  As it stands, I am convinced by the results of the algorithm. However, I don't think the important part of the model or training algorithm that leads to the observed gains have been correctly identified. 