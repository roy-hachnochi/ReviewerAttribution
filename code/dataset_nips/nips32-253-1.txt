This paper analyzes the property of high dimensional latent representation of adversarial examples, finding that the attack makes the embedding move closer to the false class so that the adversarial and natural images are almost indistinguishable. The authors implemented the proposed method on several datasets including MNIST, CIFAR-10, Tiny Imagenet and achieved good performance. They also made comparison with different adversarial methods including FGSM, BIM, C&W, PGD and so on.  This paper was in general well written. The authors provided a lot of visualization about their analysis and result. The authors also uploaded their code in the supplementary and the experiments seems sufficient and support strongly for their views.  Some major concerns  are listed  as follows:  1. As a general framework, the authors implemented their idea on only one model structure. This  seems insufficient. More  experiments on a different model structure may make the paper more convincing.  2. Although, the paper provided a lot of experiments and visualization, the authors analyze adversarial examples  only from the experiments, more theoretical analysis may be needed.   