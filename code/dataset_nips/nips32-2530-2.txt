Overall Comments One significant difficulty in a lot of feature importance/significance assessments is the issue of multicollinearity, which this paper notes as proxy features. The literature on disentangling could be potentially useful in this setting. It is interesting that the paper leverages this connection to help address this issue. In my opinion, this is an interesting connection and one that has not been fully explored in the literature.  Originality As far as I am aware, this work is the first to combine work from the disentangling literature with the SHAP method for interpretability. One could contend that disentangling is essentially clustering and that feature selection (which is what SHAP does) on top of clustering is not novel; however, I would still say that this work is.   Quality I consider this paper a comprehensive assessment of the thesis that the paper sets forth, i.e., disentangling helps to better interpret when there is proxy influence. Overall the paper is well written with the exception of section 2.1. I also commend the authors for overall clarity in general. I note some points about section 2.1 later in this review.  Significance. The issue of dealing with proxy features and multicollinearity is a timely problem and particularly potent one in the fairness + ml literature. Here it is important to properly disambiguate the impact/influence of a protected attribute like race etc on the outcome. The goal of the disentangled influence work directly addresses this issue. My biggest issue with this work is with the disentangled representations portion. It is hard to judge the output of these methods and to properly assess whether representations are disentangled or to even know what it means for something to be disentangled. I'll return to this later in my review. In spite of this, I contend that this is still a useful contribution on the part of the authors.   Some gripes - Disentangled representations: I have followed this literature lately, and it seems there is significant doubt whether most of the methods in this area works. In particular, the recent ICML paper: https://arxiv.org/abs/1811.12359.pdf. First, it is not clear what kinds of dataset it is possible to learn a disentangled representations for. Second, even if possible, the field is not clear on what metrics are useful for assessing this disentanglement.  If such disentanglement were possible and verifiable, I agree with the authors that marrying feature selection with disentangled representations is a significant undertaking that should help solve multicollinearity issues. This is my biggest gripe with this line of work; however, since several of the previous papers were accepted at top conferences, I think this work is interesting as a proof of concept of what is possible along these lines.   After the somewhat philosophical rant, this section pertains to some methodological choices the paper makes: - Why SHAP? It seems to me that if one has disentangled representations, and if the representations are 'truly' disentangled, then a pipeline with any reasonable feature selection method should work here? Here is what I mean, right now the authors propose disentangled representations + SHAP, I argue that disentangled representations + {SHAP, LIME, Correlation Coef, etc} should suffice. I do understand that SHAP has been shown to provably subsume LIME and a whole host of other methods.  - Why not compare your encoder + decoder scheme with betaVAE or the some other class of methods that can learn disentangled representations?   - Section 2.1: I read this section a few times, and I understand that it is supposed to provide theoretical justification for your work, however, I don't quite see how it fits with the rest of the paper. Proposition 1 follows pretty straightforwardly from the definition of a disentangled representation, so am not sure why it is needed?   - The synthetic example is (section 3.1) is very good, and indeed demonstrates that disentangled representations + {SHAP} can capture proxy influence from variables 2x y^2 etc.   - Does the paper address disentanglement of p, the sensitive attribute, in general or for all variables. I was confused on this issue. Does the method seek a representation that where p has been 'removed' or does the method seek to factor all 'features' in the input into 'independent' components? If it is just to remove the variable p, then it makes sense to not test against disentangled representation methods more generally.   - The paper's section on error analyses gets to the core of my biggest issue with the disentanglement literature. Do the authors envisions that people will perform this kind of error analyses before using their method?   Overall, I consider this work to be interesting since it bridges two interesting areas of research. The authors also perform pretty comprehensive assessment of the proposed method on real datasets.  Update I have read the author rebuttal, and it indeed clarified a some questions that were brought up as part of the review. I am planing to maintain my accept rating for this work.