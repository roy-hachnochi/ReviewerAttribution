Originality: the paper proposes to address exploration in reinforcement learning using a combination of posterior sampling, successor features, and Bayesian linear regression. To the best of my knowledge the proposed combination is novel. The authors also do a good job contextualizing the proposed method within the related literature.  Quality: the paper is well written and seems to be technically correct. The claims are supported by both theoretical and empirical results. The authors are also upfront about the limitations of the proposed approach (Section 4.4).   Clarity: although the paper is well written, the presentation could perhaps be slightly improved in two points. First, with the exception of Fig. 1, there is a lack of intuitive explanations that may make it difficult for a reader less familiar with the subject to grasp the ideas at first. Second, the narrative behind SU seems a bit entangled with the use of neural networks --e.g., line 19--, although some of the theoretical arguments in favor of it actually rely on a tabular representation. I wonder if it is possible to present the SU concept in isolation and later argue that it has as one of its benefits the fact that it can be easily combined with complex function approximators.   Significance: the paper proposes a method to perform "deep exploration" in RL. The method is simple and has low computational cost --as discussed in line 188, it can be seen as a small modification of previous methods resulting from the structure imposed in (5). As such, it seems to me that it has the potential of being adopted by the community and also serving as an inspiration for future research.  Post-rebuttal update: ------  As stated in my rebuttal, I think this paper is quite strong content-wise. It could benefit from a clearer presentation, though, as indicated in the reviews. I hope the authors put some effort in making the next version of the paper more accessible. 