Aside from some minor grammatical issues, I found this to be an interesting paper introducing new methods that leveraging low-rank structure to embed transition distributions and time-varying states. I think the idea of the diffusion distance can inspire further work in distance metrics for sequential models as well as new learning paradigms.   (i) Where do the authors see their work fitting in relative to existing work on Koopman operators and Dynamic Mode Decomposition.   For example in https://papers.nips.cc/paper/6713-learning-koopman-invariant-subspaces-for-dynamic-mode-decomposition.pdf, the authors use the level sets of the Koopman eigenfunction to find basins of attraction of the dynamics (Figure 5 -- right in the pdf) in a manner similar to what is done here via clustering.   (ii) Have you experimented with clustering the raw time-series (i.e. without the state embedding). What do those results look like and how do they qualitatively differ from what is presented in Figure 2?   (iii) Could you comment on the assumption in line 171 that the clustering admits a unique optimal solution. What properties of the time-series do you think might be necessary for that assumption to be satisfied?   (iv) How did the quality of the state embeddings (as visualized in Figure 2) vary as a function of the number of Fourier features used. Presumably there is some tradeoff between the number of features needed to capture the properties of the dynamics to a sufficient degree and the complexity of the underlying dynamics (for example, the number of basins of attraction and how close they are). Have you experimented with this?  