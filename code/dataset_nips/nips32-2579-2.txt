The author(s) derive a gradient-based variatational inference routine for Wishart and inverse-Wishart processes, which they use to model the dynamic covariance matrices of multi-variate time series. Their motivation is tractability as existing Wishart and inverse-Wishart rely on MCMC methods that require exact posterior inference to be performed on the underlying Gaussian processes. Their approach scales better with time-series length and has a "simpler" implementation. The proposed methods include both variatational inference for Wishart and inverse-Wishart processes as well as their corresponding low-rank factored variants.  Their gradient-based method, as they admit, is seemingly largely based on the model from van der Wilk et al. [18]. Their exposition and derivations of their gradient-based variatational inference for Wishart and inverse-Wishart processes are well done despite requiring the reader to keep up with lots of notation. Because their expected log conditional likelihoods cannot be expressed analytically, they use the "reparameterization trick" of Salimans and Knowles [16] and Kingma and Welling [11].  In the case of the Wishart process, they resolve hypothesized numerical instability resulting from matrix inversion, via the introduction of diagonal-covariance Gaussian noise--this adds to the diagonal of the covariance matrix inversion. They confirm via simulation that this proposal resolves numerical instability for the Wishart process.  Their factored covariance models use a low-rank approximation to reduce computational complexity from O(D^3) to O(K^2D) for K << D. In the inverse Wishart case, they note their model is a reparemeterization of Fox and Dunson [5]'s construction. Their previously introduced white noise addition allows for efficient matrix inversions via the Woodbury matrix identities.  They have three data sets with varied D to which they compare baseline methods to their proposed methods: {Wishart, inverse-Wishart}x{standard, additive noise}x{full, factored} (note factored models required noise). They propose 3 metrics. The metrics being closely related seemingly leads to similar conclusions regrading the relative performances of the algorithms. In the cases of Dow 30 and FX, D is small enough to run all competing baseline methods. There they attain statistically significant results on both tasks with their (inverse-Wishart, additive noise, full). While additive noise was not necessary for numerical instability in the case of the inverse-Wishart, it offers a performance benefit that they hypothesize arises from introducing an additional parameter. For the S&P 500, D was large enough that only the independence-assuming baseline could be used against their factor models. Here, both the baseline method and their proposals have large variance leading their f30-iwp model being best albeit statistically less significant.  In summary, this is a well written paper. I will argue it is lacking on the contribution side as the models are all closely related to prior works. That said, their application is well-motivated, and they do achieve competitive results. Further, they do offer solutions for tractability problems for use on real-world data problems. However, since computational efficiency is one of the prime motivations of this paper, it would have been nice to see convergence plotted according to wall-clock time to complement their asymptotic analysis. Their other stand-out finding was the numerical instability associated with the Wishart process, which they both address and warn others to use with caution.  