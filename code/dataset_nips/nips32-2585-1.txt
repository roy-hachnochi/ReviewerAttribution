This paper provides theoretical results showing that in order to learn disentangled representations grounded in symmetry transformations, as recently defined by Higgins et al (2018), it is necessary to have access to the actions producing observation changes. It them proposes a new model architecture that is able to exploit this information to learn symmetry based disentangled representations (SBDR). Finally, the paper demonstrates that disentangled representations improve sample efficiency for inverse model learning.  The work is a first step building on the recent theoretical paper by Higgins et al (2018) that defines disentangling in terms of symmetry transformations. Hence, this work is an important step that other can build on. The submission is technically sound and provides both theoretical and empirical contributions. However, the empirical contributions are quite limited, since the authors only use one very simple dataset. I would have liked to see the approach evaluated on more challenging datasets too. Saying this, the paper is very well written and I think it makes a significant enough contribution to the field to be published in providing a proof of principle demonstration of how to address a challenging problem (learning SBDR).  ---- Post author feedback ------ Thank you for your detailed feedback. I leave my score unchanged given that it was favourable to start with.