 Originality:  While the paper addresses an interesting problem, I don't understand how the definition of the problem in Section 3 differs from standard Bayesian optimization augmented with an additional input that describes the tasks. Even though in the experiments the authors compare to standard Bayesian optimization, where tasks are chosen uniformly at random, I don't see a reason why the task variable need to be treated differently in this setting, i.e why BO is not allowed to also select the task variable.  I think modelling the task explicitly becomes more relevant if the cost of evaluating the blackbox function is taken into account as described by Swersky et al., such that one aims to select tasks and actions that not only gain the most information but are also cheap-to-evaluate.   Quality:  The paper provides a lower bound on the simple regret of their method and underpin their theoretical results by an empirical comparison to related work.  However, in order to make the empirical results more convincing, the following points should be addressed in the experiment section:  - a comparison of the proposed kernel to the MTBO task kernel proposed by Swerksy et al. which achieved state-of-the-art performance on these kind of benchmarks.  - a comparison to standard Bayesian optimization where tasks are not sampled uniformly at random but instead are selected together with the input variable (i.e by optimizing the acquisition function)   Clarity:  In general the paper is well written and easy to follow. However, a few points should be addressed further:  - using Thompson sampling as acquisition function requires to sample function values conditioned on the previous samples, or to put it differently, the function sample need to be sequentially constructed while being optimized. Are further approximations used (e.g see Lobato et al.) to avoid the cubic scaling of updating the Gaussian process?   - What is the motivation of using a stationary task kernel described in Section 4?  - Equation page 3 bottom: argmin -> argmax    Significance:  As explained above I don't fully understand why one cannot tackle this setting here with standard Bayesian optimization. Given that the difference in the fusion simulation experiments between the proposed method and standard Thompson sampling with randomly picked tasks seems to be relative small, I am afraid that the contributions of the paper are not sufficient for acceptance.    Multi-task bayesian optimization Kevin Swersky, Jasper Snoek, Ryan P Adams Advances in neural information processing systems  Predictive entropy search for efficient global optimisation of black-box functions José Miguel Hernández-Lobato, Matthew W Hoffman, Zoubin Ghahramani Advances in neural information processing systems    ###### Post Rebuttal ########  I thank the authors for taking the time to respond to my review. The authors clarified the difference between standard BO where the input is augmented with task variables and their setting, however, a simple baseline that is missing, is to optimize one task at a time instead of sampling them randomly in each iteration.  I will increase my score slightly (4 to 5), but still tend towards rejection. 