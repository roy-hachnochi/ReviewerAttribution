I have read the other reviews and the author feedback. I'm curious to know the results of the comparison to the Greedy baseline that the authors promise in their response, it is hard to make a judgement without that comparison. I maintain a borderline accept rating, if the discussion points from the rebuttal make it to the final version. The discussion about how a policy-make might set K_i (size of the filter at each tier) in the response is very under-developed -- perhaps it is better to highlight and discuss this more as future work. Overall, the response addressed some of my concerns (re: practicality for the admissions setting, clarification of notation and correctness). ----- Clarity: Good. The paper was easy to follow, and the list of symbols in the appendix was a useful reference to quickly recap what the various symbols meant. The experiments and figures need to be explained more (see detailed comments).  Quality: Average. There are a few natural approaches for the problem setting that could be discussed. Greedy approaches should be a baseline in the experiments (with optimistic initialization, they often perform better than Uniform or Random policies in bandit settings).  Significance: Average. The problem setting is reasonably well-motivated but can be better. The algorithms that are developed do not yet seem practical for the motivating application. The results can be more significant if there is a better characterization of the gap (in terms of cost, utility, etc.) between direct Top-K selection vs. Tiered Top-K selection. This might even help illuminate what K to pick in each tier -- there is qualitative discussion about this after the synthetic experiments, the paper can be substantially stronger with a more rigorous analysis of this phenomenon.  Nitpick: I did not try running the code, but there are comments (see line 358) that say "Problem here". Does it implement the pseudo-code from the paper, or is there a problem there? Does it work?!  Line 176: The theorem applies for linear w. In Line 13 of Alg1, I understand w to be like w_div of Eqn5 which is clearly not modular/linear but submodular. Please clarify the theorem statement and foreshadow that w will not be linear and the theorem won't technically apply, rather than waiting till the discussion section in the end to discuss this.   Line280: The approach assumes a maximization oracle. However, we can only approximately maximize submodular functions efficiently (e.g. 1/e-approximation using a greedy strategy). Can the theorem account for sub-optimality of the oracle? For instance, Radlinski et al 2008 does such an analysis for submodular ranking.  Figure1: Please clarify whether Hardness is exactly the term inside the O() bound of Theorem1. Please explain how Figure 1 was generated.  The paper can benefit from a discussion on why simpler approaches than CACO are doomed to fail. Either a formal lower bound showing the hardness of this problem of subset selection. Or, at least an instructive example why greedy elimination in early stages can yield unrecoverable errors in later stages. That can then motivate the notion of pessimistic estimates used in Line10-11 of Alg1.  Line285: Hardness really needs to be defined more clearly!  How does greedy subset selection perform? A naive baseline that does not do pessimistic corrections of the empirical estimates but simply picks items according to their empirical mean. This seems like a marginally better baseline than UNIFORM or RANDOM.  Nitpick: The discussion in lines 358-360 should be highlighted earlier. The last statement in the abstract setup some unrealistic expectations. The expt in Section5.2 is better described as a semi-synthetic simulation (rather than how the algorithms might perform in the real world). Since both algorithms need multiple pulls of active arms in each stage (please clarify in Section5.2 or the appendix what is the maximum number of times an arm gets pulled by CACO and BRUTAS), these algorithms do not yet seem practical for the tiered interview setting. 