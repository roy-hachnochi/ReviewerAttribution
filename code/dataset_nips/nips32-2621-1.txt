- The paper proposes a method to automatically design interactive games so that player with different latent preferences (e.g., reward seeking versus loss aversion) are easily separable from observing their action choices. Designing the reward functions for these games manually or heuristically is non-trivial as even simple behaviors (reward seeking versus loss aversion) are not easily separated by certain seemingly correct choices of reward functions as shown in Figure 1. The paper formalizes this notion of "behavior diagnostic game design".  - The optimization problem of finding the game parameters is set up as the maximization of the mutual information between the player (z) and the observation (x) produced by z interacting with a chosen game. The exact problem is intractable so a variational bound is derived.   - The player space p(z) is defined using a prospect theory model which models the distorted reward perceived by a player given their latent preferences as a triple of three numbers. Now assuming the player behaves near-optimally wrt to their reward, an MDP can be induced where the reward and transition functions are to be selected such that the variational lower bound of the mutual information objective is optimized. The implementation involves training a simple RNN to learn the variational distribution as the means of the factored gaussians. Overall, the formulation seems quite elegant to me.  - The experiments involve path and grid domains and primarily involve the study of three behaviors: loss-neutral, loss-averse and gain-seeking. The reward and transition functions are learned in these simple environments. The learned reward and transition function correspond to the optimal parameters for maximizing the variational lower bound of the mutual information under the parameterization of the observation space and player space. The approach allows an effective search through the induced game space in order to identify (parameterized) games that produce higher mutual information. Larger game spaces produce better MI suggesting the search is working. Here, I'd be curious to see how different choices of baseline score. Specifically, how much variance is there in the MI as a function of the choice of R(s) in Eq 12? Some experiments with additional values of R(s) would be interesting to see. As it stands, the gap between methods seems a bit small numerically (0.107 vs 0.111) and I'm unable to tell if the difference is significant. Also, I don't understand why there is no baseline for the Grid environment.  - The next set of experiments involve parameterizing the player space as a mixture of K (=3) Gaussians, where the three components correspond to different types of player behavior. The player labels (component) induce a classification task and here again the Grid environment shows the best ability to separate the player behaviors when a version of the model augmented for classification is used to predict player labels. Here the win over the baseline is much clearer. As before, a larger collection of player baselines may help illuminate the utility of the search a bit better.  - The experiments also include studying the effect of noise as well as the effect of the priors. Overall, while the experiments do a good job of analyzing the proposed method, I think a more rigourous set of experiments with baselines would strengthen this portion of the paper.  - Also, are human latent preferences limited to the 3 types considered here? Perhaps a discussion of why these 3 were considered and other possible behaviors (e.g., risk seeking?) might make the paper more accessible to a larger audience.  - Overall, the paper contains an elegant formulation and solution to the problem of designing games to elicit separable human behaviors. Although I'm not very familiar with this body of work, I was able to understand the paper and its findings. The paper is exceptionally well written and the ideas flow very naturally. Overall, this seems like a very nice application of modern ML techniques to an application in cognition / psychology. I'm happy to recommend acceptance.  UPDATE: The authors have satisfactorily addressed the points raised by the reviewers. My score remains unchanged.