Pros: This paper provides an empirical study to show that label smoothing helps the representation in each class to be close while equally distant to incorrect classes, which is intuitive but can provide insights. Also the effect on reducing the confidence of the prediction and thus help calibration is intuitive and interesting. The author also explains that label smoothing hurts distillation, due to label smoothing erases the relative confidence information between classes and examples, which also make sense to me.  Cons: 1. Some of the findings in the paper are somewhat intuitive and natural, such as label smooth reduces confidence and help calibration.  2. What we care more is how these findings help us to get more insights or help us to better use label smoothing or design better methods. 3. The experiments in Section 2 and 4 are only conducted in image classification tasks. I wonder if the phenomenon holds in other tasks in NLP, such as text classification, machine translation. Since the work in mainly based on the empirical study but no theoretical proof, analyses on more tasks are necessary.  