The paper gives a PAC learning algorithm for the basic problem of halfspaces in a model of learning with noise.  The algorithm uses ideas from previous related results in the simpler model of random classification noise, with important new ideas.    Learning with noise is a basic topic in learning theory. It can be argued that the most studied models (random misclassification noise and malicious noise) are unrealistically benign (even though the related SQ model is very important) or malicious, and there is a great need for the study of more realistic models. The Massart noise model is a candidate for such a model. As positive learnability results in the general PAC model were not known for this kind of noise, the result of the present paper is quite significant.    The algorithm is non-proper, with a kind of decision list as hypothesis. This is especially interesting, as this class of decision lists is a natural class, which have already been studied (called neural decision lists, linear decision lists and threshold decision lists, going back to the work of Marchand, Golea and Rujan 30 years ago). It would be useful to comment on the possibility of proper learning halfspaces in this model.      Comments:    It is mentioned that an equivalent notion called ``malicious misclassification noise'' was studied (also 30 years ago) by Sloan. An explanation (or at least a reference) should be given for the equivalence of the two definitions. Malicious misclassification noise seems to be an appropriate term fitting the general terminology, and it seems that instead of using two names for the equivalent notions, one should just use malicious misclassification noise, noting that the other one is an equivalent definition.  Massart noise is also unjustified as the cited paper is due to Massart and Nedelec. A small additional point is that presumably there is some ``tameness'' assumption for the noise probability function $\eta(x)$ (or not? does this matter for the equivalence proof?). A related comment: the literature review does not distinguish between the real-valued and Boolean domains (for example, Daniely's negative result already holds for the Boolean case); some comments on that should be added.      One more terminological remark: the basic PAC model is distribution-independent, and for ``PAC learning under a fixed distribution'' is used when the underlying distribution is fixed. Thus the term ``distribution-independent'' in the title seems redundant (of course it is important to emphasize this feature as opposed to results on ``tame'' distributions in the text).    