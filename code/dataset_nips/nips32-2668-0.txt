Originality: The analysis is original to the best of my knowledge.  Quality: Seems good, except a few issues that require clarifications: 1) In section 4.1 it is claimed that the eigenvalues for sine functions are zero, but in Figure 3 we see that the leading (non-zero) eigenvalue belongs to a sine funcion. How is this possible? Also, does figure 5 include sine functions? 2) In figure 5, what is the reason for the mismatch at the low frequncies?  Clarity: Mostly clear except 1) What was the stopping critertion (\delta) in Figures 5 an 6? 2) In figure 5 and 6 the phrasing is a bit a obscure - it took me a while to understand what is going on with the odd frequencies (that they do not appear in the graph since they didn't reach the stopping critertion after many iterations). It is was not very clear that dots on top of left panel correspond to these non-converging odd runs. I would suggest adding a few example runs in the appendix. 3) I feel the authors should add the basic explanation why bias it is necessary for the network to approximate an odd function. It is rather simple, as I explain next. 2-layer ReLU without bias are positively homogeneous functions of x. Therefore, they cannot approximate odd functions in x. Specifically, in S^1, we must require cos(k(\theta+pi)=cos(k(\theta), so k must be odd.   Significance: A nice paper with some interesting observations.   Assuming my comments will be answered satisfactorily, I recommend acceptance.  Minor issues: - line 149: shouldn't cos(\theta) have some normalizing scale factor? - line 198: "makes sense" not very clear. Please use more precise terms. - The equation below 198, S should be in \mathbb{}. - line 213 x^{\ell} should not be in bold. - eq. 16 "w'x" in indicator function should be "t".   %%% After Author Feedback %%% The authors have answered my comments, and I'm voting for acceptance. I think these convergence results give a nice and informative characterization of the convergence rates for different frequencies.   