What functions do NNs learn (approximate a function) and how fast are central questions in the study of the dynamics of (D)NNs.  A common conception behind this problem is that if one trains a network longer than necessary, then the model might overfit. However, the definition of overfitting appears to vary from paper to paper. Moreover, overfitting is intimately linked with another hot topic in the area: over-parametrization. Please refer to "Advani & Saxe 2017 High Dimensional Dynamics of Gen Error for NNs" for a modern take on this link. Keeping in mind this link, we focus on fixed-size networks. If one monitors the validation performance during training, it is well established that early stopping yields better performance. Recently, Nahaman et. al. On the Spectral bias of NNs shows that NNs learn (in time) functions with increasing complexities.  The present paper, "The Convergence Rate of NNs for Learned Functions of Different Properties" builds on this idea of learning functions with increasingly high frequencies. It is a well-written paper that demonstrates the core decomposition. Moreover, the decomposition yields convergence times for each component. As a major plus, the proposed rates seem to match well with the empirical observations. Even though the paper is an important step forward, it has a few shortcomings:  - The mathematical description of the learning problem is limited to 1 hidden layer networks and mean square loss. It may be hard to extend proofs further. But the empirical evidence is not enough to justify its real practical value. - Authors didn't provide the code. It didn't affect the score I assigned. But it would have been a great chance to see how the curves change with different ways of scaling the meta-parameters and change the loss. - Derivations depend on (it heavily relies on [3] in the references of the paper) a particular scaling that puts the dynamics into a linear regime. There is no discussion of over-parametrization and active and lazy learning regimes. For a review and further references, please refer to Chizat & Bach 2018 On Lazy Training in Differentiable Programming.   I think with the inclusions of the above discussions, code, and further experiments, it would be a very nice paper. Overall, the argument is sound and the text is clear (except for a few typos and sentence constructions).   For a further point: Convergence times scale pretty bad with the dimension of the input data. But then it may depend on the intrinsic dimension of the data for real-world problems. I think it is another exciting direction that is not directly within the scope of the present work. But it would be a useful direction of research for future studies.  ======= update after author response ======== Thanks very much for considering suggested changes. I hope to be able to see the code available as well!  