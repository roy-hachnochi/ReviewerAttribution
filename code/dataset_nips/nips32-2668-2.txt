The paper presents a theoretical analysis of the convergence of  overparameterized shallow neural networks trained on regression problems. The main claim is that low frequency components of the regression landscape are learned faster than high frequency components and that this phenomenon, when combined with early stopping, leads to higher generalization performance.  Major concerns: The analysis presented in the paper is definitly insightful but it is not original. The bulk of the results presented in the paper were obtained in (Sanjeev, 2019) and (Xie2017). These results include the derivation of the Gram matrix and the connection with Fourier analysis on n-dimensional hyper-spheres (Xie2016) and the asymptitic convergence results (Sanjeev, 2019) .  As far as I understand, the only contribution of the present paper is in the inclusion of a bias term in the theoretical analysis which leads to a minor modification of the Gram matrix. The absence of the bias term apparently leads to the impossible to learn odd frequency components. introducing the bias term leads to a behavior for the odd frequency components that is asymptotically equivalent to the previously known behavior of the even frequency components. Consequently, the fix does not provide new theoretical insights but it simply confirms what it was previously known.  Most of the conclusions drawn in the paper follows from the previous work and are therefore not original.  Conclusion: The original contributions of the paper are very limited. Consequently, I believe that the paper should be rejected.  References: Xie, Bo, Yingyu Liang, and Le Song. "Diverse neural network learns true target functions." arXiv preprint arXiv:1611.03131 (2016).  Arora, Sanjeev, et al. "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks." arXiv preprint arXiv:1901.08584 (2019).