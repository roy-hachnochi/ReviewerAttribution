originality: training reinforcement learning with auxiliary losses can trace back to "reinforcement learning with unsupervised auxiliary tasks" however difficulties in cooperating auxiliary losses with main objectives block this idea from being widely applied. Unlike previous works where influences from auxiliary losses depend on manually set hyper parameters or cosine similarity on gradients, this paper originally propose to differentiate main objective  using Taylor approximation involving gradients of auxiliary losses. quality: the paper show detailed derivation of proposed update rules. Even though the remaining derivation still goes, I am not sure if the application of the finite difference approximation in the first line of equation 3 is appropriate as the difference depends on the step size alpha, which can be relatively large. The paper also provide n-step version of suggested weight update. Given assumption listed in the paper, this extension should be reliable. clarity: the writing of this paper is basically clear. My only concern is about the adaptive loss balancing technique. More explanation on this technique and the motivation of using it on this work can be helpful. Is it widely used in similar setting or does it have any limit? Does the advantage of proposed method depend on this technique or any other auxiliary losses method ,like cosine similarity, can also benefit from it? significance: in two of three listed experiments, proposed method clearly outperforms others. In the remaining one, the proposed method is still better than others even though the difference is not as big as other two experiments. Authors also provides experiments supporting the advantage of their n-step update over one step update. Again the difference in the experiment of "Visual Fetch Reach" is not as big as the other two.