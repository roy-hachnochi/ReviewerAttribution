# Update after the rebuttal I appreciate the authors addressing most of my concern regarding 1) hand-tuned fixed baseline, 2) more results on Atari, and 3) handling harmful auxiliary tasks. I think the results are much more comprehensive now. I raised my score accordingly.  # Originality - This paper proposes a novel and interesting method to adapt the weights over different auxiliary objectives. If I understand the main idea correctly, the proposed method can be interpreted as a gradient-based meta-learning method (e.g., MAML) in that the algorithm finds the gradient of the main objective by taking into account the parameter update procedure. It would be good to provide this perspective and also review the relevant work on meta-gradients for RL (e.g., MAML [Finn et al.], Meta-gradient RL [Xu et al.], Learning intrinsic reward [Zheng et al.]). Nevertheless, I think this is a novel application of meta-gradient for tuning auxiliary task weights.   # Quality - The proposed method based on gradient similarity between main loss (RL) and auxiliary loss sounds sensible.  - The results on the domain considered in this paper look promising (outperforming baselines). However, the overall results are not comprehensive enough to show the benefit of the proposed approach for the following reasons.  1) This paper does not have an important baseline: hand-tuned but fixed auxiliary weights. This paper claims that the optimal auxiliary weights may be different at different learning stages, but this claim is not empirically shown in the paper because of the lack of such a baseline.  2) The domain and the auxiliary tasks considered in this paper are different from the ones used in the baselines (gradient balancing, cosine similarity). So, it is hard to see how significant the improvement is. It would be much more convincing to show results on the same setup as in the previous papers. 3) It would be much more convincing to show broader experimental setups. For example, all the auxiliary tasks in the experiment happen to have a positive effect in the experiments (as the learned weights are all positive). It would be interesting to see if the proposed method can also deal with the negative effect of auxiliary tasks. Also, the proposed method can be used in the multi-task learning scenario, where other tasks are considered as auxiliary tasks, It would be again interesting to automatically adjust the weights over different tasks to prevent negative transfer while encouraging positive transfers. Finally, it is not clear if the proposed method is specific to RL. If yes, it would be good to have a short discussion on why this is specifically useful for RL. If no, it would be good to show some empirical results on supervised learning as well as RL. Showing this would make the contribution of the paper clear.  4) (relatively minor) The main algorithm used in this paper (HER) is specifically designed for goal-based RL, where a goal is given as an additional input. It would be better to consider a more general RL problem/algorithm instead of goal-based RL.   # Clarity - This paper is well-written and easy to follow.  - It would be good to discuss a connection to meta-learning in the method section and the related work.  # Significance - The idea behind this paper is novel and interesting. However, the experimental settings are quite limited to one specific domain (visual MuJoCo) and one specific method (hindsight experience replay) with a specific set of (positive) auxiliary tasks. Thus, it is not clear if this paper is significant enough to get interests from the Deep RL community. 