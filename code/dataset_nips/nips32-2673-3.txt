The paper studies a new multi-armed bandit setup, where after each pull of an arm, it is blocked (i.e. zero reward) for a certain number of rounds (this number is known). The setup captures some real-life applications nicely as motivated in the beginning of the paper. The paper also makes it clear in Sec 1.2 how this new setup is connected to existing ones and why those results do not directly solve the problem.  The paper includes a set of theoretical results as listed above, starting from the natural question of if one can solve the offline version of the problem with knowledge of expected rewards of all arms, and then coming back to the online setting and discussing how to ensure low regret against a reasonable offline strategy. The techniques of proving the regret bound (highlighted on Page 2) indeed seem novel.  There are still several loose ends though. For example, the regret upper and lower bounds do not match exactly. Also, it is not entirely clear to me that a computational hardness result for finding the exact solution of the offline problem excludes the possibility of efficient online algorithm with sublinear (exact) regret.   Overall, I believe the paper studies an interesting new setting and provides several solid results. I recommend accept.