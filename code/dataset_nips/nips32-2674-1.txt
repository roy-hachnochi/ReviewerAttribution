The study of finite mixture of distributions is an important issue in machine learning. Except for the Gaussian Mixture Model (GMM), there is not theoretical guarantee for the parameter estimation problem for parametric mixture models. This paper proposes to provide some guarantee for a wider class of mixture models, namely the mixture of log-concave distributions.   The expectation-maximization (EM) algorithm is the standard tool to study mixture models. However, the global convergence of the EM algorithm is not ensured, even in the simple case of GMM. Recently, the global convergence of the EM algorithm has been established for a balanced mixture of two Gaussians, two truncated Gaussians, two linear regressions or two Laplace distributions. The present paper extend these previous works to a mixture of two log-concave distributions. In particular, the explicit formulation of the density is not known in this context. To overcome this issue, the authors propose a new variant of the EM algorithm: the Least Square EM (LS-EM) algorithm. The basic idea is to replace the classical M-step of the EM algorithm by a least square problem. In particular, for GMM, the LS-EM is reduced to the EM algorithm.  The authors prove the global convergence of the LS-EM algorithm for a mixture of rotation invariant log-concave distributions, with random initialization. Convergence is established for any dimension in the infinite sampling setting and for dimension one in the finite sampling setting.   Comments:  This new convergence study is very interesting. However, some assumptions made by the authors reduce the scope of application of this paper. First of all, the covariance of the sampling is supposed known and the estimation concerns only the position parameter. Moreover, the mixture is assumed to be balanced and symmetric. Last, the distributions are assumed to be rotation invariant ; in practice many interesting distributions do not satisfy this condition (as claimed by the authors in their conclusion). Despite these limitations, the authors propose a new way to demonstrate the global convergence of an EM-like algorithm based on a geometric notion of angle-decreasing. They also provide a demonstration of the non-effectiveness of the traditional approach through the l2 distance.  The paper is well-written. Unless I am mistaken, the proofs (in the supplementary material) are correct, very clear and easy to follow. There are some typos in the supplementary material but they do not interfere with reading.  Numerical experiments are provided in the supplementary material but it is not mentioned in the main text, which is unfortunate.  Last, the author refer to a “regular condition” throughout the paper but never explicit it. I guess the authors are referring to the classical assumption for differentiation under the integral sign but I would have liked to see it explained, at least in the additional material.