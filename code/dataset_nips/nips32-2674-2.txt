~~~~~~~~~~~ After rebuttal: Thank the authors for clarifying my comments #1 and #3. Since my comments on #2, finite sample analysis for a general dimensional case, was not addressed, I would not increase my rating and would like to keep the current "accept" rating.  ~~~~~~~~~~~ This paper studies the theoretical properties in the estimation of location of mixture of two rotation invariant log-concave densities. It shows that a newly proposed LS-EM algorithm converges to the true location parameter with a random initialization. In the finite sample analysis, explicit convergence rates and sample complexity bounds are further developed.  Strengths:  1. This paper is very well written and easy to follow.  2. It addresses an important and challenging problems.  3. The theoretical analysis is non-trivial and the technical contribution is high.  Weakness:  1. A special case of the proposed model is the Gaussian mixture model. Can the authors discuss the proved convergence rates and sample complexity bounds with that established in GMM (Balakrishnan et al., 2017)? It is interesting to see if there is any accuracy loss by using a different proof technique.   Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77â€“120, 2017  2. The finite sample analysis (sample complexity bounds) is only derived for the 1-dimensional case. This largely limits the popularity of the proposed theoretical framework. Can the authors extend the finite sample analysis to a general d-dimensional case, or at least provide some numerical study to show the convergence performance?  3. In Proposition 6.1, the condition \eta \ge C_0 for some constant C_0 seems to be strong. Typically, the signal-to-noise ratio \eta is a small value. It would be great if the authors can further clarify this condition and compare it with that in Section 4 (correct model case). 