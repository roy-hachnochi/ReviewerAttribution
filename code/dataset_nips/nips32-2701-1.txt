*Originality*  The abstract claims that the current submission is the first one to systematically model the active search problem as a Bayesian formulation and the resulting algorithm outscores prior work. However, the theoretical model in paper is very limited. The authors assume a fixed prior and derive a Bellman equation. There is some novelty in the choice of the state and to include number of queries remaining in the state, but these are all standard tricks in the MDP literature.   Furthermore, at the end the algorithm presented is a heuristic one. The authors assume -- as the prior work -- that the distribution of the state of their MDP does not change with the outcomes of the currently considered points. This decoupling assumption relieves the algorithm of the complexity of computing these posteriors for each point at each step. They simply assume a fixed distribution for the state with updated parameters. Specifically, they assume a negative Poisson Binomial distribution (which they have defined in the paper and is perhaps new). This is itself a heuristic and places the current submission at par with prior heuristic search methods.   *Quality*  For me, this work is mainly experimental. The authors have not compared with the large body of theoretical work on active search, which perhaps does not lead to algorithms as efficient as the authors desire, but is theoretically more rich. The authors should have at least clarified why that literature is not relevant here. Further, as described above, the only theory developed in the paper is the Bellman equation for an appropriate state function, which is interesting but not comprehensive. Thus, I think the main contribution of the paper is a heuristic algorithm which fares better than some prior work on specific data sets.   For experiments, the authors considered two use-cases of drug discovery and material discovery and compared many existing methods. However, I found the improvement reported in Figure 1(b) rather marginal. But the numbers reported in Table 2 are perhaps acceptable as a benchmark for convincing numerical improvement. I am not an expert on this point.   *Clarity*  The presentation is very confusing. Even the problem formulation was never concretely laid down and it took me several readings to reconstruct the problem. What Bayesian formulation are the authors referring to? Is the the knowledge of the exact posterior assumed or only within a family? Is the cost the expected number of queries or you want a confidence bound? All these questions are eventually clarified but never explicitly. I would have preferred a complete formulation at the outset. Also, it was never clarified which distribution (what parameters for NPB) are used in the first step of iteration. I understand the posterior updates from there on, but what do you start with? I assume all p_is are set as 1/2 first.   Overall, I think it is an interesting paper but below par for NeurIPS publication.  [Update after rebuttal]  The authors have not really clarified my two important points: How is their assumption of distribution justified (is it the least-favorable one and matches some minimax lower bound?) and how is their decoupling assumption justified? However, reading other reviews I feel that these are not the main concerns for people working in this area. So, I can't increase the score, but I can reduce my level of confidence to 2. Perhaps the area chair can take a look and decide.