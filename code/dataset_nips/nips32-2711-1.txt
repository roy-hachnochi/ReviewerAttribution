After reviewing the feedback of authors and other reviewers, I've decided to keep my high score. This is a strong paper. -------------------------------------------- 1. Originality: 1.a. The idea of using 1-4-3 FP8 format is the evolution of prior work on using 1-5-2 FP8 format for training neural networks. 1.b. The idea and analysis of re-tuning batch normalization statistics to match 1-4-3 data is novel. 1.c. The idea of doing distributed training with FP16 reduce-scatter and HFP8 allgather is novel and is supported by an excellent analysis.  2. Quality: the paper provides a detailed analysis of hybrid FP8 training approach and highlights the benefits of having FP8 multiplication units that support both 1-4-3 and 1-5-2 formats.    3. Clarity: the paper is clearly written, and together with an appendix, provides enough information to reproduce the results.  4. Significance: the paper explores a practical technique to speed up training and inference of neural networks, at a cost of minimal model quality loss and introduction of new hardware primitives.  While this paper builds upon previous work on FP8 training & inference, it provides significant improvements useful for the research community and the industry.