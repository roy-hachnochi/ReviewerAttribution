Originality - This basically amounts to using two different floating point formats - one for forward, and one for backward.  Or another way to think about it is that we are allowing more freedom in the mantissa/exponent divide for floating point.  That's a good observation to have, theoretically, but how would a framework implement this, practically?  For example, maybe I missed it, but I don't see how you convert between 1-4-3 and 1-5-2 formats when you prepare for back prop if we were to productize this.  Do the frameworks now have to support 2 more data types?  Is the user aware of the data types, even?    Quality - They did a sufficiently thorough study, including looking at impact on batch norm layers, thinking about weight update, etc.  I appreciate the diversity of networks addressed as well - that's very convincing.  Clarity - the paper is clearly written. How do you get away with FP16 master weights when most others need FP32?  Significance - Is the intent to convince hardware vendors to provide this?  Or is this for a custom chip?  How does a reader take advantage of this?  -------------------- After author feedback. The authors did not quite address my question about frameworks using this.  I was referring more to annoying things like plumbing a new data type through every layer implementation.  That is an often overlooked part that takes real work.  However, despite that, I think this paper still passes the bar so I'll keep my score.  My other issues were addressed.