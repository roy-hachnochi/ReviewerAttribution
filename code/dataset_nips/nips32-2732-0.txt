Summary: This paper introduces Poisson auxiliary variables to facilitate minibatch sampling. The key insight is with the appropriate Poisson parameterization, the joint distribution (Eq. (1)) only depends on factors if they are in the minibatch. The authors apply this insight to discrete-state Gibbs sampling (Algorithm 2), Metropolis Hastings (Supplement), and continuous-state Gibbs sampling (Alg 3. and 5). The authors also develop spectral gap lower bounds for all proposed Gibbs sampling methods, which provides a rough guideline for choosing a tuning parameter $\lambda$ and comparing the (asymptotic) per iteration runtime of the methods (Table 1). Finally the authors evaluate the Gibbs methods on synthetic data, showing that their proposed method performs similarly to Gibbs while outperforming alternatives.  Quality: The submission appears to be technically sound with only a few minor typos in the supplement. The computation speed-up claims are supported by the theoretical results (Theorems 1-5). For Theorem 5, I believe $\delta_k$ should be proportional to $exp(L + \delta_m)$ instead of $exp(L)$ as it requires an upperbound on $\tilde{U}$ rather than $U$. The computation costs in Table 1 seem to mix worst-case running time with average-case runtime (used to evaluate Poisson Gibbs); isn't the average-case per iteration runtime of plain Gibbs sampling O(D * mean degree) rather than O(D * max degree)? This is a minor complaint: in many applications (including the experiments of the paper), the mean degree = max degree = number of states.  Originality: To the best of my knowledge, the idea to using Poisson-minibatching to reduce calculating factors is novel. The paper adequately cites previous work and distinguishes it contributions compared to previous related work of De Sa et al. (2018) in Table 1.  Clarity: The paper is well written and clearly organized. The result on Poisson Minibatching for Metropolis-Hastings (although very interesting) seems a bit out of place, as it isn't introduced or addressed anywhere else in the main paper.  In addition, there are a few areas that I suggest additional clarification: -Line 78: Algorithm 1 should be noted to only apply for discrete state spaces -As stated above, Table 1 and line 102 should be careful about whether computation cost per iteration is for the worst-case or average-case. -Line 112: "this joint distribution achieves the minibatch effect automatically", what is the minibatch effect? -Line 129: "At each iteration we will first re-sample all the $s_\phi$", but Algorithm 2 only samples $s_\phi$ for $\phi$ in $A[i]$. -Lines on Figure 1c are difficult to read printed.  Significance: The Poisson Minibatching trick seems to be a useful contribution to the literature on scaling MCMC by subsampling factors. As demonstrated in the paper, there are many applications and extensions by combining this trick with existing sampling methods. The spectral gap convergences bounds are also nice and allow users to compare the trade-off between the different sampling schemes.  Figure 1 of the experiments plots the performance of the proposed methods vs the number of Gibbs iterations. The computation speed-up claims would be better supported with the addition of identical plots with the x-axis changed to measured runtime.  Typos: -Line 294: "the average needed steps to be accept is" -The last line in the equation immediately following line 373 should have $\exp(U_\nu(x))$ instead of $\exp(U(x))$ as the sum is over factors in $A[i]$ not all factors. -As a result, the next equation (immediately following line 374) appears to be missing a factor of $\exp(U(x)-U_\nu(x))$, but the result should still hold as this factor does not depend on $x[i]$. -In line 426, I believe it should read $m = \Theta(L)$ instead of $m = \Theta(L^2)$.  