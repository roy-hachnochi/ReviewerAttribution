==============Post rebuttal ========= Thanks to the authors for their feedback. Somethings still remain unclear : 1. Label embedding methods for large output spaces can be seen as deep networks with one hidden layer. It seems that the main message of the paper is that training these as deep nets with corresponding optimization schemes, rather than methods from conventional label embedding methods such as LEML and SLEEC leads to better results. Instead, the paper's main message conveys something different - namely, over-fitting of label embedding is the main cause for label-embedding methods, and it is fixed by regularization. Instead, on these datasets, a linear classifier also overfits, and it is not much of a surprise that a label embedding method/one hidden layer network overfits also.  2. It is somewhat difficult to understand the 200% increase in the propensity scored metrics(PSP@k in Table 4) over other sota baselines, while only below par or similar performance on vanilla P@k (Table 3). For instance, compare the proposed method vs SLEEC for various datasets. Does it mean that the method is working very poorly on head labels that it deriving all its performance from the tail-labels. In which case, it should be verified that of the model is indeed is doing poorly on head labels, and needs to be investigated why that is happening. 3. There should be much more clarity on the training time in terms of number of cores, and number of hours/days it took on diffrenent datasets, since this is a standard practice in XMC community, which makes it comparable to other methods.  ================= The paper presents a simple deep learning method with one hidden layer for learning with large output spaces. The results show that the proposed method can achieve comparable performance to state-of-the-art linear methods such as DiSMEC/PPDSparse and tree-based methods such as Parabel.  On a high level, I have the following concerns about the paper : 1. The paper suggests that they have found that overfitting is the main cause for bad performance of label-embedding methds, and that this is fixed by Glas regularizer. However, that does not seem to be the case, as from Table 1 and Table 2, the results without the Glas regularization are almost in the same ball-park as state-of-the-art methods, and the regularization merely helps by couple of % points. It seems that the main idea is that take a simple deep network (fully connected) with one hidden layer(acting as embedding) it gives close to sota performance, and one can increase the performance somewhat by regularization  2. It should be discussed why LEML (and SLEEC) which is based on similar scheme architecture but does not train it as a deep network works so poorly while the proposed method works better. Is it due to different optimization algorithm scheme.  3. Over-fitting is not new for these datasets, and it is likely that even linear classifiers such as DiSMEC overfits most of these datasets. For instance, one can get close to 99% training set accuracy with DiSMEC on Amazon13k data. So, it seems that showing over-fitting with embedding scheme is not very surprising.   4. The results for propensity score metrics (Table 4) seem rather surprising, since these metrics typically correlate very well with vanilla metrics (Table 3). Even though the vanilla metrics are very close to most sota methods, the prop scored metrics are almost double in some cases, which is really surprising. Since the proposed method does nothing special for tail-labels, and 70-90% are tail labels, the vanilla metric should also much higher, in that case. In this respect, it would be useful to be provided with the code to verify the claims, or the authors could check if their is correct, and if so, then investigate the reason for such large discrepancy.  5. The proof of Theorem 2.1 is not quite clear. In line 474 it seems that the proof if for some x that exists, and it becomes 477 it becomes for all x. The transformation is not immediate, and unclear.  6. The paper does not say anything about the computing infrastructure used to train the models for big datasets. It is important for fair comparison since most other works provide some information oin this context.   7. Missing references 1.  The paper does not compare with AttentionXML [1], another deep learning approach which seems to be giving quite good results. 2. There is no comparison with ProXML [2]  which is a sota method for propensity scored metrics, even though the code is available. Moreover, it also mentions ideas related to using graph laplacian for label co-occurrence.   [1] AttentionXML: Extreme Multi-Label Text Classification with Multi-Label Attention Based Recurrent Neural Networks, https://arxiv.org/abs/1811.01727  [2] Data scarcity, robustness and extreme multi-label classification, https://link.springer.com/content/pdf/10.1007%2Fs10994-019-05791-5.pdf