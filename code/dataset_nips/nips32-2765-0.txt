This is a solid work that extended vid2vid to adapt examples from unseen domains. The key idea is the weight generation module that dynamically generates the weights of the network to transfer input examples from a new, unseen domain. While dynamic weight generation has been applied to a broad range of problems, this is the first work to address the vid2vid problem.  The main downside of this work is the lack of details for the weight generation module: 1) If I understand correctly, the way the "style" of the new images is encoded in the beta vectors. The attention module compares the similarity of the poses in the testing examples to the current pose to be translated and produces a weight for combing the beta vectors (the new style from an unseen domain). It seems such an attention scheme is the key to outperform the alternatives. Please clarify if this is the case. 2) How It is not clear to me how many examples (the K parameter) is needed for training and testing. If attention is the key, then the more examples, the better. Does the method fail when K falls below a certain value? 3) It is not clear the comparison to the baselines are fair. It seems that all baselines use a single image to encode the "style" code or compute the parameters in AdaIN, whereas the proposed method uses K images. When it is true that the proposed method is unique in being able to fuse multiple reference images using attention, it is necessary to discuss this clearly in the paper. 4) It is also not clear that weight generation is necessary. Why not simply feed the weighted average appearance presentation (Line 154) to the generator, for example, via AdaIN layers. What is really the advantage of E_c here?  Overall, the work addressed a very important problem and provided a novel solution. Although there are still artifacts, the results are very convincing from the perspective that shows clear improvement over the existing alternatives. The reason for not giving higher score is the lack of details and in-depth discussion for the network weight generation module, especially what actually makes the networks work-- the attention or the weight generation. 