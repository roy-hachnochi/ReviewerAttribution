 Update: I appreciate the authors taking my feedback seriously.  The way the results are presented now is both more complete and easier to interpret.  I count on the authors for updating the explanation of shared parametrization (figure and text).   ----------    Clarifications/typos: Line 47: inner produce -> product Line 177: not clear to me what the "Shared parametrization scheme" is. What is shared exactly? Is there a single M for the whole network?  Line 184: first transform the *inputs* to the same dimension -> are you talking about the images or the feature maps ? Line 380: sifnificant  Can you clarify the Shared parametrization strategy ? - What is the input of M(.), the images or the feature maps ? - Is there really just a single M for the whole network ? - When you are resizing "inputs" with the adaptation network, are you talking about feature map or images? Is there just a single adaptation network? - In Fig. 3 there is only one layer so it's not clear how the method behaves for many layers  This whole section is very fuzzy; please improve clarity.  For the meta-learning experiments, the results can be hard to interpret: * It is well known that for the few-shot learning task, accuracy depends largely on the backbone architecture.  * For instance, in the paper "A Closer Look at Few-Shot Classification", the fine-tuning baseline with Resnet-10 architecture achieves 75.90% for miniImageNet 5-shot. * Therefore, it only makes sense to compare meta-learning methods for similar architecture. * The static NSN can be reduced to the classic Conv-4 architecture and can be compared with the other methods.  * However, the dynamic NSN cannot really be considered the same architecture, as it has more flexibility than Conv-4.   