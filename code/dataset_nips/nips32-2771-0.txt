This paper presents a novel model design/algorithm for building compositional representations of sequences when (as in natural language or code) it is presumed that the sequences have salient latent structure that can be described as a binary tree. The method performs essentially at ceiling on two existing artificial datasets that were designed for this task, both of which have not been previously solved under comparable conditions. The method also performs reasonably well on a sentiment analysis task.  Pros:  The method is novel and solves a couple of prominent instances of an important open problem in deep learning for NLP and similar domains with latent structure: How to we build models that can efficiently learn and to build compositional representations using latent structure? This is interesting and likely to garner a reasonably large audience as a somewhat abstract/artificial result. It is also plausible (though definitely not certain) that this models of this kind could yield significant advances in interpretability and robustness in downstream applications.  Cons:  If I understand correctly, there is a modest limitation of the model that is never really mentioned in the paper: At least at training time, the model takes O(N^2) steps to do O(N) work. A binary tree of N nodes only requires N-1 merge/reduce operations, but in this model, the cell() operation must be applied sequentially N^2 times to fully build an arbitrary tree. Some of this cost could be saved at test time. This should be discussed more in the paper, but it's not fatal: It's better to have a polynomial time algorithm that works than a linear-time one that doesn't. In addition, while the paper doesn't discuss the resources used for training, I expect that this polynomial-time backprop-friendly method is more efficient than linear time models like Yogatama's that must be trained using slow and unstable RL-style mechanisms.  Also, as I hinted at above, just because this model can discover and use structure reliably on clean artificial data, it is not obvious that this model will actually be able to discover structure consistently on natural noisy data. I think this is a significant and publishable advance, and I don't see any overt overclaiming, but this is a clear opportunity for improvement. The SST sentiment results are a bit disappointing, both because this is such a straightforward/easy/longstanding task, and because there is no evaluation or visualization of what structure the model uses on that data.  The reproducibility checklist suggests that source code is available now, but I can't find any.  Nit: Most of the arXiv papers that are cited have been published. Consider citing the published versions.