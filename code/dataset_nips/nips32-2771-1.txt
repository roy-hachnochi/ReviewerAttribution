This paper proposes Ordered Memory (OM), a new memory architecture that simulates a stack based on the principles of Ordered Neurons. In practice, OM operates similar to a continuous shift-reduce parser---where given an input sequence OM decides how many reduce operations are performed before shifting the new input into the stack. OM uses a stick-breaking attention method to model the number of reduce steps and a novel Gated Recursive Cell to implement the reduce step. Experiments on logical inference and ListOps show that OM is able to model required tree structures to solve the tasks and generalize to longer sequences that are not seen in training. Experiments on sentiment analysis show that OM achieves good performance on a real-world dataset as well.  While OM relies on many principles introduced in Ordered Neuron, I think that it is not straightforward to apply it to memory and therefore is still a good methodological contribution. The writing of the paper can be improved. I have published in this area in the past but it still took me some time to actually understand how the new method operates. I suggest improving the illustration in Figure 1. I also found several typos in the paper (e.g., in Eq. 5).  In terms of technical contributions, my major comment about the paper is comparisons with other stack-augmented recurrent neural networks. The authors show that OM outperforms LSTM, RRNet, ON-LSTM, Transformer, and Universal Transformer on the logical inference task. I would think that other stack-based methods (e.g., Joulin and Mikolov, 2015, Yogatama et al., 2018) would be able to perform well on this task as well. Similarly for the ListOps task. What is the main benefit of OM compared to these similar methods?