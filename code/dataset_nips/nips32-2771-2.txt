UPDATE:  Thank you authors for your response, and for adding the model ablations. I have changed my score to accept, however my concerns still stand regarding the performance of this model on real language tasks.    I think the architecture proposed is certainly interesting. However I have some concerns with the evaluation used in this work (namely the method doesn't do well on the sole real language task attempted), and how it does not actually validate the hypothesis (e.g. that this method learns compositionality of real language and as a result outperforms other methods). Moreover, the authors identify two core contributions that are distinct from prior work (stick-breaking attention and gated recursive cell), but do not analyse the effectiveness of these two contributions through any ablation studies.  Some other comments on the writing:  - The introduction contains a substantial discussion of related works. Personally I find this distracting. Out of the ~45 lines used for the introduction, only ~10 lines actually describe what this work is about. - I think perhaps there is a better title than Ordered Memory, as "ordered memory" doesn't really motivate anything related to compositionality, when compositionality seems to be the main motivation of this paper. - In equation 5, the superscript i for the (1-\pi) term should be inside the bracket)