It was relatively easy to follow the paper. I like Figure 1, it helps a lot of immediately getting the definition of differential submodularity. I only do not understand why in the definition we need g_S if we already have \alpha h_S.  The theoretical contribution of this work is very motivated by the prior work on performing submodular maximization in parallel, which is also noted by the authors. So, the main contribution of this paper is introducing the notion of differential submodularity. I believe that this notion has future potential, but it would be nice to see in this submission more comments and more comparison with the prior work.  In general, experiments are convincing that DASH outperforms the baselines. Also, in the experiments is said :"in all experiments, DASH achieves a two to eight-fold speedup ...", and then :"This shows the incredible potential of other parallelizable algorithms". I assume that this comment refers to plots (c) and (f), i.e., the running times. I think that "incredible" here is an oversell. It is a good news that indeed one obtains faster approach, but saying "incredible" feels like it is sometime orders of magnitude more efficient.  In Figure 4(a) and 4(d), when does SDS_MA saturate? It would be good to also use larger k for those experiments so ti see how SDS_MA behave.  --- Review updates --- I thank the authors for providing additional comments on their work. My question still remains on the applicability of this notion to other problems (the authors mentioned that they are working on extending these results to other problems, but as I understand the work is in progress). My score remains the same.