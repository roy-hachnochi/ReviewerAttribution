I like this paper, as it presents a carefully designed benchmark for an emerging area. The initial evaluation on the benchmark also suggests future research directions.  Some issues remain. First, the dataset has minimal visual complexity. It'd be great to include objects of various shapes, textures, and scenes of different lighting conditions. Note that without realistic rendering and texture, which indicate physical object properties, humans (and therefore models) may not be able to solves these tasks as well.  Evaluations can be improved in two ways.  - For all baselines, especially those don't work well with raw visual input (contextual bandits and policy learners), it'd make sense to use states (object and obstacle position, velocity) as input. This additional set of experiments will help to decouple visual perception from physical reasoning and highlight the merits of the dataset. - As mentioned in discussion, it's important to include some model-based RL/planning baselines. There are many papers on differentiable physics engines (e.g., interaction networks, neural physics engines). I wonder how they perform on these tasks.  I failed to understand Fig 4. I'm wondering if the authors can explain what it's about and why we care about it.  The authors indicated in the reproducibility checklist that links to source code and the dataset have been included, but I cannot find them.