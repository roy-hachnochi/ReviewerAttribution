Here are some concerns I have for this paper: - The dataset is designed to focus on "physical reasoning", which is a very broad concept. It would be better to know what aspects of physical reasoning are tested in this dataset ([40] is a good example), how are the experiment templates designed. - Following the previous point, since there is no description of the physics tested for different templates, it is difficult to draw conclusions from the results of cross-template experiments. Cross-template experiments are meaningful if they are testing similar physical reasoning processes (e.g., there could be different templates testing gravity understanding). - The selection of evaluated methods should be justified. I understand that the authors' intention is to evaluate methods that are not hand-coded with physics rules, but it seems that more appropriate baseline methods should be able to do some predictions. For example, a baseline could be a simple CNN future state prediction module combined with a success prediction module. - More like a suggestion than a criticism: it might be interesting to see human performance on this benchmark.   =========================================== I have read the authors' feedback. Although I still feel that the tasks could be more clearly defined, I am fine if the paper is accepted.