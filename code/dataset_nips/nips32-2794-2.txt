In the paper, the authors discussed PCR, a well-know variant of regression models, and showed the existence of a "double descent" phenomenon.  The paper is technically sound and relatively well-written. I check most of the math and they are correct and reasonable to follow. I do have some concern that too much of the space is taken by the algebra which could make it difficult for readers to grasp the high-level intuition, specifically if they do not have enough time to plough through the equations. Considering the space limit for a NeurIPS submission, I think it's better to reorganize some of the proofs to the appendix, and add a discussion/conclusion session to highlight more about the intuitions.  Back to the content of the paper: the PCR model dealt in this model is different from traditional regression analysis, in that - It is high-dimensional, with n,p,N all going to infinity - It mainly concerns the generalization (i.e. expected risk, or out-of-sample performance) rather than in-sample performance (e.g. how well the model fits existing data).  Now I think both differences are worth highlighting (which are actually direct causes of the "double descent" phenomenon). And these two differences extend the traditional analysis into a relevant and modern regime, especially in our current era (high-dimensional data, over-parametrized neural nets, etc.). Although linear models are simpler, understanding of their behavior in this regime could possibly shed light on the bigger questions (why over-parametrized nnets generalizes well?)  I do want to discuss a bit of the high-level intuitions here, especially how the "double descent" actually arises.  a) p<n, or the first descent: In the high-dimensional PCR model in this paper, the authors considered the noiseless setting, where σ=0. However, when one truncates the regression model to p, the rest N-p signals are effectively becoming noise. In this case, one would expect that the risk decreases monotonically w.r.t. p, as the problem is effectively "without bias and with decreasing variance". And I would believe this is true if we are in either of the following regimes: - The in-sample regime. If instead, we consider how well the model fits the observed X, this "training loss" should decreases monotonically with p. This is in accordance with conventional wisdom that "more parameters will help you fit the model better". - The low-dimension regime. Now consider the generalization (out-of-sample) risk, but instead with fixed N. Convergence is guaranteed as n->\infty so one would pick p=N here, and the expected risk goes to 0 as n->\infty as we will eventually learn the right model.  So it was actually surprising to see that the generalization risk increases as α->β which actually seems to diverge (Figure 1). Which also means the two key characteristics (high-dimensional, out-of-sample) MUST be playing important roles here. After a careful examination one would conclude that the high-dimensionality is killing the generalization risk when α->β. I believe the following point is crucial:  The design matrix X, when α is close to β, is going to be ill-conditioned with high probability (think of marcenko-pastur law of its spectrum). When α=β, it is asymptotically guaranteed (even in the almost sure sense) that X will have singular values arbitrarily close to 0 as n->\infty. This essentially makes estimation very hard.  b) p>n, or the second descent: This observation naturally leads to the second descent as p increases pass n. p>n actually serves as an "implicit ridge regularization" here, since it tries to look for the solution with small l2 norm. This can also be seen from the above point: as p moves away from n, X is becoming better-conditioned again. This "over-parametrization serves as implicit regularization" is a pretty interesting phenomenon, as it introduces bias to the problem (which is essentially non-exist when p<n) but helped the model to generalize better. But I doubt this intuition could easily be applied to say, neural nets, as it seems to be an artifact of linear models with a random design matrix X.  And this probably brings me to the main criticism of this paper, that the authors seem to have used a different PCR model. They assumed that the dimensions of X are independent, and their variances are known a priori. In this case, choosing top-p dimensions reduces to simply pick top-p entries of X with largest known variances. While in reality, PCR is done with performing PCA on the empirical realization X itself. In this sense it is probably more appropriate to name the model as something like "oracle PCR". It's possible that the dynamics of choosing top-p dimensions could be very different between the two settings. Can we still observe the "double descent" in the "actual PCR"? I'm not sure. Of course this setting is harder to analyze, but that is closer to reality. Using the first model specification is fine, but it seems to miss a key comparison (in terms of practicability). The PCR model presented should be compared against a ridge-regularized model, which is something people would probably use in reality. My gut feeling is that a properly ridge-regularized PCR (namely, bumping every singular value away from 0 by an appropriate λ) would a) make the PCR model in the paper "inadmissible" and b) eliminate the "double descent" phenomenon. A proof (or a dis-proof) of this would be interesting and insightful. Given the space constrains I think the authors could simple mention this in a couple sentences if they have an answer ready for it.  ################################################################## This is to acknowledge that I've read the rebuttal. Yes please do clarify the problem setting and the title. And for the benefit of the readers, please add a summary and some intuitions if possible. Thanks!