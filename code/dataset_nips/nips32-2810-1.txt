Update after authors' feedback  Thanks for the clarifying answers. The rebuttal is clear and addresses many points, but not all of them. It looks like new results will be included, which is nice, but I still think the paper is quite dense and could have been several clearer and lighter submissions.  I would have liked more confidence intervals on the results / plots since the improvement or degradation is quite small and could be attributed to randomness. The argument in the rebuttal for the extension of SLU to other architecture is not very convincing and would again still be limited to a subset of models.  Also the angle of the paper (on-device training) is not quite aligned with the experiments carried out (I'd have expected continuous training, starting from a pre-trained model). Yet low-energy training is indeed an interesting topic that should be addressed before we burn the planet with deep neural network trainings. For that reason mainly, I'll raise my score to 7, but emphasis that the limitation of SLU to ResNets should be extremely clear in the final version since the title suggest that the method could work with any architecture.   ------------------------------------------------------------------------  Overall, this paper is well-written and pretty nice to read. I quite liked the effort of constantly linking, comparing and contrasting the ideas to previous results published in the literature. The topic is a very interesting one. Training or adapting models on-device is an important subject to address nowadays, and the privacy-preserving aspect is quite relevant. Yet, it is hard to believe in that setup that neural networks would be trained from scratch. While the authors point out in the paper, and especially in the conclusion, that the explored scenario is not a realistic one, it would have been interesting to see one or two experiments on adapting or fine-tuning a neural network on-device, to illustrate a real use-case, and compare it for example to training only the last layer, which could be a reasonable baseline. In the explored scenario, the training time is never discussed, and would also be a crucial parameter for the applicability of the proposed methods. That said, as I pointed out earlier, the topic of  training of neural networks has raised awareness lately in the media of a quite important issue regarding the energy consumption. This paper could also have been a good contribution, even not limited to the topic of on edge training.  Apart from that, I have some reservations. For it to be considered a definite NeuriPS paper, I would have expected either a strong theoretical foundation, a more in-depth analysis or a truly ground-breaking contribution. I am aware that with the page limit there was little room for more and this paper could easily be split into several ones.   More specifically:  On the stochastic mini-batch dropping:   - the results are surprising and to some extent interesting. I'm not aware of the literature on the topic so my comments might be irrelevant, but I have a hard time understanding why the models should be better with that method than with the standard way and the appropriate shuffling and the same amount of data.   - regarding the randomness, it looks like results are reported from single runs of experiments. Using for example different random seeds and presenting the results averaged over several runs would have made the results way more solid. Confidence intervals would have been appreciated too.    - I did not find the argument at the end of the paragraph on SMD very clear or convincing.   - Regarding the experiments, we don't really see the impact of the learning rate schedule and/or the mini-batch size. That would also have been an interesting analysis to provide    On the selective layer update:   - the author mention that they run experiments with ResNets at several places, but throughout the paper they write about CNNs making it look like all the methods are applicable to all convolutional architectures. Yet I don't see how this method is possible without the skip connections in ResNet, and as such, it should be more explicit that it is only applicable to that family of architectures.   - an analysis of what is learned by the auxiliary RNN, predicting which layers to update would have been extremely valuable here too.     On the predictive sign gradient:   - the experimental part on this aspect is very short. It would have been interesting to see the impact of the chosen precision for the gradients, of the adaptive threshold and for example the proportion of times the full precision gradient is computed (cf. eq. 2), and how it evolves during training.   - a bit more details about E1 and E2 in eq.3 would help to read the part on the prediction guarantee (even though those quantities are defined in the appendices)    Minor comments:    - end of p.7: typo "cn" -> "can"   - the resolution of figures and tables should be improved   - it might not be that relevant to talk as much about the improvement/degradation of accuracy, especially if the experiments have undergone a single run. The improvements are actually quite small, in the same order of magnitude as what is reported as a negligible accuracy loss on p.7. 