As most of the concerns have been addressed by the authors' feedback, I upgrade my score. But I still think the authors should continuously improve this paper, at least integrate the claimed new theory and experiment results in their feedback. ############################################################# This paper proposed a novel energy-efficient training framework for CNN that combine three different techniques, namely stochastic mini-batch dropping (SMD), input-dependent selective layer update (SLU) and predictive sign gradient descent (PSG). The authors show the effectiveness of their techniques with several ablation studies as well as real energy measurement on FPGA boards. Results show that this framework can save lots of energy during training with marginal accuracy drops (e.g. when applied to ResNet-74 on CIFAR10, authors get 2% accuracy drop with <10% energy and 1.2% accuracy drop with <40% energy).  I don’t find the link to the downloadable source code in the paper and supplementary material as the authors claimed in the reproducibility checklist.  Originality:  The three new techniques the authors proposed are novel and well prepared with other existing methods.  Quality: I think the methods proposed in this paper are rather heuristic with limited experiments support the authors’ claim.  SMD: The authors claimed that with the same energy consumed, SMD incur minimal accuracy loss and sometimes even increase the accuracy. However, the authors didn’t show enough experiment results in Section 4. They only showed the top-1 accuracy of CIFAR-10 using ResNet-74, which is not sufficient to support a general result on the whole class of CNN. Moreover, the authors didn’t provide the error bar of the accuracy. I think this conclusion need far more test. If SMD is not consistently better than standard mini-batch training when consumes equal energy, why we need to use it? Some theoretical analysis for SMD maybe better support this conclusion. Moreover, I don’t know the meaning of Figure 3(b). What do the authors want to show? I think beat the accuracy of a larger learning rate standard mini-batch training model in single experiment is not convincing. I think the authors should make this part of experiments solid.  SLU: This method is interesting, but a little counter-intuitive. I’m not sure if this technique is suitable for the general CNN architecture without residual connection, at least SkipNet and stochastic depth ResNet is work for only residual network. The authors should make it clear. Moreover, the effect of adding this kinds of regularization is not clear. I can hardly imagine the training dynamics of this method, though maybe useful in practice.   In the experiment part, the authors didn’t mention the benchmark model and the dataset. I think it’s still ResNet-74 on CIFAR-10? Still, there’s no error bar, no enough results on other kinds of CNN architecture, and I think SLU can have more interesting experiment result, like the implicit regularization result in the stochastic depth ResNet paper, though this paper focuses on energy-efficient training.   Will the adjustment of p_L hurt the performance of stochastic depth ResNet?  PSG: I’m a little confused on the description of PSG. The authors argues that SignSGD need to compute the full-precision of gradient before taking the signs, but I think in PSG, we still need to calculate the full-precision gradient g_w as in Eqn. (2)? And what’s the ``PSG’s predictor’’ in Section 3.3? Throughout the whole paper I don’t find any explanation, I think it’s just the matrix product of x and g_y? Then the whole procedure is predicting the sign of gradient with low-precision input x and g_y first, if cannot determined, then use the full-precision input? As the undetermined elements can be disordered in the whole gradient tensor, and calculate these elements with high-precision vector product can be much slower than matrix product, will this save the computation time (though may save energy, I'm not the expert on hardware)? I think the author should estimate the ratio of undetermined elements, and make this part clear. Authors proved that this procedure have bounded error rate, which is great.  Experiments for PSG are also limited to fixed base model ResNet-74.  Overall, I think the author proposed strong result for energy-efficient training, however I’m not sure  if this result can generalize to models other than ResNet-74 and ResNet-110. The techniques proposed by authors are interesting, but the experiments cannot support their claim well.  Clarity: Some part of the paper are not written well. For example, the PSG part I mentioned above seems confusing. The description of proposed methods is too heuristic, and I can hardly understand the mechanism behind those methods. Moreover, I think the experiment section is not well-organized. The authors should pay some attention on making their results solid and inspiring, not restricted to the accuracy and energy consumed on the two ResNet model. I know the paper's length is limited, but you can make some discussion and show extra results in the Appendix.  Significance: This paper provide a good empirical results for energy-efficient training.