Summary:   The paper investigates the usefulness of modeling human behavior in human-ai collaborative tasks. In order to study this question, the paper introduces an experimental framework that consists of: a) modeling human behavior using imitation learning, b) training RL agents in several modes (self-play, trained agains human imitator, etc.), c) measuring the joint performance of human-AI collaboration.  Using both simulation based experiments and a user study the paper showcases the importance of accounting for human behavior in designing collaborative RL agents.   Comments:   The topic of the paper is interesting and important for modern hybrid human-AI decision making systems. This seems like a well written paper with solid contributions: to the best of my knowledge, no prior work has systematically investigated the utility of human modeling in the context of human-AI collaboration in RL. The results are clearly presented, and the experimental study seems correct. Overall, I find the paper enjoyable to read and to be of an interest to the NeurIPS community, but I also feel that some more experimentation (larger scale user studies with a more diverse set of environments) would be beneficial. For example, it is not clear what type of human modeling would be most beneficial and for what types of tasks one might want to use human models. Nevertheless, I think this paper might be a good starting point for investigating such questions. A few questions, comments, and clarifications for the rebuttal:    a) This paper seem to complement the line of work on importance sampling, that includes experimental studies of human-AI interaction in RL domains. Perhaps a good example of this line of research would be:   Mandel et al., ‘Offline Evaluation of Online Reinforcement Learning Algorithms’  which argues against model-based approaches and in favor or off-policy evaluation, particularly for complex models (presumably this would include human behavior). I think it would be useful to compare the utility of human modeling vs. directly utilizing importance sampling.  How would the two approaches scale with the complexity of the environment, the availability of human data, and the complexity of modeling human behavior?   b) On page 6, the hypothesis test indicates that the main hypothesis is confirmed. However, in Fig. 5a only in two domains the associated error bars are not overlapping. Does the statistical test differentiate between the scenarios? Additionally, do the result of the statistical test hold for Fig. 10a in the appendix?   c) I think more discussion regarding the characterization of the domains for which we obtain a substantial improvement in the joint utility would be quite valuable. From a theoretical point of view, it is not surprising that we can achieve a utility increase if we train an AI agent using a faithful human model instead of self-play (since training with a wrong type of collaborator is like having a wrong transition kernel). However, it is often not easy to obtain human trajectories (e.g., privacy, safety concerns), so it would be great to know a priori if having human model would not be beneficial.    Other than that, the discussion on page 7 is quite intriguing, especially the part about leader/follower behavior. A couple of references that might be useful for this research direction:  Nikolaidis et al. ‘Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration’, Dimitrakakis et al. ‘Multi-view Decision Processes: the Helper AI Problem’.    ---------- Update: I have read the author response. Thank you for clarifying some parts of the paper.  