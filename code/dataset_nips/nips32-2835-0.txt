This paper proposes to use causal convolutions to enhance local structure and a log-sparse attention formulation to reduce the memory requirements of Transformers.  Moreover, different log-sparse variations are proposed, for example local and restart.  The experiments involve exploring different convolutional kernel sizes, the combination of convs and log-sparse, and evaluating the data at different resolutions.  Pros: - Addresses key challenges in Transformers, enhancing locality and reducing memory through log-sparse formulation. - The log-sparse formulation is intuitive as dense on recent history and sparser as history is more distant. - The figure illustrations such as Figure 1, 2, and 3 are extremely informative.  Cons: - The variations in the log-sparse formulation such as local and restart does not seem to be tested in the experiments. - Perhaps provide some plots for the real data against generated sequences to help readers see the challenges in the dataset (such as how frequently do changes points happen) and qualitatively how well the model is able to capture them.