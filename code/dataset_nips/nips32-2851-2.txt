The main focus of this paper is to outline faults in a test statistic given by a previous model (PNAS, 2017) and to design a better test statistic that circumvents these problems.  I think the paper is very well-written and clear.  The paper's contribution is theoretical in nature. It's results are, to my non-expert judgement, original.  Significance: I do think that for the machine learning community, the results are of limited significance, as no new machine learning algorithm is presented nor is a theoretical ML result obtained.  My main concern with this paper is its appropriateness for NeurIPS, as it does not contain any ML results and is only concerned with better test statistics for a case that is rather peculiar: Single-Blind review. This review format is furthermore not widely adoped anyway. One may argue that the paper should be submitted to PNAS, as this is where the original paper has been published. Another point of critique is the lack of empirical evidence: does the new statistic still flag reviewer biases for the dataset considered in the original paper?   Minor points: some small errors in use of articles, in my view, e.g., l.205 --> should be "the problem". L. 38-39: this is not a full sentence.  Possibly missing citations  (but these are much more practical): * Khang et al. 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. NAACL  * Gao et al. 2019. Does My Rebuttal Matter? Insights from a Major NLP Conference. NAACL