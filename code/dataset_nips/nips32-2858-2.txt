I like the experimental approach in trying both auto-encoding (useful for interpolation and imputation) and extrapolation of time series, and encoding the initial condition of the Latent ODE accordingly (respectively, backwards from the last observation when doing auto-encoding, or forward to the last observation before prediction of the continuation of the time series).  While the submission is original and clearly written, the following comments address a few remaining questions:  In the Latent ODE models, how does the RNN encoder handle irregularly-spaced inputs {x_i, t_i}_{i=0,... N_2} or {x_i, t_i}_{i=0,... N}? Does it work like a plain RNN with regularly spaced inputs, as lines 102-103 suggest, i.e., that irregularly spaced inputs are fed to the RNN as if they were regularly spaced? In particular, I was puzzled by the the poor performance of the Latent ODE with plain RNN encoder, on the toy dataset extrapolation task with 20 input points, and wonder this was an artefact of seemingly noisy input data. Fig. 2 in the supplement shows better extrapolation when the initial condition z_0 is conditioned on 80 input points. I am wondering if the ODE would better extrapolate with a smaller state variable, to make the prediction of the initial condition easier, especially given that the modelled dynamics are simple oscillations.  What is g_mu and g_sigma in Algorithm 2, section 2.2 of the supplement, and does this differ from z'_0? My understanding is that the ODE-RNN encoder produces two values z'_0 for each latent variable: the mean and the variance, to sample from using a VAE, but what are these intermediary values?  The Physionet, MujoCo and human activity datasets are all evaluated on auto-regressive RNNs. It is not clear from the paper what are the inputs and outputs of those RNNs - I assume that like in typical dynamical modeling, x_{t-1} is the input, and \hat x_{t} is the output at time t, with a recurrent state of various dimensions specified in section 5 of the supplement. A na√Øve question would be: why not use a simple Neural ODE, with a state variable of the same dimension as the observations, as an additional baseline?  Also, why not use an ODE-RNN as decoder in the Latent ODE? The inputs could for instance consist in the observed values x(t_i), and those input could correct the predictions of the decoder.  Minor comments: There are two references for Sutskever 2014. The bibliography style appears to be wrong (as NeurIPS uses numbers for references, not full names and years) - some space could be gained for more content.