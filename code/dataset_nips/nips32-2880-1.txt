After reading the rebuttal, I tend to agree to changing my review as well conditioned on the following:  1: The structure of the paper should be revised to move the entire validation section to validation. The split validation section (indeed at least half of the validation is presented in the Introduction) is a large detriment to the quality of the paper. 2: An introduction section needs to be added to well frame the problem and mention related work. I don't like the "motivation-as-introduction" section as it does not present a wide scope within which the paper lies. 3: A clearer well presented argument as to precisely how the dataset self-denoising experiment is relevant. The rebuttal does a great job of this and it leaves me convinced of its strong application to real world datasets in general purpose training.    Originality: The proposed method is highly novel and original. Within the scope of interpretability, there are no clear metrics to evaluate input similarity. This is further compounded as neural networks remain opaque which is increasingly becomes an issue as model interpretability becomes a leading concern. The related work is not well fleshed out and could use improvement. For example there may be a connection between the proposed method and influence functions. Consider also that generative models (in particular GANs as well) provide some notion of similarity in latent variable space. This connection deserves some highlighting. Quality: The submission is of mostly high quality. The proposed method has many useful theoretical properties that are well highlighted in the paper with insightful proofs and commentary. The empirical validation is a bit lacking as no clear use case is identified. Clarity: The paper is well written and well structured. It is easy to read and the content is well presented. The proofs are of high quality, and are easy to follow. Significance: The paper is of high significance. I think that the proposed approach and direction are both likely to yield further results. The proposed tool is likely to be useful as a visualization tool "as-is", however it is also likely to be useful as an interpretability tool in further work.