I think that their approximation guarantees are a solid theoretical contribution in the submodular optimization space and I think that this is an interesting theoretical  problem to consider. I also like that they have lower bounds showing that a dependence on the degree is necessary. Further the authors were able to improve results from (Mitrovic et al., 2018) (the authors should feature these results for prominently).   One thing the authors should point out is that their greedy algorithm requires calculating an expectation over all realizations. In practice, this would be difficult to do.  To improve clarity, the authors should formally write out the optimization problem that they want to solve. The fact that there is a cardinality constraint is just quickly mentioned in the text and the pseudocode. It should be written more prominently (preferably the problem statement should have its own definition block).  I think the hardness of approximation results should be n^(-1/(log log n)^C), so that it is a number less than 1.  The authors also write "we develop a novel analysis technique to guarantee the performance of our algorithms." The authors should discuss more about this novel analysis technique. From the proof it is not completely clear the interesting points.  The experiments in the paper are sufficient. However, I don't believe that this approach would be significant in a practical setting where there are user / item features and the distribution of realizations is unknown. I am not surprised that LSTMs do not perform well in the experiments the authors performed, as the experiments were almost purely combinatorial optimization problems.  The authors mention the probabilistic coverage utility function as being weakly submodular. What is the weak submodularity constant?  *** Post Rebuttal *** Thank you for your rebuttal. It would be good to include the (maybe heuristic) derivation on bounding the weak submodularity constant in the paper. 