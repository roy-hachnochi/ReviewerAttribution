Review update:  Thanks for your response. I have read it and I still like the paper. Nice work!  Summary  The paper uses architecture search based on genetic algorithms to find architectures on simple reinforcement learning tasks and MNIST that only have a single shared weight parameters. They demonstrate that these networks perform competitively with hand-picked and trained architectures.   Strengths and weaknesses   The paper contributes to the recent body of papers on architecture search and explores the inductive bias induced by the network architecture in a very extreme case where the network only has a single parameter. Given that most networks currently use standardized architectural components and heavily focus on weight training, this paper offers a refreshing view on different aspects responsible for a good inductive bias. In particular, it's in line with a recent report (https://arxiv.org/abs/1904.01569) that demonstrates that the graph architecture class (i.e. random connect vs. small word etc.) seems to have a relatively larger influence on the performance than the weights of the network.   Apart from a few minor comments the paper is clearly written, and represents a significant and original contribution to this line of research.   I think the paper has two main weaknesses:  1. The strategy is obviously only applicable to very small problems/networks. A demonstration that networks composed of repeating/recursive simple network motifs can solve next level complex problem such as CIFAR, would strengthen the paper.  2. Apart from the demonstration that a single weight parameter is enough, the paper offers little insight into the quantification of inductive biases by architecture. For instance, how many networks perform well? Do well performing networks differ strongly (in their components and the class of functions they implement)?  Minor comments  - Is there a reason why you don't use a delete operation? - Table 1 doesn't say what the number is. I guess reward, but it would be good to say it explicitly. - A bit of information on how the VAE for MNIST was trained would make the paper more self-contained.  