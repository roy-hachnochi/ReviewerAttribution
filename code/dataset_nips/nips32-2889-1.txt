The method proposed in the paper seems original to my knowledge. And the significance of the problem the paper trying to address is also descent in my view. I'll focus on the quality and clarity of the paper in this section.   First the paper does an outstanding job on clearly stating the problem in the introduction and giving readers a detailed, comprehensive cover on the related works. But starting at the 3rd section, the paper falls short on the clarity on the method, especially the section that related to the bayesian optimization.   More specifically,  1. In Eq (2), does the classifier C has anything to do with the classifier that being tested? 2. Line 173, the definition of L_c is not sufficiently clearly. The authors say "Loss is the classification loss", which is overly general. Does it has to be minimized for better classifier performance or maximized (it can go either way depending on what you use)? From the later section it seems it need to be minimized for a good classifier performance. But I would recommen clearly define what type of loss can be used here. Examples would be better.  3. The method section lacks of sufficient detail for the other researchers to fully understand the model. I know that the author provided the code in the review process but it does not reduce the importance of including sufficient detail of the method in the paper. More specicially, a). Eq(4) contains a set of perviously found examples, how large should this set be? Does the size of this set affect the performance of the proposed method?  b). What is the stopping criterion for the bayeisan optimizatoin? Does it has to reach a specific number of samples? Does the loss has to be non-increasing a certain number of iterations? What would be the rationale of the choice? c). The process of bayesian optimization is very vague. The paper states that the loss is modeled as a GP. And all the other thing the authors says about the process is they use RBF kernel and EI as acquisition function. I would suggest more details on how the GP models the loss function dynamics. Related equations might also be beneficial here.  d). The authors state that the image data corresponds to theta, which is a one-hot vector for genders and races. In the bayesian optimization section, does the optimization also gives one-hot vector as resulting paramters? If yes, how does the optimization algorithm constrained to generate only one-hot vectors rather than vectors in real numbers? If not, why it is reasonable to use this generated vector as input to the generator? Since the generator always takes one-hot vector as input (as shown in Eq(1)), it might perform poorly if given a vector otherwise.   Pose rebuttal: thanks for the response from the authors. Their answer is clear to me. I would recommend including these explaination to the paper which will definitely improve the clarity and quality of the paper. 