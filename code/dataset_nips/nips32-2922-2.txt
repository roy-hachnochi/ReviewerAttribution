 Originality: The paper proposes a very novel technique to hijack a physics simulator at each sample statement and to couple it with a PPL inference engine that works closely with the original simulator to produce high quality proposals to guide inference. A number of important concepts are introduced for improving Inference Compilation (IC). These include - The concept of prior inflation to ensure that the training data for IC is diverse enough. - The concept of identifying the last accepted sample in a rejection sampling loop and using that for training the IC. - Identifying certain variables that are better off for ancestral sampling (line 180-181). This seems like a useful concept, but it would have been stronger if the paper gave some rationale or examples of these types of variables to help guide PPL researchers. - A language agnostic protocol that links any simulator code to any PPL code.  Quality: There are a number of unsupported claims:  The paper claims on line 51 that their work can lead to "discovery of new fundamental physics." It's unclear how this claim is justified. How do we learn new physics by studying the predictions of a simulator which has been coded up with known physics? The authors need to provide some examples or explanation supporting this claim.  The paper claims that they have developed a protocol comparable to ONNX. This is a very tall claim for what is effectively an RPC with 2 or 3 function calls.  The paper claims on Line 143 to have designed universal PPL inference. No further details are given about this PPL other than the fact that it supports sampling and conditioning. That level of information is not enough to understand this PPL or to ascertain if it's indeed universal.  Line 238-241 is an unsupported generalization. "Differing from the conventions in the probabilistic programming community, random number draws in C++ simulators are commonly performed at a lower level than the actual prior distribution that is being simulated. This applies to SHERPA where the only samples are from the standard uniform distribution U (0, 1), which subsequently get used for different purposes using transformations or rejection sampling." For example, see reparametrization in Stan: https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html  Clarity: There are a number of aspects of the paper that are not very clear:  - The exact interface for conditioning on data needs more explanation. Line 192 indicates that the exact address of each observe statement is supplied over the execution protocol. What is unclear is how does one map real world observations such as calorimeter readings to specific addresses in the simulator code. In a Universal PPL an observation could come from potentially different addresses depending on the execution path in the trace. The traces in the supplement show all the observations at the very end and from the same fixed addresses. This doesn't seem very general, or at the very least, there is no explanation of such a restriction in the text.  - Line 190 which mentions the address embeddings doesn't seem to provide enough technical details to reproduce it. For example, word embeddings are learned from the context that they are observed in sentences either a bag of word context or a sequence context -- no such details are provided for these embeddings.  - No rationale is provided for "controlled" (line 180) vs uncontrolled variables.  Significance:  The work is significant in providing an interface between PPL research and physics simulation as well as raising important considerations for PPL researchers in particular for the emerging area of Inference Compilation. It is possible that a researcher may simply be able to plug in a new PPL into one of these simulators and get better results without knowing anything about particle decay. This is of major importance.  ** POST REBUTTAL **  I'm still not convinced that there are enough technical details in the paper.  - For the "discovery of new fundamental particles" claim, I'm not a physicist so I don't understand this explanation, but I will grant the point to the authors.  - The main issue remains about the lack of specification of the pytorch-based PPL for example how observations are specified how they deal with multiple control flows in which the same addresses that were observed in one trace may never be hit. Most PPLs specify some kind of abstractions to deal with this issue, but there is nothing concrete in this paper. And as I said above the control flow examples shown have all the control flows of the observations being identical. It is possible that they are some simplifications in their PPL design. This is where more details would have helped.  Anyway, there is nothing in the response besides a promise to provide more details, which is not sufficient for something that goes to the crux of the paper. 