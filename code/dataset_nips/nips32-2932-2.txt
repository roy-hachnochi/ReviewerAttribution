This is a well-written and carefully researched paper on what factors in a paper are correlated with it being more or less reproducible. Reproducible here means the authors were able to independently author the same algorithm and reproduce the majority of the claims as the original paper. The statistical analysis is careful and complete, and the discussion of the results is nice.  It would be interesting to note not only whether the paper was reproducible, but how long it too to reproduce (scaled by "size" of the thing being reproduced. Perhaps something like days to implement divided by lines of code in the implementation.  The authors mention that extracting the features per paper took only about 30 minutes, but didn't say how long implementation took, other than mentioning one case that took over 4 years. It would be interesting to see the distribution of how long it took to reimplement.  It would be nice if the paper had a bit more discussion about how it was decided whether an implementation successfully reproduced the claims of the paper. I'm assuming you did not require getting exactly the same results down to the hundredth of a percent, but presumably required getting "similar" results? Some more clarity here would be helpful.  There's really nothing wrong with this paper. I think it's a good, solid contribution and valuable to the field. The only potential issue is whether it may fit better at a different conference focused more on the science of science, but really I think it fits fine at NeurIPS too.  I read the author response and agree with their assesment that it is reasonable to publish this paper at a conference like NeurIPS due to the ML focus of the papers being reproduced.