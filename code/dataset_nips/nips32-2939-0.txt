Major * I found this paper to be very exciting, presenting a promising methodology addressing some of the most critical  bottlenecks of Bayesian Optimization, with a focus on large data sets (being therefore relevant for high-dimensional  BO as well, where sample sizes typically need to be substantially increased with the dimension).  * With respect to the dimension indeed, several questions arose with respect to the considered hypercubes:   * Page 4: Each trust region TR_ell with ell in {1,...,m} is a hypercube of side length L_ell \leq 1, and utilizes   an independent local GP model. So, one is far from filling the space, right?   * Page 4, about Lmin=(1/2)^6: could some more explanations be provided on the underlying rationale? * Page 4, equation just before the start of Section 3: why randomizing when the whole distribution is known and tractable? * Page 5, about "We replaced the standard acquisition criterion EI used in BOCK and BOHAMIANN by TS to achieve the required  parallelism": but there exist several ways of making EI batch-sequential (see for instance a few papers dealing with this:    Marmin et al. (2015): https://link.springer.com/chapter/10.1007%2F978-3-319-27926-8_4  Gonz√°lez et al, (2016) http://proceedings.mlr.press/v51/gonzalez16a.pdf Wang et al. (2019): https://arxiv.org/pdf/1602.05149.pdf   Not using these for some good reason is one thing, but putting it the way it is put here sounds like it is not possible to  go batch-sequential with EI... * In the main contributions presented throughout Section 3, two main ideas are confounded here: splitting the data so as to  obtain local models AND using TS as infill criterion. Which is (most) responsible for improved performances over the state  of the art?  Minor (selected points) * Page 1: What does "outputscales" mean?  * Page 2, about "For commonly used myopic acquisition functions, this results in an overemphasized exploration and a failure  to exploit promising areas.": better explaining why and/or referring to other works where this is analyzed in more detail would  be nice.  * Page 5, syntax issue in "Note that BFGS requires gradient approximations via finite differences, which is a fair comparison  when the number of function evaluations is counted accordingly. * Throughout Section 3: is the (log) loss introduced?   ******** Update afer rebuttal **********  I am happy with the way the authors addressed reviewer comments in their rebuttal, and while several points raised by the reviewing team give food for thoughts towards follow-up contributions, I feel that this paper deserves to be published in NeurIPS 2019. I do not increase my score as it is already high. 