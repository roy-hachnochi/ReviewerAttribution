This paper provides a way to divide very large Transformer-based deep Neural Networks to several modules in an efficient way for training. Transformer-based language models require a huge computational cost and time, therefore model parallelism is required if a model is too large to fit in a single computing device. However, the basic Transformer training requires to wait for the previous layers' gradients to compute the current layer gradients. Therefore, some GPUs becomes idle during training when we split the model into multiple GPUs. The proposed accelerating training method enables Transformer-based language model parallelism by avoiding the backward locking.  The novelty of the idea and the contribution of the theoretical analysis is somewhat limited, but overall this paper shows the great progress on the parallelization on Transformer-based Language model training. 