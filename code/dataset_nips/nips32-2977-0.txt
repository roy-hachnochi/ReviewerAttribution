The paper shows that sum-product operation using tensors (equivalent to so-called tensor networks) can be viewed as a generalization of previously used low-rank / factorization techniques and also such techniques as depth convolution. This is quite interesting. The problem is that tensor network approximation/factorization is typically used with fine-tuning, that is when the architecture is really useful (you first do approximation using SVD-type algorithms of pretrained network, and then fine-tune). Here the layer is introduced more formally, and a huge extensive GA search is applied "on top of it", which in my opinion, does not add to the contribution.  However, I think the idea is interesting, and the effort spend by the authors is huge and shows that such layers are working, but does not give any hint for larger datasets (other than CIFAR). 