Summary of the paper: This paper proposes a distribution aware quantization which chooses between recentralized and shift quantizations based on weight distributions in the kernels.  The end-to-end compression pipeline, when used on multiple architectures, shows good compression ratios over the original models. Finally, the new implementation proposed utilizes the maximum hardware optimization than the earlier proposed methods.  Originality: The ideas in general based on distributions of weights have been around but the proposed methods is novel in the way it uses the weight distributions in the sparse CNNs to get optimal quantization strategies. This work is novel and grounded to the best of my knowledge.  Quality: The experimental results are good but can be improved by the suggestion following in the review.  The idea is simple and easy to follow. I think it is a good quality of work which will help the community.  Clarity and writing: The paper was easy to follow and understand barring a few minor things: 1) The citation style is different from the normal style of neurips for eg. [1], [3,4,5]. Instead, the authors have used abc et al., format which takes up space. Please consider moving to the other format. 2) In Figure 2, I am not sure what / 128 means. It would be good to explain what that means for the ease of the reader. 3) The gap between table1 and its caption and table 2 and caption are very low. Please reformat it to make it look clean. 4) Please change the name of the paper from "Focused Quantization for Sparse DNNs" to "Focused Quantization for Sparse CNNs" in the manuscript. I think that is a mistake overlooked while submission.  Positives: 1) Good idea and nice utilization of weight distributions of sparse CNNs to get the right quantization technique based on bit shifts.  2) Extensive experimental results and good ablation study.  Issues and questions: 1) The only major issue according to me is that not all compression techniques provide good computation gains as well due to a lot of constraints. In order to make any of the claims made in the paper about efficiency, it would be good to have inference time for each of the models reported in table 1 and table 2. - this will make the experiments stronger 2) In conclusion, there is a claim about savings in power efficiencies for future CNN accelerators in IoT devices. Given IoT devices mostly are single-threaded it would be good if the authors could provide an example of what they are talking (expecting in the future) about and to make it even stronger a real-world deployment and case study will make the papers results stand out.  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- I have read the author response thoroughly and I am happy with the response. The response addressed my concerns and I would like to see the #gates for the 8-bit quantized W+A in Table #4 in the camera-ready. I also agree with the other reviewers' comments and would like to see them addressed completely as done in rebuttal in the camera-ready.