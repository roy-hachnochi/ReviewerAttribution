This paper proposes neural network models to represent stepwise cognitive processes similar to humans' for visual understanding to deal with VCR dataset.  The idea of their methods is very interesting and the results are good enough to outperform other state-of-the-art methods.  However, this paper would be better if the following issues are improved. Firstly, the use of notation is rather confusing and not consistent.  It would be nice to write it more concise and consistent.  Secondly, as reported in [38, Zellers et al., CVPR'19], GloVe is used as language models for the comparative methods in Table 1. Since there is no comment about them, it may mislead that all of the other models adopt BERT same to the proposed models. It would be better to explain explicitly and report the performance of both cases.   Thirdly, there is a lack of explanation in some experiment settings. How is 'Rationale' configured in each mode? For example, in case of QA->R,  will A be concatenated with Q in textual form after finishing Q->A?  If yes, the latter characters can be discarded depending on P and J values, what values are used for them?   Optionally, it will be helpful to report the generalized performance by showing the results on well-known Visual QA datasets such as VQA 1.0 and 2.0.   Here is minor comment: In reference, the order of authors in [32] is not correct with the original one.  Also, some commas are missing between the author names. In eq.3, k' in the denominator is j'.