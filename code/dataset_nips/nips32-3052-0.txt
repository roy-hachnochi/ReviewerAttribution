The authors propose a learning algorithm for adviser who solves Meta-MDP. During the learning of the agent who solves the specific task, the advisor advises the agent to take some action u and it is chosen with probability epsilon. The authors show the advantage of their method compared with MAML in the tasks Pole-balancing, Animat, and Vehicle Control.  ------ Comments after Authors' feedback -------------- Thanks for your feedback. I still cannot get rid of my concern. So I will keep my score.  > The exploration policy (the epsilon part) is the one learned by the advisor, which as shown in Figure 6, is not simply a general policy to  solve many tasks.  If my understanding is correct, what the authors compare with the proposed method is the task-specific exploitation policy. I believe it is much more reasonable to compare the adviser with the policy that is trained on all the past environments, which will presumably be robust to the change of the environment. I am wondering if such a simple domain randomized policy works as an adviser., and am also wondering if the proposed stochastic selection of the action is a really good method to convey the knowledge of the adviser. So I'd like to request the authors to compare the proposed method with the one that use a domain-randomized policy as an initial guess of new task to validate the proposed method actually works better than this simple baseline.