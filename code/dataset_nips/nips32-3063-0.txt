Update: I apologize for my confusion about the dynamics. I feel more positively now about this work, and have increased my score.  1) For this work to have a lot of value to neuroscientists, I'd want to understand the realism of the dynamics much better. There are two issues here to be addressed:  a) how realistic is it for the dynamics of the feedforward pass + recurrence within layers to run to convergence *before* sending down the top-down feedback? What happens if these are concurrent processes, such that units get both bottom-up and top-down inputs at the same time?  b) The discussion about inhibitory feedback being delayed relative to excitatory is correct, but suggests a tight constraint: the biology data tell us how much time elapses between these two feedback types. Given the time-scale of the recurrent dynamics in cortex, the authors could then ask (in their model) whether this delay is "enough" for their push-pull mechanism to work. If yes, that would strengthen the result a fair bit.  2) There are other students of hierarchical RNNs, and it is worth discussing the details of those works (there are many) to highlight the novelty of this paper. So, for example, the 2nd sentence of introduction is correct (most DNNs are just feedforward) but a bit misleading (because there are several recent papers about hierarchical RNNs).  3) If the unit activities are sign(input) (e.g., Eq. 1), couldn't the pattern overlap (eq. 6) be -1, in the extreme case where two patterns are opposite? In that case, I disagree with the inequality given after Eq. 6, of 1>m>0. This could be easy to fix (or, in case I am wrong, please just tell me why).  4) It's neat that the dynamics with feedback resemble the monkey data on success trials, and the dynamics without feedback resemble those on unsuccessful ones (no contour recognition).  5) I don't understand how the system is implemented for the natural images. It seems to me like, to define the feedforward connectivity, you need to know the "parent" and "grandparent" patterns corresponding to the "child" patterns. That's straightforward (by construction) in the first part of the paper, where you define distributions for these patterns. But for the natural images, I don't understand how you know the parent and grandparent patterns. E.g. what is the "parent" pattern for the dog images? I get that it has something to do with dogs, but I don't see how you determined the specific numerical values.