- The macro disentanglement resembles a cluster assignment process, and the micro disentaglement (encourage independence between dimensions) is a ordinary method for learning disentangled representation. However, the whole framework makes sense to me, and the use of Gumbel-softmax trick and cosine similarity is also reasonable.  - It'd be better to show visualizations of baselines (e.g. MultDAE) in Figure 2, so that we can see the comparison. As learning such an item representation (distinguished by category, like clustering) is not hard. The micro disentanglement (Figure 3) is interesting, but the quantitative measurement is missing.  - I'd like to see more experimental analysis, like ablation study of the macro and micro disentanglement (e.g. set K=1 to remove macro disentanglement).  - Is there a reason to account for the superior performance, especially on sparse data? Maybe the proposed macro-micro structure alleviates the data sparsity problem in some way? It might be nitpicking that line 218 says "consistently outperforms baselines" which is not exactly true.  - The main concern I have is the lack of baselines, as it only compares with two methods from a recent work[30], but there are many CF baselines like BPR-MF are missing, and they often show competitive performance.  --- The rebuttal addressed most of my concerns, hence I decided to raise my score.