The paper proposes a way to derive graph kernels from Graph Neural Architectures, building on similar results on Neural Tangent Kenrels. the proposed formulation allows for a deeper analysis compared to GNNs: the paper proposes a sample complexity analysis on functions of a certain form.  page 3, section 3.1 line 118. If I'm not mistaken, the \theta inside the f function in the definition of u(t) should depend on t. Otherwise, u(t) does not depend on t, and it looks weird.  Experimental results: results are not astonishing. The proposed method works better than competitors on COLLAB dataset. For other datasets, the variance is pretty high and the results are within a standard deviation from baselines. A statistical test may help in interpreting the experimental results. Also saying that the proposed method is always comparable with the neural counterpart may be an interesting result. On the other hand, the proposed method remains always comparable to GNNs (usually performing slightly better), that is a good thing.  Page 8, line 243: you state that Figure 3 shows that jumping knowledge improves the performance of GNTK in both IMDB and NCI Actually, it looks like for IMDB Jumping knowledge doesn't change the results much, while for NCI1 it slightly improves the performance of the best GNTK architecture, but we're probably speaking of 0.002 points in accuracy at most (extrapolating from the plot).   Computational complexity: experimental results would have benefitted from a computational complexity analysis / running time comparison between the different methods. Usually Graph Kernels + SVM are faster to compute compared to GNNs, that require GPUs to run efficiently. One of the main points of the present paper may be of providing kernels with generally comparable performance compared to GNN, but with a faster implementation. However, I could't find any reference on computational complexity in the paper.  Authors state in the reproducibility list that the release of source code and of the adopted datasets is not applicable. I don't agree with that, however authors did include the source code and the datasets as supplementary material. Thus, I assume is in authors' intention to release the code if the paper gets accepted.  Minor:  page 1 line 27: "worse practical performance" : compared to what? page 7 line 225: IMDBBINARY -> IMDB-B Figure 2 is not readable in black and white printing Conclusions are missing  ####AFTER REBUTTAL I acknowledge that authors responded in a satisfactory way to my comments.