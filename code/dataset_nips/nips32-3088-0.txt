Well motivated, well-executed demonstration of permutation based language modeling for both autoregressive and bidirectional purposes  The permutation strategy is new and solves an important problem around the split between autoregressive and bidirectional models  This is a complete work, but I would have liked more analysis demonstrating how the new methods change the model dynamics, attention patterns over memory and context, or some empirical attempt at explaining how the new methods create better representations.  It is clear and well-organized  Many are likely to rely on these ideas or similar ones. This does belong in the thread of literature that succeeds BERT.  Update:   Thank you for including the attentional analysis. If you have the opportunity to examine which tasks are affected most by transfer and can shed light on which examples are correct by XLNet and not by BERT, that might provide valuable insight into what these pretraining procedures are learning.