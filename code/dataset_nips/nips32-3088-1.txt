The paper proposes a clever solution to bridge the gap between training and fine-tuning a language model (i.e. BERT with masked LM objective). The proposed permutation LM allows the model to look at both left and right context, which has been shown to be beneficial for downstream NLP tasks. To achive this, the paper proposed two-stream self-attention, in which one self-attention layer prevent the  current word attend to itself (thus makes optimization become trivial, the model just learn to copy). The authors evaluate their model on downstream NLP tasks including some challenging tasks such as RACE M/H, Yelp-5, SQuaD 2.0. In all experiments, XLNet is fairly compared with BERT (Large and Base) and the results show that XLNet performs better than BERT on downstream tasks.  The paper is well-written and easy to follow.