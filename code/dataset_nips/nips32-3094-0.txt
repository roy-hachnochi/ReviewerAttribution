This work deals with the softmax bottleneck problem, which is noticed by Yang et al. (2017). Departing from the mixture of softmax used by previous work, which is computationally expensive, this work proposes to elementwise gating technique, which could potentially get around the softmax bottleneck problem by taking one single softmax computation. The gating vectors are efficiently computed with a series of sigmoid functions organize in a tree structure, and gate sharing is used to further promote efficiency. Empirical evaluation on machine translation and language modeling shows that the proposed method is able to achieve comparable accuracy to the mixture of softmax baseline with much less computation cost.  Overall I think this is a solid work: it clearly presents an interesting idea, which yields strong empirical performance.   Details: - Broken pointer in line 132.  - Line 115, on the rank of \hat{A}_{Mistape}. If I understand it correctly, $\Pi_k$ matrix and elementwise product could bump the rank up, due to rank(A\odot B) \leq rank(A)rank(B). Therefore a Mixtape model with K=1 could potentially yield a high-rank log probability matrix. Have the authors considered comparing to this baseline?  - Adding onto the above point, I suggest the authors tone down a bit the argument that it is `therefore high-rank`, unless a lower bound can be derived (which I think is tricky to do). And it would be interesting to empirically see its rank in practice.