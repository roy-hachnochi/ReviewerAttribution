The paper is original. The authors, despite similarities to LSGAN (Mao et al., 2017) do provide generality over the technique and provide theoretical guarantees about this. Overall, the paper quality is Okay. While there was some work to defend their claims, they are not fully backed up. First, they use discrepancy as both a loss function and a measure of performance. Of course their model would perform better on that metric: it's been directly trained to do so. A loss function is supposed to be a stand-in approximation for the true task for the model (E.g. no one is optimizing models for the end goal of reducing BCE loss, but rather, increasing accuracy or F1-scores on a test set), comparing losses directly is not useful. A better result would show the model performing better on some other metric (e.g. average performance of identically initialized networks on a test set trained only using generated data). Also a concern: selection of the hypothesis sets for the approximation of discrepancy, as far as I can tell, is not addressed beyond general phrases like "the set of linear mappings on the embedded samples." (This is excepting, of course, the linear case where they show it to be twice the largest eigenvalue of M(theta)) There should theoretically be an infinite number of linear mappings on the embedded samples, so how do they subselect? That would be something I'd like seen in the paper, or, at least, made more clear.  Other small issues with the paper include some grammatical mistakes.   