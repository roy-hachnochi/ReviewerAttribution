The paper extends HER to visual environments where unsuccessful trajectories can be hallucinated to appear successful by altering the agentâ€™s observations using a generative model.  While the idea presented in the paper is interesting, I have questions about the scalability of this approach. Specifically, the approach requires training a generative network to produced observations that contain the goal from the environment. I am interested to know how the authors intend to scale up this approach to more complex visual domains like Atari, DeepMind Lab etc. In these complex visual domains hallucinating a goal image that lies within the space of observations of an environment seems difficult and is unclear from the approach presented in this paper.  After reading the rebuttal: The main strength and weakness of the paper are as follows (from my perspective):  * (strength) the authors introduce a generative approach for applying Hindsight Experience Replay (HER) in visual domains: the idea is simple and has the potential to improve our current Deep RL methods. * (weakness) currently, the paper does not seem to have a detailed discussion on how their generative model was trained to produce images containing the goal information. The authors do clarify this on their feedback and it would be useful if they also add this discussion on their next version of the paper. More importantly, including this discussion is useful for the Deep RL community. * (weakness) their current approach of training the generative model relies on manually annotating the goal images, which may prevent scalability of the algorithm. Addressing this could make their approach be more impactful.