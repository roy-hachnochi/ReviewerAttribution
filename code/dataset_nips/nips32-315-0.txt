The paper formulates the problem of optimizing a strategy for crawling remote contents to track their changes as an optimization problem called the freshness crawl scheduling problem. This problem is an obviously important problem in applications like Internet search engine, and the presented formulation seems to give a practical solution to those applications.    The paper presents an algorithm for solving the freshness crawl scheduling problem to optimality, assuming that the contents change rates are known. The idea behind the algorithm is based on the deep understanding of statistics and continuous optimization, and it seems to me that the contribution is solid (although I could not very all the technical details).   For the case where the contents change rates are not known, a reinforcement learning algorithm is presented. Basically it estimates the rates as their MLEs. As a learning algorithm, the approach is not non-trivial, but I think it is reasonable.  The convergence of the algorithm is guaranteed, and it is also valuable.  The authors provided experimental results as well. They crawled 18,532,326 URLs daily over 14 weeks. It shows that the presented algorithm scales. This is an important feature because the web crawling requires processing a large number of sites.  To conclude, the paper presents reasonable formulations and algorithms for the important practical problem. The technical contributions are solid from aspects of both theoretical optimization study and practical engineering work. The paper is written carefully and is easy to follow. I slightly worry that this paper is not very suitable for presentation at machine learning conferences (to my opinion, this paper is more suitable for data mining or web conferences), but I think that the paper is a good contribution even for Neurips.  Update: After reading the author's rebuttal, I still maintain my score.