I would like to thank the authors for very clearly written paper.  I have a few small points which could potentially improve the write-up.  Page 3, 1. Generality, “the regret of OGD on m_t convex G-Lipschitz losses has a well-known upper bound of” It may be useful to have the reference to this well-known upper bound..  “The niceness of \hat{R_t(x_t)} makes the analysis tractable” May be the authors mean here the case of the OGD? Would the form of \hat{R_t(x_t)} be nice in general?  On the line 108 -> 109, it’s not clear how \hat{\bar{R_{T}} \leq o(T) + min_x 1/T sum_t \hat{R_t}  In experimental section, authors test their optimization method against Reptile with Adam. I think another sensible baseline would be Reptile+Meta-SGD, which makes the learning rate learnable, thus making it a more fair comparison to ARUBA which uses the adaptive learning rate.  For FedAvg case, it would be interesting to see whether the improvement comes from the meta-learning treatment of the federated learning problem (where we optimize for the initialization) or the ARUBA algorithm itself (e.g. the fact that the learning rate is adaptable). 