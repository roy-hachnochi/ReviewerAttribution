The main novelty in the full-information part of the results is the black-box reduction to the confidence-based framework. The main intuitive idea is that confidences are generated for experts based on how good they have looked in the past/how bad regret has been with respect to them so far (Step 7 of Algorithm 1), and the confidence is multiplied by the probabilities generated by the actual expert algorithm *with a static regret guarantee*. The algorithm and analysis provides a conceptual look into how switching regret minimization can be achieved and is interesting. Proof (of Theorem 3) appears essentially correct.   These ideas also yield new results for the sparse bandit version of the problem, which has seen recent progress. I did not check the proofs in detail, but it seems that the building blocks seem a bit easier to achieve than the original expert-based approach (here, they are achieving worst-case static regret, and worst-case switching regret over 2 actions, as opposed to directly achieving worst-case switching regret over K actions). It still requires sophisticated regularization with OMD "one-sided log barrier", which I have not seen in prior work and appears to be a technical contribution.  Proofs appear correct (although I only checked in detail for full-information) and formally written. The submission is also nicely contextualized in related work.  While the results are nice, I am not sure about the broad appeal of this problem to people in the NeurIPS community outside of the online learning community; so it would be nice to hear from the authors on whether they think the regret guarantee and this problem more generally has scope in any practical machine learning application.