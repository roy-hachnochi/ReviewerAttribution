The authors perform system-level optimisation of (block) coordinate descent algorithms on multi-core NUMA architectures. They isolate critical bottlenecks, propose both data structure algorithm modifications to overcome these limitations, and explore trade-offs in extensive experiments. A theoretical convergence proof is given for strongly convex functions, and simple to implement tuning rules for the free algorithm parameters are suggested.  Overall, I found this paper interesting and refreshing. The experiments are thorough and the improvements suggested by the authors are well motivated. The resulting performance is impressive, with significant speedups compared to many of the standard production codes that are widely available. The theoretical contributions are perhaps not groundbreaking (key algorithm components and convergence proofs already exists in the literature), but the system-level optimization is a major contribution.   I do not really have that many comments or criticism on this paper, other than that it would have been nice to scale the experiments to even more threads. Now, scaling appears to be linear and there is no clear diminishing return. Have you tried running even more threads? When does the linear scalability break down?  I would also encourage the authors to move Remark 1 from the supplementary into the main text. The additional tuning parameters was one of my main concerns before shifting to read the supplementary.   ** Update after rebuttals **. Thank you for addressing my concerns in your rebuttal letter. I look forward to reading the final version of this paper.