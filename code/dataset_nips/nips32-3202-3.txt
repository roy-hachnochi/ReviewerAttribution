This paper proposes a perturbed Riemannian gradient descent method (PRGD) for minimizing a smooth and nonconvex function over Riemannian manifold. It sheds light on understanding RGD and its capability for escaping from saddle points.   There are two main concerns about this work.   1. The algorithm depends on many unknown parameters which make it impractical, although other works such as Jin et.al. in Euclidean setting have the same issue.   2. This is more serious. The algorithm design and proof techniques are largely the same as the ones in previous works by Jin et.al. The same topic has also been studied by Sun et.al. on submanifolds of Euclidean space. As a result, it seems that the only contribution of this paper is to formalize or generalize these techniques to more general manifolds. Therefore, the contribution is a bit thin.   ----------------------------------------------- Comments after rebuttal: I appreciate the authors' detailed feedback, which clarified some of my previous concerns. I thus increased my score. 