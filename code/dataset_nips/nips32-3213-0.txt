The theoretical study of instance shrinkage in pegasos is as far as I know novel and interesting. Specially interesting is how instance shrinkage does not affect the solution the model converges to, which justifies later experiments which ignore importance sampling in deep nets.  Similarly, the idea of training a small assistant model just to predict the loss of the base model on unseen examples is straightforward and potentially useful. The algorithm is clearly described, including all hyperparameters, and it does look like it should be possible to replicate the experiments.  It's unclear from reading the experimental section, however, that this algorithm is actually an improvement over just regular training with no curriculum attached. No experiment in the paper with a deep net shows a learning curve for training with no curriculum, so it's unclear whether adding an assistant actually helps at all, or by how much. The paper is also fairly light on how the hyperparameters were tuned; ideally we'd see that the hyperparameters were tuned on baseline SGD/momentum to minimize convergence time to a given target accuracy; then we'd plot the performance of the autoassist method next to the performance of the tuned SGD/momentum baseline and other curriculum learning methods.  The paper also completely omits any analysis on how different characteristics of the assistant model affect the performance of autoassist. Some questions I'd like to understand better are:  1. How simple can the assistant model be to still observe an improvement in training time?  2. How aggressive can the example filtering be until convergence is affected?  3. What is the ideal complexity of the assistant model? Does it make sense to have an assistant almost as complex as the base model itself?  4. Is the behavior of auto assist affected by the batch size used?  5. Some learning tasks (like cifar) are often overfit by state-of-the-art models, while others (like lm1b language modeling) are underfit; does autoassist behave differently in the overfit vs underfit regime? (specifically, does autoassist degrade in performance as the training loss goes to zero on all training set examples, as happens on large nets on cifar?)  As the paper currently stands it's hard to judge whether autoassist is an improvement even over baseline SGD/momentum, and it's impossible to tell how autoassist would behave in practice. I don't think all questions above need to be answered to make this paper acceptable, but some information around more of them would be very helpful.    ---- after reading author feedback  I revise my score to 7, assuming the authors clarify the paper to cover the points I raised.