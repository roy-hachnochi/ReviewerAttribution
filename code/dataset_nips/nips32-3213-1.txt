Much of my comments are reflected in section 1 (Contributions) and section 5 (Improvements). Just provide more points here:  1. In general, the problem this paper wishes to address is super important, especially when we are in the era of big model + big data (such as BERT). The point out from which this paper tries to solve this problem is also making sense: try to abandon the useless data points and only focus on the important one. In particular, I like the idea of using shallow model as a proxy to the true "utility" and cast the training process as a binary classification task.   2. The paper is also presented in a reasonably clear manner.  3. I have major concerns towards the experimental setup, most of which could be reflected in section 5 of this review. Apart from these points, the improvements seem not that significant, especially on the WMT14 dataset (a larger scale one) in Fig.4. And I would suggest that moving Fig. 6 in appendix to the main text since we care about the final evaluation measure (e.g., accuracy and BLEU) much more than training loss/ppl.   4 Several representative related works are ignored in the paper. Indeed there is a rich literature that talks about using importance sampling to boost ML model training that is largely missing. For example, please check paper [1-4].   [1] Loshchilov, I. and Hutter, F., 2015. Online batch selection for faster training of neural networks. arXiv preprint arXiv:1511.06343. [2] Fan, Y., Tian, F., Qin, T., Li, X.Y. and Liu, T.Y., 2018. Learning to Teach. ICLR [3] Tsvetkov, Y., Faruqui, M., Ling, W., MacWhinney, B. and Dyer, C., 2016, August. Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 130-139). [4] I also remember a paper appearing in NIPS 2017 about dynamic and automatic batch selection towards NN, but sorry for that I do not remember the exact name. Will update the review once I found that paper.  ________________________________________________________________________________________________________ Post rebuttal:  I thank the author's response. I still do not think that a paper claiming faster training convergence could establish itself in NeurIPS, if 1) no convincing results on large scale tasks are demonstrated; 2) no thorough discussion/comparison with the literature is provided. 