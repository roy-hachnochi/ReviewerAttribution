 I have read the author response and have decided to raise my original score to a 6. Please do correct the erroneous statements regarding upper bounding the log marginal likelihood of the data and also regarding claim that maximizing the lower bound maximizes the log marginal likelihood of the data.   Originality: The proposed method builds marginally upon SIVI [1]  Clarity: Clarity is lacking...the abstract for example is obscure...what is the "marginal log-density" here? I believe it is the logarithm of q_{\phi}(z | x). Please make this clear at least once because the term "marginal log-density" is used throughout the paper.  Quality: Quality is also lacking. For example the related work section does not mention some related work (see [2] and [3] below). Furthermore, there is a very  misleading statement on line 205 about the core contribution of the paper: "the core contribution of the paper is a novel upper bound on marginal log-likelihood". This is a false statement. The paper proposes an upper bound of the log density of the variational distribution which leads to a lower bound (NOT an upper bound) on the log marginal likelihood of the data. Finally, the experiments don't include comparison to UIVI [4] which also improves upon SIVI and there is no report of computational time which would put the test log-likelihood improvement into perspective.  Significance: I am not convinced the proposed method is significant.  Questions and Minor Comments: 1--line 38 and elsewhere: it is "log marginal likelihood" not "marginal log-likelihood" 2--lines 57-59 are not clear...what do you mean here? 3--what is the intuition behind increasing K during training? How was the schedule for K chosen?  [1] Semi-Implicit Variational Inference. Yin and Zhou, 2018. [2] Nonparametric Variational Inference.  Gershman et al., 2012. [3] Hierarchical Implicit Models and Likelihood-Free Variational Inference. Tran et al., 2017. [4] Unbiased Implicit Variational Inference. Ruiz and Titsias, 2019. 