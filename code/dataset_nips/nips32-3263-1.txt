**Originality**: The novelty of the work is that the authors search in the context of IoT devices, in terms of evaluating measured latency, memory footprint, and prediction accuracy at the same time to satisfy certain constraints. This is new compared to prior works, according to the Related Work section and my understanding. Also, their method of evolution of a parameterized search space seems also new in the context of Neural Architecture Search.  **Quality**: The submission is complete and not a work-in-progress. The experiment design is reasonable, the baseline comparisons against previous methods are fair and the results support their claims. The authors did not mention the weakness of this method, and there are a number of typos yet to be fixed.  **Clarity**: The method description and clear and easy to understand, since the method is not very complicated. Most of the figures are self-explainable, except some require a deeper look into the text.  **Significance**: Their results show that they can search for better model hyper-parameters (kernel size, channel size) for IoT devices. The results have shown that their method is better compared to prior work. They can also deal with the target space constraint (such as memory size) by evolving the input search space, and this is useful when deployed models have such constraints. Their search for different floating point configuration also prompts hardware designers and researchers to design specialized architecture for smaller deep networks that run on CPUs. Since they target IoT devices, the results can also be useful for non-researchers with affordable hardware.