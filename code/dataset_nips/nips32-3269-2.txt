I have read the author response. I still believe the lack of numerical experiments leave a lot of room for more insights to be generated, but I recommend publication even as-is.  This is a well-written paper, which provides a theoretical analysis of an important algorithm under misspecification. Lack of numerical experiments (see below) is my main complaint. Given space constraints, I recommend publication at Neurips as is, although a thorough numerical evaluation of the algorithms---see suggestions below---would make the paper more impactful. Authors provide intuition for theoretical results, which I found to be quite helpful.   Below are a few suggestions, comments, and questions to further improve the paper.  General comments:  1) In Theorem 7, is there a sense of optimality for \gamma^*? Is there a sense of a minimal recoverable corruption level in the mixed linear regression setting? If the authors have a conjectured sense of this bound, a comparison with that given by the theorem would be helpful.  2) A numerical evaluation of the proposed algorithms would strengthen the paper significantly. For both local and global recovery scenarios, a natural question is to see if trimmed least squares genuinely does not provide good performance. Given recent advances in integer programming, an analysis of how off-the-shelf solvers scale, and when ILTS truly wins would be helpful.   Comments on exposition/writing:  1) Table 1: Explain what Q is, or forward reference to Definition 1. 2) The phrase "smallest" component is confusing since it refers to the component weights. Being more specific (in general) would help with clarity of the exposition. 3) Some intuition for Lemma 6 would be helpful, and I believe it can be presented after Definition 3 to allow better understanding. 4) If space allows, a brief description of the gradient descent result should be included in the main text.