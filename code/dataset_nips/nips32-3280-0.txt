This paper considers model-free discrete-action reinforcement learning, with the agent learning with variants of stochastic Policy Gradient. The paper introduces and discusses the Bregman Divergence, then presents how it can be used to build a policy loss that allows stable and efficient learning. The core idea of the paper, that I found is best shown by Equation 7, is to optimize the policy by simultaneously minimizing the change between pi_t and pi_t+1 and following the policy gradient. The main contribution of the paper is the use of the Bregman Divergence for the "minimizing change between pi_t and pi_t+1" part of the algorithm.  The paper is well-written and interesting to read. It nicely follows a single trajectory, that goes from the Bregman Divergence to its use for Policy Gradient, to the derivation of all the gradients and update rules needed to properly implement the algorithm. The paper is quite dense, with the reader required to be fully attentive from start to finish in order not to miss the slightest piece of notation or derivation. But everything is properly described, and it is possible to understand the idea and its implementation. Algorithm 1 provides helpful pseudocode to tie everything together, even though source code would have been highly desirable (from experience, anything related to gradients needs source code, as the tiniest implementation detail can change the exact gradient being computed).  My only (relatively minor) concern with the paper is its lack of intuition. In the first paragraph of this review, I mention Equation 7 and how it enlightened the paper for me. However, this intuition is not in the paper, and I don't even know if it is true or if I misunderstood something in the paper. I think that the paper is currently quite intimidating. For instance, in the abstract, the part with "control the degree of off-policiness" could be replaced with something like "ensures small and safe policy updates even with off-policy data". Ideally, mentioning in the abstract that the Bregman Divergence takes into account the difference of state distributions between two policies, and not only the action probabilities, would allow the reader to fully understand the position and impact of the paper from a quick glance.