This article presents bounds on the  absolute difference between a sample from a (possibly noisy) Gaussian process (GP) and the predictive mean. The methodology relies on deriving probabilistic Lipschitz constants for the GP, and does not consider the RKHS associated with the covariance kernel as is generally done. As a result, the bounds are easier to use in practice, as shown on two examples.  The proposed Lispschitz bounds on GPs are an interesting result by themselves. But it should be related to and compared with previous derivations, see, e.g., Gonz√°lez, J., Dai, Z., Hennig, P., & Lawrence, N. (2016, May). Batch Bayesian optimization via local penalization. In Artificial intelligence and statistics (pp. 648-657). Concerning the main theorem 3.3, could you precise how to achieve the condition on \sigma_N in practice. Simply adding more training points (P7L252) in an arbitrary way would not be sufficient, expecially with noise.  Overall the paper is clear and well-written, with potential for both theoretical works and practical ones.  Minor point: - Proof if Theorem 3.2 on continuity of samples: early results on this can be found, e.g., in Cramer, H., & Leadbetter, M. R. (1967). Stationary and Related Stochastic Processes-Sample Function Properties and their Applications.  Typos: P3L83: on the one hand? P3L87: kernel is usual small P7L272: The simulations  ## Added after rebuttal I would like to thank the authors for their response, which addresses most of my concerns. Accordingly I increased my score. 