This paper studies a generalized low-rank approximation problem. In this problem, we are given a matrix $A$ and a cost function $g(.)$. The goal is to find a rank $k$ matrix $B$ such that $\sum_{i, j}g(A_{i,j} - B_{i, j})$ is minimized. In the approximation version of the problem, we aim to find a rank $k polylog (n, k)$ matrix $B'$ so that $\sum_{i, j}g(A_{i,j} - B'_{i, j})$ is a constant times worse than $\sum_{i, j}g(A_{i,j} - B_{i, j})$.   The paper's main goal is to understand what kind of $g$ admits approximation solutions. It focuses on column selection algorithms, i.e., the output $B'$ is a linear combination of a subset of columns from $A$. The paper gives a tight characterization of $g(.)$, i.e., it needs to satisfy approximate triangle inequality, and monotone property.   Their upper bound result is a generalization of [62]. [62] states that if the cost is $\ell_p$ norm, then there is a good subset of columns to approximate $A$. Their major observations are that (i)  [62]'s technique can be used to build a recursive method to find columns that can approximately span $A$, and (ii) [62] is a general technique so many assumptions made there can be relaxed. The authors showed that using only triangle inequality and monotone property suffices to generalize [62].   For the lower bound analysis, the authors claim that both approximate triangle inequality and monotone properties are necessary but the paper has only one lower bound result for the function $H(x)$. They argue that when the cost function is $H(x)$, we can construct a matrix $A$ so that it is impossible to choose a small number of columns to approximate $A$. I think the lower bound result is weaker than what the authors advertised.   The generalized low rank approximation problem is an important problem in statistics. The authors gave a non-trivial generalization of [62]. The analysis looks believable. I have two major concerns:   1. Lower bound: I am not able to see why Theorem 1.3 implies that both triangle inequality and monotone property is necessary.  2. Presentation: the authors made a serious effort to explain the intuition for the analysis (Sec 1.1.2) but some of the intuitions still appear to be incomprehensible (e.g., line 158 to line 160; line 168 to line 179). In addition, I feel some of the key definitions are not explained properly. For example, for the equation between line 232 and 233, I cannot understand how Q is defined (e.g., is Q a variable to be optimized or we need a worst-case Q).   My concerns could be fixable; I am willing to change my rating if the above two questions are properly addressed/answered.  