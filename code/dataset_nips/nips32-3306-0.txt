UPDATE after reading author rebuttal: I am looking forward to the more comprehensive evaluation that you are carrying out. Regarding Q3, please include details of the setup in the main paper. Also, more analysis needed regarding why zeroes are predominant in M in the main paper (also a point raised by R3) - rather than speculation or hypothesis. Overall, my opinion of the paper does not change and feel it is a good direction of research.   Detailed comments:  1. This paper proposes an alternative to softmax-based attention mechanism - a quasi-attention technique : A dual affinity matrix approach is proposed compared to the usual single affinity matrix. One affinity matrix is created from the pairwise similarity computation. The second one is computed based on l1-distance based dissimilarity (with a negative). Application of sigmoid on the second ensures either deletion or propagation. tanh operation on the first leads to a choice between addition or subtraction. Both operation together help choose between addition, subtraction or deletion. These operations can be composed to be incorporated in the cross-attention and self-attention mechanism in transformers. With this kind of composition, the expressiveness of models in increased.  Minor comment : This is not the first work on softmax-less attention. There were works earlier like sparsemax [Martins et al 2016], sparsegen [Laha et al 2018], which incorporate weights which are positive or zero. However, this work first proposes the idea of the inclusion of negative (that is subtraction), hence, is novel in that way.  2. Experimentation and evaluation in six tasks through multiple frameworks (cross-attention and self-attention): The experiments are reasonable and includes a wide range of tasks. However, there are few things that would have made it comprehensive:  a) Experimentation on SNLI dataset for Natural Language Inference which is more well-known and considered as a benchmark for NLI tasks. b) Experimentation on EN-FR, EN-DE, FR-EN, DE-EN, etc. are much needed for better comparison in NMT as these are more common languages. c) On Sentiment analysis, SST dataset will make the analysis complete.   3. Comments regarding analysis and visualization: Even though there is good mathematical intuition regarding the choice of the affinity matrices and the composition functions, the visualization is rather surprising that the Matrix M values tend to be close to 0.   a) How the visualization is constructed is not clear. Are the values considered for a particular trained model for all data points?  b) More insights and analysis is needed as to why deletion is more prevalent compared to addition or subtraction (M has more zero values). Will having higher alpha/beta values help? No mention of these hyperparameters in the experimental setting. Only mention is in the main section where they are both set to 1. It is also not clear why you would not want M to have harder extreme values? c) The form of Eq 9. that was used for experiments is not mentioned (or possibly missed by the reviewer).   4. Minor Typos: a) dimensionaity --> dimensionality. b) Line 117 missing reference.   References:  [Martins et al 2016] Martins and Ram√≥n F. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. [Laha et al 2018]  Anirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra, Karthik Sankaranarayanan, and Harish G Ramaswamy. On Controllable Sparse Alternatives to Softmax.   Assessment (out of 10): Originality: 8 Quality: 7 Clarity: 9 Significance: 8 