Updated review in light of author feedback: The authors have recognized that their terminology was unclear, that the experiments were missing details, and that there could be additional experiments from T2T. They have indicated that they plan to address all of these concerns in the camera-ready version. Assuming they do so, I assume the paper will be improved enough to warrant an acceptance, so I am raising my score to 7 which leads to an "accept" consensus among reviewers.  Summary:  This paper proposes an alternative form of attention which replaces the standard "query-key dot-product followed by softmax" form of attention which is widely used. Given two sequences A and B, CoDA an affinity matrix A and a dissimilarity matrix N and uses them to compute M = tanh(A)*sigmoid(N) to form a gated subtract, delete, or add operation. M is then used as usual for attention, without the use of softmax. The method is tested by replacing standard attention on many networks and on many datasets, and is shown to consistently outperform standard attention.   Review:  Overall it is an interesting and worthwhile pursuit to consider alternatives to the ubiquitous form of attention. CoDA presents some interesting possibilities due to the fact that it avoids softmax and instead allows for the possibility of adding, subtracting, or gating out values from each sequence. I also appreciate the experimental approach which replaces something standard with an alternative and shows that it consistently helps. For these reasons, I think this is a good submission.  On the negative side, as I expand on below, the notation is unclear and oftentimes self-contradictory. It definitely would need to be cleaned up before acceptance. Further, there are various alternatives proposed for the exact form of CoDA - whether to subtract the mean before applying the sigmoid or multiply by 2; whether to scale before applying self-attention; various alternatives for the function "F" for standard attention, etc. However, no details are given as to which were used when and no comparison is given to the performance of each alternative. If different versions of CoDA were used on different datasets, that is worrisome becuase it implies limited generality of the approach. Similarly, most of the results are using the tensort2tensor codebase but a specific set of datasets from tensor2tensor were chosen. Why these? Did it not work on the other datasets? I also think it would be very helpful to see a comparitive visualization, for some illustrative examples, of standard attention and the CoDA attention matrix "M". This could go in the supplementary materials.   I will give this paper a weak accept and if the above improvements/clarifications are made I will be happy to raise my review to an accept.   Specific comments:  - "Softmax is applied onto the matrix E row-wise and column-wise, normalizing the matrix." This is not true, it is only applies row-wise or column-wise (depending on whether you are "pooling" the elements of A or B). For example, if you intend to pool the elements of B, you'd multiply E*B and normalize the rows of E with softmax, not the columns.  - When you center N (or E) to have zero mean, are you computing the average across the entire matrix N? Or on a per-row or per-column basis? This would be more clear if you replaced "Mean" in (4) with an explicit nested summation.  - "in order to ensure that tanh(E) is able to effectively express subtraction" This statement is confusing without context because tanh(E) has not been used to perform any kind of subtraction yet.  - I think there are some issues with the notation in 2.4. In (7), A' and B' are computed using B and A respectively, but the text says "A' and B' are the compositionally manipulated representations of A and B respectively." Also, the text reads "In the case of A' each element A_i in A scans across the sequence B and decides whether to include/add (+1), subtract (−1) or delete (×0) the tokens in B." Don't you mean "...each row M_i in M scans..."? A does not appear anywhere in the expression for A' in (7).   - You never explicitly define what the CoDA function does (used in (8)). I assume you mean you compute M via (3) and then A' and B' via (7), but this should be made explicit.  - You introduce an "alignment function" F without describing its purpose. Please expand.  - You write "We either apply centering to ... or 2*...". I think you mean "We either apply centering to ... or compute 2*...". It sounds like you are applying centering to 2*... which I don't think is what you mean.  - In some cases, the results tables are presented without including all prior work which makes statements like "CoDA Transformer also outperforms all other prior work on this dataset by a reasonable margin." For example, for IWSLT'15 EnVi, both https://arxiv.org/abs/1809.08370 and https://www.aclweb.org/anthology/N19-1192 obtained BLEU scores of 29.6, which is not far from the 29.84 obtained in this paper. To me, the most important thing is that CoDA improves over regular attention as a simple swap-in replacement, so I think the results should emphasize this -- not marginal SoTA.  - For visualization, it would be more informative to show a few examples of what M is on a few representative sequences.  - In the related work, you cite Xu et al. 2015 as a reference for "self-attention". I don't believe there is any self-attention in Xu et al.  - Given that you are using tensor2tensor in some experiments, why not include results on other standard datasets like WMT translation etc?  - In which cases do you use centering vs. multiplying by 2 ((4) or (5))? Footnote 2 reads that "removing the scaling factor [sometimes] works better". When did you and didn't you use the scaling factor? What function/network do you use for F?