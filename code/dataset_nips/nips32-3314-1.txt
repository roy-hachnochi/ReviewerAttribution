This paper proposes a novel generative model for GZSL to synthesize inter-class discrimination and semantics preserving visual features for seen and unseen classes. The proposed DASCN model preserves the visual-semantic consistency by employing dual GANs to capture the visual and semantic distributions, respectively. Extensive experimental results consistently demonstrates the superiority of DASCN to state-of-the-art GZSL approaches.