The algorithm shows that SGD plus overparameterization can learn any neural network with a well-behaved Taylor expansion.  This is interesting! The paper also makes some questionable claims: "To the best of our knowledge, this is the first result showing that training only hidden layers of neural networks can provably learn two (or even three) layer neural networks with non-trivial activation functions."  The work of Daniely already shows how to train all the hidden layers of (any depth) network to learn classes using a (different) measure of complexity.  It is true that his method is similar to the 'train last layer' approach, but in this submission something similar happens in the two layer case.  The authors seem to say that something more sophisticated happens when training a 3 layer network, but I could not understand this section.  It is not clear to me what the relationship to NTK is.   Also, in the paper the authors on line 126 indicate that sample complexity is polynomially related to network size (within poly(1/\eps)).  But then later it says the sample complexity is polylog in the network size.  Can you please clarify?  The authors do not use normal SGD: they use SGD plus noise.  Why can't standard SGD work? 