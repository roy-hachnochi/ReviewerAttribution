Overview: - Originality: low (incremental improvement over IQN) - Quality: medium (great results on Atari but lack of a thorough empirical analysis) - Clarity: good (overall easy to read, in spite of several notation errors) - Significance: medium (bigger numbers on Atari, but with little information as of why)  Overall: I am on the fence, currently leaning towards acceptance even though I believe it would be important to better understand why the proposed algorithm works (maybe on a simpler task if Atari is too expensive). Since the full code is available, at least it should be easy enough for others to follow-up on this work to perform such investigation.  Main detailed comments:  - A main motivation of the paper is that "there is huge variance in the mean of the sampled quatile values in IQN", and that this hurts performance. This is somewhat surprising since the IQN paper states that "we did not find IQN to be sensitive to K, the number of samples used for the policy", which suggests that increasing K (and thus decreasing the above-mentioned variance) shouldn’t make a significant difference. This makes me wonder to which extent the improvements observed here are a result of the proposed optimization technique, vs other differences with respect to « vanilla » IQN, namely: the weighting scheme in eq. 1 (vs uniform weights in IQN), and the fact that 32 quantiles are used both for decision making (same as K=32 in IQN) and in the double-loop defining the optimized objective (vs N=N'=8 in IQN). In the absence of an ablation study, or re-running IQN with these changes, it is hard to be sure what exactly makes FQF work.  - Assuming the improvements do come from using « better » quantile fractions, it would have been very interesting to investigate more carefully why this is the case exactly. In particular, currently the predicted quantile fractions are output by a network taking both the state and action as input. But is this really necessary? What if there was only the state, only the action, or even no input at all (fixed quantile fractions)? What if we simply used fixed uniformly spread quantiles for decision making, as in QR-DQN? I believe it is important to understand whether the dependence on the state and action actually matters, or if a fixed "good" set of quantile fractions is enough. Note that the authors state on l.107 that for IQN "there is no guarantee that sampled probabilities would provide better estimation than fixed uniform probabilities", but omit to verify this for their own algorithm.  - The RL community has been advocating the use of sticky actions in Atari for more meaningful evaluation of RL agents. Although a comparison with old approaches on non sticky actions is definitely important for legacy reasons, an FQF vs IQN comparison with sticky actions would have strengthened the empirical validation.  - I fail to understand the motivation behind minimizing the loss L_w1 (below l. 159) instead of following the gradient dW1/dtau_i computed in the Appendix proof. I can see how bringing this loss down to zero would bring the gradient to zero, but the usual practice in optimization is to follow the gradient of the loss, not minimize its square... I assume there is a good reason for this approach, but since it seems counterintuitive, it should be explained.   Minor points: - The authors use the term "probability" throughout the paper to denote a quantile fraction, as opposed to its value. This is initially confusing as it is not clear what "probability" means in this context (ex: in the abstract). To be honest I am not sure what is the best word for it: "quantile fraction" is the best I could find, but usually in distributional RL papers people just use "quantile". Anyway, I don't think "probability" should be used here since it can be misleading. - In the equation below l. 68 there should be no "Q(x, a) =" at the beginning of the equation, unless you write it for the optimal policy (and thus it should be Q*) - "Rainbow agent, current state-of-the-art in Atari-57" => specify that it's for non-distributed methods (see e.g. the chart in Fig. 2 of "Recurrent Experience Replay in Distributed Reinforcement Learning") - "In practice the number of quantiles can not be infinite and is usually set to 32": it's not clear what this number represents unless the reader is very familiar with the IQN paper (it refers to the parameter K used to choose actions). This should be clarified (ex: "In practice one needs to use a finite number of quantiles to estimate action values for decision making, e.g. 32 randomly sampled quantile fractions as in Dabney et al. [2018a]"). - The Pi^{theta, tau} below l. 126 is not defined, and it looks like tau defines a set in this equation, while in the rest of the paper tau is a single scalar => confusing notations - More generally I find this equation (below l. 126) pretty complex to parse, for a result that's actually very simple. It'd be nice to either find a simpler formulation, or/and explain it in plain English, or/and refer to a figure to show what it means. - In Fig. 1 using F_Z^-1 instead of Z to label the y axis would be more consistent with the notations of the paper  - In Fig. 1 tau_N+1 on the x axis should be tau_N, and "N=5" in the caption should be "N=6" - tau_hat in Fig. 1 is not defined yet when Fig. 1 is referenced in the text (l. 135), the definition only comes on l. 147 - In equation below l. 162, w1 should be w2 (same in eq. 6 and 7) - In eq. 7, tau_hat_i should be tau_hat_j (same in the corresponding equation in Alg. 1) - w1 and w2 should be swapped in L_w1 and L_w2 in Alg. 1 - l. 190 F_w2^-1 should be F_Z,w2^-1 for notation consistency  - Please clarify where the IQN training curves in Fig. 2 come from, since they do not match those from the original IQN paper, nor the numbers from Table 1 in the Appendix - Table 1 should have all « 54 » numbers in bold in rightmost column - « Our algorithm (...) is the first distributional RL algorithm removing all inefficient hyper-parameters that require manual tuning previously » => the number of quantiles still needs to be tuned, as well as the quantile embedding parameters (equation below l. 187), and the quantile proposal network architecture... - « we believe that our algorithm presents a whole new direction for distributional RL » => I believe this sentence should be toned down, as this paper is an incremental improvement over IQN, not a « whole new direction ». Note also that regarding the question « is it possible that we leverage the distribution and develop an exploration-aware algorithm? », the short answer is « yes » since such work already exists (Google up « distributional reinforcement learning exploration » for instance) - In Appendix eq. 1 the sum over i should be from 0 to N-1 - In Appendix proof, beginning of equations, W1/dtau_i should be dW1/dtau_i - In Appendix proof, using large parentheses $\left(...\right)$ would make some equations easier to parse - In Appendix, second line of the equations in the proof, the second integral should start from tau_hat_i-1, not tau_hat_i - In Appendix, the derivation of eq. 3 is not detailed enough in my opinion, I had to write it down to understand it. Since this is the Appendix, there should be enough room to spell it out more explicitly. There is also either a mistake or (more likely) an abuse of notation when writing e.g. d/dtau_i F_Z^-1(tau_hat_i-1) since it should be dF_Z^-1/dtau taken at tau=tau_hat_i-1 - In Appendix, last too equalities in the proof equations, tau_hat_i+1 should be tau_hat_i   English typos: - "network(IQN)" missing space (x2) - "X and A denotes" denote - "while agent interacts" the agent - "C51 out-perform" outperforms - "is able to approximate to the full quantile" remove 2nd 'to' - "with infinity number of quantiles" with infinitely many quantiles - l. 150 remove "for" - « element-wise(Hadamard) » missing space - « Motezuma »  Update after author feedback:  Thank you for the response, I appreciate the many clarifications that were made, especially regarding the motivation for minimizing L_w1 -- I definitely encourage you to "figure out why and explain it in detail" :) [feedback l. 36]  However, the other important concerns I had regarding the empirical analysis of the proposed algorithm were only partially addressed (through the promise of extra experiments -- whose results are still uncertain) [feedback l. 9, 25-28]  I still believe that this paper deserves being published, mostly due to the impressive results on Atari, so I will keep my original recommendation (6). I hope that the final version will contain relevant additional experiments to better explain why the proposed technique works so well!