Originality:  This work introduces a novel method for optimizing the selection of quantiles to minimize error in the return distribution. This method is then combined with IQN to produce a new RL algorithm that improves the ability of a network to accurately estimate the return distribution.  Quality: The paper makes the following claims:  1)  “Self-adjusting probabilities can approximate the true distribution better than fixed or sampled probabilities” -- line 53  2) “we believe that the former one would achieve better results since it approximates the quantile function with a stair case function, instead of viewing the process as sampling which leads to instability”-- line 194  3) “It shows that FQF out-performed all existing distributional RL algorithms” -- line 219  4)“FQF is generally much faster and stabler thanks to self-adjusting probabilities” -- line 224  In trying to evaluate these claims I have questions about their scope and justification.  1) In Figure 1, it is shown for some distribution optimizing the choice of quantiles can provide a better approximation to the distribution. However, the contribution of this paper is a method to better estimate the return distribution, but no experiment is conducted that shows how well the introduced method approximates the return distribution compared to previous methods.   2) Are there unreported experiments that show “sampling leads to instability”.  This would be a nice small contribution to add to the paper that could influence future research.  3) The experiments were conducted with 3 trials. With the known high variance of performance over random trials how certain is the result that FQF is better than other distributional RL algorithms? Reproducibility worksheet indicates that there is a clear description of the central tendency and variation, but I do not see mention of the variance or uncertainty of the performance measurements. The error bands on the learn curve plots are also undefined. Additionally, FQF used the same hyperparameters as IQN. This comparison only shows that FQF obtained higher performance for these hyperparameters. Does this imply that FQF will always outperform IQN with the same hyperparameters? I believe the result needs to be qualified with the uncertainty of the performance estimate and to what extent the experimental data shows FQN being greater than the other algorithms.  4) This claim has two parts: faster and more stable. I assume (please correct me if I am wrong) that faster is with respect to how quickly the algorithm learns. Under what measure is the speed of each algorithm being compared? Similarly, for stability what is the measure that is being used to compare these algorithms?    In my reading of the paper, I developed other questions that I could not find answers for and that would improve the quality of the paper if answered.  If actions are selected via expectations, then does it matter how well the distribution is approximated so long as the mean is preserved?  How does minimizing the error in the approximated distribution affect the approximation of the mean?  It is mentioned that “learning rates are relatively small compared with the quantile network to keep the probabilities relatively stable while training”. How stable is this algorithm to the relative setting of learning rates? Is selecting hyperparameters easy?   "First, does the 1-Wasserstein error converge to its minimal value when we minimize the surrogate function derived by its derivative? While the answer is yes on some toy problems we test with fixed quantile functions,"  This toy problem would be great to see! It would add a lot to the paper to see how well the distribution is approximated. This is the critical claim of the paper. Experiments demonstrating this are more valuable than getting SOTA performance results on Atari. These experiments could also be conducted on domains that require less computational than Atari.   The discussion about the increased computational cost of the new method is appreciated (lines 228-231).  Clarity: some minor notes to help the clarity of the writing   Add mention that there is a proof of Proposition 1 in the appendix.   In the discussion section, it states that: “Our algorithm, by the time of writing, is the first distributional RL algorithm removing all inefficient hyper-parameters that require manual tuning previously. What does “inefficient hyperparameters” mean? I am looking for a precise clarification of this statement as to which hyperparameters are removed.    Significance: The main idea presented in this paper of optimizing the selection of quantiles could have significant impacts for distributional RL. Answering the question “How important is it to have an accurate approximation of return to the performance of distributional RL algorithms?” would provide a valuable contribution to the investigation and development of new distributional RL algorithms. This paper attempts to answer how performance is impacted but does not address to what extent the accuracy of the approximated distribution impacted performance. Thus, the knowledge generated by this paper is limited to the presentation of the idea and a performance measure. These are less actionable than furthering the understanding of how to best approximate the return distribution.   ---Update I give my thanks to the authors for their clear response. I think my question about to what degree does having an accurate estimate of the distribution affect performance was not conveyed correctly. It was not about comparing distributional RL methods to regular, but about how accurate does the distribution approximation needs to be? If one can use say 3 quantiles versus 30 and still select the optimal action, then having a close approximation does not matter. Unless it allows for better representation learning and quicker policy improvement.   Nevertheless, I appreciate the experiments detailing that FQF did learn a better approximation to the return distribution than some methods and updated my score. Please include these experiments with IQN and a description in the next version of the paper.  