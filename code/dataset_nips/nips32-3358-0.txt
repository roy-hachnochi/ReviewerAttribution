Originality: The work proposes an interesting framework for distributional reward decomposition. The work is not particularly novel since it is based on several prior works, albeit in a non-trivial way. For example, as discussed in point 3 in the contributions section above, the authors are inspired by [Grimm and Singh, 2019] but they propose their own disentanglement loss term. Quality:The intuitions behind the various steps seem quite reasonable to me. The experiment shows significant improvements in RL performance, and the authors provide experiments that show how the computed rewards are correlated with the ground truth. On the other hand, only decompositions of 2 or 3 terms have been performed and other decomposition methods are not assessed. Significance: Disentangled reward decomposition is a very important area in RL, so the work can be used by other researchers or practitioners.   UPDATE Thanks for the authors' feedback. I acknowledge that I have read the rebuttal and the other reviews. I believe this is an interesting work that successfully brings together various promising ideas from the existing literature, so I think it is qualified for acceptance. Therefore, I keep my original score of 6. The reason why I am not giving a higher score is that the experiments are not entirely convincing to me. For instance, I raised the point about high-D experiments - the authors mentioned that their framework is in principle applicable to high-D settings but I think experiments would be needed to back up this claim. Nevertheless, I still think that this is an interesting work and relevant to the RL community.