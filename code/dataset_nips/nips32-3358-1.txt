The submission introduces a method for distributional reward decomposition which is more generally applicable than prior work, removing requirements for arbitrary resets as well as domain knowledge. The method models subrewards as Categorical distributions, treating reward composition as 1D convolution, and relying on update rules from prior work on distributional Q-learning (C51) for update rules.  To further strengthen disentanglement the objective is extended to maximise the KL divergence between the distributions resulting from actions optimising for different subrewards (treating the learned Q functions as epsilon greedy policies).  Overall, the work provides a valuable contribution to RL by investigating (and benefitting from) reward decomposition in a distributional setting. The combination of reward decomposition and distributional RL provides novelty and as demonstrated in the experimental section better agent performance by exploiting task structure. It would be interesting in this context to see how the approach fares in tasks with only a single source of reward and potential situations where the method might perform worse than the baseline.  On the experiment section it would be important to additionally compare against distributions with the same number of atoms as it might be simply easier to fit distributions with M/N atoms than with M atoms, leading to an unplanned benefit of the proposed algorithm. In Figure 3, it would be interesting to investigate situations when either subreward spikes but not the original one to better understand the model. To be fair to prior work and provide a more complete evaluations, it would be good to compare against Van Seijen et al and Grimm and Singh in environments where their requirements are fulfilled.   Minor: Couple of spelling/grammatical mistakes in lines 10, 89, 90, 101 Statement about stochastic policies could leave out the epsilon greedy part to be more general as stochasticity can also apply to continuous action policies After introducing the UVFA like trick in 3.4 is the state splitting still required? Figure 1a: To be more self-contained it would be beneficial to also explain the symbols here Possibly related work: RUDDER: Return Decomposition for Delayed Rewards; Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, Sepp Hochreiter  I appreciate the author feedback in particular with the additional ablation and investigation of the model which will help answer some open questions. In addition to this, I hope the authors will spend time on the promised detailed evaluation of failure cases and investigation why the approach improves performance even when its assumptions seem broken (one reward source). 