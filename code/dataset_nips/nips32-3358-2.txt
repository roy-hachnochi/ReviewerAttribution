I find the ideas presented in this work to be sound. Learning decomposed reward representation seems to give more representation power, which leads to improved performance. However, the ideas are quite incremental (combining two well-studied approaches), there is no new analysis and the experiments are not too impressing. To be more convinced, I would like to see ablative analysis of the results - how each component in this work (new loss, new architecture, distributional rl) contributed towards the final solution.   Detailed comments:  While many reward decomposition papers try to learn a different policy for each component, to be combined later on, here the focus is on learning better representations by using the decomposed representation. I would like the authors to emphasize that more in the text and explain why they took this approach.   Section 3.1. This section is quite confusing. Equations are derived, but then it is explained that they are ignored. The authors mention that they performed experiments with the full distribution method (the none factorial) but they did not perform well. I would like to see the distributional model developed for this case as well as the supporting experiments to be convinced as well.  The fact that the sum of two random variables is given by convolution is true for any two random variables (this is taught in basic probability courses). The way it is presented here may confuse the reader to think that there is some novelty here. I would like to see a derivation of equation 4, I am not confident that the optimal bellman equation is linear.   Section 3.3. Is the projected Bellman loss novel to this work or was it proposed in the Bellamere et al (2017) paper? please be specific. If it was proposed before, then why wasn't it implemented in the Rainbow architecture? is this new loss only improves results when combined with the reward decomposition? I would like to see more experiments about this loss, with an without other components, as well as a detailed explanation regarding when was it first proposed and when was it used.  In equation 7 there is a subscript i, but it is not used in equation 8, can you please explain how you move between these equations?   Experiments. How were the hyperparameters selected? how are they different than the classic parameters of Rainbow? did you find the algorithm to be sensitive to these parameters? The regularized \lambda seems to be quite small. Can you elaborate on that? was it needed at all?     --------------------------------------- Following the rebuttal: I appreciate the authors' effort in addressing my questions. I also find the additional experiments provided in the rebuttal to be interesting, and I believe that they improve the quality of the paper significantly. Since the authors also agreed to address most of my concerns in the final version of the paper, I am increasing my score from 4 to 6. 