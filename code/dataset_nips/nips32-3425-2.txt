Post-Rebuttal Feedback  Thank the reviewers for your feedback. I think this is a good paper to appear in NeurIPS. #######################  Uncertainty estimation has always been an important problem. This paper tackles the uncertainty prediction via directly predicting the marginal mean and variances. For assuring the reliability of its uncertainty estimation, the paper presents a series of interesting techniques for training the prediction network, including location-aware mini-batching, mean-variance split training and variance networks. With all these techniques adopted, the paper demonstrates convincing empirical results on its uncertainty estimation.   Weakness, I am surprised by the amazing empirical performance and the simplicity of the method. However, many proposed techniques in the paper is not well justified. 1, Overfitting is still a potential issue that might occur. Because the network is simply trained via MLE, it is possible that the network fits all training points perfectly, and predicts zero variance by setting gamma-->infty, alpha-->0 and beta-->infty. The toy experiment in the paper has a densely distributed training points, thus the proposed method performs well. But I would like to see another experiment with sparsely distributed training points, in which case we can see better on the overfitting issue. 2, Scaling to high dimensional datasets. The proposed locality-aware mini-batching relies on a reliable distance measure, while the paper uses Euclidean distance. However, for high dimensional datasets, the Euclidean distance is hardly to be trusted. Similarly for the inducing point in variance network, the distance selection has the same issue. However I feel this this is not fatal, as RBF is known to work well, and other metric learning methods can be applied.  3, Computational complexity. Although the locality-aware mini-batching supports stochastic training, searching for the k-nearest-neighbour takes O(k N^2) computational cost, which might not be feasible in large datasets. In particular, if you learns a metric (Check Q3) along training, the searching process needs to be repeated, which makes it unfeasible.  4, I am not convinced by the local-likelihood analysis for location-aware mini-batching. Because the location-aware mini-batching is also an unbiased estimation for the total sum of log likelihoods, at end of the day, it should converge to the same point as standard mini-batching (of course there might be optimization issues). 5, I think Eq5 is a typo, that it should be sigma^2 (1- v) + eta * v  Strengths, I have covered it in the contribution part.