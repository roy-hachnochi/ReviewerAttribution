This manuscript proposes a novel method for Bayesian batch active learning through sparse subset approximation and a convenient set of reductions to arrive at a tractable algorithm. This method is validated and explored through a series of special cases (linear regression and classification), illustrations, and experiments. Overall the method appears to be competitive with the state of the art.  Overall this manuscript is well written, insightful, and enjoyable to read. The proposed approach outlined on page 3 is elegant, appears to work well in practice, and the approach may be useful in other settings.  I only have a few concerns about the work as presented that I will outline below. These are mostly in linear order:  - The fact that naive adaptation of sequential active learning algorithms to the batch setting by ranking and taking the top-scoring points produces highly correlated batches is completely unsurprising. However, there is a simple mechanism that can be used to address this phenomenon, which has for example been used in Bayesian optimization [1][2]:    - begin with empty batch   - for i = 1 .. batch size     - choose next point x using sequential AL     - impute observation y for this point (e.g., by sampling or MAP)     - add x to batch     - update model given (x, y)     - return batch  I _think_ this may be the "greedy" procedure you mention in for final experiment but I'm not sure. Obviously there is a tradeoff here in that you must do (batch size) model updates to compute the batch, although the overall running time is only linearly more expensive than choosing a single point. By imputing observations you get a natural "repulsive effect" lessening the correlations among batch members. I think at the very least some more discussion is warranted on this point.  Added after response: after some reflection, I am lowering my score slightly since this simple and pervasive batch construction strategy is completely ignored and note even acknowledged in the discussion. The paper would be much stronger (and more intellectually honest) if it were discussed and evaluated, even if the evaluation concluded the runtime was too great. I don't buy the idea that a linear slowdown in active learning (where evaluations are expensive) is as huge a deal as the authors suggest.  - The black box in (5) should be a w.  - I think the discussion on logistic regression would be more clear if you simply replaced it entirely by probit regression. Why mention the logistic function at all if you're just going to throw it away a few lines later?  - I disagree that extending entropy-based methods to the batch setting is necessarily difficult. For example in logistic regression it would be trivial to at least greedily maximize the entropy (the determinant of the predictive covariance matrix) offline using a series of rank-1 updates. This is effectively the above procedure although in this particular case you don't need to impute y at all.  - The manuscript would be strengthened with some mention/study of the running time of the proposed method compared to the others.  [1]: https://hal.archives-ouvertes.fr/hal-00260579/document [2]: Jiang, et al. Efficient nonmyopic batch active search. NeurIPS 2018       