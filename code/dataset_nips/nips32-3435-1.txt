 Positives: - an almost tight answer to a fundamental problem - good practical performance on some tasks - the lower bound argument is quite nice  Negatives: - writing and positioning w.r.t. related work can be improved - experiments on training small CNNs or LSTMs with sparsity could be added, but are not absolutely necessary   I am quite positive about this paper. My main observation to the authors is to stress less about the relation with previous work, and stress more about explaining your algorithm in detail. For instance, I think the discussion in lines 101-115 which basically re-explains how your algorithm is different to a finer level of detail could be left for *after* you've actually described your algorithm and its guarantees. You could have a whole sub-section for that, where you can carefully cover the similarities and differences. Currently, the draft is very defensive on this point; this makes sense, but notice that you are the first to be proving tight bounds, so it's kind of OK if your algorithm is not super-novel (given the amount of activity in this area one wouldn't really expect that anyway).  The main shortcoming of your algorithm though is from the practical side: you need to synch the normalization factor, which means that your algorithm is two-rounds. This could potentially negate the benefits of reduced communication. It would be nice if you could comment on this.   The paper has many typos (some collected below), which should be fixed.  - maybe add "asymptotically" optimal to the title. sometimes in communication complexity people actually care about constants (see e.g. list decoding) 105 -> doen't 110 -> trade-off of 101 -> 115: maybe move this discussion for *after* you've presented your algorithm? 128 -> summarizes 157-> sends it 181-182: repeated discussion