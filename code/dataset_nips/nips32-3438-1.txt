Originality: This paper has two major drawbacks in its originality segment: 1) the field of NN-pruning is quite busy with many related papers populating the field and 2) it does not compare against the following very similar paper: Faster gaze prediction with dense networks and Fisher pruning by Theis et al 2018. This paper uses the fisher information to prune features during gradient descent subject to user-preset-computational constraints.  Quality: The paper is technically interesting, but makes one leap which is unclear: the authors claim to be model agnostic and instead to be putting all their assumptions into the SGD method. However, the curvature calculations (via Taylor approximations) are model-dependent and actually exploit model structure to determine if a weight should be pruned. It would be great to relate this to the Hessian and the Fisher Information (see: Fisher pruning) to clarify the relationship to the model. Apart from that, another drawback of the paper is the need to express the compression ratio, which is quite an unnatural quantity to have to hand-specify and is not really what a user wants to control. Constraints typically exist in speed or memory space, not in compression ratios. The experiments are pretty well executed, I particularly enjoyed the study of the feature-re-activation, which studies a specific property of this model.  Clarity: The paper is well written and concise.   Significance: This paper manages to not need complex criteria or multi-stage models to achieve its goal of sparsifying. In the long term, this can be an important property to make pruning a pragmatic modeling tool enshrined in software. 