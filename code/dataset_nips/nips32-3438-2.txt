(1) This paper is well written. (2) To my knowledge, most of the preceding methods only prune relatively shallow models like Alexnet and Vgg, where it is possible to manually set the layer-wise pruning rates based on trial-and-error. But the proposed method requires no pre-defined layer-wise pruning rates, which is especially good on very deep models. (3) The proposed method (GSM) achieves lossless pruning. Compared to the classic L1/L2-based pruning method [Han et al. Learning both ...], which use L1/L2 regularization to reduce the magnitude of parameters (at the cost of compromised accuracy) and then prune the parameters (with accuracy reduction again), the model encounters no accuracy drop when pruned after GSM training. (4) The proposed method is intuitive and easy to understand. The method utilizes momentum in a natural and creative way: to accelerate the process of a parameter moving towards a constant direction. (5) The main reasons for me to vote for accepting the paper are the novelty and potential insights. The idea of directly modifying the gradients to accomplish a certain task is intriguing. Actually, we always customize the loss function to indirectly modify the gradients which control the direction of training, but rarely directly transform the gradients.