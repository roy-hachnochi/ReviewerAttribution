This well-written paper is the latest in a series of works which analyze how signals propagate in random neural networks, by analyzing mean and variance of activations and gradients given random inputs and weights. The technical accomplishment can be considered incremental with respect to this series of works. However, while the techniques used are not new, the performed analysis leads to new insights on the use of batch/layer normalization.  In particular, the analysis provides a close look on mechanisms that lead to pathological sharpness on DNNs, showing that the mean subtraction is the main ingredient to counter these mechanisms. While these claims would have to be verified in more complicated settings (e.g. with more complicated distributions on inputs and weights), it is an important first step to know that they hold for such simple networks.  Minor points follow:  - on the sum in E(\theta) (end of page 3), upper limit is defined but not lower one  - \lambda_max seems to concentrate, is it possible to compute an average value rather than a bound?  - worth adding a comment about the difference between (20)/(21) and (23)/(24)  - the normalization techniques are studied considering the whole dataset, and not batches as typically done in SGD; is there any way of adapting it?  - I wonder if the authors have any comment on Kunster et al., "Limitations of the Empirical Fisher Approximation", who show that the empirical FIM is not necessarily representative of the geometry of the model  # Post-rebuttal addendum I was previously unaware of the publication of [21] at AISTATS, and as such I share my concern with other reviewers regarding the originality of this work. I recommend the clarifications given in the response are included in an eventual updated version of this manuscript.  Otherwise, I am quite pleased by the effort the authors have put in the rebuttal, and in particular with the experiments on trained networks. I think this is yet another evidence that this work might be useful in practice.  Following these two points⁠—one negative and one positive⁠—I have decided to keep my score the same.