The paper describes methods for estimating aleatoric and epistemic uncertainties in a neural network. While many existing methods are based on sampling based approaches, the paper proposes a sampling-free method. The aleatoric uncertainty is estimated using quantile regression based on the pinball loss. The pinball loss had been used for quantile estimation. But I am not certain if it has not been used in a neural network before. The epistemic uncertainty is estimated by the proposed orthonormal certificates, which finds sort of the null space of training data points.  - Overall, there are some interesting aspects about the paper but it is not convincing enough to support its acceptance.   - The benefits of the proposed method are not fully illustrated in experiments.  - Synthetic datasets can be used to show convergence of the proposed method and its sample efficiency against competing methods.  - In experiments, reporting the training times and inference times can help position the proposed method against others.  - Recently, there are some sampling-free methods for estimating epistemic uncertainty. But they are not compared in the paper.   - On aleatoric uncertainty estimation, the paper proposes to use the pinball loss. Since the loss function is different from the popular loss function, such as cross entropy, it is important to verify if the use of the proposed loss does not hurt the performance on a large model, such as ImageNet (not UCI datasets).   - On epistemic uncertainty estimation, the paper assumes that the feature space is linear. This fact needs to be more elaborated as not all output feature spaces of neural networks are vector spaces. For the loss function l_c in (4), the investigation of the effects of different norms, other than l2 used in the paper, will be interesting.   Minor: - The dataset used in 4.1 is not mentioned. - An incorrect legend on the right most figure of Fig. 2. 