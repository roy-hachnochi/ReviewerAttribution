This paper is about expressive abilities of deep neural networks and the related gradient-based optimization processes for distributions generated by iterated function systems. The first result shows that such a distribution can be expressed efficiently by deep neural networks, but not by shallow ones. The second result shows that gradient-based algorithms are likely fail in processing such a distribution. A key assumption is Assumption 3 requiring that the positive class is contained in the set of points that are far away from the boundary of the n-th iterated set K_n by at least gamma. Note that the limiting set of K_n is a fractal set having no interior point in most cases. So the required margin gamma depends on n and would be extremely small when n is large. This observation leads to the reviewer's concerns over the main results: the nice depth separation argument is too special, and the evidence for the failure of gradient-based algorithms is not strong enough. 