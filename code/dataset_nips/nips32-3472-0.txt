My main concern is that the problem of domain generalization is not very well-defined. It is necessary to explicitly specify the underlying assumption of the proposed method. "Directly generalize to target domains with unknown statistics" as mentioned in the abstract would be impossible if the target domain is very different from the training domains. In comparison, MAML has a task distribution from which the tasks (including target) are sampled. If MASF uses the same assumption, then the problem is not really domain generalization but meta-learning from multiple tasks as in MAML.  The experiments focus on learning from multiple domains then applied the model to *only one* target domain, which is, in fact, more related to the multi-source to single-target adaptation than domain generalization. For example, the papers below [a,b,c] are theoretically justified and it would make sense to discuss and compare to them.  [a] Mansour, Y., Mohri, M., and Rostamizadeh, A. (2009). Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems, pages 1041-1048. [b] Zhao, H., Zhang, S., Wu, G., Moura, J. M., Costeira, J. P., and Gordon, G. J. (2018). Adversarial multiple source domain adaptation. In Advances in Neural Information Processing Systems, pages 8559-8570. [c] Hoffman, J., Mohri, M., and Zhang, N. (2018). Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pages 8246-8256.  The necessity of global class alignment (Sec.3.2) is not very convincing: certain design choices are not clearly explained. It is not clear why the soft labels need to be computed based on average class features. We can alternatively compute the average soft labels given the data from one particular class directly (i.e., averaging over the final predictions instead of features). It is also not clear why symmetrized KL divergence is used instead of Jensen-Shannon divergence. Any comments or explanations on these alternatives would be helpful.  What is the "linear-sized random subset" in L188?  Experiments - The VLCS, PACS and MRI results in Table 1, 2 and 4 have no error bars. Are these results from one single run? Besides, it is not clear what the error bar in Table 3 means. Is it standard deviation, standard error or something else? Are these results statistically significant? - Why clipped gradient is needed? This indicates the proposed algorithm is not very stable or easy to train. - How is the margin parameter \xi selected in the experiments? And what criterion is the selection based on? - Compared to Table 1, some alternative methods are not included in Table 2. Why?  Minors: - L95, samples are "drawn from" a dataset -> "drawn to form"? - L219, the highest  ==================================== Update after rebuttal ==================================== The rebuttal resolves some of my concerns about the underlying assumption and design choices. It is essential to provide explicit assumptions about the proposed method. It also indicates that several hyperparameters are chosen heuristically. Without proper selection strategy or seeing sensitivity analysis about these hyperparameters, it is difficult to tell whether the improvements are due to better objective function or extensive hyperparameter-tuning. Overall, this is a borderline paper.