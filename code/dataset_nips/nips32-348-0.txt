==== Based on the authors response, I find the comparison against gradient checkpointing they provide satisfactory. Please ensure it is included in the final draft ====  This work considers handling sequences of networks layers with identical weights (i.e. weight tied layers) via fixed point computation. Instead of directly computing the sequence, a quasi-newton method is used to approximate the fixed point of the sequence. This has the advantage that the gradient has a simpler form, although one which must also be computed iteratively. The advantages are:  • Much lower memory usage as intermediate tensors do not need to be stored for use in the backwards pass. Approximately 4-10x lower for the considered models.  • Empirically (sometimes) better perplexity compared to iterating.  The disadvantages are:  • About 3-5 times slower to train, 1.7-2x slower at evaluation time.  • Significant additional implementation complexity  • If there is no fixed point the method may not work.  The paper is well-written and generally polished. I'm disappointed that the runtime comparisons are relegated to the appendix, it is poor scholarship to hide the disadvantages of your method.  Reducing the memory requirements of language models is an important goal, as they are typically memory constrained during training for the largest SOTA models. A practical approach here could have significant impact.  The experiments seem reasonable, the authors compare against current SOTA level models.  It's unclear if there is any advantage compared to using gradient checkpointing. Checkpointing is discussed in the related work section but not tested against. Checkpointing requires only 1 extra forward pass, which is typically a 1.5x overhead, although the memory reduction may not be as significant (depending on the exact network architecture). The additional computational resources saved could be used to train a slightly larger model, which may reach similar perplexities. If this paper clearly showed that the approach was superior to checkpointing, it would be a much stronger submission.