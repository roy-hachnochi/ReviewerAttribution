This paper seems to accomplish two feats at once: it provides a rather deep dive into the specific topic of gradient flows w.r.t. MMD, while it also lays out some original propositions and theorems that establish the paper's main contributions. The first two sections of the paper are excellent and provide a solid introduction to the material the subsequent sections.  Per C1, it appears this is fully realized in Proposition 7 in Section 3.2. As an outsider to this level of detail in the field, it is unclear how strigent this assumption is to provide convergence to a global optimum. The proofs seem correct and the other assumptions (e.g. A) are quite mild, but its unclear if this condition can be even checked in a simple case.  The authors mention this can be checked in the case of a (nonweighted) negative Sobolev distance, but it would be nice to clarify if such was known for their weighted distance.  Per C2, the authors propose a method to avoid local minima both theoretically and empirically. The idea of mollifying the particles before pushing them through grad f is their way of making sure the support of the particles doesn't collapse over time.  This is demonstrated empirically in Section 4.2. There are a few questions I have for this experiment:  * How sensitive are the choices of beta_n? It appears from a support collapse perspective, you want them large, but if they are too large then they bias the diffusion. * Is it true the validation error eventually plateaus? Is this b/c the beta_n > 0? * Are there any other methods that the noisy gradient method can be compared to, e.g., SGVD?  Originality: The paper appears to provide to novel results, including many supporting propositions along the way. It is unclear how readily these results extend to GANs or the training of other NNs. The authors provide a theoretical result in the appendix (Theorem 10) but its empirically not clear how useful these updates are in comparison to standard gradient descent.  Quality: The paper appears correct, although I have not read through all the proofs in the appendix (its a 33 page paper!).  Clarity: The paper is wonderfully written. Despite the technical nature of the content, it is very clear and approachable for those not completely familiar with the subject matter.  Significance: Not being an expert in this field, I have uncertainty in my assessment on the overall impact of this paper. However, the paper is well written and provides two novel ideas relating to gradient flows for MMD, which seem like a worthy contribution to the field.  APPENDUM: Upon reading the other reviewers critiques and the author feedback, my score here stands. It isn't exactly clear how much weaker the assumptions used in Proposition 7 are than the state-of-the-art in the literature, so I'm unable to make any meaningful changes.