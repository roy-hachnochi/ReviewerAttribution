This paper presents an interesting approach of using the duality relationship between Code Summarization (CS) and Code Generation (CG) to improve the performance of a neural model on both tasks simultaneously. The main idea is to exploit the fact that the conditional probability of a comment given some source code, and the conditional probability of source code given a comment, are both related by their common joint probability. Moreover, since both the tasks of CS and CG use an attention-based seq2seq architecture, this paper also proposes to add an additional constraint that the two attention vectors have similar distributions, i.e. the attention weight of comment word i to source token j for the CS task is similar to the attention weights of the same pair for the CG task. The method is evaluated on two datasets of Java and Python programs/comment pairs and the dual training outperforms several baseline methods including the same architecture trained without dual constraints (basic model).  Overall, I liked the idea of exploiting the dual relationship between the code summarization and code generation tasks. The proposed dual regularization terms relating to the factorization of conditional probability distributions and similarity of attention matrices are quite elegant. The experiment results also significantly improve the baseline approaches, and the ablation results show that both the duality constraints are useful.  One thing that wasn’t clear to me was which parts of the dual relationship modeling were novel and which parts were taken from previous works. For example, Xia et al. [2017] proposed a supervised learning approach for imposing duality constraints and presented a similar probabilistic duality constraint (similar to Equation 8). The learning algorithm also seems similar except with the addition of the second regularization constraint. Is the only novel thing proposed in the paper is the dual regularization constraint corresponding to the similarity of attention vectors in Equation 9?  In the experiments, is Basic Model the same as the seq2seq with attention model? In the DeepCom [Hu et al. 2018a] paper, the DeepCom model outperforms both Seq2Seq and Attention-based Seq2Seq models on summarization of Java methods. Can the authors present some insights on why the basic model might be outperforming the DeepCom model?  It was also not clear whether for comparisons between different baseline models in Table 2, all the models have comparable number of trainable parameters? In the dual task, there are essentially twice the number of parameters, so it would be good to state how to compensate the baseline models with equal number of parameters.  In section 4.1, the text states that the original Python dataset consists of 110M parallel samples and 160M code-only samples, and that the parallel corpus is used for evaluating the CS and CG tasks. But in Table 1, it seems there are much fewer samples (18.5k) for Python dataset. I was also wondering why not use the comments from the parallel corpus to pre-train the language model rather than using the language model for Java dataset.  In section 4.2, it states that the best model is selected after 30 epochs in the experiments. Is this the case for the basic model or the dual model? Also, is the case for Java or Python dataset?   Minor:  page 1: Varies of → Various page 1: Specifically, z → Specifically page 1: studies before has → studies before have page 2: attracts lots of researchers to work on → (possibly something like) has attracted a lot of recent attention page 4: larger than it of → larger than that of    