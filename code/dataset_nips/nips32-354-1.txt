This paper proposes a hard attention model named Saccader together with a pretraining procedure for efficient training of the model. The network is pre-trained in two parts using self-supervision, and the whole network is trained after that. The use of hard attention allows for understanding the image with only a small portion of the original image, and also enables understanding which part of the image is useful for classification. The authors experiment on ImageNet and discuss possible applications to other image-based tasks.   Obtaining a good hard attention models is intriguing due to the computational cost it might save and the interpretability it brings. I think training a hard attention model is an interesting and important task, and the proposed model and pretraining procedures are straight-forward and seem to be well-motivated. I do have a few concerns (listed below) but at my current understanding I believe the authors have proposed an effective model that is widely applicable.   Some concerns:  - The experiment section could be improved. It would be helpful to include a comparison of computational cost and/or parameter counts of the Saccader to other image classification networks, either with hard attention or not. The reported accuracy could be more impressive if the size of the network is taken into account. It is probably also helpful to try the network on large-scale and/or fine-grained datasets.  - I feel the description of the model could be improved in terms of clarity. For example, I don't think I see in section 3.1 how the final prediction is made--is it based purely on the "logits" at the predicted location or does the network also see the original image at given location? Also the description of the Saccader cell seems a bit fast to me, a sentence or two on what equations (1), (2), (3) do might help.  - Does the "location network" in line 137 refer to the attention network, the 1-by-1 conv and the Saccader cell? Might be helpful to include an introduction before the term appears.