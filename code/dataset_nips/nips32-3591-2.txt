The proposed architecture is well motivated and interesting, and the paper is well written, however, the evaluation is less convincing.  The new architecture is evaluated on a set of arithmetic tasks where it has generalization capability on par or worse than the NeuralGPU (depending on the task) but is significantly faster. Sometimes, we want to make a trade off between generalization and how fast the model is, but it is not clear how the lower generalization performance compares to other similar models and so whether it is still relatively good. The model can generalize to longer sequences better though.  The second task the model is applied to is a real and difficult natural language task Lambada where the model performs well compared to other models but does not outperform the Universal Transformer, which is fine. My concern is that this task has short inputs (up to 128 tokens) and so doesn’t really necessitate the main selling point of the new model architecture: scalability to much longer sequences and faster run time / better complexity. Hence, even though the eval time is faster than the Universal Transformer (by about 3x?? at 128 tokens) this doesn’t support that this new architecture will work well in the setting it’s designed for (for long inputs).  The paper has a nice ablation study of the network (without Benes, without swap, without residual) that shows differences for the multiplication task but the ablation study on the Lambada task doesn’t seem to support that all the new features of the model are important.