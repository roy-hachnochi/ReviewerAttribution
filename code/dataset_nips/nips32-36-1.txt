Comments -------- notation comment -- it would be great to simplify the readability of the notations, particularly the 'asterix' was hard for me to read. Here is one suggestion that I have, please consider adapting it, {x_i, y_i} as the labeled set, y_i \in {Y}, and {u_i, z_i} as unlabeled set z_i \in {Z} and Y \intersect Z is null.  motivation of why to use VAE -- "In order to capture the complex distribution, VAE can be a useful tool." seems a bit weak, there are a few concrete choices here, VAE vs specificying a parametric generative distribution vs GAN. Given the experiments on image dataset, why VAE ?  A key difference from prior work is the reliance of the number of unseen classes and their attributes to be known apriori, I wonder how realistic this assumption is, most other work rely on a new attribute (a.k.a class) being provided at inference time, whereas this work cannot handle that scenario.   A number of key ablations seem to be missing, (a) How much is the EM actually helping ?  (b) how much is the *multi-modal* prior actually helping ? (c) how big are the model parameters, are they comparable to other work ? (d) if the number of EM steps is just 1, this is similar to some of the other cited work. Specifically, when the parameters are optimized based on seen classes fully and the unseen classes attributes are used to generate examples on which a deep-NN classifier is trained. How much of the benefit comes from having a more expressive deep-NN vs a simpler baseline like AVM ? What if we made some work (there are many, just to cite one from the paper - 'Generalized Zero-Shot Learning via Synthesized Examples') use a deep-NN than the svm used by the cited work ? (e) How much of the benefit even just from this work can be improved by using a discriminative deep-NN instead of a generative one ?   Main concern [Originalty & Significance] ----------------------------------------  Significance: Most of this work to me seemed to me like "yes, I think if you do that it is likely performance will improve compared to other work", I found the work to be more like a bag of tricks than some key insight that the work was providing. While overall this is not a bad thing and there can be good work that improve the sota on well-established benchmarks, this particular work worries me in tending to just improve end-2-end scores without providing any key-insights.   Originality: AFAIU, the new insights seem to be using EM, and multimodal, both of which are relatively mild. Focusing on running ablations will perhaps help the authors to focus on the key contributions and refine their work.   After rebuttal ------------------  Thank you for the responses and the references, they were helpful. They have addressed my questions/concerns (partially). Overall, I think there is still room to perform clearer ablations to study the effect of each component, for e.g. one proposal/contribution here is the generation of samples for unseen classes.. for a practitioner it would be great to have some clear set of experiments proving its utility (as a thought, the generated samples can be used to train a discriminative classifier than continuing to be a generative one.) 