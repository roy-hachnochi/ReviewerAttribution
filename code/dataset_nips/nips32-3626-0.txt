This paper shows that GD/SGD can minimize the training loss of RNNs with linear convergence rate assuming the hidden layer width is sufficiently large (polynomial in data size and time horizon length). In order to prove this, the authors show that within a small region around the initialization, the norm square of the gradient can be lower bounded by the function value (Theorem 3). The authors further show that the loss function is somewhat smooth (Theorem 4), which guarantees that moving in the negative gradient direction can decrease the function value.   This paper builds new techniques to analyze multi-layer ReLU networks. This paper shows that with appropriate initialization, ReLU activations avoid exponential exploding and exponential vanishing. This paper also shows that within a small region around the initialization, the multi-layer networks is pretty ``smooth’’. These techniques are very useful in the analysis of multi-layer networks and have been used in many following works.  This paper is an important step towards the optimization theory of RNNs. The RNNs is much harder to analyze because the same recurrent unit is repeated applied and at initialization the randomness is shared across layers. This paper uses randomness decoupling techniques to analyze the spectral norm of RNNs at initialization. These techniques can be useful in other problems of RNNs.  Overall this is a strong theory paper proving that GD/SGD can optimize RNNs in the over-parameterized setting. The techniques developed in this paper can be very useful in the analysis of multi-layer ReLU nets.   My only concern is that the required hidden layer width is a polynomial in the number of samples (might be high order polynomial), which is not very practical. Also, the step size is very small, and the total movement of the weights is very small. In practice, the step size is much larger, and the weights move a lot. So, in some sense, this theory cannot explain the success of over-parameterization in practice. Reducing the dependency on m might require a very different idea.    Here are some minor comments: 1. Line 142: shouldn’t it be h = D(Wh + Ax)? Ax is missing here. 2. It might be good to add a figure to illustrate the network architecture if space allows.   -------------------------------------------- I have read the authors' response and other reviews. The authors have partially addressed my concerns on the network width and step size. I agree that as long as we assume the data is generated by some simple model, the requirement on the network width can be significantly reduced. Regarding the step size, the authors argue that this work can give some intuitions on the second phase of NN training when the step size decays and training loss goes to zero. However, the weights have moved a lot in the first phase (when the step size is large) and are not random anymore, it's not clear whether the current techniques can still work after phase 1. Despite these limitations, I still think this is a good theory paper and I will keep my score.