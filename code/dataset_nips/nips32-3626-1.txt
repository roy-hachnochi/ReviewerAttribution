The paper proves that with overparameterized RNNs (polynomial in the training data), GD/SGD can find a global minima in polynomial time, with high probability, if the neural network is initialized according to some distribution (and is based on a three-layer RNN with ReLU activation functions).  The paper does not eliminate the existence of worse local minima for proving  these results, which means even if there are bad local minima, with high probability GD/SGD can still find a global minima. This could explain why in practice overparameterized networks perform better.   Overall I feel the paper helps understanding how neural network works in practice. It can be better if an easy to understand intuition of why overparameterization helps (both proof and practical problems) is provided.   ====  The authors have answered my concern on better explaining the intuition. I'll keep my score.