The authors propose a phenemenological model of the loss landscape of DNNs. In particular they devise the landscape as a set of high dimensional wedges whose dimension is slightly lower than the dimension of the full space. The authors first start by building a toy model where they show that the assumptions hold. Afterwards they run experiments with a simple CNN model to show the behavior of the loss landscape. They also show how the optimizer traverses the loss landscape for common hyperparameter choices.  Overall I think the paper is well written and the different observations are sound.  I like the view the of the loss landscape presented in this paper. I also like the experiments that show the behaviors of the different hyperparameters as well. One thing that would have excited me more is a discussion regarding how this view of the loss landscape helps with training (I know it is hard to comment on immediately).    Minor Comments: * Typo in line 8: a similar ways -> similar ways * Line 191: is it SimpleCNN or a simple CNN model?  After Author Response:  I thank the authors for their response. I appreciate that you have run more experiments and that the results look promising. But at the moment I am not going to update my scores without seeing the results.