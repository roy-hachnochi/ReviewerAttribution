This paper studies the large-scale structure of neural network objective function (loss landscape). It uses a new idea to not only confirm some known properties of neural network loss landscape but also introduce some new prperties. The authors use the idea of wedges to show three previously known properties of loss landscapes as they call it (1) long and short direction (2) distributed and dense manifold, and (3) connectivity. I should say that because of poor structure of the paper, I could not understand the core part of this paper in which they construct the wedges. All arguments in this paper are based on the concept of wedges. Even though the authors are presenting some nice pictures to make their idea better understood but the text and the pictures are not connected. For example, I do not understand why wedges are disks in Figure 1 when the wedges are supposed to be cuboid (maybe I do not understand the meaning of cuboid here).  Another assumption that all other arguments in the paper is built upon is that the loss values stays constant on a wedge. Also the authors are talking about long and short linear dimensions, which I cannot connect. These terms and concepts need to be more explained and defined. 