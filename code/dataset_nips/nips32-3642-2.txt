This paper is clearly written and the results are interesting and mathematically sound. The unification of the complexity-based and stability-based analysis into learning with data-dependent hypothesis set seems a significant contribution. Under the assumption of data-dependent stability, the generalization ability was proved in Theorem 2. The bound in Theorem 2 was obtained by combining several theoretical techniques. Then, Theorem 2 applies to a wide range of learning algorithms.  minor comments: - I guess that the invariance under the sample permutation is implicitly assumed for the correspondence from S to H_S, i.e., the permutation of the sample S does not affect H_S. The last equation on page 3 uses such a condition. Supplementary comment on the invariance might be helpful for readers.  - line 117: The sentence "hat{R}_T(H_{S,T}) is the standard empirical Rademacher complexity" sounds slightly confusing, since H_{S,T} also depends on the sample T. In my understanding, the standard Rademacher complexity does not deal with such a hypothesis set.  - The term "sensitive" was separately defined in Section 5.2 and 5.4 but not in Section 5.3. It may be good to show a general definition of the sensitiveness before Section 5.2.  ----- after Author Response: Thanks for the response. All points in my comments are now clear.   