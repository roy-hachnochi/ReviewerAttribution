Overall, this paper proposes a novel and interesting idea, the writing is done well, and the numbers seem to back up most of the contributions. Below are my detailed comments.   The proposed framework takes a 3D point cloud as input and outputs a fixed number of object hypotheses. For each hypothesis, the method regresses an axis-aligned bounding box, an objectness score, and a point mask. In order to set regression targets, the authors propose a novel association layer to associate each ground truth object to one object hypothesis. This is done by solving Hungarian matching on a hand-crafted pairwise cost matrix. To backprop through the non-differentiable matching process, the authors propose to estimate the gradient using Policy Gradient.   As far as I know, incorporating Hungarian matching to associate hypotheses and ground truths as part of an end-to-end pipeline is novel. However, I feel the difference of this particular design was not emphasized enough neither in the writing and the experiments. For example, in terms of the one-to-one mapping, it makes sense to map every hypothesis to at most one ground truth object, but it is not as clear why we do not allow mapping one ground truth to multiple hypotheses. My concern comes from the fact that many well-known approaches (e.g. MaskRCNN) allow such ground truth “sharing”. I am aware that this might result in duplicated predictions and these methods often run NMS. Nonetheless, an empirical comparison one-to-one and one-to-many mapping would help justify the importance of this particular design choice. On a similar note, it would be interesting to see an empirical comparison between solving one-to-one mapping optimally and solving it greedily.   I am also curious about what these MLPs learn. They might get mapped to different ground truth randomly after initialization. But after a while, do they learn to consistently respond to a specific part of the point cloud (or block)? My lack of understanding what these MLPs learn makes me wonder what happens if we apply the same idea presented in this paper to instance segmentation on images. Specifically, we would learn a model that takes an image and spits out scores, bounding boxes, and pixel masks. Though it seems viable algorithmically, I am not sure if it would work better than MaskRCNN. I wonder what the authors think about this idea.   Regarding Algorithm 1: First, I was not sure how to interpret the step 1. My best guess is that it computes the squared distance of the n-th point to the min/max vertices of the i-th bounding box. If so, p_{xyz} should be a scalar value but why do we take the minimum over p_{xyz} in step 4? In addition, it looks like the probability is biased towards larger bounding boxes. I thought instead of Euclidean distance, it makes more sense to normalize distance w.r.t. each dimension. 