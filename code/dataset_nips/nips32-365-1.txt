I have read the author response and found that the authors significantly addressed my concerns -- they provided new theoretical and empirical results. The results are convincing, and I increase my score.  -------  In this paper, a tensor block model -- a multiway extension of a stochastic block model -- is studied. The main contribution is to derive a statistical convergence rate of the least square estimator under sub-Gaussian noise. The authors try to confirm the theoretical results by numerical simulations.   The strength of this paper is in the theoretical result, which improves the existing convergence rate and also proves the consistency of clustering results. However, my current evaluation is slightly below the border of acceptance. The follows are my major concerns.   Originality. The problem of tensor block models has been at least studied since Jegelka et al. (2009), who proposed an efficient algorithm with an approximation guarantee. Chi et al. (2018) derived a statistical convergence rate earlier than this work. Thus, strictly saying, the originality of this paper is to improve the convergence rate. Although a few extensions such as sparse estimation is proposed, I feel they are somewhat incremental.   Measuring MSE in a clustering problem. The convergence rate studied in this paper is for the mean square error between a true (noiseless) tensor and a recovered tensor. I agree with this setting if it is for a tensor recovery problem. However, how about the MSE setting for a clustering problem? For example, suppose we have two estimators A and B such that MSE(A) <= MSE(B). Can we say the clustering result of A is always better than B? I mean, it seems there is a gap between MSE and the correctness of the clustering, and I'm not sure measuring MSE is a right thing for clustering.   Toy data experiments. In Figure 2, I'm not convinced that the results are consistent with the theory. Specifically, I feel a gap between (4,4,4) and others. This may be because the range of the x axis is different between (4,4,4) and others after rescaled N. Also the number of R settings is not large enough.   Real data experiments. The baselines, CP and Tucker decompositions, are general methods for tensor decomposition but not proper methods for clustering. It should be compared with Jegelka et al. (2009) and Chi et al. (2018).