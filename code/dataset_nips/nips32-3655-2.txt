Thanks to the authors for the helpful response and the additional experiments.  Original Review:  This paper makes an argument that standard LDA is biased in favor of recall, and it presents a method that can remove the bias.  In experiments, the new method performs well on a precision metric and on topic coherence.  This paper seems to be making an interesting insight.  However, I had a hard time understanding the arguments, and I think the paper’s analysis and experiments do not sufficiently evaluate how much the recall focus of LDA depends on specific choices for hyperparameters or symmetric priors.  The derivation around Equation 1 is true for any model that is trained to maximize likelihood, so when the paper declares there that Equation 1 is “sensitive to misses” it is hard to understand why.  It is not until later that the paper aims to explain, but the explanation comes only in terms of a uniform model and the argument is made informally.  It would help me if the paper stated its results more formally, as a conjecture or theorem with proof, so that I can better know what is being claimed, and to what extent the result relies on assumptions like uniformity.  Also, the paper would be stronger if its mathematical illustrations used more realistic settings (e.g. Zipfian distributions for word frequency).  The algorithm ends up being a fairly simple variant on LDA that mixes in a document-specific distribution (estimated from the ground truth document counts) along with the standard topic model.  As the paper says, this means that the model is not a complete generative model of documents, because its likelihood requires knowing the counts of words in each document (b_m) in advance.    In the experiments, the paper does not consider altering the LDA hyperparameters.  This is a concern because the fact that precision and recall are monotonic in topic entropy (Fig 1) suggests that simply tuning the LDA hyperparameters toward lower-entropy topics might serve to boost precision at the cost of recall.  In particular, allowing an asymmetric gamma that reflects corpus-wide word frequency would start to look somewhat similar to this paper’s model, if I am not mistaken, and I think the paper needs to experiment with that approach.  Minor: The empirical results state that in their experiment “LDA topics capture frequently occurring words but the topics are not as meaningful and do not correspond to any evident themes”---we know that very often LDA does infer meaningful topics, so investigating and explaining why it failed here would be helpful.