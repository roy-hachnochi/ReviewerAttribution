[I have read the author rebuttal. Since it does address my essential concern of whether the performance observed in the paper generalizes, I've upgraded my score.]  Summary:  I liked the step in the direction of a more principled and generic approach for leveraging dataset characteristics for hyperparameter selection. However, the limitation to a single model class and instance space appear to constrain broad applicability. Moreover, even in this restricted setting, there is a fairly large design space to consider: embedding distribution or combination of, neural network architecture(s) for the embeddings and potentially for the BLR data matrix, acquisition function settings, optimization settings. This makes we wonder how easy the approach would be to apply generally. A broader experimental section would help assuage this concern.  Originality:  The incorporation of sampling distribution embeddings within BO for hyperparameter selection is novel to my knowledge.  Quality:  The two toy examples are nice proof-of-concepts, but a broader set of experimental results on interesting real-world scenarios would go a long way in providing convincing evidence that the restricted setting and various design decisions aren't issues. The results on the real dataset are positive for the proposed method, but underwhelming in that the easier utilization of a kernel over dataset meta-features (manualGP) and warm-starting with dataset meta-features (initGP) also perform fairly well in comparison.  How did the authors make their decisions on the neural network architecture, acquisition function, and optimization settings?  Clarity:  I don't have major comments here. I found the paper fairly easy to follow. Also, even though I have concern regarding the design space, I would like to point out that the authors have provided a lot of detail on their experimental setup.