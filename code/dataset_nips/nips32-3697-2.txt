- Originality:  I am not that familiar with the space of high-dimensional forecasting with modern deep learning methods.  - Quality: This paper appears technically sound, the ideas are sensible and the experiments do a good job empirically testing the approach.  Like all empirical investigations, more can be done --- in this particular case, there were are some tuning parameters that could affect performance that require (e.g. time series embedding vector size M and MSE/log like or time-varying residual analysis).  - Clarity:  This paper is very clearly written and the details of the model and training algorithm are thoroughly described.  Some questions below address clarity.  Other specific questions and comments:  - line 76: This was unclear to me --- the pieces are of size epsilon^{-N}?  Or there are that many pieces? - line 82: "order of magnitude larger than previously reported" needs a citation and to be made more precise. - line 85: "a principled, copula-based approach" is vague --- what principles are you referring to? - line 105: "...LSTM is unrolled for each time series separately, but parameters are tied across times series" --- what assumptions about the data does this particular model constraint encode? - line 124: Are all training chunks of size T + \tau?  How well does look-ahead forecasting perform when the number of steps is greater than or less than \tau?  Can this be made more robust by increasing or decreasing training chunk sizes? - line 135: How are these empirical CDF marginal distributions specified within the model?  Do these distributions describe the observed marginal distribution of time series data?  Or do they model the distribution of the residual given the model mean? - line 141: How does the discretization level m (here m=100) affect speed and model prediction accuracy? - line 164: How important is the embedding e_i?  How does forecasting perform as a function of feature vector size E? - line 175: I don't fully follow the logic --- why does this Gaussian process view enable mini-batches? - line 186: How long does the time series need to be to fit a large LSTM that captures the dynamic covariance structure?  In this synthetic example, how does the approach deteriorate (or hold up) as T shrinks? - line 225: regarding CRPS: it would be nice to give a short, intuitive explanation of CRPS and how it is different from other metrics, like log likelihood or MSE?  Why not report MSE and log likelihood a well? - Table 2: How are the CRPS-sum error bars being computed? - Line 242: Besides the larger test error and a higher number of parameters do you see other signs that the Vec-LSTM is over-fitting (e.g. train vs test error)?