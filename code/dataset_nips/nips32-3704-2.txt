This paper studies the challenging problem of learning from data with noisy labels. In particular, a novel transition revision (T-Revision) method is proposed, which does not require anchor points. T-Revision can effectively learn transition matrices which lead to better classifiers. A deep-learning-based risk-consistent estimator is designed to tune transition matrix accurately. Experimental results on multiple benchmark datasets show that T-Revision outperforms the state-of-the-art methods.  This paper makes a significant contribution to the label-noise learning problem. The proposed method is well motivated and clearly presented. Technical details are easy to follow, and theoretical analysis on generalization error is provided. Moreover, the implementation details of the proposed method are also provided, which will be very helpful in reproducing the reported results.  My comments are as follows. 1. The difference between two categories of label-noise learning algorithms has been mentioned in Section 1. It will be helpful if the authors can elaborate more about the advantages of risk-/classifier-consistent algorithms. 2. In the experiments, the Sym-50 setting on MNIST can still obtain a good performance (about 98%). I wonder what the performance would be in extreme cases with even more label noise. 3. I'm curious if the proposed mechanism can be easily extended to semi-supervised setting.