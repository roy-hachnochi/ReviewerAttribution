The authors present a method for training overcomplete generative ICA models using a GAN approach with no posterior or likelihood calculation.  Overall, the methods is clearly described and a simple method to use GANs to perform OICA. The evaluations are clear if somewhat limited in scope. The generative model does not have a corresponding latent inference algorithm, which limits its use.  Comparison with other OICA models RICA will likely find degenerate bases when overcomplete [1] and so may not be a great comparison method. Score matching might be a better OICA method [2]? However, score matching ICA models are better described as analysis models and may not be well matched to generative recovery [3].  It would also add to the generality of the method if the performance on a higher dimensional dataset was explored. Natural images patches are commonly used. Does this methods scale up?  Small comments Line 89, “some empirical estimator” What estimator are you using?  Table 1, what are s.e.m.s? How significant are differences?  [1] Livezey, Jesse A., Alejandro F. Bujan, and Friedrich T. Sommer. "Learning overcomplete, low coherence dictionaries with linear inference." arXiv preprint arXiv:1606.03474 (2016). [2] Hyvärinen, Aapo. "Estimation of non-normalized statistical models by score matching." Journal of Machine Learning Research 6.Apr (2005): 695-709. [3] Ophir, Boaz, et al. "Sequential minimal eigenvalues-an approach to analysis dictionary learning." 2011 19th European Signal Processing Conference. IEEE, 2011.  POST RESPONSE UPDATES: Author response addressed my concerns.