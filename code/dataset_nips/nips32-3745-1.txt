[Concerns in Modeling] 1) Isomap was developed to extend the multidimensional scaling (MDS) by determining the neighbors of each point and measuring the distance by computing the distance by finding the shortest path via Dijkstra algorithm. Except the presence of each edge is probabilistic than deterministic, the core idea is quite similar to Isomap. The novelty should be better addressed by comparing to Isomap.  2) If there are many nodes and the graph is almost connected, is PRODIGE still scalable when finding the shortest paths between all pairs of nodes?   3) Typically, edges in the graph is far from being independent. For example, edges between words that frequently co-occur in the same contexts are not independent to each other. Edges between pixels in small coherent regions are not independent. Do we eventually need to know such dependency structures a priori to correctly represent arbitrary geometry in the data?  4) The suggested applications often require knowing the similarity in the original data space. However, in many cases, those similarity metrics are just the convenient choices rather than the correct metric. If we know the cosine similarity is the right metric to compare two different user-item preference vectors, for example, it automatically means our vanilla inner product space is the right realization of the data geometry. Can PRODIGE still learn the right/better geometry even if the original similarity metrics are incorrect?  5) The performance of Isomap is highly sensitive to the definition of neighbors (e.g., how many nearest elements will be declared as my neighbor). Is PRODIGE robust to such effect? As a related question, what is the reason to add synthetic anchor objects rather than using the known hub objects as anchors?  6) What is the purpose of adding random edges? Are 32-100 edges mentioned in Line 144 is general or specific to the particular tasks?  7) What happen if PRODIGE tries to represent already the data that is already represented by a graph? In other words, what happen if PRODIGE represent the graph data as its own graph with the same number of nodes?  [Concerns in Evaluations] 1) I saw most of evaluations are done within highly limited memory budgets. Does PRODIGE show the same performance boost against the pure representations when we increase the budget to a usual laptop-sized memory?  2) What is the reason to tune the lambda parameter for matching the memory space rather than measuring the test error on the development set?  3) For the qualitative results by t-SNE visualization, is it true for all graph that non-hub vertices tend to have only a few edges? If hub vertices are defined as nodes with many edges, it is trivial that non-hub nodes have only a few edges. This property is generally called scale-free. It is better to quantitatively argue on the graph space by measuring whether the degree distribution follows the power-law.  4) Are SVD, ALS, and Metric Learning used in Table 2 the state-of-the-art approach for the collaborative filtering? If they are vanilla SVD, ALS, and metric learning, it may not be difficult to outperform.  5) What is the relation between the number of anchor objects and the performance in sentiment classification? Does increasing the dimension of the node embedding (by increasing K) keep improving the performance?   ------------------------------------------------------- [After the author feedback] I thank the authors for their feedback to various concerns and additional experiments. I have read all other reviews and the corresponding feedback. Overall empirical results (especially the additional ones shown in the author feedback) seem useful and convincing, upgrading my original evaluation. But modeling the edge dependency and overall scalability from computing all-pair shortest paths is not clear. If the paper gets accepted, I encourage the authors to address these points in addition to adding many useful answers in the provided feedback.