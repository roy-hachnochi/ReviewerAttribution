--- Added after feedback --- All reviews are in agreement that this is a strong contribution and given that consensus and the convincing nature of the author response, which carefully addressed all of my concerns, I have increased my score to a 9. I firmly believe that the technical developments in this paper will enable further research on the detailed dynamics of neural networks within and beyond the student-teacher paradigm.   --- Original review ---- In this compelling and well-written paper, the author(s) derive a closed-form and tractable system of ODEs to study the dynamical evolution of neural networks using a "student-teacher" paradigm. While the student-teacher set-up, in which one attempts to learn a neural network with a neural network of similar architecture, is perhaps not the most practically significant case, this framework also for a precise formulation of notions that are often left vague, most notably "over-parameterization". Typically, a less meaningful comparison between the cardinality of the data set and the number of parameters in a neural network is used. In this case, it is much more straightforward, simply being the discrepancy between the number of student vs teacher hidden units.  I found the analysis to be satisfyingly complete, deriving asymptotic expressions for sigmoidal, linear, and ReLU networks. For the soft committee machines, numerical experiments seem to agree extremely well with the asymptotic expressions, to the extent I left wondering what magnitude of output noise is required in order for the asymptotic expressions to fail. Perhaps my only major complaint about the paper is that it does not describe the numerical experiments in detail. There is a promise of eventual github links, but I would have liked to know, at least at a high level in the appendix, how they were set-up, how long they were run, what was used as a stopping criterion, etc.   The paper contrasts the case of the soft committee machine in which the weight of every neuron is unity with the more general and more typical set-up of training both the pre-activation and the post-activation parameters. The results are different in the two scenarios---to a surprising degree. Some of the work on mean-field neural networks that the authors cite is related to this phenomenon, I believe: the dynamical accessibility of optimal solutions hinges on the evolution of the linear coefficients of the neurons.   Regarding clarity and quality: While I have a number of minor comments on some of the logical steps in the proof, as well as the more significant point about the lack of information with regard to the numerics, on the whole the paper is sufficiently clear. Overall, I found the paper to be of a high caliber, both timely and complete.   I have a few comments about the proof of the paper's main theorem:  - The meaning of $\mathbb{E}_{\mu} $ is not clearly stated anywhere that I could find, but I took it mean an expectation over the initial conditions taken at time-step $\mu$ - The use of $m$ and $q$ is fairly confusing---$q$ only contains the time-dependent order parameters that involve the student, whereas $m$ contains both the student and teacher order parameters. Is it necessary to separate the two rather than just using $m$ throughout? - The step going from S11 to S12 is not well-explained. While the assumption that the order parameters are uncorrelated at $\mu=0$ seems perfectly fine, this does not necessarily imply $q^0$ is independent of $f_q(m^0, x^0)$. As a result, there appears to be an extra expectation in the first line of S11. If as written the equation is correct, I think a more detailed explanation is merited.  - The discussion of the coupling trick glosses over the functions $d$ and $g$ and what properties they require    *Specific comments on originality*  I would not say that the question being investigated is particularly original, nor is the basic framework of using order parameters and deriving ODEs for the evolution of these parameters. As the authors acknowledge, this basic approach has existed in the statistical mechanics literature for many years. However, I would emphasize that using this approach to resolve questions regarding implicit regularization in SGD and quantify over-parameterization directly is a novelty.  Below are some entirely inconsequential typos that I noticed which the author(s) may want to correct:  Line 156: "case studied most commonly so far" seems like a hard thing to know  Line 196: misplaced "*"  Line 255: "fulfil" -> fulfill   Line 285: "exemplary" is a bit awkward due to the connotation of "exceptional" rather than "typical"  Fig S2 is highly pixelated  Fig S5 caption "sigma" -> "\sigma"  SI 548: "namely namely"  SI 640: "to first order \sigma^2"