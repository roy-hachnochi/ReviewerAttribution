Summary: This paper studies a generalization of online reinforcement learning (in the infinite horizon undiscounted setting with finite state and action space and communicating MDP) where the agent aims at maximizing a certain type of concave function of the rewards (extended to  global concave functions in appendix). More precisely, every time an action "a" is played in state "s", the agent receives a vector of rewards V(s,a) (instead of a scalar reward r(s,a)) and tries to maximize a concave function of the empirical average of the vectorial outcomes.   This problem is very general and models a wide variety of different settings ranging from multi-objective optimization in MDPs, to maximum entropy exploration and online learning in MDPs with knapsack constraints.  In section 2 the authors introduce the necessary background and formalize the notions of "optimal gain" and "regret" in this setting. Defining the "optimal gain" (called the "offline benchmark" in the paper) is not straightforward.  In section 3, the authors first show how the exploration-exploitation dilemma faced by the agent is intrinsically more challenging since the optimal policy may be non-stationary (example of Fig. 1 and Claim 3).2. They then introduce a variant of UCRL2 (called TFW-UCRL2) which can efficiently balance exploration and exploitation in this setting. The algorithm mainly differs from UCRL2 by the stopping condition of episodes: an episode also stops whenever the "gradient " becomes too far from its initial value at the beginning of the episode. This prevents TFW-UCRL2 from converging to a stationary policy whenever the optimal policy is actually not stationary. The gradient is updated using a Frank-Wolfe algorithm. When the objective function is both Lipschitz and smooth, the regret of TOC-UCRL2 can be bounded by O(1/\sqrt{T}) (Theorem 3.1).  A detailed proof sketch of Theorem 5 is presented in section 4 (the main steps and arguments are given).  Finally, in Section 5 the authors report experiments illustrating the theoretical findings.   Clarity: Overall the paper is well-written and clear. All the statements are very precise. The examples of Fig.1 are very helpful to understand the challenges of the setting (and they are well-explained).   Correctness: The paper is very rigorous (both the main body and the appendix). The proofs are very detailed and still concise (and thus easy to read and verify). The derivation of the regret proof of Theorem 3.1 is very rigorous while being easy to follow.     Significance: In my opinion, the contribution of this paper is quite substantial and non-trivial. The paper (including the appendices) makes a comprehensive and insightful analysis of the problem addressed in the paper. In my opinion, the problem addressed is very relevant for the community (the setting and results are very general).