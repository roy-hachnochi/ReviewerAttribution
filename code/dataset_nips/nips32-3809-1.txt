This paper studies feedforward neural networks with quantized activation function by using MFT, deriving the characteristic time scale \xi which is preferred to be large because it directly determines the trainable layer depth. The result shows that \xi strongly depends on the quantization level N and diverges only in the infinite levels of quantization (N: infinity) or continuum limit. An explicit condition to maximize \xi given quantization level N is derived, yielding an interesting scaling \xi~N^{1.82} for large N. This maximization condition can be used to actually initialize the quantized networks and will be also useful for practical use. For computing the backward gradient, the authors use the straight through estimators to overcome the analytically bad behavior of the activation function. These theoretical predictions are checked by numerical experiments and the theoretical prediction on the trainable layer depth is well confirmed.   The authors also discuss about the test accuracy dynamics from a viewpoint of Neural tangent kernel (NTK) or neural network Gaussian process (NNGP) in Section 3.2 and Appendix L, concluding the sufficiently deep networks with sufficiently small learning rate and without critical initialization cannot generalize in principle, because the kernel converges at deep layers to a trivial form which cannot discriminate any test points. This point has, however, been already discussed in arXiv:1711.00165 titled ``Deep neural networks as gaussian processes'' and more detailed numerical experiments have been given there. Hence I think the authors should cite this reference. Even with this overlap with the preceding study, the main claim of this paper about the quantization effect in activation function is surely new and important, and thus I think there is no need of large modification in the manuscript.  I have one concerning point in the main text. For the experiment described in lines 208-217, I cannot find the result, though I may misunderstand something. Please clarify this point.  Originality: Application of MFT to studying the effect of quantization is clearly new and important.   Quality: The submission is technically sound and the theoretical prediction is well supported by experiments. The quality is high enough.  Clarity: The paper is well organized and equipped with nice appendices well summarizing the detailed computations. No need of large modifications. I only give the list of typos and uneasy-to-understand expressions which I found: Line 114: gradient form -> gradient from Line 215: We than -> We then Line 574: do not obtain -> are not satisfied Lines 578-579: Should explain what is N_d Lines 621-623: I could not understand the manipulations here. Could you explain them in more detail? Line 660: phenonmenon -> phenomenon   Significance: I think this work is an important one because it provides a theoretical knowledge about the quantization-depth tradeoff and the ``best'' initialization method under a given quantization level which is also useful for practitioners. These are firm theoretical and methodological advance which are never known. 