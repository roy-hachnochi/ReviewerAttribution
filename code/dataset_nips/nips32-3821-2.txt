The actual idea of cross-lingual language modeling pretraining is a natural next step in the landscape of language modeling pretraining, and coming up with the actual idea was definitely not so difficult. The difficult part was executing this very intuitive idea into solid work and empirically validate its potential. The paper is therefore not very original and methodologically novel, but it does a very solid engineering work overall.  The whole idea is largely inspired by the recent BERT model (which the authors consider concurrent work, at least the multilingual version of BERT), but it is clear that they borrow the crucial idea of Masked Language Model (MLM) from the BERT paper. The only real methodological contribution - the TLM version of the MLM objective is a simple extension of MLM to cross-lingual settings, and similar extensions have been observed before in the literature on static cross-lingual word embedding learning (e.g., see the work of Gouws et al. or Coulmance et al., all from 2015). The whole methodological contribution and novelty are quite thin.  Despite its low novelty, the paper is well-written and very easy follow, and all the necessary details required to reproduce the results are provided in the paper. The results are strong, although I'm a bit puzzled, given the similarity of multilingual BERT and this work, why more extensive comparisons to multilingual BERT have not been made in other tasks such as unsupervised NMT initialisation or language modeling transfer.   Having said that, I would like to see more discussions on why the proposed method, being so similar to multilingual BERT, obtains large improvements over multilingual BERT on XNLI. What exactly is the main cause of difference? Is it the subwords (WordPiece vs BPE), is it the slight adaptation of the MLM objective? Is it something else? Overall, why the paper does a good job in reporting a plethora of quantitative results, I feel that deeper insights into why it works, when it works, and why it works better than some very relevant baselines (i.e., multilingual BERT or LASER) are missing from the paper. I would suggest the authors to maybe remove the parts on LM transfer and cross-lingual word embeddings, as these results do not contribute much to the paper and are very much expected and intuitive (and also stay at a very shallow level, e.g., why don't the authors report BLI scores with xling embeddings or why don't they do more LM transfer)? Removing these experiments would provide space to add more analyses of the main results.  Another experiment I would like to see is adding more distant language pairs (it seems that the method works well for some more distant language pairs), but all the other language pairs tested are from the same family, and it is still debatable how well the method lends itself to applications with distant language pairs. Also, for UNMT only one method is tried. I would be interested to see if the same initialisation could be applied to some very recent robust UNMT techniques such as the method of Artetxe et al. (ACL-19).  Overall, I find this paper as a good implementation of a simple idea with some strong results, but the paper requires more in-depth analyses, and the novelty of the paper remains limited (as it heavily relies on prior work, i.e., BERT).  [After the author response] I thank the authors for a very insightful response which has strengthen my belief that the paper should be accepted despite some of my criticism.