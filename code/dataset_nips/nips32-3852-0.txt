This paper brings an novel perspective on probabilistic frameworks for new reinforcement learning algorithms, and the adaptive temperature reweighting may lead to more insightful exploration built into our RL algorithms. The paper is written clearly, and is also well-organized and easy to understand, and the appendix is structured clearly as well, although the full length of the appendix + paper makes the paper a little unwieldy to read. The authors have clearly put in a lot of work into developing the theory and presentation in this paper, and although empirically the performance of the derived algorithms do not show significant improvement over max-ent RL methods (with twin Q functions as in TD3), the approach is interesting and I believe this paper would be well-suited for NeurIPS.  Some specific comments: - In the definition of the residual error on L147, over what distribution is the L^p norm being referred to?  - Instead of e_w being a global constant, have the authors considered parametrizing e_w as a function of h - this would allow for state-adaptive uncertainty and exploration, and I believe a majority of the results would still hold. - On L96, L227-229, the paper claims that MERLIN relies "on a variational distribution to approximate the underlying dynamics of the MDP for the entire trajectory". However, most works with the Max-Ent framework parametrize variational distributions through only the action distributions, and fix the variational distribution on dynamics to the actual dynamics model. The empirical evaluation on the Gym environments doesn't validate this hypothesis too strongly, but it would be interesting to see a more carefully designed test of this hypothesis. Perhaps results on environments that require long-horizon planning (where algorithms modelling full trajectories will be less performant) may be illuminating - Why were experiments run on different domains for the comparisons with the twin Q functions, than those run with a single Q function? - Putting an algorithm box or more extensive description of the evaluated algorithm in the main text would be useful, instead of just in the Appendix - How were hyperparameters chosen for all the algorithms?