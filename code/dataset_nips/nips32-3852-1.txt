Originality: The probabilistic model and variational inference approach is new and interesting. Related work is cited and it differs from previous work through a new adaptation of the temperature of the policies.  Quality: I am not sure if I understand the definition of \epsilon_w in line 147. This seems to be recursive to me. Are you claiming that for any flexible Q there exist one (or many) softmax temperature \epsilon_w>0, so that the L^2 temporal difference error is exactly the softmax temperature? I get it in the limit but not for \epsilon_w>0 (proof?), also do you mean that the error is the same for any L^p norm? I have the same issue with the projected residual errors in Section F1. Further, the paper claims in line 231 "function approximator is used to model future dynamics, which is more expressive and better suited to capturing essential modes than a parametrised distribution". Can you explain why and give an example for this? Maybe related, the Boltzmann distribution might be more flexible than a simple tanh transformation of a Gaussian used in the Soft-Actor-Critic paper. Have you tried SAC with more flexible distributions having say multiple modes to see if the performance is due to the adaptation of the regularisation or just because of more flexible policies? Furthermore, as you consider some adaptation of the regulariser, can you compare your approach to entropy-regularised approaches that either reduce the entropy-penalty via some fixed learning rate (see for instance Geist et al, A theory of regularized markov decision processes, 2019) or optimized via gradient descent approaches (Haarnoja, et al, Soft actor-critic algorithms and applications, 2018)? Notwithstanding all these points, the submission seems technically sound in general with claims supported by theory (and with more technicalities than some related work) and experiments!  Clarity: The paper is generally well written. I find the residual error in line 147 not so clear and find the introduction of the residual errors in Appendix F2 clearer and more plausible instead. Also there seems to be bit of a disconnect between the exposition of all the gradients in the main paper through a variational inference perspective, and then the algorithm pseudocode in the appendix that more or less uses policy improvement and Q-TD-error minimization. Can you elaborate more on those loss functions like J_virel(\theta), why do you have a constant \alpha there, does \epsilon_w depend on t in J_beta(\theta)?  -Significance: The idea of a variational EM scheme to address RL is useful and I expect that others can build up on these ideas, either theoretically (like what can be said about performance errors) or empirically. The approach appears to be competitive with state-of-the art approaches.  -Minor points: You say in line 969 that " minimising \eps{w,k} is the same as minimising the objective \eps_w from Section 3.1". Why? Is this not contradictory to lines 993-5? missing gradient after 843 w' versus \tilde{w} in 920 line 929   POST AUTHOR RESPONSE: I thank the authors for their feedback. Having read it along with the other reviews, I keep my initial score of 7. The rebuttal provides some clarification on the definition of \epsilon_w and indicates that for the Bellman operator, further theoretical work might be worthwile. They have also given some clarification concerning the flexibility of the parameterisation used for the policies.  The authors also intend to reference additional related work that consider different types of adaptation for the entropy coefficient/penalty. While it would be nice to have some empirical comparison with such work, even without it, I think this is still a complete, long enough and interesting paper and I vote to accept it.  