The writing of the article could be improved. Besides a few typographical errors across the paper (lines 72, 160, 160), there are some other parts where sentences could be rephrased, for example: 24: 'apply them' -> 'deploy them' 47: rephrase whole sentence 68: 'to develop the powerful networks'-> 'to deploy the deep networks' 87: Do not use 'while' as 'though' unless it is at the beginning of a sentence. 98: Rephrase whole sentence.  The style of the article could also be improved. State more clearly the main contributions of the paper in the introductory sections and use Fig. 1a and Fig. 1b, instead of saying 'the first line of Fig. 1'. ----------------------------------------------------------------------------------------------  The proposed method is quite original, as if attempting to 'grow' a reduced network, instead of pruning a larger one. The NAS method is also very elegant mathematically; since it is setup as a differentiable problem, now the error can be propagated and gradient descend family of methods can be used to search a locally optimal solution (a structure in this case) in an efficient way.  Having said that, yet this is another case of apply relaxation to a hard problem and call it a day. In fact, it is not easy to spot how much of the proposed TAS method is new or just a variant, a minor incremental improvement, or an use case, of the work already presented at [1].   [1] Dong, Xuanyi, and Yi Yang. "Searching for a robust neural architecture in four gpu hours." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.