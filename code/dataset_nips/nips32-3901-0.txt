1. motivation of the method. I do not quite understand the motivation of the work. Although the method gives better empirical performance, why is the GAN objective necessary? This is not justified either intuitively or theoretically.   2. a missing baseline In Section 2.3, the "entropy regularization for LLP" appears to be a plausible idea. It adds a low entropy term to encourage high labeling confidence. The authors presented this section but "will not look at the performacne of this extenion in this paper". This is strange, and I think this should be an obvious baseline in the experiment.  3. details of the optimization After Remark I: "it is easier to perform the gradient method on the lower bound, because it swaps the order of log and the summation operation." I cannot agree with the argument here. All popular deep learning packages are based on automatic differentiation. Implementing one is not more difficulty than the other.  -----after rebuttal and discussion --- Most of my concerns have been addressed in the rebuttal. I suggest adding the new results in the final paper. 