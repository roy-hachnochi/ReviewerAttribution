# Originality The submission mostly combines the domain adaption loss Maximum Classifier Discrepancy [23] with additional learned local features "Local Feature Alignment" to point cloud classification tasks with unlabeled target domain samples. The driving classification architecture is borrowed from PointNet.  The main contribution here lies in the local features that bring up the predictive performance on the target task: small regions, which are centered around sample point cloud points, are first moved with a learned offset (to better support commonalities of the current object) and then weighted by an attention network (to identify important features). The features of these regions are derived from early stages of a PointNet architecture. The final local features are then fed into later layers of a PointNet architecture for classification. The training is done by alternating the training steps from the publication of Maximum Classifier Discrepancy [23].  # Quality The ablation study shows that on average across multiple domain adaptation tasks the added adaptable local features seem to improve over a direct application of general-purpose domain adaptation techniques. However, the effect on different classes seems to vary.  # Clarity The description of the architecture and methodology are clear enough.  # Significance The contribution -- though successful -- might be of limited significance to the community for mostly two reasons: the derived local feature alignment seems to be mostly a learned weighting and offseting of PointNet features, and the success across classes as shown in table 3 seems noisy; some classes profit from the proposed method (e.g., cabinet) and some don't (e.g., lamp).  Minor fixes: - line 25: systems? - line 156, eq 2: Maybe rewriting the equation in the style of an assignment would make sense here? - line 212, eq 10: missing closing parenthesis for h_1(x)? - table 3: MCD and table: Probably remove '1c' here? 