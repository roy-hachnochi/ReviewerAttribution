UPDATE: I have read the authors response and increased my score.  Specifically, the authors fixed my understanding of Property 1 and properly framed the relaxation of the problem in Section 5. Please include similar clarifications in the final work.  There was also a lot of discussion among the reviewers about how the paper relates to the Robust MDP literature, which needs to be covered better in the current work.  Papers such as "Reinforcement Learning in Robust Markov Decision Processes" and "Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions" were brought up by others and seem applicable in the current setting and could be empirical competitors to RATS.   I very much like the constraints used to study planning in non-stationary environments in this paper and the min-max inspired RATS algorithm seems like an appropriate game theoretic approach.  However, there are several clarity points that need to be addressed in the assumptions, and I feel the experiments need at least one other competitor from the long lineage of non-stationary planning or game theoretic decision-making approaches.  Still the paper as is seems correct and makes progress in the literature.  Clarifications needed:  Property 1 seems stronger than I would have expected.  By bounding changes in the reward function based on evolution of the transition function, you are assuming their evolution is correlated, and that lays the framework for the ultimate proof because the resulting Bellman backup will be bounded.  Can the authors comment on how often they expect Property 1 to hold in practice?  Are there environments they encountered where it did or did not hold.  To my reading it sounds like a very strong assumption.    The description of the assumptions around line 205 need clarification.  I find this section difficult to follow.  The authors claim be solving a relaxation of the problem, and violating property 2, but it is unclear to me what the new assumptions are.  Can the authors clearly state “considering snapshots only constrained by MDP_0” actually means?    Property 4 needs justification in the paper itself.  Particularly, the construction and solving of the linear program referenced in the supplemental material should be mentioned here.  It is an important detail.  The experiments are both informative, but ultimately seem incomplete.  I actually appreciate having a small grid-world where one can easily analyze the behavior of the algorithm, and I liked having the two book-ends as the competitors.  I learned something from this experiment.  However, I was disappointed that (1) none of the many other methods mentioned in the related work were applied, even if their assumptions did not hold, to see if RATS is really state of the art and (2) after all of the discussion of a heuristic function, there was no experimentation with various heuristics.    Lesser points:  Definition 3 – While Wasserstein is an acceptable choice in continuous physical environments, it seems like total variation may be a better fit in discrete environments.  Indeed, much sample complexity analysis in discrete stationary MDPs utilize TV to compare learned models, so I would urge the authors to note this may yet be an acceptable measure in those environments, even though the current work focuses on more continuous domains.  The literature review is fairly complete but lack discussion of game-theoretic tree search approaches, which seem highly applicable (though ultimately probably overpowered) here.  Line 90 consider -> considers 