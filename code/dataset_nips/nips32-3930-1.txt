The paper is written on a good level and includes the ideas which allow to characterize the properties of loss-function for distribution learning on the finite discrete domain. Main such property is the calibration which allows to state several results on the size of sample size which ensures concentration of the empirical loss around true loss.    However, there are some points, which to my mind should have been enlighted more in paper.   Namely, for the case of log-loss function optimizing a loss-function with respect to measure $q$ is the same as minimizing the KL-divergence for given measure p and q - to choose.  Authors only firstly mention this when considering the example of properness of the log-loss function.  Results for the concentration sample size of the empirical loss function (provided restriction to calibrated distributions) are interesting, but, taking into account the folklore result on the log-loss sample complexity are not surprising.      Several typos:   line 137: will be are characterized line 138 : we will generally assume functions are differentiable.   Efficient implementation strategy would be interesting to consider in the paper as well as the explicit procedure how we build the estimator $\hat{p}$. Is it the estimator based on the relative frequencies of occuring of each element? In this case what value should be considered for the elements which are in the domain but did not occure in the sample?  