Strengths: The paper is very clear and the small parts remaining dark (such as what the authors refer to when dealing with unclearly defined parts or losses decomposable by part in general) are studied in detail in the supplementary. This works is a natural follow up to the previous work by Ciliberto et al which introduced a learning bound similar to Theorem 4 in a more general setting. Showing how to take advantage of the specificities of the data and characterising this in terms of improvement on statistical guarantees is very appreciated in this domain.  Weaknesses: Despite the shown results and the details added in the appendix K, I think that the experimental part remains the weak part of this paper. The results displayed are convincing but I am disappointed that the authors did not tried their approach on more popular problems mentioned in the supplementary such as hierarchical classification. Even if this could be improved (in order to be at the level of the theoretical treatment), the proposed content is already solid and does not change my decision concerning the quality of this work.  Remark: - The use of the sequence example at different step of the paper is really useful, however I'm a bit surprised that you mention in Example 2 a 'common' practice in the context of CRF corresponding to using as a scoring loss the Hamming distance over entire parts of the sequence. I've never seen this type of approach and am only aware of works reporting the hamming loss defined node wise. It would be great if you could point out some references there.  - After reading the paper a few times, I still think that the notation $\Delta(z,y|x)$ is a bit strange and I would have preferred something of the form $\Delta(z,y)$ since in practice the losses you mention never takes into account the input data and $z$ is already a function of $x$. Maybe this is only personal taste and will be contradicted by the other reviewers.   Minor remarks : missing brackets [ ] in theorem 4.  