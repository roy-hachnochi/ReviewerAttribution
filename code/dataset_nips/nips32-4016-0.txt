POST-REBUTTAL COMMENTS  Notation: Thanks for considering my remarks. Your proposed modifications sound good.  Quality of results: I mostly found the theoretical framework and the subject of scale-equivariance to be interesting, therefore I did not require the paper to achieve state-of-the-art results.  Related work: Thanks for following the suggestions of the other reviewers. The FPN paper is particularly relevant because I believe that they apply the same output function to all levels of the representation, encouraging some degree of scale-equivairance.  My overall rating of the paper remains unchanged.  Other comments...  Related work: Another interesting multi-scale (but not scale-equivariant) paper is "convolutional neural fabrics".  Aside: After reviewing this paper, a question came back to bug me... Isn't normal convolution on non-infinite signals a kind of semi-group convolution? That is, if we take the domain to be the non-negative integers {0, 1, 2, ...} and the operation to be addition, this forms a semi-group (closed and associative but without an inverse). However, we deal with finite signals all the time (by inserting zeros or restricting the output i.e. "same" or "valid" padding). Could a simple approach such as this work here? I have probably missed something because this doesn't incorporate the notion of band-limiting. Anyway, it might be good if you could address this somehow.   ORIGINAL REVIEW  # Originality  This paper develops an elegant and rigorous framework for scale-equivariant conv-nets. Much of the originality lies in the development of this theory, and this is definitely sufficient for me.  It might be good to cite: - Kanazawa et al. ""Locally Scale-Invariant Convolutional Neural Networks" (NIPS Workshop 2014)  # Quality  Overall, the quality of the theoretical investigation and the experimental procedure is very high.  I wonder whether the inductive bias of scale-equivariance provides better performance for the same amount of training data. I would have like to see a graph showing accuracy versus amount of training data for scale-equivariant and normal CNNs.  In equation 22, I don't think there is any need for L'_s[x] to be a left-action? Since the notation "L_" has been used for left-actions, maybe it's better to use a different notation for this function of s and x? Furthermore, the map L'_s[x] = xs is actually a right-action not a left-action, since L'_s[L'_t[x]] = L'_s[xt] = xts = L'_ts[x].  It is stated that one advantage of semi-group convolution is that we can apply an action to the whole signal rather than just a transformation of its input domain. However, all of the examples use L_s[f](x) = f(x s). Can you provide one example of a semi-group convolution that goes beyond this restriction? (I realise that the restriction is later required for pointwise non-linearities.)  Was any pre-training used for the segmentation task? I wonder whether even larger gains could be expected if the scale-equivariant architecture were employed during pre-training?  # Clarity  The writing is clear and the approach is well motivated.  However, I found the use of the symbol L_s to be really confusing. Please see that additional section below.  The experimental details are perfectly clear. This makes the paper highly reproducible.  The initial introduction of s_0 was slightly confusing in the text. It might help to move equation 1 from the supplementary material to the main paper? In this case, you might want to use a different symbol for f, since it is overloaded.  I mostly followed Section 1.2 in the supplementary material except: - I thought it might be more clear to use f rather than f_0 in equation 10. (Technically both are correct since the condition holds for all f.) - I did not understand how the middle expression in equation 11 was obtained, although I agree with the equality of the first and last terms. Could you clarify the explanation?  # Significance  The tool of semi-group convolution and the development of scale-equivariant conv-nets may both have a significant impact on the field.  # Notation  Here is a description of the confusion that I encountered as I read the paper:  The first difficulty came at line 112. I did not realise immediately that there was no relationship between this L_s and the L_s from three lines earlier. Furthermore, when you say X = S, as a reader my mind immediately goes to the group action L_s[x] = s x, and it is confusing to see L_s[f](x) = f(x s) with x and s in the opposite order. It is correct, but it would be nice to walk the reader through this more gently.  The next confusion came at line 129. In equation 12, "L_s is a group action" defined on the group H. However, in equation 13, it seems that L_s denotes an action defined on functions on H. Should we assume in general that, when L_s[x] is an action defined on the group H with x and s in H, and f is a function on H, that L_s[f] denotes the "example" action L_s[f](x) = f(x s) from line 112? Or could this L_s[f] be an arbitrary action defined on functions on H, with no relationship to the group action L_s[x]? Either way, please make this explicit.  The next confusion arose in equation 14. Firstly, L_s is defined (line 145) as a mapping from X to X, where X might not be equal to the semi-group S. Then in equation 14, L_s[f] seems to be an action on functions on X. Is the action on functions L_s[f] defined by the group action L_s[x]? The "example" action L_s[f](x) = f(x s) cannot be used here because it depended on the fact that X = S. Perhaps this was just a typo, and L_s should have been defined on functions on X?  Next confusion, equation 15. Throughout this equation, L_s and L_t represent a left-action on functions on X. Then, in the final expression, L_t is used to represent a left-action on functions on S. In particular, it seems to represent the "example" left-action L_s[f](x) = f(x s)?  It might be helpful to provide a simple example of a left-action L_s[f] where f is a function on X and X â‰  S?  Could you simply always assume that X = S for the definition of the semi-group convolution? This might have made things much clearer. You could also explicitly state that, when X = S, the action L_s[f] on functions on S is always the canonical left-action L_s[f](x) = f(x s)?  For the example of rotations (line 131), the notation "R_s x" was confusing for me. I cannot immediately think of an N-D parameterization of an N-D rotation matrix such that associativity is satisfied: s t u = R_s (R_t u) = R_{R_{s} t} u This greatly confused me on the first pass because it seems like s and x are of different types in the expression "R_s x". Edit: I can see how this is true for 2D, where x = (cos theta, sin theta) and R can be constructed from cos and sin terms. Perhaps restrict the example to 2D? Or else, you could simply use L_s[x] = s x where s and x are rotation matrices?  # Suggestions and minor corrections  Line 62: There is a minus sign missing from the definition of G. Line 117: Should be "functions on X to functions *on* X"? Line 137: I didn't understand in what way the group action may "look different". Line 250: You might have meant to remove the word "whenever"?  