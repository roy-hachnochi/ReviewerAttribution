***Update**** My positive review was mostly from the point of view of work on group equivariance\covariance\invariance in neural networks, and the clarity of the presentation (hence the confidence rating as well). Nevertheless R2's thoughts have rightfully corrected my assessment. I am hopeful that the paper will also engage with work on multiscale CNNs in a better way. Needless to say that I really like the formalism; its transparency and potential to generalize to other domains and applications, but since the results are on images, adding a comparison to some of the baselines indicated should be considered more seriously. Not doing so might cloud some of the contributions of the paper and/or limit its dissemination in the vision community -- to which the paper also is clearly aimed at.   While I was aware of some of the papers mentioned, thinking about them was easily missed while reading the paper through the GCNN lens, and I assume the same happened with the authors at least at some level. My opinion that this is a great paper does not change, but I will lower my rating one notch, as I do feel that the scholarship could be improved by heeding R2's concerns, which can help improve the paper even more.   **************** Older version.  Group equivariant neural networks have recently emerged as a principled approach to guide neural network design. The main idea in them is to explicitly bake in the symmetries of the input in the neural network architecture so that they are respected, thus obviating data augmentation for those set of transformations. Moreover, the use of group theoretic machinery also facilitates the use of techniques from harmonic analysis, making implementation clean and simple, especially for compact groups.   One particular success story of GCNNs is that of simple discrete groups acting on images, like 90 degree rotations, flips and reflections (Cohen and Welling, 2016). Such networks are equivariant to not just translations as in classical CNNs, but also to these transformations in addition to translations. What is more, they can be implemented efficiently just in the "real space". Such simple generalizations of CNNs give a strong boost in image recognition, and as indicated above, without using explicit data augmentation of the nature described. It is however clear that another symmetry that matters in image recognition is that of scale. An image at different scales really represents the same object and thus the output should be invariant to scale. This is implemented in various ways in existing literature: Like considering various scales, training networks in parallel and voting; averaging images at different scales and feeding them in a network and various other permutations and combinations. Moreover, the role that scale plays in CNN architectures is not really understood. Use of atrous convolutions also shows this lack of understanding quite clearly by working so well.   The main insight of this paper is that scale invariance could be incorporated in the GCNN framework seamlessly, while handling it in a similarly principled manner. This is because scale by itself forms a semi-group. While this causes some issues while defining correlation/convolution due to a lack of an inverse, but nonetheless a useful notion can be defined. The paper introduces the reader to the necessary literature on GCNNs, equivariance, scale spaces, and then describes their approach in a very clean manner. They do so by defining semi-group correlation, induced actions and the appropriate notion of convolution/correlation there and so on. Some subtle differences owing to the lack of inverse are also emphasized, such as the fact that the signal can be transformed but not the filter.   The experimental results described are strong, and thorough, with even the quality of equivariance achieved clearly described, validating the approach.   In short -- the paper is written in a very easy to read, and elegant manner. Proposes a generalization of group equivariant neural networks to semigroups, and uses this notion to incorporate scale invariance in image recognition. There aren't many comments that I can think of to improve the paper further. This short review is merely a reflection of the elegance and clear presentation of this paper. 