Post-rebuttal response: I read the authors' response and don't have any further comments. -------------  ORIGINALITY ===========  Linear regression in the presence of noise is a fundamental problem in machine learning. This paper considers the model of "robust statistics" where an alpha fraction of the training data comes from the ground truth distribution while the rest are corrupted arbitrarily (i.e. adversarially). Traditionally, research has been on the setting where alpha is large, so that the parameters of the true distribution are information-theoretically identifiable. However, recent focus has been on the small alpha setting. Here, the parameters of the true distribution cannot be uniquely identified even with infinitely many samples. However, it may be possible to output a small list of parameters which is guaranteed to contain the ground truth.  Previous work in this regime has focused on mean estimation. Here, the authors give the first efficient list-decodable learning algorithms for linear regression. In my opinion, the work is quite original, both because of the foundational nature of the problem and also because of the new technical contributions, as I discuss below.  QUALITY & CLARITY ==================  The submission is well-written and sound. I verified at a high level most of the proofs in the supplementary material and was convinced. Also, the authors do a good job of motivating and giving intuition for their algorithm and the anti-concentration condition they require of the input distribution.   SIGNIFICANCE ============  The techniques introduced in this paper should be influential in the future for analyzing other list-decoding learning algorithms. The work relies on the "identifiablity to algorithms" paradigm that has been very successful recently in designing a variety of robust learning algorithms. The key idea is to first design a non-efficient algorithm that involves searching over distributions, then to relax the search over distributions to "pseudo-distributions" (which is in poly time), and then to devise a "rounding" scheme that allows use of the pseudo-distribution in the rest of the algorithm.   In this work, the target distribution mu is over subsets of the input examples, so that (1) each set S in supp(mu) is of size >= alpha*n, (2) each set S in supp(mu) is consistent with a linear hypothesis, and (3) for any i, the marginal probability of i being included in a random subset S from mu is approximately the same. Given such a mu, the authors give a nice and simple list-decoding learning algorithm, assuming that the samples come from an anti-concentrated distribution.   Next, the search over distributions is relaxed to a search over pseudo-distributions. Condition 3 is relaxed to an optimization in terms of "pseudo-expectations". The whole optimization problem is now in poly time via the ellipsoid algorithm.  Finally, the issue of rounding, which is where this paper shows its originality. The non-efficient algorithm samples from mu O(1/alpha) times and returns the list of consistent linear functions. But with a pseudo-distribution, it doesn't make sense to obtain independent samples. So, what they do is to let each i output a linear function v_i that is the average according to the pseudo-distribution mu~ of all the linear functions corresponding to the sets S in supp(mu) that contain i. They show that assuming "certifiable anticoncentration", if the i'th sample is uncorrupted, then with probability 1/2, v_i is close to the true linear function l*. But if we sample i with probability proportional to the marginal probability of the pseudo-distribution to i, then i is in the uncorrupted set with probability at least alpha. So, with probability at least alpha/2, v_i is going to be close to l*, and we are done.  The authors state, and I agree, that this method of "rounding by votes" could be used in other settings as well.   The paper also shows that the gaussian distribution and more general spherically symmetric distributions are certifiably anti-concentrated. This uses an application of approximation theory to construct a polynomial approximator for the indicator function of an interval. Also, if the true linear function is boolean, then they show that the anti-concentration assumption can be relaxed so as to allow product distributions (such as uniform). Finally, they also show that anti-concentration is necessary for identifiability even for noiseless regression with examples from the uniform distribution on {0,1}^d, and even for the potentially simpler problem of mixed linear regression.