This paper provides a new view of word embeddings. This paper introduces the notion of global relatedness, which is constructed by PMIs between the specific word and other context words. This paper shows the global relatedness can capture various semantic similarity, by considering geometric and probabilistic aspects of such vectors and their domain. Moreover, this paper shows low dimensional word embeddings built by word2vec or Glove can be viewed as the linear projection from the global relatedness vectors.  This paper is well-written and easy to follow. This theoretical contribution is novel and provides a new tool to understand why word embeddings can capture various semantics of words. The originality and quality of this paper would be above the threshold. 