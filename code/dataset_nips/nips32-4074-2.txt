   The authors introduced the formal notion of risk monotonicity, that the risk does not deteriorate with increasing training set sizes in expectation, which is a natural phenomena for learning curves.    Then the authors presented a surprising result that various standard learners act non-monotonically.    The authors provided a theoretical condition for non-monotonicicy with numerical experiments for classification, regression, and density estimation.    Since the paper presents a natural framework of monotonicity and a strange counter-example of monotonicity, i.e. non-monotonic behavior of simple learners, originality and quality of the paper is quite high, and the paper is well-organized and its clarity and readability is sufficient.    I think the notion of risk monotonicity includes a lot of important mathematical problems, however, significance for neurips community is unclear because presented numerical examples are too limited and not practical. 