In this paper, the authors proposed Fast Low-Rank Metric Learning (FLRML) and M-FLRML for large-scale and high-dimensional metric learning problem, by employing the low rank constraints and the stochastic learning methods to reduce the computational complexity of the method. I think this paper is well prepared. I have the following comments. (1) In my opinion, it is a common way to process the large-scale and high dimensional data by using low-rank constraints and stochastic learning strategy for metric learning. Besides, the main idea of FLRML is to replace Y by $BV^T$ and is very similar with anchor-based strategy [1]. Please explain the difference between them and point out the main contribution of this paper.  (2) Is there existing any theoretical results of the process of using $BV^T$ to replace Y, which ensures the performance of accelerated low-rank metric learning?  (3) For the stochastic metric learning methods, there are some recent methods, which are not summarized in the related works of this paper, such as [2] and [3]. Meanwhile, what the differences between FLRML and OPML[3] and the method in [2]? [1] Liu, Wei, Junfeng He, and Shih-Fu Chang. "Large graph construction for scalable semi-supervised learning." In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 679-686. 2010. [2] Qian, Qi, Rong Jin, Jinfeng Yi, Lijun Zhang, and Shenghuo Zhu. "Efficient distance metric learning by adaptive sampling and mini-batch stochastic gradient descent (SGD)." Machine Learning 99, no. 3 (2015): 353-372. [3] Li, Wenbin, Yang Gao, Lei Wang, Luping Zhou, Jing Huo, and Yinghuan Shi. "OPML: A one-pass closed-form solution for online metric learning." Pattern Recognition 75 (2018): 302-314.