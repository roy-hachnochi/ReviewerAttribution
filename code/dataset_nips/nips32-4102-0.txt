Update 2019-08-07  The authors rebuttal shared with us more convincing ImageNet results. They did not address my other concerns as thoroughly. Nonetheless, I improved my score from 6 to 7.  **************************  Some general comments:   The paper is well written and easy to read. The introduction clearly states the objective and the context.  The text and the 2 pseudo-algorithms clearly explain how the method works, and how it differs from previous methods. I did not feel the need to ask the authors to share their code.  The CIFAR-10 experiments are solid and involve a rigorous tuning of the baseline hyperparameters. The ImageNet experiments seem preliminary but are rather promising.  Other comments:  A) Though the proposed method is more straightforward than previous methods, it is unclear whether it is easier to tune.  Some of the previous methods only required to tune ADAMâ€™s alpha. The latent weights were initialized and clipped using Glorot-et-al or He-et-al initialization methods. In comparison, the proposed method, BOP, requires tuning 2 hyperparameters, an adaptivity rate and a global threshold. It would be nice if the method did not require to tune a global threshold (but it is no big deal). Besides, it seems a bit awkward that there is a single global threshold for the whole network. Intuitively, I feel like wide layers may require a smaller threshold than thin layers.  B) I agree with the authors that it would be nice if their method used the gradient second moment (like ADAM).  C) It seems that the method is very dependent on Batch Normalization to maintain unit variance across the units. Maybe scale the binarized weights (after the sign function) so that the method can work without batch normalization?  D)  The method currently only works with 1-bit weights. It would be nice if it could also work with ternary, 2-bit or any number of bit weights. It would be even nicer if the infinite bit method was equivalent to SGD or ADAM.  