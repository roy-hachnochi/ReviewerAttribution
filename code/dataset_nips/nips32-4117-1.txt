The paper looks at reinforcement learning with constraints and provides an analysis of the problem. The authors prove that under common assumptions, the problem has zero duality gap and can be solved exactly in the dual domain. This finding doesn’t necessarily hold when the learned policies are function approximators but the authors provide further arguments to show that under the right type of parameterization, convergence to the optimal solution can still be shown to hold. The paper also describes an algorithm for optimizing the constrained RL objective.  I haven’t discovered any gaps, mistakes or other problems in the proofs and argumentation. The paper cites relevant prior work but could be improved by discussing some of this work in more detail. The proposed algorithm differs very little from previous approaches based on primal-dual algorithms and it would have been insightful if there was more discussion about how this algorithm differs and why it would be better or at least complementary. Without this discussion, the presentation of the algorithm only serves as a prototype for which the analysis of the required number of updates is done without getting more insight in the efficiency and expected behavior of similar algorithms that have already been shown to be useful empirically.   I find the novelty of some of the individual arguments hard to judge but I do think that the main result is important and that the analyses provide useful insights. I couldn’t find any other work on constrained RL that even mentions the duality gap, so even without the result that the gap doesn’t exist for general policies under certain conditions, the paper points out an important issue that seems to have been glossed over so far. Since the paper mostly shows that the issue is actually not expected to be a problem in practice, the impact is mostly a firmer understanding of why previous approaches work but it also gives some new intuitions about why they might fail if function approximators are not flexible enough. The presented algorithm doesn’t seem to differ much from prior work and without empirical results, it is not possible to judge its value as a contribution on top of its usefulness as an illustrative example.  The paper is mostly easy to follow but could be more clear about the main contributions of the paper and how they relate to previous work. 