1. The proposed MAVEN method extends the idea of deep exploration via Bootstrapped DQN to the QMIX algorithm, aiming to provide committed exploration. One question is that this new exploration solves the sub-optimality issue of QMIX? Is the sub-optimality of value function decomposition methods (e.g., QMIX and  VDN) intrinsic due to the full decomposition representation or due to the exploration? 2. Although it is shown that MAVEN significantly outperforms QMIX in the toy game problem, the experimental results in the SMAC domain seem not to demonstrate the expected significance of MAVEN over QMIX. The authors mentioned in the paper that the SMAC domain may not be ideal for testing exploration strategies, but it is highly desired for the authors to find an interesting challenging domain for evaluating the significance of MAVEN, as did by Bootstrapped DQN in the Atari games.  In addition, it is also important to analyze the weakness of MAVEN. 3. Did the authors perform an ablation test for evaluating MAVEN against QMIX with each agent using Bootstrapped DQN? 4. How stable is the learning of MAVEN using the alternative training of the value function and the hierarchical control policy? 5. As shown in Figure 1, is the epsilon-greedy exploration still used in MAVEN? Some typos: Line 113: removing “a” Line 116: missing a “)”  UPDATE: After reading the author's rebuttal, I have chosen to increase my score from 5 to 6 because the authors provide stronger results and partially address my Question 2.