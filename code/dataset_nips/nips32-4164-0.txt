I like the problem introduced in the paper and the approach taken. I have few questions and suggestion to authors: 1) “For dynamic expansion, we set the threshold for the negative log-likelihood (approximated by the ELBO) at cnew = 100”: How did authors decide cnew? Was there any tuning done to decide cnew? What is the effect of low cnew in experimental results?  2) What’s the time complexity of the algorithm? How long did it take to run all the experiments on Tesla V100 GPU?  3) Did authors try any preliminary experiments on little more complex datasets like CIFAR-100? 4) Figure 3b: Why did performance of “1” is getting worse with time but performance of “0” is not?  5) Figure 3B: Performance of “5” never seem to pick up beyond 50-60%. Is there any reason why?   The submission is technically sound and claims are well supported by experimental results. The submission is clearly written and well organized. Authors address a novel problem setting with combination of old techniques. It is clear how this work differs from previous contributions and related work is adequately cited. Researchers and practitioners are likely to use the ideas presented in this paper and build on them.   