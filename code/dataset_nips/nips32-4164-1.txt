============== After rebuttal ============== I thank the authors for their response, they have managed to clarify some of my concerns and overall I vote for acceptance of the paper.    The authors introduce a method for continual unsupervised learning. They propose a generative categorical model, in which the latent space is modeled as a mixture of Gaussians, with a Bernoulli decoder. An expansion technique is used to include new mixture components for poorly modeled examples, and the generative model is used with previous model parameters to prevent forgetting old tasks. Their method is analysed on tasks constructed around MNIST and Omniglot, with an ablation study on the expansion and generative replay. The extension to a more standard supervised setting is also presented.    Novelty and quality:  The exact setting proposed in the paper, as well as the proposed model, are novel to my knowledge.   Significance:  The main contribution of the paper is empirical. The setting of unsupervised continual learning proposed by the authors is relevant and provides an interesting proof of concept for other tasks which can benefit from it, such as reinforcement learning. The experiments in 4.1 to 4.3 lack comparison to simple baselines, such as a hierarchical clustering technique. Since comparison to other methods is not possible, I believe this would strengthen the paper. Could the authors provide such a baseline? The fact that the method does well in supervised tasks is reassuring.   Clarity: The paper is overall clear and the method is well presented.   I have the following detailed comments:  1) The lack of comparison to baselines, as previously mentioned.  2) Why is the accuracy in Omniglot so low in Table 1? It is difficult to draw any conclusions if the method has such a high error on the dataset.  3) Why is only one sample \tilde(z)_k used in eq 3)?  4) I would appreciate more details to understand how eq 4) is derived, it is currently rather intuitively motivated.  5) What value is chosen for N_new? How is this selected? Moreover, is it sensible to initialise the parameters of a new cluster as in eq 5) if the new class is very different from previous classes? Did you try random initialization?  6) What exactly is p_{\theta} in eq 6)?  7) How are the training and test splits determined in the experiments, and how are the error bars computed? 8) How reproducible are Figure 4 and Figure 2b?   