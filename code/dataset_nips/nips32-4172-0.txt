Post-rebuttal: I am satisfied with the response. I hope the authors can include the QBF data in the final version of this paper. --------------------------  ORIGINALITY ============  The work's main originality is in introducing the study of CPS as a parameterized computational problem. The techniques used to establish fixed-parameter tractability or intractability are fairly standard in the area and cannot be considered new contributions in my opinion.  QUALITY & CLARITY =================  The paper systematically analyzes the following parameterizations: * number of algorithms * number of tests * length of the optimal schedule * algorithm and test failure degree * algorithm and test success degree * failure cover number, and more generally, treewidth of failure graph * success cover number  The proof sketches in the main submission, and the full proofs in the supplement, are very clear and well-written. The proofs follow standard techniques and are all correct (to the best of my judgement).  Where the paper severely lacks, especially as a submission to NeurIPS, is in justifying that any of the above parameterizations are relevant to applications in machine learning and artificial intelligence. Can we expect the failure graph in QBF solving or ImageNet classification models to have a small treewidth or have a small success cover number?  SIGNIFICANCE ============  I have mixed feelings about this submission. On the one hand, CPS captures a basic problem in software engineering, and the work establishes some fundamental results about the complexity of the problem. On the other hand, it is going to be of limited interest to the NeurIPS community because it's not clear how the parameterizations relate to actual instances. Also, the submission would be significantly strengthened if the paper looked at the hardness of the specific scenarios 1, 2 and 3.