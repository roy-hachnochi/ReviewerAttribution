The paper proposes a simple measure to replace diameter to improve the regret bound and the sample complexity of UCRL2. As UCRL2 is widely studies algorithm in theory side of RL, the analysis is relevant broadly.  The improvement on the bound is rather small on usual reinforcement tasks where the reward is much smaller than rmax for most of the transitions. In particular if the task is sparse reward task where the agent gets rmax on reaching some specified goal and zero otherwise, then MEHC is almost always the same as diameter. It is unclear how much more knowledge we get from using MEHC if we do not have good reward or shaping. Section 2.5 analyzes the effect of shaping on MEHC which is very impressive, but it shows that MEHC can only be halved at best.  >  Our results show that the informativeness of rewards, an aspect of “the complexity of the learning problem” can be controlled by a well specified potential without inadvertently changing the intended behaviors of the original reward.  While the analysis looks very impressive, I do not take it as an evidence to support this claim. In my opinion, a multiplicative factor of two is not impressive enough for many practical applications. As the sample complexity of UCRL2 is linear to MEHC, it only reduce the sample complexity to half. Good heuristic based on expert knowledge can speed up the algorithm much more than that (e.g. intrinsitc reward by Singh et al. 2005). Thus, I would rather understand Theorem 2 as showing the limit of potential-based reward shaping. If we are constrained on using potential-based shaping, then we can only get twice as fast speedup, and in order to get more we have to rely on heuristic which may change the optimal policy. Of course the actual speed up you get seems to be much faster than twice, as shown in the original potential-based shaping paper (e.g. Ng et al. 1999, Figure 1). Overall, while I am very impressed by the bounds shown in Theorem 2, I do not see it as a strong evidence to support potential-based reward shaping.  -----------------------------  Thank you very much for clarifying my questions.  The author clarified their claim on the impact of reward-shaping which was my only concern on the paper. In the paper at glance it seemed they claimed that "the complexity of the learning problem can be controlled by PBRS" but they clarified in rebuttal that it is the complexity when using UCRL2 algorithms that can be controlled by PBRS. With that qualification it is a valid statement from the theoretical results they provided. I believe the paper has significant theoretical results which should be discussed in NeurIPS. I am very interested in their future work on extending their analysis.  With this I will vote for accepting the paper. 