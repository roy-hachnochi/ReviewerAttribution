Updated review: The authors have indicated that they will run additional experiments and make the clarifications I requested, so I will raise my score in 7 in agreement with the other reviews leading to an "accept" consensus. However, I do note that in their rebuttal the authors describe Gu et al., Stern et al., and Welleck et al. as "concurrent work". To be totally clear, all three of those papers were posted to arxiv in early February; the NeurIPS deadline was over 3 months later and it is now 6 months after they papers appeared online. I would argue that 3 (or 6) months is long enough to provide a more direct comparison and would not consider this submission "concurrent work". I don't think this warrants rejecting the paper, but I do want to note that I disagree with the authors here and still believe that a more direct comparison is appropriate.  Summary:  This paper proposes a method for generating output sequences in an arbitrary order in sequence-to-sequence tasks. To do so, the model outputs an insertion location and a token at each output timestep. To avoid the combinatorial search problem which results from natural optimization of this procedure, the authors propose a lower bound which can be computed reasonably efficiently. Experimental results are given on machine translation and image-to-text tasks, and the proposed approach consistently outperforms left-to-right and right-to-left generation.   Review:  Overall this is an important problem to tackle, and an interesting approach. The main drawback of the method is its increased computational complexity, but if it consistently leads to better performance then the additional complexity is arguably worthwhile. The paper is overall written reasonably clearly (though I have some questions and suggestions for improvement below). Overall, I think it's a solid contribution which warrants publication. However, I have one significant criticism: It is experimentally somewhat weak. While the authors consider some common benchmarks, they reimplement only a limited set of baselines and do not compare directly to previously produced numbers. The results would be much stronger if 1) additional baseline methods were included, e.g. at the very least *all* of the related methods of Section 7, 2) this idea was applied to a stronger baseline (e.g. some SoTA seq2seq model) and improved performance was still observed, and 3) a more direct comparison to published numbers on the studied datasets was provided. As it currently stands I will give it a weak accept; I will be happy to raise my score if the authors make these improvements.   Specific comments/questions:  - Why not consider tasks without language output? That would help show that the gains from this method are not specific to language generation tasks. For example, you could consider music generation tasks, where a "good" generation order is not obvious.   - You find that "the model has a preference towards producing 'easy' words at the beginning and leaving more complicated choices for later". Why not include this as a baseline for comparison?  - Notation issue: You formulate your model as predicting \tau from the input X, a partial output ~Y(\tau_{0:t-1}), and parameters \theta. In (6) and later, you compute an expectation over \tau \sim p(\tau | X, T*(Y), \theta). But T*(Y) is defined as "the set of all trajectories leading to Y", which is different from a (single) partial output ~Y(\tau_{0:t-1}). Do you mean that you also compute an expectation over sampling from T*(Y) first, and then use that to sample \tau?  - The footnote 8 on page 6 is important. You should have introduced [28] earlier and more explicitly.  - In Section 3, you write "Our model does not require the attention mask preventing attention to subsequent positions. Decoder self-attention is re-applied at each decoding step because the positional encodings of most tokens change when inserting a new one into an incomplete subsequence of Y." This will make the reader wonder the extent to which your model is parallelizable. You only address this later in Section 5.2. I would suggest moving some of this discussion up earlier; it will make the presentation of the algorithm clearer.