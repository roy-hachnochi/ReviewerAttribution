The paper is clearly written.  I find the variational EM training of MLNs to be interesting. In the E-step, it cleverly uses graph embeddings to sample the truth values of hidden triplets, thereby completely defining the Markov blanket of each triplet for the M-step. This circumvents the need for (intractable) inference to obtain the values of the hidden triplets. However, I do not find the M-step to be novel because the maximization of pseudolikelihood for MLNs is standard fare (e.g., [20]).   I have two misgivings about the paper, both of which relate to recent hybrid approaches RUGE [15] and NNE-AER [9]. First, the paper does not position its proposed system clearly vis-a-vis RUGE and NNE-AER. Both systems combine knowledge graph embeddings with first-order logic, and their logical rules are soft and hence capture their inherent uncertainty. The rules are also encapsulated in a maximization equation in a principled manner. Hence, both systems seem to integrate soft logical rules with graph embeddings in as principled a manner as the paper's system. This begs the question of why the paper's system does better than RUGE and NNE-AER in the experiments. Second, I do not think the experimental comparison with RUGE and NNE-AER are truly apples-to-apples. The empirical numbers are taken from the RUGE and NNE-AER papers. In those papers, NNE-AER only considers "non-negativity" rules and "approximate entailment" rules, and RUGE only considers Horn clauses of length at most 2. In contrast, this paper considers four rule types: composition rules, inverse rules, symmetric rules, and subrelation rules. Hence, it is possible that the paper is doing better than RUGE and NNE-AER simply because it considers a different (and possibly bigger) universe of rules, rather than through a more principled combination of soft logic and graph embeddings.  UPDATE: In their feedback, the authors have sufficiently addressed my two misgivings with regards to RUGE and NNE-AER. Hence, I will upgrade my score to "marginally above acceptance".