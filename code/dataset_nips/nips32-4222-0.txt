The paper follows on a previous distinction between the weights of a neural network and the function that is realizes, and investigates under what conditions can we guarantee inverse stability: That networks with a similar realization will also be close in the parameter space (or more accurately, whether there exists an equivalent realization of one network that is close in the parameter space to the other). First, the authors identify that inverse stability fails w.r.t. the uniform norm, which motivates the investigation of inverse stability w.r.t. the Sobolev norm. Thereafter, the authors identify 4 cases where inverse stability fails even w.r.t the Sobolev norm, and lastly they show that under suitable assumptions that eliminate these 4 pathologies, inverse stability w.r.t. to the Sobolev norm for one hidden layer networks is achieved, where they argue (without proof) their technique also extends to deeper architectures. The authors motivate this line of research by establishing a correspondence between minima in the optimization space and minima in the realization space, and arguing that this connection will allow the adaptation of known tools from e.g. functional analysis to investigate neural networks.  While I find the paper well-written and clear, my main concern is with its novelty, as specified in the improvements part.   Minor comments: Line 22: year -> years. Line 72: "The, to the best..." is unclear. Possible typo? Line 218: an restriction -> a restriction