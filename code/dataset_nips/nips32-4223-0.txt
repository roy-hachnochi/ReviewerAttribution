Mein Concerns:  The main motivation of the paper, to solve Backprop in spiking neurons, is not an open problem in computational neuroscience. In fact, learning in spiking neural networks using standard methods is not a problem at all as recent work shows. It has been demonstrated multiple times that Backprop can be applied without much changes by applying pseudo-derivatives to circumvent the non-differentiable spikes. See: [6-8]. This works very well in practice and scales up to midscale benchmark problems (and possibly beyond) without performance loss compared to classical (analog) neural networks.  In this context it hard to pinpoint the main innovation of the manuscript. The presentation of the model is mixing existing ideas with details that are original to the present paper. For example the outline of the spiking backpropagation in Figure 2 is very hard to decode. It is unclear what the dashed lines represent without additional knowledge. A caption, labels and/or legend would help. The figure is also not sufficiently explained in the main text. The derivations on page 4-6 could be compressed a great deal since they are very standard. Additional details could be pushed to a supplement making space for the relevant details.  Finally it is unclear why the proposed model is more biologically plausible than previous models. The learning rules that are presented are applications of standard Backprop to spiking networks. This was shown before. For example, also the model presented in ref. 4 in the manuscript could be applied to spiking recurrent networks (not just LSTMs). Other recent approaches that use feedback weights (synthetic gradients) for deep learning in SNNs [1,2] and their recent extension to recurrent neural networks [3], don't need unrolling the network over time. These prior related work should be discussed and cited. What is the advantage of the proposed model compared to this prior related work?   References:  [1] Lillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. (2016).  Random synapticfeedback weights support error backpropagation for deep learning.Nature communications,7:13276.  [2] Samadi, A., Lillicrap, T. P., and Tweed, D. B. (2017).  Deep learning with dynamic spikingneurons and fixed feedback weights.Neural computation, 29(3):578â€“602.  [3] Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets Guillaume Bellec, Franz Scherr, Elias Hajek, Darjan Salaj, Robert Legenstein, Wolfgang Maass. https://arxiv.org/abs/1901.09049 