This paper presents a novel multi-objective BO (MO-BO) algorithm that extends max-value entropy search to the multi-objective case. This is an original and significant contribution. Overall the paper is well written and structured.  There are however few aspects of the paper that are not sufficiently thorough and overall decrease my score of the current manuscript.  One aspect of the paper that should be improved is the related work. The current manuscript does not accurately place the current work in the existing literature: - In the introduction, it is mentioned only PES as a MO-BO algorithm (line 31). It would be good to also mention the more widely known ParEGO, SMS-ego, and EIHV. - page 3 line 94 state that HV-based MO-BO is not feasible for more than 2 objectives. This is not accurate (as also demonstrated in the experimental section when using SMSego on a 6-objective task) and it should be rectified. - The connection to the max-value entropy search acquisition function for SOO should be highlighted much earlier and more clearly in the text.  Another aspect is that the benchmark functions used to evaluate MESMO are not standard functions in the MOO community. To increase reproducibility and allow direct comparisons to past literature it would be good to include also experiments with more standard MOO functions. Some of these functions can also be scaled to arbitrary numbers of objectives, which would also be valuable when computing the computational time, e.g., by plotting the computational time of the different algorithms vs the number of objectives.  Additional comments: - In page 3 line 96, it is stated that reducing to single-objective is sub-optimal. This statement is ambiguous and potentially inaccurate and should be better explained (e.g., by defining sub-optimality w.r.t. some criteria) and motivated (and cite appropriate references). - In page 4, paragraph "cheap MO solver" it might be clearer to explicitly define the optimization that you are solving. It might also be interesting to connect this optimization to previous literature such as [Binois 2015] and [Calandra et al. 2014] given the similar underlying idea (but different tools).  references: - Binois, MickaÃ«l. Uncertainty quantification on pareto fronts and high-dimensional strategies in bayesian optimization, with applications in multi-objective automotive design. Diss. 2015. - Calandra, Roberto, Jan Peters, and M. P. Deisenrothy. "Pareto front modeling for sensitivity analysis in multi-objective bayesian optimization." NIPS Workshop on Bayesian Optimization. Vol. 5. 2014.  --- Edited after rebuttal: Thank you for taking the time to answer my comments. As a result of the rebuttal, I raised my score. Here are some remaining comments: - Point 3: This point is still inaccurate. It is well known that linear scalarizations might result in sub-optimal solutions (depending from the convexity of the PF), however claiming the same for the non-linear case is not something to be done lightly (E.g., arguing about the sub-optimality of the Tchebishev scalarization in ParEGO would require appropriate references). - Point 5: I am familiar with Entropy based methods. Your reply does not however really answer the question of why using entropy works better than previous MO approaches. - Point 7: You should include clearly in the text a reference to pygmo and the algorithm used (Nowak et al.). Generally, this is not the most efficient algorithm to compute HV, and I would suggest you to mention this in the text as well. For more efficient implementations see "Lacour, Renaud, Kathrin Klamroth, and Carlos M. Fonseca. A box decomposition algorithm to compute the hypervolume indicator. Computers & Operations Research 79 (2017): 347-360.", "Jaszkiewicz, Andrzej. "Improved quick hypervolume algorithm." Computers & Operations Research 90 (2018): 72-83.", and follow-ups.