This paper examines how overfitting in deep-learning classification can be detected through the generation of adversarial examples from the original test data. The authors have developed an unbiased estimator of error based on adversarial examples that relies on a form of importance weighting. This estimate of error should be in agreement with estimates based on the training data - otherwise, if hypothesis testing shows that the discrepancy is sufficiently great, we can conclude that the learner has overfit to the training data.  The authors take great care to avoid the trap of other uses of data perturbation, where the generated data may follow distributions that do not reflect the original data. They outline a generation process that uses certain density preservation assumptions to ensure unbiased estimation of the error of the original classification process, and then give practical ways of generating adversarial examples for images that are valid for these assumptions. The authors then illustrate their method in two learning scenarios (involving CIFAR-10 and ImageNet).   The authors are also careful in addressing the practical issues surrounding the use of their estimator, including that of managing variation in the estimation outcome.  Overall, this is a good result, well motivated, well written and well explained.   [UPDATE] The issues raised by the first author regarding the generation of adversarial examples by translation alone seems neither here nor there to me. Any adversarial perturbations used should remain realistic, and translations are a good way of ensuring this.  I am satisfied with the authors' response, and my score is unchanged. 