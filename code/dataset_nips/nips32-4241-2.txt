This paper proposes a novel way to detect overfitting via adversarial examples. The authors construct a new unbiased (error) estimator which uses adversarial examples as well as importance weighting to take into account the change in data distribution. Moreover, the authors use translation attack in this paper which simplifies the computation of importance weighting. This method is quite general as it doesn't restrict the training procedure and is based on the original test data. The theoretical analysis for the high probability error bound given independence assumption looks correct. The paper provides a thorough experimental validation of the proposed algorithm, showing that there's strong evidence that the current state-of-the-art classifier architecture are overfitted to the CIFAR10, and no such evidence in case of ImageNet.   I like this work a lot as it makes progress towards the answering an important question to deep learning community --- whether the recent progress on model architectures come from overfitting the benchmark test dataset? Different from the reason given by Recht et al.(distribution shift), the extensive experiments of this paper suggests that the test error on CIFAR10 are unreliable and due to overfitting.  Also, this work is well-written and structured clearly.  Thus I suggest accepting this work.  =========post-rebuttal update======== The authors clarifies my questions in rebuttal and my score remains the same.