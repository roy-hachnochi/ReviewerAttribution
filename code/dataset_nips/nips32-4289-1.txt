The submission is well-written with sufficient experiments supporting the argument. The proposed method is clear and convincing.   The reviewer is just concerned with the significance of the submission. It seems that the biggest difference of the proposed method compared with AIRL frame work lies on the off-policy training, which however is borrowed from [20] and also SEEMINGLY related to  Rakelly, Kate, et al (2019). Therefore, it would be great if the author can further emphasize the differences of the proposed method compared with all these methods in the rebuttal.    Rakelly, Kate, et al. "Efficient off-policy meta-reinforcement learning via probabilistic context variables." arXiv preprint arXiv:1903.08254 (2019). ================================================================== The rebuttal of authors is clear and convincing. Therefore, I increase my score to 7. 