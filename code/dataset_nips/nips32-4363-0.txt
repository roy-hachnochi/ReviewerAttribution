The authors' provide an approach for post-training quantization deep convolutional neural networks down to 4-bit weights and activations. Their approach includes several methods: 1. Analytical Clipping for Integer Quantization (ACIQ): ACIQ clips activations to minimize MSE, assuming Gaussian or Laplace distributions.  2. Per-channel bit allocation (PCBA): In this scheme one channel that needs a greater number of bits can take them from a channel that requires fewer as long as the average number of bits remains at the target (e.g., 4 bits). 3. Bias correction: This method analyse and compensates for some of the weight quantization induced bias.  The authors created many combinations these approaches, applying them to ImageNet on many networks at a few activation and weight bit precisions.  The authors are among the first to apply post-training quantization approaches to 4-bit and lower precision networks. The paper is interesting, and it has clear significance, creating a state of the art for post-training quantization at 4 bits. Table 1 is clear and shows how different methods effect results. The writing and results are mostly clear, but could be improved.  The work may overstate its value a bit. The assumption that networks will not be retrained seems week. If there is great value in a 4-bit network, then it will be fine-tuned to achieve the best score it can.  The per-channel bit allocation is difficult for inference. The benefit is reduced model size when applied to weights. Hardware must support the worst-case bit widths, so there is no real benefit for activations, so 4W4A is somewhat misleading. Also, model parsing is more complex.  line 187: "5.2 Interaction between quantization medthods" should be "methods"  