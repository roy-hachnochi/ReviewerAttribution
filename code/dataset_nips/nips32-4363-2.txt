1) The paper is clean, focused and novel. Directly applicable to various area and research, well reflecting the current trend on quantized neural networks. 2) Can you explain why ACIQ on InceptionV3 is less effective than Resnet-50 or Resnet-101? Is it related to distribution assumption? 3) For clarity, it would better specify "signed" or "unsigned" quantization. When using activation quantization after ReLU, using [0, a] range and "unsigned" 8/4-bit for quantization. In this case, "round to midpoint" also valid?