In this paper, the authors proposed a multi-task learning approach for few-shot learning using deep neural networks. The proposed method can automatically adapt to new tasks at testing time after initial multi-task training. To make this possible and achieve better performance over existing methods, the authors proposed to use two networks to learn task-specific parameters in the classifier. One is for task-specific parameters in the final classification layer. The other one is to learn task-specific parameters that adapt the common feature extractor (a network) shared among tasks to be task specific. The superior of this method is demonstrated by not only the better than the-state-of-the-art performance on few-shot learning problems, but also competitive performance on continual learning and active learning tasks.   Even though there have been several existing works on few-shot learning, as demonstrated by the empirical results in the paper, this work significantly moves the-state-of-the-art. The paper is well organized and easy to follow. I found the illustrations, i.e., Figure 1-3 are very helpful for me to understand the architecture of the proposed method. There is just one typo that I noticed on line 246, D_{\tau}, should it be D_^{\tau} to keep it consistent with the notations in the rest of the paper? 