-- Post author-feedback comments --  Following the author responses and suggested improvements, I am raising my score and fully recommend accepting the paper.   One minor remark (since this has not been directly addressed in the author feedback): reference [21] in the original submission points to a preprint version of Zou & Lerman "Graph convolutional neural networks via scattering". This paper appears to be in press in Applied and Computational Harmonic Analysis (https://doi.org/10.1016/j.acha.2019.06.003), and therefore this reference should be updated accordingly to point to that in-press journal version.  -- Initial review --  This paper explores the stability of graph scattering transforms, which extend the scattering transform that has been studied for some time in traditional spatiotemporal data such as image classification and signal processing. Indeed, the extension of scattering transform has gained attention lately, together with a wider effort to extend the success of convolutional neural networks to network analysis and graph learning. In particular, in the past two months several graph scattering works have appeared, for example, in the last ICLR (Gama et al., 2019), ICML (Gao et al., 2019), and in the journal of applied and computational harmonic analysis (Zou and Lerman, available online June 13, 2019). It should be noted that the mentioned papers have all been published after the submission deadline of NeurIPS, and therefore I would recommend the authors update their introduction and related work sections to reflect the recent interest in this topic and to position their work with respect to recent advances.  The results presented in this work do seem to further advance the understanding of graph scattering and one's ability to extend the theoretical foundations of spatiotemporal scattering to graphs and networks. The authors consider here a generic graph scattering transforms that can be realized by choosing appropriate graph wavelets. Similar to Zou and Lerman, they consider invariance to permutations as an equivalent of translations (from traditional signals), and a certain family of perturbations as an equivalent of the deformations considered in traditional scattering studies. The latter result is interesting, as it formulates a new notion of deformation families on graphs, together with an associated notion of stability that does not directly depend on the eigenvalues (or spectral gap) of the associated graph laplacian. The authors also essentially remark that the bound provided by their Theorem 1 is a more natural extension of traditional scattering stability results, as it partially recovers the result from Mallat (2012) when applying the theorem to a line graph, albeit with certain caveats. I believe this point warrants more attention than a minor remark, and I would recommend it be formalized in the main paper in the form of a corollary.  However, while the work presented here is clearly timely and the results should be of interest to those familiar with Mallat's work (and related extensions), I am not sure how accessible it would be for a wide audience targeted by NeurIPS. I would imagine the main communities targeted by such work should be the ones looking into network and graph representation learning. If this is the case, it might be difficult for many readers to really understand the impact of the presented bounds and results. For example, how would the error matrix E relate intuitively to the type of operations one might expect to perform when "deforming" a graph? In the traditional case, operations phrased in terms of translations, rotations, scaling, etc. are quite easily perceived. Further, the structural constraint posed in Proposition 2 seems to severely limit the type of operations that are within the scope considered here. This includes some rather simple operations like adding and removing random edges, to which one would certainly want the representations to be stable. These restrictions need further discussion and some intuitive motivating examples of nontrivial perturbations of interest (i.e., beyond just permutations) for which the provided theory applies.  It should also be noted that since this work targets the task of comparing graph structures via extracted features, it should properly refer to current notions of graph similarities. A classic task considered in this area is whether two graphs are isomorphic to each other, which gave rise to the Weisfeiler-Lehman test. This in turn stemmed extensive work on WL graph kernels and more recently graph neural networks based on message passing (or message propagation) over networks. The authors should discuss the relations (or differences) between their notion of graph perturbation and the ones considered by such previous work.  The last point leads naturally to the main weakness of the paper, which is the somewhat unconvincing numerical evaluation. The experiments presented here do not reflect the vast and extensive amount of previous work done on graph representation and classification tasks. It is disappointing to find the evaluation here does not compare the classification results of the proposed graph scattering to any graph kernel or graph network. Moreover, this evaluation also ignores the many publicly available graph datasets (e.g., from biochemistry data to social networks) that have been used for a while as a standard benchmark for graph classification, both with graph kernels and graph networks. The evaluation here instead uses three somewhat anecdotal and obscure examples. I guess the lack of proper evaluation could be excused in some sense since previous work (cf. Gao et al., ICML 2019) have demonstrated results indicating graph scattering (albeit with somewhat different filters) could be competitive with other established methods on standard benchmark datasets, but if this is the intention here, it should be explicitly stated and properly explained. Even so, the results in figure 2c, discussed in lines 303-309 seem to clearly point to a certain weakness in the graph scattering representation, which the authors attribute (as a "plausible explanation") to the sensitivity of the proposed transform to dropping edges. As mentioned above, this warrants further discussion to clarify the strengths that would balance such weakness, i.e., in terms of which perturbations would not "degrade the stability of the GST".  In conclusion, this is a good paper with interesting results, but the presentation should make it more accessible to wider audience and better position it with respect to the current state of the art in graph representation learning.