This paper consists of two contributions.   1) it provides an interesting analysis results on the MDI feature importances of random forests, which is now one of the most widely used variable importance criteria. Their implication "deeper trees cause more bias" has a very important meaning for RF. The core idea of RF is bagging + feature bagging in each split search, which implicitly assumes that each base learner would be overfitted (fully-grown; low training errors but quite dissimilar tree structures), whereas in boosting, we usually restrict the base learners underfitted ("weak learners"). (in scikit-learn, the default options for RandomForestRegressor or ExtraTreesRegressor are max_depth=None, but max_depth=3 for GradientBoostingRegressor).  2) it also proposes the use of MDI (the mean decrease of impurity) calculated by the OOB samples (collected during bagging of RF), instead of the standard version based on the samples in the bags (bootstrapped samples). I liked this idea very much and I even think that everyone should also use this version instead of the standard ones.   Each point would be very informative and nice, but the link for the proposal of 2) from 1) is a bit unclear to me. In particular, how this simple proposal can "debias" the problematic behaviors indicated in the theoretical analysis in the first part. The MDI-oob can alleviate the bias from deeper trees?   Though I liked the idea of 2), it is simply the use of OOB. If the problem occurs due to the finite-sample variability between OOB samples and in-bag samples, the proposal could be somewhat natural. But the first part 1) describes the behaviors from the depths and min-leaf sizes, and how they are connected to the use of OOB is not well explained.  In particular, what Proposition 1 suggests was totally unclear to me. The paper states "Without the new expression, it is not clear how one can use out-of-bag samples to get a better estimate of MDI". But it is just feeding OOB samples to each tree in RF, and just recalculate the impurities at each node, isn't it?? Whether or not the samples are from OOB, we can always calculate the impurity of any nodes, and thus, can get the MDI from any samples {(x,y)}. (though the sample-covariance viewpoint might be somewhat interesting...)  The fact that the use of OOB estimate for feature selection improves the AUCs would be understandable without any understanding like Theorem 1 and fact 1. But it sounds unexplained that it is way better than MDA (which is also OOB-based estimates, isn't it?).   Minor comments:  - The scores and discussions for trees and forests should be more clearly distinguished. In particular, for section 4 (experiments), the number of trees in RF should be indicated. Readers can misunderstand that Figure 1 and 2 are from single trees. (I checked the code, and found n_estimators=300 though) Also, some might even think that 'bagging' can mitigate the behaviors like Figure 1 and 2.  - symbols f in Proposition 1 is overlapped with f in eq (6), and confusing - line 253: Proposition 1 ==> Theorem 1...?  - MDI='mean decrease impurity' is a common name? 'the mean decrease of impurity' sounds more natural?   * Comments after the author response  I declare that I have read the response. Thank you for making clear the questions. I understand that the link between 2) and 1) is more from a practical point of view, and we have not yet a theory for whether or not MDI-oob can "debias" MDI.  I would appreciate it if you can make clear how to compute MDI-oob in the final manuscript. The response states "the bias of directly computing impurity using OOB samples could still be large for deep trees." But I'm not quite sure whether or not the proposed MDI-oob is different from directly computing ones...? If they are different, you are claiming that the proposed ones are better than "directly computing MDI-oob"..? Can we empirically compare them..? I'm a bit confused with this response, and quite happy if this point is clear in the revised manuscript. 