This paper studies the convergence performance of natural gradient descent for training over-parameterized neural networks. Specifically, the authors prove that NGD can achieve faster convergence rate and similar generalization performance compared with ordinary gradient descent. Moreover, this paper also proves that K-FAC can also converge to global minima with a linear rate under certain assumptions.  This paper is well written and theoretically sound. The limitation of this paper mainly lies in the significance and proof novelty given the existing literature. More specifically, I have the following comments:  1. The studied optimization algorithm is of limited interesting, especially in training deep neural networks. Indeed, the computation of natural gradient is extremely expensive for over-parameterized neural network, since it involves the operations for high-dimensional matrices (matrix inverse and matrix production) in each iteration. Besides, the authors should also clarify why K-FAC can reduce the computation and memory costs compared with the exact calculation of natural gradient, it also involves the inverse calculation for high-dimensional matrices. 2. The proof technique is quite similar to Du et al., 2018b regarding the convergence analysis and Arora et al. 2019b regarding the generalization analysis. Therefore the proof novelty is not significant, and the theoretical results are somehow incremental. 3. In Remark 3, the authors should clearly state the required iteration number to achieve epsilon accuracy, and discuss the comparison with those in Du et al., 2018b, Oymak and Soltanolkotabi, 2019. In addition, the authors should also rigorously illustrate why X^\topX typically has a smaller condition number than the Gram matrix G. 4. In Theorem 6, the initialization of model weights depends on the target accuracy \epsilon, however,  in practice the initialization scheme is fixed regardless of the choice of target accuracy. 5. The statement of the 3rd contribution summarized in the introduction is not accurate. Using a larger step size may not be directly related to a faster convergence rate. In Wu et al., 2019, the authors showed that a variant of adaptive gradient descent can also leverage O(1) step size but attain the same convergence rate as gradient descent. 6. The authors should run some simple experiments to verify the theoretical results, including comparison with GD in terms of both iteration number and running time, as did in Bernacchia et al., 2018.  7. In the proof of Theorem 4, condition 2 is verified in (55), which requires that (54) holds. However, to prove (54), condition 2 is necessary. This implies that when verifying condition the authors implicitly assume that such condition holds.  The authors should clarify this.