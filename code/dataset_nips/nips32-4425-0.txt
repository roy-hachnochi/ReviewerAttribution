A popular generative model these days is as follows: pass a standard Gaussian noise through a neural network. But a major unanswered question is what is the structure of the resulting distribution? Given samples from such a distribution, can we learn the distribution parameters? This question is the topic of this paper.  Specifically, consider a 1-layer ReLU neural network, which is specified by a matrix W and a real bias b. Given a standard Gaussian as its input, the output of this networks is some complicated distribution. Given samples from this distribution, this paper gives an algorithm that learns the parameters W and b. Namely, they show if the number of samples is large enough, then they can recover W W^T and b approximately (Theorem 1), and they can also learn the target distribution within a small error in total variation distance (Corollary 1). The main assumption is that each component of b must be non-negative. Indeed, in Claim 2 a simple example is given demonstrating that if b has negative components, learning this negative component requires exponential sample complexity.  This is the first paper that gives an algorithm with a rigorous guarantee for this important problem, and the paper is well-written. The technique of the paper is also interesting: they first learn the norms of the rows of the matrix W, and then they learn the angles between the rows.  The main limitation of the paper is the assumption that each component of b is non-negative. It's really a limiting assumption, and there is no reason to believe that this will happen in practice. The authors have argued that if some component of b is negative, then to estimate this component we need exponential samples. But then maybe estimating the components of b is the wrong objective. In many applications, you just need to output some distribution that's close to the target in the total variation distance. Can we do this even if b has negative components?  Other comments for the authors: - line 24: it's not clear what do you mean by "latent code z" - In equation (10) and elsewhere: write P(A) instead of E[1(A)] - Line 243: matrixl -> matrix - There are lower and upper bounds for the total variation distance between Gaussians, look at the arXiv preprint "The total variation distance between high-dimensional Gaussians"  == After reading the response: thanks, I have increased my score.