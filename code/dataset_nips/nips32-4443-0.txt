The approach works for cases where a given agent-task pairing defines a fixed policy, so it's hierarchical in nature. It formulates planning as an agent-task constrained matching problem with singleton and pairwise terms, optimized by a constrained LP or QP. The formulation is scalable (polynomial in nature) and generalizes to larger variants of the scene.   The novelty:  -- The pairwise agent-task formulation is similar to [22], but focused on agent-task matching in the policy network. 2 different score schemes (Direct and PEM), the latter of which somewhat novel.  -- The idea that through structured matching, training from smaller tasks, generalizes to larger tasks and evidence to this effect.  -- Experimental results demonstrating intuitive results, that for matching problems QUAD+simpler DM or LP+simpler DM features are superior to PEM features. PEM has higher expressivity, but does not learn as well and does not generalize as well. Rather compelling videos of the resulting strategies are shared in complementary material.   The paper is clear, all details are described and code is shared. It would help a bit to elaborate on why exactly AMAX-Pem and Quad-PEM did not converge. Is that a result of the optimization or the model collapsing, or both? 