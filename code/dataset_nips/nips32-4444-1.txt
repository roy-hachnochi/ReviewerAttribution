POST-REBUTTAL UPDATE:  I have read the rebuttal and other reviews; my score remains unchanged.  =======================================================  Originality: This paper builds upon a line of works on the equivalence of DNNs and kernel methods and adds results for fully-trained CNNs to that list. The authors also prove a new convergence theorem for neural tangent kernels with a better convergence bound.   Quality: The paper seems technically sound. However, I did not verify the provided proofs.   Clarity: The paper is well-organized and clearly written.  Significance: The provided experiments show that the proposed kernel outperforms existing kernels on CIFAR-10. It would be nice to see the performance on other datasets as well.   It is not clear from the experiments how significant the results from the 21-layer kernel are. Does its performance degrade further with depth? Or is the difference between a 21-layer kernel and an 11-layer kernel not significant?   Also, the idea of using less data for training to aid model selection is very widespread in the neural architecture search community. It is not clear why this could have been any different for CNTKs. I don't think these experiments are very relevant to the scope of the paper unless the authors show, for example, that CNTKs generalize better from less data or can be used for the design of finite CNNs.  Finally, 