Originality: A new theoretical framework of approximating functions defined on low dimensional manifolds with ReLU networks  Clarity: Needs more improvement (see below)  Quality: Needs more improvement (see below)  Significance: The topical area is important because ReLU networks are widely applied in deep learning  More details comments:  In the abstract, line 2, please be more specific when denoting line of research; this line seems very generic.  Line 14, you say you implement a sub-network but there are no experimental results to demonstrate that, please reword with correct terminology.  Line 48, it is not intuitive what it scales to, please consider rewriting it Line 68, you denote the \sigma function for sigmoidal then you use the notation again for the quadratic rectifier. Please stay consistent with terminology. Also line 68, max(0,x^2) = x^2, it looks like a typo since it will be non-negative. Line 145 (definition 6), to cite https://arxiv.org/pdf/1705.04565.pdf since they have similar definitions that the two books you cite do not contain Moreover, their definition of reach implies that \tau is ALWAYS zero because p can equal q. If we say that p \ne q, then we get something that makes sense. HOWEVER, we can still get \tau = 0, for example when M = {(x,x): x in R^+} \cup {(0,x): x in R^+}. Hence, their statement that \tau > 0 is false. Line 153  the claim you make with small \tau requiring multiple charts is only for your method. The M denoted above has \tau = 0 but only requires one chart. Line 159 (Assumption 1). Please define that B is an arbitrary constant. Line 160 (Assumption 2), \tau was already a definition. It is unclear why it would need to be an assumption. Line 177, it is not clear at all if it is possible to partition a manifold with open sets. Additionally, it is not clear if you need finite or infinite open sets, if it is even possible. Figure 2 there needs to be a caption with the definitions of the terms used in the figure. The terms are defined later on but we needed to scroll down several pages, so for completeness when looking at the figure, it would be better to define it. Line 208, Please define that the function you are trying to implement which is f(x,y) = xy Line 213, your subheadings are inconsistent in these sections, either capitalize the subsection headings or do not. Please set a precedent since in Step 1 you only capitalize the first word but in step 2 you capitalize. Thickness footnote needs to be elaborated in detail. Line 240, please consider renaming the approximate indicator function, it is a bit confusing the terminology since it is more of a unit step function. Line 269, do you mean uniform convergence? If so, please do write detail it. Line 272, is each term in the Taylor expansion a different layer of the network? That is what it seems like but by the wording it can be confusing. Line 324, please cite that relu networks are easier to train. Line 332, please cite again the papers that make the claim. Discussion, it would be better to have a paragraph that summarizes the conclusion instead of 5 sub-paragraphs that feel disjointed and not cohesive. You discuss implementing the network, but there are no experimental results. Since this paper is theoretical, it would benefit a lot if some experimental results were provided.