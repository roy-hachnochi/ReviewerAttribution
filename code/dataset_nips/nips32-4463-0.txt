The paper proposes a new approach for unsupervised learning of text embeddings. The method is grounded on theoretical results and the proposed method seems efficient (faster training) according to the experimental results.  The big problem with the paper is the experimental setup/results: 1) Experiments are performed with embeddings of size 100 only. Methods such as word2vec are known to perform better in analogy experiments when using embedding dimension of 300 or more. 2) The authors mention that they achieve state-of-the-art results for word similarity and analogy tasks. However, the authors do not compare their numbers with the ones reported in previous works, they only compare with their own runs of previous embedding models. For instance, the numbers reported in the original fastText paper (https://aclweb.org/anthology/Q17-1010) are much better than the ones reported in the experimental section. In the word analogy task, (the original) FastText achieves accuracy of 77.8 (SemGoogle) and 74.9 (SynGoogle), which are significatively better than the numbers reported for JoSE. Is this only related to the embedding size? The original FastText uses embedding size of 300. Does JoSE improve when the embedding size increases? Is it still significantly superior to the baselines when using embedding size of 300? Are embeddings of larger dimensionality less sensitive to the effect of training on Euclidean space and testing on spherical space? 3) Although the clustering task is interesting, I believe that a text ranking task such as answer selection (that is normally tackled using cosine similarity on embeddings) would give good insights about the effectiveness of the paragraph embeddings for a more real-world task.   === Thank you for adding the new results. I've change my score to 6.