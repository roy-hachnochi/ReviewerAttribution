This paper proposes JoSE, a method to train word embeddings. Their unsupervised approach is rooted in the principle that words with similar contexts should be similar, where they have some novelty in their generative model using both word-word and word-paragraph embeddings and the novelty largely lies in their constraint that all embeddings are on the unit sphere - where they derive an optimization procedure for this constrained problem using Riemannian optimization. They also utilize word, paragraph s  The empirical results form this paper are strong - outperforming the GloVe, Poincare Glove, and Word2vec baselines considerably in some cases. FastText is also outperformed as well, though less so, but FastText does have the advantage of using character n-gram information which is not used in JoSE. They also evaluate on analogies and embedding documents from the 20 newsgroups dataset and clustering them, evaluating on the purity of the clusters.  While using spherical topology for embeddings is not anything new, I have not seen it applied in this manner. I find the results impressive enough for acceptance - and I checked the paper for experimental issues that would give them an advantage. They do have a hyperparameter they tune but they use the same value for all experiments. Other methods keep default hyperparameters. One concern I also had was efficiency, but their method is actually the fastest of the methods they compare to as well.  A couple minor nits. SIF is used as a baseline, what does that mean exactly? SIF largely corresponds to techniques to leverage existing embeddings using SVD etc. Was that done here? Or by SIF do you mean the SIF embecdings were downloaded? If the latter, "Towards Universal Paraphrastic Sentence Embeddings" should be cited since those are largely the actual embeddings in SIF.