Overall I think the idea is quite interesting, but because it is an empirical result, it should be validated better.  Resolution difference during training and testing (mainly due to data augmentation) has long been known to the community, but few have been done to handle it. The proposed fine-tuning, albeit simple, works quite well.  However, since this is an empirical submission on an empirical topic, I tend to look for more points that are "useful". For example, what is a good ratio between the data augmentation hyperparameter(s) and the train/test size ratio? In other words, a general rule of thumb for practioners in image recognition on the ImageNet dataset.  And, are the observations generalizable? If I transfer the ImageNet model to a specific task (e.g., CUB), will the findings in this paper be useful? If the answer is yes, how can it be useful?  How about other domains where scale difference is not caused by data augmentation? e.g., object detection?  ----- I raised my recommended score a bit after reading the response. The author response answered most of my questions (except the detection one), and the answer to R1 concerning state-of-the-art (86.4%) on ImageNet is interesting.