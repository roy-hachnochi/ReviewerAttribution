The paper is well written and the mathematics looks correct. The technical presentation of the paper is concise, and concrete examples are used to illustrate the fundamental ideas. All theoretical results are supplemented with complete proofs.  Also such research could shed some light on poissoning attacks in adversarial machine learning, a current hot subject.  Limitations: - The empirical results should be moved from the supplemental, to the main paper. As the paper stands, its presentation is incomplete. Whole sections of a paper shouldn't be moved to supplemental in order to make space. Page limits exist for a reason and should be respected.  - One thing that bugs me is that the authors assume there is such a thing as a "learning phase" and "testing phase". I doubt such a division of concerns is reasonable in real-life. Also, assuming that the space of possible attack types for the attacker / follower is public known is rather a very restricted assumption. In "real-life", a defender would eventually learn the attacker's type modulo statistical noice associated to the fact that only a finite number of interactions are available. Thus a defender could replace empirically learned follower / attacker type with an uncertainty set around it (as is done in "distributionally robust optimization" literature), and play against an imaginary attacker with worst-case type in this uncertainty set. This would be optimal and data-driven. No ?  - The new proposed goodness measure EoP, seems somewhat adhoc. What interesting conceptual / theoretical properties does EoP have ? A theoretical comparison to more classical measures would be worthwhile.  Typos: - line 57: improvement ==> bring improvement ? - line 64: please give concrete examples of such "many real-world" situations