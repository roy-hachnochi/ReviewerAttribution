To the best of my knowledge, the paper is the first to introduce an *exact* coreset/sketch construction for linear regression problems (i.e. there is no loss in quality by using the coreset/sketch instead of the original input data). In contrast, other work, e.g. [13] (see paper), focuses on coresets/sketches for which one can only obtain approximation guarantees.  To achieve this result, the paper combines known results, but in a non-trivial non-obvious way. The paper gives proves for all theorems (in the appendix); these proofs are easy to follow. Also the overview on the contributions, related work, and the high-level overview on the approach are useful.  In the experiments, the contribution of the paper helps to speedup known LMS solvers in the scikit-learn library by a large factors. Hence, the contribution of the paper adds a real measurable value for practical applications.  However, sometimes the descriptons are a bit hard to follow and non-straightforward. Especially the "weighted set" vs. matrix  notation is a bit hard to follow: - The definition "P={p_1,..,p_n} is an ordered finite set" is strange. The given set might very well be a multiset (and ordered), so a tuple notation like (p_1,...,p_n) might be clearer. - In Algorithm 3, "C in R^{...} a weighted subset of rows in A", matrices are interpreted as sets. How about "rows of C ... are linear combinations of rows in A" (to avoid the "ordered multi-set" problem alltogether)? - In Algorithm 2, "S in R^{...} whose union of rows is a weighted subset of A". It's not a weighted subset. Its rows coincide with a certain subset of rows of A that have been scaled. (For people that are used to  coresets/sketches in the context of clustering this can be very confusing).  Some more minor remarks: - [8] is mentioned Theorem 8.8 directly, while [27] is mentioned separately in the text twice - but it seems [27] explains Theorem 2.2 as well? - "Then (C,u) is a Caratheodory set of (P,w) that can be computed in time..." - The given algorithm has this running time, not some other algorithm that can compute (C,u) - wikipedia as a reference... - In l. 95, "linear in the input O(nd) for asymptotically large n" is a weird way of ignoring the dependency on d, better write "linear in n and polynomial in d". Moreover, "only log(n) calls to an LMS solver" is not so meaningful because it's unclear to what the LMS solver is applied. - I think it's nice that the paper does use O-notation correctly but the explicit remark in l. 141 seems unnecessary since it's only about one "\in" instead of "=" (and all the theorems the  use abbreviations anyway ("comuted in time O(...)" = {...}=set) - The font size used in the figures in Figure 2 are way too tiny. The results shown in Figures 2(r) and 2(s) are nice (if you can zomm ;)) - but it's aweful how the sub-figures have been pasted together (and again, how tiny the fonts are) - l. 212 "time for .. becomes neglected" - l. 215 "is much significant"  Some typos: - l. 43: "this errors increases" - l. 178 "from [the] previous section" - l. 68 "on the web T" - l. 199 "the common Python's solvers" - l. 130 "a sufficiently coreset" 