After rebuttal:  I have carefully read the comments of other reviewers and the feedback from the authors. The authors provide insights about generalization to high dimensional cases promise to include the experimental results for high dimensions, which address my main concern. However, the experiment results are not provided in the rebuttal. Thus, I would like to keep my score unchanged.  -------------------------------------------- Summary:  This paper analyzed the dynamics of gradient descent algorithm training an over-parametrized one-hidden-layer ReLU neural network on an interpolation task in three settings and compare them.  In this paper, the authors are using a neural network with one hidden layer and one output layer. The activation function is ReLU, and each hidden unit also has a bias term that can be trained. The input is 1-dimensional, i.e., a scalar, so the task is equivalent to interpolation. The loss function is square loss, and the authors use gradient descent to learn the network weights. The authors use an over-parametrization scheme where the number of nodes in the hidden layer tends to infinity, which means the network is infinitely wide.  There are two main learning schemes mentioned in this paper: Kernel and adaptive. The Kernel scheme is that the weights of the neural network stay very close to the initialization and the optimization process works in a linear approximation in this neighborhood. The adaptive scheme is using mean-field setting and the evolving of the weights follows a PDE. In the adaptive scheme, since ReLU function is positively homogeneous, the weights have some redundant terms. Thus, the authors divide the learning scheme into the reduced parameters version and full version.  The authors identified a condition \delta, which only depends on initialization, that decides the style of the learning process. If \delta is very large, we are in adaptive scheme, resulting in adaptive linear splines, while when \delta is very small we are in the Kernel scheme, resulting in cubic splines. Otherwise, we are somewhere in the middle of these two schemes. The authors also characterize the dynamics in these two schemes, and their experimental results validate their theoretical analysis.  My comments for this paper:  1. The experiments in this paper focused on the exact setting as the theoretical analysis, but the theoretical analysis doesn't provide much intuition about whether these kinds of phenomena can generalize to more complicated cases like high-dimension case. Thus, it would be better if the authors could do some experiments about general cases and whether some similar dynamics preserves.  2. For the Kernel methods, some recent results showed that even if the hidden layer is only polynomially wide, the weights will still stay in a small neighborhood with high probability. I wonder whether in this case the dynamics are the same as the result in this paper.