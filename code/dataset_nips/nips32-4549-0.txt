This paper propose a new algorithm to solve a stochastic optimization problem distributively over the nodes of a graph of computing units.  As usual, the algorithm involves a communication step and a computing step at each iteration. The algorithms only involves local computations.  To be robust to the effect of stragglers among the computing units, the proposed algorithm imposes a deadline in computation time at each iteration. To avoid the communication cost, only quantized version of the local iterates are exchanged during communication steps.   Two theorems are given :  Th 1. The loss is strongly convex and a bound is provided for the expected square distance to the solution.  Th 2. The loss is non convex and a bound is provided for the expected square norm of the gradient **at the average iterate**. A bound is also provided for the distance to the consensus.  The problem is relevant. Deadlines, local computations and quantizations has been considered in the literature of collaborative learning but separately.    The explanations are a bit technical but clear and the paper is well written.  Numerical simulations are provided.   I have two comments for the authors.  1. Assumption 2. It is assumed that the variance of the quantization is bounded whereas some authors allow it to grow with \|x\|. Could you remove this assumption?  2. Eq 10 should include the cost of computing the average iterate since in this paper, communication time is a bottleneck.   ***********************************************************************************  I appreciate the authors's response, especially the considerations for the convex setting and for the variance of the quantization (not for the convex setting, of course)