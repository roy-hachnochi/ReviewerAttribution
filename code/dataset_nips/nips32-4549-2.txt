This paper combines gradient with deadline and quantization techniques into the decentralized optimization algorithm. Although the convergence in the nonconvex setting looks interesting and solid, the combination limits the novelty of this paper,  1. The experiments are too simple to show the efficacy of the proposed algorithms, merely MNIST and CIFAR10 datasets.  2. The size of the tested neural network is too small. In addition, whats the activation function for the fully connected neural network? If the ReLU activation was used, the tested loss function is not smooth.  3. For first order stochastic methods, their performances are sensitive to the learning rate. The authors should report the best results for each solver with well-tuned learning rates 