The paper proposes an algorithm to solve sparse reward MDPs with expert demonstrations by learning a set of sub-goals from the demonstrations. To learn the sub-goals, they train a multi-class classification model that assigns agent states to different clusters. To ensure that states later in the trajectory do not belong to a previous sub goal, they used dynamic time warping to adjust the output of the model and use it to generate new training targets. To deal with the problem that the demonstrations do not cover the entire state space, they used the deep one class classification algorithm so that the final algorithm only uses prediction for states near the demonstration trajectories.  The method seems reasonable to me and the results seems solid. The paper is not difficult to follow. However, I have a few concerns and questions regarding the proposed approach, as listed below:  First, it’s not clear from the text how important it is to learn a neural network model for the sub-goals. From the results in Figure 7, it seems that the results could be produced by some clustering algorithm as well? For example, what would happen if one uses the equipartition (eq 2) with a nearest neighbor classifier to specify the sub-goals? Also, despite that the use of DTW to ensure temporal consistency is interesting, it’s not verified if it’s necessary to do so in the experiments.  Also, the sub-goals used in the experiments seems to be the COM position of the robots (based on the look of Figure 7) and all the problems have a navigation nature. It’s not very clear how well can this generalize to higher dimensional states, like for manipulation or locomotion tasks where the full joint state of the robot is needed.  What happens if the expert trajectories solve the problem in different ways, i.e. there are branches in the middle of the trajectory?   In addition, there has been some existing work that uses self-imitation to deal with sparse reward problems [1], which could be applied to the problem of interest here. It would be nice if some comparisons can be done.   [1]. Learning Self-Imitating Diverse Policies. Gangwani et al. ICLR 2019.   ================================================== The authors' response has addressed most of my concerns and the additional experiments regarding sub-goal modeling is well appreciated. I have updated my score.