The paper is well motivated and quite clear. I like the distinction between statistical, functional and heuristics methods of normalization. Also, investigating normalization techniques that do not rely on mini-batch statistics is an important research direction. I have however a few remarks concerning ON:  1) How does it compares to Batch Renormalization (BRN)? Both methods rely on running averages of statistics, so I think it would be fair to clearly state what are the differences between the two methods and to thoroughly compare against it in the experimental setup, especially because BRN introduces 1 extra hyper-parameter, while one need to tune 2 of them in ON.  2) How difficult is it to tune both decay rates hyper-parameters? Is ON robust to poor choices of decay rates? It would be nice to have an experiment showing their impact on performances (maybe a heatmap?).  3) It is stated several times that "ON has the best validation performances". However, without proper mean and standard deviations of the results, it is difficult to infer how good it is compared to other methods (especially when the difference in results is so small). So having the standard deviations on each curves and in each table would be more compelling. Also, I think it is more typical to compare models using accuracy rather than loss, because it is the metric we actually care about (and sometimes the loss is not a good proxy for accuracy, as one can see on the ImageNet results).  And here are a few remarks on the experimental setup:  CIFAR10: - What is the Batch Size used for online normalization? 1 or 128 like in BN (footnote "a" in table 2 confuses me)? - The "validation" set you used is actually the test set of CIFAR10. Validation should be performed on a fraction of the training set (usually the last 5000 examples), and final results should be reported using the test set.  Language Modelling: In Figure 11, Layer Normalization performs as poorly as no normalization. After looking at your implementation, it seems that you implemented LN wrongly: the gates should be computed like this: gates = LN(Wx) + LN(Uh) + b rather than: gates = LN(Wx + Uh + b)  After Author Response: Thanks your for your response and clarifications! Here are my last comments:  - I appreciated the comparison with BRN, and I really do think it makes a strong case for your paper.  - Given the rather big overlap in Figure 4 (at least in the feed-forward case), I would suggest to change the claims: "ON has the best validation performances" to something more subtle: "ON performs similarly as BN, while removing the dependency on the mini-batch" (or similar).   That said, I increase my score to a 7.