The paper "Online Normalization for Training Neural Networks" proposes a novel technique for normalization when training neural networks. While classical batch normalization results in biased gradients even for moderate batch sizes, online normalization introduces exponential decaying averages of online samples, which presents interesting formal properties. The paper appears theoretically well-sounded. The problem is important in the field of neural networks. Experimental results are pretty convincing.   My main concern is about the presentation of the paper. I had a really hard time with it, as a lot of details are omitted and contributions not enough highligthed. Authors should take a more pedagogical approach, where they present in more details classical normalization techniques, so that the reader can have everything in mind, in a formal version that allows him to better apprehend the notations used in the paper. Several parts are particularly hard to follow, in particular 2.2, 2.3 and 2.4, where plots given in the corresponding figures are not sufficiently detailled.  More discussions about results would also be needed.  Also I am still not sure whether the mu and sigma are computed over samples of the current batch for each feature or they are computed over all features. For me it's the former but some sentences make me rather uncertain about this (particularly sentence 187 about image samples). Please clarify.   I have read the author response and my opinion remains the same.  