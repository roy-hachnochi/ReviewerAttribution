The authors propose a new normalization method, called Online Normalization. The new method drops the dependency on batch size and allows neurons to locally estimate forward mean and variance. Furthermore, Online Normalization also implements a control process on backward pass to ensure that back-propagated gradients stay within a bounded distance of true gradients. Experimental results show that Online Normalization can reduce memory usage largely with the matching accuracy comparing to Batch Normalization.   Strength: 1. This work drops the dependency on batch size, leading to lower memory usage and wider applications. 2. The authors resolve the biased problem in Batch Normalization and introduce an unbiased technique for computing the gradient of normalized activations. 3. Experiments are solid and convincing. The experimental settings are described clearly.  4. The paper is written clearly. The structure is well-organized and It is easy to read and understand.    Weakness 1. Online Normalization introduces two additional hype-parameters: forward and backward decay factors. The authors use a logarithmic grid sweep to search the best factors. This operation largely increases the training cost of Online Normalization.   Question: 1. The paper mentions that Batch Normalization has the problem of gradient bias because it uses mini-batch to estimate the real gradient distribution. In contrast, Online Normalization can be implemented locally within individual neurons without the dependency on batch size. It sounds like that Online Normalization and Batch Normalization are two different ways to estimate the real gradient distribution. I am confused why Online Normalization is unbiased and Batch Normalization is biased.    ** I have read other reviews and the author response. I will stay with my original score. ** 