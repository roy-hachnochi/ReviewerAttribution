The paper considers TD algorithms with linear function approximations. The key observation is that the learning dynamics of weights in a TD algorithms follow MJLS dynamics. Then the paper uses MJLS theories to analyze the dynamics of TD algorithms with linear function approximations, and then from the dynamics provides stability analysis for the learning algorithms.  The method provides a new systematic way to analyze the behaviors of TD learning algorithms which are important for RL research. The technical proofs are correct, but the presentation could be improved.   Some comments:  - The definitions of \mu^k, q^k and Q^k are hard to find and in the paper, and they are a bit confusing. It's probably better to define them in Section 3 together with the jump dynamics (12) and provide more explanations.   - Theorem 1 and 2 are basically some algebraic manipulations of the TD dynamics, and the stability and limiting behavior are the main results of the paper. Therefore, it feels like providing some formal theorems for the stability and limiting behavior might better highlight the main results.  - In Theorem 1, (14) uses \mu^k but (16) uses q^k. It seems like a typo in (16).   - The assumption \sum p_i b_i = 0 is desirable, but not clear if it would actually hold. It feels like this assumption is related to the representability of the linear function approximations. More discussions on this assumption is important. In particular, what is the dynamics when this expected value is not zero? What is the limiting behavior when this assumption does not hold? 