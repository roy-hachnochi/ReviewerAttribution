1. Originality: while the algorithm studied in the paper is standard (i.e., approximate policy iteration and LSPI), I think the sample complexity results are new for LQR setting. Previous work on approximate PI on LQR is for deterministic setting and only studied asymptotic convergence.   2. Quality & Clarity: I think the paper is well written and organized. The survey on related work is thorough as well.    3. While I enjoyed reading the paper and analysis, I'm not quite certainty about the significance of the results presented in the paper. The proposed analysis seems not that straightforward to be extended and be leveraged to analyze more general settings (in general, achieving a uniformly accurate value function is also hard, which could make naive policy improvement unstable). Regarding the practical performance, the model-based (certainty equivalence method) clearly dominate the performance. The certainty equivalence method itself is straightforward and is recently proved to be efficient as well. It seems to me that when I'm trying to solve LQR learning problem, in default I would use the nominal control approach as planning in LQR is straightforward, which leaves the question when/under what circumvents I would use the proposed LSPI algorithm?  Some discussion on the significance of the results will be useful. 