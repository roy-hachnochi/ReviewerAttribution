Compared with existing work [8], the contribution of this work is incremental.  The key idea of proposed M-ASG is to use a exponentially decreased step size and increased number of iterations for each stage. In fact, this idea is similar to the results presented in [8]. More importantly, the proposed results only work for  strongly convex objective. As discussed in [8], compared with generally convex objective, the accelerated method is much less sensitive to noisy gradient estimate if the objective is strongly convex. Therefore, more challenging and important problem  is how to design an accelerated algorithm so that it is robust to noisy gradient and able to achieve the optimal rate.   Compared with u-AGD [8], as showed in Figure 2 and 3, he proposed algorithm does not show clear difference and improvement  in terms of empirical performance.   After rebuttal: the authors' feedback partially address my questions and I have changed my score.