**SUMMARY * Goal: Increase the capacity of memory in memory models while keeping attentional-access cost feasible. * Model innovation: Product-Key Memory (PKM). Use as a key for a stored memory a pair of half-sized sub-keys. Treat a query as a pair of half-sized sub-queries. There are two codebooks of N sub-keys each, and the Cartesian product of these codebooks defines the set of K = N^2 full keys for the memory. For retrieval (attention): Given a query, limit retrieval to a linear combination of the values associated with the k keys that best match the query.  For each sub-query, find the closest k sub-keys (maximal dot-product). The Cartesian product of these two sets of k sub-keys must contain the k full keys with largest dot-product with the full query. * Complexity: inference is reduced from O(K*d_q) to O((sqrt(K)+k^2)*d_q).  * Tested models: The contents of memory are fixed at test time. (It is like Long Term Memory, not Working Memory.) In Transformer models, the FFN is replaced with a memory-retrieval network in typically 1 layer [158]. Memory size is K = 512^2 = 262k, k = 32 selected keys [226] * Task: Language Modeling on 28B-word news articles from Common Crawl corpora. * Results: E.g.,  Inserting one PKM layer in a dim-1600, 16 layer Transformer drops perplexity from 14.4 to 13.2 [Table 1] "Models with 12 layers and a PKM outperform 24-layer models of the same dimension, while being almost twice faster at inference." [Fig 4] Compared to comparable models with standard flat keys, "models with product keys are not only faster but also they have a much better memory usage, and consequently obtain a better perplexity" [266].   **REVIEW *The proposal is a low-level implementational modification that, for language modeling with huge training data, does seem to enable performance improvement while lowering inference cost. The memory layer proposed is in principle insertable into any model. * It is unclear how general the improvement would be across models and tasks. * The thorough set of ablation studies provided lends support to the numerous design choices. * The results are not presented with error bars, significance levels, or other indications of variability across runs. 