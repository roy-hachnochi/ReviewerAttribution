The paper provides a "memory layer" with a large number of parameters. The parameters are accessed by a content-based attention. The attention takes a query and finds the top-K parameter vectors with the nearest keys. The innovation is in the efficient way to find the top-K keys. The efficient search splits the key to 2 parts and searches over 2 * sqrt(N_{items}) sub-keys, instead of searching over N_{items}. This still provides exact results, because the memory holds an item for each (subKey1, subKey2) combination.  The experiments are done on a dataset with 28 billions words. The extra parameters are helpful there.  Comments: The paper is good in all aspects. The writing is clear and pleasant to read. The experiments convincingly show the benefits of the model at the very large dataset. The memory layer may become a standard building block for large networks.  I like that conditional computation models are mentioned in the related work section. The memory layer provides a nice alternative way to increase the network capacity.  Minor details: - Line 122: The usage of top-K attention weights is not fully differentiable, because the k-th attention weight changes with a discontinuity when switching from rank k+1 to k. If needed, it would be possible to design a fully differentiable output, by ensuring that the k-th attention weight is zero at the tie-break.  - Under line 140: The "in practice, the cost of ..." sentence should probably be: "in theory, the cost is O(k \log k x d_q) with a priority list ...".  Update: Thank you for the polite feedback. The answers were informative. I hope the paper will be accepted.