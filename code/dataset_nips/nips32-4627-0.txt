I enjoyed reading this paper. This paper builds on the recent NTK paper and develops rather surprising theory that the gradient descent dynamics of a deep neural network is actually very close to the dynamics of a simple linearized model, for wide neural networks. I’m also very impressed by the empirical results that the paper provides, because the experimental results corroborate the theory even for realistic-sized networks.  Given that NTK-related results are getting much attention these days, I believe this paper would be worth reading for many people. I vote for acceptance.  Minor comments: - I guess \hat \Theta^{(n)} is not defined anywhere. Does it mean the empirical tangent kernel evaluated of a width-n network? - Line 127: There is no e^{-\eta \Theta_0 t} term in Eq 2 and 3? - Line 263: Effects of depth? Not width? The paragraph doesn’t have any discussion on depth. - Figure 4: dashed lines are almost invisible; can you try to improve readability?  [After rebuttal] I have read the authors’ response and the other reviews and I think my concerns were well-addressed. I’ll keep my score.