The paper was proofread, well-structured, and very clear.  The experiments were clearly described in detail, and provided relevant results.  Below we outline some detailed comments of the results. 1) Relation to "On Lazy Training in Differentiable Programming" by Chizat and Bach - Some of the main results of this paper are very similar to those proved by Chizat and Bach. In particular, Chizat and Bach prove that the training of an NTK parameterized network is closely modeled by "lazy training" (their terminology for a linearized model).  This paper is not referenced in the related work section.  This seriously detracts from the novelty of the submission. 2) Applicability of the proven results to modern networks - The authors claim that the NTK parameterization closely models the modern neural networks used in practice.  While it is true that the 1/sqrt(n) scaling is used in many modern networks, the optimization algorithms, and in particular, the learning rates used during training may invalidate the assumptions of this paper.  In the aforementioned paper by Chizat and Bach they present and cite empirical evidence that the linearized networks do not model modern networks.  For instance: a) Modern neural networks for image tasks have layers that learn interesting (non-random) filters. b) While the ``Are all layers equal'' paper does show that most layers can be reinitialized, this is not true for all layers.  In particular, the first layer of each residual block in a residual network cannot be reinitialized, and do show more parameter movement than the other layers.  In particular, the parameters in the first layer of the network tend to move substantially in comparison to the other layers.  3)Experimental results - While experimental results were presented for the CIFAR-10 dataset and do show that NTK models track linear models well, this doesn't provide convincing evidence that NTK parameterized networks provide a good model for modern networks.  In particular, the test accuracies of the wide residual networks used are far from state of the art. 