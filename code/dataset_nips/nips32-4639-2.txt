The paper proposes a method to perform VQA tasks using self-critical reasoning.  The paper argues that the model should not only focus on the right/important reason  when giving the correct answer, it should also not focus on the important regions while providing wrong answers. The main contribution of this work is to how not to focus on those important regions while providing wrong answers. Identification of important regions is a hard problem, so they reduced this to identifying influential objects in the image for the QA pairs. The influential objects are obtained using 3 methods: a visual explanation using VQA-HAT,  textual explanations using Park et al and automatically by parsing QA pairs. Using selected influential objects, VQA model based on UpDn approach is proposed such that it penalizes the high ranked incorrect answers to make them less sensitive to influential objects. To do this, they propose an influence strengthen loss such that the correct answer is more sensitive to the corresponding object.  To decrease the influence of the influential object on the incorrect answer, a self-critical loss is proposed. Author's have provided the utility of the method on the generalizable VQA-CPv2 and VQAv2 dataset.  Results clearly show that the proposed method outperforms other methods, on the VQA-CP dataset and comparable performance on the VQAv2 dataset. However, having a 0.3% improvement using the textual description is not that significant.  The ablation study clearly shows that having a self-critical loss only improves performance by 6.3%, which is very significant.  At the same time, having only influence strengthen loss improves the performance comparatively more. It will good to comment on that also.  Specifically, Yes/No and Other type questions are more effected by influence loss.  Do authors thought about having a significance test on these results and can show that both these loss performances totally different or similar manner? Overall the proposed method is novel.   Some of the things which are not clear:   - L25-30: says that "which we have found occurs quite frequently". How this "frequently" is being calculated. - L111-112: when authors say "we assume that it at least includes the most relevant object.", again do they did some sort of quantification of this, at least on the small subset - L127-131 authors describe automatic creation of proposal set. However, in experiments, it is not clear that all 3 is being used or only some of those. It will be good to have all 3 separately and then combinations.  - For the VQAv2 dataset, how much results are comparable because it has extra data in terms of explanation data like HAT & VQA-X - Did the authors try to have a different weighting scheme for the loss? Minor -L111: maybe -The figure needs to be improved, slightly bolder box and the number is very hard to read -If available, please cite the published version of the paper e.g [27, 30] 