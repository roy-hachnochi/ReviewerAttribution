Overall the paper is well written and the proposed algorithm is well explained. Since it is variant of the existing ADMM, the authors are encouraged to spend more texts to explain the key difference, for example, what kind of advantages we can get by using this new ADMM variant. On the other hand, I find the experimental part is pretty weak. Using real and large scale data may further improve the paper.  Line 36, it seems the existing ADMM can be extended to the case N \ge 3, based on certain strongly convexity condition.   Is the reduction of the communication round due to the two layer-structure in Algorithm 1? Besides this part, what could be the essential advantage of this proposed ADMM method? On the other hand, the authors may want to show how much computation time or resources we can get by this communication round reduction.  This paper doesn’t talk much about the fault tolerance. Does the model convergence rely on the success of the communication? If one communication fails, what will the results look like?  This proposed method is still limited to the decomposable property in the variable. If it is not the case, for example, apply a L_{21} regularization term in the objective, will the Algorithm still work?  In Section 4, these experimental results are all based on synthetic and small data sets. The effectiveness will be more convincing if real and large data sets are used for evaluation.   Some minor comments:   (1) On Line 28-29, the presentation can be be simplified as “Ax = b, x_i == x_j \forall i, j”. Apparently x_i == x_j if i == j. 