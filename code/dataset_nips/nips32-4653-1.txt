This paper proposes a new communication efficient multi-block ADMM for linearly constrained stochastic optimization. The proposed method is as fast as (or faster than) existing stochastic ADMMs but the associated communication overhead is only the square root of that required by existing ADMMs. Although the paper is theoretically sound, there are still some questions need to be discussed in this paper: 1. This paper assumes that the strong duality hold in Assumption 1, but in many applications this assumption does not hold. Therefore, does the algorithm in this paper have some limitations? 2. Whatâ€™s the stochastic objective function? In addition, there are many related algorithms, for example, E. Wei and A. Ozdaglar. Distributed Alternating Direction Method of Multipliers. CDC, 2012. F. Huang, S. Chen and H. Huang. Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization. ICML, 2019.  T. Chang, W. Liao, M. Hong and X. Wang. Asynchronous Distributed ADMM for Large-Scale Optimization-Part II: Linear Convergence Analysis and Numerical Performance. IEEE Transactions on Signal Processing, 2016.   3. The experiments in Section 4.2 of this paper lack the detailed information about the experimental environment, and the tools and hardware facilities used in the experiment should be appropriately introduced. 4. More compared algorithms (including distributed deterministic ADMM and stochastic ADMM algorithms) should be added to reflect the superiority of the proposed algorithm, and the performance of the proposed algorithm in terms of running time should be also reported. 