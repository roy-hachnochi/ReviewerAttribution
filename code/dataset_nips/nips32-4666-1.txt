This paper builds on recent work on finite-time analysis of linear TD learning and provides finite-time results for linear Sarsa. The key insight is constructing a Markov Decision Process for each policy obtained during the optimization.  The contribution is incremental yet significant. Non-asymptotic behavior of Sarsa with Markovian samples is still an open problem and these results will be interesting to the community.  The paper is well-organized and well-written. I have pointed out a few small suggestions to make the proof easier to follow.  Minor comments:  - Write the proof for Lemma 5 explicitly.  - Remind the reader that Eq (50) follows from (49) because of the relationship between \alpha_t and \w_s.  - The environment's transition dynamics is denoted by P while P_\theta shows the invariant measure (rather than the transition dynamics) induced by \theta. This notation is slightly confusing. I suggest using both P and P_\theta for transition dynamics (the latter being induced by \theta).  Questions:  - How realistic is assumption that for any \theta the induced Markov chain is ergodic (right before Assumption 1)? Is it guaranteed for any special class of environments or representations?  - Does this result bear implications for framing an RL problem or designing features?  ---------------------------------------------------------------------- Update: The projection step in the algorithm is problematic because the analyzed algorithm is different from Sarsa and the projection radius can have a large impact on the bounds.  I was initially not concerned with the projection step. I had accepted that it was a limitation of a recent work [4] and that eliminating it was not in the scope of the submitted paper. After discussions with the other reviewers, I became aware that a different line of analysis had already avoided this extra step and provided finite-time bounds for TD with standard and practical assumptions [33]. I lower my score as it is not clear why the submitted paper did not pursue the direction that studies unmodified RL algorithms.