The problem studied here is important since there is no previous finite-sample result for SARSA/policy-improvement without an converged value iteration inside of it, and it is helpful for us to understand the learning dynamics of policy improvement type of algorithm. This type of finite-sample analysis for SARSA is technically interesting to me due to the two challenge mentioned above. The method in this paper is novel to my knowledge. The only concern I have is that the analysis is limited in linear function approximation case. In general I am happy to see this paper get accepted.  === Update === After read the author's rebuttal and discussion from other reviewers, I have some updates: 1. This paper studies the finite sample guarantees on time-varying markov chain, caused by the updating behavior policy during collecting samples. The key insight to deal with this difficulty is novel and clear. 2. As other reviewers pointed out, the proofs seems heavily rely the projection step. The projection radius is used to bound the update term g_t. However some recent work on finite sample analysis of SARSA/Q-learning seems to work without this assumption [33,37]. Given the recent advances, the assumption here seems too strong and also not really necessary. 3. Another concern is about the \theta^* mentioned in the paper as "limit point of SARSA". It is not clear if SARSA with/without the projection have the same limit point or not , which need to clarified. (Even the limit point it self is in the radius, the update dynamics could change.) 4. It is a little bit hard for me to understand how strong assumption 2 is and verified that (even conceptually) with some instances. This assumption is not very intuitive since it depends on the global information of MDPs.  Overall, the novel analysis here is indeed interesting and valuable. So I tend to vote for a weakly accept, though I still keep my concerns as listed in 2,3,4 above.