-------Originality The paper claims to propose a novel method in SSL that learns the graph instead of using a fixed graph. However, some closely related work sharing the same idea has been explored in [1, 4] and is, unfortunately, not mentioned in the paper. SNTG [1] is recent work on graph-based SSL, which also uses an auxiliary model to predict whether a pair of nodes is similar or not. The difference lies in the co-training part. [4] proposes a method based on dynamic infection processes to propagate labels. Please include [1,4] in the related work and add more discussions.  -------Clarity The writing is really good. The paper is clear and easy to follow.  -------Methodology 1. The introduction of agreement model incurs more network parameters. Is there any comparison with the baselines in terms of the number of network parameters? What is the performance of baselines with the same number of parameters as GAM? This seems to be a fairer comparison.  2. The convergence of the algorithm not guaranteed. Since the agreement model can make mistakes including the top confidant predictions, it may augment errors and propagate them into the classification model. How could the improvement be guaranteed at each iteration of the interaction in the co-training? Figure 5 in Appendix 5 also supports the concern. E.g., in Fig. 5(b) the test accuracy peaks at around iteration 15 and no longer improves after that (oscillates up and down). An extreme case that the augmented errors lead to diverged results may happen if the agreement model performs poor at the beginning and is learned slowly. See the sudden drop at about iteration 12 in Fig.5(a). It may not increase at the next iteration but gets worse.   -------Experiments 1. What is the result of GCN_1024+VAT? It is not listed in Table 1. I noticed better results of GCN+VAT than those in this paper were reported in [2]. I was curious why VATENT failed as stated in line 291. Is it due to your implementation or the method itself? 2. In Sec. 4.2, a simple CNN is used for image classification. But this is not commonly used in the SSL literature. The paper only compared to VAT while several important baselines in SSL are missing. E.g., SNTG [1] and Fast-SWA [2] should be included in the related work and Table 3, which achieve much better performance than VAT. I recommend the authors use the standard stronger 13-layer CNN in the SSL literature and report the results, rather than omitting the closely related work [1, 3], especially SNTG which shares the same idea of learning a graph to improve the classification model. 3. How do you choose M, the number of most confident predictions?  -----References--- [1] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018. [2] Batch Virtual Adversarial Training for Graph Convolutional Networks, arxiv 1902.09192 [3] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average, ICLR 2019. [4] Semi-Supervised Learning with Competitive Infection Models, AISTATS 2018.