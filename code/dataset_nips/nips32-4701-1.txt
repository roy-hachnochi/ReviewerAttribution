This paper uses a coupled CNN and RNN architecture to predict wind speeds from 2-second clips of flags (specifically, observing their flow-structure interactions). The authors construct a dataset of checkered flags in three environments: (1) at a field site in Lancaster, CA, (2) at the same field site and with the same type of flag, but 3m away from the original flag, (3) in a wind tunnel, with a smaller-sized flag, and with monochrome camera videos. They then employ their model on this dataset, using a CNN to do feature extraction and then using an RNN to predict the wind speed associated with a 2 second (30 frame) flag video. The authors employ mean subtraction on their dataset to remove background features so that their model will better generalize to different conditions. They also contextualize the performance of their model at different wind speed ranges via frequency analysis (of the Nyquist frequency and clip duration-limited frequency) and turbulence analysis (to understand error bars).  Originality: To the best of my knowledge, this is the first work that uses ML for wind speed prediction from flags. The closest related work (which the authors cite) is in extracting wind measurements from instrumentation (their instrument of choice here is nontraditional -- a flag) and in video prediction (which has not before been applied to this domain). From an applications perspective, this is therefore certainly a novel contribution (and one that would be useful to practitioners). The model evaluation portion also demonstrates originality, as model predictions are contextualized using frequency and turbulence equations.  Quality: The work seems very well-executed, with assumptions and limitations both explained and contexualized in the domain. The model seems to predict wind speeds well on the main and adjacent flag test sets (i.e. the ones at the field site) for a variety of wind speeds, and does okay on the wind tunnel test set (which is not surprising given the different camera type and different flag size). I wish there had been more analysis of why the authors feel predictions for the tunnel test flag set were so uniform. I also have questions about the error bar calculations (see next section of the review). Overall, however, I believe this is a solid, well-executed contribution with good results, and that it poses a lot of challenges for future work to build on.  Clarity: The submission is very clear and easy to follow. The description of the related work, dataset, method, model evaluation are all very clear and written in an engaging manner. Given the dataset, I could likely reproduce the authors' approach.  Significance: I believe this work is significant from multiple perspectives. First, it provides a novel method for predicting wind speeds using commonly-available "instrumentation." Second, it provides a great case study of domain-specific considerations that are necessary for applying ML in the field -- for instance, previous ML algorithms have overfit to background features, and the authors explicitly account for this by collecting and using a diverse dataset. Third, it provides a dataset that the community can use for further exploration of this problem (though this dataset can and should be expanded upon with a more diverse set of flags and conditions in future work).   EDITS AFTER DISCUSSION & AUTHOR FEEDBACK: * Initially, my biggest concern about this paper was that the dataset was not diverse (and I believe this concern was shared by the other reviewers). That said, the author response indicates that the authors have collected additional data (tree canopy movement), and that their model performs well on the tree canopy validation set when trained on tree canopy data. This additional data increases my impression of the value of the dataset presented here, and I do believe the dataset is one of the biggest contributions of this paper. * I am now less convinced that the model generalizes due to the flatness of the test set results (though it's worth noting that most NeurIPS papers evaluate on the equivalent of the validation set -- on which the authors' models perform well -- as opposed to the test set). (I also laud the authors for their honesty in providing corrected results.) Additionally, while I am glad to see that the model performs well on the tree canopy validation set when trained on tree canopies, to understand generalization, it would possibly be more important to see whether e.g. a model trained on the checkered flags generalizes to other flags/the tree, or a model trained on this tree generalizes to other trees.  Given my increased impression of the dataset and my slightly decreased impression of generalization, my score remains the same.