Post-Rebuttal Comments: Thanks for the response, where the authors have the explained the posterior approximation and added additional results, I appreciate that. Also, I agree with the comments from other reviewers, that the proposed method still has several limitations and there is still room for improvement. Taking everything into consideration, I have raised my score by one.    -------------Original Comments------------------  This paper proposes a new random function prior called functional neural processes (FNPs), and discussed approximate inference procedures. Empirical experimental analysis is also provided. The submission is well-written and very easy to follow, and I quite like the idea.  *Significance*:  1, The FNP is significantly different from the previous NP formulation. The FNP improves the NP by replacing global latent variables in NP by the local ones. FNP combines local latent representations with graphical modeling that encode dependencies (which, in a sense, serves the role of global latent variable).   2, By this combination of NP and graphical modeling, FNP addresses some of the limitations of NP architecture. The original NP uses a finite-dimensional global latent variable and performs inference on this variable instead of functions, which does not construct a valid nonparametric prior. FNP removed this limitation and defines a nonparametric prior which could be potentially very useful.   3, Also, the original NP approach focuses more on meta-learning scenarios. On the contrary, FNP model enables learning in a more traditional regression/classification settings with improved uncertainty estimation.   Overall, I believe FNPs could be a useful addition to BDL literature.    *Weakness* While the high-level picture of this submission is solid, I have found that some of the details, especially the experimental evaluations, are not quite convincing.  1, A question regarding posterior derivation: I am not sure about the validity of equation (12). This essentially means the posterior distribution of test point only depends on the choice of inducing data, but not the value of training data. If the derivation is correct, then it indicates that under the approximation, FNP reduces to a parametric model and losses some advantage of being a "functional" prior. Am I missing something? Consider the sparse GP counterpart (which has a very similar graphical model of the relation between the inducing points and training points), all training data should also be used during prediction.  2, I appreciate the toy example part that demonstrates inductive biases, which is illustrative. I believe that FNP is expected to give better performance. However, we will have no idea how general those behaviors are unless more visualizable examples are included (even in the appendix) to thoroughly demonstrate this. It will also help rule out the randomness due to the choice of data-generating mechanism.   3, Furthermore, I have a few questions for the first task: i), how is the NP trained? The behavior of this NP is somewhat weird. If it is pre-trained on samples from RBF-GP, it is expected to behave like a GP, as demonstrated in the original NP paper. ii), How does FNP compare to other BNN inference algorithms? MC-dropout can often be very inaccurate in practice, therefore including other VI/MCMC BNNs could be more meaningful.     4, Since the contribution mainly comes from the architecture side, readers normally expect a thorough comparison to more BDL baselines other than dropout BNNs. Also, Apart from BDL literature, how does FNP perform compare to variational GPs and Deep GPs? According to the paper, FNP also uses GP kernel to carry some GP behaviors. I understand that FNP can be a useful non-GP alternative, and its non-GP behavior could be useful. However, this potential power has not been well-explored in the paper. Then, why not use GP and/or its extensions instead?   5, It seems that most experiments in this paper lack multiple runs and significance analysis. It is difficult to confirm the significance of FNPs for sure unless the mean and error bars from multiple runs are reported. 