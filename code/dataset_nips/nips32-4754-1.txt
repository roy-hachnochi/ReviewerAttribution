Originality: I find the idea of using a surrogate objective instead of the expected reward very interesting and novel. the extension to the partially observable setting is interesting as the proposed form finds a common denominator to multiple estimators, but its underlying idea is not novel.  Clarity: the paper is overall very well written, and it has a nice flow. Although I am not very familiar with the topic, I certainly enjoyed reading this paper. The only comment is that it might be good to highlight more clearly the form of the convex surrogate.  Overall, this seems to me to be a good paper, and my only main concern is the experimental protocol used and the presentation of the results. Specifically: - in Sec 2.2 the decision to show only 100 epochs is arbitrary. I would prefer if you could show the learning curves instead - All results (Fig 1 and 2, and Table 1) should include std or equivalent - Visualizing Train and Test error with the same scale does not really make sense, either use two different scales or separate the plots. - The last sentence of Sec 2.2. is crucial to your finding. I would strongly encourage to replace Fig 1 and 2 with learning curves instead and run them until convergence. Then any difference in optimization landscape will be visible as a change in the integral of the curves.  - Similar comments about learning curves apply for Sec 3.6 - It is unclear to me if the reward estimation algorithm is actually evaluated in the experiments. Could you clarify? (it would be nice to include results) - Can you comment on the increased variance demonstrated by Composite on Table 2?  Additional comments: - Abstract. "Here we ..." sounds a bit strange since you already list other contributions. - Generally speaking, I find curious that in a paper talking about policy optimization all the experiments consists of classification tasks "reworked" to be decision making. I wonder if it wouldn't be more interesting to use other decision-making benchmarks. - It might also be valuable in Sec 3.6 to run experiments with different data size and distributions other than uniform. - The code was available only for the CIFAR-10 experiments, and not for the MNIST  --- Based on the rebuttal, I am increasing my score. The learning curves in the rebuttal might benefit from using log-axis.