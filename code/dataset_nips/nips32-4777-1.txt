This paper reconsidered online learning and bandits from the stability viewpoint by means of differential privacy tools. Though differential privacy is a concept about data security, its main idea is about the output stability, which itself is an important topic in many learning problems including online learning.   Back to this paper, authors defined two types of stability based on original \delta approximation max divergence, which were then used to derive first order bounds under full information and partial information feedback respectively. In detail, for online smooth generalized linear loss function, authors obtained first order bound through the first type of stability. As a special case, first order bound for OLO is new. Besides, authors found a close relation between stability defined here with differential consistency, which is also a key concept in the analysis of online learning and bandits. For MAB, authors re-derived some zero order and first order bounds via a unified analysis. Further, a new perturbation based algorithm was proposed to achieve first order bound for bandits with expert advice. Though its theoretical guarantee is not optimal, the algorithm looks interesting and computationally efficient compared with previous optimal but sophisticated algorithm MYGA.  The paper is well-written. The idea and analysis in this paper are new, and provide a new approach to analyze general online learning and bandits. Besides, considering some new results are obtained, I tend to accept this paper. However, in my personal view, results in this paper are not so strong in the following sense:    1. First order bounds under full information feedback (mainly Theorem 3.2 and Corollary 3.3) depend on dimension d. Dependency over dimension is a notorious phenomenon in differential privacy literature, so it doesn’t look so surprising that most results in this paper depend on dimension d. However, since first order bound in [1] (see reference below) doesn’t have a such dependency, and we do not need to protect privacy here, I wonder whether it is possible to get rid of the dimensional dependency here;  2. Algorithm 1 is actually computationally inefficient, since we have to solve an ERM at each round, is it possible to provide an elegant update (for example, like perturbed OGD) here? 3. In partial information setting, since we can only calculate the probability p_t approximately via Geometric Resampling, it would be better if authors could derive results under the consideration of approximation error.  Overall, I tend to a weak accept about this paper.  