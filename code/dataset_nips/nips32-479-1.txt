I agree the motivation of "learn, imagine and create". The whole model is intuitively simple and easy to implement. But I have a few concerns. First, since the Text-image encoder directly embeds the sentence, how can the model be able to control each word as presented in the experiments? Second, as far as I know, there are so a lot of works on text-image generation.  Finally, the useage of the concept "prior knowledge" is very inappropriate. The prior knowledge ofter refers to the symbolic knowledge which can be interpretable and explained the reason. But this paper only named the text-image encoded feature as the prior knowledge, which is overclaimed. And the claims about "mimicing human imagination" is also not good as it is just a fusion strategy. If we think this paper as a practical model , it is fine. But the authors used too much overclaimed phrases to try to highlight some contributions which are not true.This paper only compares with AttGAN and lacks of enough comparison with recent works. Finally, the ablation studies to validate each key component  is missing and not enough to support the contribution of this simple model. 