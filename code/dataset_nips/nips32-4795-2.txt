The paper is well written, the problem is well motivated, and references are sufficient. I don't see any major problems.  Lines 47-54 present important related work. Did the authors try to compare their results with the results of attention models? My understanding is that attention models would provide regions that the classifier is looking at. Would those methods provide more accurate, or similar, or the same regions? I understand that the main point of ProtoPNet is to identify prototypical cases, but perhaps the regions identified by the attention models would be equally good? Also, what is the problem with the existing attention methods that would not allow one to extract prototypical cases? Is there anything wrong with those architectures that would not allow for that? It would be useful if the authors could explain that.  What is the impact of m_k (the number of prototypes for each class k)? Did the authors try smaller/larger values of these parameters? How was the current value of 10 determined? Humans probably look at fewer than 10 prototypical regions in their identification tasks.  How important was the L^2 distance in this algorithm? Did the authors try any other metrics? How was L^2 selected?