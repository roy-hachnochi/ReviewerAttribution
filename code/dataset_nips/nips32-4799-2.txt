Authors properly justify that for image captioning several attention steps (in the decoder) is reasonable. Also fixing the number of attention steps as in recurrent attention modules does not yield the best results. Their target task is image captioning. The model architecture that they use for encoding image is a standard Faster-RCNN pre-trained on ImageNet and Visual Genome. For the decoder they  use a attention-based LSTM model.  They augment the attention module by outputting a confidence score (through an MLP on hidden state) at each step and halting the recurrent attention as soon as the confidence score drops bellow threshold. They use similar loss as ACT (graves, 2016) to encourage model toward fewer steps. In the end by allowing their model to take between 0-4 attention steps, they have an average 2.2 steps, while getting better performance in compare to a recurrent attention baseline with 2, 4 or 8 steps (fixed). Their ablation study is helpful as it clarifies the effect of the loss (scst vs ce), number of attention steps, and the lambda factor for the act loss.   *********************************************************************** Thank you for answering the comments. I still believe this is a grounded and strong work and I will keep my score at 7.