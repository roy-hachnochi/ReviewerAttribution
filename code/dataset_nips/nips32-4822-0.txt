In a nutshell, the GMN architecture aims at: * any-modal to any-modal conditional inference, regardless of any modality configurations present during training * handling partial observability in each modality and achieves these goals by constructing a multimodal, abstract scene representation in to stages:  1) observations are deterministically embedded in an input-modality specific way 2) embedding are fed (alongside a one hot indicator of the input modality) into a shared expert-amortizer (implemented with ConvDraw).  The changes in the scene encoder architecture, as well as the implications on the performance of the model compared to the prior art are very significant. Figure 2 is particularly striking: comparing the baseline model (CGQN) and GMN it's possible to appreciate how the fundamental change is moving DRAW before or after the aggregation step. In the baseline the observation encodings r_i are summed component-wise first and fed to DRAW to produce a distribution over scene latents z - it is up to the modality encoders (tipically low capacity resnets/convnets) to produce embeddings that, after being summed together, can be correctly interpreted by DRAW. In GMN, r_i are passed along side the id of their corresponding modalities to DRAW, which returns a set of distribution over encodings that can be easily aggregated via component-wise sum in close form.  Based solely on this description I would wonder about this tradeoff in representation power: in GMN more capacity is used to produce a representation that can be more easily aggregated, in CGQN more capacity is used to transform the aggregated signal into a distribution that could better capture multimodalities and uncertainty using all the inputs. Am I correct in this interpretation of the results? Could the authors comment on this in the text? The authors mentioned they have considered using a probabilistic observation encoder, but preferred using a deterministic one in their experiments; does this imply that they tried to merge the two approaches using a deeper, perhaps autoregressive observation encoder, but failed to train the model?   The experimental section is very thorough and thought provoking, providing benchmarks on both the model performance in observation prediction, and analysis of the scene representation. The supplementary materials provide lots of detail on implementation and training/testing regimes to facilitate reproducibility. Furthermore, in order to obtain multiple observation modalities the authors generate a new dataset with both vision and haptic data which will be opensourced - this is indeed great, but I wish they had also included as 3rd modality depth data (perhaps this could be provided to the model at a lower resolution and degraded with noise to make the task a bit harder).  A few presentation issues/questions:  Line 57: Why does aPoE improve computational efficiency? You still have to forward pass and back propagate to the network, even if the weights are shared. Can the authors clarify?  Line 60: At this point in the paper I could not follow the comments about 'modal-to-metamodal hierarchical structure': this part of the introduction is a bit confusing! Please consider commenting further to make the statement clearer (the paragraph at line 73 was helpful, but it was much later in the paper) and substantiate the claim pointing to experiments or relevant literature.  Figure 3 is visually crammed and not very readable - perhaps it's the section of the paper that took me the longest to parse. The author should consider redesigning the figure with readability in mind.  I found that Figure 5's caption is also very hard to parse. I found the final note on using moving averages vs not-moving averages is baffling. I think you shouldn't confuse the reader and burden them with the work of interpreting the curves when the underlying data is not consistently generated; or you should at least explain in detail what and why is unexpected. Nonetheless, the experiments in figure 5 are extremely interesting wrt to generalization considerations.  Line 299: in this paragraph the authors should also included the data regarding the baseline model (which is reported in the supplementary material) - it would make the paper even stronger, since the GMN is not significantly bigger that CGQN, but consistently outperforms it in the experiments.