         [Originality] The manuscript proposes a combination of GQN-style conditional latent-variable models with multi-modal generative models to be trained with variational inference. The combination is well motivated given the multi-modal nature of observations of a 3D scene, although the novelty is hard to fully specify since multi-modal generative models easily allow for conditioning through queries, but actually employing the GQN architecture is not entirely trivial either.         To perform inference, the authors propose an amortised PoE variational posterior that ties together some weights in the standard PoE construction due to parameter constraints of the ConvDraw encoder used (via GQN). This claim does not seem all that novel given the main PoE formulation of the posterior for multi-modal observations exists [1]. In fact, the derivations given (l165-l169) seem to be the same as those in eqns 3 and 4 in [1]. The shared weights, and potentially distinct priors per modality (l133-l135) seem to be the only additional elements.          [Quality]         The formulation appears to be sound---the multi-modal conditioning model seems right.         I do have one particular issue with regard to related work however---the claim in Sec 3, that previous studies have assumed full-modality configurations does not appear to be entirely true. In [1], sections 2.2 and 5.1 talk about how the PoE variational posterior can be used to deal with missing modalities just as discussed here. Unless I misunderstood the claim, this is a feature of PoE itself, and is not novel to this work or indeed [1]; just that it is an additional feature that PoE endows upon the setup.         On the experimental results front, the reported results look good, but, given that the APoE is the claimed addition over PoE (which has been previously derived in [1]), it is puzzling that the authors do not include a comparison to a standard PoE model---surely this would be necessary to tell if the gains seen in the results are because of PoE or the additional amortisation? This should have not been too much trouble given that [1] have released code [2] for their work.          [Clarity] The manuscript is well written, with good organisation.         The experimental details and exposition could be made a little less dense, but given space constraints, is understandable.          [Significance] The experimental setting is an apposite one, and timely given the recent interest in learning robust scene representations that have better utility on downstream tasks. The new environment looks interesting, and pending release, is a useful contribution to the community.         As described above, this work extends GQN-style methods with multi-modality, but it's value in extending PoE variational multi-modal inference is not clear.          Overall, I'm close to giving it a positive score, but for the fact that the experiments don't compare against an actual pre-existing and applicable model [1], instead comparing to a GQN baseline, and the relative work appears to mischaracterise prior work on their requirement for full-modality configurations.          [1] Multimodal Generative Models for Scalable Weakly-Supervised Learning, Wu and Goodman, NeurIPS 2018         [2] https://github.com/mhw32/multimodal-vae-public         **Update** I've read through the rebuttal, and I'm a little bit underwhelmed by it.  I find it difficult to agree that sharing some parameters between encoders in a PoE posterior counts as a 'contribution'---this is way too obvious a thing to do when the encoders themselves are hefty; even more so when considering their own response effectively casting this as purely an computational-efficiency change.  As for the argument that the PoE couldn't be run because of the practical issues, that was a predictable answer, but still something I would like to see a _concrete_ response for, not just a hand-wavy one! Going by Table 1 in the supplement, the authors could have compared for M=2,5 --- where both the number of parameters and iteration time appear comparable? For M=5, the #parameters stays mostly the same as M=2. Even for M=14, 131M parameters for the PoE suggest approximately 525Mb of memory (assuming floats) to store the model, which definitely is not so large as to be prohibitive---are there resource constraints limiting access to GPUs of 2GB, or 4GB even?  And for the iteration time, for M=14, it seems only double that of the APoE, which again does not seem prohibitively large given that most runs appear to only be 10 epochs? Now, I'm not saying I expect comparisons for M=14---that is a function of what hardware resources available---but lower M should have been possible because the APoE was runnable?  I'm happy with the clarifications on the abilities of Wu & Goodman---I hope the authors do make it very clear that the formulation and capabilities are derived from that work, and avoid confusion about claimed contributions (citations in the right place, etc.).  Apart from the above: I think the MESE data and environment is a clear contribution.  The multi-modal extension of GQN I'm unsure about as a contribution, but am willing to give the benefit of doubt to. However, my main worry with things is that the implications are being drawn, or the insights reported, against a baseline that is _not_ an apples-to-apples comparison. I'm not totally convinced that a PoE comparison is not feasible (see arguments above); at least based of the authors' response. I do believe some satisficing for the improvements I asked for, so I can upgrade my score a notch, but I'm worried the narrative is somewhat problematic as is. I would hope that the authors take the baseline comparison seriously and make the necessary edits to actually show that their conclusions are well-founded, doing the utmost to make sure that improvements claimed are purely a result of their actual contributions (APoE, not PoE). 