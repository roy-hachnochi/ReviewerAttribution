Dear Authors: I read your rebuttal.  I do indeed understand the point of your paper.  I also agree that solving linear equations is not a good example of something kernel methods can't do.    The 'isotonic regression' algorithm for learning a ReLU is a simple SGD algorithm that uses a straight-through estimator.  The high-level message of your paper is that there are problems that gradient-based methods can solve, but kernel methods cannot.  Learning a ReLU is a fine example.  Your paper has many other interesting contributions such as working with vanilla SGD and also formally ruling out a variety of kernels.  That's very nice, and I continue to recommend acceptance.   The construction in this paper seems interesting and noteworthy.  It is unfortunate that high level description of the lower bound is left until supplemental.  Also, the paper leaves out many relevant works on this topic.   I do not understand this submission's description of "Regularization Matters: Generalization and Optimization of Neural Nets vs their Induced Kernel." [37] does give a separation between neural nets and NTK (the submission here is better in the sense that they get a separation for all correlational kernels).  Another recent paper by Yehudai and Shamir shows that a ReLU cannot be learned by random features.  This implies that a ReLU cannot be learned by NTK, recursive kernel, etc.  But a ReLU can be learned for all distributions by gradient descent on a surrogate (convex) loss see "Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression."  So ReLU already seems to be a separation for many classes of kernels (though it is not proved for all correlational kernels).  Building on this work "Learning Neural Networks with Two Nonlinear Layers" there is an algorithm that shows how to learn neural networks that cannot be embedded into known kernels (NTK, recursive) because they can learn ReLU, so again this goes beyond current kernel methods. 