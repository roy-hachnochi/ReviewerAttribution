The paper presents a novel idea, is overall clearly written, and presents an interesting contribution. However the paper falls short in the following aspects which need to be addressed before publishing. Figure 2 shows experiments where the reward function changes every 20 trails. According to my reading, the “Single SR” baseline experiment is almost identical to the transfer experiments presented by Lehnert et al 2017 (https://arxiv.org/pdf/1708.00102.pdf). Why does the performance of at least the “Single SR” baseline not degrade right after a signaled reward change? Rather than reporting cumulative steps, the results would be much clearer by reporting per episode steps or per episode returns and comparing the actual convergence rates of all tested algorithm.   When making these performance comparisons, which learning rates where tested? Was a grid search pass performed? Performing a gridsearch pass over a range of learning rates, exploration settings, etc. is important for ensuring reliability of the presented results. This should be done at least for the tabular experiments.  Further, it would help to benchmark/compare simulations that do not use the CR-map rewards and analyze how exploration degrades/changes. I think the idea of convolving the reward map is interesting, but according to my reading the paper does not empirically support why this is necessary.  Other related papers studying the dependency of the SR on a policy at transfer are https://www.nature.com/articles/s41562-017-0180-8 and https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005768. These two papers should also be discussed, as the submission attempts to improve over these previous findings. 