Detailed comments:  Contribution 1 [Significance: High]: New approach for learning and generating multi-field documents i.e. documents where not only a "body" of text is to be generated, but which also contain other fields such as domain, date, authors and headline. The approach is a language modelling approach where these fields are used as context. The approach consists in ordering the fields into two subsets during the training in an attempt to address issues such as expensive marginalization or dealing with a large number of field orderings during inference time. The approach seems novel to me as I am not personally aware of previous work that approaches this problem in this way.  Question: It wasn't clear to me if some of the fields have some restricted domains from which they can be sampled. For example, for the authors name, are these being sampled from the general vocabulary or some subset of it that focuses on names? Because it's not evident how the model will abstain from, say, sampling something evidently wrong (e.g. the word chair or bed) for a person's name. If I'm missing something, this needs some clarification.   Contribution 2 [Significance: High] New dataset: The authors present a new dataset RealNews which was crawled from Common Crawl. The dataset should be a valuable resource for future research (on generation and classification) around the news domain given its scale and its inclusion of the accompanying metadata.   Contribution 3 [Significance: Medium]: Extensive experimentation and evaluation: The authors present extensive experimentation along with many intuitions and observations.  Comment 1: One main concern I have is that an overall read of the paper might give the impression of blurring the distinction between the notions of real, fake, generated, human-written. In Section 5, the authors talk about the Verifier being used to classify “human-written” vs “machine-generated”. In Section 2, the Verifier’s “goal is to classify new stories as real or fake”. Indeed, not all machine-generated text is outright “bad” or fake. Ultimately, as NLP researchers, we want to research and design and build stronger generation systems that can be used, e.g., for summarization, translation, answering, etc. I found it disappointing that the paper did not touch on the aspect of classifying articles as “real” or “fake” in the sense of truthful versus not truthful given that the threats of these massive models is not that their output is “fake” in the sense of “machine-generated” as much as it is not truthful. As the authors point out and use as part of their data, many websites are already considered propaganda and so their content is likely to be not truthful or biased. And this is one dimension they could’ve touched upon in their evaluation.  Comment 2: Evaluation only touches on aspects related to body of text. However, the model is supposed to generate a full article and so I think the authors should have had any kind of evaluation for metadata (e.g. similarity between headline and body of text, how often the generated author names even make sense and are actual names of people, how often the dates are actually correct dates, etc).  Comment 3: While the paper makes the point that Grover is the best at detecting Grover generations, it would’ve been nice to see how Grover does on the generations of other models versus those corresponding models.  Contribution 4 [Significance: Low]: The ethical agenda: I think this aspect of the paper is very much appreciated and welcome and needed to foster more discussions around the ethical issues revolving around text generation especially in the era of these recent massive models. I have to admit that the intro led me to have high expectations and I found that this ethical agenda was kinda underwhelming. It's a part of the conclusion and doesn't provide new insight nor does it add much to the existing conversation. For example, using models that work as filters is something that is already being done (to a certain extent) by social media platforms like Facebook or Twitter. Similarly, a lot of the heat that OpenAI received following its decision to not release its full model focused on the fact that their models could be used as classification models that could deter the attacks   Clarity: Overall, the paper is written very well. It's well-structured and flows smoothly. Few parts were somewhat difficult to follow e.g. mapping Section 5.2 to Table 1 was confusing, making clear what the work is evaluating (discrepancy between end of Section 2 and Section 5 as highlighted above). Clarification 1: One main concern I have is that an overall read of the paper might give the impression of blurring the distinction between the notions of real, fake, generated, human-written. In Section 5, the authors talk about the Verifier being used to classify “human-written” vs “machine-generated”. In Section 2, the Verifier’s “goal is to classify new stories as real or fake”.   Clarification 2: What do you mean by “news domains” (Line 132).   Significance: The work is important both because of the problem domain it’s addressing (the spread of fabricated news using neural ML) and the analysis and observations it presents. It’s also important because it’s introducing a new resource that should foster more research on this topic (hopefully, the dataset would be given to any credible team of researchers as the authors do not specify their intention to publicly release the dataset, instead making it available through requests).    Typos: Figure 5 in the legend: +Grover-Base Generations*  Other comment: This is more of a philosophical comment. I understand authors of papers usually – and rightly so – want to have catchy titles for their paper or their models. In this case however, the authors via their paper –potentially to be published at our field’s flagship conference— would be serving to popularize a concept, namely “fake news”, that was used by the current US administration to not only shut down essentially any criticism of the administration’s policies or actions, but even more dangerously to attack entire groups of media outlets calling them “fake news” and enemy of the people. A quick Google Trends search shows that the expression “fake news” was barely used before 2016 and only spiked after the new administration came in, so that combination of these 2 words was not popular at all before that point in time. The authors already use in their paper alternative expressions such as “neural disinformation” which seems to me rather catchy, and they also use other synonyms for “fake” such as false (others that come to my mind include falsified or fabricated). As such, I encourage the authors to reconsider their choice of words both in their title and in the body of their paper.