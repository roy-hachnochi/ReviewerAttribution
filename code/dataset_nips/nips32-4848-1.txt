POS AUTHOR FEEDBACK I thank the authors for their feedback.  In particular, the comment around having used BERT in a multi-field setting I believe is an important addition. Unfortunately, the allowed space is not enough to explain how you used BERT for generation. Regarding the annotator guidelines however I am more concerned, as it seems that the questions did not ask about the content specifically, but about the trustworthiness of the source, of which the article is only a proxy. In my opinion this only augments the risk that you are doing style classification, where one source is main-stream media and another are media with strong political biases. To support the claim that GROVER is good in discriminating machine-generated content in general this should be benchmarked with other (generative) language models in all directions.    ===== ORIGINAL REVIEW ======  This paper proposes a modification of GPT2 in order to:  - guide the textual generation without the need of prompting. It does so by providing "contextual" information in the form of fields  - generate multi-field documents In practice, this boils down to generate the field sequentially and generate them left-to-right; plus a carefully tuned training mode that drops out fields randomly.  The author show that they are able to generate extremely credible news articles. One of the strongest argument of the paper is the ability of that model to generate more credible news-article than real (human-generated) fake news. They then benchmark different models to detect machine-generated text, concluding that the best model is the one that uses the representation learnt by the same model that generated the text. The paper concludes on some insights on what are the signals to detect machine-generated text and discusses ethical considerations on the release of NLG models, adding to an important discussion in the field  There are lots of things in this paper! It is obviously a very timely contribution to an important debate, it is already being discussed in NLG circles and will probably be topic of conversations in the corridors of the convention centre of Vancouver if accepted. The paper is very clearly written, (important) details of training are given in the Appendix and the authors make a good work of condensing many ideas into 8 pages.  None of the contributions however is very significant by itself. In particular, with respect to what seems to be presented as the strongest point of the paper:  - presentation of a new model: this is a bit over-sold, it is just a modification of GPT2  - "humans find these generations to be more trustworthy than human-written disinformation". This point (Fig 4, last two groups, 2.42 vs 2.19; lines 170-172) is indeed striking. However, it is hard to understand what this means without a complete description of what was asked to the turkers. Was "overall trustworthiness" the only wording given? If so, what exactly means "content sensibility" (I would have a hard time figuring out what this means without more information)? Also, it is not clear why the same trend does not hold for Human-News vs Machine-News. Nothing is said about how many annotators labeled each article and what the inter-annotator agreement was. As the trends in both groups are the same for Style and Overall, one interpretation (which is not discussed) is just that humans get fooled by the style more than by the content. This is, if a human would write a propaganda article in the style of (let's say) the NYTimes then its Overall score would go up. Propaganda websites might not do that because that is not "their" style and what their readers expect. While the generator is conditioned on the source, Fig 4 could well indicate that the style of reputable news article leaks towards propaganda web-sites  - Grover generates better news article than GPT2 (Fig 3). Isn't that obvious? Grover is trained on in-domain data, while GPT2 is not. Also, it seems obvious that the meta-data (in particular headline and domain, but also authors) helps to reduce perplexity. This is a good sanity check, but not really impressive.  - Grover is better to detect Grover-generated text (Sect 5.2). This seems to say less about Grover itself than about "it takes a thief to catch a thief". This is, I would expect a GPT2 model trained on RealNews to be better to detect generated text with that model than Grover trained on out-of-domain multi-field documents. This conclusion is pointed out in the paper, but not tested beyond Grover. I believe a paper which starts from this as main idea would be a very strong submission.   Finally, I believe the choice of sources (Appendix B) is at least controversial. I understand that it is hard to find a good training data for "fake news", but currently the "propaganda sites" mix "web-sites that spread misinformation" and those with "strong political affiliations". For those last one, I am not an expert in US politics, but it seems to be rather "strong political affiliations, *on the right* ". Why are there no extreme-left websites?  More-over, this mix of obvious fake-news with other more fringe-news creates biases (both at training and at human evaluation) which are not sufficiently discussed in my opinion. 