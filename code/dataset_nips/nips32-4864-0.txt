The paper proposes a variance reduction optimization method for stochastic composite optimization problem where the objective function is the composition of a smooth function and an expectation over some random variables. In particular, this problem formulation covers the standard empirical risk minimization and other more advanced target function like risk-averse/robust optimization involving the variance. This problem is very challenging because unbiased gradient estimator is no longer accessible due to the composition of expectation. Therefore one needs to carefully use the biased gradient.  The proposed algorithm is based on recursive variance reduction technique in the spirit of SARAH/SPIDER. Extensive convergence analysis are provided under different scenarios. Overall, the paper is very clear and easy to follow, I am quite positive of the paper even though I have some minor concerns. Here are my detailed comments.   The main contribution of the paper is to carefully address the biased gradient issue. In the current presentation, it is unclear (at least in the main text) how this is handled.  After a quick check on the supplementary material, it seems to me that the way to handle it is to perform an inexact stochastic methods where we are suffering from some additive noise on the gradients. If this is the case, then we could apply standard analysis with inexact gradients. It is possible that I have missed some intrinsic difficulty, please comment on it and provide some intuition how the biased gradient estimator is handled.    In typical non-convex optimization, the step size of the algorithm needs to be small , which usually depends on \epsilon, like in SPIDER. It does not seem to be the case here, please comment on it.   ===Edit after rebuttal === I thank the authors for clarifying my concerns and I raise my score by 1 accordingly. 