*** Post rebuttal  I have read the rebuttal and the other reviews. The author addressed my concerns, thus I keep my score unchanged.  ****  The paper presents a generalization of the variance reduction technique for solving the stochastic composite optimization problem of the form f( E[g(x, xi)] ) + r(x). Although not classical in stochastic optimization, this model can be seen for portfolio optimization with risk aversion or reinforcement learning.  The core of the algorithm consists of the stochastic approximation of the derivative of the second term. Shortly, the technique maintains two sequences, y_i^t and z_i^t, that estimate g(x_i^t) and it's derivative using a variance reduction technique. The descent direction is thus computed using the formula [z_i^t]^T f(y_i^t), which is a biased estimate of the gradient of the function. The algorithm is then restarted after every epoch (or less, depending on the regularity of the function). The whole algorithm can also be restarted again if the function is strongly convex or gradient-dominated, and thus can potentially have a linear rate of convergence on those problems.  It is the first time I see such setting in stochastic optimization, but the motivation paragraph did convince me that studying this case is impactful enough for the machine learning community, even if it looks a bit narrow.  Globally, the paper is well written. The authors put some effort to present the results clearly, which is appreciated. I suggest to the author to write somewhere a table showing the different settings and their associated parameters for Algorithm 1.  Moreover, the contribution of the paper seems strong. The authors show that in many studied cases, they recover the right rate of convergence, but their analysis is also valid for new settings.  Overall, this is a good paper, and I recommend acceptance.  I have a few questions/remark for the results presented in the paper. - In section 4.2, restart is required to ensure the fast convergence rate. It took me a bit of time to understand that because the parameters S/eta/T/tau/B seems independent of t. In the end, I finally realized that epsilon was changing over time. I suggest making this point more explicit in this section, for instance, by writing epsilon_t instead. - Again in section 4.2, I feel like the restart on top of a two-loop algorithm seems a bit complicated to implement in practice. Besides, I feel like it may be possible to avoid restart by using time-varying parameters S/eta/T/tau/B. D you this it is possible to prevent the restart strategy in that case? - For all the methods, especially for section 4.2, A sensitivity analysis for the parameter eta can be useful. In practice, the strong convexity constant or the gradient-dominated constant are not well specified. Thus this can slow down the algorithm. - For Katyusha and SVRG, there exists a loopless version that makes the theoretical analysis more straightforward, and this also improves empirical results. (See this paper on arxiv: Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop, Dmitry Kovalev, Samuel Horvath, Peter Richtarik). In your case, I think this may help to simplify the algorithm as well as its theoretical analysis. As far as I understand this paper, it consists of changing the deterministic inner-loop into a stochastic one. To you think this approach can be applied to your algorithm?