The presented routing mechanism differs from routing by agreement in that routing coefficients are not iteratively determined by evaluating agreement of votes, but by computing self-attention scores and binary routing decisions for each combination of input and output capsules. The combined routing procedure seems to be novel, while the individual parts are inspired by self-attention in the transformer and Gumbel-softmax decisions as used in discrete domains like text processing.  The paper is technically sound and is very well written, precisely explaining the method and architecture. I feel confident that I could reproduce the method given the provided information.  The paper achieves its goal in providing a more efficient substitute for routing by agreement, which allows the architecture to be applied on real-world datasets like ImageNet, as is shown by the experiments.  For me, the biggest issue is that I am not fully convinced that the approach still provides similar advantages and properties as capsule networks with routing by agreement. Routing by agreement introduces a very strong inductive bias, assigning high routing coefficients only to connections that are supported by a lot of other connections. In fact, routing by agreement can be interpreted as robust pose estimation, finding the most plausible model and filtering out outliers in supporting evidence through fixed-function, iterative re-weighting. We contribute robustness to changes in pose to this procedure. In the proposed approach, this fixed procedure is eliminated, as routing coefficients only depend on the individual pre-votes and not on the joint behavior of input capsules. The question I am asking myself is: Are these still capsule networks (in the sense of Sabour et al. with routing by agreement), or are these "just" CNNs with multi-head attention?  In any case, the experimental results support the effectiveness of the approach when it comes to classification accuracy. However, if it is the case that the properties of capsule networks are not preserved and the method does also not beat other CNN architectures in terms of classification accuracy, I think the results are not significant enough for the paper to be accepted. I therefore ask the authors to provide additional evidence and/or arguments that indicate that the positive properties of capsule properties are preserved by the proposed relaxation (see below).  ---- Update: I thank the authors for the extensive response, which addresses most of my issues. I increased my score to 6 since the authors provided evidence for the preservation of capsule network properties on smallNORBS and affNIST. Also, they provided a large ablation study, which is able to compensate weaknesses in motivation by empirical evaluation and brings the work to a level on which it can be accepted.   However, at least for me, it is still not clear why exactly the proposed approach works as good as it does. I encourage the authors to further work on the motivational background to improve the takeaway of the reader. A thought: P is called a pose but it is modified by element-wise multiplication instead of linear transformations, thus, ignoring the underlying space of geometric poses and treating it simply as a set of scalar features. This means that the output poses of a layer are able to behave arbitrarily when the input poses change consistently. The network may be able to learn consistent behavior (is it? That seems hard to grasp). However, the forced inductive bias is gone. I find it surprising that it seems to be not necessary for the good generalization to novel viewpoints. 