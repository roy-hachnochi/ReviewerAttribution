The paper studies the following problem: If we consider a base space that admits a transitive action, and if the feature maps in neural network layers operating on this space are fields, then what is the most general way to write equivariant linear maps between the layers? The first contribution of the paper is to state and prove a theorem that says that such a linear map is a cross-correlation/convolution with a specially constrained kernel, which is called an equivariant kernel. The proof follows in a very straightforward manner from the application of MacKay's theorem aka Frobenius reciprocity, which essentially describes how induction and restriction interact with one another. Turns out that this is precisely the language needed to describe the equivariant networks talked about in this paper (and implicitly in many experimental papers). The proof is elegant and natural, and no details are omitted.   Next, in a somewhat abstract manner it is also describes how such constrained kernels will look like. This to me personally is the most useful, as for practitioners, it gives a systematic procedure to derive the right kernel for convolution. This is also useful in different ways -- for example there has been recent work that posits that for continuous groups it is perhaps useful to always operate in Fourier space. To enforce locality we then need an appropriate notion of wavelets. The two approaches are equivalent, but I find the approach presented in the paper more transparent vis a vis jointly enorcing locality and equivariance. Appropriate equivariant non-linearities are also described.   Lastly useful examples are given re spherical CNNs,  SE(3) steerable CNNs that do a good job in making the discussion a bit more concrete (although still in the abstract space :)