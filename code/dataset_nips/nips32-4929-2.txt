UPDATE: I thank the authors for their feedback, which I have read. I am not inclined to alter my score from an 8, but once again emphasize that this paper is a good one and I hope to see it published.  ----------------------------------------  Thanks to the authors for the hard work on this paper.   ==Originality== The work is original in that it is the first rigorous study of the MetaKaggle dataset as pertaining to the problem of adaptive overfitting. As the authors point out, some prior work has been done but only on a few of the most popular image datasets.  ==Quality== The work is of high quality, and the experiments are well designed. It is unfortunate that the authors did not also perform the analyses on the regression problems, and I hope they intend to publish follow-up work.   It was not clear to me why the authors conclude that only the accuracy metric has "enough" data for statistical measures. The reasoning in lines 275-278 is insufficient. Please be more concrete here. What notion of "enough data" are you using?   ==Clarity== The paper is clear and well-written.  ==Significance== In general, I think this paper will be of high significance as it begins to cast doubt on a common fear (adaptive overfitting). However, one point that kept coming up is that some of the datasets were "outliers" because their public and private test sets were not IID. It is true that this paper is entirely about adaptive overfitting in the IID case, so it makes sense to somewhat put aside non-IID cases, but in my personal experience adaptive overfitting is particularly problematic when the data is not quite IID, but close.   For example, competition 3641 has 7 subjects, and the public/private split is done on subject. This is actually reasonable when considering how most ML algorithms are actually applied in the field. I.e., this non-IID split is appropriate and better serves practical algorithm development than an IID split would have. An IID split would have given an artificially high generalization estimate for what the trained model would be able to achieve "in the field".  So, it is of high interest to also investigate how adaptive overfitting works in the non-IID (but realistic) cases, but in this work the non-IID "outlier" competitions are effectively put to the side. I would love to see an additional discussion (even in the appendix) on what conclusions can be drawn from this work about adaptive overfitting in non-IID cases.