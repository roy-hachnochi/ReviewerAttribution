Originality: As far as I know, such a sequential teaching model (where the preference function generally depends on both the learner's version space and current hypothesis) has never been considered in the literature.  I feel it is an elegant idea; moreover, the fact that the general parameter \Sigma-TD_{\chi,\mathcal{H},h_0} reduces to the RTD/PBTD and NCTD for different families of preference functions is evidence, I think, that this paradigm is a natural generalisation of previously studied teaching models.    The proofs combine combinatorial insights with known ideas in computational learning theory.  The proof of the main lemma (Lemma 4), for example, defines a family of sets H_{x_i}^{y_i} that are very similar to the "reduction" C^{x} of a concept class C w.r.t. some instance x (see, for example, Section 6.1 and Theorem 9 in "Sample compression, learnability and the Vapnik-Chervonenkis dimension" by Sally Floyd and Manfred Warmuth, Machine Learning 21, 269--304, 1995).  The fact that the VC-dimension of H^1 w.r.t. \Psi_H is at most d-1 follows almost immediately from the proof of the fact that the reduction C^{x} has VC-dimension at most d-1, where d = VCD(C) (Ibid.).  As I see it, a new insight of the author(s) is to take a "compact-distinguishable" subset of the instance space in order to ensure that each set H^i is nonempty; another new insight is to recursively apply this construction so as to obtain a partition of the original concept class (using the fact that at each step of the recursion, the VCD of the next partition is strictly reduced).  Quality: In general, I think the paper is technically sound.  There are a few small issues regarding notation and definitions (please see "Minor Comments" below for details), all of which can probably be quickly fixed.  All the claims in the paper are substantiated with complete proofs; I checked the main proofs in the appendix and they appear to be generally correct.     Clarity: I could generally follow the definitions, theorems and main arguments in the proofs.  I was a bit confused by certain aspects of Algorithm 2 at first, but after working through it, using the Warmuth class as an example, I think (at least I hope) I understand it better.  Please refer to the comment/question below regarding Algorithm 2.  There seem to be quite a few typos and grammatical/spelling errors (especially missing articles) throughout the paper, particularly in the supplementary material.  These errors were not taken into account in the overall score; however, I strongly recommend that the whole paper, including the supplementary material, be thoroughly proofread again.   Significance: That the author(s) found a (sequential) teaching complexity measure that is linear in the VCD, and in doing so unifying previously studied teaching models within a general paradigm, is in my opinion significant.  It establishes a firm connection between the "hardness" of teaching and the VCD of any given finite concept class, complementing earlier similar results on the relation between the RTD and VCD (the current best upper bound on the RTD in terms of the VCD is a quadratic function).  Reformulating the NCTD as a teaching parameter corresponding to a family of preference functions depending on the learner's version space might also open up a new approach to solving the open problem of whether the NCTD/RTD is linear in the VCD (though at this point I am not sure whether such an approach makes the problem any easier).   Comment/question regarding Algorithm 2:  After Lines 7 to 13 are executed for x_1,x_2,...,x_i in \Psi_H for some i < m-1, is the instance space X updated to X \setminus {x_1,...,x_i} ?  In Line 8, h \triangle x is used in the definition of the set assigned to H_x^y.  Based on the example of the Warmuth class, it seems that h \triangle x here denotes the hypothesis that only differs with h on the label of x with respect to the instance space X \setminus {x_1,...,x_i}, not with respect to the original instance space X.  If this is indeed the case, then it might be helpful to include an update of the instance space explicitly between Lines 7 or 13, or emphasise in Line 8 that h \triangle x denotes the hypothesis that differs from h only on x when the instance space is X \setminus {x_1,...,x_i}.   Minor Comments:  1. Page 1, line 17: Perhaps delete O(VCD) in this line, since it is already mentioned that the teaching complexity is linear in the VC dimension of the hypothesis class.   2. Page 3, lines 98 and 99: Perhaps write \mathcal{H}(z) as \mathcal{H}({z}) or mention that \mathcal{H}({z}) is written as \mathcal{H}(z) "for the sake of convenience".  3. Page 3, definition of U_{\sigma}(H,h,h^*): in the first condition, it looks like there is a type mismatch error; the left-hand side is a number but the right-hand side is a set.  Should it be \exists z[C_{\sigma}(H,h,z) = {h^*}] ?  4. Page 4, lines 121 and 122: Explain what the vertical bars surrounding argmin... denote, or perhaps remove them.   5. Page 4, definitions of \Sigma_{const}, \Sigma_{global} and \Sigma_{gvs}: Should the existential quantifier precede the universal quantifier in each of these definitions?  For example, in the definition of \Sigma_{const}, it seems that we need a single value c for all h', H and h.  6. Page 5, line 155: The term "teacher mapping" was not defined.  7. Page 5, line 164: The meaning of the notation VCD(\chi,\mathcal{H}), though perhaps not too hard to guess, could be mentioned in Section 2.   8. Page 5, line 159: Perhaps explain here why the Warmuth hypothesis class is interesting/significant, e.g. it is the smallest concept class for which RTD exceeds VCD.   9. Page 5, lines 176 and 179, definitions of \Sigma_{local} and \Sigma_{lvs}: Similar to point 5.  10. Page 6, line 196: Perhaps make clear whether or not H_{|X} is a class of sequences (since "pattern" seems to suggest that the order of the h(x_i)'s is used).  11. Page 6, line 201: "...does not contain any pair of [distinct] examples..."  12. Page 6, line 207: "...we can divide H [into]..."  13. Page 6, line 209: It looks like there should be another pair of braces surrounding {1-h_H(x_j)}.  14. Page 6, line 217: "...provide the detailed proof [in] the appendix" or replace "provide" by "defer".  15. Page 7, line 10 of Algorithm 2: I think the braces around x,y should be replaced with round brackets.   16. Page 8, lines 248-249: Missing article between "with" and "help".   17. Page 8, line 263: H_{j^*} -> H^{j^*} ?  18. Page 8, line 268: Missing article between "and" and "learner".  19. Page 8, line 280: Missing article between "have" and "win-stay, lose shift".  (Also, it looks like "loose" should be replaced by "lose"...)  20. Page 8, line 297: family -> families  21. Page 12, line 385: Superfluous comma after the first word "that".  22. Page 12, line 390: Delete "of set" ?  23. Page 12, lines 395 and 399, and also at a few other places: I suggest writing "consequently" rather than "subsequently".  24. Page 17, line 478: hypothesis -> hypotheses  25. Page 17, line 486, and also at a few other places: "...is contradiction with..." -> "...is in contradiction to.." or "...contradicts the assumption that..."  26. Page 18, lines 519 and 521: H_{x_1}^{1-y_1} -> H_{x_1}^{y_1} ?   27. Page 18, line 534: H^{m-1}_{{x_{m-1}}} = y_{m-1} -> H^{m-1}{|{x_{m-1}}} = {{y_{m-1}}}  ?  (Missing vertical bar before x_{m-1} and missing braces around y_{m-1} ?)  28. Page 18, line 540: examples -> example  29. Page 18, line 541: V_j -> V^j ?  30. Page 19, line 549: Explain what z is.  In the same line: "...there [do] not exist [distinct]..."  31. Page 19, line 557: Capitalise 's' in "since".  In the same line: drive -> derive ?  (I might have missed other typos or spelling/grammatical errors; please proofread again.)  * Response to author feedback Thank you very much for the detailed feedback.  I think this is a very good piece of work, and I will keep my current score.