The paper provides interesting techniques for reducing the number of gradient oracle calls in a Frank-Wolfe algorithm. Their main idea (in full-info) is dividing rounds into several subrounds and treating functions in each subround as one virtual function. Although this is a fairly known technique in online convex optimization (OCO), carrying it out in DR-submodular optimization seems nontrivial. Also, estimating a gradient in bandit feedback via querying on sphere is already known in OCO, but combining it with Frank-Wolfe requires some additional efforts.   The weaknesses of the paper are: - The resulting regret bounds are worse than the previous O(âˆšT) bound. This is due to subdividing rounds and shows a limitation of their techniques. - Analysis is considerably longer and messier compared to known online DR-submodular maximization algorithms [17, 18].  - Paragraphs are not well structured especially in Section 3: Technical descriptions go on and on without a high level picture.  Even though the present paper has several weaknesses, I believe that its technical contributions are strong enough to accept in NeurIPS: especially the first no-regret algorithm in bandit DR-submodular maximization is worth to be published.  ----------- update after author response ------------- The authors resolved some concerns raised by the other reviewers. I like the paper and vote for accept.