The paper extends the previous H. Cui et al. work [2,3,4] to POMDP problems. While the previous method factorizes the state space, the authors consider factoring over the representation of belief state. A symbolic computation graph of the value function is introduced and is used for optimizing the policy by gradient. The numerical experiments show better performance than the state-of-the-art baselines.  Strengths are as written in 1 Contributions.  Weaknesses: - There is no theoretical analysis. - The numerical evaluation might be limited since the tasks used in the experiments seems to be biased. While these tasks have a large number of states, they might have small impact on the technical assumptions, especially the factorization over the representation of belief state, compared to the classical POMDP tasks like the T-maze task [Bakker, 2002].  - Since the proposed method employs several heuristics, which are used in SOGBOFA [2], it is unclear which heuristics contributed significantly to performance in the numerical experiments.   Minor issue: - Line 96, p(x=1) should be p(x=T), p(i=T), or something? Also, the notation of T will be missing. At first, I misunderstood it as the time step T.   Bakker, B.: Reinforcement learning with long short-term memory. In: Advances in Neural Information Processing Systems, vol. 14. MIT Press (2002)   === Update after the rebuttal === I would like to thank the authors for submitting a response to the reviews and addressing my concerns.   The explanation of limitations of the proposed approach in the response is interesting and important. I would like to recommend to include the explanation in the main body of the final version. 