This paper proposes a new approximate POMDP planning algorithm with a few tricks: approximating beliefs using product distributions, sampling observations, and aggregate simulation. The approach enables action selection using gradient-based optimization. The proposed algorithm performs competitively against POMCP and DESPOT on three large IPC problems.  The ideas are interesting, and seems to work effectively on some large problems that current algorithms do not scale up. One main limitation is that the method has no guarantee of finding out the optimal policy even if the search time is unlimited though. While POMCP and DESPOT can find optimal policies given sufficient time and performs much better with some prior knowledge, the proposed approach sacrifices performance guanrantee, but seems to perform better without prior knowledge.  Does the approximations used in the algorithm have a big impact on the best possible solution found by the algorithm? Comparing the algorithm with other POMDP planning algorithms on smaller datasets can be helpful.  Minor comments - Eq. (2): b_{t}(s) should be b_{t}. - Line 88: why is a sequential plan p not a policy? - Line 96: not clear why p_{i} is p(x=1), and it is confusing to use p for different quantities (plan, probability).  * Update after the rebuttal The authors provided some examples to illustrate the limitations of the proposed algorithm. They stated that "DESPOT only runs with POMDPX". This is not true. DESPOT can work with a model implemented in C++ as well. So their additional result need to be updated to take this into account as well.