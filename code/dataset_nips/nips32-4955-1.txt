This was an overall nice and fun paper to read. The problem was well-motivated and the background well-covered. Possibly some important definitions were skipped (such as sequence form) but with space constraints I feel the authors chose the right level of abstraction for presentation (leaving the right amount of detail for the appendix).  One (fairly minor) concern is that the scope is a bit narrow. There is a small community working on these notions of equilibria, and addressing only the 2-player case without chance feels like a bit restrictive, given previous simple algorithms that solve the n-player setting such as Dudik & Gordon.  Another concern is that the algorithm is fairly complicated, more so than von Stengel and Forges, which is already fairly complex. Implementing this may be quite involved. If accepted, I would encourage the authors to publish source code; this may help promote the adoption of the benchmarks as well.   The benchmark games are indeed interesting. I wonder if there might be any n-player variants where an EFCE would demonstrate a similar pattern of sequential codes (even if they cannot be computed by this algorithm). Any thoughts?  Another few suggestions, questions, and comments:  - "Contributions: Our primary objective with this paper is to spark more interest in the community towards a deeper understanding of the behavioral and computational aspects of EFCE." This was a refreshing honest statement of the goals, thank you! It set a nice tone for the paper, which I felt delivered on this front.  - Is it possible to use a form of online convex optimization, as proposed in Zinkevich 2003 or one of the many in Hazan '16? I seem to recall several methods for projecting back into the feasible sets (ZInkevich uses L2 as well, so possibly the greedy algorithm or its adversarial variant GIGA could be applied here?)  - Along similar lines, there was work done on early Poker AI from Gilpin and Hoda and later Kroer, that proposed using Nesterov's excessive gap for two-player zero-sum games (a similar saddle-point formulation). In particular, they develop "treeplex" constraint sets (similar in structure to \chi_1 and \chi_2) and define prox functions that enable the smoothened gradient descent to be efficiently expressed. I wonder if these ideas could now be carried over to solve EFCEs given the reformulation.  - I wonder if it another possibility is  to use iterative learning, such as fictitious play (which has efficient sequential variants now, see Fictitious Self-Player of Heinrich, Lanctot and Silver  '15 and Heinrich & Silver '16, which essentially uses the sequence-form to compute the updates correctly and compactly)  - Finally, if approximate solutions are acceptable then it seems another potential route is open up that is even simpler: end-to-end learning with neural networks. The feasibility constraints could be softly implemented by additional L2 loss on the optimization criteria. Might guarantees still be possible with linear models? Even if not, the simplicity of this is very attractive, and there has been a lot of new ideas for training in this saddle point optimization from the adversarial training and generative models community over the last few years. 