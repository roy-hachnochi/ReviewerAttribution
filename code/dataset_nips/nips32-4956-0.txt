    originality: graph neural networks did not have a method for explainable predictions    significance: GNNs are widely used and explanations are important. Work shows existing NN explanation frameworks don't work well with graphs    quality:    The authors propose a novel method to explain the predictions of Graph Neural Networks - both for explaining single node predictions or predictions of a class of nodes. This is achieved by isolating multiple possible subgraphs relevant to the classification, whose edges are selected via mutual information maximization and variational inference. Use of masks allows for explanations to jointly include node features.    Convincing empirical results (except for how the hand-picked thresholding affects the results).    significance:    The paper is solid and make good arguments for the decisions made in the method.       some issues:    1. The proposal distribution for VI assumes independent edges in the subgraph, which is not ideal if one thinks that some explanations must jointly depend on multiple edges: say, either edges (a,b) and (b,c) or edges (a,d) and (d,c).     2. A threshold is used to remove low-weight edges, and identify the explanation subgraph G_S. It is unclear how the different explanation methods are affected by the choice of this arbitrary threshold. Moreover, there isn't justification for the chosen number of edges and nodes selected to justify the classification. (minor: Automatically discovering subgraph sizes would be a nice feature to have).    3. minor: The work does not provide an assessment of how well the mean field approximates the original objective.      Minor typos:  "is fixed, trained GNN" => " is fixed, a trained GNN"    