Originality: The paper is clearly linked to dictionary learning, unsupervised feature extraction, and also with the original LIME paper (where the interpretable features aka concepts are designed by hand).  Regardless, the paper seems completely original.  Quality: I really like the idea of providing explanations using higher level concepts, and also the fact that the authors did carry out experiments with human subjects.  The proposed method is very, very simple (it is basically segmentation + clustering w.r.t. the inception distance + TCAV afterwards), but this is not necessarily an issue.  I also really like Figure 6, which should be given more prominence, I think.  Still, I do not really like the desiderata, which feel ill-defined, inconsistent, and overall vacuous:    - Meaningfulness to me sounds like it is referring to concepts with a meaning---where both "concept" and "meaning" are not well defined.    - Coherency, or actually "perceptual similarity": it seems to be much lower level than meaningfulness.  For instance, cars definitely represent a meaningful concept, but they can be perceptually very different to each other. The same goes for most concrete concepts (person, chair, phone, ...).  After reading meaningfulness and coherency, I don't know if the authors would consider "zebra pattern" as a high-level concept or just a texture pattern.  These two desiderata don't seem to be helping my understanding.  I feel like the desiderata don't add much to the paper (if not confusion), and should probably be compressed to two / three inline sentences.  As things stand now, I feel like the desiderata detract from the presentation.  Also, the proposed method is very image-centric;  this is probably a liftable limitation.  The second thing that I don't like is that, at a higher level, the paper carries a very bad message: that debugging models using automatically extracted concepts would remove the need for human intervention (line 47).  The problem is that if the extracted concepts are not good (and segmenters *can* extract garbage, despite the experiments with human subjects presented here), then it is impossible for anybody to debug the learned model.  I would prefer if the message was about helping or augmenting human experts, not about replacing them.  It seems both harmful and irrealistic.   Clarity: The paper is reasonably well structured, but the presentation is not always clear.  For instance:  - lines 27-28 are unclear  - line 47: "ace **removes** the need to have humans look ...": way too strong;  please reword.  - line 61:   "meaningfulness should also [...]", I cannot parse this sentence.  - lines 108-131 are extremely dense and hard to parse;  please rewrite.  - line 122: "replaces of"  - line 150: "examined at" Most importantly, the description of the human pilot experiments are *very* hard to follow, and should be heavily revised.  Significance: The paper touches upon an very important topic and proposes a useful baseline solution approach.  I am confident that it would be of interest to other researchers.  Figure 6 is especially interesting, and I am sure it will garner a lot of attention.