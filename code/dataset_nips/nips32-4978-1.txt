Summary:  The paper introduces a SGD based learner for the discovery of questions in the form of General Value Functions (GVFs) [Sutton et al. 2011] as auxiliary tasks. A question network is trained to predict real values based on previous observations such that an answer network has to predict the discounted cumulative value of these real values. A gradient of a gradient (meta-gradient) is used to update the parameters of the question network to maximize the cumulative return of the policy that relies on the weights of the answer network.  Significance  In principle, auxiliary predictive tasks may be able to speed up training or improve the final policy quality when used in conjunction with a standard RL objective. In the reviewed paper, this has not been shown, and I am not confident that the presented results are extremely relevant. Furthermore, there are other issues in terms of clarity and empirical evaluation.    Clarity / Quality  The introduction lacks clarity.  While general value functions are central to the paper, their correspondence to predictive questions is not formally defined and the reader is required to be familiar with the concept.  The paper is poorly motivated. Why are such predictive questions important?  Some claims are not given evidence for or are imprecise, e.g. line 23, 'GVFs are very expressive' and line 37, 'Our second contribution is a precise and efficient algorithm'  Originality and missing related work  The author's second contribution of doing multiple meta-gradient steps has appeared in similar form in MAML [Finn et al. 2017] (and possibly even before that) and I would not consider it highly novel.  l 16-21 and 55-61: the authors mention recent work on auxiliary tasks, but not the original work on inventing explicit, well-defined auxiliary tasks or questions or problems, namely, the PowerPlay framework of 2011 [1,2]. PowerPlay automatically creates a sequence of auxiliary tasks/skills for Reinforcement Learning (RL), with a bias towards simple tasks, including action and prediction tasks, including predictions of internal states as consequences of action sequences, etc. PowerPlay can perceive and generate descriptions of arbitrary tasks with computable solutions. It learns one new computable task after another and uses environment-independent replay of behavioral traces (or functionally equivalent but more efficient approaches) to avoid forgetting previous skills.  Refs [2,3] have less strict variants of PowerPlay.   Of course, the early approaches to RL with curiosity since 1990 [4-6] also defined auxiliary tasks through intrinsic rewards, so that's even older, but back then the tasks were not clearly separated and formally defined through goal-defining extra inputs, like in PowerPlay [1,2]. One should probably point out this difference.   Given the above, the authors should clarify what's really novel in their approach. Here a list of relevant references from Schmidhuber's lab, with various authors:  [1] POWERPLAY: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem. Frontiers in Psychology, 2013. Based on arXiv:1112.5309v1 [cs.AI], 2011. (Very general framework for automatically defining a curriculum of incremental learning tasks.)  [2] First experiments with PowerPlay. Neural Networks, 41(0):130-136, Special Issue on Autonomous Learning, 2013.  [3] Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots. Artificial Intelligence, 2015. (Intrinsic motivation from raw pixel vision for a real robot.)  [4] A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. SAB'91, pages 222-227. MIT Press/Bradford Books, 1991. Based on [8].   [5] Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments. TR FKI-126-90, TU Munich, 1990.  http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf  [6] Curious model-building control systems. In Proc. International Joint Conference on Neural Networks, Singapore, volume 2, pages 1458-1463. IEEE, 1991.    Methodological / Evaluation  Why would one artificially constraint the setup to not backpropagate the error from the RL algorithm into the learned representation [where the learned representations are updated through the learned general value function]?   I agree that, as the authors stated, this does help to assess whether the auxiliary rewards can learn useful representations.  But it is unclear to me why these learned representations are useful at all if they don't ultimately lead to an algorithm that is more sample efficient or reaches better final cumulative reward compared to the base RL algorithm (here A2C).  Instead, transfer to other environments / agents would be interesting, i.e. how well the learned general value functions generalize.  Alternatively, the use of auxiliary tasks could speed up training / improve final cumulative reward in complex environments. This, however, was not shown in the present paper.  Furthermore, some baselines would have been interesting:  A linear policy network trained by A2C. This should be the lower bound for any method because it would correspond to an encoder that just passes the observations through.  Random encoder network (Instead of GVF system). Random projections of the observations might also aid training of a linear policy and should be outperformed by a learned system.  Figure 2 is missing the pixel control and value prediction baselines, although these appear in the legend.  For now, I vote for rejecting this submission, although I would not be upset if it were accepted, provided the comments above were addressed in a satisfactory way - let us wait for the rebuttal!  Response to rebuttal:  Provided the revised version (which we would like to see again) takes into account all comments as promised in the rebuttal, we increase our score from 4 to 6!  Here our thoughts on how the authors have addressed each of our concerns:  1. The authors promise to revise their related work section as requested.   2. "Why would one artificially constraint the setup to not backpropagate the error from the RL algorithm into the learned representation [where the learned representations are updated through the learned general value function]?"  This has been addressed with new results in the rebuttal l 1-10  3. "The paper is poorly motivated. Why are such predictive questions important? It is unclear why these learned representations are useful at all if they don't ultimately lead to an algorithm that is more sample efficient or reaches better final cumulative reward compared to the base RL algorithm (here A2C)."  The new results in figure 1 suggest that indeed the learned auxiliary losses are improving final cumulative rewards compared to the A2C baseline.  3. "In principle, auxiliary predictive tasks may be able to speed up training or improve the final policy quality when used in conjunction with a standard RL objective. In the reviewed paper, this has not been shown, and I am not confident that the presented results are extremely relevant."  See above  Conclusion:  These new results considerably boost the paper and address most of our concerns. While the paper could be further improved through multiple seeds, confidence intervals for all plots, and a wider range of environments and RL algorithms, the presented auxiliary task discovery in the form of GVFs now seems to be of adequate novelty with supporting results.    