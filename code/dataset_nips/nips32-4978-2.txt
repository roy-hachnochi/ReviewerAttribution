Originality:  As far as I am aware, using neural networks to define generalized value functions that are to be learned as an auxiliary task is a novel and interesting combination of existing ideas, and could be used in future work. More in general, the automatic adaptation of auxiliary tasks has been previously hinted at, but not explored in depth; in that regard too, the authors present work that is new to the field. I do feel, however, that the novelty of the meta-gradient computation procedure is being overstated, as it seems equivalent to [1]. Could the authors kindly explain the difference between the two algorithms?    Quality:  Overall the paper is technically sound (sections 2 and 3), though with a few small issues that I would like to authors to comment on. Line 94: the authors state that the backwards pass can be computed at a cost similar to that of the forward computation. While generally true, I find this statement a bit misleading. If my understanding is correct, this method computes several forward (and thus backward) passes per single meta-update, which requires significant computation. Line 285: the authors claim that the “variance of the meta-gradient estimates increases with the unroll length”. I’m not sure how this conclusion can be made from the graphs in the supplementary material, which only show that increasing the rollout length leads to an increased variance *of the returns*.  The proposed method sounds interesting and promising to me. However, I am not convinced by the experimental evaluation (this is my main criticism of the paper). The experiments demonstrate that the meta-learning procedure is capable of improving the usefulness of the auxiliary tasks when these are the only source of representation learning signal, but this scenario is somewhat contrived (usually the main task also contributes to representation learning). No experiments explored how this algorithm performs when gradients are allowed to flow from the main A2C task to the encoder. I wonder therefore whether it might be possible that the meta-learning procedure actually simply amounts to minimizing the A2C loss w.r.t. the encoder through a different (longer and partially implicit) computation graph. In that case the meta-learning procedure would not provide the encoder with any information that it could not receive from the A2C loss, were that being “directly” minimized w.r.t. the encoder. Then one could expect none of the meta-RL agents to perform better than the A2C baseline, which is what we see in the experimental results.   Clarity:  The quality of writing of the paper is good overall. In particular, the authors explain the proposed algorithm and the environments used to evaluate it thoroughly and with clarity. As a result, an expert reader with access to the supplementary material would be able to reproduce the results without significant obstacles. However, I found parts of the paper difficult to follow, since some key concepts such as generalized value functions were not clearly defined, and the review of related literature was somewhat scarce. For example, the section relevant to meta-gradients focused on only two specific examples from reinforcement learning, and did not consider work on meta-gradients beyond RL. I also found the terminology of “questions” and “answers” a bit confusing, as it is not commonly used in the multi-task literature.  Significance:  Auxiliary task selection and/or generation in asymmetric multi-task learning is an important yet relatively unexplored issue. Indeed, current practice is to design auxiliary tasks manually, and as such, work in this area has the potential of being highly significant. Unfortunately, in its current form the paper falls short of demonstrating that the proposed algorithm is a viable replacement for manual auxiliary task design. It can however be seen as an encouraging exploratory result and as the basis for interesting further investigation, which I warmly encourage the authors to pursue.  References:  [1] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pages 3981–3989, 2016.   ---------------------------------------------------------------------------------------------------- -- UPDATE --  I read the author's feedback and other reviews. I was happy to see that the authors addressed most of the concerns initially raised, and the new results look promising. I therefore increase the score of my review from 5 to 6.   I still have some remaining concerns which I hope the authors will consider for the next revision of their paper: - It seems like the main difference between the author's method and L2L is that they apply it to RL instead of supervised learning. Therefore the meta-update computation is not itself a novel contribution, which should be clarified in the paper. - I think that the most significant contribution made by the authors is showing that their method could be used to generate auxiliary tasks that are more helpful than hand-engineered ones. I believe that the authors should clearly focus on this aspect of their research. The new results should be confirmed by testing the method on more seeds and a broader range of environments.