Post author response: I have read the author's response and other reviews. I believe author's contribution in understanding architectural bias merits publication. My overall score remains the same.   Additional comment: is AlexNet on CIFAR-100 without data augmentation? Besides being harder than CIFAR-10, that may explain why 40% accuracy seems lower than other reported numbers on CIFAR-100. Authors should either clarify this fact; or even more interesting show effects don't change in the presence of data augmentation.  -------------------------------------------------------------------- Author’s study architectural bias of convolutional networks compared to the fully connected networks. For many interesting tasks, we’ve observed that CNN perform much better than FC networks. Although we understand this superior performance from CNNs incorporating prior knowledge of structured data, understanding of the training dynamics at the level of loss landscape is quite low. By mapping CNN to equivalent FCN(eFCN) and studying training dynamics of eFCN, the authors provide a new tool to investigate CNN architectural bias in loss landscape.  Although the practicality of proposed method is probably low (finding efficient way to relax locality constraint would be an interesting future work as suggested by the authors), the utility of the method is quite novel and and shed light on SGD training dynamics of different architectures.   The paper is very clearly written with proposed ideas and methods are easy to follow. The experimental results are presented in a logical and understandable manner.   Proposed method is simple to understand but yet quite novel, as far as I could tell, providing unknown insights of GD based training on CNNs and FCNs.   The paper provides significant novel insights through their new proposed method. Few interesting results observed 1) CNN initialization in the ambient FC space provide better model than FC init on original space 2) Some intermediate switch time after CNN init to ambient eFC space can find better model than full CNN model. 3) Sharpness indicators (gradient norm/max eigenvalue of Hessian) are large during that intermediate switch time and then becomes quite small trained in eFCN space.   In the supplementary material, the authors show that the findings also show on different architecture and dataset. Also they provide codes to reproduce results which will allow researchers to build on the findings to gain more insights in CNN architectural bias regards to FCN. 