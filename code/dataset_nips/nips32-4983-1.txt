The authors show how a CNN prior (local and sharing constraints on the weights) on FCN weights can find better local minima compared to FCN with no constraints. This  is suggesting that the architectural bias is only required in the initial part of the optimization to avoid trivial local minima which don't generalize better. One interesting observation is that even with an initial CNN prior FCN tends to performs quite well compared to regular FCN.  The experimental results also suggest that a combination of template matching and local filters tend to give better performance compared to only template matching or only local filters.   Many interesting insights presented based on experimental validation only.Perhaps either a more thorough experimental analysis or some theoretical evidence can be provided. If the authors claim achieving better performance by relaxing constraints at right point during training, it needs more experimental validation. 