Overall, the proposed method is original (it harness the linear modulation idea from the visual question answering field and the style transfer field) and efficient. The paper is well written and clear. The scope of the algorithm is large, making thus the proposed method useful and significant.  The algorithms of the MAML family (Finn, C., Abbeel, P., & Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. 2017) are designed to find model weights that are a good starting fine-tuning point for a task not seen during training. To achieve this, these algorithms rely on a set of different tasks, a task being a set of annotated data and an error criterion. The model will be trained in such a way that a few steps of fine-tuning on these different tasks yield the lowest possible error on these tasks. Because several tasks were seen during training, it provides an efficient initialization for fine-tuning on a new test set task. Those frameworks yield great performance on several family of tasks, such as few-shot learning or reinforcement learning.  MAML (and related) seeks a common initialization for all the tasks at hand, no matter how different they are. This paper stated that seeking a single initialization for an entire tasks distribution is limiting the achievable performance over this distribution and will thus prevent the algorithm to work on a diverse tasks distribution. Then, the paper proposes a new meta-learning method able to overcome this limitation, by having au auxiliary network modulating the initialization depending on the task mode and evaluates its effectiveness on several family of tasks. The proposed method is state-of-the-art on all the evaluated tasks.  The Introduction and the Related Works sections of the paper deal with meta-learning in general and its limitations. There is no missing major works in the bibliography. The Related Works and the Preliminaries are focused on the MAML algorithm, which is normal because the current algorithm is built upon MAML and is fairly different from the other kinds of meta-learning methods.  The method is well explained. Reading the supplementary materials may be required to understand the details of the modulation of the parameters. The FiLM modulation operation is taken from a paper in the visual questions answering field, but the field of style transfer has also used similar methods (AdaIN) to control the style of the output image based on the style of an input image. The figures are clear, but some of them (Figure 2 and 3) absolutely needs to be viewed in color.  The Experiments section follows the same architecture as the MAML experiments : regression, few-shot classification, reinforcement learning. The experiments are adapted for the method but do not look like hand-crafted for this particular algorithm. For instance, merging datasets (with the same labels) seems a reasonable strategy  to  creates a multi-modal tasks distribution.  The section is covering all the aspects of the conducted experiments, and many more details are available from the supplementary materials (such as the used networksâ€™ architectures or the training hyper-parameters). The comparison baselines are well chosen, even if the MAML method has gotten a few general improvements.  The source code is released along the paper, and is of great quality. 