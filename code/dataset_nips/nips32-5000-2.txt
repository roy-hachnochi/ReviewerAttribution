The submission is original and very clearly written.  The contribution is significant if one is a theoretician and okay with large iid Gaussian designs and iid ground truth "beta"  with known distribution "B". Indeed, the work is mathematically deep compared to typical NeurIPS papers.    The numerical simulations show that AMP is very fast relative to other typical algorithms, at least when the measurement matrix is sufficiently large and iid Gaussian, giving some practical motivation to the effort.  But, from a more practical perspective, there are serious limitations.  First, if the matrix "X" is non-iid or non-zero-mean, AMP may diverge, despite the fact that the optimization objective (1.2) is convex.  Second, to use AMP, we must translate the lambda parameters in (1.2) to alpha parameters somehow, and the solution proposed in (2.9) requires that the user knows the distribution of the ground truth "B".  Both are unlikely to occur in practice.  Now, if we accept the large iid X assumption and the AMP framework, then there are various other approaches that one could consider, and it would be good to understand how these compare to SLOPE.  For example, if one knows the distribution of the ground truth, then one could use the MMSE denoiser and presumably arrive at a much better solution than SLOPE (in the sense of MSE).  How much better would be interesting to know.  And if the distribution of the ground truth was unknown, one could still assume a parametric distribution and tune its parameters using EM- or SURE-based AMP strategies.  These two would be valid competitors of SLOPE in large iid Gaussian regime.  ====================== UPDATE AFTER REBUTTAL ======================  I thought that the authors did a good job preparing the rebuttal.  As they say, the main motivation of SLOPE is to perform variable selection while controlling FDR; the goal is not to minimize MSE. But my main concern, which still stands, has to do with the possibility for AMP to diverge in practice, in the case that the matrix is not drawn iid sub-Gaussian.  This certainly happens in medical imaging applications and it may very well happen in some applications of interest to the statistics community.  Demonstrating that it works for one application (e.g., GWAS) does not guarantee that it will work for all.  One small comment is that the two non-iid-Gaussian examples in the rebuttal seem to be in the class of iid-sub-Gaussian matrices (certainly the Bernoulli example is) where AMP is already proven to work.  So, those examples are not very convincing regarding universality.  