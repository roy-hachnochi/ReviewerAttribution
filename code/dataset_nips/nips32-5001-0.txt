After rebuttal:  Thanks the authors for addressing my concerns. I have read the authors feedback and other reviews. I'll keep my original score.  ------------------------ This work proposes an E3-style model-based algorithm for the finite horizon MDPs with known reward. The agent is able to collect data when running the algorithm and the goal is to find a near optimal policy. Early work includes [4], [21], and [39]. Using ranks of error matrices to represent complexity and some proof techniques are related to [18] and [41].  The paper is technically sound and clearly written. In the theoretical side, the authors prove a polynomial sample complexity bound in terms of |A|, H, and the rank of the model misfit matrix, thus avoiding the dependence on |S|. This work is also supported by the experiments. The empirical result shows that Neural E3 algorithm can achieve similar progress as standard algorithms.  Some comments:  1. In general, I like the idea of this work. The algorithm utilizes the uncertainty over predicted states instead of directly maximizing the expected return. In the exploration step, the algorithm finds a policy that leads to mismatch between two models. Therefore at least one model is not the true model. The algorithm will then collect enough trajectories of data to eliminate the inconsistent model.  2. This paper assumes that the true reward R* is known and it seems to be too strong. Predicting future states include predicting future rewards when the true reward is known. My major concern is that the idea of considering the mismatch between probability distribution between models only works under this assumption. I was wondering whether this work can be extended to the case when R* is unknown.  3. The sample complexity result is important. However, the analysis might not be novel since some are closely related to and adapted from [41].   4. The experiment further strengthens this work. Closely related work [18] and [41] do not provide practical instantiations. In some simple tasks, the proposed Neural E3 algorithm achieves similar results as baselines. However, the Neural E3 works only in the deterministic environment so there is still a gap between the theory and the experiment. Is it possible to extend the Neural E3 to stochastic environment?