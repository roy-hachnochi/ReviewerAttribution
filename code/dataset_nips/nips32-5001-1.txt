UPDATE after rebuttal ====== Thanks for the author response. I read the response and other reviews as well. In response, the authors explained the discrepancy between the idealized version and the practical version of algorithm. I marginally tend to accept this work.  Quality ====== The theoretical analysis is sound for the problem and the proposed algorithm. Under the realizability and optimal planning assumptions, the authors prove that algorithm 1 and algorithm 2 can find near-optimal policy in polynomial sample complexity with factored MDP.  The experiments show that the proposed practical algorithm (though may not exactly match the ideal algorithm analyzed) can overperform previous method.  Clarity ====== The organization of the lemmas and theorems is good, and the statement is clear, precise and relevant.  The model elimination part for the practical method is seemly not clear according to the paper. The authors tell us the methods to update model accuracy in practice. The model elimination stage seems missing in the practical algorithm (model update as in Sec.4.1 instead of model elimination?), which makes it unclear how the theoretical analysis could guide practical exercise.  Originality ====== The paper extends the classical Explicit Explore-Exploit(E^3) Algorithm to large or infinite state spaces. The use of maximum disagreement as uncertainty measure is novel for exploration. Although there is some literature (‘MAX’ and [Deepak2019]) also using disagreement among ensemble models for exploration, the methods for disagreement measurement in this paper is different. However, for large state space, the author uses the structural properties in factored MDP, which is well-studied. Also, it seems that the technical part of proof follows similarly as in [41].   [Deepak2019] Deepak Pathak, Dhiraj Gandhi, Abhinav Gupta. Self-Supervised Exploration via Disagreement. In ICML 2019.  Significance ====== The theoretical result is well enough to point out that the maximum disagreement as uncertainty measure is sample efficient for exploration and can led to find near-optimal policies in model-based RL. However, the bound seems not as sharp compared to the sample complexity of other model-based algorithms e.g. UCRL. On the other hand, the empirical results show that the proposed method has superoir performance than previous ones. However, my main concern is that there is still a gap between the theoretical algorithm and the practical implementation (especially in continuous control), which should not be neglected.  