The paper introduces a novel slice-based programming abstraction. It presents a method for training models given the slice information that is closely related to past work on ensembles, mixture-of-experts, and multi-task learning. The main advantage for the proposed method is that it limits the number of model parameters required; otherwise it is very similar to instantiating a mixture-of-exports model on top of shared backbone features (rather than the inputs directly). The slice-based abstraction introduced is highly applicable to practical applications of ML model, and has the potential to be widely used for ML deployments.  The presentation of the model architecture (Sec 3.2 and Figure 2) is not very clear, so I haven't been able to fully figure out the details of the approach. As I understand it at a high level, the architecture involves computing slice-specific representations, and then combines them into a single representation based on a module that predicts which slices are active. However, the text confused me more than it helped me understand the fine details of the approach. It might be helpful to refine the presentation, and also update Figure 2 to use distinct visual cues when presenting quantities such as attention logits, attention probabilities, and ground-truth slice membership labels as computed by the slicing functions. [Thank you for promising to clarify the presentation here; I look forward to seeing a revised version of this section. The author response points out the text assumed a binary classification setting where there was only one logit, instead of one per class. This wasn't clear to me so I kept expecting there to be vectors of class-specific logits instead.]  160: the letter "p" is often used for probability distributions, but is used for hidden vectors here. Switching to another letter would have reduced my confusion in reading this section. 165: is g(P) introduced here the same as in section (d) previously? But in that case, g(p_i) in R^c from the previous section doesn't dimension match with g(P) in R^(k+1) on line 165. Maybe g(P) is in R^(k+1 x c) instead, or perhaps I'm misunderstanding? 166: What does "abs" here refer to? It looks like you're reducing a set of logits to a scalar confidence score. My only thought is component-wise absolute value, but that can't be right. 173.5: I don't quite follow how the model makes its final prediction. Earlier in the paper the authors claim that their method "model[s] residuals between slice-level and the overall task" (56-57), whereas it seems here that the original backbone representation "z" is no longer used here (except as part of p_BASE earlier). Also, what is the motivation for doing the slice-based modeling using dimensionality "h", rather than the dimensionality of the backbone (r)? It sounds like the backbone features are being projected down to a lower-dimensional space; is there a concern that this will discard relevant features?  56: attenton -> attention Table 1: +/- -2.1 should be positive 2.1 