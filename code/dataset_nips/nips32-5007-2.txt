Update: I read authors' response and I am satisfied with it. I like the plot that they included, which shows that indicator becomes less certain the more noise is encountered. I would suggest to include it into paper's final version, space permitting. I also find architecture impact responce interesting, and again suggest to try to put it into the main paper. I keep my Accept rating  Summary: this paper operates in a setting when it is known that the model must perform well on a some subsets (not nessesarily not overlapping) of data. The authors formulate this problem as multiple objectives problem (lernt generic representation, learn slice representation, combine with self attention) and demonstrate in a number of experiments that the method is able to both improve performance on the slices and overall performance of the models  Detailed review: Overall an interesting paper with several good observations. I like the learnt "indicators", that account for noise in slices. I assume it also allows for the slices that developers "think " are important but might turn out to be not.  Concerns: - The architecture seems a bit of black magic. For example, are all parts required and crutial? Is it designed or "found" during extensive search for architectures? - The combined loss (sum of the sublosses) - why all the objectives are equally important (weight 1). Did you try to introduce hyperparams there that pay more attention, for example, to l_pred - what would a dummy baseline that introduces losses on each slices and then sums it with the normal weight (may be with some weights) learn? Basically it is a continuation to a question of which part is the most important - experiments: for figure1 - it seems with the current setup u are fitting to noise. What would be more indicative is to introduce also a slice that is noise but not critical (eg have 3 slices, 2 are included in your learning, 3rd was used to generate the data but not included - eg no slice indicator func) and see what it does on the third non critical slice - Is the idea that learnt indicator allows to understand what slice is noisy? Because how does a developer knows that the slice is important or it is just a more noisy segment of the data, for example where the measurements for the car are not as precise (night vs day speed detection)  Questions: - how many slices are you able to handle? From your experience, did you observe that it is beneficial to include all potential slices or it hinders the model - what happens when you don't learn the "indicators" and just use hard indicator functions (both during training and inference)? I would assume it would still improve on slices, but would be more susceptible to noise   Minor line 75 this procedure yields achieves -> choose one