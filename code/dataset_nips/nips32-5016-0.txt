 Variational Autoencoders (VAEs) are an effective approach to unsupervised learning but they suffer from a problem known as "posterior collapse". There has been a lot of focus on solving this problem from the ML research community. This paper belongs to that line of research.   Clarity: The paper is well written however there are few things that hinder clarity. For example having section 5 as is is misleading...it makes it seem as though the same theoretical conclusions made in previous sections also hold for deep VAEs. Furthermore, the discussion makes other misleading statements that were not verified in the draft; for example the statement "We demonstrate empirically that the same optimization issues play a role in deep non-linear VAEs" is misleading because the experiments did not directly show this.  Quality: The experiments conducted do not align well with the theoretical analysis in the earlier sections. The experiment section heavily focused on showcasing the performance of KL annealing. The related work section is also limited as there are tons of related work on posterior collapse in VAEs that were not referenced. See for example [1] and [2] below.  Significance: I find the paper of limited significance. The analysis for the linear case does not apply to the non-linear case and empirical evidence does not show that the conclusions from the linear case still hold in the nonlinear case. Because of this I think significance is lacking.   Minor Comments: 1--Throughout the paper: it's "log marginal likelihood" not "marginal log-likelihood" 2--What happens when the decoder is linear but the encoder is not? Does the analysis on linear decoders still hold?   [1] Tackling Over-Pruning in Variational Autoencoders. Yeung et al., 2017. [2] Avoiding Latent Variable Collapse with Generative Skip Models. Dieng et al., 2018. 