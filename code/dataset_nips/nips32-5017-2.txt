Thanks to the authors for the response - these new experiments certainly are a step towards better demonstrating the role of compositionality in this work. However, these experiments need elaboration and further analysis, especially since the formulation of the high-level policy is a new one. This changes the paper quite a bit and I feel would necessitate another round of evaluation. --------- This paper proposes to use instructions in natural language as a way of specifying subgoals in a hierarchical RL framework. The authors first train a low-level policy to follow instructions (using dense rewards) and a high-level policy to generate/choose instructions (using sparse environmental rewards). The goal is to enable better generalization of HRL algorithms and to solve the sparse reward problem.   I really like the idea of this paper - it is definitely novel and worth pursuing and the paper is written clearly. However, the execution is a bit lacking and the experiments do not clearly demonstrate the advantage of using compositional language, which is the main premise of the paper.  Re: compositionality:     While the paper's idea of using the compositional nature of language to improve HRL is definitely interesting, the experiments do not back up the claims.  First, the high-level policy only chooses from a fixed instruction subset and therefore does not learn or output anything compositional.     Second, the non-compositional baseline for the low-level policy doesn't make sense. Why should a lossless representation be non-compositional, especially with a sequence auto-encoder? For fair comparison with the language representation of subgoals, the auto-encoder representation should also be fed into a GRU and not used directly? Further, another baseline would be to simply have a bag-of-words representation for each instruction (note that this is different from having a one-hot representation for each instruction). From the current experiments, it is unclear that there is an advantage to using language as an intermediate representation (even though theoretically it makes perfect sense!).   Other comments: 1. Some of the experimental details are not clear. Do you assume access to the ground truth state of the world for the HIR procedure? If so, this should be made clear as an assumption. 2. Are the number of actions used for the high-level policy the same for your method (80) vs the other HRL baselines? From C.4, it looks like the goal space is much larger for the baselines.  3. (minor) How well does the model generalize to real instructions (with potential noise)? 4. Relevant work: a) Speaker-follower models for vision-and-language navigation  D Fried, R Hu, V Cirik, A Rohrbach… - Advances in Neural …, 2018 - papers.nips.cc b) Grounding language for transfer in deep reinforcement learning K Narasimhan, R Barzilay, T Jaakkola - Journal of Artificial Intelligence …, 2018 - jair.org c) Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan d) Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout Hao Tan, Licheng Yu, Mohit Bansal