*** FURTHER UPDATE: After discussion with the AC, I am happy to change my score to 5 as we agreed the novelty of this paper should be judged before the publication made by Cohen et al.  *** UPDATE: I thank the authors for their response.   I have just some further comments. The certified bound for L_infty=0.3 for MNIST shown in Figure 2 shows that it is approximately 70% accuracy? Whereas TRADES seems to be closer to 100% and Gowal et al is above 90% - it seems low compared to the numbers I am used to. This might be due to the bound being too loose.  I definitely agree that the goal of the adversary is to find an image where the difference is imperceptible to the human eye, however, when the perturbation radius is larger we should be less sure that **all** images within this space are imperceptible to the original.   To test gradient obfuscation just looking at black box attacks is often not enough. Here I would like to refer the authors to the paper: Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples - Section 3.1.  Again I thank the authors for the response, but given the concerns I still have I will keep my original score. *** Overall I found the paper slightly hard to read, my main comments are below:  -  The initial figure (Figure 1) has a mismatching caption and seems to be missing some curves that are referenced below ? It is unclear to me how to interpret the figure. The caption mentioned that there should be a pink line which shows TRADES, but I cannot find a pink line on the plot. The brown line and purple line I guess should be the green and orange line shown shown in the Figure? The difference between the caption and the figure itself makes the results presented very confusing.    On top of this the experimental section in general is hard to follow, weakening the paper. For example in Table 1 the authors list a result of  “1.58 (69.0%)” this is either suggesting a certified bound of 1.58 in an unclear metric (with corresponding accuracy of 69%) which would not be a very good result, or it suggests a robustness bound (measured in error) in which case the accuracy contradicts the bound.  - The results are not good compared to state of the art and it is unclear how the theoretical result can help to push the boundaries of current research going on in the field. For example: ‘To interpret, the results, for example on MNIST, when sigma=0.7, Algorithm 1 achieves at least 51% accuracy under any attack whose l2_norm size is smaller than 1.4.’ This seems to be well off the state of the art as Gowal et al: ‘On the effectiveness of Interval Bound Propagation for Training Verifiably Robust Models’ which has verified accuracy on MNIST 92% on l_infinity norm of 0.3.   - Figure 2 shows that TRADES gives better results for L infinity norm considered, more worryingly - STN does not seem to perform well when the epsilon is small (namely when the perturbation is most imperceptible) for L2 norm. The authors also made a comment on the fact that TRADES is expensive to train - TRADES used 10 FGSM steps to train the network, this means that it would be approximately 10X more expensive than standard ERM. The authors alluded to the fact that their method is cheaper, can the authors quantify how much cheaper it is in comparison to TRADES?   -The authors also talked about gradient masking - here the comments were made about what they did to combat gradient masking but there is no strong empirical evidence that the gradient masking is indeed not occurring. In order to do this, can the authors show the adversarial accuracy of the networks trained under a very strong attack - e.g. PGD 1000 steps and maybe 20 different random initializations of the initial perturbation.   Other comments:  - Can the authors comment if this theoretical result can be extended to other L_p norms? The authors alluded to the fact the theoretical lower bound is not tight in the conclusion, how would this change if we consider other norms?  - A side note, I would prefer the figures to have corresponding Tables in the supplementary, this allows us to see the exact adversarial accuracy achieved.    - A general note in talking about adversarial accuracy under PGD. The authors should always report the step size and the number of PGD steps performed as the adversarial accuracy is highly dependent upon these parameters. In fact a lot more details are needed to describe the attack: if you are using PGD, is the optimizer you are using just standard SGD or Momentum or Adam? To know how strong your results are you will need to specify these parameters so others can have an idea of how strong the attack is.