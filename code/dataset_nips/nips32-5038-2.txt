***EDITED REVIEW*** I thank the authors for addressing my concerns in the rebuttal.  I thank the authors for including a comparison to Cohen et al and I am surprised to see that their method outperforms Cohen et al. Cohen et al argue that it is not possible to surpass their bound with a randomized smoothing approach. If indeed stability training is that much more powerful than Gaussian data augmentation, I think the results are interesting to the adversarial community, since Gaussian data augmentation is still a very common baseline (e.g. https://arxiv.org/abs/1906.08988, https://arxiv.org/abs/1901.10513 and others). The authors acknowledge that Cohen et al improve their bound; therefore, the adversarial community would benefit most from a combination of Cohen's better bound and stability training. For this reason, the paper should be rewritten, since the main contribution stems from stability training. However, stability training itself is not a novel contribution as it was introduced in "Improving the robustness of deep neural networks via stability training" and it was already demonstrated in reference [32] of this paper that stability training works better than Gaussian noise for increasing performance on noise corruptions.  Due to the Discussion with another reviewer, I agree that the results on CIFAR-10, l_inf are not convincing and there are other methods that are faster than TRADES that achieve better results (e.g. https://arxiv.org/pdf/1811.09716).  I have changed my score to a 6, because I find it interesting that the results of Cohen et al are outperformed. I do think that the authors should rewrite the paper focussing more on the improvement due to stability training and maybe use Cohen's better bound.  ********   The authors derive and prove adversarial robustness bounds based on the Renyi divergence and utilizing stability training. They build upon the methods presented in Lecuyer et al [2018] and show higher robustness bounds. The paper is written in the context of randomization-based defenses where noise is added to inputs; if the inputs have been perturbed adversarially before that, the predictions of a classifier are smoothened by the added noise. Originality, Quality and Significance: My main comment is that the current state of the art in certified adversarial robustness, namely Cohen et al. [2019] was not discussed and not compared to. Cohen et al. use a very similar approach as they also add Gaussian noise to adversarial examples to “drown out” adversarial perturbations and even prove that it is impossible to certify higher l2-robustness with a radius larger than their derived R. In more detail, Cohen et al use Gaussian data augmentation during training, while this paper uses stability training where an additional stability term is included during training.  The stability term is the cross-entropy loss between f(x) and f(x’) where x is clean data and x’ is data with added Gaussian noise. The original publication on stability training [Zheng et al, 2016] reported that data augmentation with Gaussian noise leads to underfitting compared to their stability training approach. Another recent study demonstrates better model performance of stability training compared to data augmentation and also decreased sensitivity to hyperparameters [Laermann et al, 2019]. I would be interested to see whether this difference (stability training is better than data augmentation) leads to a better robustness bound compared to the approach using data augmentation. It would be crucial for the authors of this paper to compare to Cohen et al. as the approaches are very similar and it is not clear whether the approach of Cohen et al. can be superseded at all with another randomization-based method due to the tight bounds argument of Cohen et al. Additionally, Cohen et al. provide results for ImageNet and the authors should include them in their discussion/ experiments’ section. Instead, the authors compare their results to Wong et al. [2018] which is a baseline that was already beaten by Cohen et al. If the results from comparing to Cohen et al are worse, I would suggest rejecting this paper as the approaches are very similar. On the other hand, the authors achieve better performance than TRADES on l2, both for MNIST and CIFAR10 with a less time-consuming approach. Cohen et al have not compared their method with TRADES, so it is not clear which would perform better. In case this approach performs better than Cohen et al (and as already shown better than TRADES), the paper would be worth accepting. Clarity: The paper is well written and it is easy to read. The form of the presentation is therefore good and acceptable for NeurIPS. I have several minor comments. In Figure 1, the colors in the caption do not match the colors in the image. In Table 1, I could not find the cited results in Wong et al. [2018]. In Wong et al. [2018], in Table 4, the results look similar to the numbers in Table 1, but they are not exact. For example, in Table 1, Cifar-10, Single, the robust accuracy is displayed as 53% which would correspond to an error of 47%. This robust accuracy however does not occur in Table 4, Wong et al. [2018]. The natural accuracy seems off by 10% for CIFAR-10. The other numbers are also slightly off. Did the authors of this paper use the code of Wong et al. [2018] to rerun their simulations which naturally results in slightly different numbers? Also: Which model was used in this comparison as there are 3 different models both for CIFAR-10 and for MNIST in Wong et al. [2018], in Table 4. In Figure 2, the orange curve displaying the Natural case. Please state whether this is a vanilla trained model and how it was attacked. There are some typos in the text, but not many, such as eg. Page 3, line 110: ‘robustss’ instead of robustness. In the Appendix, there are a few typos in D1 such as in the sentence: ‘Since only PixelDP [18] is able to obtain non-trivial certified bound on ImageNet, we compare out bound to theirs’ -> 1) obtain a non-trivial… and 2) our bound.  References: Lecuyer, Mathias, et al. "On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning." arXiv:1802.03471 (2018). Cohen, Jeremy, et al. “Certified Adversarial Robustness via Randomized Smoothing”, arXiv:1902.02918 (2019). Wong, Eric et al.  „ Scaling provable adversarial defenses”, arXiv:1805.12514 [2018] Zheng, Stephan et al., “Improving the Robustness of Deep Neural Networks via Stability Training”, https://arxiv.org/pdf/1604.04326 [2016] Laermann, Jan et al. “Achieving Generalizable Robustness of Deep Neural Networks by Stability Training” https://arxiv.org/pdf/1906.00735 [2019] 