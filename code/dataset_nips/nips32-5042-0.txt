This work achieves an improved bound on the sample complexity of random tensor projection and it is argued that this bound is tight and nearly optimal. A key observation is to view the random sketch as a bilinear form of a random matrix. It makes the analysis suitable to apply general matrix concentration inequalities. The authors can obtain better bounds by analyzing both operator and Frobenius norm of the random matrix, which is the key challenges of this work. Their proof techniques are different from previous approaches but very impressive. It has a great impact in term of applying complex matrix concentration inequalities for exponentially improved bound. This work can give a good direction to challengeable analysis in many other works.   The paper is also well-written and its organization is comprehensive. Although the proof techniques can be complicated and require a large amount of prior knowledge, the contributions and proofs are straightforward to understand. However, readers not familiar with prior works of random tensor sketch may have some hardness since there is no kind preliminaries section. It would be better to provide some backgrounds with a concrete description including Johnson-Lindenstrauss transform, CountSketch, and TensorSketch.   In overall, I believe that the impact of this work is impressive and vote for acceptance.  Some minor issues or typos: - Please write the detail information of reference [28]. If the reference is not published yet, it would be better not to refer it. - ln equation between line 135 and 136, the notation of norm is not complete. - In line 191, the right parenthesis is missing. - In below the equation (2) (between line 155 and 156), S^i x^i should be vector while the right-hand side seems to be scalar.  ============================================================================== I read all reviews and author feedback. The final manuscript with authors response can improve the writing quality and highlight the novelty of this work.