Summary:  The paper presents a sketching method (a variant of the method given in Kar and Karnick, 2012) for tensors and provides its theoretical analysis. Specifically, the authors improve the previously known rate for the sketch-size n^q (exponential in data dimension n) to log^q(n).  Further, the authors combine CountSketch with the proposed method, which enjoys a better theoretical guarantee (the dependence on n is removed) while fast for rank-1 tensors (the computational cost given by the order of sketch size * nnz(x^i), x^i is a n-dim component vector of a q-th order tensor).  It is shown that the proposed method outperforms TensorSketch (Pham and Pagh, 2013) and RandomMaclaurin (Kar and Karnick, 2012) in approximating polynomial kernels. Also, the proposed method has a better test-error performance in compressing a neural network compared to Arora et al., 2018 with a small sketch size.   The analysis given by the authors is technically sound. Tensor sketching is an important task in machine learning, and the exponential improvement of the sketch size is useful particularly in high-dimensional settings such as NLP (e.g. a vocabulary set for text representation could be large). I also value the practicality of the proposed method using CountSketch as sketching itself could be computationally expensive. Together with the results in the experiments, I consider the contributions given by the paper significant.   Some clarifications that could make the paper more accessible:  * The scaling given in Kar and Karnick, 2012 is different from the proposed method due to the coefficient by Maclaurin. Should not this be clarified?  " Line For each sketch coordinate it randomly picks degree t with probability 2âˆ’t and computes degree-t Tensorized Random Projection." What does degree mean here?   Minor points:  * The statements regarding epsilon-distortion in Theorem 2.1, 2.2, and Lemma 2.3: should they not use equality? e.g. P(1-eps||x|| <= ||Sx|| <= 1+eps||x||) * The scaling 1/\sqrt{m} is missing in line 151 (S^i) and 168 (S_0). * Should the proof lemma clarity that it deals with the case where x^i is a unit vector? * Line 437: p-th entry -> i-th entry?  * Lemma 4.6 Inductiion index should be q instead of k (i.e. prove for q=2, and then q=k>2?) 