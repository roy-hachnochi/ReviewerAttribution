Comments after rebuttal and discussion ======================================  Thank you for clarifying my misunderstandings in the rebuttal.  I no longer have any major technical concerns and I have adjusted my score to reflect this.  It still strikes me as slightly odd that the proposed algorithm does not make use of any data of play, i.e., it isn't really inverse reinforcement learning.  Original Review ===============  Overall, I enjoyed reading this paper.  It is fairly clear and well-written.  I have some concerns about the fundamental assumptions and simplifications that lead me to believe it is not ready for acceptance, though.  Hopefully I have a misunderstanding that can be corrected by the authors.  My understanding of Rosenberg and Vieille 2000 is that the information state reduction (where the players need only consider the distribution over the utility parameters instead of the entire history) is only valid when the attacker must disclose its behavioral strategy prior to the defender responding.  At the very least, they only consider the maxmin case and discuss that in general such games need not have a value (i.e., the minmax case, where the defender acts first, results in different value).  They state in the concluding remarks, "Remark 3: Unfortunately, we say nothing of the minmax [case]" implying that their analysis somehow breaks otherwise.  This is troublesome for a number of reasons:  First, typically when modelling security games as Stackelberg games, the defender acts first.  We are interested in the defender's strategy, which when acting second is a function of the attacker's strategy.   Realistically, we will only have access to the attacker's strategy through samples of play. There is no analysis of how the quality of the defender's play degrades due to approximation error in the attacker's strategy.  This seems to be a concern even should the information state reduction hold in the minmax case, as the posterior update (eqn 4) requires access to the attacker's one-stage strategy.  Second, if we wish to consider the defender acting first, which seems more natural, we may have to consider the full strategy space, i.e., behavioral strategies that depend on the complete history of states and actions.  This is quickly becomes intractable.  At the very least, it has not been shown that the reduction is lossless.  Third, it implies that N-CIRL cannot be "solved" in general.  i.e., the strategies resulting from convergence of the contraction mapping may not form a Nash equilibrium of the game defined in 2.1.  Furthermore, there may be no efficient dynamics that converge to an equilibrium of the N-CIRL game.  At a more high level, does it make sense to call this "inverse reinforcement learning"?  It seems related only in that the attacker's intent is unknown, but there are no observations of an actual agent anywhere.  Once the value function has converged, the strategy profile is fixed.  Presumably if the attacker plays anything other than the strategy derived from that particular value function then the defender must change its response.  Minor comments:  16: This paragraph comes across as value alignment is necessary.  I think it is more correct to say that it is a desirable property.  Disaster is not guaranteed to happen if it does not hold, and disaster can still happen if it does hold. 24: IRL is "recent"? 27: Observer in IRL isn't really an agent as it has no actions or reward. 108: The attacker can observe the reward, since they know their intent. 149: \Theta is required to be finite on line 95, but here it is [0, 1] 201: \sigma^D = (\theta_0^D, \theta_t^D, ...), typo \theta_t^D => \theta_1^D 206: It would be helpful to make it explicit that the one-stage strategies      depend on distribution of intent parameters. 423: if => \text{if}