CNNs, like visual cortex, build a representation of the visual world that is useful to the “viewer”. We have known for a while that CNNs trained on object recognition tasks capture some (but not all) aspects of the representation computed by primate visual cortex. Here the authors propose to bridge the gap by explicitly encouraging a CNN to build a representation that is “similar” to the one computed by the visual cortex of mice.  This is a neat idea and certainly a novel one. The paper is clearly written, which I appreciated. The research question being tackled is clearly explained, the experiments are properly designed (I really like the randomized matrix control).  The research being presented is properly placed in the context of existing literature and all the moving parts are well summarized, which made the paper very easy to follow. I wish the authors would consider citing at least one piece of work by Poggio and collaborators. Poggio spent the first half of this decade characterizing the robustness of neural and artificial representations, so I think that line of inquiry is relevant. Maybe either Tacchetti, Isik, Poggio Annual Reviews of Vision Science 2018, which is a good review of that work, or the book Poggio, Anselmi 2016 with MIT Press.  I genuinely liked this paper, and I think it presents a very interesting idea that I am sure will inspire further inquiry. There are a few things I wish the authors had included, and hopefully might serve as a suggestion for a revised camera ready version, and which might improve the significance of this work.  1) While this paper certainly presents a “cool” idea, it is unclear what we, as either neuroscientists trying to understand the brain, or computer scientist trying to replicate human visual intelligence, are to make of this result. Is there a way to dive deeper and understand which nuances the unregularized CNN representation was missing? What is the “computational goal” encoded in the mice similarity matrix that was not present in the baseline CNN representation? How does this result help us build better CNNs, or what did we learn about the mouse visual system that we did not know before?  2) One possible way to start getting at this, would be investigating what happens if one trains the CNN exclusively with the regularization term (no task). Is that representation worse? Where does it fail?  3) In a similar spirit, random noise and adversarial perturbations are solid choices for a sanity check, but do not really reveal where the two representations differ and why. I wish the authors had designed more semantically relevant “attacks”. Maybe it’s 3-D rotations, maybe it’s illumination, what is it that CNN are missing.  4) Finally, a minor suggestion: it is my understanding that mice are pretty much blind, and either way do not “use” their sense of vision all that much. Why did you choose mice? Could you put a sentence or two in the text to explain your choice?  Thank you for sharing these cool ideas and results! I hope these suggestions help. All the best!