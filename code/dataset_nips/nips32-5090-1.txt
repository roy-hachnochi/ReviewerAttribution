---------- I read the author response and am satisfied with their promise to fix the minor issues and provide a more elaborate related work. I think the Kliendesener and Luxburg (JMLR 2017) works with noisy triplets to build kNN graph.   I do not feel the need to change my review score ---- The algorithm though yields good results is relatively straight-forward involving standard probability concentration bounds and using triangular inequality to the full extent.  I believe the method is technically correct and the overall quality of the presentation is good. There are many minor typos and grammatical mistakes (such as equation (5), repetitions on line 181 and 183, etc.) I think the second item (Line 79) is not stated correctly, but it does not affect the algorithm or its subsequent analysis.   The notation of the paper can be (and should be) simplified, it is overly technical and notations are introduced on the go within the statements (such as in Lemma 3.1 with =:).   My main reservations on the quality and significance of work are its limitation to the metric case, the number of query analysis of the simplified algorithm only, the very large space complexity. In the experimental section, the distance measure defined on the real dataset is not a metric (as acknowledged by the authors), but its effect on the quality of results is not discussed. This is particularly important since triangle inequality is the fundamental building block of the algorithm. The related work section is very poorly written; very closely related work must be elaborated on.   The results should be compared with techniques that build nearest neighbor graphs based on triplets (directly) without ordinal embedding.   The reproducibility checklist shows that a link to the code is provided, while in the paper the author(s)  state(s) that it will be provided upon publication. 