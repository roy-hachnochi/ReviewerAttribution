UPDATE:  After reading the rebuttal and discussing with the other reviewers, I'd like to increase my score to 5. Particularly I think that the theoretical part is weak, and it needs significant improvement.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  This paper presents a Lookahead algorithm for optimizing deep neural nets. The algorithm consists of an inner loop, which updates fast weights using conventional optimizers like SGD, and an outer loop, which updates slow weights by linear interpolation. Also some theoretical analysis on optimal learning rate and convergence is provided as well. The paper is written clearly, easy to follow.  My main concerns about the paper are as follows: 1. Novelty: The proposed idea has been explored in Zhang et al. 2018 (https://www.merl.com/publications/docs/TR2018-206.pdf) where they proposed an algorithm called Time-Delay momentum, equivalent to fast/slow weights, though I think there may be some errors in their theoretical analysis. Similar performance such as training and testing behavior on image classification was reported as well.  2. I do not see any usage about Sec. 2.1, as stated in L91 that in practice the authors are still using fixed learning rate. So what is the benefit of providing such a result?  3. In Yanet al., Unified analysis of stochastic momentum methods for deep learning, IJCAI 2018, the convergence of momentum in expectation was proved for (convex) deterministic loss functions. Sec. 3.2 verifies such a result.  4. The empirical results are weak, and I do not see the benefit of using the proposed method over traditional ones like SGD and Adam.