Update: I have read the author's response and have kept my score. Please note that in DeVries and Taylor'17, 'ResNet-18' is not truly the ResNet-18 model (it consists of 4 stages and has more than an order of magnitude more parameters than the original ResNet-18 due to wider channels). This should be made clear in the paper in order not to cause more confusion in the community.     Originality: Medium/High  The proposed algorithm is considerably different than recently proposed methods for deep learning, which gravitate towards adaptive gradient methods. It has some similarities to variance reduction algorithms with inner and outer loops, however Lookahead has a very simple outer loop structure and and is easy to implement. I consider it a significantly original work.   Quality: Medium  The experimental evaluation is very extensive, including many challenging tasks, and improvement on language modelling is significant. The analysis, although helps motivate the proposed algorithm, is restricted to a very simplistic setting -- any analysis for the smooth non-convex case would be very helpful since the method is aimed at training neural networks, even if it provided no advantage over SGD.  The CIFAR experiments, however, look a bit off. To achieve 4.7-4.8% error on CIFAR-10 a very deep ResNet is typically requried (a ResNet-1001 achieves similar performance, as reported in [1]), and I find it unlikely that the authors achieved such performance with a ResNet-18. I would guess that a Wide ResNet was used instead of a standard ResNet model, with a considerably widening factor, but I expect this to be addressed in the rebuttal.   Clarity: High  The paper is clear and well written.   Significance: Medium/High  The paper manages to successfully convince that variance reduction-type methods with simple inner/outer loops can provide significant advantages to deep learning applications, and might increase interest in such kind of research. The idea is refreshing in face of how much focus is aimed on adaptive gradient methods and Adam-type variants currently, and the paper shows through extensive experiments that Lookahead can provide performance boosts when compared to other methods -- especially in tasks where adaptive methods outperform SGD (in the paper, language modelling and machine translation).