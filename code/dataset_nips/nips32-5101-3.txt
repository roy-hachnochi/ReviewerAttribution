The Authors provide a simple but powerful approach to empirical bayesian inference under rather broad assumptions. The method is relevant in settings where both a) standard statistical estimators (such as the average) can be evaluated and b) covariates can be used to train machine learning models to estimate the same value. The paper tries to solve this problem in the setting where the standard estimator is not reliable enough (e.g. because sample size is too small) and the covariates only give weak information on the target variable. The problem setting considered is highly relevant in many real-world settings. Considering the practical relevance and theoretical interest in empirical bayes methods, it seems quite surprising that this approach has not been investigated earlier (only for special cases such as linear models). As such, the presented paper fills a very important gap by giving the proposed method (that apparently has already been used in practice, as noted in the related-works section) a clear theoretical basis. The focus of the paper lies on the theoretical analysis; the main result are minimax bounds that explicitly incorporate the ML model, as well as (potentially unknown) model variances. Additional theoretical contributions are robustness under misspecification of the model (leading to very general results), as well as an analysis of the practical implementation of the proposed model. The paper concludes with an empirical evaluation on both synthetic and real-world data.  The paper is very well written; even without strong background in the topic it is easy to follow, results are being discussed, explained and put into context.  As a minor drawback, a discussion of potential disadvantages of the proposed method could have been enlightening.