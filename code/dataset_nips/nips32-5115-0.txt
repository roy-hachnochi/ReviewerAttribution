The proposed method is a novel (and elegant) combination of existing techniques from off-policy learning, imitation learning, guided policy search, and meta-learning. The resulting algorithm is new and I believe it can be valuable to researchers in this field.  I think the paper does not sufficiently discuss the related work of Rakelly et al [1] (PEARL). The paper is cited in the related work section as one of the methods based on "recurrent or recursive neural networks". If I remember correctly they don't use recurrency (only in one of their ablation studies to show that their method *outperforms* a recurrent-based encoder). Furthermore, this paper should be pointed out as being an *off-policy* meta-RL algorithm. While I think the methods are sufficiently different, this difference should be explained in the paper. I also believe that a comparison in terms of performance and sample efficiency (during meta-training) would be an interesting addition to the experimental evaluation.  The quality and clarity of the paper high. The method is clearly motivated, and the technical contribution well explained. I think the pseudo-code helps a lot, and together with the supplementary material and code provided and expert reader should be able to reproduce the results.  Overall, I think this paper has high significance. The proposed method is novel and well executed and there are multiple good ideas that go along with it. I'm surprised the paper does not compare to PEARL [1], but I believe this can be easily addressed.  References:  [1] Rakelly, Kate, et al. "Efficient off-policy meta-reinforcement learning via probabilistic context variables." arXiv preprint arXiv:1903.08254 (2019).  ------------------------------------------------------------------------------------------------------ - UPDATE -  I would like to thank the authors for their rebuttal and for adding the comparison to PEARL - these are very interesting insights! Together with these results, I think this is a strong submission and I therefore increase my score to 7, and vote for acceptance.