originality:  This paper follows on from the state-of-the-art in meta-learning. Meta-learning consists in improving the sample efficiency of any reinforcement learning algorithms by finding an initial set of parameters from which it can quickly adapt to any task of interest. This paper proposes a new meta-learning algorithm that decouples meta-objective learning and meta-training by using two nested loops: the inner loop can be any reinforcement learning method, while the outer loop optimizes the meta-objective using a supervised learning method. It goes pretty much against the current trend of unified the deep learning black box and therefore is sufficiently original on its own.  quality:  The most significant application cases are discussed thoroughly and the authors provide strong theoretical convergence guarantees. However, the theoretical analysis covers only the particular case of on-policy learning, while the method is intended to be used for off-policy learning. The examples are appropriate to assess the performance of the method although they are quite simples and not very challenging control problems in the first place. I'm afraid it may still untractable in practice for real-world applications.  clarity:  The previous works are very well introduced. The relation with other works and the theoretical background is made very clear throughout the paper, making this paper readable for those without knowledge in meta-learning. The implementation details are very helpful and make the results pretty straight-forward to reproduce.  significance:  This paper is a valuable contribution to the literature on meta-learning, showing significant improvement over the state of the art. The idea of designing a meta-learning algorithm explicitly separating the meta-training and meta-objective learning into two distinct phases that can combine any reinforcement learning method to any supervised learning method is clearer and surprisingly effective. This architecture is very convenient to build on it, try different combinations, and improve its performance even further. It presents some theoretical convergence analysis, significant improvement of sample efficiency relative to some reference examples. It applies successfully to the challenging case of sparse reward, which is not the case of the other meta-learning methods, to my knowledge. 