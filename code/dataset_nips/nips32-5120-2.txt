This paper explores contextual bandits with cross learning, i.e., when an arm i is pulled with context c, the learner not only observes the rewards for arm i in context c, but also observes the reward of arm i for all other contexts. The authors investigate the settings in which the context/rewards are stochastic or adversarial and show the corresponding regret bounds (or regret lower bound).  Overall I think it’s a well-written paper. The results seem sound and reasonable (though I didn’t carefully check the proof), and in particular, I found the analysis of UCB1.CL (which drops the dependency on C in the regret bounds) to be non-trivial and interesting.   I would lean towards acceptance.  Other comments:  I’m not entirely convinced that auctions are best formulated as contextual bandits with cross-learning (which seems to be the major motivating example of this paper). As the authors also pointed out in the experiments, it requires the assumption that other bidders’ valuations to be independent of the bidder’s valuation. The other examples illustrated in the paper (e.g., multi-armed bandits with exogenous costs) are reasonable though don’t seem as impactful/pressing as the auction settings. Honestly, though I don’t have other applications in mind, I think the setting is general enough (especially with the added discussion on partial cross-learning) and worth investigating. However, given one of the main contributions of the paper is to introduce this cross learning setting, it would be nice if the papers can provide stronger motivations for this setting.  In the experiment setup, the authors assume the “true” valuations of some participant is known. How is this achieved? Are there additional assumptions on bidders’ beliefs, so you can infer the bidders’ values from bids in first price auctions? 