This manuscript assembles and describes a set of benchmark data sets for use by the machine learning community in evaluating semi-supervised learning in relation to various properties of proteins. The authors also demonstrate the application of several self-supervised pretraining methods to these tasks.  The manuscript is extremely clearly written, which is critical in work for which one of the primary goals is communicating to non-specialists in computational biology.  The benchmarks are well constructed, though the actual work involved in curating them was not particularly substantial, since most of the benchmarks have been previously published and are merely collated in this work into one collection.  Still, the utility is clear, since practitioners can now go to one place and test their methods using a single interface on a variety of tasks. In terms of modeling, the approaches they use represent architectures that frequently yield state-of-the-art performance in NLP or computer vision, such as transformers or ResNets.  One concern is that the models employed here are huge (on the order of 38M parameters).  The manuscript should provide some justification for using such large models. The concern here is two-fold: (1) these complex models may not be as powerful as simpler models, especially on some of the smaller benchmarks, and (2) beginning with massive models that require significant compute resources may discourage potential users of the benchmarks.  A significant problem with all of the benchmarks is the simplicity of the evaluation, where only global accuracy metrics are considered. Methods like precision-recall and ROC analysis are frequently employed in this domain.  It might also be helpful to explore metrics such as accuracy at predicting long range contacts or error partitioned by the true label. Note that this would mean evaluating accuracy for low stability and high stability proteins separately.  line 150: The CB513 data set is quite standard in secondary structure prediction, but it is also quite old, dating from 1999.  A newer, larger benchmark should be employed.  Minor comments:  line 1: "Protein modeling" is vague. Clarify what task is being addressed here by including information from lines 37-38.  line 75: The term "homology" is reserved only for proteins that share a common evolutionary ancestor. If the proteins share similar function but no common ancestor, they are not homologous.  line 125: The text says that the test set is constructed by holding out families, but in the next sentence "For the remaining data we construct training and test sets using a random 95%/5% split."  This makes no sense.  line 134: "8 thousand" -> "8000"   