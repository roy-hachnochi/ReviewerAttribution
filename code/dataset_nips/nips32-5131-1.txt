This paper shows that regularization is helpful for the sample complexity in some settings. Specifically, they consider two-layer ReLU networks, and carefully construct a data distribution such that the optimal regularized neural net learns with O(d) samples while the NTK network requires \Omega(d^2) samples. Basically, their data distribution has very sparse features such that the regularization helps to find informative features while NTK overfits to noise. The authors also show that for infinite-width two-layer networks, noisy gradient descent can find global minimizers of the l_2 regularized function within polynomial time.   For the proof, in order to upper bound the sample complexity for regularized neural nets, the authors show that with a small regularizer (goes to zero), the global minimizer of regularized logistic loss will converge to a maximum margin solution. This holds for any positive homogeneous predictor, including multi-layer ReLU nets. The authors develop new techniques to lower bound the sample complexity of NTK. These techniques can be applied to other cases.  Overall, this is a good theory paper addressing important problems with novel techniques. The paper is well written.   Here are my main comments: 1. The optimization result of the infinite-width nets with regularized loss function requires the activation function to be differentiable, which does not hold for ReLU. I was wondering whether some smoothing techniques can resolve this issue. It might be better to discuss the challenges to extend to ReLU nets and the possible approaches. 2. Theorem 4.3 only means using a wider network won’t hurt the generalization bound. It will be more interesting if the authors can show using a wider network can strictly improve the maximum margin achievable. Of course, this may not be true. The authors can discuss a bit more about the possibility of establishing a gap of generalizations between narrow nets and wide nets.  Here are some minor comments: 1. It might be better to move Figure 1 to page 4 since the figure is an illustration of the distribution. 2. Line 44: ``than than’’ 3. Line 126: ``a<b’’ -> ``a<b, a>b’’  ------------------------------------------------------------------ I thank the authors' clarifications, which have successfully addressed my questions. I will keep my score as it is.