The paper's idea of the benchmarking salience methods on retrained models is simple, original, and clever.  It addresses one of the basic problem with evaluating salience of pixel maps, which is that the pixels are not independent of one another.  ROAR elegantly sidesteps this problem by proposing simply evaluating a retrained model on a dataset in which estimated salient pixels have been grayed out: if redundant information remains, the retrained model will find it and use it.  However, the method raises a few key questions, that if answered, could make this a stronger paper.  1. What is actually being measured by ROAR?   While the proposed test is an interesting setup, it seems different from the original question tackled by salience maps, which is "how is the model that I have making its decisions?" Since ROAR retrains (many) new models, it is not testing faithfulness of a salience method the original model that I began with.  The paper would be improved if it stated more clearly what ROAR is actually aiming to measure.  2. Specifically, will ROAR penalize a salience method for identifying a biased sensitivity (e.g., from a model that attends to only corner pixels in decoy mnist [AS Ross 2017 "Right for the right reasons"]).  That is, would ROAR incorrectly score such a faithful salience model that highlights the model's erroneous attenion on decoy inputs as less accurate than one that also highlights the redundant main input pixels?  Another question:  3. I am confused by the warning on lines 84-87 of page 3.  My understanding of the method as described is that the most salient pixels are replaced by a gray average over the entire training set, and then after this data erasure, the model is retrained several times over this modified data set.  Does something different have to be done when a model with L1 regularization is used? 