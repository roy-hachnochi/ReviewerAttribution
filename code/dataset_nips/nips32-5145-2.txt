UPDATED REVIEW AFTER REBUTTAL AND DISCUSSION:  After discussing with the other reviewers, I'm increasing my rating. However, my concerns still stand (and are shared by the other reviewers to different degrees); the paper can be greatly improved if these are addressed in the final version.  My main concern is as follows: there's little in the paper that helps the reader analyze the results of their experiments and the meaning of their method: In lines 260-267; pg 8: the authors report that all saliency methods roughly perform the same, with the exception of SG-SQ-GRAD and VAR-GRAD. They explain the similarity between SG-SQ-GRAD and VAR-GRAD; however, they fail to explain substantively how these methods differ from the rest (besides mentioning that they capture whole objects better) or why the other methods are so indistinguishable from each other.  In particular, ROAR may be vulnerable / bias to the "style" of a saliency method (i.e., coarseness vs. finegrained-ness, as represented by CAM/Grad-CAM vs. gradient).  R1: "1. Experiments that evaluate a coarse importance estimator" R2: "1. A clearer statement of what ROAR is trying to measure (and ideally experimental evidence to back it up) and how it might differ from the problem traditionally tackled by salience maps."  I'd recommend the authors do the following for the final version: 1. discuss and compare against the existing post-hoc deletion game, i.e., like the one in RISE (this is fairly easy to do, as no re-training is required) 2. discuss (at the very least) and analyse whether / to what extent ROAR is vulnerable or bias to the "style" of the saliency method (i.e., try a coarse method like CAM/Grad-CAM, as initially suggested by R1 or add a "patch extraction" step that fixes feature coarseness across all methods) -- this can be a single experiment (not an exhaustive addition of another saliency method to the existing experiments)  Lastly, I did not ask for evaluation on more datasets and methods (as the authors claimed in their rebuttal). My only related mention of this is in the context of my second suggestion as a way to iterate more quickly in order to improve the technical aspects of the method (i.e., use a smaller dataset / easier task such as fine-tuning; and that using a quicker iteration pipeline would allow for evaluation of more methods).  ---  This paper introduces ROAR (RemOve And Retrain), a novel metric for evaluating saliency methods. ROAR involves removing a percentage of features identified as "salient" and retraining a model with these features removed. The authors provide strong motivation for ROAR in section 3 and Figure 2; principally, this is the typical "deletion metric" used which involves evaluating models on images with removed features causes models to evaluate on out-of-training-domain distribution (i.e., images with removed features). The main drawback of this work is that it presents negative results, in that it is generally unable to discriminate the quality of different saliency methods (with the exception of SmoothGrad-Squared and VarGrad). This work would benefit from more thought and improvement on the metric itself to diagnose why it's currently not discriminative. The significance of this work is severely hampered by the negative results; I would only change my score if the method is remedied to demonstrate empirical value. Generally, the paper is clearly written.