Review Update: "For this reason, SKMS’19 only apply their techniques to sparse (sampled) softmax and embedding layers with block versions of the hashes. They leave the treatment of hidden layers to future work."  This statement is incorrect. The ICML version of the paper showed results compressing all of the layers of the neural network for ResNet-18 and Transformer-XL.The paper showed how to adapt count-sketches for GPUs and did not show significant computational slow-down compared to the full-sized baseline. Their code is publically available for PyTorch.   "Space saving: SKMS’19 uses sketches that are 5x smaller than the original tensors, whereas our sketches are 100x-1000x times smaller."  It is important to compare how much overall GPU or TPU memory is saved rather than the data structures themselves. Your paper didn't demonstrate 100x-1000x overall memory savings. i.e. From Table 2, Adam 6.15 GB => SM3 4.9 GB (Overall Savings - ~20% savings). Those saving are on-par with those reported in the SKMS'19 paper.  "We could not obtain reasonable results with SKMS’19 (and they do not report such results)."  Their paper reported comparable results against SGD on ImageNet with ResNet-18 and their source code is publically available. I don't see why it wouldn't perform as well as SM3 if you used the AmoebaNet-D architecture.  I also have new several questions about the compression quality experiment in Figure 1 of the rebuttal. 1. Is the error in the count-sketch significant enough to hurt the convergence rate and performance of the optimizer? 2. At what time step in the optimization process, did you perform this comparison? i.e. iteration 1, 1000, 1,000,000 3. Does this gap hold throughout the optimization process? 4. How much memory was allocated for the count-sketch? What are the dimensions of the count-sketch? 5. Why did you select only the top 100 parameters? A single neural network layer has millions of parameters.  The author's algorithm and experiments are geared towards Tensorflow and TPUs, while SKMS'19 worked with PyTorch and GPUs. Algorithmically, SM3 is a modified-version of the count-sketch data structure. The rebuttal did refute how you could not modify the count-sketch data structure to behave like the SM3 algorithm.  -------------------------------------------------------------------------  -- Summary -- Modern adaptive optimizers require additional parameters that grow linearly with the number of parameters. This constant memory overhead causes problems with large-scale training. This paper proposes an adaptive optimization method that retains per-parameter adaptivity while reducing memory overhead. The additional memory savings is used for either a larger batch-size or more expressive model.  For a tensor [N_1, N_2, ..., N_p] where d = prod(N_1, N_2, ..., N_p). Each parameter i in [d] is associated with p variables. A variable for each dimension of the tensor. All parameters in the same tensor dimension (i.e. row or column) share the same variable. For parameter i, we take the minimum value among the p variables.  -- Originality -- In "Compressing Gradient Optimizers via Count-Sketches" (ICML 2019), the paper proposed using the count-min sketch (CMS) data structure to compress the additional parameters to reduce the optimizer's memory overhead, which is the same objective as this paper.  There are clear similarities with the cover set and the count-min sketch (CMS): 1. For the count-min sketch, parameters are randomly mapped to p variables. For the cover set, parameters deterministically mapped along the tensor's dimensions to p variables. 2. In both approaches, the minimum value among p variables is used as the parameter's estimate. 3. The count-min sketch aggregates all updates, while SM3-I takes the maximum value from all parameters in the cover set.