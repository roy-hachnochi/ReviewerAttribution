I have read the rebuttal and other reviewers' comments. I tend to accept this paper.  As to the breakdown of memory usage, I was wondering what is the composition of the (memory) working set during the model training. In practice, when some in-place operations are not available or practical for certain hardware architectures, some additional copies of temporary tensors for computation might be needed. So how would this affect the adoption of SM3?  As to the over-parameterization, please check the following papers to see a concrete assumption and its effect on simplifying and improving the convergence analysis.  Mass: an accelerated stochastic method for over-parametrized learning Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron ---------------------------- This work presents a practical solution (SM3) for reducing the memory space used by some adaptive optimization methods. The proposed solution trades some convergence speed (since it uses smaller learning rate according to the maximum) for some memory space dedicated for maintaining cumulative gradient information. This solution is applicable to AdaGrad-alike methods which uses second moments of the gradients to adapt learning rate during training. The authors show the convergence of SM3 and demonstrate its capability of memory saving without performance degradation on training several important neural networks.  I like the proposed method, which is simple and practical. The idea behind is also likely applicable in many other settings.  However, I tend to give this paper a borderline score for concerns on novelty.  Specifically, the techniques used by Adafactor and SM3 are similar. The second claim in the comparison to Adafactor (end of the section 4) is misleading. Just like Adam, the learning rate of Adafactor should take into account the second moment of the gradients, which does change a lot at the beginning of the training. It is also hard to tell why the technique used by Adafactor cannot be easily applied/extended to high-order tensors.  In regard to the experiments, I wish that the authors could give a detailed breakdown on the memory usage. In addition, I would appreciated a deeper discussion on the memory saving with SM3, e.g., what kind of architecture fits SM3 better and what do not?  As to the theory, it might be worthy adding more assumptions such like over-parametrization (or interpolation) for better convergence bounds.