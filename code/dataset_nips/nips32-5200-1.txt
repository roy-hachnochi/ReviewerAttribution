The paper shows that systematic generalization in seq2seq transduction can be achieved by training a memory-equipped model to rapidly bind symbols to their meanings and use these bindings in the context of a new query. The method is framed as meta-learning, although it is important to note that “learning” in this case essentially just means “loading into memory”. The experiments are conducted on adapted versions of the SCAN and mutual exclusivity tasks from recent literature.  The paper is clearly written and the proposed method is easy to understand. The experiments seem to be well executed.   My main concern is the significance of the results. As correctly mentioned in the paper in lines 83-87, application of meta-learning has required a massive data augmentation compared to the original SCAN task. After this augmentation is performed, it is rather unsurprising that the resulting model generalizes systematically, as it is forced to treat primitives like “jump”, “walk”, “run” as mere pointers in the memory. Performing such a data augmentation requires a substantial amount of background knowledge, limiting the generality of the proposed method. In particular, one needs to know beforehand which positions in input sequence correspond to which positions in output sequences. It is nontrivial how this can be known for more realistic data, and this question needs to be discussed (like e.g. in “Good-Enough Compositional Data Augmentation”, where a heuristic algorithm for this purpose is proposed). It is furthermore unclear how an appropriate support set would be fetched for a given query in the real world case.   In line with the above criticism, I find it rather unsatisfying that results only on toy synthetic tasks are reported. Such tasks are great to highlight weaknesses of existing methods, but they are less appropriate when one proposes a new approach, especially when it has so many underlying assumptions.  Lastly, I believe there is a more subtle issue with the proposed method, namely that the design of the data augmention procedure is very strongly informed by the kind of systematicity that is being measured. For example, the specific data augmentation proposed in this paper may or may not help for a hypothetical split in which the word “thrice” is only used in short sentences at training time, but then appears in long sentences during training time. One can construct a meta-learning setup which trains the models to do exactly that: quickly learn (e.g. by loading examples in memory) meanings of words from short sentences and then apply the learned meanings in long ones. But there is an infinite number of ways in which test data can be systematically different from training, and arguably, all of them can’t be anticipated.   To conclude, I think the current version of the paper needs improvement. In particular, it should explain and empirically prove the significance and the generality of the proposed method.   A few related papers:  - https://arxiv.org/abs/1904.09708 proposes a simple approach to get a high performance on SCAN and should be cited (even though it seems like a concurrent work).  - https://arxiv.org/abs/1703.03129 is relevant in that it uses a memory is used to quickly learn meanings of words  UPD. I have read the rebuttal and other reviews. I still think that the fact that the network exhibits the desired behavior is rather unsurpising after 23 possible meaning permutations are used to effectively augment the data.  The authors have suggested that they are working on applying the proposed method in other settings, namely few-shot language modelling and Flash-fill program induction, and that in those settings a “compositionally-informed episode-generator” is not required. While this sounds quite interesting, these experiments are not a part of the paper. I find the results of the paper insufficient to believe without evidence that the proposed method will work when a compositionality-informed episode generator is not used.   More broadly, I think there are 2 ways to look at this paper. If it is viewed as a cognitive science paper, then like R1 correctly mentioned, better positioning would be required. Can the proposed protocol be viewed as a model of child language acquisition? Seems dubious to me, but I am not an expert. Can we conclude that essentially neural networks can learn the rules like humans do but we just have not been giving them the right data? In my view the training episodes in the paper may contain more evidence for compositionality than any real world data stream would. This needs to be discussed in more depth, and the paper currently doesn’t do so.   Alternatively, the paper can be evaluated as a machine learning work, that allegedly addresses an important shortcoming of seq2seq models. In such case, I think the paper should have shown that the proposed method can be useful in the absence of or with a less task-specific “compositionally-informed episode-generator”.