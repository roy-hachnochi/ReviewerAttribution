** SUMMARY * Goal: A NN learning approach that produces a high degree of compositional generalization (in a specific type of setting).  * Setting: In seq2seq tasks, if a model has learned that the input 'x twice' should produce the output 'Y Y', for a given set of input/output token pairs x/Y, then given a single example showing that a new token 'b' should produce output 'Q', the model should without training map 'b twice' to 'Q Q'. More generally, a single model must deal with multiple 'episodes' of seq2seq training/testing. In a single episode, the mapping from input to output tokens is fixed. Across episodes, certain input tokens (which I'll call "lexical") map to different output tokens, while other input tokens (which I'll call "functional") map consistently to operations over output tokens, in all episodes. (In the example above, 'b' is lexical, 'twice' is functional.) Within an episode, example I/O pairs show that particular episode's mapping of lexical input tokens to their corresponding output tokens.  * Model innovation: Provide a memory that, in each episode, can store given episode-specific input/output pairs (called the episode's "support") which can be used to determine the episode-specific mapping of lexical input tokens (which I consider "declarative knowledge").  (Key-value-query) attention over this memory can retrieve the episode-specific output correspondents of lexical input tokens, while fixed weights learned across multiple training episodes encode (as what I consider "procedural knowledge") the meaning of functional tokens (like 'twice'). * Complexity: The innovation does not entail new complexity issues. * Tested models: In a given episode, for each of a set of I/O "support" examples, the final state of a non-episode-specific bi-LSTM encodes the example's input sequence as a vector which serves as a key to a memory slot, the value of which is the final state vector of a second non-episode-specific bi-LSTM encoding the example's output sequence [Fig 1]. The actual test input string (as opposed to the support inputs) is processed by a third LSTM. Each input token generates a query vector which retrieves information from the memory containing the support examples. The retrieved information (along, I believe, with the query vector) is accumulated across the whole input string. The resulting matrix is attended to by a fourth LSTM which generates the output tokens.  * Tasks:  - Expts 1-3: Versions of the SCAN compositionality task, mapping an input string (e.g., 'jump twice') describing an action sequence to the described sequence of primitive actions (e.g. 'JUMP JUMP'). The input strings combine lexical tokens (e.g., 'jump') referring to primitive actions (like 'JUMP') with a set of functional modifiers (including 'twice', 'around right', 'and', 'after'). (Expts 1, 2 have 4, 24 types of lexical tokens, respectively.) Across episodes, the meaning of the functional tokens remains constant (e.g., 'twice' always means twice) but the meaning of the lexical tokens is permuted to a unique mapping between input and output tokens (e.g,'jump' denotes primitive action 'RUN' in episode 1, it denotes 'LOOK' in episode 2, etc.). One mapping is withheld from training and used as the test episode: 'jump' -> 'JUMP', 'run' -> 'RUN'", etc. In Expt 3, withheld from training are *all* permutations that map the specific token 'jump' to 'JUMP' (including permutations that map some other lexical token 'x' to 'Y' =! 'X'); tested are all input sequences containing 'jump'. In each training episode, a support set of 20 randomly-chosen input sequences and their corresponding output sequences is provided from which the episode-specific meaning of lexical tokens can be deduced. In the single test episode, the support contains 4 I/O pairs, each consisting of a single input token and its corresponding output token.  - Expt 4: 4 input tokens are mapped to 4 output tokens, differently across episodes; the support set in each episode consists of 3 I/O pairs each containing a single input and single output token: the fourth pairing is not in the support, and must be deduced from the Mutual Exclusivity generalization, exhibited during training, that each output corresponds to only a single input token, so the output token missing from the support must correspond to the input token missing from the support.  * Results: In Expts 1-3 [Table 2], over all complex instructions containing 'jump', at test a standard seq2seq model achieves 0.03% (whole-output-string) accuracy while the meta seq2seq model performs at 99% on Expt 3 and 100% on Expts 1-2. A strong Mutual Exclusivity bias emerges in Expt 4, with an accuracy of 96%, indeed "showing it can reason about the absence of a symbol in addition to the presence of a symbol" [300].     ** REVIEW * The problem of compositional generalization addressed here is very important. * The results are striking. * What is demonstrated is that, in a seq2seq task, if the training data show that the I/O mapping contributed by certain input tokens varies, then a NN can learn to treat those tokens as variables, using a key/value/query memory retrieval system (a.k.a. "attention") to look up the values of those variables. This is true even when the mapping between input and output tokens is only indirectly specified via complex-input-sequence to complex-output-sequence examples. I think this is an important result.  * It's not clear that people require the kind of training given here, explicitly being shown evidence that 'jump' could mean entirely different things in different contexts, while 'twice' always means the same thing, before they can use 'jump' compositionally with 'twice'. I'm not sure what to make of that, though. * It would be good to consider this work in the light of work on program synthesis, program execution, and mathematics problem tasks. Answering math questions or producing the output of computer programs also involve binding values to variables, but there the assignment of values to variables is explicit in the input to the network. In the present paper, the problem is more like example-based program synthesis, where a sample of I/O pairs of an unknown program' (like 'jump twice' -> 'JUMP JUMP') is the model's input, and the model needs to determine the implicit binding of values to variables in order to produce the output of the program for novel inputs. Here, however, the program is hardwired into the network connections, and not a symbolic output of the model. * The distinction among input tokens I'm calling lexical vs. functional should be made more explicitly in the paper. I find the term "primitive instructions" confusing because the instructions consist of sequences of tokens which are the primitives of instructions, and these include both what I'm calling lexical and functional tokens. * The term "meta-learning" is confusing, to the extent that that means "learning how to learn", because there is not learning in the testing episode. During training, it's just learning how to use a memory during testing to find the current value of a variable: that is, learning how to *process*, not learning how to *learn*. * It would be good to insert "implicit" into "Our approach ... can learn to apply [implicit] rules to variables." [59] as this is not about explicit rule interpretation. * The description of the precise activation flow in the model is unclear (hence my hedging when attempting to summarize it): a proper, complete set of equations should be provided in Sec. 3. My problem may be that I'm not familiar with "Luong-style attention" and don't have time to read the cited paper. * "During training, models are exposed to the “jump” instruction in only a single training pattern when the instruction “jump” appears in isolation" [164] is confusing given the later "Episodes consist of 20 support and 20 query instructions sampled uniformly without replacement from the complete set of SCAN patterns" [179] (It's also unclear what a "SCAN pattern" is, exactly.)  [ADDED NOTE: I read the authors' feedback, the other reviews, and reviewer discussion several times each, and pondered for a considerable time. However, in the end, I see no reason to change my score or review, except to acknowledge here that the proposed revisions in the authors' feedback concerning a few of the points in my review would all improve the paper: thank you. There are a number of points in my review that were not addressed, but this is inevitable given a single page to reply to three reviews. If more of the points I raised in the review can be remedied in a revision, so much the better.] 