I updated my score to 6 since the author's provided some nice explanations in their author response including timing estimates and Bayesian characterization.  ----- Original review ----- Originality: Overall the paper seems weak in originality given the amount of previous work on similar problems. The Bayesian viewpoint is somewhat interesting but the paper suggests using a MAP estimate anyways, which is equivalent to a penalized likelihood optimization.  Quality: The theoretical results seem reasonable but not particularly surprising given the amount of work on sparse estimation including multiple precision matrices.  The empirical results are slightly more promising but given that only one real dataset is used, I'm concerned that this won't generalize to other datasets.  Maybe other datasets from [5, 11, 12, 15] could be used?  Clarity: Overall, it would be beneficial if the paper included more insights and intuitions about the theorems. Why are the theoretical results particularly surprising or interesting?  Additionally, the theorems seem to require many definitions and conditions, which makes it difficult to separate out what's new.  Could these be simplified or reorganized for better clarity?  Significance: Overall, the paper seems relatively incremental since many others have explored the general problem of learning multiple graphical models together including some theoretical results.  The theoretical results are slightly stronger than previous papers but I'm not sure this work would impact ML practitioners or inspire new theoretical work.  Other note: What is the computational complexity of this method compared to others?  This is always an important thing to know in these situations.