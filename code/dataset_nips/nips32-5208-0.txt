I very much like this approach.  It is well explained.  Many of the existing approaches are overly complex cobblings of discrete-time and continuous-time ideas.  By contrast, this is a clean solution that is computationally reasonable.  The experimental results show a good variety of uses of the proposed method.  It is here that the paper could be most improved:      - The details of the MLPs (number of hidden units, number of layers,           method for training) are completely omitted.      - The paper does not compare with Neural ODE, particularly where           the authors of that paper used an RNN to encode the           starting latent value.      - The paper does not do comparisons in Section 4.3 to other methods.           The neural Hawkes process can be easily augmented (by adding           another MLP from the hidden state) to predict marks (locations)           associated with each event.  Overall, I like the paper, but it really should supply more information on the experimental set-ups, in order to judge the significance of the effects shown.  The "Reproducibility Response" claims this information is in the paper, but it is neither in the paper nor the supplementary material.  One question which should be addressed in the paper:      - Why are the RNN and Neural JSDE better than a Poisson process      on Poisson data? If this is because of regularization, then this further underscores the need for clear experimental set-up descriptions so that this can be judged for all experiments and all methods. 