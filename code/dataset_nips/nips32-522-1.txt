The paper considers the task of dense pose estimation, which can be decomposed into finding a semantic segmentation of body parts complemented with the regression of u-v coordinates for each pixel, relative to each body part.  The authors explore several error models which all result in the addition of output heads to an existing dense pose network. The simplest one is a local error model, the most complex model (the `full’ model) considered, is one that adds in a global error plus local errors that are independent for the u-v coordinates. The output heads are trained by maximum likelihood for the regression model of the u-v coordinates.  The full model performs as well as the simpler error models but results in a higher neg. log-likelihood and is therefore presented as superior. Two other cases are considered: 1) training a model whose output heads for the error estimation are additionally conditioned on the ground truth and 2) a way of combining independently trained models by using their uncertainty estimates.  It is unclear why the authors do not present the results of related work, which makes it somewhat difficult to assess their models’ performance, but of course still allows to compare their relative performance. Although the work ablates the model adaptations it considers, it does not seem to discuss the results and implications very well. For instance the simpler models (simple-2D) perform slightly better than the full error model, which however in turn receives a better neg. log-likelihood. Why is that? The model ensembling using uncertainty predictions only performs marginally better than a simple model average, there is no discussion or significance assessment. The model whose uncertainty heads are conditioned on the ground truth during training perform better at test time (w/o access to the ground truth). It is unclear why this conditioning helps training of the core prediction network beyond using the regression loss alone. There is no discussion of that. This model could also not be used to assess the model’s uncertainty at test time, also prohibiting an ensemble of this kind of model.  The term `introspection ensemble’ seems a bit far-fetched, more accurately it is an uncertainty-weighted ensemble.  Being concerned with highly structured aleatoric uncertainty and modeling uncertainty in the annotation process, there is missing related work, e.g. the Probabilistic U-Net which models aleatoric uncertainty for semantic annotations. Also, the error model is concerned with modelling the error of subvectors in u-v-space but not between the part label predictions, i.e. the inter subvector covariance and thus the semantic segmentation, why was there no attempt to incorporate this?  Another current limitation of the approach is that the error model does not consider the correlation of errors specific to regions, e.g. individual body parts. Instead they are either local and/or global, despite the discussed observation that the error of individual body parts may be strongly correlated (lines 193 - 196).  The discussion of why learning with an uncertainty model helps training and final performance seems insufficient. It is stated that a `model’s better understanding of the uncertainty of the data’ helps. The reason that as to why it helps is presumably the loss attenuation, that allows the model to down-weight the loss of difficult and thus likely ambiguous / noisy examples or pixels. There is quite a bit of literature on that, which could be part of a discussion.  Lastly some typos need fixing: `ensemlbing’, `cosntrained’, `gradient descend’, `locationS’.