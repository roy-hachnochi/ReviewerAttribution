While the paper is structured as a grab bag of improvements to the i-ResNet model, the methods are original and well explained via text and ablation studies. The work will be helpful for increasing the maturity of these promising residual models.  Overall the quality and clarity of the paper is good, although I think there are a couple points in the paper that could use elaboration/clarification.  Regarding the inequality in the appendix following equation 16, I presume the reason why these are not equal is because the Jacobian matrix and it's derivative might not commute? Maybe it would be worth mentioning this. If these are indeed not equal, have you observed a difference in the variance of the estimator going from one form to the other (irrespective of the difference in memory costs)?  For the backward in forward, it's not clear to me where the memory savings are coming from. It is mentioned that precomputing the log determinant derivative reduces memory (used in logdet computation) from O(m) to O(1) where m is the number of residual blocks. It seems like if these are stored during the forwards pass, then the memory usage for doing so will still be O(m).  I think section 3.3 could use a little elaboration on why the LipSwish activation function better satisfies (2) than the softplus function.  For the minibatching, presumably the same N is chosen for all elements in the minibatch but I didn't see this mentioned in the paper.  The appendix mentions that the number of iterations used in power iteration is adapted so that relative change in estimated norm is small, but doesn't state the exact value. This detail would be important to have. 