The rebuttal addresses my concern on the matrix game. I encourage the authors to improve the presentation of the matrix game example in the final version. ---- The paper proposes a framework for multi-agent RL that relies on common knowledge.  Overall I consider it to be a high-quality paper. The idea of applying common knowledge to multi-agent is important and interesting, and this paper takes a solid step in this direction with modern deep learning techniques. The framework is well presented with a specific architecture that considers using common knowledge in pairwise agents.  The paper is overall well written, but I do have some questions/confusions listed below.  Regarding the results in Figure 3 (middle). It is surprising to me that JAL does not dominate IAC. It would be much better if the authors explicitly illustrate the policies that those methods learn when P(common knowledge) is 0, 0.5, and 1.0, respectively.  Also, visually the expected return of MACKRL is linear when P(common knowledge)>0.5 but wagging when <0.5. JAL is linear and IAC is wagging in [0,1]. This phenomenon is non-trivial and deserves some explanation from the authors.  The last paragraph of Section 3.1 is confusing. It reads “we first of all share network parameters between *all pair selectors* with identical action spaces to greatly increase sample efficiency”. Isn’t it true that we have *only one* pair selector here which is the root of the policy tree. What parameters are we sharing?     