The paper considers an interesting and relatively novel machine learning problem, learning on set functions. It proposes a new architecture called powerset convolutional NN tailed for this problem.  The proposed model is sted from solid theoretical analysis and gets reasonable good experiment results. In every dimension of "originality, quality, clarity, and significance", I think this paper reaches the bar of NeurIPS.  Section 2 is my favorite part of the paper. It is very clear and constructs the powerset convolutions by analyzing the shift-invariant and from easy one to more complex ones. I also like Table 1 a lot. It is really good to see the corresponds between convolutions and their shifts. The author did an interesting analysis of convolutional pattern matching. I am wondering since powerset convolution and graph convolution has different pattern matching behavior comparing to 1D or 2D convolution, does it finally cause some behavior difference between CNN and GCN/Powerset CNN.  The paper also has limitations.  First, its set function representation is very costive. It uses R^{2^N} vector represents a set function on a set with N elements. Thus it is hard to apply this method to a large set. It may restrict its use in real applications.  Second, the experiments conducted on relatively small datasets. For example, even in the real dataset, results are about classification on subhypergraph of size 10. While graph neural network can handle graph with a much larger size. 