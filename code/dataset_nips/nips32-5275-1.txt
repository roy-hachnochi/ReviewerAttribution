NETSOR gives us a unified perspective on Gaussian process (GP) on various wide neural networks.  I think that the framework of NETSOR itself is sufficiently worth to be published.  However, I have still several concerns on GP of (a simple) RNN and recommend authors to more clarify their contribution on it. While feedforward neural networks (FNNs) have different weight matrices layer by layer, RNN uses the same weight matrix throughout the whole of time steps. In that sense, FNN and RNN are totally different architectures and it will be highly non-trivial that the RNN also works as GP.  Therefore, it will be better to deal with the topic of RNN more carefully.   - I recommend Authors to move Section C in Supplementary Material to Section 5 because of the non-trivial property of RNN’s kernel.  When I first read the paper, I was confused whether the following two networks have the same kernel or not; (i) a simple RNN with (Ws^1, Ws^2, … Ws^{t-1}) in NETSOR program 2 and (ii) an alternative network with (W_1 s^1, W_2 s^2, W_{t-1} s^{t-1}) where W_k (k=1,…,t-1) are independently given as a fresh i.i.d. copy of W. It should be better to show Section C in the main text and clarify that the RNN has essentially different kernel from the network (ii) because of sharing the same weight throughout the time steps.   - What happens in simulations of a simple RNN when the time step t (the length of input sequences k) is large? When the time step t is large, dynamics of generic RNNs relaxed to various equilibrium states; convergence to a fixed point, oscillation or chaotic state. It is a natural question whether GP of a simple kernel has any suggestion on such equilibrium states or irrelevant to them.  - RNN includes repeated multiplication of the same weight matrix (e.x. in the case of linear activation function, (W)^t s^1 appear after t time steps) and this is a substantially different point from the previous works on FNNs [13, 29, 31]. As briefly discussed in lines 79—85,   Gaussian conditioning technique can overcome this problem. However, the proof of Lem. 7 is written in a very general style, and it is not so easy to imagine how the Gaussian conditioning deals with the repeated multiplication terms in a simple RNN. It should be better to add a brief overview of Gaussian conditioning specific to the simple RNN in Section D.  --- After the rebuttal --- I keep my score because Authors have sufficiently responded to other Reviewer's comments.