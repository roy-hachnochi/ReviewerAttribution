---- Added after discussion  After discussion with the reviewers that strongly support publication of one of the two submitted papers, we have reached a consensus that the NTK paper should be incorporated into the GP paper. The strength of the combined papers and the originality and substance of the Gaussian conditioning trick have accordingly led me to raise my score to a weak acceptance. However, I am unwilling to strongly support publication because the following items remain unsatisfactory 1) the utility of NETSOR has still not been adequately articulated and 2) the scope of the proof has not been clearly distinguished from the proofs in Lee et al and de G. Matthews et al. However, with the author's rebuttal and discussions with the other reviewers, I have been convinced that ultimately the strength of the contribution is in extending the GP-NN correspondence even if there is not substantially new theoretical content.  ----  This submission focuses on establishing a correspondence between neural networks at initialization (of essentially arbitrary architecture) and Gaussian processes. The primary claim is that any neural network instantiated as a program in the specification language NETSOR introduced by the authors converges to a Gaussian process in the following sense: given Gaussian iid inputs and Gaussian iid initialization the output vector of the NETSOR program converges in distribution to a centered multivariate Gaussian distribution. The result is established using two principal technical tools: first, the NETSOR language itself introduced and used to specify the class of random variables involved; second, a Gaussian conditioning trick originally used in the context of establishing convergence of the TAP equations from spin glass theory is employed to identify the form of the underlying distribution. While I found the general approach of the paper interesting, I have serious concerns about its originality and clarity. What is more, assessments of previous works on the NN-GP correspondence have questioned its significance within the greater context of machine learning because these results hold only at initialization and typically do not provide practical guidance for optimization or training.  *Originality*  The manuscript thoroughly cites previous work on the topic, but, as far as I can tell, fails to go beyond the results in Refs. 28 and 29. Lee et al. establishes the NN-GP correspondence for arbitrary fully connected deep neural networks using a straightforward induction argument. Matthews et al. arrives at the same result but further provides rates of convergence for the limit in the case that the width of each layer grows at different rates. The present paper aims to include a broader class of networks. In fact, the inductive proofs are only provided for linear combinations and matrix multiplications. Appendices A and B argue that all the other architectures can be decomposed into variables that result from these two operations, meaning that the previous work should also encompass these cases.    *Clarity*   One of the main contributions of this submission is its NETSOR language for specifying the architecture of a neural network. Basically, this ends up being a scheme for expressing 1) Gaussian random matrices as "A-vars" 2) Gaussian random vectors as "G-vars" 3) the image of Gaussian random vectors as "H-vars". Both conceptually and also in the proofs I found essentially no advantage to using this scheme. It is typically clear that a given weight matrix is a Gaussian random matrix at initialization, so we do not need another name for it. I found that the use of these names for the random variables in the context of the NETSOR programs did more to obfuscate than to aid the proofs.   The inductive proof by the Gaussian conditioning trick, on the other hand, is clearly written and explicit.  *Significance*  I am not sure that the present results are significant: unlike previous work on this topic, the authors do not seek to motivate the correspondence through an explicit connection to training using GP as a prior. I do not   believe that it is currently common to use the exact marginal likelihood afforded by the GP to do, e.g., hyperparameter selection, though this may be a route to making the present result actionable. Because the results are for networks at initialization, the practical applications are severely limited in my opinion.  *Minor comments*  54: What is the utility of illustrating multi-dimensional output? Isn't it obvious what this means?  79: It would be very useful here to explain in words the conditioning trick and its consequence.   130: give the technical reason for using controlled functions  132: trained neural networks are not Gaussian, so there's no a priori guarantee that the controlled function will be integrable against the limiting parameter distribution after training  206: Why is the output scaling done in this way?