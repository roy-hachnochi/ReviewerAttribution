The paper studied the problem of efficient derivative computation. The  advantage is that the Jacobian is diagonal matrix for the given structure and the vector-Jacobian multiplication reduces to vector inner product.    The major improvement of given method is computation efficiency. In the experiments, why there is no wall-clock time result to show such efficiency? Why an efficient gradient calculation can lead to better convergence point of the optimization objective?   The paper is slightly out of  the reviewer's area but the reviewer cannot get a clear understanding from the paper alone, probably due to some ambiguous notations:  i) On page 2, it says setting h_i = x_\i can recover standard neural net, but doing so gives f_i = \tau_i(x) where standard neural net should be (f_1,â€¦, f_d) =  \tau (x) while here there are d networks? ii) What is g(x, h) on page 2? Is it \tau(x, h)?  In general, the claimed improvements are not clearly reflected in the experiments and the improvement of writing is desired.  ######  I have read the authors rebuttal and make the change of rating accordingly.