It has been previously suggested that the currently massive reuse of common test data sets can lead to overfitting of machine learning models. This work provides new analysis and support for this observation that the similarity between alternative models, following from shared prior knowledge and modeling choices, can reduce the potential overfitting substantially when compared to the theoretical expectations. The paper demonstrates empirically that predictions provided by alternative models are more similar than what could be expected just based on the prediction accuracy and provide an exact quantification of this effect. The authors use this observation to derive modified confidence interval for predictions that incorporates the similarity information. Application is demonstrated with real data.    Quality. The work provides new information on is of good quality; the derivations follow standard style. A weak point is that the discussion is limited to relatively specific data sets / modeling tasks. Earlier work is being appropriately discussed.  Clarity. Overall the text is well written and clear to follow. A shortcoming is that the Introduction does not include any references, and existing theoretical results are mentioned without citation (e.g. rows 19-21).  Originality. The idea that overfitting is mitigated thourh similarity between models is not novel as such but compared to earlier related approaches that are being cited, this work provides new empirical evidence and theoretical understanding of this phenomenon.   Significance. The results are useful for understanding fundamental aspects and limitations of machine learning techniques, and the pragmatically motivated, improved error bounds can be useful in practical applications. The vast applicability across a range of sample sizes, data set types, and modeling tasks remains to be shown, however.