To my knowledge, the concept of performing video frame prediction from a sparse keypoint representation is novel. However, in my opinion, the technical novelty in the paper is somewhat thin. The unsupervised keypoint detector, proposed in [12], is being reused with little modifications. Same thing can be said about the VRNN model.  The differences that I can see are the temporal separation loss and keypoint sparsity loss in keypoint detector training. However, without enforcing temporal consistency and matching (i.e. optical flow or tracking), the keypoints can "jump" around between frames, how do you deal with this problem? Second, choosing a large k (sparsity loss) at the beginning will interfere with the temporal loss, did you observe any issues in the training process? Finally, what is the size of the feature vector in CNN-VRNN? My assumption is that the size of the feature vector in CNN-VRNN would be smaller than a K x 3 vector of keypoints, thus the performance gains could come from a increase in information being stored in the feature vector.  Other than that, I think the authors did a great job to evaluate their method through detailed experiments. I have no comments about the quality of writing and presentation. Although I would prefer a stronger line stroke for the proposed method to emphasize the results in the plots (Figure 3, 4, and 7), and use box plots for Figure 3 and 8. Figure 3 (bottom left) has a small text (XID: 5985036) in the plot. 