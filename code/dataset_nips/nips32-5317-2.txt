This paper provides interesting results for zero-order methods in non-convex optimization. Under some common assumptions, it shows that the Approximate Gradient Descent algorithm leads us to a second-order stationary point asymptotically with probability 1. It also shows that the PAGD algorithm in this paper can speed up the convergence by escaping strict saddle points efficiently. The convergence guarantee matches that of first-order methods, and experiments were done to verify their theoretical findings.  The results in this paper are improvements to previous results, e.g., [JGN+17],  [LPP+17] and [JLGJ18]. Specifically, this paper extends previous results to zero-order methods where one do not have access to the gradient. The results are interesting because the zero-order case is harder than the previous ones.  This paper is generally written in a clear way and it's easy to understand. However, the writing style of the proof sketch in part 4 and part 5 seems a bit different. In part 4, the authors seem to use a lot of theorems and lemmas, but in lemma 5 there are mostly words. I would suggest the authors hide some details of section 4.  Typo: line 270, "i.e. that is" -> "i.e., ".