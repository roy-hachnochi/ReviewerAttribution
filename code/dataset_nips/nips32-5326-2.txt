After author response: I might have been a bit harsh in my initial quantitative assessment and now corrected my overall score accordingly. The authors do correctly point out that earlier papers do not even report how closely they actually fitted the model representations with the generated metamers and thus might well suffer from similar problems as I described.  Also, the paper does provide evaluations on both vision and audition and comparisons to humans and between different networks, which is definitely a broad set of results, which I somewhat undervalued in my initial review. I still think that most results presented in this paper could be expected based on earlier literature. Nonetheless, I would definitely no longer argue for a rejection.  -------------------------- This manuscript presents evaluations for networks which recognize words from speech and for ImageNet trained object recognition networks. In both cases the authors optimize stimuli from white noise to match the representation in a specific layer of the network to generate a metamer for the network. These metamers are then shown to humans or other networks who perform the same task. Generally stimuli are well recognizable and very similar to the original stimulus for early layers and then deteriorate into noise for higher levels, which cannot be recognized by humans or other networks.  The ends of this spectrum were known: low layers loose little information, i.e. metamers are very close to the original stimulus and the final layers are subject to noise adversarial examples, i.e. there are noise patches which give any logits. Nonetheless a more detailed comparison where this change happens and whether there is a range in-between is an interesting question. Also the authors present experiments on different networks and for both vision and audition.   However, I have two larger criticisms of this paper:  1)The stimuli used are not really metamers for the model layers they were trained for. Although the authors introduced soft ReLUs to get closer the activations they produce are sometimes only .9 correlated with the model activations, which is not optimal, as there clearly is the original image, which got the exactly right activations. As the optimization also seems to be harder for deeper layers this introduces an interpretation issue, how much of the effect is due to imperfect optimization.   2) Started by the papers [15] & [16] cited in the paper there is a literature trying to explain crowding, i.e. metamerism in the periphery by deep neural network representations. Papers on this have done similar experiments to the ones in this paper, although they introduced pooling in the periphery and asked whether images are metameric for humans as well instead of only requiring equal categorization. Example papers include:  Deza, A., Jonnalagadda, A., & Eckstein, M. (2017). Towards Metamerism via Foveated Style Transfer. ArXiv:1705.10041 [Cs]. Retrieved from http://arxiv.org/abs/1705.10041 https://openreview.net/forum?id=BJzbG20cFQ Wallis, T. S. A., Funke, C. M., Ecker, A. S., Gatys, L. A., Wichmann, F. A., & Bethge, M. (2017). A parametric texture model based on deep convolutional features closely matches texture appearance for humans. Journal of Vision, 17(12), 5. https://doi.org/10.1167/17.12.5 Wallis, T. S., Funke, C. M., Ecker, A. S., Gatys, L. A., Wichmann, F. A., & Bethge, M. (2019). Image content is more important than Boumaâ€™s Law for scene metamers. https://doi.org/10.7554/eLife.42512  These seem highly relevant for the comparison done in this paper and I think a discussion what is similar or different is warranted.  Overall, I thus believe this manuscript does not contribute enough new insights, given the technical interpretation problem and that there was considerable knowledge about the curves measured beforehand.