# Paper structure - the paper is clearly written and well structured. Although I have the impression that if the reader is not fully aware of the SOTA in this subdomain, or what is standard or what is not, the current SOTA literature review is not enough.  - Some parts need additional explanation, and I will detail below where. In general, I have the feeling that many details have not been addressed carefully enough leaving few doubts about the contribution.   # General comments - The paper makes an interesting contribution, and mainly because, as stated in the introduction, i) a polar transform is applied to the database-retrieved aerial image to be matched  and ii) the spatial attention module. However, these two aspects in conjunction with a non crystal clear experimental setup raise some questions.  - in the polar transform, what are the coordinates at which the aerial image is transformed, and how are those chosen? Are all possible coordinates tested against the ground level image? L60-63 seem to clearly point to the fact that polar transform is applied _before_ the deep metric learning, and therefore the location is not learned jointly.  - How are the aerial images sampled in terms of size and coverage? Is the GPS of the ground image used for a ballpark cropping? How is orientation and alignment chosen? This question arises naturally and I think that depending on the answer further comments might happen, e.g. whether the prior establishment of some of these parameters make the results realistic or not. E.g. in the data used, the pairs showed in figures seem to be nicely pre-aligned, such that the center of the aerial image corresponds to the position of the ground level image. Although this is fine when training, when testing, one does not have this amount of knowledge and images should not be pre-aligned, if not, there is no point in stating that the method is useful for localization. There is a full question mark about the retrieval part, which I hope will be clarified in the rebuttal. On the same line, datasets and experimental setups should be presented a bit in more detail. This point and the one before, make me wonder on whether results are very accurate only because the problem is simplified greatly not direclty by the polar transform, but by the implicit massive reduction of potential false positive rates. I expect a setup like [18], but it does not seem the case?  - The development of the spatial attention module seems a bit arbitrary. Depending on the number of neurons, input images, amount of detail, clutter, etc. A full max-pooling along channels might well result in non-discriminative masking. L170-171 seem to be the main justification of this doing, and it sounds extremely arbitrary, which make the statement at L172-173 a pure hypothesis at this point. From the images in the results, although being able to focus on some features, these seem not to be really that discriminative, as e.g. in Fig 5 left the non-polar transfromed image seem to detect both streets (or the same one just distorted), while the polar transformed image activation seem only to highlight one. I probably miss something here, but it would be nice to have clarification on what is doing what, and, mostly, why.  -  L175-176 states that "we make SPE generate multiple embeddings ... ". How is this achieved? Intuitively, I'd say that several polar transformations are applied, and then features concatenated to perform matching. Still, in what written in the paper this does not seem the case as polar transformation and SPE seem independent modules. This further strenghten my question about how is the polar transformation applied in practice, and how are target orientation and coordinates chosen.  - Would this pipeline really work for natural ground-level images, or panoramas would be needed because of the polar transform? Can this be tested somehow?  - What is the level of "prior" matching needed from aerial images and ground images, in terms of content? Is the method robust  to changes? Aerial images are verly likely to contain outdated information, can this be tested somehow by artificially changing content of current dataset? (maybe once the method is trained?) - Experiments are done properly and ablation studies are good, I just wonder the actual retrieval / matching problem how it influences the final alignment, e.g. if instead of small pre-centered image, only very large and coarse resolution aerial images are available, how the method scales to testing all possible locations for the polar transforms, etc - I wonder if other competitors could be added to the evaluations of another dataset used, since for now this does not really helps in positioning the contribution in SOTA. i see big improvements over the previous SOTA, but it is still hard to figure it out. Since such improvements are so big, and most of the time such massive improvements are suspicious, I'd be careful in motivating even further. Ablations really help in that direction, but might be not enough.   # Specific comments - in the introduction, I'd explicitly state very clearly how the pipeline is applied at test time as well, from the selection of candidates to the final scoring. This information is missing also in the experimental section.  - L171-173 need to be clarified - Section 4.datasets. data should be described a bit in more detail, although are freely accessible. the fact that so many pairs are established, it seems that aerial images are pre-cropped and pre-centered, which, if true, would invalidate the results in my opinion, because they would be unrealistic. I understand the competitors might evaluate in the same terms and settings, but the final numbers would still be unrealistically high for real world, because the dataset is unrealistic.  - Tab 3 and L269: M is used as "the position embedding map" and cannot be used now to identify counts 