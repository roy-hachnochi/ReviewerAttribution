On the positive side, the paper makes two interesting and effective technical contributions in the polar transform and the spatial-aware feature aggregation. Detailed experiments show that each of the two contributions alone is sufficient to obtain state-of-the-art results and that their combination further improves performance. I especially like the polar transform as it is a neat idea that is simple to implement, requires no parameter tuning / learning, and can be used as a pre-processing step to boost the performance of previous work (as shown in the supplementary material, where Polar CVM-Net (which uses the polar transform before applying CVM-Net [5]) already reaches state-of-the-art performance compared to standard CVM-Net and [12] on both CVUSA and CVACT_val). Given that cross-view localization between ground-level and aerial images is a challenging problem, I am surprised and very impressed by how much the proposed contributions improve performance. The paper clearly advances the state-of-the-art by a significant margin. Both the polar transform and the spatial-aware feature aggregation are technically sound. I am not aware of previous work on cross-view localization that uses either of them.  The paper does a good job in terms of motivating its technical contributions, clearly explaining the need for both of them. The very detailed experimental evaluation further verifies the two contributions by showing the impact of each one individually and combined.  The references to prior work seem adequate (although [Regmi, Borji, Cross-View Image Synthesis using Conditional GANs, CVPR 2018] and [Vo et al., Revisiting IM2GPS in the Deep Learning Era, ICCV 2017] also seem relevant) and the paper does a good job of explaining the differences of the proposed approach to previous methods.  My main point of criticism is the clarity of presentations of parts of the paper, which I believe could be improved: 1) I found the description of the SPE module rather confusing. Looking at Eq. 2, it seems that embedding map M is strongly tied to the input feature map (basically selecting the maximum activation over all channels at a given pixel position). In particular, no additional learning seem to be involved, an impression that is corroborated by Fig. 3. If this is the case, I do not see how the embedding map provides further information that is not present in the input feature maps from Fig. 3. In contrast, Fig. 2 suggests that the SPE module performs a set of convolutions to obtain the embedding map M and then combining this spatial layout with the original input features. This would make more sense to me as I would not see how one would obtain benefits from multiple SPE modules without introducing additional trainable parameters. 2) No information is provided which layer of the VGG16 architecture is used as input to SAFA. If SAFA contains trainable information, then the architecture of the SPE modules used should be specified as well. Otherwise, it will be very hard to replicate the results reported in this paper (unless source code is released). In addition, it is unclear to me whether the weights of the Siamese network from Fig. 2 are shared between aerial and ground-level images. The color coding used in the figure seems to indicate that the parameters are not shared, which would make sense given that the two image sources are not geometrically aligned. 3) In order to produce similar descriptors for warped aerial images and ground-level panoramas, they would need to be aligned. There is a potential rotation ambiguity between the original panoramas and those obtained via the polar transform. However, I do not see how this would be handled by SAFA. Does the paper assume that the transformed aerial image has the same orientation as the original panorama, e.g., that the center of the panorama corresponds to the north direction? 4) Splitting the results on CVUSA and CVACT_val over multiple tables (Tab. 1, 2, and 3) makes it hard to directly compare the different variants of the proposed approach with state-of-the-art results. Combining all results into a single table should make the presentation of the results clearer to read. It should also open up enough space to include the results for Polar CVM-Net from the supp. mat. (which I think add quite some value to the paper by showing that the polar transformation can be used for other approaches as well). 5) Sec. 4.2 states that the CVACT_test set is used for evaluation while the tables only mention the CVACT_val set. Which one is used? 6) [15] (VGG) seems to be the wrong reference for the statement: "[15] aims to learn image descriptors that are invariant against large viewpoint changes." In general, it would be good to cite the conference versions of papers (e.g., [15] was published at ICLR 2015) rather than their arXiv versions.  While the shortcomings listed above impact the reproducibility of the paper under review, and thus decrease its potential impact, I do not think that they are severe enough to justify a rejection of the paper as they can be addressed in a potential camera ready version of the paper. As such, I am recommending to accept the paper.  ----- update after rebuttal and discussion ---- After reading the other reviews and the rebuttal, I still believe that the paper is worth to be accepted.  While I had hoped for some more details regarding SAFA (I agree with R1 that this part is not really described in detail), I am satisfied with the rebuttal and trust the authors to provide more details in the final version of the paper. However, given the lack of technical details in the rebuttal on SAFA, I am reluctant to raise my initial score.  Regarding the concern of a prior on the orientation alignment raised by R2: I also have the impression that the orientation (north direction) of both ground and aerial images seems to be roughly known. I don't think this is much of a limiting assumption though. For the aerial images, it should be possible to get a good estimate of north from other sensors. For ground level images, one could just use multiple orientation hypotheses at test time (for panoramas, this would simply be a shift of the center of the images). While this would come at the price of increased run-time, I don't think that this would be a problem in practice, especially given the significant increase in performance by the proposed approach. However, I think this part should be made clearer.