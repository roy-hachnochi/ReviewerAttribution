This paper studies low rank approximation of the matrix A + Delta, where Delta consists of i.i.d. noise variables. The paper considers the L_1 cost function and makes only a mild assumption on the moment of the noises.   The new algorithm proposed by the author is a column-selection type algorithm. Roughly speaking, the authors first argue that there is only a small subset of columns that have large entries (coming from the tail of the noises). An existing algorithm can be used to approximate these columns. For the rest of small entries, the author argues that using probabilistic method to randomly choose columns suffices.   I think this is a fairly strong theoretical result. While the general framework may not be entirely new (i.e., identify heavy hitters and use a different strategy to deal with heavy hitters), there are quite a few interesting technical innovations, e.g., building specialized concentration bounds optimized for their own analysis. I start to be unable to follow technical details after Sec 2.2 (see below for more questions).   My major concern is that the success probability is only a constant (0.99). It is not clear to me whether the success probability is with respect to the data or with respect to the random tosses in the algorithm (i.e., whether the success probability can be boosted with more running time). If the success probability is with respect to the data, I would consider it a much weaker result. A "standard requirement" for the failure probability is $1/n^c$ for a sufficiently large constant $c$. Technically, it may not even circumvent the lower bound from [24] as advertised by the authors --- information theoretic lower bounds usually are in the form "any algorithm fails with constant probability" and this is consistent with such kind of result.   The most worrying building block is Lemma 2.3 (where a constant failure probability with respect to data shows up). There, only a Markov inequality is used so it seems unlikely to significantly push down the failure probability. This is also consistent with properties of "heavy tail" noises --- proving tail bounds for such noises is usually difficult.   Detailed questions:   1. I am not able to understand Definition R_{A^*}(S) (line 220). Specifically, Q is not explained in the definition.   2. Averaging reduces noises (around Lemma 2.2). It looks this is an important technique but I am unable to see the high level intuition. The paper spent considerable effort to explain a simple fact resembling central limit theorem/law of large number but then it was hand-waiving in explaining how this simple fact may be used.  