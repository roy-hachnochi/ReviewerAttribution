This paper proposes novel neural architecture search method dubbed Petridish which is based on gradient boosting of "weak learners" (i.e. small subnetworks attached to the main network) that are attached to the main network.  Originality: The main contribution of the paper is applying basic ideas from gradient-boosting of weak learners to the task of neural architecture search. This is an original idea, which allows a more guided exploration of the space of neural architectures compared to the random steps done, e.g. in evolutionary algorithms. Most related work is adequately discussed. The connection/differences to NAS methods combining network morphisms with evolutionary algorithms should be discussed in more detail as these explore the search space based on similar steps (modifying a model by small incremental additions) but select steps randomly and not based on gradient boosting.  Quality: The authors motivate and evaluate the main design decisions of the method carefully. A short summary of the main results from the supplementary material in the main document would be helpful. I am also proposing two control experiments in Point 5 which could further strengthen the paper. Only including models with fewer than 3.5M parameters (which rules out e.g. ProxylessNAS) in Table 1 is somewhat arbitrary. I propose to include at least the models corresponding to the ones in Table 2 (SNAS, ProxylessNAS) for completeness.  Clarity: Generally, the paper is very well written and organized. One information missing for being able to reproduce the results (without looking into the code) would be a complete summary of the entire training pipeline of Petridish including data augmentation, regularization etc.   Significance: The proposed work is competitive with other recent NAS methods but does not clearly advance the state-of-the-art in terms of search time, test error, number of parameters of the network, or other dimensions. The main significance of the method is in my opinion that it is not restricted to architectures that are subnetworks of a manually defined supergraph. Thus, it allows in principle a more open-ended architecture search without requiring excessive compute resources (since it still allows for weight sharing). This point, however, is only briefly mentioned in the introduction but not explored more thoroughly later on (e.g. in experiments). A more detailed discussion and some experimental evidence whether lifting the requirement of a predefined supergraph is helpful would greatly increase the significance of the paper.  Overall, the work introduces an approach for NAS which is novel and presented clearly. The significance of this work is at the moment limited to "yet another NAS approach" (albeit with a nice connection to gradient boosting of weak learners). More clearly carving out the unique advantages of the approach would increase significance.   Minor comment:  * The bibliography entry for ProxylessNAS uses a wrong order for first and last name of authors.