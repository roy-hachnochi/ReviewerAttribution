The paper proposes to perform architecture search in the following way. A basic architecture is extended by adding a layer to the side. However during the forward pass that side layer is ignored. During the backward back propagation is used to update the parameters of this layer as if it were contributing during the actual computations. (But gradients are not propagated beyond the layer). Each of the components has an L1 regularised scalar alpha which is also trained that represents their "contribution". These alphas are used in the selection stage to pick the components to add.  To me the similarity is that the weak learner in gradient boosting is selected based on the learning process (the gradient) but this computation did not take part in the forward pass.    The paper is mostly well written and clear. I am mainly struggling with the iterative process. Are the cells extended once or is this done in an incremental growing manner? This is not described properly in the paper and I would like to see a clarification on this. Could you also clarify this for macro search vs cell search. Are the different layers updated all at once or one by one for macro search? How exactly is the coupling done in cell search? Do you share the same alpha parameters across cells?  I think the idea is original, but the evaluation could be improved. Many of the choices were experimentally validated and these results are presented in the appendix. However key experiments are missing.  A problem is that the methods it is compared against all use different search spaces. It is unclear whether the benefits come from growing the model/the search space/the actual implementation of the algorithm. For this reason I think the following experiments need to be included 1. (required) Compare the method to the baseline in which you would select a specific model and grow it by randomly selecting operations. This would show that the selection of operation by using the boosting trick is effective. 2. (required) Take the seed model and scale it up/down until it is equally expensive as the final model. This would show that the architectural changes are actually important and that the performance gains do not just come from the additional capacity.  Additionally it would be interesting to see whether a more advanced model could be further improved.  # Post rebuttal The authors added additional control experiments. Increased the score.