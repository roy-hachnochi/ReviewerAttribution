Post-rebuttal: ---------- While I appreciate that the authors have emphasized the importance of domain knowledge for constructing augmentation functions, I believe it is essential that they demonstrate the effectiveness of the method on other (non-vision) problems. The method, as it is currently presented, is probably better suited to a computer vision venue as opposed to a general machine learning venue.   Pre-rebuttal: ----------  Major Comments ---------- My primary concern with the UMTRA framework is the hand-designed augmentation function. My interpretation of the data generation problem is as follows: Given one (or more, from the same class) data points, x_{1:n}, we would like to generate data point x_{n+1} (or possibly multiple data points) that have the same class with high probability. The approach to this problem taken in CACTUs is to perform this class-conditioned unsupervised learning problem by constructing a representation of the data in which clustering tools can be used effectively.   In contrast to the automatic clustering approach, UMTRA effectively aims to hand-solve this class-conditional unsupervised learning problem. I worry that this is quite a fragile approach to this problem. Moreover, the authors choose to demonstrate their approach on data for which rough approximations of this unsupervised problem are easily solved. In particular, the authors leverage invariances that are (mostly) unique to image/video data to generate samples (for example, flipping an image). I am skeptical that this approach can be easily performed for other domains. Consider two similar, common machine learning problems: credit card fraud detection and spam detection. How could one hand-augment or perturb samples x_i to maintain their status as non-spam or spam? While it is not impossible that the approach presented herein may be valid for general problems by leveraging domain-specific knowledge, I am skeptical that it is generally applicable. The degraded performance of Mini-Imagenet also reflects these fears. I would be willing to improve my numerical evaluation of this work if the authors could show that their appoach is capable of effectively handling problems without obvious invariances.  Overall, I found the paper to be fairly clearly written. The authors could improve clarify by being more explicit about the generating distributions of each quantity introduced.    Minor Comments ----------  - I believe in line 168, epsilon should have variance sigma^2? - It is unclear how much it impacts training if samples from the same class appear multiple times in the few-shot learning problem. While the authors give a reasonable statistical argument that this is infrequent, an experiment showing the impact of this would be interesting. 