  This paper extends previous work for online control of observable linear systems under a strongly stable assumption and online strongly convex losses. The main contribution of this work is new analysis techniques that can exploit the strong convexity of the cost function, and as such allow first order methods to obtain faster rates.   As in previous works, the main theoretical tool is to reduce the learning problem to an OCO with memory problem by considering a specific policy class with finite memory. Results are reused from past papers that argue the sufficiency of this policy to represent arbitrary linear policies (when the strong stability assumption holds). The main result is that the regret upper bound has significantly been improved from sqrt(T) to polylog(T), and the loss function can by strongly convex (with a know condition number).   Given the recent activity on this problem, I think this paper presents an important contribution. The reduction to OCO with memory is a nice idea, and this paper makes another gets more mileage out of this idea. Is strong-stability necessary for your policy class to be close to optimal? Is there intuition there? Also, can the authors provide a description of the analytical innovations that were necessary to apply OGD (and specifically what was the barrier before)?       