The paper concerns linear dynamical systems under the full knowledge of its dynamics, but with adversarially changing strongly convex costs. Two online/iterative algorithms are proposed which attain regret poly-logarithmic in time horizon T. On the one hand, this is somewhat expected as strongly convex losses lead to fast rates in online learning; on the other hand, the problem here is much more challenging as the past actions of the controller affect the future states of the system.  The main trick is to parametrize the controller to depend linearly on the past H noise terms (called "disturbance-action policy" class); this goes almost without loss of generality as it has been shown in [4] that such parametrization allows to approximate any strongly-stable linear policy.   The crucial fact shown here is that such parametrization retain strong convexity of the costs. Interestingly, this is due to the randomness and independence of the noise terms, so that they form a non-degenerate representation basis for controllers being their linear functions. Once strong convexity is established, the remaining part is to reduce to problem to online convex optimization with memory. This leads to two algorithms that attain poly-logarithmic regret: online gradient descent and online natural gradient. While the authors claim they are efficient, from what I understand they still need to compute the ideal cost function (Def. 3.4) and its gradient, which requires taking expectation of the cost over noise variables. This seems easily doable in some cases (e.g., Gaussian noise + quadratic costs), but might be quite complicated in some other cases.  Up to my best knowledge, achieving logarithmic regret in this particular setup is a novel and original result, which should be of interest to (at least) theory part of machine learning community. I wish the authors included an instantiation of their algorithm for some nice setup, such as the case of quadratic cost function with Gaussian noise variables, as otherwise the algorithms look quite abstract. But this in no way diminishes their interesting theoretical result.   This ends the positive part of the review.  I found the paper very hard to follow. The only exception is the introduction, which is very well written and clear. However, once the paper goes into details of problem setting, it becomes dense in definitions and equations with very little accompanying explanation.A large part of it looks like a compressed version of [4], and I eventually ended up reading that paper as well, as otherwise I would not be able to grasp the motivation behind disturbance-state transfer matrix, ideal state and action, etc. I don't think this is how a good conference paper should look like. Furthermore, the proofs are also compressed to an extent that it sometimes requires quite some effort to check their validity (e.g., proofs of Theorem 4.1 and 4.2). To summarize, the paper is too dense to deliver its main idea clearly without the need to read past work.  Moreover, the paper heavily relies on the result obtained in [4]. In particular, the disturbance-action policy representation with its approximation properties, the reduction to OCO with memory, analysis of the algorithms by means of ideal state, actions and costs are not novel, but rather directly taken from [4]. This somewhat diminishes the overall contribution.  Having said that I still think the paper should be accepted if the authors revise the presentation of their results in the final version.  One more remark: the inverse dependence on the bounds on the covariance of the noise sounds counter-intuitive. If anything, I would expect the bound improve when the noise is small. This is probably due to additional factor W^4 in front of the bound which controls the magnitude of the noise. I wonder a more intuitive factor would show up in the bound if the authors assumed, e.g., a (sub-)Gaussian noise.  ---------------------------------------- After reading the authors' response ----------------------------------------  Thank you for posting a detailed rebuttal. All my questions were adequately answered. After reading the other reviews, it seems that I am the only one who complained so much about the presentation. Thus, I believe this might be partially due my smaller expertise in control of dynamical systems, and I increased my score by one to account for this.  I still think that including a simple example of a dynamical system with an instantiation of your algorithm (such as the mentioned example of 1-d system) would be beneficial.