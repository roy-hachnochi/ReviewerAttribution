BRIEF SUMMARY ------------------------- In this work, the authors study the online control of known LDS under adversarially changing strongly convex cost functions. They extend the work of [1] as they deal with noisy dynamics, and generically strongly convex cost functions. However, the regret is still defined comparing with the best linear controller in hindsight. They show that the strong convexity is, as in the non-dynamical case, the key to obtain "fast-rate" i.e., logarithmic regret and they provide two algorithms (OGD and ONG) that achieve it.   The main idea of the proof is to preserve the strong convexity following the approach initiated by [4] as opposed to the SDP relaxation of [9]. It consists in considering an over-parametrized policy, which linearly depends on the previous noise terms, up to lag $H$ that preserves the strong convexity and can approximate any stable linear policy in term of cost. With this Disturbance-Action Policy class, they connect to the Online Convex Optimization (OCO) with memory framework to design the algorithm and derive the regret analysis. They do so by considering "ideal" states, actions and costs, which discard the effect of previous states at a given lag. Such approximation however can be well controlled provided that the system is stable and that the lag $H$ is big enough.  GENERAL REMARKS ------------------------------ The paper is well written and relatively easy to follow even though it uses a lot of material and notions from [4] and [5]. The result is novel and significantly improving over known results (from $\sqrt{T}$ to $\log(T)$.  COMMENTS AND QUESTIONS ------------------------------------- - The result crucially relies on the preservation of strong convexity which was discarded in the SDP relaxation of [9]. It would be great to have a more detailed discussion on this matter, maybe recalling briefly the approach of [9], providing a comparison of approaches and sketching how a disturbance-action policy overcomes the removal of strong convexity. - The Asm on the cost functions are generic as it only requires strong convexity. However, the policy used for the comparator (in the regret definition) remains linear, which makes sense when the cost functions are quadratic (at least in the non-adversarial case). Is there any high-level arguments which could speak in favor of linear controller in the strongly convex case (as a 'good' policy) ?  - I do not fully understand the need for Asm. 2.2 for the OGD regret bound. The LDS is known, and the Disturbance-Action Policy class allows a linear controller $K$ which, under controllability assumption can 'pre-stabilize' the system. As a result, the system is governed by $\tilde{A}$ which is a stable matrix. Could it be possible to replace Asm. 2.2. with a controllability assumption and to use a non-zero $K$ (stabilizing) in Thm. 4.1 ? If not, where does the proof break?  - What is the controller $K$ used in ONG (it does not appear in the statement of Thm. 4.2.) ? I also think that the choice of $K$ should be given in the algorithm (either as an input, either in the two different instances). - It seems that there is a typo in Cor. 4.3. Should Asm 2.3 be replaced by 2.2 ?   REFERENCES ------------------- [1] Yasin Abbasi-Yadkori, Peter Bartlett, and Varun Kanade. Tracking adversarial targets. [4]Â Naman Agarwal, Brian Bullins, Elad Hazan, Sham M Kakade, and Karan Singh. Online control with adversarial disturbances. [5] Oren Anava, Elad Hazan, and Shie Mannor. Online learning for adversaries with memory: price of past mistakes.  [9] Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal Talwar. Online linear quadratic control.