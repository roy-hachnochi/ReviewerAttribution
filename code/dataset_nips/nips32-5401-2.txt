Originality:  The concept of learning to explain is relatively new and unexplored.  I can only think of a few references: some relatively unrelated NLP papers, the L2X paper (which the authors reference), and "TED: Teaching AI to explain its decisions", 2019 [which I do not particularly like]. However, as far as I know, the proposed methodology is novel.  Quality: I really like the direction that this paper goes into, namely combining learning to explain and causality, as well as the experiments.  The related work section is also fairly extensive.  Still, I have a few issues with it:   (a) Like many others, I am a fan of structured equation models and Pearl's  theory of causality, and was a bit disappointed when I discovered deep in  Section 3 that they were not used.  I strongly suggest the authors to clarify  that the paper is about Granger causality as soon as possible.   (b) The uncertainty estimation idea seems disjoint from the main contribution;  I think that the latter would have stood on its own.   (c) From a more methodological perspective, I really dislike that:     (1) feature contribution is determined by masking features one by one;  this    is essentially equivalent to assuming that feature contributions are additive,    which is *very* unlikely to be the case in practice, especially for    very non-linear models like deep nets.     I realize that this is computationally advantageous, but it is a scientific    duty to point out that it is an extreme approximation, probably worth    revisiting in the future.     There are also techniques that tackle similar problems in the literature, and    it may be worth to point the reader in their direction, e.g.:       "VOILA: Efficient Feature-value Acquisition for Classification". 2007.     (2) Replacing a masked value by a point-wise estimation can be very bad,    especially when the classifiers output changes based on the masked feature.    Why would the average value (or, even worse, zero) be meaningful?     It would have made sense to formalize this step better, for instance by    replacing a point-wise estimante of the response variable with a distribution    instead, like done in:       "Quantifying Causal Influence", 2013 (a masked feature is replaced by its marginal distribution)       "What to Expect of Classifiers? Reasoning about Logistic Regression with Missing Features", 2019.      Again, it is a matter of pointing out that alternative can be conceived.     (3) It would also be interesting to compare the proposed method with causal    inference technique for SEMs, or at least mention them.  I assume these to    be more precise but also fatally slower.     (4) It seems to me that the chosen performance measure may correlate much    more with the Granger-causal loss than with the objectives optimized by the    other explainers.  This would bias the experiments.  The authors should    explain why this is not the case, or fix the performance measure otherwise.   Clarity:  The ideas are presented clearly.  Significance:  This paper has a lot of potential.