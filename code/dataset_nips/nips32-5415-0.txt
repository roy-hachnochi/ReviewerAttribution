QUALITY I like the approach but I have one fundamental problem in understanding the method. The policy is a mapping from S x G -> a while the inverse dynamics map from (S x G) x (S x G) -> a. How are pi and theta equal and how are they parametrized?  In the classical sense, when learning the inverse dynamics, this problem is more about the environment and not the policy, if I am not mistaken. If so, why is it important to relabel data to learn them?  Also the combination of PCHID with PPO is not fully sound (as is noted in the paper) but could be solved by just exchanging PPO with an off-policy algorithm, for example TD3.  Nonetheless, the experimental results section shows some interesting results with much improved performance and sample-complexity over standard DQN, HER and PPO.  CLARITY The paper is well written and structured, only some sentences need some proof reading for some expressions. Regarding my lack of understanding the difference between a policy and inverse dynamics, maybe some more words are necessary to better link these two functions.  In the grid world setting, k-step solvability is straightforward to understand and implement while in continuous action spaces it is not so straightforward. Can you comment a bit more on how to determine k-step solvability in continuous action domains?  ORIGINALITY The paper introduces some fundamental concepts and combines them into a novel algorithm type.  SIGNIFICANCE Improving sample-complexity is of high importance to the field and the results look promising.  #####################  Post rebuttal  The authors clarified my main question on the connection between inverse dynamics and the policy. The other questions were also answered appropriately. Therefore I would keep my current rating and increase my confidence score.