The paper is interesting and notable for tackling the difficult case of using recurrent neural networks to do learning.  The function class it learns seems to be identical to the function classes learned by Allen-Zhu et al. in prior works, which are known to be learnable in polynomial-time via other methods.   The main technical difference from this paper and previous work (per the authors) seems to be Lemma 5.2b.  What happens if the inputs are not norm 1? What sort of complexity bounds are obtained in this case? Also, how does the complexity scale with the lipschitzness/boundedness of the loss function (currently assumed to be 1)?  Are there hardness results indicating that your results are best possible? For example, the 'almost polynomial' result of L^{O(log 1/\eps)}, is it possible this can be improved?  Can you give some explanation as to what this function H is from Lemma 5.2? Roughly what techniques are involved?   Why is "n" considered a noise parameter? I did not follow.  The main concern is how much does this paper overlap with several other works analyzing SGD on overparameterized networks, most notably the work of Allen-Zhu et al.  The authors indicate that the proof here is substantially different and more difficult. 