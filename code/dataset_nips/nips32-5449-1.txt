This paper show that Elman RNNs optimized with vanilla SGD can learn concepts where the target output at each position of the sequence is any function of the previous L inputs that can be encoded in a two-layer smooth neural network.   There are multiple assumptions and complications in showing the main result. The crux of the proof is to show that if the RNN is overparameterized enough, then if we start from a randomly initialized RNN matrix W, there exists a function which is linear in matrix W* whose value at a specific W* is a good approximation to the target in the concept class. Showing that SGD moves in a direction similar to such W* gives the desired result.   Another interesting aspect of the main result is that the number of samples that SGD needs depends only logarithmically with respect to the number of RNN neurons, making it applicable to overparameterized settings. Indeed, for the result to hold, the number of RNN neurons must depend polynomially (or slightly more depending on the complexity of the concept class) on the length L of the sequences.  Pros: - Novel result for RNN learnability with generalization bound polynomial in input length. - Sample complexity bound almost independent of size of the RNN - Different properties of RNNs at random initialization (not sure which of these are novel when compared to [2])  Cons: - RNN is so overparameterized that there's a quick shortcut leading close to the optimal solution - Similar in vein to [2]  Other: - Can you give some concrete (possibly practical) examples of functions belonging to the concept class? For example does the concept class allow to count, or, say, determine whether an a^nb^m sequence is such that n=m? - Can you give an intuition on the function complexities C_\eps and C_s? 