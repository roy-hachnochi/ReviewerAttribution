The authors studied the sample complexity of semi-private learning a VC class. The paper is well-written and clear.  The first main result is that any VC class can be (agnostically) learned with VC(H)/alpha^2 private data and VC(H)/alpha public data. The algorithm is the same as [BNS13], which studied the realizable case, but the analysis in this paper is novel.  The second main result is that when H has infinite littlestone dimension, then any private learner must have at least 1/alpha public sample complexity, therefore the upper bound is nearly tight. The proof is based on a result from a recent STOC paper and a new "public data reduction lemma". As a consequence of the lemma, the authors showed a dichotomy of pure semi-private learning.  Overall, this paper provides solid and near-optimal theoretical results on semi-private learning.  I suggest acceptance.