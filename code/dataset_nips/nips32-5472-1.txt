This paper address the problem of sparse reward learning where a biased but helpful distance-to-goal metric is available at every state. The difficulty in solving these problems that directly optimizing for minimum distance to goal can often lead to poor local minima, but only relying on the sparse reward is too data inefficient for practical use.   Originality: The paper proposes using "sibling" trajectories to avoid falling into local minima. Sibling trajectories are trajectories that share the same goal, start state, and policy. Instead of optimizing the distance to goal, each trajectory is relabled with an "anti-goal", taken from the sibling trajectory, and the reward is to minizme distance to goal while maximizing distance to anti-goal. This method would increase the diversity of states visited in order to find optimal paths. However, this new reward function is a heuristic and is not guaranteed to return the optimal policy with respect to the original sparse reward. The method is novel to my knowledge.  Quality: The method performs very well in the maze and ant experiments, which compare to HER and ICM. I would be curious to see how a maximum entropy RL method would perform on these environments (SAC/SQL). The Minecraft environment performance is not compared to any other method, this should be corrected. The SR method does not seem to hurt final performance compared to other methods. Unlike most Goal-conditioned RL problem statements, this method does aim to solve the any-state-to-any-state problem, and does not compare to these.   Clarity: The "Self-balancing reward" section is not clear. The purpose of the derivation is unclear given that the authors can only hypothesize that "the original gradient definition [..] obeys a similar relationship" with what they are optimizing. Other than that, the paper is readable. A better explanation of how this method avoids local minima that discusses the difference between V(s,g) and d(s,g). Does this method do anything more than simply increase the diversity of states?  Significance: The problem of only having heuristic distance functions that do not directly correlate to the state value is a significant one. It would be useful to experiment with how bad a distance can be where this method still works. Overall, I find the method interesting.  However, the method assumes that use can choose what initial state to reset the environment to (in order to generate sibling trajectories from the same s_0). This property is not generally assumed for MDPs, and may lead to this method being difficult to use in the real world, or in environments that have much wider initial state distributions. It would be good for the authors to discuss this assumption.  ----------- After the rebuttal and discussion, I have decided to increase my score to 7. While the resetting is a big assumption, other papers have made this kind of assumption and it is feasible for a number of settings. The experiments cover a large variety of types of environments. I could personally compare to the method in my future work. I think this work is interesting, as long as the authors clarify the method section.