Overall an interesting paper. However, there are several issues that I would recommend the authors address. Most of the issues revolve around the robustness of their approach.   In particular, there proposed approach critically depends on the support superset recovery, in order to proceed to the next step of signal recovery. Misidentification of the support will result to incorrect recovery. Furthermore, the larger the identified support the more measurements will be required to fully reconstruct the signal based on a larger support. It is not clear how group testing behaves in the presence of measurement noise. Furthermore, quite often practical signals are not exactly sparse. i.e., the support might be the whole signal even though the signal is sparse because the coefficients decay very fast. Thus, some are approximately, but not exactly, sparse (often known as compressible). Images are such an example. Is group testing robust to this? Will the support returned represent the largest coefficients? If the group testing fails, so will the rest of the process.  Another issue is that, it seems, the process requires knowledge of k, to design the group testing. This might often not be available, (admittedly, also a problem with some conventional algorithms, such as the BIHT). However, conventional algorithms seem to be more robust to lack of knowledge of this parameter than group testing ones. Is that an issue?  In addition, the experiments are not as convincing as I would expect. The improvements are modest, if any, to justify the extra complications of the two-step process. Also, I would recommend the authors include a line (i.e., a third experiment) in which the superset method matrix is used whole with BIHT (i.e., without the support recovery step first). It might be that the matrix they designed is actually better than a fully Gaussian matrix, even when used with conventional methods.  ===== I've seen and taken into account the author's response, and it does not change my score. 