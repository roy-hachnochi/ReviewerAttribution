This paper studies proper agnostic learning of the class of large margin halfspaces. Prior to this work the best known result of [BS00] proposed a proper learning algorithm that achieves error OPT_gamma + eps in time poly(d/eps)2^O(log(1/eps) gamma^2). The current paper shows that if one is happy with a PTAS, i.e, (1+delta)OPT_gamma + eps, then there is a proper robust learning algorithm that uses O(1/eps^2 \gamma^2) samples and runs in time poly(d/eps)2^O(1/\gamma^2) (for constant delta). The paper also shows a matching lower bound that any constant factor approximation that is a proper learner must incur exp(1/gamma) run time.   Techniques: ---------------  The key algorithmic insight is that if the current guess w has a large error, namely more than (1+delta)OPT then there must be many examples where w has margin less than gamma/2 whereas w*, the optimal halfspaces has margin more than gamma. This implies that w-w* has a large projection onto the matrix M = E[xx^T] where the expectation is over examples where w has low margin. Hence, in time exp(1/gamma^2) one can try all large projections onto the subspace, add them to the current vector to produce a new set of candidate vectors and proceed. One then needs to argue that the depth of this search tree is small.  Overall the paper is well written and makes a solid contribution towards the understanding of proper agnostic learning of large margin halfspaces. 