1. The dependence of SNR is extreme. It scales as an exponential function. I wonder whether it only occurs in the proof or a fundamental limitation of the approach. The authors did not provide a empirical comparison to any competing method even to [27] on which the presented algorithm improves. It would be interesting to see how the algorithm competes with the state-of-the-art in its empirical performance particularly in the presence of noise.  2. The statement of Theorem 3 hid the dependence on L by assuming L = O(1) outside the theorem. Isn't the proof providing any dependence on L?  3. Some key definitions are missing. For example, \epsilon-precision is not clearly stated when it first appears in line 194. The authors can add just a single equation for the definition.  4. In Algorithm 1, it has not been clearly stated how a k-sparse vector is recovered from 2k Vandermonde measurements. It suffices for the identifiability. But the recovery by a practical algorithm requires more measurements. While algorithm and how many more measurements suffices for its guarantee need to be clarified.  5. In Algorithm 1, step 6, how does one choose L unique results out of L\log(Lk^2)? 6. The proof of Theorem 1 looks interesting but unfortunately I was not able to fully understand it. Some critical steps need more explanation. Line 154: does the interval here need to belong the set of positive real number? Can it include any negative numbers? How does draw the consistency in Line 155 from the above line. This step does not look obvious to me. I think the consistency is the key to draw the main conclusion but I lost the track at this point. Hope the authors can elaborate on their proof.  7. The min-distance estimator in Line 15 of Algorithm 2 is one of the main ingredient and needs to be included in the main text not in the appendix.  8. The paragraph after Lemma 2 is not very clear. In line 225, the authors say that the estimation of means in the scalar Gaussian mixture can be obtained up to a finite precision. But anyway \beta_i's are assumed to be of a finite precision. In the next line, the authors assumed the knowledge of exact means without any error. I am not following the flow of logic in this paragraph. In the statement of Algorithm 2, in many locations it is assumed that we have access to error-free estimates of oracle information, which are inner products of v_i and \beta_j. It would help if the authors can confirm that these exact estimates available by a practical algorithm by Lemma 2.  9. Finally, continued from #1, it would be interesting to see (even without a comparison to any other method), how sensitive the algorithm is to additive noise. I am curious about the empirical performance of a component that estimates scalar means of a Gaussian mixture in SampleAndRecovery per varying SNR.  