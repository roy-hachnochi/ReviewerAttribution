Update: I read the author responses and the other reviews. Overall I saw nothing that substantiated changing my score: reproducability / accessibility is an important component of impact to the community. I acknowledge the importance towards representation learning, which was one reason for my original score.  ------  Overall the paper is very well written and technically correct. It was enjoyable to read.  However, the contributions are more or less incremental and this work to me represents a technical report / demo on what happens if you scale up the architecture of BiGAN. It is incremental as it takes well-established ideas (BiGAN + BigGAN) and combines them in a straightforward way that would be only available to those with the most compute.   Making models much much bigger has become popular lately. BERT showed that using a very large transformer on a masked language model achieves SOTA results on important NLP benchmarks (model / approach novelty and SOTA). BigGAN showed that very large GANs can be used to generate high-resolution datasets reliably (high impact as this was the first instance of full imagenet generation with GANs). GPT-2 showed that large RNN models along with lots of data could generate uncannily realistic paragraphs (high impact as this was the first instance of language generation of this sort at this scale). All of these works were important as they challenged expectations of what was possible as we scale neural networks up.   And now there's BigBiGAN. Which is mostly just as the title describes it: Big + BiGAN (or BigGAN + BiGAN if you will). BiGAN is important, I suppose, as it demonstrates a particular paradigm of adversarial learning, but its significance isn't really well established in this work beyond "good linear probe classification performance". I would say that the modifications to the discriminator are useful for training BiGANs and worth exploring more. But it's not entirely clear just making BiGAN big is really an important step forward toward answering some fundamental question in ML other than "what if we make it big?". This question used in this way is not very useful to those outside researchers that can actually train these models.  As a tech demo, this paper is excellent. However, I can't imagine it having much significance in the community other than hitting a benchmark as most researchers cannot reproduce these results. So the reproducability score on this paper is naturally low (though the techniques are described very clearly). Why not provide a more accessible demo across some of the models being compared with smaller datasets / models (i.e., reimplement some of those models and train them on similar settings)?  Everything above sounds very negative, but overall the paper is very good and I think it deserves to be accepted.   It would also help though if the representation learning / self-supervised works mentioned / compared to weren't just from DeepMind / Google, as there are many other relevant works out there: e.g., Noise as Targets (FAIR), Deep Infomax (MSR), and Invariant Information Clustering (Oxford) to name a few.  One question: how do the generative results compare to VQ-VAE-2? Do you think VQ-VAE-2 outperforms yours in classification?  ------ One last thing, which *did not* affect my score: The claims of SOTA are already dated, but I understand that the authors did not have access to these works at submission. But I would expect the message to be augmented and these works to be at least mentioned for camera ready, if this paper gets accepted, in order to not mislead (which is important):  Big CPC: Data-Efficient Image Recognition with Contrastive Predictive Coding CMC: https://arxiv.org/abs/1906.05849 both of which get scores close to yours without a generator. AMDIM: Learning Representations by Maximizing Mutual Information Across Views which gets much better results than yours (68.1% on imagenet)