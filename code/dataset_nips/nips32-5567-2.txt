Though there isn't much new technical content in this paper, I think it provides impressive empirical motivation for continuing to explore generative models as mechanisms for representation learning. The authors demonstrate the power of ALI/BiGAN as a technique for representation learning by reimplementing and re-tuning the core idea using the biggest, strongest current GAN model (i.e., BigGAN). The result is a model which matches the representation learning performance of concurrent self-supervised methods, while also improving on the prior state-of-the-art for unsupervised GAN-type image generation (by beating BigGAN).  The empirical results are impressive. My main concern is that the work is generally unreproducible due to massive computation costs. Nonetheless, I think the work is worth publishing since it will encourage people to keep developing generative approaches to representation learning, some of which will hopefully be more efficient and practical to work with.  The paper was clear and easy to read, which is a plus.  Post Rebuttal: I found the authors' response to my statements about model size a bit misleading. I think their main result is the "state-of-the-art on ImageNet" result, and the encoder they use for this result is the RevNet x16 model from the CVPR 2019 "Revisiting Self-Supervised Visual Representation Learning" paper. That model is massive, and has perhaps 10x the compute cost of the ResNet50 to which they refer in the rebuttal. Additionally, training the encoder requires training the rest of BigGAN too, which isn't exactly fast and cheap.