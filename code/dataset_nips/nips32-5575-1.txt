The procedure for generating SAT instances is original as far as the reviewer knows (although the reviewer is not very familiar with related literature). The proposed method seems technically sound. The empirical evaluation seems fair: the results are compared to several relevant baselines from the literature and the generated SAT instances seems to align better with the data they are trying to mimic on most of the metrics, including the ranking of industrial vs random SAT solver performance. It would be nice to see actual run-time numbers for the SAT solvers - even though the argument in lines 263-264 suggests that post-processing can significantly change these numbers, the reviewer thinks that the results should be still useful to the reader. The application of tuning SAT solver hyperparameters seems useful. It is unclear how much learning contributes to the success of the method - comparison of variations of the method with a higher or lower capacity GCN would be useful to gauge how much learning occurs on the rather small dataset of SAT instances. The paper is clearly written, although the reviewer didn't find exact description of the networks and optimization procedures used for training in the submission. The reviewer thinks that the paper explores an interesting problem at the intersection of SAT solving and machine learning, and future work is likely to build on the ideas presented in the paper.