This paper studied the proximal policy optimization (as well as TRPO), where the policy and action-value function are parameterized by two-layer neural networks. Due to the overparameterization of neural networks, the convergence rate for the policy improvement and policy evaluation can be analyzed. The overall convergence of the PPO algorithm then leverages the global convergence of infinite-dimensional mirror descent.   Though I found the paper provides a valid of proof of convergence for PPO algorithm, I don't find the proof techniques to be novel enough. It appears that the entire proof lies in putting together several pieces of techniques/proofs that are already in existing work. More specifically, the proofs for policy improvement and policy evaluation are straightforward adaptations of the existing proofs for supervised learning over overparameterized neural networks. The overall convergence adapts the existing proof for mirror descent algorithm. The error propagation adapts the proof for the neural network Q-learning. Hence, the particular contribution in this paper is not very significant.  More specific comments are as follows.  - The proofs for the policy improvement and policy evaluation require taking average over the random initialization. However, the statements of the theorems (Theorems 4.5 and 4.6) do not include such an averaging. Please explain.  - Typically, PPO even under linear function approximation may not convergence to global minimum. Is the convergence here due to the sufficiently large function class (i.e., Assumption 4.3)? 