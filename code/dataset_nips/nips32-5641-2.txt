The paper is well written and complete with theoretical proof and experimental results. -  The authors present theoretical analysis and experimental proof for modification  of the gradient descent algorithm (for non-overlapping CNN case) called approximate gradient  descent. The proof desmonstrates that, with high probability, the proposed algorithm with random initialization grants a linear convergence to the ground-truth parameters up to   statistical precision. The results are applicable in general non-trivial, monotonic and Lipschitz continuous activation functions including ReLU, Leaky ReLU, Sigmod and Softplus etc. The sample complexity beats existing results in the dependency of the number of hidden nodes and filter size and matches the information-theoretic lower bound for learning one-hidden-layer CNNs with linear activation functions, suggesting that the sample complexity is tight. 