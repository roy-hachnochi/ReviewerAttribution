This paper shows how regularization is a process with critical periods rather than a smoothing part of the loss function. While exploiting the same intuitions as a previous publication [1], it is clearly geared towards DNN practitioners, and hopefully theoreticians (rather than biologists) with a tutorial-like quality and great clarity.  The connection (in related work) with stereo-view reconstruction is particularly well written and illuminating.  While I have seen this issue being discussed, I have not come across a paper exploring so thoroughly the issue before, with a systematic exploration of all the cofounding factors:  data distribution, learning rate, batch normalization.  This is also the first time I have seen one dares to state, in the appendix to be cautious “That means that decades of theory of regularization from Tikhonov onwards do not apply to Deep Learning, and we need to rethink generalization.”  The Fisher Information analysis seems similar to [1] but reaches sometimes opposite conclusions (correlation between Fisher and generalization). Figure 5 center, showing how the FIM trace max value correlates so well with test accuracy, is so counter-intuitive it could trigger new research if confirmed.  One puzzling omission is any form of layer-wise analysis that could have better explained the strange results observed with FIM. The authors should explain why they are not showing such analysis (have they tried it?).   Another decomposition, less trivial to try, would be sample-wise. Motivated by Fig.(2) in https://openreview.net/forum?id=SJfb5jCqKm, one could split examples along their confidence scores for a more detailed analysis. 