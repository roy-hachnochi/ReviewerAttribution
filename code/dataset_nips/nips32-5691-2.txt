The paper provides novel observations in terms of deep network regularization. The observations in the paper suggest that, for deep network training when to regularize the networks is critical; applying regularization at different phases of the training can yield different outcomes, and there is a critical period during the initial epochs of training which the regularization is more effective and after that, regularization may not have any benefit in terms of regularizing the networks to generalize well. These empirical results allow the authors to challenge the traditional view of regularization: altering the landscape of local extrema to which the model eventually converges. Basing on the new observations, the authors suggest that the “critical regularization period” of the regularization directs the training towards regions with multiple extrema with similar generalization properties. To support these claims, the authors conducted experiments with weight decay and simple data augmentation methods (i.e., random cropping and horizontal flipping) using two CNN architectures: ResNet-18 and All-CNN.  The paper is very well written and easy to follow. The novel view that the timing is critical for regularizing the deep learning networks could have significant impact on the research field. The experiments conducted make sense to me. In particular, the observations provided in the paper could motivate researchers to focus on the transient rather than asymptotic behavior of the learning of the networks.   On the other hand, experiments on more datasets, using other popular regualarizers, such as dropout and more sophisticated data augmentation methods (such as out-of-manifold regularizer Mixup[Mixup: Beyond empirical risk minimization, Zhang et. al.] and AdaMixup[Mixup as locally linear out-of-manifold regularization, Guo et. al.]), and with learning architectures beyond convolutional networks could significantly improve the paper. Also, some parts deserve more discussions such as the different behaviours of weight decay and data augmentation on Page6 in the section “Sliding Window Regularization”.   Questions to the authors:   1. Is the critical regularization period correlated with the training loss? In this sense, it would be useful to plot the training loss curves of the models in the experimental section as well.   2. Is the critical regularization period data dependent? And how one could find this period?   *********************after the rebuttal*********************** I would like to thank the authors for the rebuttal.   The authors’ responses address most of my concerns. I like the additional experiments on two more data sets (i.e., SVHN and Imagenet) and on a recent Out-Of-Manifold data augmentation approach Mixup/AdaMixup. Also, the further clarification/discussion regarding the critical periods of regularization is also very helpful.  I like this paper.  ***************************************************************** 