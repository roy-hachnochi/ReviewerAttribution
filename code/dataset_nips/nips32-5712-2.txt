Originality: the effort to predict links in dynamic graphs through a combination of variational autoencoder, graph neural network and recurrent neural network is the interesting contribution of the paper. I have some doubts about the formulation that subtracts from my evaluation of originality.   - As I understand the paper, X represents node attributes, A represents dynamic network, Z represents the code of all X and A up to the current time, and h represents a latent state of all X, A and Z up to the current time. Then what additional information does Z_t provide in Eq. 4, since in Eq. 2 Z_t contains the same amount of information as h_{t-1}? - In Fig. 1 (b), the code Z_t is only used to generate the dynamic network A_t, not the node attributes X_t. This is not symmetric if we consider that Z_t encodes both X_t and A_t in Fig. 1.  - In the formulation, Z_t has the same dimensionality as X_t, meaning that Z_t is a code for each node. This doesn't necessarily need to be so. - In the experiment, I would like to see more detailed analysis of what GCRN gives us, in comparison with other neural network algorithms, as well as non-neural-network algorithms.   Significance: This paper has significance as an application paper. But for application paper, I would like to see deeper analysis of what GCRN gives us in comparison with other algorithms. In particular, some visualization and comparison at the link level might be very helpful. The variational solution of evidence lower bound is pretty standard.  ###  Thanks for answering the key questions in the serious rebuttal! While I am still not totally convinced about novelty, I believe the authors will seriously revise of the submission and address reviewers' concerns if it is accepted. As a result, I would move my overall score to 6. 