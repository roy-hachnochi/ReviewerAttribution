Originality The paper is a combination of a number of ideas in the literature, where a careful combination of existing techniques leads to really good representation learning for graphs. In that sense the work is original and interesting.  -----POST REBUTTAL----- I thank the authors for addressing my concerns / questions around a VAMP version of VGAE as well as questions around Eqn. 5. In general the rebuttal seems to include a lot of relevant experiments for the concerns from the review stage, and based on this evidence I am happy to keep my original score for the paper. Clarity The paper is generally clear and has clear mathematical formulations written down for all the methods considerered.   Quality The paper has a number of thorough experiments and generally seems to be high quality in empirical evaluation. It also has a clear intuition for why the proposed method is better and extensively demonstrates and validates it.  Significance The paper seems like a significant contribution to the graph representation learning literature.  Weaknesses - It would be good to better justify and understand the bernoulli poisson link. Why are the number of layers used in the link in the poisson part? The motivation for the original paper [40] seems to be that one can capture communities and the sum in the exponential is over r_k coefficientst where each coefficient corresponds to a community. In this case the sum is over layers. How do the intuitions from that work transfer here? In what way do the communities correspond to layers in the encoder? It would be nice to beter understand this.   Missing Baselines - It would be instructive to vary the number of layers of processing for the representation during inference and analyze how that affects the representations and performance on downstream tasks.  - Can we run VGAE with a vamp prior to more accurately match the doubly stochastic construction in this work? That would help inform if the benefits are coming from a better generative model or better inference due to doubly-semi implicit variational inference.  Minor Points - Figure 3: It might be nice to keep the generative model fixed and then optimize only the inference part of the model, parameterizing it as either SIG-VAE or VGAE to compare the representations. Its impossible to know / compare representations when the underlying generative models are also potentially different. 