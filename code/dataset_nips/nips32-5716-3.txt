### edit after author response ### I read the feedback and found the additional results impressive.  I am still uncertain about \psi: l116 says "implicit prior distribution parametrized by \psi" (I assumed this means q(Z|\psi)) and "reparametrizable q_\phi(\psi|X,A)" and the feedback states \psi is \mu and \Sigma.  I think Gaussian random variable Z is reparametrizable and q(Z|\psi) is explicit. Since the major strength of this paper is the empirical performance, the clarity of method/model description and experimental setups (mentioned by other reviewers) are very important.  I hope the reviews are helpful for improving the presentation. ########################### This paper presents SIG-VAE for learning representations of nodes in a given graph.  This method is an extension of VGAE in two ways.  The use of semi-implicit variational distribution enriches the complexity of variational distribution produced by the encoder.  The decoder uses the Bernoulli--Poisson likelihood to better fit the sparse link structure.  Experiments in Section 4 compares SIG-VAE with many modern methods on various tasks and datasets.   My questions and suggestions are as follows.  * The reviewer failed to comprehend the connection between the semi-implicit distribution in Eq.(2) and the following part lines 122--142.  Specifically, an absense of \psi in Eqs. (3,4) is confusing.  Is \epsilon_u ~ q_u(\epsilon) interpreted as \psi ~ q_\phi of Eq. (2)? If so, \psi conditioned on X, A is misleading information.   * Figure 1 is not informative either.  The caption says 'diffuses the distributions of the neighboring nodes', but VGAE already achieves it: the latent representaiton is propagated to adjacent nodes, and it infers distributions.  What does SIG-VAE attain on top of it? The illustration is also obscure.  Does this tell that the distribution of latent representaiton of each node can be multimodal and that the neighboring distributions influence the distribution of certain nodes?  * CONCAT operators are unclear in Eqs. (3,4). In particular, I want to know the size of \epsilon_u and h_{u-1} to see what information is added to X.  After checking the code, I assumed \epsilon_u is concatenated to node feature for each node and layer, but not convinced due to unfamiliarity with TensorFlow.  Why is X fed in every layer? For u>1, h_u seems to carry some information on X.   * Dots in middle and right panels of Figure 2. Suppopsed the latent representation is infered as distributions, what are the dots in the figure? Mean values? As visualized in Figure 3, the distributions may take multiple modes.  I am curious if it is okay to shrink such distributions to a single point, though the full visualization is challenging.   * Evaluation on graph generation task.  Density and average clustering coefficient are adopted as metrics of the graph generation task in Section 4.3.  While these scores indicate the efficacy of Bernoulli--Poisson likelihood for sparse edges, they may not fully characterize graph structures.  Using the following metrics may further strengthen the result:  MMD-based score in Section 4.3 of [a], and KL-divergence between the degree distributions [b].  [a] J. You et al. GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models, ICML 2018. [b] Y. Li et al. Learning Deep Generative Models of Graphs https://arxiv.org/abs/1803.03324