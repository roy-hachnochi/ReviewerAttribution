1. The authors propose an unsupervised representation learning task that - a) first predicts a fixed number of K keypoints (unsupervised); b) takes two frames and erases these keypoints from the features of frame A. copies the keypoint features from frame B into frame A. constructs a final feature map consisting of the modified features of frame A and keypoint features of frame B. c) a small CNN uses the feature map as input to reconstruct frame B in pixel space (l2 error). This task is interesting as the representation itself is learnt via the "feature swapping" trick guided by keypoints. 2. The experiments in the paper are geared to show that the design decisions taken by the authors are empirically valid - either in terms of how long/accurate the keypoint tracks are, or how it helps in sample efficient RL. These are two solid directions but neither answer why this task should work at all. For example, in unsupervised or self-supervised learning, most tasks have a "trivial" solution where the network learns low-level information not useful for end tasks. It is not clear what (if) the trivial solutions of this method are? The feature swapping trick must introduce discontinuities which might lead to trivial solutions?  3. Analyzing the effect of the bottleneck: How is the number of keypoints K determined and how does that effect the final performance? Is it better to set a slightly larger K in the hope that the model finds multiple useful keypoints? I think one of the simpler baselines to compare against is a denoising auto-encoder which does not have such a keypoint bottleneck but still uses a similar objective (l2 reconstruction error) to learn the representation. This will show why the keypoint bottleneck is necessary and what properties it brings. 4. Comparisons with other unsupervised feature learning methods: Methods such as [24] use a forward prediction error (curiosity) objective to learn a representation. This has been shown to be a powerful intrinsic reward. The authors should discuss their own method's empirical performance as well as theoretical comparisons. 5. Analyzing the effect of the feature representation better: I like the last experiment (Figure 7) that looks at the most controllable keypoint. I think the paper can be made stronger by considering a setting where the features themselves are kept fixed while the agent (Figure 5: Recurrent Agent) is learned.  6. Please also aim to release your code and implementation. RL algorithms are hard to reproduce.