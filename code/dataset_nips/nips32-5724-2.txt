Originality:  The proposed approach is a novel combination of well-known techniques such as RL and GAN for recommendation. Related work has been adequately cited. It is clear how the proposed approach differs from the existing literature.    Quality: The approach appears to be technically sound. The theoretical analysis and the experiments support the claims.  This is a complete piece of work. The experiments and results sections are quite terse. I wonder if authors could have identified any more experiments relevant to their work and cut down some of the theory/derivation (or, move them to the supplementary section) to make space.   line 271:  We randomly selected 65,284 sessions for training and the left 3,437 for testing. Were the parameters tuned on the test set? Did you have a Validation set?    Clarity: The paper is mostly well-written except for some typos:  line 32: we Â **exploring framing** recommendation as building reinforcement learning (RL) agents  line 37: Classic model-free RL applications requiring collecting large quantities of interaction data with self-play and simulation. line 39: In contrast, such methods suffer from very high sample complexity so that simulation for generating realistic interaction **experience nonviable**. line 169: And at each step, **the will** generate next click by line 174: Simultaneously the model **needs to decides** the probability line 248: The last term is the **objection** for generator in GAN.  Significance: The overall problem of learning to recommend is an important problem. Any approach that can improve recommendation performance will be of interest to multiple entities - industry, academia.  Statistical significance numbers are missing in the results. Otherwise, the results look good.   I have carefully considered the authors' response. The rebuttal looks fair. However,  my questions were more of a request for clarification. So, it doesn't change my score.  