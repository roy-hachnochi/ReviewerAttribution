My major concern lies in the experimental configurations. The authors use the VOC07 test set for evaluation and use the VOC07 trainval (labeled) + VOC12/COCO (unlabeled) for training. As the VOC dataset is less challenging and has gradually fallen out of fashion in recent years, in my opinion, a fairer evaluation should be made on COCO dataset, say, by partially removing the annotations of the training data as auxiliary ones.    Besides, I also notice that the best results of the proposed method are obtained when the auxiliary COCO images only contain objects belonging to the 20 VOC classes (see table 2 and table 4). In this case, the comparisons would not be fair enough since the authors are using manually selected data and the class labels are essentially considered during training. At least, in my opinion, I do not agree considering the above sensoria as a “semi-supervised” detection setting.  The idea of alleviating the class-imbalance problem in object detection by using “background elimination (BE)” is not new. If the authors insist considering it as one of the contributions, they should better compare it with other approaches with similar motivations, e.g. hard negative mining in SSD and Focal loss in RetinaNet.   The authors simply use horizontal flip as the perturbation of an input image so that to achieve a one-to-one correspondence between the predicted boxes in the flipped image and the original ones. My question is why not try other operations, like illumination changes, image wrap, and minor rotation, where such correspondence can also be easily guaranteed. Will a more diverse perturbation be beneficial to improve performance?  