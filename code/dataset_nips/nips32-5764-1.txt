     The paper investigates the connection between PAC-Bayes and Rademacher complexities, two framework in statistical learning theory to upper bound the generalization error of predictors. The paper is quite pleasant to read, and clarity is remarkable.      Detailed comments:      * The paper investigates the binary classification case, with the loss taking values in $\{0,1\}$. Is there hope to transpose the present results to more general losses? It is unclear whether the technique used by the authors depends on the fact that the loss takes only two values -- or is bounded. There are also a few papers on PAC-Bayes with unbounded losses, see e.g. Alquier and Guedj (2018), "Simpler PAC-Bayesian bounds for hostile data", Machine Learning and references therein (note: slow rates).      * Eq. (9) is reminiscent of the Legendre transform of the Kullback-Leibler divergence which has been long known and used in the PAC-Bayes literature. See for example Lemma 1 in the recent survey "A primer on PAC-Bayesian learning" (Guedj, 2019), https://arxiv.org/abs/1901.05353     Is there a connection here?      * Lines 182-186: I disagree with that comment. Even if $\frac{c}{m}\sum_{i=1}^m \mathbb{E}_Q[f(z_i)-(1+h)\mathbb{E}_Q f(z_i)]^2$ is negligible, the right-hand side in (14) is still larger than the corresponding term in Catoni's bound, due to numerical constants (in the present paper: $\frac{4}{C}(3\mathbf{KL}(Q||P) + \log(1/\delta)) + 5$ compared to $(1+t)(\mathbf{KL}(Q||P)+\log(1/\deta))$ in Catoni's bound, where $t$ is arbitrarily close to 0).      * Related work: a few PAC-Bayes references seem to be missing (see the afomentioned survey Guedj, 2019 and references therein). In particular,     Tolstikhin and Seldin (2013), "PAC-Bayesian-empirical-Bernstein inequality", NIPS (PAC-Bayes bound with a variance term)     seems relevant as the authors prove a PAC-Bayes bound with a variance term. I am unsure why the authors of the present paper insist that their notion of "flatness" is "\emph{not} related to the variance of the randomized classifier". While it is technically true, the terms are still quite similar and one would expect the actual values to be close when computing the bounds (especially when $h$ is small). The paragraph on lines 249-262 could be enriched with additional comments and comparisons. In particular, see Theorem 4 in Tolstikhin and Seldin (2013).      * References: please remove "et al." for [3] and [39].      === after rebuttal ===      I thank the authors for taking the time to address my remarks in their rebuttal. After reading other reviews, rebuttal and engaging in discussion, my score remains unchanged.      Nevertheless, I must say I am disappointed by some parts of the rebuttal. Point (5): I don't agree with the rebuttal and I think discarding a comparison with Tolstikhin and Seldin's bound on the ground that their term may be large whereas the authors' is zero is wrong: I see no evidence for that claim. Last but not least, see Alquier and Guedj (2018), propositions 1 and 2: PAC-Bayes bounds *can* be easily specified to the ERM case, contrary to what is written in the rebuttal.      I still have an overall positive opinion on the paper but I sure would like to see a more thorough and rigorous comparison with existing bounds, both theoretically and numerically.    