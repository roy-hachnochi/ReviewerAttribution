Originality: To the best of my knowledge, the results are novel and provide important extensions/improvements over the previous art.  Quality: I did a high level check of the proofs and it seems sound to me. Clarity: the paper is a joy to read. The problem definition, assumptions, the algorithm, and statement of results are very well presented. Significance: the results provide several extensions and improvements over the previous work, including training deeper models, training all layers, training with SGD (rather than GD), and smaller required overparameterization. More importantly, the proofs are simpler compared to the related work. My only concern are the followings. 1) the width requirement is still very stringent, and not realistic; and 2) It is not clear how to compare such a bound with other results. In particular, it would have been nice to see how small the NTRK error (the first term in the right hand side of Thm 3.3.) can get in practice for real models.    ******************************************************************** *******************after author feedback********************** ******************************************************************** I have read the authors feedback and am happy to increase my rating from 6 to 8. I have also increased my confidence score from 3 to 4 after authors clarified some part of the proof. Overall, I think this paper is well qualified to be published in NeurIPS. 