Post-rebuttal update: The author's rebuttal addresses my (minor) concerns well, and my overall score remains the same. ---- This paper presents a principled randomized optimization method for high-dimensional convex optimization via a data-dependent random linear sketch. The approach is similar to earlier work such as:  - M. Pilanci and M. J. Wainwright. Randomized sketches of convex programs with sharp guarantees. IEEE Transactions on Information Theory, 61(9):5096–5115, 2015.  - Y. Yang, M. Pilanci, M. J. Wainwright, et al. Randomized sketches for kernels: Fast and optimal nonparametric regression. The Annals of Statistics, 45(3):991–1023, 2017.  that also solve a sketched version convex optimization problems. The main innovations here are to extend this sketching technique to a wider class of convex objectives and to introduce a data-adaptive sketching technique that greatly improves the error bounds on the solution relative to a data-oblivious sketch. The proposed technique can also be performed iteratively to improve the accuracy of the solution without having to change the sketch matrix, so the sketch on the data only has to be performed once. Overall, I thought this was a high-quality paper. The proposed sketching approach has clear practical benefits and comes with strong theoretical guarantees. The adaptive sketch approach is particularly appealing, and has clear benefits over an oblivous sketch, and may be of broader interest to the optimization community.  The paper is generally well-written and clear. I only have some minor comments, given below:  I think the title should be changed to “High-dimensional Convex Optimization in Adaptive Random Subspaces” (i.e., add the word “Convex”) to better reflect the contents of the paper. Namely, the approach hinges crucially on convex duality so it seems limited in applicability to convex optimization problems.  One limitation is that the optimization model (1) studied in this work assumes the loss function f is strongly smooth, with rules out L1 losses among others. Also, the model (1) can only accommodate L2 regularization of x, and not other common types of non-smooth regularization used in regression settings, e.g., L1 in the lasso or L1+L2 in the elastic net. Some discussion about whether the present approach could be extended to this more general case would be nice. I see there is a section E in the appendix that addresses this some, but this section does not appear to be discussed at all in the main body.  At the start of Section 4, the paper states “By the representer theorem, the primal program (1) can be re-formulated as…”, but I do not think invoking the representer theorem here is necessary. If I understand correctly, the equation in (11) is just due to a linear change of variables x = A^T w.  The results on classification of CIFAR10 in Table 3 are far from state-of-the-art (~50% classification error). Is there are different problem setting beyond classification for which the method could be demonstrated?  The notation 5.10^{-5} is a little non-standard. Maybe use a \cdot or \times rather than a period?