Originality The PIAG algorithm has attracted a lot of attentions in recent years, but few of them use Lyapunov function for analyzing. As a result, I think the originality is good enough for accept.  Quality The main contribution lies on the analysis. The proof is neat, but there are some typos, especially in taking expectation. I think the authors should spend a few words to clarify which r.v. is taking expectation.  The experiment is weak. There is no baselines for comparison.   Clarity The clarity is also hurt by the issue in taking expectation. I think the authors could fix them in the revision.  The authors should spend more words on KL property. There are quite a lot of references on KL property, e.g.[1][2]. But I cannot find any of them.   Significance i. Convergence rate. The non-asymptotic rate achieve by assuming 1) boundedness of $\{x_t\}$; 2) semi-algebraic of objective, which is a sufficient condition for KL property. This is not new in optimization community. ii. Lyapunov function. I think the main contribution is that this is the first paper for analyzing PIAG by Lyapunov function. iii. Experiment. The experiment is not convincing. The authors should include more comparisons in the revision, e.g. [3].  iv. The paper is mainly based on the assumption that $\sum_i \sigma^2_i<\infty$. However, the authors did not provide a method to guarantee the assumption in stochastic setting. This dramatically hurt the contribution of the paper.   Overall, I would upgrade my score if the authors could give a feasible plan for fixing above issues.  [1] Bolte, Jérôme, Shoham Sabach, and Marc Teboulle. "Proximal alternating linearized minimization for nonconvex and nonsmooth problems." Mathematical Programming 146.1-2 (2014): 459-494. [2] Attouch, Hédy, et al. "Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequality." Mathematics of Operations Research 35.2 (2010): 438-457. [3] Peng, Wei, Hui Zhang, and Xiaoya Zhang. "Nonconvex Proximal Incremental Aggregated Gradient Method with Linear Convergence." Journal of Optimization Theory and Applications (2018): 1-16. __________________________________ I have read the authors' feedback. I believe most of the issues will be fixed in the revision.  The error summable assumption is still a challenge. I think we can make it hold in certain cases, but more analysis might be required. I think the paper is significant enough for the conference. Thus I’d like to upgrade my score. 