# Strengths - This submission copes with a problem of general importance: the optimization of weight-normalized networks. Coming-up with the right initialization strategy can prove to be critical in that case and the theoretical analysis suggests optimal init. strategy to avoid vanishing / exploding gradients in very deep networks. - The draft is clear, reads nicely and the research is well motivated. Allowing to improve the generalization capability of weight-normalized networks is an important research direction. - The proposed experiments seem to show the effectiveness of the initialization scheme. - The experiments reported in Table 1 and Table 2 are rather convincing.   # Weaknesses / questions - First of all, I would like to know why is the assumption about asymptotic setting with infinite width needed. Where in the proof is it used? In terms of the parametrization proposed here it means that we work with n->infty. Eventually, in the experiments, the networks have rather small width compared to the depth. Following-up on that, could it be somehow empirically validated? Is this width assumption important? - What is the exact setup in the experiment presented in FIg. 2 (right). The text of the paper fails to exactly describe the experimental setup. What is the depth of the network considered here? - I find the experiments with resNets contestable. First of all the range of depths considered here is very surprising. Indeed, a resnet with 10000 blocks per stage is rather uncommon. Second, this experiment is ran for "one epoch of training" (l. 215) which in my opinion fails to show anything else than "PyTorch default init fails completely with such deep WN resnets".  - The formulation on l. 243 is rather surprising: "Unlike previous works which use the test set for hyperparameter tuning". I do agree that proper hyper parameter tuning is essential in ML, but the wording is a bit hard in my opinion. Moreover, the "smaller training set" (l. 245) could be avoided by training the network with optimal parameters on the complete train+val. - In Table 1, I would expect a baseline WN model to be presented for other architectures than Resnet-110.  Overall this is a good paper, coping with an interesting problem and is executed properly. I would like the authors to react to my negative comments / questions, and await the discussion period to make my final decision.  # Rebuttal After reading the other reviews and the author's response, I decide to stick to my accept rating of 7. The author's response answered most of my concerns and doubts.