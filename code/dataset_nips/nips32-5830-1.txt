This paper proposed a new initialization strategy for weight normalized neural networks where the weight matrix in each layer is normalized. The key ingredient is to appropriately rescale the weight by a factor depending on the width. A theoretical analysis is provided showing that the proposed initialization strategy prevents vanishing/exploding gradients in expectation. Extensive experiments are provided showing that the proposed initialization does help the training and outperforms standard initialization strategy. Overall, I am very positive of the paper although I have several minor concerns, following are my comments.  1. It is mentioned several time in the paper that the initialization schemes is developed where the network width tends to infinity. I am a bit confused by it since none of the analysis explicitly used the infinite width condition. Maybe I am missing something, please clarify on it.    2. There is a closed form formula for the surface area of the unit ball, check for example Wikipedia, it would be better to prove that K_n=1 instead of a empirical check.  3. When performing the analysis on the backward pass, the gradients are evaluated according to intermediate pre-activation a^l. However, a^l is not a parameter that we are optimizing, what are the motivations of taking the derivatives with respect to a^l instead of W^l?   4 In the synthetic example in Figure 1, are the width of the fully connected network constant? In particular, the rescaling parameter in thm 1 and thm 2 match to each other when the width is constant, thus it is an extreme case. It would be good to perform experiments when the width varies and see if there is a difference.  5 If n_l > n_{l-1}, there is more rows than columns, how do we set the rows of the weight matrix to be orthogonal?   6 Is it possible to combine weight normalization with batch normalization? As far as I could see, batch normalization does not effect proposed strategy, maybe it would be good to try some experiments heuristically.   ===Edit after Rebuttal=== I thank the authors for the clarification. I believe the initialization strategy introduced will be helpful for training deep networks in practice.   