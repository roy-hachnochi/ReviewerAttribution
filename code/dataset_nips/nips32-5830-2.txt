Summary ------- The paper proposes several initializations for weight normalized deep neural networks. The initializations are justified from simple theoretical considerations, then tested on numerous benchmarks, from which several conclusions are drawn.  Originality and Significance --------------------------- While the justification of the initializations are simple and the approach is not new, the experiments provide a solid material for further exploration of weight normalized deep neural networks. In particular it seems to reduce the gap of performance of weight normalized networks compared to batch-normalized networks. This paves the way to better optimization of other structures such as in the reinforcement learning applications provided in the appendix.  Quality and Clarity ------------------- The paper is well written and organized. In particular the experiments are well presented.  - it is finally not clear what initialization is chosen for the residual networks, forward or backward ? - the authors could better emphasize that the plots separate the initialization of the scaling and the initialization of the weights. - also numerous plots are superimposed which makes their reading difficult.  Conclusion: --------------- Overall the paper proposes a dense experimental study of initialization techniques for weight normalizations. This provides interesting material for future research.  After rebuttal: ----------------- The authors answered my concern about the warm-up supplementary boosts and definitely showed the benefits of the approach. Overall this paper provides a very neat experimental material, the code is well written (I quickly skimmed through it) and therefore could be easily used for future research. The paper itself is well presented such that it can be used for the community. Overall I think the paper deserves publication. I hope that it could also open some discussions to relate weight normalization and the kernel perspective in the future.