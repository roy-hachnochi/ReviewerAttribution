Originality: The beta-Bernoulli interpretation for model aggregation is new to me. To briefly summarize, the paper uses a subset of global model parameters for local groups, and the subset selection process is modeled as beta-Bernoulli with a matching afterward. This method is novel to me, but I still have a few questions. 1) The model requires performing alternative updates for each group, is it possible to parallelize the algorithm? 2) It would be non-trivial to adaptively tune the cardinality of C_j. How do you do that in your experiments? If |C_j| diverges among groups, can your method accurately estimate |C_j| for each data group j? Overall I think the paper totally makes sense to me, but several details need to be verified.  Quality: This paper is well-written, and I have not found any technical error within the paper.  Significance: I think all experiments in this paper make sense to me but not quite enough since the paper does not compare over their method with provably convergent methods such as online learning methods (e.g. stochastic variational inference). So it is hard for me to judge the total quality of this novel learning strategy.  Clarity: This paper is well-written and clearly explained.