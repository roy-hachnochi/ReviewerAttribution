Following work of Acharya,, Das, Orlitsky, and Suresh [5] which showed that a single estimator, the "profile maximum likelihood" (PML) and its approximate version (APML) achieve near-sample optimal guarantees for several property estimation tasks, this work investigates the applicability of the (A)PML for testing and estimation tasks on discrete distributions: - Sorted distribution estimation: estimate the vector of sorted frequencies - symmetric property estimation: estimating the Shannon, Renyi entropy of a distribution, or its support size, etc. - identity testing: (one-sample testing)  This work shows that the (A)PML is near-optimal (in terms of sample size) for the above tasks, up to either constants or polylogarithmic factors (in the domain size k).   ================= Comments:  - The organization of the introduction seems strange to me. It starts by introducing additive symmetric properties before anything else, and devotes a lot of space to it; however, it then pivots to "problems of interest" of which two are not additive symmetric properties... this needs reorganizing to make more sense, outline-wise. - l.50 (and abstract): do not title this "distribution estimation": write "sorted frequencies estimation", or "sorted distribution estimation" - the problem you are solving is *not* the standard distribution estimation one. - abstract and l.49 and throughout: "statistical-learning" -> "statistical learning"; l.70: "median-trick" -> "median trick" - l.104, l.285: why do you have *three* citations for the Charikar et al. paper (one talk, one arxiv, one peer-reviewed)? It's one paper, cite it as one paper. ([23], *not* [21,22,23]) l.115: no need to cite [21] again (also, that'd be [23]): you mentioned that near-linear computation a few lines before. l. 123: log|eps| is somewhat strange. log(1/eps) is more readable and intuitive. ***l.130: I do not understand. How can it be that an *approximate* (hence, "not as good") computation of the PML can give *better* bounds than the PML? Is there a mistake? Can you explain this? ***Theorem 6: What about constant error probability? Can you achieve the optimal sqrt(k)/eps^2 for that? - l.165: you are being a bit misleading here, as *nearly all* uniformity testers (except Paninski'08 and [31]) actually provide this very same L2 testing guarantee (as they basically work by counting collisions, or a  chi^2 variant thereof) - l.251: actually, maybe give a short motivation for this sorted L1 distance estimation? You never did, and it does have applications, so... l. 271: [3] is not relevant (it's not the same testing setting, and it's under monotonicity assumptions), and [17] either (it's 2-sample testing (closeness), not identity; the l2 tester they give is relevant, but then, never phrased in terms of identity testing, and basically subsumed by other works anyway)  ===  Question: you mention several times PML as a "universal tester" (or property estimator). However, can you get a *truly* universal statement in terms of quantification (and error probability)? "with probability 9/10, the APML computed is (gives tests that are) optimal *simultaneously* for all the following tasks: [...]" (the way that a plugin approach with the empirical estimator would work, given O(k/eps^2) samples: as long as the estimate is close to the true distribution in TV distance, *all* plugin estimators computed on it are accurate at the same time)   UPDATE: I read the authors' response, and am satisfied with it.