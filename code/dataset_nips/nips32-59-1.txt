There exists a strong correlation between model size an accuracy on large benchmark datasets. Accordingly, it is valuable to push the limits of scale and see if/when the gains from increasing model size saturates. One of the leading approaches for scaling model sizes is model parallelism, but model parallel training is complex and often much slower than standard training.  The authors propose a framework for combining *pipeline* model parallelism with gradient checkpointing ("re-materialization"). Compared to other model parallel approaches that split individual layers across accelerators, the proposed TensorPipe approach partitions whole layers across devices and uses pipelining to improve overall efficiency and gradient checkpointing to reduce inter-device communication. The proposed approach works, and the authors demonstrate strong results scaling model sizes by an order of magnitude for image classification and machine translation models.  The paper is well written, the code is open source, and the results are compelling. This work is likely to have a large influence on future model scaling efforts and serves as a good reference for future adoption/implementation in other frameworks.