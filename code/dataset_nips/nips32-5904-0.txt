This work leverages concepts from PAC-Bayes style generalization bounds to the estimation of local mutual information in neural networks and its application to bounding generalization (similar to Xu and Raginsky).   Unfortunately there are numerous spelling and grammar mistakes, and more problematically, critical notation in the main theorems are not defined anywhere. Additionally, important quantities are glossed over and not discussed thoroughly enough. As a result, the theoretical sections are quite difficult to follow.  It is not clear to me how  the information bounds are used, it seems that instead of these bounds the authors end up focusing on KL-based bounds which are more reminiscent of PAC-Bayes.   For the experimental results, the improvement over non-data-dependent bounds is to be expected. The figure should be relabeled, "Numerical Results" is not informative, and the y axis should be labeled.  Edit: Thanks for answering my questions. The authors seem willing to put in the effort for properly revising the paper, so I tend towards acceptance.