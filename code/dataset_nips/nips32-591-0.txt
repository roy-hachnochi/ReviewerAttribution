* Quality  Overall the paper does execute the combination of the spatial transformer layer with the VAE well, but this seems relatively straightforward.   My main concern is with the quantitative evaluation. The paper follows Eastwood and Williams 2019 which proposes a framework to evaluate disentanglement. However:  1. Why this approach? What are the other methods? How to measure “disentanglement” seems non-trivial (and "unsolved") and as such, for a paper in this domain, the evaluation method will be critical.   2. The framework consists of 3 scores, of which the used “disentanglement score” is only one of them. Why are the others not used? If there is a good reason, it would be important to mention this.  3. The framework relies on synthetic data, data where the ground-truth latent structure is known and available. But for MNIST these are not known, the paper assumes the generative factors are the class labels, which is quite different from the type of generative factors generally considered: size, position, color, etc. The paper shows similar style variations controlled with the latents qualitatively, but these aren’t “known” and thus its disentanglement of them can’t be measured by this metric. This makes the comparison of this metric for MNIST less relevant.   4. All generative factors considered in Eastwood and Williams are continuous, whereas in this paper they are all discrete; for SMPL they are intentionally discretized whereas in Eastwood and Williams they even argue in the related work that such discretization is unnecessary.  5. The likelihood differences between the different models look very small (Table 3). It would be good to show the variance across different runs and have some measure of significance for these differences.   * Clarity  The paper was quite easy to follow, and seems well-structured.   * Originality  The paper mostly combines two well-known methods: VAEs and spatial transformers. There is extensive related work discussed and the approach seems properly positioned with respect to it. However, as noted in the Quality remarks, how to evaluate disentanglement seems non-trivial. Some clearer argument of why the current metric (from Eastwood and Williams) is being used, and what other approaches have used, is important.     * Significance  I would consider the significance relatively minor, the scope seems too limited to be of wider significance.  ----------------------------------  I thank the authors for their response.   I do think, as in my original review, that more justification / background for the choice of metrics is important. Currently, there is no discussion about the evaluation itself, only a statement about the chosen metric L195-197, which is insufficient, especially when the technical novelty is relatively minor. If "Locatello et al. (2019, ICML best paper) find that most proposed disentanglement metrics, including the disentanglement score from Eastwood et al. (2018) that we are using, are highly correlated" -- also noting the "most" -- why not report more metrics and provide more evidence for that (at the very least in the appendix)? In Locatello et al. they conclude that the level of correlation across metrics depends on the dataset, such that I don't think we can conclude that there will be high correlation for _all metrics_ and _all_ datasets (e.g. CelebA which isn't investigated in Locatello et al.). Regardless, it seems premature to pick and fix a single metric for such a notoriously hard to measure task (with many different interpretations), even if there is very recent evidence that _most of_ the scores are correlated _for the current (class of) models_ on some of the current datasets.   This seems similar to the difficulty of measuring sample quality and/or diversity in generative models (e.g. [1]). For generative models Inception Score was, and still is, used to provide some measure of sample quality / diversity of GANs, and it qualitatively seemed correlated with improved results. However, it clearly has multiple flaws, see [2], and simply seemed useful at the time as some proxy of progress within GANs but broke down in certain regimes / model classes. Similarly, IS and Fréchet Inception Distance (FID) seem quite correlated with progress in visual sample quality, but do behave quite differently in some settings (see e.g. [3]). Anecdotally, although somewhat discussed in [1], improvements in likelihood of generative models is correlated with improvements in sample quality, when measured within the same model class. Once you try to compare different models you can get nonsense results.  Regarding "We focus on the disentanglement score as it is the only one that seems to be of general interest in the field", this seems unconvincing given how only very recently (Locatello et al.) there has been some evidence that "most of the metrics" are correlated (with different levels), as discussed above. I feel we should be much more careful just immediately dismissing any other metrics on such a hard-to-measure task.  However, I appreciate the authors have done more experiments on more datasets, and addressed some of my comments. I am still unsatisfied with the evaluation and the somewhat dismissive rebuttal for that, such that I'm only changing my score to 5. Commitment to more discussion about the evaluation (with reference to e.g., Locatello et al., which is currently only referenced in different contexts) and more evaluation results with more metrics, feels critical to me and would have made me satisfied enough to give a higher score.   [1]: Theis et al., A note on the evaluation of generative models, https://arxiv.org/abs/1511.01844 [2]: Barrat & Sharma, A Note on the Inception Score, https://arxiv.org/abs/1801.01973 [3]: Heusel et al., GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, https://arxiv.org/abs/1706.08500  