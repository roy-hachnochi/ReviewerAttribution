Originality: This paper proposes to combine VAEs and spatial transformer networks (STNs) to learn a disentangled representation of appearance and perspective. While STNs have been combined with generative models before (e.g. AIR by Eslami et al, 2016), to my knowledge this is the first time this approach has been used for learning disentangled representations.  Quality: The paper appears technically sound and the results are supported by both quantitative and qualitative measures.  Clarity: This is one of the most well written papers I have read recently.  Significance: This paper makes an important step towards a new direction towards unsupervised disentangled representation learning, where transformations take the centre stage. This is in line with the recent theoretical work that suggests defining disentangled representations in terms of symmetry transformations (Higgins et al, 2018). While the current implementation with STNs is limited to affine transforms, the setup is general enough to replace STNs with more powerful transformation learning models. I believe that others are likely to build on top of this work in the future.   ----- After author feedback ------ The additional experiments provided by the authors in the rebuttal have made me even more happy to support the acceptance of this paper. I keep my original high score unchanged but argue for the acceptance.