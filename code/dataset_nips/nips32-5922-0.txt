 Originality and Significance:   In light of submission with ID 6290, the method proposed in this paper is not novel. However the applications are different: submission 6290 focuses on learning generative models that are not influenced by the bias in the data whereas this submission focuses on reducing the mismatch between the trained generative model and the data distribution for Monte Carlo estimation at test time. Although the idea of importance weighting and estimating density ratios with a classifier is not new I believe the applications in the two papers are potentially useful.  Quality and Clarity:   The paper is well written. The related work section is good but there is a missing line of related work that was not mentioned, that of using importance weighting in the context of variational inference. See for example [1] and many follow-up papers. The experiments section is nicely done.   Questions and Minor Comments:  1--how do you ensure that the generative distribution p_{\theta} has bigger support than p? 2--Is there a way to measure the bias introduced by p_{\theta}? What is this bias? 3--How did you deal with high variance in importance reweighting? What usually happens in importance weighting is that all the components of the importance weights collapse to 0 except for one component which eats all the mass...This happens even with your proposed trick on line 135 (self-normalization) and I can imagine this would also happen for the solution on line 137 (flattening). Clipping (as proposed on line 141) might be promising in practice however you do lose the asymptotic unbiasedness of importance weighting in this case. Please clarify. 4--how did you choose the clipping threshold \beta?   [1] Importance-Weighted Autoencoders. Burda et al., 2015. 