The paper proposes an importance weights based scheme to correct the bias in the distribution learned by a generative model. Specifically, the authors employ a classifier to distinguish between a true data sample and a sample from the generative model. This binary classifier is used to estimate the likelihood ratio when the true distribution is unknown and the model distribution is intractable.   Authors also discuss practical techniques to address the issue of imperfect classifier and reduce the variance of the Monte Carlo estimates. There include normalizing the weights across a batch of data samples, smoothing as well as clipping.  Standard metrics to evaluate sample quality of the generative models such as FID and Inception score show improvement when the Monte Carlo estimates using the learned importance weights are used. This also appears to help on downstream domain adaptation task on Omniglot task.  Questions/Concerns:  It is not clear how the learned classifier is calibrated. The calibration seems quite crucial and this is often difficult, especially for deep neural network classifier trained on high dimensional data.  The post-hoc normalization schemes are not well motivated. How do these interact with calibration? p, p_data, p_theta needs to defined early on. It is nor clear if p_mix, eq (2) and related discussion add much to the discussion. Why does D_g + LFIW not show much improvement over D_g. In the toy experiment, how do the results shown in Figure 1 change as the two modes in the true distribution get closer and closer? Domain adaptation on standard benchmark datasets and comparison with baselines will strengthen the empirical results.  Typo: Policy evaluation numbers are missing in the introduction.  Update: Thanks for answering most of my questions and of other reviewers. I have raised my score.