This paper studied the local updates with periodic averaging to solve distributed non-convex optimization problems, and provided an improved bound over the number of required local updates. The theories are validated through experimental results.   Overall, I feel the paper contains new results and insights, and is well-organized. I have comments in the following three aspects.  Originality:  1. The upper bound of the required local updates is new, which improves the existing one by a constant order. And it is good that the paper also explained why this improvement can be achieved.   2. The adaptive communication rule also seems to be new, but it needs further explanation. Specifically, only the convergence is proved under the adaptive communication rule, which is not that surprising. Is there any theoretical improvement?   3. How does the adaptive communication rule relate to/differ from the adaptive communications rules in the following work?  Chen, Tianyi, Georgios Giannakis, Tao Sun, and Wotao Yin. "LAG: Lazily aggregated gradient for communication-efficient distributed learning." In Advances in Neural Information Processing Systems, pp. 5050-5060. 2018.  Wang, Jianyu, and Gauri Joshi. ``Adaptive communication strategies to achieve the best error-runtime trade-off in local-update SGD." arXiv preprint:1810.08313, Oct. 2018.  4. It is mentioned that under the current analysis, the logarithmic number of communications may be not possible, it is encouraged to comment on whether the bound is tight or match its lower bound.   ==== Rebuttal ==== I have carefully read the rebuttal. Thanks. 