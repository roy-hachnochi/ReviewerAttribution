The paper presents a novel approach to curriculum learning, by introducing two new sets of parameters to learn, one per class and one per example, which correspond to temperatures in the softmax classification layer, and can easily be trained by gradient descent.  When considering the class-based version (equation 1), I wonder why the model without that extra set of parameters cannot learn the same thing through the existing weights (it's just a scaling of the logit, after all).  For instance-level temperatures, it makes more sense but again, this can only work with small enough datasets where each example is expected to be seen many times; in the context of a very large dataset, it's unlikely that each example would be seen enough by the model to learn a relevant temperature. Having a temperature parameter that is a function of the example (and maybe the label) could alleviate this (but this might become similar to other competing approaches, no?)  I would suggest numbers given in the text page 5 around line 170 be put in the relavant Table 1 for ease of comparison.  I didn't understand the difference suggested on page 8, line 285-286.