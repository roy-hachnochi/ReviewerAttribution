This paper proposes simple algorithmic modifications to popular approximation algorithms for the k-centers and k-means clustering problems. These modifications limit the selection of outliers, and thereby allow the authors to translate existing theoretical properties of the standard algorithms to the corresponding problems where outliers are present. The modifications in essence either constrain the selection of initial centroids based on their distance from the current set, or reduce the probabilities associated with selecting outliers which arise in the kmeans++ algorithm which has d^2 proportional probabilities. Empirical results show that the methods work well on some popular benchmarks.  The paper is clear and well written, and the methods, although simple modifications of existing algorithms, are intuitive. The theoretical analysis is also well presented and persuasive. My main concern here surrounds the availability of a good estimate of OPT (the minimum objective value), especially for the k-means problem. The authors claim that this is a common assumption in clustering literature, but don't provide a reference.  To conclude, a few minor comments/issue/typos: 1. Theorem 1.2 appears to dominate Theorem 1.1 when c = 1. If my understanding is correct, what then is the use of Theorem 1.1? 2. In Theorem 1.4 there is a typo in the statement of the approximation (extra right brace after "\beta + 64"). 3. In line 235 I presume you mean "... considerably more technical than our analysis for K-CENTERS..."