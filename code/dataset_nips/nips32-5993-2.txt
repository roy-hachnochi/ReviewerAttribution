This paper theoretically and empirically shows that guarantee of non-trivial adversarial robustness only requires more unlabeled data.   Strengths: 1. The paper theoretically proves that under the Gaussian model, more unlabeled data is enough to certify small robust accuracy (1e-3 in the paper) by their robust self-training algorithm. 2. The paper also empirically shows on cifar10, robust self-training algorithm with unlabeled data can outperform state-of-art models and standard self-training. 3. The paper empirically illustrates on SVHN that robust self-training with unlabeled data almost achieves the same robust accuracy as the robust training with labeled data. 4. The paper is clearly written. It is a pleasure to read it.  Weakness: 1. The main concern is that the connection between the theory and the experiment is loose. The theory has very strong assumptions on the true model (Gaussian model). This is totally different from the real world dataset model like cifar10 and SVHN. The authors never addresses the connection anywhere in the paper. Theoretical guarantee for the real world data still remains an open question. 2. The comparison seems to be unfair with the state of art models because robust-self training has extra unlabeled data information. Some empirical analysis of state-of-art model utilizing unlabeled data can be interesting.  Minors: 1. Why the state of art model for l_inf attack is different for epsilon = 2/255 and 8/255? Does that mean state-of-art model can only guarantee one specific epsilon? 2. RST standard accuracy 80.7 (Figure 1 b) when epsilon = 2/255 is much lower than standard accuracy 89.7 when epsilon (Table 1). Why is that? Training with small epsilon intuitively should give higher standard accuracy.  -------------------------------------------------- Updates after rebuttal -------------------------------------------------- Thanks the authors for providing a super clear rebuttal. My questions are addressed.