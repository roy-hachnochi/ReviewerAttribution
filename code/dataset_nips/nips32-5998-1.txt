This paper frames the learning problem as a cooperative game between the machine and human, where the human has a mental model of the machine's task.   In this situation, the learner poses queries (similar to query-driven active learning) and the teacher responds strategically to them, having knowledge the learner doesn't have, to best guide the learner.  This is called "machine teaching". The learner may be aware of what the teacher is doing or not. Both cases are considered as well as a mixture model of the two.  The learner then has an "inverse reinforcement learning" (IRL) problem to solve, to the teacher's - in this case the human user -  "inverse machine learning" problem.  The learner queries the teacher, who then makes a possibly strategic recommendation based on state knowledge that the learner does not have, then the learner makes a move knowing that the teacher has this knowledge. This model contributes to understanding of user models, where the user has a mental model of how the system engaged with should perform.   In the case presented of a multi-armed bandit, the teacher's knowledge comprises the reward probabilities of individual arms, with model parameters \theta, which allows the teacher to strategically alter responses y_i, by means of an MDP. For this to work, one "bandit" must inform another, else were they independent then no strategy of offering the "wrong" y_i would help.  Figure 2 offers an example of this; however how this may work in the general case begs more elaboration, beyond the claim that the teacher uses a modified reward based on R_t(h_t;\theta*).  Exactly how does this work for Bernoulli bandits?   Then should we assume that this modified reward, and specifically the \theta* is what the learner seeks via inverse reinforcement learning?  What is complicated here is that the reward probabilities do depend on \theta (Equation 1), so shouldn't the learner be able to infer \theta without the actions of the teacher? Maybe another way to state this is that dependencies among bandits due to \theta should be evident in the naive case, irrespective of the teacher's strategy.  An experiment revealing the differences in the sequence of y_i and R_i with for the naive versus strategic teacher would be revealing.   Insights into the claimed success of this method are limited by the lack of a complete model that comprehends both teacher and learner in a cooperative game. Both apparently share the same "uber" reward to be optimized, perhaps with different horizons. As a cooperative game one might expect an equilibrium solution that reveals properties of their combined actions.  Without a joint game theoretic model (perhaps by solving the I-POMDP) it's not apparent why the combination works.   This is a fascinating area with rich implications for understanding human-machine behavior, but currently this paper is limited to computational results suggestive of general system  properties.  Specifics: The paper mentions available source code, but none was provided; perhaps an oversight? 