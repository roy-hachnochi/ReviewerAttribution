This paper introduces a novel approach called NAOMI for imputation of arbitrary missing values in dynamic time series.  NAOMI fills in missing values in a multi-resolution fashion from recursively coarse to grain, such as first the beginning and then the end and then the middle of the two, and then the middle of the left-half, sharing weights for gaps of the same size.  The architecture is "iterative" from coarse to grain but the generation is non-autoregressive (using the re-parameterization trick for cases that are stochastic), allowing for the generator to be fully differentiable and adversarial training to be applied.    The paper presents improved empirical results on multiple datasets, ranging from real-world traffic time series, billiard ball trajectories from a physics engine and team movements from professional basketball gameplay.  Multiple competitive baselines are being compared to, allowing one to compare results from modifying different components of the model design such as autoregressive versus non-autoregressive, with or without adversarial training.  Strengths - The method is novel, the recursive divide and conquer formulation is well-suited for capturing long-term dependencies.  Figure 3 shows a clear case for the advantages, both qualitatively and quantitatively.  Also, the procedure is quite efficient, memory for hidden states are linear to sequence length while the recursive (shared among similar gap lengths) imputation weights scales logarithmically with sequence length. - The datasets are challenging and cover a wide range of tasks.  In the case of traffic time series (4.1), the data is very high dimensional (963 different sensors).  Multiple metrics, in additional to L2 loss, are used to capture different aspects of the time-series, accounting for invariant surface variations.   - The visualizations are very helpful in understanding the complexity of the tasks and also for comparing the strengths and weaknesses of different methods. - The paper is well-written. - The proposed method is clearly superior in all tasks.  Potential weaknesses - Motivation: how does modeling these time series vary or similar to modeling audio, images, or text? The paper mentions time series are more dynamic, involving temporal irregularities such as discontinuities and varying levels of rate of change in different regions, perhaps motivate a bit more why a fixed “sampling” scheme of "halving" the sequence addresses the challenges in the domain or other considerations for future work. - The method assumes fixed-length sequences.  Is this common in time series problems and datasets? - How much are the given anchor points (distributed across the sequence) giving the long-term structure versus the model learning them? - [masking] The masking scheme is not described.  The figures seem to show that the unmasked positions are those in order with how the divide-and-conquer scheme would proceed.  Does make it harder for certain baselines to cope with?  Is the masking per time step, that is all dimensions within a time step is masked if a tilmestep is masked? One of the baselines, the GRUI paper, uses random masking across both time steps and dimensions.  Given the divide-and-conquer scheme it might not be directly applicable, but would a variation of it be?  One question the reader might have is that if for example if the unmasked positions are less well distributed throughout the sequence, how would it affect the performance?   In line 235 “Robustness to percentage of missing values”, the word “percentage” could be a bit misleading because it’s not a random percentage but portion in a pre-determined ordering. - [baseline] Would another baseline be generating auto-regressively but using the divide and conquer ordering, without adversarial loss? - Out of scope for this paper, perhaps question for future work, would some modification of the Transformer architectures be a competitive candidate for capturing these long-term dependencies?  Clarifying questions: - In the complexity section (line 139), why is only the backward hidden states updated when the step is imputed and not he forward states also. - From the description of the text (line 131 to 133), would line 10 (would include how g^{(r)} is obtained?) and 11 be swapped? - In Figure 2, would g_1 and g_2 be reversed since the subscript r refers to the resolution (size of gap)? 