Originality: This work combines existing techniques in a meaningful way. The attack is very closely related to the Expectation over Transformation attack in "Synthesizing Robust Adversarial Examples" by Athalye et al(which needs to be cited). It is already established that stronger attacks can help in adversarial training, and the authors verify this in their experiments. The authors also attempt to use Stein's lemma to get a better estimate of the gradient, although they claim it does not help empirically.  Quality: The authors experimentally validate all the claims they make, and the paper is technically sound.  Clarity: I think the paper is clearly written and organized, and the code can be reproduced by other researchers.  Significance: Although this submission borrows most of its technical content from other works, I think it is significant because it improves the empirical and certifiable robustness of Cohen at al. The authors decrease the empirical robustness of Cohen et al via their attack, but there is still a big gap between the certifiable robustness and the empirical robustness on Cohen et al.  Miscellaneous: It would help to change the scale of accuracy plotted in the figures. For example, in Figure 3, at L2=0.5, the empirical accuracy against your attacks looks similar to vanilla PGD, simply because 50% and 62.5% are very close to each other in the plot.  ---Edit after rebuttal--- After reading the rebuttal and other reviews, I have increased my score to 7. I accept your argument that your modification from Athalye et al is significant and may look obvious in hindsight. Although I did not criticize the theory in my original review, I agree that you should include your alternative proof.