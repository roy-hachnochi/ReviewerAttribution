Originality: ========= The paper relies on recent results on the implicit local linearization effect of  overparametrized neural networks in the context of supervised learning, and on recent nonasymptotic analysis of Linear TD and Linear Q-learning. Perhaps the main insight is the relationship between the explicit linearization of Linear TD and the implicit linearization of overparametrized neural TD. Related work is properly referenced.  Quality: ========= The paper seems to be technically sound (although I have just skimmed over the proofs).  The convergence of the three algorithms, namely Neural TD, Neural Q-learning, and Neural Soft Q-learning constitute a complete piece of work.  The authors explain the assumptions for the analysis, and discuss their generality and how they relate to previous literature.  The only confusion on my side is that the authors talk about 2 layer neural networks, which made me think about a deep architecture with 2 hidden layers. However, Eq. (3.2) seems to define a single hidden layer with linear output.  Clarity ========= The paper is clearly well written and well organised.  Significance ========= The fact that they have been able to use the same approach to study nonasymptotic convergence of three different Bellman operators is promising. In addition, it is worth to remark that similar ideas to those presented here have been used to study a control algorithm in another submission titled "Neural Proximal Policy Optimization Attains Optimal Policy (5585)," potentially with overlapping authors. However, such submission is different enough from this one, which highlights the usefulness of the ideas presented here. Although the authors assume a simple neural network architecture, I imagine that similar linearization effects can be explored in more complex architectures.    