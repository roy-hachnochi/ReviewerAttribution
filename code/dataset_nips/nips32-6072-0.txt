originality:  Somehow novel. This paper uses "effective number of examples" and "weighted sampling", to reduce the used samples in each boosting round.  quality:  The idea is intuitive and seems can work. The author provides theoretical analysis and explicit experiments to check the performance of the proposed method.  clarity: This paper is well written and organized overall. But the abstract is harsh. It is unclear what's the core idea and intuition of the paper from the abstract. It simply names the three techniques.   significance: somehow significantly. The experiments show that Sparrow reduces the memory needed to train boosting trees, and in some cases converges faster than other baselines trained in memory.  However, two benchmark datasets tree_size=4 make the results  less attractive.    cons and questions: - It is claimed that the inverse of "effective number of examples", i.e. the "variance of the estimator" is the standard quantifier for the accuracy of the empirical edge. However, there's no theory or referring to previous work supporting this claim. It would be better if the authors can provide at least the intuition behind this claim.   - For experiments, the tree size is restricted to 4 leaves. Some datasets, especially large ones, would require much larger trees to get best result. Failing to train large trees faster makes boosting with external storage less attractive.   - Stochastic Gradient Boosting (SGB), which randomly uses sub-samples and sub-features in each boosting round. I think this can reduce the memory cost for in-disk learning. Why not include it as the baseline? - Besides of the number of samples, do you consider to reduce the number of features in each boosting round? What is the hardness of this method? - I think only two benchmark datasets isn't sufficient to support the efficiency of Sparrow. - Does the time cost include data loading time?  - As far as I know, the histogram algorithm in LightGBM isn't memory costly, and LightGBM has a parameter named `two_round_loading`, that can use reduce the peak memory cost for large dataset. But from the supplementary, it seems this isn't enabled. It would be better to have the results with this setting.  - Could you report the result of LightGBM without GOSS? From the LightGBM paper, the speed-up of GOSS is limited and hurt the accuracy a little bit.  - The early stopping rule is based on a previous theorem (Theorem 1 in appendix B). However, the theorem requires the M_t to be a martingale. It is unclear how this condition is satisfied by the "difference between the true correlation of h and \gamma". Does it require the expectation of h(x)y to be exactly \gamma ?  - In the weighted sampling experiment, only XGBoost is compared. As is stated in the paper, LightGBM provides another weighted sampling algorithm GOSS. Could you explain why GOSS is not used as baseline for evaluating the sampling method?  - In appendix A, Sparrow does not scale well as the memory increases. Larger memory does not guarantee faster training.  - The subscript i is missing in eqn (7).   - Is the results run in SSD? if not, do you have results over SSD? 