The paper considers the recommendation systems, where each user/item is represented by d-dim vector x/y. The goal is to predict the user-iter, relevance score. The score is modeled as dot(Phi(Ux) ,  Phi(Vy)). This can be considered as a single layer network with parameters U and V and activation function Phi. The paper analyses the squared loss objective function in this system for sigmoid and ReLu activations. The paper studies local geometry of the loss function and shows that, close to the optima, the function is strongly convex.  - My main concern is the practical application of this approach. In table 1, the RMSE results of NIMC and IMC are shown on movielens datasets. These RMSEs are very large for these datasets. Simple SVD methods are able to get RMSE error of .95 in ml100k. So the first question is why both NIMC and IMC work so bad in these two datasets? - Another concern is the generalization capability of the recommendation systems.  It is well known in the literature that most methods suffer from overfitting since the rating matrix is sparse. So most methods stop training long before getting close to the local/global solutions, which are able to reasonably reconstruct the matrix perfectly. The paper shows that for a simple one-layer neural network, close to the optima, the function is strongly convex. But, considering the facts about generalization, do the authors think convexity around the optimal solution is helpful in understanding why neural network based methods perform better than the linear ones?