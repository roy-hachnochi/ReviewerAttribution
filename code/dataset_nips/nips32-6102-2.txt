Summary: -------- The paper proposes a new perspective on constructing Bayesian coresets. It is shown that the coresets are in fact in the exponential family. Based on that observation, the construction is formulated as an exponential family variational inference problem with a sparsity constraint. The optimization problem can be solved via greedy optimization and an interesting information-geometric view is provided.   Review: ------- First of all, the paper is well written and the math seems to be sound. The derivations are easy to follow and authors do a good job communicating the main results. I find the exponential family view on the Bayesian coreset construction quite interesting. The greedy optimization method seems to be a "natural" way to solve the problem. As a second interesting contribution, I enjoyed the information geometric interpretation which gives a unifying view on coreset construction algorithms. I think that these theoretical insights are from interest to the community and could inspire new research in that field. However, the empirical evaluation seems to be quite limited. Based on the provided experiments it is hard to tell how significant this work is in practical terms. My concerns are:  -  The considered problems seem to be rather simple. They are more of a toy experiment nature (Gaussian toy data, logistic regression, Poisson regression). The significance of the experiments could be increased by considering more challenging (real-world) problems.  - I have a couple of concerns about the plots in figure 3. - First of all, the plot (a) is hardly readable (too many lines in one plot). Also, there is no legend provided, which makes a direct comparison of the methods for one model impossible. - A more serious concern is that in plot (a), the GIGA optimization curves don't seem to be converged. Therefore, the claim that the proposed method achieves a better KL divergence using a smaller coreset size is not really justified. - In plot (c), why does the proposed method sometimes achieve very bad KL (points on the top)?  - Additional to the KL plots, an experiment with a downstream task (i.e. MC sampling using the different coresets) would be interesting.    Minor Comments: --------------- - Eq. 5: \pi_1 is used before it's actually defined - Is the fact that the KL divergence here is a Bregman div anywhere used?   Conclusion: ----------- The proposed method is an interesting contribution and provides new insights on the construction of Bayesian coresets. However, in my opinion, the experiments don't show that the proposed method is superior in practice. First, the considered problems are quite simple. Second, the experiments seem to be immature and the significance is limited. I think the claims about the method are not justified by the experiments. By addressing the issues, the paper would have a much higher impact and would be suitable for acceptance at NeurIPS. I encourage the authors to improve the experiments and think the paper would then be a strong contribution. But in the current state, I tend to reject it.