My concerns were addressed in the rebuttal. Look forward to seeing the accepted version of this paper.   ------------------------------------------- This paper rigorously studies the interaction between the geometry of the constraint set and the geometry of the gradients of online and stochastic convex optimization methods. A complete characterization of necessary conditions for adaptive gradient methods with a diagonal re-scaling to be minimax rate-optimal was presented. The results offer concrete recommendations for when one should (not) use adaptive gradient methods to train her/his machine learning models.    This paper is very well-written and well-motivated. It is of both theoretical importance as well as practical merits. Specifically, the established minimax rate-optimal results for stochastic/adaptive gradient methods are novel and interesting, and they certainly shed light on understanding the role of adaptive gradient methods in training machine (deep) learning models. As training deep neural models involves solving (highly) nonconvex optimization, it would be great to comment on the possibility of establishing such convergence guarantees for adaptive gradient methods in optimizing non-convex functions.  Overall, this submission constitutes a very strong work connecting theory with machine/deep learning practice. It would be inspiring if numerical tests validating the theoretical claims can be included. Below are some minor comments.  1) Line 46. 'conclusions'--->conclusion. 2) Line 60. Typo in D_h((,x),y). 3) Please also define the notation like X_1^n and x_1^n. 4) Line 246. `]1,2]' should be [1,2]. 