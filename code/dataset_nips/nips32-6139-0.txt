The paper connects variational inference with thermodynamic integration, so that the data log-likelihood can be formulated as a 1D integration of the instantaneous ELBO in a unit interval. By applying a left Riemann sum, TVO, a novel lower bound for the marginal log likelihood, is derived in which the traditional variational ELBO is recovered when only one partition is used. The authors then design an importance-sampling-based gradient estimator to optimize the objective, and compare with other methods on both discrete and continuous deep generative models. The paper also unifies other methods like wake sleep into the TVO framework.  Originality and Significance: the formulation of TVO is an interesting idea. Better optimization methods than the importance-sampling-based approach are worth further exploring. The connections to previous methods provides a new insights about unifying different learning methods.  Quality: In section 2, the development of TVO is a solid derivation. But in section 3, the authors claim the proposed method uses neither the high-variance REINFORCE nor re-parameterization. This is confusing because during the derivation, both the score-function trick and the re-parameterization trick were applied in the proposed method. During the rebuttal period, the authors provide more explanations on this both theoretically and experimentally, which I think is very beneficial. Some minor concerns:  1) f was not defined before it's used in section 3. Is it the terms multiplied by 1/K in the lhs of Eq. 2?   2) the authors provides detailed comparison with REINFORCE during the rebuttal period, and shows in math why the proposed method has lower variance, which is nice. However, the gradient of the covariance estimator in the rebuttal feedback is not very obvious to see, so it'll be good if more derivation details could be provided in the final version.  3) In the section of "The effect of S, K, and Î² locations", why does it "increase bias" and "are likely to be more biased" to use the importance sampler? Should it be higher variance instead?  4) As a followup of 3), is it possible that the fact limiting K=2 is enough is because the proposed method has too large variance?  5) It's great to see the authors agree to add more experiment comparisons with the results of regular ELBO optimization using REINFORCE or reparameterization.  6) It's great that the authors explain during rebuttal how the variance reduction technique in Owen (2013) can be applied in the proposed method, and provide additional experiment results to show its effect.  Clarity: The organization of the submission is fine, but in terms of the overall clarity, there's still plenty of room for improvement. For example, more math background needs to be provided on WS and RWS before discussing their connections with TVO in section 4. And the readability of the second half of the paper is clearly worse than the first half. A couple descriptions about the subplot places in Figure 2 are wrong (top left should be top right, etc.). Many sentences in section 6 and 7 could be better phrased to read more like scientific writings.