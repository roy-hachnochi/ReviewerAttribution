Epipolar geometry describes the relationship between views of the same scene captured by a pinhole camera. The authors  refer to the classic reference Hartley & Zisserman's Multiview Geometry book for the details, but clearly explain the fundamental properties they leverage in the proposed scene encoder design. Namely, given two calibrated views of the same scene, for each point in one view, if a match can be found in the other view, it will be located along a line whose equation is a relatively simple function of  the corresponding relative camera poses. That is, if the relative pose of two cameras is known, the search for a matching point between two frames is a 1D search problem.  The authors exploit this fact to reduce the amount of data that a GQN scene encoder needs to ignore/discard when merging information from multiple view into a sensible scene representation, in two steps. After the context frames are encoded, given the query camera: * epipolar lines are extracted and stacked; * search is implemented as an attention mechanism. By using this architecture the encoder doesn't have to learn to approximate multi-view geometry and can use its capacity to solve a simpler matching problem and to fuse information into a more coherent representation. The architecture is described in full detail, and even without source code I am confident the new model could be reimplemented fairly easily (in comparison reimplementing Conv-DRAW would be much harder).  The authors show that  EGQN can consistently outperform vanilla GQN on a subset of the original benchmarks, and that with the new architecture let them scale the model to capture more complex scene geometries. To this end, the authors introduce 3 novel datasets, and commit to make them available (although the website the point to for extra results in not operative).  One aspect of the experimental setup is disappointing: the Maze dataset (available alongside the other datasets) is not used for comparisons. I would have expected this setup to be particularly interesting, and a more difficult one for EGQN as many of the frames are collected by RL agents walking down corridors. The peculiar relative pose of forward motion is a corner case that perhaps cannot be handled by the model (specific implementation)?  There is one detail of the presentation that I found confusing: the abstract at line 7 reads 'requiring only O(n) comparisons per spatial dimension instead of O(n^2)'. The statement is reiterated and somewhat clarified at line 34, but then it's not further discussed in the text. Whilst I do have an intuition of what the author means, I think it would be helpful to explicitly clarify the subject in the discussion of the algorithm. 