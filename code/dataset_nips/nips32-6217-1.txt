= Originality This is an application paper, proposing a new model/task combination. I am not an expert in the area, but the application of using zero-positive learning with autoencoders to such low-level hardware instrumentation data was an interesting new idea to me.  = Clarity The paper is very well-written, and after reading it, I feel that I understood the approach well and could reproduce it.  = Quality The article presents a basic idea together with a number of practical extensions (such as the clustering of functions) that make it more applicable. The experiments support most claims, though the number of examples and example runs are low, and choices there are unexplained: Why were 4 four benchmark programs (blackscholes, bodytrack, dedup, streamcluster) run more often for the "normal" version of the program than for the anomalous version? [The authors replied to this saying "Performance anomalies in some applications do not occur for all test cases". I do not understand how that changes the number of executions: Shouldn't the number of test cases actually executed be constant across the two versions of the program?]  The phrasing of the metrics could also be misunderstood, as the "false positive" rate is often interpreted as the ratio of wrongly generated warnings in program analysis, whereas here, it related to the overall number of analysed runs. This may be problematic because the system is likely to analyse "normal" programs far more often than those with performance regressions, and so the rate of wrongly reported runs may still be very high.  I am also somewhat surprised that considering differing functions alone is sufficient, as many performance regressions stem from a mismatch between allocation and access patterns, but the performance will only be measured (and hence noticed as degreaded) at access time. Hence, in cases where changes to data allocation causes a slowdown at access time, the heuristic of only considering changed functions can easily fail. [The authors replied to this explaining that additional analyses could identify functions that could be impacted by changes; however, I believe that the reply does not consider the implication that such heuristics would mean that much more code would need to be analysed, making it harder to find anomalies]  It would be helpful if the authors could comment on these issues in their feedback, and provide some insight if they were observed in the benchmarks.  = Significance I believe this submission has little significance for the machine learning community, as it does not put forth a new research direction or insights into ML. If the authors indeed release their implementation and dataset, other researchers may also investigate this task, but I feel that a submission to a conference of the software engineering or performance analysis communities may be more impactful. However, I understand the pain of getting cross-discipline work published, and believe that NeurIPS would be a suitable venue for this work.