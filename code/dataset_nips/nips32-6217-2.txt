--Summary-- This work introduces AutoPerf, a tool for software performance monitoring. This approach attempts to detect regressions (performance bugs) in software by examining changes in hardware telemetry data. In particular, they examine HPCs (Hardware Performace Counters) which contain metrics about low level system events such as cache misses and instruction counts. This can be useful for detecting performance bugs encountered in parallel programming scenarios such as cache contention. The authors train an autoencoder on HPC samples from a non-buggy program and compare the reconstruction error against samples from a candidate program in order to detect whether the candidate program introduces a performance regression. Since the number of functions can be large, the authors suggest clustering similar functions using k-means. The authors evaluate their method across 3 types of regressions, 10 performance bugs and 7 datasets and show that their method outperforms competing methods.  --Strengths--  - This is a well written paper that makes an important contribution to an emerging field of ML for systems.  - The authors do careful analysis and present a strong case for the value of their method.  - This method is likely to be immediately useful as a tool for catching performance regressions in software. --Quality-- Overall this paper is of good research quality and the authors have undertaken careful analysis. The authors can strengthen their paper if they address the following issues:  -  From a modeling perspective, the authors need to justify why they chose an autoencoder. The main argument put forward by the authors is that supervised data is not available. However, the authors did not consider other unsupervised density modeling techniques such as Gaussian Mixture Models (GMMs) or Variational Auto Encoders (VAEs). These models can be used to directly evaluate the likelihood of test data instead of going through a reconstruction step.  -  Reconstruction error may not be Gaussian distributed. Looking at figure 3(a) it seems like the distribution is somewhat skewed. The authors may need to study the skew and kurtosis of reconstruction errors in order to strengthen this claim. Alternatively, if they modeled the density directly (for instance using a VAE), then they could apply the “likelihood ratio test” in order to pick deviants.  - K-means clustering for scaling up to many functions has tradeoffs. For e.g. Sec 4.3.3 mentions that a function is assigned to a cluster if a majority of its HPC signatures belong to it. This is heavily context dependant. Suppose a test scales O(n) in input size, then differently sized tests will belong to different clusters.  - The authors should provide end-to-end timing analysis of their method. Right now it takes 84 minutes for autoencoder training, precluding testing at presubmit time as part of a continuous integration setup. Alternate models such as GMMs can be trained faster.  - Sec 5.2 describe how regressions were introduced in the code. However no commentary is made regarding the interestingness of the regression. The authors should specify if they took efforts to break the code in varying and interesting ways. - Minor comment - performance enhancement might be flagged as a bug as well since it causes deviation from expected behavior. Please clarify that deviation is one-sided.  --Clarity-- The paper is well written and was easy to follow. A few minor comments: L231 replace x with y for false negative rate. The variable x is overloaded.  --Reproducibility-- The authors can aid reproducibility by providing the following (if necessary, in a supplementary section): Details of using PAPI and all the HPCs considered. Details about the hyperparameters used for training their autoencoders. The training procedure and how they split their data for training and validation.  --Clarifications/comments-- - The claim in Sec 1, P3 “...AutoPerf, can learn to diagnose potentially any type of regression…”  is perhaps too strong. There can be various causes for regressions many of which are not fine-grained enough to be captured by HPCs. - The authors need to address limitations of their method and list causes of false alarms for a candidate program such as (1) inherent randomness  (2) unrelated confounding factors. As well as false negatives - AutoPerf is only as strong as the tests.   - It’s not clear from the paper, the dimensionality of input to the autoencoder. Does it take in metrics from all 50 HPCs in a vector? If not, then cross-dimensional correlations will be lost.