 The authors present a new formulation to structured graph learning related to Gaussian graphical modelling. The paper contains an interesting theoretical section and is well illustrated.  However, I also have a few remaining questions and suggestions: - It seems, if I understand it correctly, that not a true underlying structure is assumed but one can rather play with different hypotheses as a user.  For example in section 2.4 and section 3.6 remark 1 the authors mention different possible scenarios related to Erdos-Renyi graph, grid graph, scale free networks etc. I feel that in this sense the term structured graph learning is potentially somewhat misleading. - A main result is eq (8) which reformulates (3) with graph Laplacian operator. At this point I am somewhat puzzled whether this should be interpreted rather as a new numerical implementation framework or as an important novel theoretical result? - Though it is interesting that the authors comment on convexity issues of sub-problems in section 3.3, it is not clear whether an alternative convex formulation to (8) would exist or not. Another commonly used regularization technique, which is not mentioned is nuclear norm regularization, which often leads to convex formulations. It would be good if the authors could comment on nuclear norm regularization. - On p.5 line 186 the authors mention that || L w ||_1 = tr(L w H). Is this a new result? If yes, I suggest to explicitly formulate it as a Lemma; otherwise, please provide a reference to the literature.  - is the proposed method also applicable to large scale problems? if yes, how?   I have read the authors' response.