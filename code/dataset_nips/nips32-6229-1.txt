This is a very interesting theory paper showing that a neural network trained with a large learning rate and annealing generalizes better than the same network trained with small learning rate. The authors construct a data distribution which contains two types of features (low noise, hard-to-fit features, and high noise, easy-to-fit features). Under such a data distribution, the authors show that for a two-layer ReLU network trained with large learning rate and the same network trained with small learning rate, the order of learning two types of patterns is different, which eventually results in the gap in generalizations. In the experiment, the authors confirm on modified CIFAR-10 data that different learning rate schedule can indeed influence the learning order and generalization performance. The authors propose a fix to the small learning rate (inject noise before activations), which works both theoretically and empirically.   In the proof, the authors carefully design a data distribution which contains low noise, hard-to-fit feature (Q-feature) and high noise, easy-to-fit feature (P-feature). In the data distribution, a very small fraction of data only has P-feature, a large fraction of data only has Q-feature and the remaining data has both P-feature and Q-feature. For the large learning rate and annealing, the network first learns P-feature and learns Q-feature after the annealing. On the contrast, the network with small learning rate quickly memorizes Q-feature and can only learn P-feature from the samples with only P-feature. Since the number of samples with only P-feature is small, the network can only learn a small margin, which results in the bad generalization performance on samples with only P-feature.  Here are my major comments: 1. In the paper, the authors consider logistic loss with l_2 regularization. I was wondering whether this analysis can be extended to other losses (for example, mean square loss). It would also be good to explain the reason we need regularization here. 2. I feel this result requires the fraction of samples with only P-features to be small, otherwise the network with small learning rate can learn P-feature well just from these samples. So I was wondering whether it’s possible to identify a data distribution with only one type of features in which the large learning rate schedule still generalizes better than small learning rate. Of course, in this case, the order of learning features is the same (only one feature) and the generalization gap must due to some other reason. Although this is beyond the scope of this paper, it’s still good to talk about such possible directions.   ----------------------------------------------------------------------- I have read the authors' response, which resolves most of my concerns. I think this is a very interesting theory paper. I will keep my score as it is. 