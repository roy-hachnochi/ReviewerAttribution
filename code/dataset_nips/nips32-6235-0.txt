After rebuttal: thank you for the additional experiments. They strengthen the empirical contribution of the paper, so I've increased my score to a 7. ________________  Originality: The paper is a novel combination of known techniques: by reinterpreting the the iterative node aggregation procedure of Kipf et al's GCN as feature smoothing technique, they develop a novel feature mapping function for learning positive semi-definite (psd) graph kernels. The key difference from the Kipf et al approach is they separate the node aggregation and non-linear representation learning components: node features are the output of a multi-layer perceptron and then aggregated once (rather than at every layer) by a multi-hop aggregation function. They argue theoretically that this approach is universal in the sense that it can approximate any invertible psd kernel.  Quality: I thought the empirical results of the paper were interesting because they suggest that decoupling the aggregation and representation learning components of GCN-style models leads to better performance (at least on these datasets). I would have liked to see more ablation experiments to explore this further: e.g. how does performance change with fewer / more hops? What role is \omega_h playing empirically? e.g. Would a fixed constant, say c^h, c \in [0, 1] lead to similar performance or is it necessary to have a trainable parameter? Do you need an MLP or would a single projection layer suffice? If not, how many layers do you need?  I didn't find the theory as interesting - it essentially says you can learn psd kernels (Theorem 1) & that they are universal (Theorem 2). That may be true, but I'm not sure it buys us anything beyond the fact that you're learning a similarity function?  Clarity: in general the paper was well-written and clear. That said, table 1 was missing what units it was in - accuracy? AUC? Also are those standard error or standard deviations?   Significance: I thought this is an interesting contribution because it gives a new perspective on the relative importance of the components of GCN-style models. That said (as mentioned above), a proper ablation study that explored the implications of this perspective would make this a far stronger contribution.