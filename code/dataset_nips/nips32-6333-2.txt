Originality: This paper proposes a new method for sample selection in continual learning, which is based on GEM and reformulates the problem from the perspective of constrained optimization. Comparing with other related works, the experiments in this paper have been conducted in a different setting of continual learning,  that is, the task boundary is unavailable and the model can only iterate on a mini-batch for a few times.  This setting is worth to bring out as it is more likely in the real world.  Quality:  The paper is well written and easy to follow. In my understanding, the intuition is straightforward and the formulation is reasonable.  In the experiments, the proposed methods have been compared with several different strategies of sample selection as well as some existing methods (i.e. GEM and iCaRL) by average accuracy. It could be more complete by comparing Backward Transfer with GEM since GEM has shown an advantage in it and this work is based on GEM.   Clarity: The problem setting and formulations are clear. However, there are still some  issues I would like the authors to clarify a bit: 1. Is it possible to give a consistency analysis for the greedy method (Alg. 2)? As increasing n to M, is it equivalent to the IQP method? 2.  What's the value of n you used in the greedy method in your experiments? Is this value critical to the performance of the greedy method?   3. You used a batch size of 10 samples in all experiments, which is quite small, did you try other sizes? Would a larger size perform better or not?  4. The results were averaged over 3 runs, but how about the standard deviation? Does the greedy method have a larger variance than other methods? 5. In Tab. 3  T4, the random selection has much higher accuracy than others, do you have any insights on this?     Significance:  The proposed methods show promising results compared with some state-of-the-art approaches and the greedy one shows practical feasibility for large scale buffers. The paradigm of continual learning used in this paper could be preferred in some real-world applications.  ############################# I generally feel no reason to change my score after reading authors response. The extra experiment results are quite interesting, the performance decreasing while increasing n. It would be great if authors can provide some insights on it. 