Update after author response: - I thank the authors for their response and for the work they put in. I see now that the derivation for eqn 7 is clear; I think I forgot to include the detail that xb was 0 when I tried to replicate the derivation, for which I apologize. However, it seems that to get from the second line of eqn 7 to the third line of eqn 7, the term with the second derivative is dropped. I am not sure whether this is justified, and it is probably the reason for the counter-intuitive conclusion of that theorem. But more importantly, I don't feel Theorem 1 is even necessary for the method as a whole to make sense; I think people will accept the statement that features that have low importance are not likely to change the output when perturbed, assuming that the importance scoring method is good, even if there are edge cases where this may not be true. - I appreciate that the authors included a comparison with Platt scaling. From my understanding of Figure 2, it seems that Platt Scaling also assigns lower confidence to out-of-distribution examples, as reported in the prior literature. It would be interesting if the authors could combine the confidence rating from Platt Scaling with the confidence rating from their proposed method to develop and even more effective algorithm than Platt Scaling alone. I know the authors said that the advantage over Platt Scaling is that they don't require access to a held-out validation set for calibration, but in practice people do tend to have a held-out validation set, so I am not sure the need for a held-out validation set is much of a barrier. - I also appreciate that the authors included a quantitative evaluation in Figure 3. It would have been ideal if the authors had reported the results from Platt scaling in this figure too. - It is interesting to see that DeepSHAP outperforms Integrated Gradients and the authors should definitely include this in the paper.  Since it is not clear to me that the method does better than Platt Scaling, and I think in most practical situations people do have access to a held-out validation set for calibration, I am leaving my rating at "marginally above acceptance threshold"; the paper has value in terms of the originality of the attribution method proposed, but appears to need some further work before it can become a practical tool that people will use. If the authors could find a way to combine their proposed confidence metric with baselines from the literature to find something that works better than the baselines from the literature, I think that would be what is needed to encourage adoption.  --- I tend to vote in favor of this submission because I think the core idea, that of rating the confidence of examples according to how easily the prediction changes when the examples are perturbed, is a potentially very promising approach that has not been explored enough in the literature. The major drawbacks are (1) Theorem 1 seems suspicious given the ability of Integrated Gradients to highlight the importance of saturated inputs where the partial derivative is locally zero, (2) the choice of all all-zeros baseline may not be a good idea, and (3) the empirical benchmarking could be improved.  Originality As mentioned, I think the core idea of rating the confidence of examples according to how easily the prediction changes when the example is perturbed is clever. Although this approach may have been used for other types of ML models, I haven't yet seen it adapted to the context of neural networks. The idea of using some form of importance scoring to prioritize the perturbations also makes a lot of sense.  Quality - I feel that the empirical benchmarking could be improved. Minimally, I feel that the authors should compare to a baseline that uses calibrated predicted probabilities. The authors do discuss the baseline of calibrated probabilities in the related works section and argue that a downside of this approach is that it requires access to a held-out validation set for calibration. However, modern neural networks are almost always trained with a held-out validation set for early stopping and hyperparameter search, so there are not many cases where a held-out validation set is unavailable. In the caption of Figure 4, they said "The softmax confidence (green) is always high and does not reflect accuracy", but it was unclear whether this softmax confidence was calibrated. - I would have also appreciated seeing more quantitative evaluations. For example, if examples were ranked according to the confidence score, what would be the auROC for identifying misclassified examples? Alternatively, they could quantify by how much the performance of the classifier improves if the least-confident examples are abstained on. Quantitative metrics such as these would make it easier to compare different methods. By simply viewing the distribution plots, I cannot clearly tell, for example, whether the confidence estimates from the Bayesian model are better or worse than the confidence estimates obtained by the proposed method (though I understand that the proposed method has several advantages over building a Bayesian model). - I am concerned about the choice of an all-zero baseline for Integrated Gradients. The choice of baseline is a very importance consideration for reference-based interpretation methods (see https://github.com/kundajelab/deeplift#what-should-i-use-as-my-reference), and using an all-zeros baseline may not be a good idea, because it will make it impossible to highlight pixels that are black (as those pixels will be equal to their baseline value). It may be better to sample several different reference values and average the results over them, as is done in the DeepSHAP implementation. An approach like DeepSHAP would also have the advantage of being computationally faster than Integrated Gradients.  Clarity Theorem 1 also seems suspicious because, as mentioned earlier, one of the main selling points of Integrated Gradients is the ability to assign importance to features that have saturated in terms of their partial derivatives. In other words, Integrated Gradients is designed such that it can assign high importance to features for which the partial derivative is zero. This seems quite contrary to the statement of Theorem 1.  Significance I think the core idea of this paper - that of assigning confidence for a deep learning model based on how much the predictions change when an example is perturbed - has the potential to catch on. At its core, this method is like the nearest-neighbors-based density approaches - however it does not require access to the training set at prediction time, which is a major advantage. The idea of sampling perturbations based on some form of an attribution also has the potential to catch on, in my opinion. I am less sold on the use of Integrated Gradients with a zero baseline, and I think the empirical evaluation could be improved, but to me the core idea makes a lot of sense even if the evaluation is a bit lacking.