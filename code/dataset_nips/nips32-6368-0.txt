-- The two theoretical results are, to the best of my knowledge, novel and make use of well-established ideas from information and probability theory in an interesting manner for their proofs. My concern with both the results is their significance and interpretation. The first theorem claims to show that the use of the exponential family guarantees robustness, but the actual content of the theorem concerns the growth of the divergence with the parameters of the added noise. The probability of this divergence being large is what actually controls the risk. The relationship between the noise parameters and this growth is not elucidated in the paper, making it hard to see how the robustness actually comes about theoretically. Further, the theorem concerns a quantity that has been derived from the true risk on line 99, but the connection between misclassification (the true risk) and the magnitude of the divergence used is not clear. -- For Theorem 2, from the plots in Figure 1, the accuracy guarantee falls to 0 even with a small adversarial budget, especially for smaller noise magnitudes. It would be instructive to provide a discussion of how the accuracy guarantee can be tightened since, for real classifiers, the accuracy is much higher. Further, the gap in Theorem 2 is between two true risks, so referring to it as ‘generalization’, which is a term usually used for the gap in performance during training and test, is a misnomer.  -- The evaluation results are interesting but need more clarity in their presentation. For example, in Figure 1, how is the accuracy guarantee with no adversary calculated when from Theorem 2, only the gap can be computed? Is that an empirically obtained quantity? If yes, then that has to be mentioned.  -- In general, related work has been cited adequately but a reference to Ford et al. (Adversarial Examples are a Natural Consequence of Test Error in Noise) is missing, in spite of it being very closely related.