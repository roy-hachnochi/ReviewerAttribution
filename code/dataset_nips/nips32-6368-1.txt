Many adversarial defense techniques have been developed recently for improving the robustness of neural networks against different adversarial attacks. This paper focuses on defending neural networks by injecting random noise drawn from a subclass of the exponential family of distributions during both training and testing. As listed above, the paper makes significant novel contributions to formalizing robustness in the considered setting and provides both experimental and theoretical evidence for increased robustness. However, I have a few concerns below I hope the authors can address during rebuttal: 1. Line 72-74 states that the paper provides results on certified robustness. However, the setting considered in this paper is probabilistic and does not provide absolute guarantees on the absence of "any" adversarial example in an adversarial region. There has been considerable work on training with certified defenses, see [1][2] and [3]. It would be good if the authors can compare and separate their work from those based on certified defenses. 2. In table 2, it is not clear what conclusion can be drawn from the reduced success rates for the two attacks wrt Madry et al. 2018.  Can it be the case that the addition of noise make the attack problem harder for both attacks and thus they are unable to find adversarial examples while the number of adversarial examples remains the same as Madry et al. 2018?  References: [1] Provable defenses against adversarial examples via the convex outer adversarial polytope. ICML'18 [2]Differentiable Abstract Interpretation for Provably Robust Neural Networks, ICML 2018. [3] On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models, Arxiv 2018.