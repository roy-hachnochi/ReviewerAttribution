This paper describes an approach to improve rehearsal-based continual learning techniques (either replay-based or with a generative model) by identifying samples that are most useful to avoid forgetting. This is achieved by computing the increase in loss on the replayed samples, and using this to determine which samples should be used during learning.  It is a simple and intuitive idea, the paper is clearly written, and experiments on multiple datasets are compelling. I think it could make a nice addition to the conference, but needs a few improvements first.  My main criticism is that the approach requires a separate virtual gradient step for each actual step, to compute the change in loss on the replay samples. I think the discussion around this could be stronger, eg. explicitly mentioning the overhead in computation / memory for this approach versus others. Is there a way to alleviate this additional step (eg. keep a running estimate of 'utility' for each sample based on previous gradient steps)? If not, a fairer comparison with baselines should include twice as many gradient steps (though still online). Does this make any difference?  A few other comments and questions: - Why is the hybrid approach in the appendix rather than the main experiments? It seems like one of the central proposed methods to this paper - and given it's introduced in the main body, the experiments should appear as well. - The performance of baselines in Split MNIST seems poorer than that reported in related work (eg. iid offline is 97, and Deep Generative Replay (which is not included in this paper as a comparison) is at 92. Is this because of multiple vs a single pass? Additional baselines and clarification would be useful here. - The only external comparison is with GEM and, effectively, DGR (the GEN case), with the rationale that prior-based approaches like EWC don't perform as well, but I think other performant methods should be included (eg. VCL). This may not require reimplementing the approach, but comparing with the same conditions as previously published results. - In the single-pass setting, things like learning rate become more important - how were hyperparameter sweeps for baselines performed? - Some of the reproducibility checklist items are missing, and are easy to fix (eg. description of compute)  POST-REBUTTAL: After reading the response and other reviews, I think the authors have done a great job of addressing my concerns. The additional baselines (both on the side of additional gradient steps and on other approaches like VCL) strengthen the paper. The discussion around gradient steps and computation provided by the authors in the rebuttal is convincing, and I think this should appear in the camera-ready version. i.e. comparison of computation / gradient steps taken by other approaches, and the 2-step baseline to show that this doesn't help with a standard approach.  I think this is a good paper, and I have increased my score to a 7.