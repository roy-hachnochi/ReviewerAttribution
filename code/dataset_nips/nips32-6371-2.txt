Originality: This paper considers generative modeling in the online continual learning setting for the first time. While previous algorithms in continual learning are based on random sampling to store the past samples, the authors proposed more sophisticated method to construct the memory structure.  Moreover, the authors treats Hybrid approach of experience replay and generative replay to obtain the beneﬁts of both.    Quality : The authors do not provide theoretical guarantee for the proposed algorithms. Although the experimental results show its effectiveness compared to the existing state-of-the-art algorithms, the proposed methods are not applied to longer task sequence than 10 different tas and real-world data beyond relatively easy benchmark dataset.   Clarity: Although this paper is well-organized, there are some concerns about the clarity. What is the definition of the sample loss l in Section 3.1? Do not it have two variables like l(f_\theta(x),y)?  Similarly, do not \mathcal{L} have two variables in equation (1)? I do not understand the meaning of “Single trains the model …” in line 181.   Significance: This paper has great significance because this paper considers generative modeling in the online continual learning setting for the first time as stated above. In this sense, this paper address a difficult task which was not treated in the previous studies. Moreover, the authors propose a better sampling method for memory structure.  