This paper addresses finding bugs in short pieces of code. The dataset used in the paper is created from student's submissions/instructor's tests to a introductory programming class. The technique consists in two parts: (1) a component that predicts which programs are buggy (fail a test) without running the program by training a convolutional network that operates on flattened code ASTs.  I found the encoding of the programs/ASTs to matrices and the use of CNN quite interesting. I'm guessing it might be faster to train such networks than the RNNs that are usually employed when representing/embedding code.   The application of prediction attribution to bug localization is also interesting and most likely novel.  The paper is generally well written and straightforward until the evaluation section. Please see comments below.   ================ UPDATE: Thank you for your feedback. I encourage the authors to polish the experimental section which is hard to follow. The explanations in the feedback helped me better understand the work, thank you.   In addition, I would state - even if informally - the experience with running RNNs on this problem. Thank you for confirming my intuition that the RNNs are super-slow to train for this type of problems. I think it's important to communicate this type of results. Currently we have few/no venues to communicate "negative" results, and, IMO "negative" results are as important (if not more important in some cases...) as "positive" results. (I'm using quotes because everything is relative).   I'm increasing my overall score from 6 to 7 in the light of the clarifications during author feedback. 