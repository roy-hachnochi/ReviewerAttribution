************ [updated after the rebuttal] I am quite positive about this paper, robust approaches are important in practice. I understand that for heuristic approaches like this, proving theoretical guarantees is hard. The authors at least give some intuition about its asymptotics in the rebuttal, which should be easily formalized (and I would request it to be added to the paper if accepted). The authors did a very good job argumenting for their approach and evaluating its performance numerically, on realistic examples.  However, NeurIPS might not be the best venue for such work. It only covers evaluation of a policy. Perhaps a follow-up paper which covers incorporating this approach into optimization (policy improvement), which the authors say is left for future research, would be more suitable.   If the paper is rejected from NeurIPS, I would like to encourage the authors to add some (even if weak or asymptotical) theoretical guarantees and submit it elsewhere. Also some of the approaches mentioned in the books by Warren Powell (on Approximate Dynamic Programming and on Optimal Learning) might help the authors to refine or support their approach, or serve as additional benchmarks. ************   Originality: I can't judge since I am not familiar with work in this area. Quality: The proposed algorithm is designed based on in-depth understanding of TD and MC approaches and on their strengths and weaknesses. Clarity: very good, easy to read, no typos. Significance: A potentionally useful algorithm as shown on a wide range of problem instances, but its significance hasn't been evaluated by benchmarking with other methods.  I have not identified ay minor comments such as unclear sentences or typos. 