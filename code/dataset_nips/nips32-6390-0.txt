= Originality Improving the translation procedure by leveraging error detection on the target programming language is not a new idea, even though the authors do not seem to be aware of this. In semantic parsing, this has been used quite a bit over the last year, under the name of "execution-guided decoding" (see for example the WikiSQL leaderboard, where the top 4 entries currently use this). I believe this first appeared in "Execution-Guided Neural Program Decoding" (Wang et al, 2018); the idea is closest to the prefix-based pruning idea in this submission.  This leaves the new "multiclass classification" error localization technique as original contribution, as well as the new dataset.  = Clarity The core contribution of the paper is a new algorithm, which is only described in prose; I would have preferred pseudo-code here, which would have been more concise. There are a number of technical choices that are not explained in detail, nor evaluated in the experimental section. For example, the positional encodings in the mutliclass classification error detection setting were surprising to me. I would speculate they may help because (a) the true error never being /after/ i_err (is that the case?), (b) positional encodings allowing to filter out later lines, (c) positional encodings allow to downweight lines that are far away from the reported error location. [The authors have provided some additional information on this in their response, and it would be nice to include this in the next iteration of the paper as well] This could be tested by an alternative version in which the positional encodings are replaced by a single bit for 1_{i_err = i}, and removing all lines after i_err from consideration. I would also be curious to see alternative sequence models here (e.g., a simple self-attention layer).  The authors also never comment on the fact that the transfer from training to unseen programs seems to work significantly less well than the transfer from training workers (and idioms) to new workers. This seemed like a very surprising result to me, and merits some analysis.  = Quality The presented decoding strategy seems sound, and the experiments seem to be carried out to a reasonable standard.  = Significance While I agree with the authors' point on the problematic shortness of existing program synthesis tasks, I am not convinced that their pseudo-code to code scenario is realistic: In what setting can we expect a user to provide a line-by-line description of the target program? I cannot conceive of a user that would able to provide all the algorithmic detail, but not be able to write this in an actual programming language. The opportunity for machine learning is exactly in the resolution of ambiguity of high-level user descriptions, not in the mechanistic translation from statements like "set min_i to j if A[min_i] > A[j]" to code.  Hence, I do not expect the new dataset to be of substantial impact. While it allows for evaluation of program decoding with very strong supervision, I do not see that as the crucial problem in the research area. [The authors also do not comment on their relationship to approaches that use knowledge about the target language's syntax and semantics in their decoder and subsequently lead to almost no compilation errors (e.g. "Generative Code Modeling with Graphs", Brockshmidt et al., 2019)]