The paper is clearly and well written. The proposed synthesis benchmark from natural language pseudo-code with I/O test cases opened new directions for tacking various challenges in program synthesis (e.g., credit assignment, execution-guided synthesis).   However, an issue with the newly collected dataset is that it assumes line-by-line mapping between the pseudo-code and the machine-executable code. However, in real-world use cases it is common for pseudo code to "omits details that are essential for machine understanding of the algorithm, such as variable declarations, system-specific code and some subroutines."[1]. Have the authors considered relaxing this assumption of having line-by-line mapping between the pseudo-code and the C++ code?  [1] https://en.wikipedia.org/wiki/Pseudocode  **Update**: Thanks so much for the detailed response! To add a bit:  (1) [Dataset] The newly proposed dataset considered multi-line code generation given manually annotated pseudo code. Compared to NAPS, it is a line-by-line annotation (finer granularity) with significantly larger amount of human-annotated examples. Compared to the other (relatively large scale) line-annotated Django dataset, examples is this dataset are at functional level (more context), with extra I/O examples to guide the search of possible programs.  (2) [Synthesis Algorithm] The proposed synthesis method is a simple line-based approach without considering context, with error localization methods to prune the hypothesis space.  For (1), the central argument is that the line-by-line annotation is far from real-world settings, where the alignment between user's intents and the target code blocks is latent. This assumption would make the proposed synthesis algorithm not applicable to such real world settings, neither would the proposed approach shed light on how to advance its modeling towards tackling this latent alignment problem. In contrast, existing benchmarks like NAPS consider a more realistic setting, although most inputs are synthetically generated.   For (2), while the algorithm is inline with the general idea of execution-guided decoding, in my opinion, the idea of using a trainable error feedback mechanism in synthesizing open-domain multi-line programs still has novelty, compared with using a deterministic executor to guide the generation of domain-specific logical forms.  To summarize, I feel the technical contribution from (1) might be limited apart from introducing a new dataset, while (2) would inspire future work on leveraging execution information for synthesizing complex programs. Therefore, I lowered my score to a weak acceptance.