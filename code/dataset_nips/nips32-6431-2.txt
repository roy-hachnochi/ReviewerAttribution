The authors provide (almost tight) lower bounds for the generalization errors for boosting based classifiers, in terms of the empirical margin, thus showing that the algorithm due to GZ13 is almost optimal when the only problem parameter is the margin. This work, hence, encourages a search for other data-dependent empirical quantities for the problem exhibiting stronger generalization.  It is good to understand the problem well, but I fear that I do not see a lot of impact of this result on the way people currently think about boosting.   All their lower bounds, construct a distribution D (which may depends on  m, H and A). This is the standard flavor of lower bounds for the generalization error.  However, the lower bounds developed are not fully satisfying. Theorem 1 does not suggests that the returned classifier which has a large test error, has small \theta-margin error on the training data. Theorem 2 does not suggest that the presented f which satisfies both the properties is the one returned by any “interesting” algorithm or is even computable from data.  An ideal lower bound should look like - “No algorithm can return a classifier f which has small error (margin) on the training set, but a large test error for all data-distributions D.”  Originality: The authors combine old ideas to prove lower bounds under different settings in a very clever way to get the required lower bound. The idea of lower bounding the population risk by the sum term (claim 4 - supplementary doc), which is further easier to lower bound with high probability, is also quite interesting. However, the techniques used to prove the lower bounds in each of the pieces are not new.   Writing:  The paper is overall well written. Using the same variable d to represent both the sparsity parameter and the data-set split (in lines 274-285 in the supplementary) is confusing, and I would recommend the authors to fix it. Line 289 also contains a small typo in the definition of \psi_1.    ** After Author Feedback: Thank you for clarifying my confusions regarding Theorem 2. I find it quite interesting now. I would like to keep the same scores as Theorem 1 is still quite unsatisfying and the author responses did not really help. 