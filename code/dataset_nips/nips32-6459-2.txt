Post Response Comment: ========================================== I think the authors have addressed my initial concerns, therefore I maintain my initial stand and incline to accepting it.  Originality ========================================= The setting is new as far as my knowledge can tell. Previous work such as "Certified Defense for Data Poisoning Attacks" considers contaminated instance within a feasible set, but modifying each training point by a small amount for an offline learner is new to me.    I saw a backdoor attack in reference ([5]), but it is not referred to in the main body.  I think the difference between this attack and the backdoor attack is that this one doesn't require the backdoor pattern to activate during test-time.   Quality ========================================= The paper is technically sound overall. The most interesting part of the algorithm is using an encoder-decoder net instead of directly doing gradient ascent on the clean inputs to generate attack instances.   Clarity ========================================= The writing can be more compact. There are running sentences here and there. Despite these flaws, the ideas are clearly conveyed.   Significance ======================================= My main concern about the paper is the practicality of the setting. The author proposes a (hypothetical) setting in intro, but I'd like to see more explanation about the purpose. Is it to protect privacy? If so, how is it different from using synthetic data, say generated from GAN?   I also have two suggestions.   First, since SVM is also undermined by the attack, is it possible to visualize the SVM weights? Maybe it allows us to identify the pattern injected in the data. (Instead of a digit classifier, it might now detect whether a horizontal grey stroke is present.)  Second, again for some convex model such as SVM or LR, the optimal model should satisfy some KKT condition (e.g. gradient=0), which can be a function of the training data. Maybe the conditions can shed light to how and why the training data are perturbed in some way.