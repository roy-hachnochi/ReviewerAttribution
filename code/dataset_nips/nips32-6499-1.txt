Originality: To my knowledge, the proposed tree positional encodings are novel. They are fairly straightforward, using concatenated one-hot embeddings for each level of the tree (below the maximum depth).  Quality: Some terms are inappropriate. In particular, the transformations between embeddings at different layers are not linear or affine (for which there is a unique inverse). The experiments are sufficient and demonstrate the viability of the approach. The authors obtain state-of-the-art results on some, but not all tasks considered. The authors correctly mention that the composition property breaks down past the maximum depth. Uniqueness also fails at this point, but there are no experiments evaluating whether this can be a significant issue in practice.  Clarity: I had some issues understanding the paper. The term "linear" or "affine" was particularly confusing. In figure 1, it would be helpful to show the actual representations, in addition to the sequence of transformations. Datasets are often introduced with little details, referring to other papers.  Significance: As sequential data isn't always the most appropriate representation, extending the transformer models to other data structures could be useful for many tasks.