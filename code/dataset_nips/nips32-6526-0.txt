The problem being tackled is that of describing the dynamics of Q-learning policies (through Q-values). This problem has been tackled before under a two player settings and this paper's main claim is its extension to n-player settings.  The paper is well written and does seem to tackle a problem currently not reported elsewhere, however, I find the work lacking in two fronts: i) it is too derivative and incremental. Given the small contribution on the technical side, I would have liked a strong experimental section, but that is not the case and ii) experimental validation of the model is insufficient. The experiments are not showing anything insightful and (as I argue below) are not very useful.   In order to improve this work, I suggest the authors to focus on developing good insights into the use of these methods for the design of new multiagent RL algorithms. In its current form I really do not see how the results presented here could be used in any way to design new algorithms. More so, given that there are already published works on multiagent RL in the mean field setting  [Mguni et al. AAAI 2018, Mguni et al. AAMAS 2019], which by the way, these works are not even sited here, they should be used to validate this paper's models.   