The paper proposes using lagrange multipliers + stochastic gradient descent to incorporate constraints during the learning of deep neural networks.  Both the incorporation of lagrange multipliers (except for the nice hinge trick to turn N constraints into 1 constraint and limit the number of dual variables) and the language to encode constraints are relatively straightforward and easy to follow.  The results look positive, and seem to show an improvement over applying constraints only at inference time, and a nice gain from using the constraints in a semi supervised setting; both of which reproduce constrained learning results which were known in linear models but haven't been replicated yet in deep models.   ----- after author response  No changes to my score.