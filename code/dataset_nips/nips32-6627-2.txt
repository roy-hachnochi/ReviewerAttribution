Post feedback:  1. thanks, this clarifies the speed up confusion. So, you want the factor p^*\tau^* to be not too large, both by ensuring high sparsity and by having a small enough number of parallel processors that \tau^* does not blow up.   2. Got it, but this does limit the usability of the algorithm to a narrower class of problems that adhere to p^*<1. I don't recall too many practical examples of this in standard machine / deep learning.  3. I think you may be missing a \mu in your expression, it should be f+\phi - \frac {\mu} 2 ||x||^2 ? I see what you are saying here though.  Thanks for the clarification; I'm happy to raise my evaluation to 7.    --------------- Originality: The authors claim a broad-framework style contribution that subsumes a lot of open or previously solved analysis cases.   Clarity: The paper, while written fluently, is addressed to an audience that is well-versed with the precursor work. Key terms such as "linear speedup" seems hard to grasp for me, not being from the asynchronous optimization area (see below in "improvements").  Quality& Significance: The results sound significant in the breadth of cases covered in the analysis framework proposed by the authors. A main claim here is that the analysis allows for optimization over arbitrary convex constraint sets, while previous async optim algos required unconstrained or box-constraints only. This improvement is achieved by pushing the constraints into the prox-operator (pg 4) which is assumed to be easily solvable. 