*****  Post-rebuttal update: after reading the authors' feedback, I confirm my positive evaluation of this paper.  *****    I would like to thank the authors for their submission.  Summary  The paper presents a novel unified theoretical framework and new measures for the calibration properties of multi-class classifiers, which generalize commonly used ones. Estimators for the proposed measures, based on vector-valued RKHS, are then proposed. The statistical properties of such estimators are theoretically characterized (including proofs), and statistical tests associated to the estimators are presented. Finally, the properties of the proposed estimators are exhaustively validated in supporting simulated experiments.   Originality  The proposed ideas are novel in the context of calibrated multi-class classification. The proposed methods (e.g. the definition of KCE and the estimators) draw tools from matrix-valued kernel methods and kernel two-sample tests, which are appropriately referenced. To my knowledge, this area of research has a rather scattered coverage in the literature. The paper presents potentially high-impact novel contributions and provides a much-needed rigorous unifying view on the topic. To my knowledge, other relevant works in the area are correctly referenced and differences clearly stated.   Quality  I have found the paper to be of rigorous technical soundness. Definitions and statements are all clear and quantities properly introduced. Statements are remarkably supported by both theoretical statements, including full proofs, and exhaustive, well-designed experiments on synthetic data.   Clarity  The motivation, context, literature review, problem statement, theoretical claims and experiments are all delivered with excellent clarity and well-organized. The paper is smooth, polished and pleasant to read.   Significance  As stated in the "Contributions" section, I deem the overall significance of this work as high, from both the theoretical and practical perspectives. Developing rigorous tools for characterizing the quality of predictive models' confidence predictions is an important priority for the field, and I have little doubt about this paper representing an important step forward in this direction.   Typos and minor comments: - Capitalize first letters in title and section headings - L15: patients.Since --> patients. Since - L17: increasing the training data set is... --> increasing the training data set's size is... - L22: Thus, - L24: uncertainty, this - L45: complementary - L46: miscalibrated, - L56: Recently, - L58: ..., 0.3) since --> ..., 0.3), since  - L70: detail, - Eq. (1): is the conditioning on $max_y g_y (X)$ right, or could it just be on $g(X)$? - L104, L114, L121, ... : Thus, - L104, L117, L118, ... : eq. --> Eq. - L159: a general calibration measure of strong calibration --> a general measure of strong calibration - L180: setting, - L257, ...: fig. --> Fig. - L310: Consider citing as: Carmeli, C., De Vito, E., Toigo, A., & Umanit√°, V. (2010). Vector valued reproducing kernel Hilbert spaces and universality. Analysis and Applications, 8(01), 19-61.