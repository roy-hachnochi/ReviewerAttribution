Summary: In this paper the authors propose a unifying framework for testing multi-class classification calibration, i.e. if a classifier gives a probabilistic output, how close (or if, as a statistical test) are these outputs to the actual probabilities given the classifier (the actual definition is actually a bit more subtle, but this is the general idea). To do this they introduce a framework involving an integral probability metric. They propose using a "matrix kernel" RKHS as a nice way of controlling the function which they maximize in the probability metric. With this kernel setup they have a nice closed form expression for finding the calibration error and some finite and asymptotic bounds describing its behavior. Finally the authors perform experiments where they empirically evaluate the distribution of the test statistic, as well as its type I and II errors, and compare to a classic method.  Overview: Overall the paper reads well and is fairly clear. The theory looks good and I see the kind of theorems and bounds I would expect to find in this kind of paper. The experiments seem good, although I think they could consider more competitor techniques (assuming they exist, I am not terribly familiar with this specific topic). Perhaps it would also be interesting to see how the test performs on a real world dataset; however this test statistic does not depend on the value of X, just on Y values and the prediction output distributions, so perhaps a real world dataset doesn't really add much to what the authors have done already.  The supplementary material is quite extensive and contains quite a bit of good, mathematically correct, theory along with more experiments.   Potential Issues: It is unclear to me why "matrix valued kernels" are necessary for this test. I haven't personally derived anything but it seems likely that one could do this sort of test using standard kernels. Perhaps the "matrix valued kernel" arises naturally from the obvious test statistic, but it would be nice to know why it is necessary for this concept to be introduced.  I am unsure how significant this paper is. The fact that the paper only provides a statistical test is a bit concerning. Again I am not familiar with this particular problem, perhaps it is difficult and/or important enough that this paper is important.   I found l.213-216 a bit too mysterious for my liking. Is it not possible for other estimators to estimate (3). Does this method allow (3) to become tractable, or are you simply observing that one should keep in mind that test statistics come from (3).   Verdict: While I'm not entirely sure of the significance of the topic the paper is well written and the research seems high quality, so I recommend that it should be accepted.  Small errors or potential improvements:  l.6-8 This sentence is grammatically incorrect, or at least strange. Maybe something like "We present a new method based on matrix-valued kernels which offers consistent and unbiased..."  l.15 Missing a space after the period  l.18 "would be" should be "is"  l.35 maybe this is nomenclature for the topic, but I'm not totally sure what "subjective" means here  l.38-47 I found this paragraph confusing while reading. I think it could be improve by using a concrete mathematical expression or two. I didn't really understand the problem until I got to (1) and (2)  l.52 "were" should be "have been"  l.133 "matrix-valued" kernel should be \emph  l.139-148 Its difficult for me to figure out what here is established theory and what is stuff that you have developed and have just left underived. For example is "universal kernel" totally analgous to the standard definition or is it a bit different? Also how does the Micchelli and Pontil 2005 relate to this? Did they actually introduce the matrix valued kernel or just a vector valued one which you use as the basis for the matrix one.   l.161 its not clear to me why the CE has to be infinity if its not 0. If F only contains bounded functions that it seems to me that maximizing over F should remain bounded. I wonder if you are already adopting some of the intuition from the RKHS function class (e.g. assuming F is a vector space)  l.219 "greater or equal than" should be "greater than or equal to"  I find the P[...] notation a bit strange, I think it is definitely less standard than P(...)  l.225 "weak" should be "loose"