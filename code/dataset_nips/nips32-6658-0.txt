Originality: the paper is original in many aspects: definition of classwise-calibrated vs. confidence calibrated, as well defining classwise-reliability diagrams, the use of a specific parametric family for calibration which had not been done in previous research, as well as defining ways of visualizing/interpret the results. They also show that using L2 regularization for non-neural models and ODIR regularization for neural models improves the performance of existing methods. All the related work is adequately cited and connections are made between existing work and the proposed method. Clarity: the paper is very well written and easy to follow, enough details are given so that the results can be reproduced. I only think that figure 2 deserves a bit more explanation as what exactly the arrows means and what they mean by "fact centres of the probability simplex". Quality: as far as I could check, the paper is technically sound, with a clear theoretical motivation for the method and empirical evaluation that supports the claim. they also point out limitations of the method and show that for neural models the neural-specific calibrators work better with a certain regularization method. Significance: calibration methods are important for many applications and the results presented in this paper paint a clear picture of the current state-of-the-art both for non-neural and for neural models specifically when classwise-calibration is required.