Summary:  The paper showed that two samples testing statistics between kernel based distribution representatives (i.e. kernel mean embeddings and smoothed characteristic functions) using L^2 distance [5] can be generalised to any L^p distance with p>=1. Theorem 2.1 and Theorem 3 of the paper showed that this definition give rise to a metric on the space of Borel probability measures and that it metrise the weak convergence.   The paper showed when using L^1 distance instead of L^2 distance, the power of the tests of [5] are better with higher probabilities [Proposition 3.1 and 3.4]. Like [5], the paper considered the asymptotic null distribution of the normalised difference between distributions representatives, which give close form asymptotic null distributions [Proposition 3.2]. Further, they provided lower bound on the test power of these two l1-based tests [Proposition 3.3]. This results led to the conclusion that it is sufficient to optimise the test statistics jointly in the kernel parameter and the test locations in order to maximise such lower bound.    Empirically, the paper investigated the proposed methods on 4 synthetic and 3 real data problems, illustrating the benefits of using the proposed l1 geometry.   ====== Clarity: Overall, the paper is well written with clear logical structure. Notations are clearly define except at a few small places that it was mentioned before definition (e.g. line 73: D_\mu, J was not defined?) I understand that the authors follow the synthetic experimental set up of [16] and therefore chosen the same parameter settings. It would be nice to inform the readers again of these numbers. Also in line 221, the blobs experiments with dim = 30, is that a typo?   Quality:  The paper presented several theoretical results with proofs on statistics with l1 norm. In the case of two samples testing, they have shown theoretically that l1 geometry gives better statistical power. Such claim is also well supported by experimental results on real and synthetic datasets. Though, I hope to see a little more intuitions on the theoretical results presented. I am wondering if the authors could comment on any weaknesses of their work? Do we lose anything when change the norm from l2 to l1? Or are the authors trying to say that practitioners should now always use l1 over l2 norm? It seems when the samples sizes are large, the run time of the optimised proposed test is longer than the other linear time tests (though still small in this case). Have the authors tried on problems that require even large samples (e.g. 10^7)? For a given computation time, should one opt for the l1-based tests over a l2-based test?   Originality & Significance:  The methods proposed are a clear significant extension/ generalisation of the work from [5]. The theoretical results are important. The l1-norm based two samples tests proposed seems likely to be used widely in various applications competing with/advancing performance of the current state of art.    ===================== Authors feedback read. I am happy with the response provided by the authors.