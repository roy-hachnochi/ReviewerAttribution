This paper proposes a soft, top-down attention mechanism for reinforcement learning. The main framework is similar to [1], with self-attention replaced by top-down attention generated by an LSTM. The network is adapted to RL scenario where the agent is involved in a sequential decision-making process and no explicit "query" exists.  This paper provides a interesting approach to interpretable reinforcement learning. It is clearly written and well-motivated. The authors examined several examples of agent attention and showed interesting behavior of the RL agent.  My only concern is that the contribution seems a bit incremental. The recurrent attention structure is not too far from previously proposed . The introduction of spatial basis is interesting but is not completely new either. That being said, I still think the idea and experiments are quite interesting and there are enough new thoughts to benefit future research.  Another minor point is that in Table 1 in the appendix it can be seen that sometimes the model with attention performs significantly worse than baseline (e.g.  battle zone, chopper command), but to my understanding the agent should always learn to perform at least as well by placing uniform soft attention. Maybe the authors can comment on this?    Ref. [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017.  