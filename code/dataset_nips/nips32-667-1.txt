The proposed scene representation is a map phi from the 3D physical space to a feature space encoding properties such as color, distance from closest scene surface, in practice implemented with an MLP. This choice of parametrization results in a natural way of controlling the level of spatial detail the map can achieve with the chosen network capacity, without using a fixed/discrete spatial resolution (as in voxel grids).  Images are generated from the scene representation phi, conditioned on a given camara (inclusive of intrinsic and extrinsic parameters) via a differentiable ray marching algorithm. Ray marching is performed using a fixed length unroll of an RNN which can operate on phi effectively decoding distance from the closest surface and learning to correctly how to update the marcher step length. This formulation has the nice bi-product of producing depth maps 'for free'. The output image is finally decoded from phi, sampled along camera rays according to the ray marcher predictions, independently per pixel using another MLP.  The parameters of phi are themselves the output of a (hypernetwork) function mapping a low dimensional latent vector z to scene function.  The authors propose to learn a 'per class' hypernetwork, so that z effectively becomes a per object vector. The raymarcher parameters, the hypernetwork, and z all all optimized jointly.  If the hypernet and the raymarcher for a given class have been previously trained, embeddings z  for a new object instance are recovered via gradient descent.  If the z are known a priori (for example the parameters of CG model), by training the rest of the model conditioned on z, the system learns to correctly factor geometry and appearance.   The experimental section of the paper is extensive and the authors compare to multiple models in the space of 3d understanding. Whilst I found the experiments convincing on showcasing the properties of SRN, I think they are not always particularly fair with their competitors. For example, when comparing with dGQN - the work I am the most familiar with - I believe there's one disclaimer missing, that is that dGQN performs amortised inference, hence is very data hungry and massively underperforms in the experiments in a low training data regime. On the other hand, SRN require to optimize z for each object instance that is presented to the system and this optimization will be very expensive, espcially becuase each parameter update will require backpropping through multiple steps of the ray marching LSTM, whose state will be at the same size as the final output image. since that the final rendering convultions are 1x1 to guarantee multiview consistency, as claimed by the authors (perhaps this could be indeed mitigated by not using 1x1 convolution and improving on the design of the final rendering step).  To the best of my understanding, all the claims in the paper are backed up by convincing experiments, but one. At line 59 the paper reads 'formulation generalizes to modeling of uncertainty due to incomplete observations'; I believe this  is an over-reaching statement, as the paper doesn't contain any analysis of the model uncertainty, and only refers to the auto-decoding work in the DeepSDF paper for details. In the reference I could not find a specific analysis of the model uncertainty either, but only experiments supporting the claim that DeepSDF can indeed handle partial observations (which is not equivalent to modelling uncertainty).  There are a few natural questions that the author do not address in the submission, namely: * How does the model scale to multi-object scene? The learnt priors are class specific - can the model be adapted to be composable and scale to complex scenes? * The model will not handle specularities, reflections, transparent materials - could the renderer be adapted?   * The paper only discusses the benefits of pixel independent decoding, what are the limitations? * In GQN they authors show that the scene representation can be semantically interpreted and manipulated (they show something like scene algebra), and enables view independent  control in RL.  Can the proposed scene representation be used for anything else than new view synthesis? I think that grounding the work in a broader context would massively increase the impact of the work.  Although I think this is overall a very clear, I would like to offer some feedback with suggested changes of things that I found misleading/confusing:  * line 11: 'end-to-end from only 2D observations' is misleading, since the techniques requires calibrated views; the abstract should reflect that main body the paper and read as line 42 'posed 2D images of a scene'. * line 71: 'with only 2D supervision in the image domain', same as line 11.  * line 184: for the input tuples I would the same formalism as line 200, making it clear it's a set of tuple. 