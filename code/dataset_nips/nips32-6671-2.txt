The idea of incorporating hashing techniques for speeding up adaptive sampling in SGD is interesting. However, if my understanding is correct, the proposed scheme heavily relies on the requirement that the gradient takes the form as a nonlinear transformation of the inner product between the parameter (or augmented by some constant) and a per observation-dependent vector. Therefore, the only example in the paper is linear regression (although the authors mentioned in a short paragraph that the method can also be applied to logistic regression. This raises the concern of how useful this method is for solving general problems. In addition, some arguments in the paper do not seem correct. For example, in line 124, the method assumes each feature vector x_i to be normalized. This does not seem possible since in linear regression, the design can only be normalized per dimension of the feature, not per data point. In addition, the calculation in Theorem 2 just displays the resulting variance, and it is clear when this variance would be smaller than the variance with uniform sampling. I noticed that in the supplement, lemma 1 provides a sufficient condition for so. However, the condition is still very complicated and not easy to verify. Last but not least, I tried to read some proofs in supplementary material, but failed due to bad proofreading. For example, the proof of Lemma 1 refers to equation (16) and (17), but there are only 11 labelled equations in the supplement and 5 in the main paper.