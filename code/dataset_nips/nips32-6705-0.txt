       The authors propose a new normalization scheme called RMSNorm. They provide an interesting (although somewhat straightforward) theoretical analysis, and empirically show benefits of the approach. Detailed comments follow:       - The paper is quite nicely written, and it is in general easy to follow authors' line of thought.       - Figure 1 doesn't seem to indicate that there is a large problem with LayerNorm, as the difference is quite small. Maybe choose a different example that better illustrates this issue.       - In the related work, the authors mention "internal covariate shift" as a fact, while there is a lot of recent work that questions this explanation. Some the authors do mention (such as [21]), but some are not (e.g., Bjorck et al. "Understanding batch normalization", NeurIPS 2018). Would be good to add a discussion on ICS to reflect these recent works, and not consider the ICS as a fact.       - In (2), should the second part use vector notation as done in (1)? Or should it be vice versa, (1) should use scalar notation instead?       - Please never use "obviosuly" in any of your papers (or "clearly", or any other similar word). What is obvious to you may not be obvious to others. Rephrase this sentence, or you could even add a short equation explain it (although not really necessary).       - In Fig 3, is the x-axis really in "%"? Doesn't seem to be, is the max value considered really just "1%"?       - What is a difference between various versions of OE and OE+LayerNorm in Table 7? Several rows have the same caption. This should be explained and elaborated, it is quite confusing at the moment.       - Typos: "cloze", "shorting"         ** Comments after the author response ** Thank you for a very detailed response! I am staying with my `accept` recommendation.