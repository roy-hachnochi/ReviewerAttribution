Update after rebuttal: My original score was primarily based on the core idea, which seems like a reasonable way to integrate lambda and experience replay, but I also agree with the concerns of the other reviewers.  As for the seemingly ad-hoc nature of some of the design choices, the rebuttal did a reasonable job at explaining the high-level rationale, but it didn't provide solid theory to justify these choices. At the same time, standard TD(lambda) doesn't have particularly solid theory surrounding it either, but is often useful in practice. I also appreciate the addition of the median experiment, which in practice seems to do a bit better than dynamic lambda, but also feels a bit ad hoc.    One concerning thing is that I strongly disagree with one point made in the rebuttal -- I don't think that it can be argued that Fig 8 says anything coherent about the relative computational cost in general, given the extreme variance in performance across problems. On a related note, I also have concerns that the paper does not establish that the variance across problems is due to something fundamental about lambda, rather than an artifact of one of the design choices. Overall, I've dropped my score two points after considering the points that the other reviewers made and taking the rebuttal into account.  --------------  This paper is significantly original, in that it comes up with clever ways to apply old ideas (eligibility traces) to a new setting (deep RL with experience replay) in an efficient way, where others have failed.  The methods are technically sound, and even the slightly hack-y elements (such as the internal cache and oversampling) are well-motivated and there is sufficient discussion of the issues that they can cause (e.g. slight sampling bias from the cache).  The paper is very well written, clear, and significant for reasons stated above.  Now, a few criticisms and/or opportunities for improvement:  - In the Peng's Q(lambda) experiements, the paper points out that at least one value of lambda matched or outperformed the baseline 3-step return.  However, it is a different value of lambda for almost every problem, and many of the values do much worse than the baseline in some problems.  Since there is no clear way to set lambda (the dynamic settings aren't reliable either), this adds another hyperparamter to tune.  However, this has always been a problem of lambda-based methods, and has not stopped the field from using them.  I still think the core contribution is solid and that future work can maybe figure out these issues.  - However, given the weakness of the results, it seems overly optimistic for the paper to claim that "Our experiments showed that these contributions improve the sample efficiency of DQN by a large factor".  This claim should be scaled back and made much more conditional.  - When discussing auto-tunin / dynamically selecting lambda, one piece of literature worth citing might be work that side-stepped the problem of setting lambda by formulating a parameter-free way of doing eligibility traces:  @inproceedings{konidaris2011td_gamma,   title={TD\_gamma: Re-evaluating Complex Backups in Temporal Difference Learning},   author={Konidaris, George and Niekum, Scott and Thomas, Philip S},   booktitle={Advances in Neural Information Processing Systems},   pages={2402--2410},   year={2011} }  