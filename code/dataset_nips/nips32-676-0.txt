Theory-wise, the authors overlooked to discuss several prior works, some of which suggested opposite theories to theirs. For example:  - "Don't Decay the Learning Rate, Increase the Batch Size", ICLR'18, seems to support a constant batch size/lr ratio empirically    --- after rebuttal ---  After reading the comments and the authors rebuttal, I am satisfied with the responses. The paper theoretically verifies that the ratio of batch size to learning rate is positively related to the generalization error. Specifically, it verifies some very recent empirical findings, e.g., Donâ€™t decay the learning rate, increase the batch size, ICLR 2018, which empirically states that increasing the batch size and decaying the learning rate are quantitatively equivalent. I think the theoretical result is novel and timely and would interest many readers in the deep learning community. I value the theoretical contribution and thus would like increase my score and vote for accepting the submission.  Experiment-wise, I have lots of reservations in the thorough/convincing level of the current experiments presented. - Only {ResNet-110, VGG-19} and CIFAR-10/100 are examined, albeit each with a large variety of lr/batch size. One would rather see more variety in model/data (like ImageNet models) - Where are the actual accuracy numbers achieved by those models? After changing the training protocol, are those results same competitive with SOTA numbers reported on the same model/dataset? Practically, a reduced generalization gap does not automatically grant a better testing set result. - All training techniques of SGD, such as momentum, are disabled. Also, both batch size and learning rate are constant without annealing. I wonder how those affect final achievable accuracy.  - I believe the authors confused S_BS and S_LR definitions on lines 198-200.