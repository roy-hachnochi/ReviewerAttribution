 The PAC bound in this paper is very similar to that in London 2017, which also derives an O(1/N^2) generalization bound. (London, B., "A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent", NeurIPS 2017.)  In the experiments, the authors seem to suggest that a larger learning rate is always better, using the pearson correlation coefficient to argue that there is a positive correlation between test accuracy and learning rate.  Obviously, this reasoning breaks down at some point --- at some point using a large learning rate keeps you from learning --- and it is strange that the authors did not experiment with large enough learning rates to see this effect.   Specific comments: - Equations in 5 are only true if we haven't seen training data batch n. If we are cycling through the training data, then these do not hold. - The author's assume that the stochastic gradients are Gaussian distributed. I don't know if this is a good assumption --- I can certainly think of simple contrived examples where the gradient is either +d or -d for some large value d. - The authors should cite two other important papers in this area, who do a much more detailed analysis of this subject: Smith and Le, "A Bayesian Perspective on Generalization and Stochastic Gradient Descent" 2018, and Smith and Le, "Don't Decay the Learning Rate, Increase the Batch Size", 2018. - The experiments show a relationship between the batchsize, learning rate, and test accuracy, but that in itself is not very surprising. Computing Pearson correlations does not seem appropriate because the relationship is not linear, and the p-values are overkill.