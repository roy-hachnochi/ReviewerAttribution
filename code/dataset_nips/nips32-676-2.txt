This paper presents a novel strategy for training deep neural networks with SGD, in order to achieve a good generalization ability. In particular, the authors suggest controlling the ratio of batch size to learning rate not too large. Extensive theoretical and empirical evidences are provided in the paper.  Pros. 1. In general this paper is very well written and clearly organized. This paper brings new insights on training deep models to the community. The strategy has been well justified using both theoretical analysis and empirical evaluations.   2. Theoretical analysis is provided. In particular, a generalization bound for SGD is derived, which has a positive correlation with the ratio of batch size to learning rate. Tightness of the bound is also discussed. 3. A large number of popular deep models are trained and analyzed in the experiments. The Pearson correlation coefficient and the corresponding p values clearly support the proposed strategy.  Cons. 1. Existing generalization bounds for SGD are discussed in Section 5. It will be helpful if the authors can discuss if the existing bounds involve learning rate and batch size or not.  2. A few typos should be corrected in final version. --Page 2: in terms of --Theorem 2: hold --Section 4.1: the elements in S_BS and S_LR  ------------------------------------------------------------------------------------------- The authors have provided detailed and reasonable responses in their rebuttal. I still believe that this paper presents important theoretical results and brings new insights to the community. Thus, I vote for acceptance.