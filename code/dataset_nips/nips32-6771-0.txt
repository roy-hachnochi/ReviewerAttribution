Having read through the other reviews and the author response I will maintain my review of a 6. I really like the core idea of the paper and would be happy if it were accepted based on that alone. I appreciate the author's clarification of the experiments, and I now have a much clearer understanding of what was done. With that said I was a little disappointed that the answer to my question about bootstrapping was basically "a quirk of the learning dynamics". In general, the main reason I have not raised my score is that I found the significance of the experiments to be hard to judge and they don't necessarily clearly illustrate the merit of the approach.  -This paper is fairly clear and well written all the way through  -The main idea of this paper was to learn the impact of current actions on future trajectories as a means to improve credit assignment to actions and produce alternative advantage estimates which are suggested to be lower variance in many common situations. I found this idea to be very interesting and reasonable, and as far as I know fairly novel.  -experiments are fairly clear and seem to do a reasonable job of testing certain properties of the proposed advantage estimators  -Why show standard deviation in the error bars? wouldn't the standard error be more informative?  -I found Figure 3 (right) a little confusing. If I'm reading it right it's showing the advantage estimate of taking the shortcut computed by different methods after training for 1000 episodes under a number of fixed policies. Is this correct? If so I think it could be written more clearly. Also in the text it says "Note that return conditioning is only able to do so when the preferred action is likely" but If I'm reading the graph correctly it shows that this is the case when the preferred action (shortcut) is *unlikely* instead, is this right? If so this should be fixed since that adds to the confusion.  -In figure 4 (left) does HCA-state use bootstrapping or pure monte-carlo? If it uses Monte-carlo then it seems strange to compare with policy gradient using bootstrapping. This isn't showing anything about your proposed estimator but simply a limitation of bootstrapping under partial observability, which I presume the proposed method would suffer from too if it used bootstrapping. It is also unclear from the text whether HCA is using bootstrapping here or not. If it is I don't understand why it is able to perform better than policy gradient, could you offer an explanation if this is the case?  -the baseline is just called "policy gradient", but since bootstrapping is discussed I assume this is an actor-critic algorithm? This could be clarified.  -lack of explanation of how hyperparameters (such as learning rate) were selected in the experiments make the results difficult to interpret