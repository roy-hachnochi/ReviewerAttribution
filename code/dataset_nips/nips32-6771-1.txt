The authors introduce a new reinforcement learning algorithm based on directly attempting to estimate attribution of credit. The authors achieve this by modeling the likelihood of a particular action a being taken at a state x given an outcome state y later in a trajectory. If the ratio of likelihoods between the P(a|y) and P(a|x). When the likelihood of a past action given a future state is much greater than the probability of that action being taken, then the state is highly predictive of a past action, and thus that past action was probably highly causal to the current state y. Using this ratio the authors are able to augment the typical SARSA setup to include this notion of credit assignment, and use it to improve training.  The authors demonstrate the value of this new modeling in several experiments where they vary how much future states depend on past action, what is the delay, and noise confounding the setup. They are able to confirm the utility of the new modeling.  The setup, motivation, and experimental details are very clear. The idea and formulation are original and highly central to many problems in reinforcement learning.  The main drawback of the setup is a lack of a proper experiment or setup for how to achieve this result in a typical benchmark RL task (Atari, Mujoco, Gym-retro, etc..) where the benefits of this approach should also be visible. It is also difficult to know how to properly define and parametrize a function that predicts P(a|y), if the future state y contains some recurrent state from x which might memorise that a was taken (thus nullifying the value of this modeling). It is also important to know how to efficiently compute these functions -- in actor critic setups, it is sufficient to record the log probabilities of the taken actions and value function predictions and use those to perform updates (e.g. with clipping as done in PPO), however in this setup, given that it is necessary to compare the probability that at a past state a specific action was taken, what is the right way to model 'referring' back to a past state given future state information?  