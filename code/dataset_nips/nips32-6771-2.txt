 Originality:   As far as I am aware this is a novel approach to estimating the value functions that has some potentially big advantages, as outlined in the paper (however, I am not familiar with niche literature in this vein). It takes a backward view of the learning problem and draws inspiration from importance sampling approaches to Monte Carlo estimation. This opens doors to new directions, which could prove useful. It seems like a hindsight distribution is something that humans naturally have access to, which is nice.   I should note that the direction taken is not incremental, but more of a blue ocean idea---a true alternative. This is what makes this a “high risk, high reward” paper from my perspective.    The related work section is adequate, although connections/parallels to other works could be clearer. E.g., it is unclear to me what the connection to hindsight experience replay is (besides the use of the word “hindsight”).    Quality:  I went through the main proofs/derivations (with the exception of the derivation of the beta hindsight distribution) and things appear technically sound. There are no extravagant/unsupported claims made, although the discussion in Subsection 4.3 failed to persuade me that learning the hindsight distribution would be easy in non-toy tasks (esp. as it is policy dependent).   I would say the present work is a complete package, although it would benefit greatly from at least one non-toy experiment. The authors frame this as a novel idea/framework and do not make empirical claims, so I am OK with the lack of non-toy results.    Clarity:   This paper is well organized and a pleasure to read. My only comments/suggestions/questions are as follows:  Line 101: Why is h_k a higher entropy distribution in general?  Line 192: extra space Section 5: Although these are toy experiments, I think not enough information is given to reproduce (missing, e.g., learning rate used, and hyperparameters tested) Why is Figure 4(center) cut off at 200 episodes? Does MC PG overtake both HCA curves?    Significance:   I discussed significance in the Contributions+Originality sections. To summarize: IMO this is an interesting, novel direction that is worth pursuing. As the idea was not validated in any non-toy example, I would rate the immediate significance as low, but believe this could have very high significance if it successfully scales / handles non-toy tasks. But even if it fails to scale, I would like this to be a part of the literature, as it provides a new perspective/form for the value function and may inspire related ideas.    *** Additional comments after author response ***  - I found the author response helpful on minor points, although it does not address the lack of a proper experiment or persuade me that the hindsight distributions can be learned. I think any proper task, even a simple Gridworld, would make this a much better paper. For this reason, I am maintaining my original score.