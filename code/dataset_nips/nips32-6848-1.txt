This article studies concentration (or large deviation) inequalities for quadratic forms involving vectors with a martingale difference dependence structure.  The article is overall well written, the results most likely exact. Yet, I have several important concerns:     * on relevance: the arguments offerred by the authors on practical use of their results are vague and the concrete examples (the corollaries) are clearly unconvincing, if not artificial. The authors really need to dig this question as there seems to be no obvious case of evident interest in the standard literature.     * technically speaking, there appears to be little fundamentally new in the derivations of the main results. Worse, in my opinion, the authors concentrate their efforts in quite obvious preliminaries (around page 3 and 4) only to scheme quickly over the actual key innovative arguments (deferred to appendices). There also appears to be essentially no difference between the iid and MDS setting, so what's the conclusion?     * in terms of adequation to NIPS, I find the article more of a (not substantial) mathematical exercise providing an epsilon-improvement over existing results with no prior thoughts on the entailing machine learning consequences. The keywrod "adaptive learning" raised by the authors in introduction and conclusion is possibly a good anchor, but this is as far as the reflexion goes unfortunately.      For these reasons, I believe the work cannot be math-wise nor ML-wise considered at the demanding level of NIPS.  I: - "The RIP is a well known ... result of this type, which ... major impact" and then "The JL is ... well known result of this type, which has major..." --> please be more inventive in phrasing. - the word "such" is (unbearably) used in almost every sentence. Please avoid these repetitions. - "to get the large deviation bound" --> which large deviation bound? This has not been introduced yet. - "existing analysis for specific cases goes to great lengths to work with or around such dependence" --> I do not understand this sentence, please restate. - "For decoupling, we present a new result on decoupling" --> restate  II: - reference missing after [19],[31] - the discussion on converting zeta into a random matrix X, etc. is redundant with the previous section. - MDS acronym already introduced - the sentence preceding (4)-(5) ("we decompose the quadratic form") could implicitly suggest that C_A=B_A+D_A which obviously is not the case. A more appropriate statement would avoid this possible confusion. Also, a quadratic form per se has no "diagonal" or "off-diagonal" term; so this makes little sense anyways. - "the off-diagonal terms of E|Ax|² is 0" makes little sense (this is a scalar, not a matrix) - the result (6) is somewhat straightforward and could have been schemed over more quickly. On the opposite (7) is more interesting and would have deserved a better, more extensive treatment (rather than delegating it to Appendix). In particular, it would be appropriate to explain here why each has a different order of magnitude in 'p'. - I suppose 'a', 'b' and 'c' are independent of 'p'? - why |C_A|? C_A is positive by definition. - "A naive approach..." --> which approach? - the way the section is organized is difficult to follow. If I understand correctly: first we decompose C_A into B_A and D_A, to show that the final result is (9)... proved in Appendix B and C. Then, why get into gamma-functions here? Are we detailing a specific argument to reach (9)? Please help the reader through in the way the results are introduced, and go sequentially from arguments to conclusions, not the other way around. - "an bounded" - "such an approach needs to first generalize several keys results..." --> why not do it? - "We have also... which is also... It is also" --> please check these repetitions  III: - I do not understand the practical relevance of Corollary 1. In which context would the matrix X be organized with tis "vec-wise" MDS property? The provided example is not convincing. Which concrete scenario do you have in mind? Please defend your case here. - "with iid entry" --> entries - the introduction of the Toeplitz matrices is a bit naiv. Toeplitz matrices are not a "desirable alternative" to matrix with iid entries. There's little in common between these objects... - again, please justify the practical relevance of Corollary 2. In which context zeta could form an MDS? To me, Toeplitz matrices are mostly used to model the coefficients of stationary processes which, by definition, are immuable with time and do not depend on each other, certainly not at least with an MDS structure. So what again do you have in mind here? - am I wrong to say that (28) is simply saying that |Xu|²=|u|² for all u in A? - "for the iid case" --> full stop missing - I do not see why vec(X) satisfies an MDS in CountSketch. The constraint on the eta_ij's is not "sequential". As you state, each row entry depends on all other entries (for indices below and above), so it's not an MDS to me. Could you clarify this point?  IV: - "over the past few decades" --> I wouldn't go that far. - "We anticipate our results to simplify and help make advances in analyzing learning settings based on adaptive data collection" --> ok, but how precisely? This argument from the introduction is replicated here with no further argument. I do not see in which context precisely and the corollaries provided in the core of the article are not convincing. Please elaborate. - "sharpen our analysis" --> what do you mean? In which direction? Do you intend to get rid of the log(n) terms? If so, I do not see this as a worthwhile ambition. - all in all, I was expecting to see in this conclusion a real opening to the practical relevance of the article findings, but I'm still waiting for it.   