In this paper, the authors prove a concentration result for quadratic forms, which removes the usual iid assumption from the random variables, and replace it with an "MDS" assumption (each new variable is sub-gaussian and has zero mean when conditioned to the previous ones). Their proof uses generic chaining. Then, they apply this result to proving a new version of the Restricted Isometry Property, and the Johnson-Lindestrauss lemma. They also provide some special instantiations in each case.  I liked this paper. The writing is pretty clear, if sometimes a bit verbose (some things are repeated quite a few times, eg line 116-118), and the exploration of the topic is thorough. Despite the many works around concentration of measure and chaining, as far as I can tell the proposed bound is new, however it is possible that some work that I am not aware of may be similar in the literature (which is, again, very large). The new bounds for the RIP and JL may be re-used by practitioners in data compression and machine learning, since they are at the core of many analysis. The authors especially emphasize the use in a sequential context. A possible critic would be that it is not very clear for the reader how "natural" or "applicable" the MDS assumption is, and it can sometimes give the impression that the authors slightly oversell how much of a "substantial generalization" departing from a "key limitation" it is in practice (even if it might be mathematically speaking). The authors briefly mention bandits or active learning, would it be possible to describe a bit more these contexts, in a few equations, and emphasize how an MDS system would appear ? This may be more useful and convincing for the reader (especially from these communities) than some examples (eg RIP Toeplitz), if space is needed.  Minor comments / typos: - line 3 : the the - line 99 : missing ref - maybe (6) should be encapsulated in a small Lemma, and what precedes (simple computations) in the proof ? - both historically and mathematically, it can be argued that the JL lemma precedes the RIP (in fact, the RIP can be seen as a "uniform" version of JL for sparse vectors, and the classical proof of the RIP is based on JL and a covering argument). So it would make sense to invert the two sections on RIP and JL - the results for the RIP are corollaries, and the one for JL is a lemma, is there a reason for this ? It can be a bit confusing. - Even though the countsketch is based on JL, it is always mentioned as a separate example, it can also be a bit confusing  **EDIT after feedback** I thank the authors for their feedback, which I find convincing on the points that I raised. I stand by my score, however I encourage the authors to take into account the critics raised in the other reviews, some of which I find legitimate.