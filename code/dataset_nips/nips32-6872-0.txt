The paper borrows tools from combinatorial optimization (i.e. for the facility location problem) in order to select hindsight goals that simultaneously has high diversity and also being close to the desired goals. As mentioned, the similarity metric used for the proximity term seems to require domain knowledge that euclidean distance works well for this task. This may be problematic if we have obstacles that mislead the euclidean distance, or in another environment where it is less obvious what the similarity metric can be. I am aware that this dense similarity metric is only used for hindsight goals, and that the underlying Q function/policy is still trained on the sparse reward (without the bias).   There are several related works that can be discussed and potentially benchmarked against in terms of hindsight goal sampling schemes: Sampling from ground truth distribution half the time for relabeling, and using future the other time (in Appendix). A. Nair, et. al. Visual Reinforcement Learning with Imagined Goals. NIPS 2018.  A heuristic goal sampling scheme: D. Warde-Farley, et. al. Unsupervised Control Through Non-Parametric Discriminative Rewards. ICLR 2019  The paper supported its claims on the Goal and Curiosity Driven Curriculum (GCC) learning with qualitative plots of the selected hindsight goals over the course of training. The plots seems to indicate that indeed the earlier episode hindsight goals have higher diversity while latter episodes are closer to the desired goals. The ablation studies on the lambda_0 value indicates that having both the diversity and proximity terms can affect the performance. To prove that lambda curriculum is necessary, I think that it will also be helpful to compare different *fixed* value of lambda (i.e. no curriculum) vs with the lambda curriculum.   The paper is fairly clearly written, with understandable high level ideas. There are some clarification details/suggestions: What is the similarity metric used, Equation 1 or 2 (or neither) for the experiments? What is the \eta and \lambda_0 value? This will also tell us how large \lambda gets by the time by the end of training. How sensitive is the performance to this parameter? Figure 4 gives a nice qualitative view of the selected achieved goals in relation to the desired goals and achieved goals. Having a quantitative value can also be valuable, i.e. plotting the value of F_prox(A), F_div(A), over the course of training.  The large performance in the hand manipulate pen rotate task deserves some attention, as previous approaches so far have not been able to make much improvement. While the method seems more of a heuristic, I think that the approach proposed will benefit the goal-conditioned RL community.   *** Post Author Rebuttal Comments ***  Thank you to the authors for their response. I am fairly satisfied with the author response: - Given the additional ablations in their rebuttal that I have specifically asked for (i.e. with the fixed \lambda, sensitivity on the \eta parameters, F_div/F_prox curves), they have proved the importance of having the curriculum to balance the proximity/diversity (exploit vs explore).  - On using Euclidean distance proximity, the authors also reasonably addressed this in their rebuttal, especially emphasizing that the main contributing point is on balancing between the proximity/diversity, *given* a metric. I will still encourage them to have a discussion about having the right metric for the domain in their final draft. There is unfortunately probably not enough space in the publication format to do a thorough investigation with other metrics / domain spaces other than L2, so I am ok to leave that to future work.  - On related works: as they pointed out the works I mentioned are not directly comparable (i.e. using image input instead of object position state representations), but those works also touch on balancing some sense of diversity in the hindsight goals (i.e. via sampling from prior) versus the using seen states. So I did not expect them to try to directly compare by applying CHER to domains with image inputs.   Overall I increased my score from my original review.  *************