This is a clear well written paper on using meta learning to learn better initial parameters for training deep neural networks. It uses automatic differentiation to optimize a ratio of magnitude of gradient change in order to learn scale value for initial parameters for different layers.  While it is a simple algorithm, figure 2 is very interesting. However, why not also show results for non-random data? The paper also mention it operates in 'data-agnoistic' fashion, what advantages does it bring to be data agnostic? If the algorithm is truly data agnostic, are there results on how scale learned for a network can transfer to multiple datasets?  The paper makes the statement regarding 'less curvy' starting regions. However the loss in equation 2 is only looking at the magnitude of the gradient and not necessarily the curvature, due to the absolute value taken.  - What happens if metainit is combined with batchnorm?  - Can you show a training error plot as function of update iterations? that would be helpful to compare with metainit vs baseline optimization. 