[Originality] While modifying and extending PathNet is exciting, most concepts of PathNet are reused in the submission. The submission is rather incremental in the sense that the framework is practically a combination of useful existing techniques: parameter sharing/reuse, knowledge distillation and retrospection and model plasticity.  The novel part of the submission is the implementation details of the framework and the corresponding training methodology that make the framework powerful.  [Quality]  The submission is technically sound, and the framework is comprehensive and flexible, incorporating many existing techniques. The experimental results clearly show that the proposed method outperforms previous works. But some detailed analysis is missing. For instance, how does the varying number of exemplars affect the performance?  Also, the submission claimed the framework effectively reuses modules, but no comparison of model sizes with previous works is provided in Table 1. Last, how important is the augmentation mentioned in L223? As many other methods do not utilize augmentation strategy, the effect of the augmentation should be enunciated. [Clarity]  The paper is not easy to follow details, especially in Section 3. Reproducing path selection algorithm is not straightforward, and figuring out the detailed algorithm only with the descriptions in the submission is difficult.  [Significance]  The empirical results are strong, and the framework is clearly one of the top performing methods in continual learning. The novelty, however, is weak, and the framework is the composition of existing components. The paper is not clearly written, and more analysis is necessary.   [After Rebuttal] The authors have addressed many of my concerns, and I have raised the overall score after the rebuttal.