This paper studies the deviation of the (discrete) trajectory of optimization algorithms from the (continuous) trajectory of their continuous-time ODE counterparts. The optimization algorithms considered are gradient descent and heavy ball method (also known as momentum method). The paper shows that, under some conditions of the objective function (strong convexity, smoothness and Lipschitz condition), trajectory of the gradient descent method with small enough step-size does not deviate too much from the trajectory of its ODE counterpart. The paper also extends the discussion to cases like strong concavity, saddle points and stochastic GD, but obtain conclusions that is somehow weaker than the one mentioned above.  To some degree, these results verify the conjecture that analysis of the corresponding ODE would be helpful for analyzing the optimization algorithms, especially for the gradient descent on strongly convex objective functions. However, the result in Theorem 3, which is the main result, is not surprising to me at all, since the step-size is required to be very tiny and objective function is strongly convex (only one global minimum exists, which makes the trajectory stable). Note that in Theorem 3,  it is almost always true that 2\mu\epsilon/(L\ell) << 1/L. Then the step-size h would be very tiny, compared to the common practical choice 1/L.   The result in nonconvex case (Theorem 4) is also limited. It only holds for quadratic functions. For general non-convex function, this means Theorem 4 only holds for a small region/neighborhood, which can be well approximated by quadratic function. Outside of this region, nothing is known.  More comments/concerns:  >Theorem 3 requires that the objective function is strongly convex as well as Lipschitz. These two conditions are contradictory, since the objective function has to grow no slower than quadratic as required by strong convexity. Obviously quadratic function is not Lipschitz. Although this contradiction can be resolve by limiting the radius of the feasible parameter space, the Lipschitzness would depend on this radius. In this case, according to Theorem 3, larger feasible parameter space results in even smaller step size h. 