Summary: The authors investigate the degree to which regularization in linear models can actually lead to better causal models. They show that in the case of linear confounding there is an elegant connection between ridge regression and estimating the true causal effect vector for the observed covariates. By making a few approximations, they come up with a practical algorithm, similar in spirit to the adaptive lasso (i.e. fit OLS, then choose the right lambda penalty and refit) but for causal inference with ridge (though they also look into lasso as well, because why not?). The algorithm is shown to work well in simulations and has interesting results on real data. They conclude with an alternative look at the problem, through a learning theory lens.  Review: This is an excellent paper. It was a joy to read. The questions raised and points made were thought-provoking and informative. I appreciated that the authors were clear on their contributions and that they were not overstating them. Rather, the paper does exactly what it aims to do: stimulate a conversation. I'll continue that conversation here.  - Can anything be said in the high-dimensional (n << p) case?  - Does the adjusted ridge penalty always end up being larger than the cross-validation chosen one? Or are there cases where CV actually chooses to over-regularize? The authors repeatedly claim that a high-level point here is that causal inference may require "stronger" regularization, which makes me think the former is the case.  - Is there a connection to information criterion methods in the lasso case? Let's assume our model is not just linear, but linear and sparse. In this case, one would expect the lasso (or elastic net) would work better for the usual reasons. In the case of the lasso, it explicitly restricts the degrees of freedom of the model by inducing sparsity. A common approach in choosing lambda for the lasso is to use information criteria like BIC or AIC that tradeoff the fit with the degrees of freedom.  - Should we expect nonlinear models to also require stronger regularization? I understand that theoretically these models are much harder to work with. I'm simply looking for some intuition here. Is the suspicion that the ridge result due to the linearity of the confounding and covariates, or is there something more fundamental going on here?  Overall, I do not have any substantially negative things to say about this paper. There were several typos that I think the authors should hunt down in a re-read. And it would of course always be nice to have more experiments. However, I think as-is this paper is a nice contribution and deserves to be accepted.