The authors present a variational EM algorithm for inference in a multivariate Hawkes process. This allows learning in the setting with many hyper-parameters and limited data, where ML approaches will fail or at least hyper-parameter estimation would be difficult. ML approaches therefore tend to use less hyper-parameters which may limit their performance.   This class of model has seen a lot of interest recently and I’m not aware of a previous approach to variational inference in the continuous-time setting. A previous stochastic variational inference approach from Linderman and Adams (cited here) used a discrete-time approximation and other Bayesian approaches that I’m aware of are based on MCMC. This therefore looks like a timely and valuable contribution.  Experiments are run to look at performance in terms of link and event prediction and for both parametric and non-parametric excitation function models. The VI approach is shown to outperform ML approaches on limited data and looks better even for pretty large datasets according to the synthetic data evaluation. VI also seems to deal better with an over-complex non-parametric function compared to ML. Results on three real datasets also look very good compared to MLE approaches.  