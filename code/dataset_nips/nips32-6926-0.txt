The authors present a regret analysis for learning state representation. They propose an algorithm called UCB-MS with O(\sqrt{T}) regret, which improves over the currently best result.  The paper is well-organized and easy to follow. The authors also explain the possible methods and directions to further improve the bound.  The paper could be more clear if lemma 3 was proved in appendix. It may be obvious for someone who has read the similar proof. However, since it is crucial for the proof of the main theorem and is not explicitly stated in [9], explaining the steps can be helpful for a slightly more general audience.   I wonder if there is really novel technique or idea in the paper. UCRL2, which is based on optimism in the face of uncertainty principle, chooses the optimistic model in plausible MDPs set in each episode. It is natural to choose optimistic representation model based on the same principle, which results in a regret bound relative to S_{\sum} instead of S in UCRL2 setting.   Since there is not empirical result in the paper, I am not sure the significance of the result. For example, when solving RL problems such as Atari games, we may test different representation methods. If one of them approximately satisfy Markov property and achieve good performance, then we use this representation and the problem is solved. Are there some cases where we need to use algorithms to learn the best representation methods in a finite representation set? Besides, the regret can scale linearly in \sqrt{|\Phi|} since S_{\sum] scales linearly in \sqrt{|\Phi|} in worst case, which means the algorithm cannot perform well for large or even infinite representation set.  Another tiny comment is that there are many algorithms such as UCBVI which enjoys square root dependence on S. Is it possible to use the framework and techniques of UCBVI to improve the  dependence of the number of states? I believe the improvement of the dependence on S is important since S is always assumed to be extremely large.  Overall, I believe the paper is correct and interesting, but I am not sure about the above issues. Maybe I will increase the score after rebuttal.   ---------after rebuttal--------- I thank the authors for their response. The authors addressed my concerns in the feedback. Though I still believe that the algorithm is natural and simple since the framework is similar to UCRL2, the regret bound does improve over the currently known best result. Considering these factors, I updated my score to weak accept.