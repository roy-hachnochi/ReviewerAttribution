This paper provides theorems and experiments that the well-known curve distance, the (continuous) Frechet distance, is preserved under JL random projections.  The main theoretical result is not just a (1+eps)-relative error, it also requires a roughly gamma * sqrt{eps} additive term where gamma is the maximum distance between two points.  This is not too surprising to have the additive term, but also not entirely clear it is needed.    The experiments use a state of the art code (from SOCG 2019) to compute the distance, and the paper mentions a GPU / parallel extension.  It shows results where curves are of length n and the dimension d = Theta(n), that the random projection provides significant speed up with reasonable loss of accuracy.     These results are nice, and the mathematical / theoretical part is very well written.   I have few concerns:   * the paper does not specify what dimension the curves are projected down to.  It shows these results empirically in terms of eps.  But the JL results these are based on are assyptotic results require O((1/eps)^2 * log (n/delta)) dimensions.  The paper does not say what constant is assumed, what delta is, and what n is.  I can only guess they are all 1, but am not sure.     * It would be useful to compare the result to just using PCA, and also understanding when PCA works just as well as JL for these problems, empirically and theoretically.  Often PCA works much better in practice, but can be slower.  Does that hold in this setting?     * I am not completely sold on the application of the Frechet distance in high dimensions.  Is this a good modeling choice, and how is that justified?  In particular, why focus on the bottleneck-distance based Frechet, and not DTW or something else that uses a more "average" case approach to curve distances.  And do the sequence information really matter as opposed to the distribution of the n data points (and thus a cosine or KL sort of distance)?     Overall, the paper is fairly nicely done, and the results will generate some interest, so I am ok accepting.  But there are some (small) concerns with the presentation of the experiments, and with the empirical motivation  (I'll add, mathematically, the question is clearly interesting).      Typo in Theorem 11.  I think "(2+delta)-app..." should "(2+eps)-app..."    ADDENDUM:   I have read the response, and it has definitely addressed the main concerns above.  I'd appreciate even more discussion on the application, since I think these insights into why specifically the Frechet distance is useful is pretty interesting.  Also, the poor performance in this case of PCA (in initial experiments) is surprising, and deserves further investigation, IMO.  The other comments, if leading to changes in the paper, I feel will make it stronger.  I am elevating my score.  