The authors target a very particular problem, that of predicting the uncertainty in predicting the type of event which is going to happen asynchronously in the future and where the probability of the event is dependent on the time. This is an important problem and is different from other settings in uncertainty prediction which have been explored elsewhere.  The paper is very well written (besides some minor reorganization issues) and the illustrations are of high quality. The techniques described are sound and novel, and bringing them together is an important contribution. The experiments are described in adequate detail and the provided code is relatively easy to parse through and re-run to reproduce a subset of the results.  However, there are two points which could improve the quality and clarity of the submission. The first is that the related work [6, 13] is mildly mischaracterized, and the cost of introducing sampling are not fully elucidated. Most notably, Neural Hawkes Process can successfully model multi-modal distributions of events, akin to FD-Dir-PP do it, and, hence, its characterization (e.g. in line 230) could be made better. Similarly, while it is true that RMTPP models type and time of the next event independently of each other (which precludes multi-modal event distributions) it does so in order to allow rapid training and efficient use of GPUs. This is a subtle difference, which leads to the second point of the true cost of the sampling step. Tensorflow, and GPU based training in general, works best when the entire training iteration happens on the GPU (i.e. no use of feed_dict with computed values). However, the sampling step, to the best of my knowledge, cannot be done on the GPU, and needs to happen on the CPU. The Neural Hawkes Process [13] too suffers from this, because they need to perform Monte-Carlo sampling in order to numerically evaluate integrals. This otherwise minor detail introduces a significant bottleneck in the training times. I believe an honest discussion of the pros and cons of the approach adopted would further embellish the paper and help put the contributions in context. Along the same line, it is unclear what the tradeoff of increasing/decreasing the number of samples 'M' is on the training time/quality and that pareto-optimal front would also be interesting to speculate/demonstrate.   Given the overall contributions of the paper, I do not have any hesitation in recommending it for publication.  Minor points:   - Line 131: "a a"  - Line 94: y_j^{(c)} is not defined without looking at the appendix.  - Line 137: Claim about the model being fully differentiable is not justified until the loss function is provided.  - Line 386: First denominator, the sum should go till C.  - Line 444: Missing Figure reference.  - Line 459: Comment in blue.  ----------------------  Update after the author response: I have read the response and it has cleared up some misconceptions I had. A more nuanced treatment of the related work will also be appreciated.