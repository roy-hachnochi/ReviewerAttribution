I have read the response. ---------------------------------------- This paper makes strong contributions to the recent line of work that demonstrates the equivalence between over-parameterized neural networks and a new class of kernels, neural tangent kernels (NTK). The new class of kernels is different from classical kernels like Gaussian kernel and we do not have a good understanding on why it gives better performance than classical kernels. This paper initiates a rigorous study on the properties of NTK.  This paper provides a smoothness analysis of NTK induced by two-layer neural network. The result may be useful for understanding the robustness of neural network as well. The paper further provides approximation analysis of the NTK. I really like the result as it demonstrates the advantage of NTK comparing with previous kernels. The analyses on the smoothness and stability of CNTK are also interesting and authors did a good job in establishing the connection with previous work on CNNs in this direction.     Overall I really like this paper and recommend to accept.   Minor: 1. If I understand correctly, the proof for deriving CNTK relies on the sequential limit. This should be stated explicitly in the final version.  