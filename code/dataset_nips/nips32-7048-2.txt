This submission proposes to use a GAN to generate adversarial examples. The key difference from prior work is that the discriminator uses a relativistic loss (and perturbations are projected to satisfy l_infinity norm <= 10, rather than being unbounded). This relativistic loss enables the generator to create successful cross-domain adversarial examples; in other words, it can generate adversarial examples for an image distribution (e.g., ImageNet) that it was not trained on.  The use of a relativistic loss in training GANs is not novel, as the authors acknowledge, but this is the first time it has been applied to generating adversarial examples, and the result is quite impressive. It leads to state-of-the-art adversarial attacks on both naturally-trained and adversarially-trained ImageNet models, in *both* white-box and black-box settings.  Originality and Significance: This submission is low in originality, but high in significance. Although the contribution has relatively low technical novelty, the fact that it leads to state-of-the-art adversarial attacks is highly significant. The community should be aware of this, so that it can take these attacks into consideration when generating defenses.  In terms of related work, the submission did a good job of acknowledging existing work on adversarial attacks that are image-agnostic or produced by generative models. One recent work (in the latter category) that should be included is [1]. When comparing with prior methods (those in Table 1), why not also compare to other methods that use GANs, for instance that work or [19]? I would also appreciate a more detailed comparison of how the proposed GAN framework differs from those two works.  Clarity: I found the submission to be clearly written and well-organized.  Quality: I'm curious about the use of "instance-agnostic" to describe adversarial examples produced by GANs. Is this common terminology? There is still a separate adversarial perturbation computed per image, rather than a single perturbation that is added to all images (as in UAP). It's true that only a single forward pass is necessary, instead of a backward pass as well, or multiple forward and backward passes. But it doesn't seem truly instance-agnostic to me.  [1] Song et al. Constructing Unrestricted Adversarial Examples with Generative Models. NeurIPS 2018. https://arxiv.org/abs/1805.07894