*** UPDATE *** Thank you for your helpful response. It is clear that there are shortcomings in the presentation to be addressed, but on the basis that these will be addressed in a revision of the manuscript I am prepared to increase my score from 5 to 6.  This paper considers kernel quadrature (KQ) methods for numerical integration. The novelty comes from taking a sample from a determinental point process (DPP) as the point set. The paper focuses on a theoretical analysis, and includes a cursory empirical check that the method performs as expected. An interesting finding is that DPP-KQ empirically out-performs Bayesian Monte Carlo (BMC).  The theoretical contribution looks carefully written and technically accomplished. However, I am sorry to have to say that I do not think the main result is useful. I will try to explain why:   Let's take the Sobolev space of order s on [0,1], as indeed the authors also consider in the manuscript. In that context, it has been shown that the minimal worst-case error (over the unit ball of the Sobolev space) for a quadrature method based on a deterministic point set is O(N^(-s)). For a randomly selected point set, the minimal mean square error (MSE) (over the unit ball) is O(N^(-2s-1)). I believe such results can be traced back to Bakvalov and Suld'in in the 1950s Soviet literature, but more modern accounts can be found in the information-based complexity literature of Novak and Wozniakowski in the 1980s.   The authors do not appear to be aware of these results and, more importantly, they do not appear aware that the optimal O(N^(-s)) rate for a deterministic point set can easily be established for Bayesian Quadrature (BQ) by taking a uniform grid as the point set and applying the fill-distance-type bounds from e.g. the book "Scattered Data Approximation" of Holger Wendland. See e.g. [BOGOS2019,KOS2018] and the references therein for details. Such "uniform grids are optimal" results cover a range of settings and kernels, which rather raises the question of why one would want to use a random point set. Indeed, in the BQ work of O'Hagan the point set was selected to minimise the worst-case error (c.f. posterior standard deviation of the integral). Of course, BMC uses a random point set, and in some cases - such as integration on manifolds - a random set can be justified since it may not be possible to easily construct a grid. But in simple situations like integration on [0,1], I do not see why randomness would be helpful.  The authors justify their analysis on several occasions by the claim that BMC does not have theoretical guarantees. In fact, a rate O(N^(-2s)) for the MSE was established for BMC in the Euclidean context in [BOGOS2019] and in the manifold context in [EGO2019]. This is actually a better rate than what the authors have demonstrated for DPP-KQ, and this rather undermines the extent of the contribution.  All this being said, the authors demonstrate that DPP-KQ out-performs BMC. This is a nice finding, but it is not demonstrated extensively enough to justify the paper in its own terms. For example, the experiments were limited to dimension d = 1 and also g = 1.  Minor:  l23: Higher-order QMC methods also exist, which achieve optimal Sobolev rates - see the work of Josef Dick and colleagues.  l24: Reference did not compile.  l74. O'Hagan was not the inventor of BQ, that can be traced back at least to [L1972].  l76. The greedy selection of points in BQ can be theoretically analyses as a special case of the work of Ronald DeVore on greedy approximation in Hilbert spaces, so it is not true to say that there are no theoretical guarantees.   l79. The authors state that "implementing it [BQ] usually requires some heuristics". This rather overlooks the substantial contribution of [JH2018], [KS2018], who reduced the computational cost of BQ to near linear in N.  l103. \mathbb{N}^* undefined.  l110. "speaking, seeing" -> "speaking, the probability of seeing"  l144. It is not clear in the main text that k is being defined as thr limit of (10).   l144. The property being assumed is sometimes called "unisolvency of the point set".  l147. There is a Corollary 1 in the main text and the appendix, but they are different.  l155. The use of "quadrature error" is not appropriate - this is not a quadrature error, since there is no integrand. More precisely, it is the worst case error over the unit ball of the RKHS.  l178. The decreasing nature of the \sigma_n needs to be explicitly assumed.  l195. The bound in (21) seems quite loose, especially if the eigenvalues are geometrically or exponentially decreasing?  l224. The authors should take care that when they write "BQ" they really mean "BMC". Otherwise, they need to explicitly explain in the main text how the points for BQ are being selected.  [BOGOS2019] Briol F-X, Oates, CJ, Girolami, M, Osborne, MA, Sejdinovic, D. Probabilistic Integration: A Role in Statistical Computation? (with discussion and rejoinder) Statistical Science, 34(1):1-22. (Rejoinder on p38-42.)   [EGO2019] Ehler M, Gräf M, Oates CJ. Optimal Monte Carlo Integration on Closed Manifolds. Statistics and Computing, to appear, 2019.  [JH2018] R. Jagadeeswaran and F. J. Hickernell. Fast automatic Bayesian cubature using lattice sampling, 2018.  [KS2018] Karvonen, T. and Sarkka, S., 2018. Fully symmetric kernel quadrature. SIAM Journal on Scientific Computing, 40(2), pp.A697-A720.  [KOS2018] Karvonen T, Oates CJ, Särkkä S. A Bayes-Sard Cubature Method. Advances in Neural Information Processing Systems (NeurIPS 2018).   [L1972] Larkin, F. M. (1972). Gaussian measure in Hilbert space and applications in numerical analysis. Rocky Mountain J. Math., 2:(3) 379–421.