Paper summary: motivated by the need to improve convergence rates for quadrature rule for functions living in an RKHS, the paper proposes to sample quadrature nodes from a determinantal point process (DPP), and the weights are found by solving a least-squares problem. The paper analyzes the expected squared error of the proposed quadrature rule, bounding the convergence rate in terms of the spectrum of the kernel. The paper then empirically validates the proposed method with numerical simulation for functions in RKHSs associated with the Sobolev and the Gaussian kernel, showing faster convergence than kernel herding and leverage score sampling.  Strengths: 1. The idea of sampling from a DPPs, whose kernel is associated with the kernel of the RKHS, is interesting and novel to the best of my knowledge. 2. The proposed quadrature rule gets explicit convergence rate, for example in the case of finite N (number of nodes) when lambda = 0, unlike that of Bach [3]. 3. Numerical simulation shows that the proposed method performs wells, often on par with Bayesian quadrature (but with convergence rate) and better than Monte Carlo, kernel herding, and leverage score sampling [3]. 4. The bound in expectation using the DPP (Section 4.2.2) is elegant.  Weaknesses: 1. The proposed method only gets convergence rate in expectation (i.e. only variance bound), not with high probability. Though Chebyshev's inequality gives bound in probability from the variance bound, this is still weaker than that of Bach [3]. 2. The method description lacks necessary details and intuition: - It's not clear how to get/estimate the mean element mu_g for different kernel spaces. - It's not clear how to sample from the DPP if the eigenfunctions e_n's are inaccessible (Eq (10) line 130). This seems to be the same problem with sampling from the leverage score in [3], so I'm not sure how sampling from the DPP is easier than sampling from the leverage score. - There is no intuition why DPP with that particular repulsion kernel is better than other sampling schemes. 3. The empirical results are not presented clearly: - In Figure 1: what is "quadrature error"? Is it the sup of error over all possible integrand f in the RKHS, or for a specific f? If it's the sup over all f, how does one get that quantity for other methods such as Bayesian quadrature (which doesn't have theoretical guarantee). If it's for a specific f, which function is it, and why is the error on that specific f representative of other functions?  Other comment: - Eq (18), definition of principal angle: seems to be missing absolute value on the right hand side, as it could be negative.  Minor: - Reference for Kernel herding is missing [?] - Line 205: Getting of the product -> Getting rid of the product - Please ensure correct capitalization in the references (e.g., [1] tsp -> TSP, [39] rkhss -> RKHSs)  [3] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. The Journal of Machine Learning Research, 18(1):714â€“751, 2017.  ===== Update after rebuttal: My questions have been adequately addressed. The main comparison in the paper seems to be the results of F. Bach [3]. Compared to [3], I do think the theoretical contribution (better convergence rate) is significant. However, as the other reviews pointed out, the theoretical comparison with Bayesian quadrature is lacking. The authors have agreed to address this. Therefore, I'm increasing my score.