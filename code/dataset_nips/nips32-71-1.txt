Quality : The paper is technically sound and tackles a non-trivial and important issue  Significance : good (see comment on the contributions)  Originality : The contributions are original (but it would be better to position the paper in the main body, not the in the appendices)  Clarity : While the English is good and sentences are in general pleasant to read, the paper lacks in clarity mainly because it is poorly organized. Important aspects of the papers can only be found in the appendices making the paper main body difficult to fully understand and not self-contained. The most important aspects in question are : - review of prior art allowing to position the paper - Algorithms allowing to generate modified inputs for sections 3.1 and 3.2 Clarity would also be greatly improved by exemplifying things. Experiments are made in a deep learning context. It would be good to explain the introduced concepts in this setting progressively throughout the paper.  Comment : utility and robustness analysis are made feature by feature. The most discriminative information is often obtained by combining features. For example, take a two concentric circles dataset (such as those generated by the make_circles function from sklearn). Each raw feature achieves zero usefulness. The feature corresponding to the distance from the circles origin achieves maximal utility and is obtained by mixing raw features. If the function space in which features live is extremely general, it can be argued that any combination of the raw features belongs to it and therefore the definition matches its purpose.  BTW, I believe that a feature with maximal utility is any non-rescaled version of p(y|x).  I don’t think that it challenges the validity of the authors’ contributions but maybe they should comment on that because the definition might, at first sight, seem too simplistic.   Remarks :   Following line 83, investigated classifiers are perceptron like. To what extent is this a limit of the authors’ analysis ? Is this general definition useful for other parts of the paper ?  Fig. 2 / right side : ordering of methods in the caption and in the figure are not consistent.  What is the adversarial technique specifically used to obtain this figure ? I later found out that this is explained in appendix C but is can be briefly mentioned in the caption.  Line 84 : it’s unclear to me that the features are learned from the classifier … Parameters w_f are learned but not mappings f.  Line 107 : « vulnerability is caused by non-robust features and is not inherently tied to the standard training framework ». Isn’t there training frameworks prone to produce non-robust features ? (Sounds like chicken or the egg causality dilemma)  Line 124->135 : very unclear . Some explanations from appendix C need to be brought back here.  Line 159 : The definition of the deterministically modified class labels t is ambiguous. Do we have t = -y, or something more subtle ? -> again clarified in appendix C  Fig. 3 : how is transfer performed. What is transfer success rate ? A short recap would be appreciated (can be in the appendices)  The theoretical analysis of section 4 is interesting and illustrative yet based on a particular (and very simple) setting. To what extent can we generalize these results to more general situations ?  UPDATE AFTER REBUTTAL:  I thank the authors for their additional comments. I am confident that they can improve clarity as they committed to in their feedback and that their work can lead to interesting further developments.   score +=1