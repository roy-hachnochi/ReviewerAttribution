This paper offers a new perspective on the phenomenon of adversarial examples, imperceptible small feature perturbations that cause state-of-the-art models to output incorrect predictions. While previous work related this phenomenon to peculiarities of high-dimensional spaces or the local linearity of the models, this paper posits that adversarial examples are in fact due to sensitivity of models to *well-generalizing* features. In order to present their results, the authors define *robust* features as features that remain useful under adversarial manipulation. The central hypothesis is that there exists both robust and non-robust features for image classifications, and the authors present empirical evidence for this premise by explicitly disentangling both set of features.   First, they construct a “robustified” dataset by leveraging a pre-trained robust classifier to explicitly remove non-robust features. More specifically, they construct robust features by optimizing the input (with gradient descent) to match the penultimate layer of the pre-trained robust classifier. Next, they train a standard classifier on these robust features and show that the resulting model achieves strong standard classification and *robust* classification (i.e. is resistant to adversarial examples).   Second, the paper introduces a “non-robust” dataset by exploiting a pre-trained classifier to add adversarial perturbations to existing inputs (and relabeling the corresponding outputs, either uniform randomly or deterministically according to the original class label). As a consequence, these examples appear to be mislabeled to humans. They then train a standard classifier on these non-robust features, and show that these models still generalize on the original test set. In other words, even if the robust features are distracting the training signal, these models learn to pick-up non-robust features (which still achieve strong performance).   Finally, the paper also studies this phenomenon for maximum likelihood classification for two Gaussian distributions. They show that 1) adversarial vulnerability is essentially explained by the difference between the data-induced metric and the l2 metric and 2) that the gradients for more robust models are better aligned with the adversary’s metric (which support some recent empirical observations).   Strengths: 1. A very well-written paper with a new thought-provoking perspective on adversarial examples 2. A very creative and thorough set of experiments to support their claims  Questions: 1. Looking at the examples of the robustified dataset, they appear to be more prototypical examples of the classes. Did you observe that the produced examples are less diverse than the original dataset? 2. The classifier trained on robust features appears to be more adversarially robust on ImageNet than of Cifar10 (comparing Fig. 2 with Fig. 12 in supplementary material here). Is there a good explanation for this difference? 3. I’m intrigued by the experiments on the non-robustified dataset, as the robust features now confuse the training signal for the classifier. Although the experiments show that the classifier mostly relies on the non-robust features, I’m wondering how sensitive this result is to the epsilon parameter in the adversarial generation process?  UPDATE AFTER REBUTTAL: Thanks for addressing my questions. I'm keeping my score. 