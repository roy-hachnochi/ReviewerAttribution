This paper studies gradient descent learning with fixed step sizes in two player, two strategy zero sum games. This paper makes several contributions and the results are obtained via a careful analysis of the dynamics that is rather insightful. Moreover, the paper is clearly written and polished. Several interesting examples are given that are valuable to the exposition of the paper.    The main result of the paper is showing that online gradient descent achieves sub-linear regret with fixed step sizes in two player two strategy zero sum games. A matching lower bound on the regret is given, showing the main result is tight. This result is significant, given that such a result has not been obtained for fixed step sizes and without knowledge of the time horizon. This work built on a lot of recent works to obtain the results and the techniques are novel in my opinion. I believe that the deep understanding of the problem demonstrated in this work will be useful for future works.   The primary drawbacks I could see about the paper is the limited scope of the problem and in some sense the objective. In particular, since the paper only focuses on two player, two strategy games, the applications are limited and it is primary theoretical contribution. However, this is where the field is now and obtaining a strong understanding of simple games will help the study of general sum games and more complicated action spaces. There is some question in my opinion on the significance of time-averaged play converging to a Nash equilibrium when the empirical play does not. I would only suggest that the authors provide some discussion in the paper on why it is desirable for time-averaged play to converge to Nash when the empirical play does not.   The simulations in higher dimensions are interesting and simulating outside of what can be proved gives some intuition about what stronger results could be obtained. However, I am not sure it should be claimed that the experimental evidence demonstrates stronger regret bounds hold for all zero-sum games. The games that are simulated are rather benign, and as a result, I am not convinced the results indicate that it is likely results extend to higher dimensions. For this reason, I suggest the authors weaken the claims in this respect.  I have a few suggestions that may improve readability, if space permits. Lemma 6 is mentioned several times in the main body of the paper, without a statement of the result or much explaining it and without reference to where it can be found. I suggest the authors consider moving this result into the main paper if they can find space, and if they cannot, I suggest given a brief explanation of the claim and pointing to where it can be found in the appendix. Along these lines, the authors may consider giving a brief explanation on page 5 of the maximizing argument in Kakade, 2009.  --------------------------- Following author response ---------------------------- The points the authors make in the response are interesting and would certainly help future work extend this paper. If the authors feel comfortable sharing this information regarding challenges proving results for higher dimensions and other algorithms, I would encourage them to include much of what was in the response in a final version. Since the authors did not address it in the response, I want to reiterate that the authors may consider stating or giving an explanation of lemma 6 in the main paper since I was not the only reviewer who mentioned that it may improve readability. 