Although the proposed analysis is quite elegant, a natural question is whether it has to be this sophisticated. A large portion of the efforts was devoted to bounding the description length of compositing an arbitrary function with the activation function \rho. I noticed that all activation functions studied in the paper are contraction functions. That is, for any t and t', |\rho(t) - \rho(t')| <= |t - t'|. I am wondering if the proof can be greatly simplified (or the description length argument can be completely circumstanced) if this property is properly used.   In particular, if matrix W' is an approximation of W such that ||W' - W|| <= \eps1, and two vectors x' and x satisfies ||x' - x|| <= \eps2, then   ||\rho(W'x') - \rho(Wx)|| <= ||W'x' - Wx|| <= ||W' - W||*||x'|| + ||W||*||x - x'|| <= \eps1*||x'|| \eps2 * ||W||.  It means that we can upper bound the difference of the output (\rho(W'x') - \rho(Wx)) using an upper bound on the input (||x' - x||). By recursively applying this inequality layer by layer, it leads to a function approximation bound, and can be used to derive a covering number bound and finally a generalization bound. I got a feeling that this bound is also sub-linear to the number of parameters, but I am not sure.   Hopefully the authors can convince me by the rebuttal that I am wrong.  ==  I have read the authors' rebuttal. I am not totally convinced but I tend to believe that it's true. My score remains unchanged.