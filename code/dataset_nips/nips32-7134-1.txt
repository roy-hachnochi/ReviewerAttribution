The authors proposed a provable solution to adversarial training of decision stumps and tree ensembles and also exact and upper bounds of the "robust test error". This problem is hard in general for arbitrary complicated classifiers, because the inner maximization in adversarial training is a non-concave problem in general. But the authors show that the inner maximization is concave in case of boosted decision stumps by exploiting the fact that the objective function is piecewise constant and one could perform maximization by sorting the "boundary points" in the piecewise constant function. For boosted decision trees, one could rely on upper bounds on the robust error, which are found to be tight empirically. Empirical results show that the model is indeed provably robust for reasonably sized perturbation (l_inf norm is considered).  The paper overall reads pretty well and the idea sounds interesting. But I have some concerns regarding the evaluations: 1) It would make sense to compare the method against [7] at least for the datasets that are not too big. This could better highlight advantages of using the proposed method over [7]. 2) As another baseline, it would be insightful to compare the results against other classifiers that are adversarially trained (that use approximations in the inner optimization), for example using Projected Gradient Ascent. This could highlight the importance of how solving the inner optimization accurately could lead to better robustness. 3) The training computational complexity is O(n^2 T log T), which means that O(n) passes over the entire dataset may be needed to train the classifier. This may limit practical use of the proposed method for huge datasets, where O(n^2) could be prohibitive. Is there a way to bring the complexity down to less than O(n^2) running time?