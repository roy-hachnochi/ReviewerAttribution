Post rebuttal update:  I thank the authors for taking the time to prepare a detailed response to my questions, suggestions, and criticisms. I believe that they did a good job in addressing my main questions and concerns. I do agree with other reviewers that comparisons with existing (policy-based) techniques could make this a stronger submission. However, I also agree with the authors that both policy- and cumulant-based approaches should have advantages and disadvantages, so it make sense for them to co-exist in the literature. I suggest trying to better situate the proposed method within the existing literature and to discuss possible downsides of this general approach---e.g. the fact that the simple linear combination of cumulants may often be insufficient to properly span the space of desired behaviors. Given the concerns of other reviewers, I have lowered my score from a 9 to an 8. Overall, though, I still believe that this is an important contribution to the field. ----  This paper introduces a method to combine existing options not merging or analyzing them in the space of policies or actions, but by manipulating them in the space of pseudo-rewards, or cumulants. In particular, the authors first show that every deterministic option can be represented as a cumulant function defined over an extended domain that includes an option-termination action; then, they show that novel options can be obtained by linearly combining the cumulants of the existing options. Importantly, the authors also show that if the cumulants for a given set of existing options is known a priori, the policy for the novel option being synthesized can be directly identified without requiring any learning.   This is an interesting paper introducing a novel and important contribution to the field. It is very well written and the experiments clearly demonstrate the contribution of the proposed method.  I have few comments and questions:  1) I understand that it may be possible to synthesize novel options by linearly combining their corresponding cumulants. However, I believe there must exist some implicit assumption on what type of behaviors that can be practically defined by computing linear combinations of their corresponding pseudo-reward functions. E.g., the authors introduced, as a motivating example, the goal of learning to walk while grasping an object, given separate options for walking and for grasping. Would it be possible, via the proposed technique, to synthesize options whose constituent options may interfere with one another? The idea of linearly merging reward functions seems to ignore the fact that, in some cases, a reward function for achieving a specific combined goal may need to be more sophisticated than a simple linear scaling of how desirable each of the two (possibly conflicting) sub-goals are. Could you please discuss this possible issue (and the possible underlying assumptions of this approach) in more details?  2) in Eq2, what is "t"? This time variable does not appear anywhere in the definition of the quantity being defined---Q^pi_c(s,a);  3) immediately before Eq3, how is Q^max_c computed, exactly? The text says that Q^max_c = max_i Q^{pi_i}_c, so I assume it corresponds to a maximization over each possible policy pi_i. However, that seems to assume that one Q-function Q1 would always be strictly larger than another Q-function Q2 (i.e., so that Q1(s,a)>Q2(s,a) for all s and a). Shouldn't this max operation, however, be defined over the individual Q-values for each (s,a) under each policy pi_i?  4) the discussion (in page 4, in the paragraph immediately after Eq5) for how the initiation set I_e is defined is not immediately clear to me. It seems to assume that the initiation set is formed by *all* states in which the option does not directly terminate; but this shouldn't necessarily be the case: the termination set may be defined as a strict subset of those states. Could you please clarify or justify this statement/definition?   5) the proposed method seems to assume that, in order to combine options, one needs to know (in advance) the Q-function of each option's policy when evaluated under the cumulant function of all other possible options. Is this correct? If so, how scalable is the method, in the sense that it may require each possible behavior to be evaluated a priori under every other possible pseudo-reward function?  6) in the experiments section (section 5.1) the authors say that "We used Algorithm 3 to compute 4 value functions in Q_eps". It is not immediately clear to me why there are 4 value functions in this case, since m=2 and there are 3 types of good. Could you please clarify?  7) it is very hard to read Fig3 in a black-and-white printed version of the paper. I suggest picking a better set of colors for the curves, if possible.