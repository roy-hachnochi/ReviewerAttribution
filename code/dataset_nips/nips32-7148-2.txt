This paper presents a hierarchical reinforcement learning (HRL) framework. The proposed HRL is based on the option framework where the option policies are trained with generalized policy improvement (GPI).　Based on the property of GPI, a new option can be obtained by combining evaluated options.　The higher-level policy that activates the option is learned through a variant of Q-learning.  Originality: Composition of policies used in this paper is briefly mentioned in the original paper by Barreto et al. [2017]. In addition, other methods for composing policy is investigated in  [Hunt et al., ICML2019] and [Haarnoja, ICML2018].  Thus, I think the originality of this paper is minor.  Quality: The proposed method is a simple extension of GPI to the option framework, and theoretical contribution is minor. Although the empirical results show the benefit of the composing option policies, the performance of the proposed method is not compared with existing methods.  Significance: The paper presents a method for obtaining a new option by combining learned options. Although this feature may be novel in the option framework, I think that this contribution is incremental since composing a new policy from existing policies have been investigated in recent studies.  === comments after the rebuttal === I appreciate the authors' response. I would like to request authors to address the following points when revising the paper.  - What is reported in the rebuttal is comparison of the higher-level policy for composing a new option. The result does not mean that the proposed framework outperforms the method in [ Haarnoja et al., 2018] because the options are not trained with soft-Q learning. Authors should explicitly state this point when addressing the new results to the paper.  - I also think that the fact that the options need to be pre-trained is a big limitation since many "policy-based" HRL methods can jointly train the higher- and lower-level policies. This property can be critical in performance since the composition would work very well if the option cumulants properly represents the task dynamics, and if not, it works very poorly as pointed out by other reviewers. In the rebuttal, it is mentioned that "OK and player can be learned together (we are currently working on it).", which means that it has not been achieved yet. I would like to ask authors to honestly discuss the difficulty of jointly training OK and players in their framework.  