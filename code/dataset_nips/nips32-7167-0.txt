The results established in this work are significant from a theoretical standpoint. Through the proposed algorithm SCG++ , the authors have developed a method where quite literally both the upper and lower bounds match for maximizing a monotone DR-submodular function 'F', being true to title of the paper. The approach taken in proving this result based on a variance reduction method that estimates the difference of gradients in the non-oblivious stochastic setting is novel. The ideas developed in Section 4 are very interesting and useful.  So on originality, quality and significance, I would rate the paper in the top tier.   My primary concern with the paper is how much these results add value in a practical setting. Since no experimental evidence are provided, it is hard to judge the computational speed up gained by SCG++ over competing methods like SCG, SGA, Continuous Greedy methods etc. when applied to an actual problem. For instance: a) When both SGA, SCG and SCG++ use the same number of gradients, what is the difference between the function values at the obtained solution? b) Experimentally, how many number of gradient evaluations does each of these algorithms need to obtain the same quality of the solution? c) In the setting when the monotone DR-submodular function 'F' is available in closed form and the its gradients can be computed exactly, how much the variance reduction method based on difference of gradients actually help say in reducing the variance and giving a superior estimate?  Disclaimer: I have not read the proof in detail to aver its accuracy.  Typo: 1. In line 120, I believe it should be \nabla f(y, \mathcal{M}). \nabla is missing before 'f'.   EDIT: ----- The authors have addressed my concerns in their rebuttal report.