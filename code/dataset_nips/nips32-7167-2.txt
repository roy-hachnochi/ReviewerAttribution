Originality: The paper studies a known problem of maximizing a continuous submodular function under convex constraints. The main technique to achieve the improved number of queries are 1) adapt the gradient variance reduction technique to the non-oblivious case using Hessian and 2) the approximation of the Hessian with O(d) gradient evaluations. The proposed bound needs the assumption about the smoothness of the Hessian (assumption 4.7), and I think without such an assumption, it would require O(d^2 \epsilont^-2) oracle calls. The dependence on d seems also important to me. Maybe the authors can illustrate more on this point.  Quality: The paper has theoretical analysis of the proposed algorithm as well as a lower bound.  Clarity: The paper is well organized.  Significance: The proposed algorithm achieves the best-known results of the number of queries to get an almost optimal guarantee for DR submodular functions. The number of queries is also shown to match the information theoretical lower bound. The paper's result is therefore significant.  After rebuttal: I have read all other reviewers' comments and the authors' feedback. I keep my evaluations unchanged.