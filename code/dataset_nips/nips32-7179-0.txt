This work proposes algorithms for the online-within-online meta-learning setting as oppposed to the more prevalent statistical setting. In this particular meta-learning setting tasks arrive sequentially manner (outer loop) and then the learning per task itself happens in an online fashion. The aim is to have low average regret over tasks.   The inner loop optimization is done via Online Mirror Descent (OMD). The inner algorithm design is carefully chosen to provide good approximations of (sub)-gradients of the outer meta objective. Specifically two nested online primal-dual online algorithms are utilized.  This algorithmic framework is then extended from the adversarial to the statistical setting by two nested online-to-batch conversions.  Experiments on synthetic data and on the movielens-100k dataset which contains the rationgs of different users to different movies are presented where the proposed algorithms perform significantly better as a function of number of training tasks than treating the tasks independently (ITL). In the movielens-100k dataset each of the 943 users' ratings are considered a separate task.    Comments:  While I will leave the judgement of theoretical novelty to more qualified reviewers I have a number of more practical questions:  - Does this framework extend to meta-reinforcement learning settings trivially? (the online feedback is episodic sparse reward) or does that require fundamental change in viewpoint?  - Minor nitpick: While the experiments on movielens dataset is a good one and I understand that scope of the paper is to study OWO setting theoretically, are there fundamental challenges to scaling up to say supervised datasets in rich observation spaces (meta-datasets of images as proposed in Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples by Zhu et al. recently)?  - Minor nitpick: The relegation of related work to supplementary is a bit odd. It actually served to better situate the paper in light of related work and especially with respect to the batch statistical setting papers and other online meta-learning settings.   Update: Thanks to the authors comments on extensions to reinforcement learning. I have maintained my good scores.