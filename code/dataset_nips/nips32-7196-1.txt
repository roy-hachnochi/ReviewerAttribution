Summary: The paper proposes the use of two extra term losses that encourage positive signalling and listening in multi-agent reinforcement learning settings where agents have access to a communication channel. The authors show that this leads to agents that learn to more robustly use the communication channel (across different runs) compared to agents that are not trained with these extra losses (or intrinsic rewards) or only use one of these terms.  Strengths:  The paper is generally clear and well-structured  The paper addresses an important problem in MARL and particularly, attempts to tackle the more challenging and realistic setting of decentralized training and execution of the agents, without direct access to other agents’ parameters, rewards, actions, states, or internal beliefs and preferences.  I like the various discussions throughout the paper that provide explanations and intuitions for the behavior of the agents or the effects or different losses on their performance (e.g. section 4.2). The paper appears to be technically correct.  I also appreciated the fact that the authors openly acknowledged some of the limitations of their method (e.g. section 4.2)  Weaknesses: The paper could benefit from comparisons against stronger and more diverse baselines, such as the method proposed by Jaques et al. 2019 (which also uses decentralized training and execution), Foerster et al. 2016 (e.g. RIAL which is concerned with the same setting), or Sukhbaatar et al. 2016 The paper lacks a large number of references to related work. The auxiliary losses proposed are a form of reward shaping for improving MARL algorithms (e.g. prosociality, curiosity, empowerment, optimistic Q-learning etc.), on which there exists a large body of work which is not discussed in the paper.   Some examples of related papers are: Peysakhovich & Lerer (2018), Devlin et al. (2014), Foerster et al. (2018), Oudeyer & Kaplan (2006), Oudeyer & Smith (2016), Forestier & Oudeyer (2017).  While the simplicity of the chosen tasks helps to understand in greater detail what the agents’ behavior, I think the paper requires more evaluation on more complex tasks. In particular, I think many readers would be interested in a discussion of the scalability of this method to more than 2 agents. Moreover, the number of symbols used in the communication channel is quite small and it would be good to see how the method performs as the vocabulary increases.  Some of the results don’t seem that impressive. While the proposed method allows agents to use the communication channel more often than the alternatives, it does not improve performance by a very significant amount, when comparing the runs that make use of communication. Numerous details about the algorithm used for optimization are missing. In particular, what is the total loss used for optimization in both tasks? How does that relate to RIAL or other previously proposed MARL algorithms?   Other Comments:  It would be good to provide a plot or table with the final reward across all runs. I expect that number to show their method as obtaining much better rewards on average, given the larger proportion of runs in which the communication channel is actually being used by the agents.   It is not very clear to me why the agents still learn suboptimal communication protocols even when learning to use the communication channel. Have the authors tried to use a more powerful RL algorithm such as PPO, SAC or A3C?  Why did the authors decide to use a multi-step version of the CIC algorithm? It would be useful to provide an ablation study in which they also compare against the single step version.   There are some missing details when going from equation (5) to (6) that should at least appear in the supplementary material.   The proposed method is novel (as far as I can tell) and the problem is of wide interest, so I believe a stronger version of this submission would be of interest to the community. However, I do believe the paper as it stands right now requires more empirical evaluation against stronger baselines and on more complex environments.  ------------- UPDATE:  I have read the rebuttal and the other reviews. I appreciate the fact that the authors took into account the feedback, clarified some parts of the paper, and promised to add more comparisons with prior work, analysis of the learned communication protocols and relevant references. Their rebuttal helped me understand why they made certain decisions and what the scope of the paper is. I now lean towards acceptance and I have updated my score to 6.  