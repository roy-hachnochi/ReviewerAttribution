One of the most important open problems in emergent communication is the problem of discovery: how can two randomly-initialized agents stumble upon a communication policy that transmits useful information and helps solve a given task? Many previous approaches have used gradient information passed from the listener to the speaker in order to improve the learning of communication protocols, however this has some drawbacks. This paper provides an alternate approach, which is to add additional rewards to the decentralized, discrete-message framework to encourage communication. The authors show that these biases improve performance on two separate communication games.   I quite like this paper. It is well-written, the methodology makes sense, and the results clearly show that the proposed biases (which incentivizes the ‘speaker’ agent to send messages correlated with its state, and incentivizes the ‘listener’ agent to take the speaker’s messages into account when acting) improve performance. The environments tested (MNIST adding and the new ‘treasure hunt’ environment) are fairly simple, but in my opinion interesting enough to show the benefit of the proposed approach. The implementation of these biases is also non-trivial, and the paper walks through how they are derived in detail.   My main concern about the paper is the comparison to Jaques et al. (2018). As the authors mention, incentivizing ‘positive listening’ was previously investigated in Jaques et al. by giving the *speaker* a reward for sending messages that had a large influence on the listener. In contrast, this paper rewards the *listener* for being influenced by the speaker. Given that these two objectives are fairly similar, it is surprising to me that the paper doesn’t compare to the influence formulation of Jaques et al. I think the experimental results would be much stronger if this comparison was included, along with an explanation of the pros / cons of each approach. Finally, given that a similar approach to the one in this paper was taken by Jaques et al., the novelty of the paper is more limited.    Overall, I would recommend this paper for acceptance if there was a more extensive comparison to Jaques et al. For now, I will give this paper a weak accept, but am willing to update my review accordingly.   Small comments: - L166: We use is the -> We use the - L172: is new -> is a new - L176: the results protocols -> the resulting protocols   -------------------------------------------------------  After reading the author's rebuttal, I will also increase my score by 1 point, from a 6 to a 7. My main concern was the comparison to the similar formulation from Jaques et al., and I'm convinced by the authors assertion that this does not work in the MNIST setting. I'm hopeful to see this comparison in the final version. My sole remaining concern is how Jaques et al.'s method will compare in the Treasure Hunt game (the authors state that they 'will run this'), and whether this result will be included even if it shows their method does worse. Despite this concern, I now reside more firmly in the 'accept' camp.  