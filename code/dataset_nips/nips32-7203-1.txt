The authors propose a very simple and practical method for evaluating Bayesian model uncertainty by running SGD with a fixed step-size and fitting a low-rank Gaussian to the resulting iterates. Surprisingly, this method works better than many more involved methods for approximating Bayesian inference. This method is currently one of the most practical ways of evaluating model/parameter uncertainty when training deep neural networks and could be very useful to many people.  The paper is written very clearly. There is useful theory supporting the method, with relevant references to Mandt et al about the approximately Gaussian distribution of SGD and how this relates to the Bayesian posterior. There is a useful experimental evaluation of the posterior landscape. The method is shown to work at Imagenet scale. There is a thorough comparison against competing methods like Laplace approximations, SGLD and dropout.