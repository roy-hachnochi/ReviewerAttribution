The paper is well written, and the empirical evaluation is thorough. However I am concerned by the interpretation the authors propose, which I think will add confusion to the literature. I feel that the paper should not be accepted without significant re-structuring.  The authors describe their method as a baseline for Bayesian uncertainty, however it is straightforward to show in simple cases that the stationary distribution of SGD does not approximate the Bayesian posterior.  Indeed Mandt et al. ('SGD as approximate Bayesian inference') showed that, under Bernstein-von Mises type identifiability assumptions, SGD will converge to an isotropic Gaussian distribution near the minimum as the dataset size grows. This Gaussian distribution is independent of the Hessian of the posterior, depending only on the batch size and the learning rate. In order to get Bayesian samples from SGD, one must replace the learning rate by a learned pre-conditioning matrix.  On a more minor note, I would have appreciated a comparison between SWAG and conventional model ensembling. While ensembling requires k times more computation at training time, it requires the same computation as SWAG at test time, and I would expect it to perform substantially better on the test set.   Edit: I thank the authors for their response, however I remained concerned about the framing of the paper as a Bayesian method. In my view, it is not appropriate to call any distribution over parameters Bayesian. One should show that the distribution is close to a valid posterior under some reasonable (if often unrealistic) assumptions. By contrast, it is possible to prove that under reasonable assumptions, the stationary distribution of SGD is independent of the shape of the posterior. Furthermore, the authors themselves demonstrate that Mandt et al's method for predicting the learning rate fails in practice. I presume that this learning rate, which is what sets the uncertainty scale, must be tuned on the validation set, although the authors claim otherwise in the paper.  I do agree with the other reviewers that the proposed method provides a strong baseline for handling uncertainty in deep learning. However I believe that the paper should be re-worked for a future conference, emphasising that the method is primarily empirical, rather than being a Bayesian approximation. I am particularly concerned that this paper furthers existing misunderstandings about the Mandt paper in the community.  With this in mind, the authors should compare against other non-Bayesian baselines for uncertainty. To my knowledge, the "deep ensembles" method already outperforms all the Bayesian techniques considered in this work, and additionally I remain unconvinced that SWAG would outperform ensembling in settings where the bottleneck is the compute cost at test time.  Finally, I note that SGLD was run at a reduced temperature below 1 in order to prevent the iterates diverging. This may indicate that the posterior is improper, in which case any accurate Bayesian method would be likely to produce poor results.