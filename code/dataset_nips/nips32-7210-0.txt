— The proposed task, agent architecture, experiments, and analysis are all clearly described, and the writing overall is easy to follow (which is great!).  — The proposed task is inspired by and is a simple adaptation of prior work in visual reference games. Each of the two agents (speaker and listener) gets to see a pair of images (target and confounder). The listener agent understands various visual attributes to varying levels of accuracy. During the training phase, the speaker agent gets to see observations of the listener's predictions / obtained rewards and must build a mental model for the listener agent. During the evaluation phase, the speaker must decide which visual attribute to emit in order to maximize the chances of the listener being able to pick the target image.  — Experimental evaluation is technically sound. The comparisons with and without the agent embedding, different policies, and different perception modules (all across datasets) are all quite insightful, and validate the hypothesis that an agent that builds a mental model of its partner's abilities works better than an agent which does not.