Highlighting strengths and weaknesses of the submission below.  Strengths  a. The paper is generally easy to follow. The authors do a decent job of motivating the problem / task at hand and the key axes of variation (different perception machinery and different degrees of conceptual understanding).   b. Apart from the issues highlighted under weaknesses, the authors do a decent job of describing details associated with the experiments section. Within the controlled setup, the heuristics used to construct the population of listeners, modeling different degrees of understanding in listeners, etc. is explained clearly.   Weaknesses  a. The paper does not discuss some amount of recent related work in the space of reference games (using images or otherwise) and discrete communication (single / multiple time-step episodes), using dialog (natural language or otherwise) or single-round QA. Some pointers being https://www.aclweb.org/anthology/D17-1321, https://arxiv.org/pdf/1904.09067.pdf (deals with a population of agents) and https://arxiv.org/pdf/1703.06585.pdf. The authors highlight (L73 - 75) that the focus of the works on emergent language / communication is on the communication protocol unlike that of this work (which is on the agent’s mental model of other agents). It’s not entirely clear if the former is independent of the latter. This, combined with the fact that performance in these settings is mostly reported in context of the downstream performance, implies that the existing experimental settings could’ve been built on top of. Could the authors comment on this? Was there any other reason except for the fact that learning communication in conjunction to models of the listener adds another stochastic component?  b. In section 3.3, under “Attribute Selection Policies”, it is mentioned that the speaker has access to a parameterized policy and a function V (s_k, a_k). It is unclear what the function V is trained on / how it is trained. The equation above L144 seems to suggest it is trained over the train (practice) and evaluation runs. But the following sentence implies evaluation sessions are handled greedily w.r.t. V.  Finally, the structure of the cumulative reward function used to train the active policy is confusing. Why isn’t just the cumulative reward \sum_k r_k used as R? The provided reasoning is equally confusing -- “encouraging the … practice episodes” -- in the sense that an informative embedding (given perfect communication) should ideally form even if R = \sum_k r_k. Unless I am missing something, this choice seems odd. Can the authors comment on / clarify this?  c. How do the authors sample the target and confounder images?  My major concerns with the paper revolve around the ones mentioned above under weaknesses. The mentioned weaknesses also affect the clarity and originality of the paper to some extent. The paper would benefit from addressing the same.  -------------------------------------- Post-Rebuttal Updates -------------------------------------- Thanks to the authors for responding to the concerns raised in the reviews.  The rebuttal satisfactorily addressed my concerns regarding situating the work properly in context of recent related work in this sub-space.  While the process described in L143-151 is easy to follow, I found the choice of training V(s_k, a_k) on practice + evaluation runs and choosing greedily w.r.t. the same (L144-145) odd at first as this was not stated in context of a continually adaptive speaker in this specific section. This was somewhat evident from how the rewards for the active policy were structured (above L148; error of V on practice episodes) but was not clear enough. R2’s comments and the author response helped alleviate this concern. I would suggest the authors to make this more clear in the final version.  As R2 and I pointed out, the paper would benefit from discussing in detail a comparison to a practice-stage reward setting so as to highlight the importance of the meta-learning aspect of the approach.   The comment that I had made in this context was regarding how the reward for training the active policy was structured and the associated justification provided in L148-150. I specifically asked how the current choice (error in V over evaluation episodes) compares to just maximizing the reward over practice episodes and whether that would / would not lead to informative agent embeddings. The response in the rebuttal document pointed to the fact that “epsilon greedy leads to more informative embeddings in spite of the similar downstream performance as the active policy” — which seemed orthogonal to the question I had asked. While I believe this does not negate the contributions of the paper but it does seem like an important point to clarify / address to justify the training paradigm.  Having said all the above, I generally like the paper and the experimental analyses seem sound and thorough. I agree with R2 that the work is well-positioned to inspire interesting work in this setting. Therefore, I am increasing my score to 7.