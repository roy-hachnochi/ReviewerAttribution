I enjoyed reading this paper and support its publication in NeurIPS. The primary contribution of this work is technical and theoretical, but it provides a sharpened analysis of a very practical algorithm for robust regression. This work shows that under certain natural conditions, ell-1 penalized least-squares achieves the minimax rate of convergence for this problem, up to a logarithmic factor. I am not an expert in this area, but according to the authors, previous analyses of this same algorithm failed to show this sharp rate, and previous algorithms achieving this rate were complicated and difficult to use in practice.  The technical innovation applies the KKT condition for beta, at the estimated outlier-contamination vector theta, to derive a recursive bound for the squared-error of the beta estimate. A main term of this bound is v'X'u for (u,v) the errors in (theta,beta), and a main technical insight is that this can be controlled by ||u||_2*||v||_1/sqrt(n) + ||u||_1*||v||_2/sqrt(n), rather than the naive operator-norm bound ||v||_2*||u||_2, when the design X satisfies an incoherence property with the standard basis. The improved bound then applies this insight and an a priori bound on (u,v) from a more standard Lasso-regression analysis. The authors demonstrate that the required incoherence property holds for (correlated) multivariate Gaussian designs, using a Gaussian-process and peeling argument.  In my view, this insight is non-trivial, and both the paper and proof are also well-written.  A few comments/minor typos:  (1) I think some more explanation is needed after Definition 1, to explain how this captures the notions of restricted invertibility and incoherence in the previous paragraph. In particular, the role of the transfer principles in restricted invertibility is a bit confusing, as the RE condition for Sigma is only discussed later on the page. Explaining why (ii) captures some notion of incoherence would also be helpful.  (2) It took me a while to find where X^{(n)} and \xi^{(n)} are defined---perhaps this can be clarified more explicitly.  (3) Line 101, p-th largest should be p-th smallest  (4) Line 476, J should be S and beta_j should be beta_j^*  (5) Is there a minus sign missing in the first term on the right of Lemma 1, and its application in line (525)?  (6) I'm not sure what trace means above line 564 for W_{b,v}.  ------------- Post-rebuttal: Thanks to the authors for the response, clarification, and discussion.