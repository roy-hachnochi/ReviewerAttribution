[Update: thank you for providing the author response. I look forward to the final version including more details about the tests, as requested by reviewer 2.]   This paper studies the presence of social biases in contextualized word representations. First, word co-occurnce statistics of pronouns and stereotypical occupations are provided for various datasets used for training contextualizers. Then, the word/sentence embedding association test is extended for the contextual case. Using templates, instead of aggregating over word representations (in sentence test) or taking the context-free word embedding (in word test), the contextual word representation is used. Then, an association test compares the association between a concept and an attribute using a permutation test. The number of significant associations across tests is recorded for every word representation model.  The results show that some representation exhibit more bias in more tests than others. Some of the interesting trends are that larger models exhibit fewer positive effects and that contextualized word representations present different biases than sentence encoders. Race bias seems to be more pronounced than gender bias, both on its own and when intersected with gender.  The discussion acknowledges certain limitations of the work in representing gender and more nuanced identities, and points to directions for future work.  Originality: the work is a natural next step after the investigation of bias in context-free embeddings and in sentence representations. The method is a simple extension from WEAT/SEAT. The contribution is well situated w.r.t prior work.   Quality: the paper presents a nice set of experiments that explore different possible manifestations of bias. The appendix provides more complete results. The appear also acknowledges and discusses potential limitations. Some further analysis would be welcome, such as: - Relate the overall results in section 4.5 to the corpora statistics in table 1.  - Clarify and expand the discussion of overlap or no overlap between tests revealed by contextualized representations and sentence encoders (sec 4.5, second paragraph). It's a bit hard to follow.  - Test the claim that bigger models exhibit less bias by training models of increasing size (but otherwise identical), e.g. Glove with different sizes.  - More discussion of the intersectional results in 4.7. What do they mean?   Clarity: the paper is well written and very clear. A few minor comments: - Define intersectional earlier. - Define acronyms on first occurrence (e.g. LSTM) - corpuses -> corpora?  - Say a bit more on the counting method from [37].  - line 113: table 1. Also refer to other tables from the text.  - Table 2: give total number of tests per type (say, how many race tests are there in total). On the other hand, the current "total" column is misleading because it counts the same tests multiple times (across models).  - Line 130: sentence seems broken.  - Line 142: the the  - Section 4.2: line 153 says "we use the contextual word representation of the token of interest, before any pooling is used to obtain the sentence encoding". If I understand correctly, the contextual representation is used instead of pooling; that is, in this case, there is no pooling. Right? The specific method should be described in the main paper in my opinion.  - line 259: onto (a) the   Significance: The results are important for the NLP community, where both contextualized word representations and bias are topics of much research. The results show complementary benefits to previous work on sentence encodings (although this part can be analyzed better). 