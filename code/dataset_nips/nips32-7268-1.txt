       The paper presents a method for scaling up classifiers for tasks with extremely large number of classes, with memory requirements scaling with O(logK) for K classes. The proposed model is effectively using count-min sketch to transform a very large classification problem to a small number of classification tasks with a fixed small number of classes. Each of these models can be trained independently and in parallel. Experimental results on a number of multi-class and multi-label classification tasks shows that it either performs as well as other more resource-demanding approaches or it outperforms them, depending on the dataset, with a controlled reduction of model size based on the selection of the B and R parameters. When it is used for modeling retrieval as a classification with a very large number of labels, it outperforms Parabel, which is also limited by memory requirements.         