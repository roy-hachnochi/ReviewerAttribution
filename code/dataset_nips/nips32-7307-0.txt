The paper proposes to use a Gossip based algorithm for performing gradient updates across distributed agents in RL. In this approach each agent is connected to a set of other agents  to which it sends updated parameters, and from which it receives updated parameters (the outgoing connections do not have to be the same as incoming ones, in theory). At each step of learning (every N actions) each agent performs updates to the value function using TD error gradient, and to the policy using policy-gradients. The updated parameters are sent out to the outgoing peers that it is connected to and new parameters are received from incoming peers it is connected to. The incoming messages are combined (averaged, it seems, although it appears that the matrix P does not have to be this way) and parameters are updated.   The results shown in the paper seem to highlight improved performance over A2C under the same wall clock time. I could use some clarification however for Fig 1b -- how do I compare the number of steps for A2C vs the steps for Gala-a2c -- were the same number of workers being used in both cases ? From the differernce between 1(b) and 1(d) it would appear that steps pass faster for Gala-A2C, and I'm wondering why..  In fig 1a, why does the performance degrade so catastrophically with larger number of simulators ? Can this be ameliorate by changing the sparsity of P ?   Did the authors try stochastically sampling P, or is that assumed to happen just by variability in communication  ? 