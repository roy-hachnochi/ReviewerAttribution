The paper is clearly written and very well presented. Although inherently technical, the results are explained both precisely and in plain language, with proof sketches to convey intuitive understanding. This paper is a great model of clear communication of technical results.  The results are novel to my knowledge and well situated in terms of previous literature. I found no obvious technical errors, although I wasn't able to closely check the proofs in the supplement. My impression is that the results themselves don't involve significant new technical ideas and are more or less straightforward extensions of previous theorems. Nonetheless, actually doing this technical work is a valuable contribution.  My main concerns about significance, which largely apply to Bernstein-von Mises theorems more generally, is that by focusing on the asymptotic regime the work assumes away essentially all of the practically relevant structure in Bayesian inference problems. Behind all the technical machinery, the intuition behind these proofs (which, to its credit, the paper does a good job of conveying) is that for identifiable models in the iid asymptotic regime, the likelihood dominates the prior, and the posterior concentrates at a normal shrinking to a point mass, so we can ignore the prior and we can mostly ignore posterior uncertainty. But if you really believe you're in this regime, why not save yourself the trouble of VB and just fit an MLE? The argument that the MLE minimizes KL between the true data distribution and a misspecified model is so trivial that it's more of an observation (that the non-constant part of KL is just the expected model log likelihood) than an argument. This work dresses up that argument with substantially more mathematical machinery, but not (as far as I can tell) much more insight. It tells us that if you run VB in the setting where there is no uncertainty to quantify, it preserves the properties of a point estimate. This is well and good -- it's always possible that something could have gone wrong, and there's some pedantic value in checking that it doesn't -- but it's also kind of not the point of VB. Practical Bayesian inference involves quantifying uncertainty; without that, why are we here? We only get to do so much with our wild and precious lives, and it's not my place to question the authors' choices, but I can't help but view this as something of an example of math for math's sake with limited takeaways for the broader field.  All that said, theoretical papers are in scope for NeurIPS, and this one is well done within (as far as I'm qualified to judge) the standards of the community.