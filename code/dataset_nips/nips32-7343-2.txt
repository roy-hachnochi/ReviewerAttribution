Update After Rebuttal ----------------------  After reading rebuttal and other reviews, I continue to argue for acceptance and leave my original score (good paper, accept) unchanged.  I thank the authors for willingness to discuss issues like local optima in the VB result in a revision, and also for willingness to describe simulations more carefully and share complete simulation code.   I was glad to see comments from other reviewers about relevance of the LAN assumption or the Bernstein-von-Mises approach, and I hope a revision addresses these issues in more depth (as the rebuttal hints). In particular, I'd encourage a thoughtful response to the question R2 raises: "... if you really believe you're in this regime, why not save yourself the trouble of VB and just fit an MLE?". This which isn't really addressed in the rebuttal, and I think it's important to both raise and answer the point in the paper.  Review Summary -------------- I appreciate the paper's focus on determining what happens to the optimal approximate posterior in the inevitable case that the model is "wrong", and thus the contribution of providing theorems to guide our understanding of how tractable approximations like VB behave in the asymptotic limit will be of interest at the conference. I wish the paper had a bit more to say about how local optima fit into this story (at least acknowledging the practical problems) and I'd like to see more details about the simulations, but overall this seems like interesting work and would lead to productive discussions as an accepted poster.   Technical Comments ------------------  ## Comment on local optima?  One inevitable issue with the practical outputs of variational Bayes iterative optimization is that we almost surely return a local optima rather than a global one (the variational ELBO is usually non-convex for any model of interest, such as the LDA topic model in the examples). While the theorems rather nicely govern the behavior of the global optima, I'm not sure they can say anything about local optima.  Given this, I'm a bit surprised that the Simulations in Sec. 3 seem to gloss over the practical issues of local optima in VB as well as the practical mixing issues of MCMC on real finite datasets. For example, I'd be very reluctant to say that HMC has ever really converged to an "exact posterior" for a model like LDA, especially given only the results of one chain (often mixing issues aren't apparent until one chain does much better than 50 others). Perhaps with simple 15-dimensional observations it's possible to avoid serious problems (but still, if you look closely the curves don't match perfectly, so maybe there is a complication here). Would be interesting to see results from a much larger topic model fit (perhaps with several hundred observations), where local optima might play a larger role.   ## Comments on rates of convergence?  For practitioners that will never have access to "infinite" data, I wonder if the analysis here provides insight about when to expect that the convergence conditions are "close enough" to being satisfied (e.g. as argued in the Fig 2 plots, where 20000 examples is a proxy for "infinite" data).     ## Tail condition assumption: How strong is it?  When providing the key assumptions required for Thm. 1, while I agree it's likely true that many common priors satisfy the required tail condition -- second derivatives of log p(\theta) do not grow faster than exp(\theta^2) -- I wonder if there are any known (or easy-to-construct) counterexamples? It might help to give a bit more intuition for what kind of smoothness this implies.   Comments on Simulations -----------------------  * Code to perform the simulation study is missing (seems that only the stan model specification is provided, not the code to produce the plots). Sharing this would be crucial to help readers reproduce results and understand their practical import.   * How are the errorbars/intervals in Fig 2b calculated? Is this showing the full spread of the KL across topics, as well as average (mean) across topics?  * Likewise, how are the errorbars in the remainder of Fig. 2 calculated? Should we be bothered that they do not seem to go to zero with more data (N)?  * I might recommend that each figure show something like a 10x higher maximum N values than currently used. Doesn't seem you're showing real convergence (e.g. the intervals in Fig 2c at N=20000 don't quite line up yet).   Presentation Comments ---------------------  I found Sec. 1, especially the Main Idea, quite easy to follow. Well done.  Fig. 1 does a nice job illustrating the main idea, with perhaps the exception that visually, the change in distribution over \theta from observing one to infinite data is larger than the whole space of possible factorized distributions Q. 