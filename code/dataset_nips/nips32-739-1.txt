This paper proves optimal generalization error bounds for Distributed Gradient Descent in the context of multi-agent decentralized non-parametric least-squares regression when i.i.d. samples are assigned to agents.  The derived results show that if the agents hold sufficiently many samples with respect to the network size, then Distributed Gradient Descent with a number of iterations (depending on a network parameter) achieves optimal rates.   Provided the “communication delay” is sufficiently small, the Distributed Gradient Descent yields a linear speed-up in runtime compared to the single machine protocol.  To the best of my knowledge, the derived result is the first statistical result for Distributed Gradient Descent in the context of multi-agent decentralized non-parametric least-squares regression, making an important contribution for understanding the generalization properties of decentralized distributed gradient descent. Overall, I find the results are very interesting. The novel key step of the proof is the estimation of the network error. I did a high-level check of this proof step and I did not find anything wrong. The presentation is fine, although I believe it can be further be improved (for example, making more concise in some of the statements).     Minor comments: Line 145 of Page 4, the marginal distribution on X… The presentation in the supplementary material could be further improved.  For example, the notation $T_{\rho}$ is not introduced in Line 93 on Page 3. In the appendix, the proof involves many notations. But it seems to me these notations are necessary.  Could you derive similar results for the non-attainable cases?   --------After Rebuttal---------------- I have read the rebuttal. I am happy with the acceptance of the paper.