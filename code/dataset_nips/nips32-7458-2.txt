The authors propose a new method for sampling exploration goals when performing goal-conditioned RL with hindsight experience replay. The authors propose a lower bound that depends on some Lipschitz property of the goal-conditioned value function with respect to the distance between the goals and states. The authors demonstrate that across various Fetch-robot tasks, their method, when combined with EBP (a method for relabeling goals), outperforms HER. The authors also perform various ablations that show their method is relatively insensitive to hyperparameter values.  Overall, the empirical results are solid, but the math behind the paper is rather troubling. Specifically, the theorem seem rather vacuous: Writing “x” in place of “(s, g)”, the theorem basically says that if V(x1) >= V(x2) + d(x1, x2), then if you take the expectation of both sides (w.r.t. any coupling over x1 and x2), the inequality still holds. Taking the minimum overall couplings gives the theorem. Reading the (brief) proof only makes me more confident that the theorem is not insightful. Math aside, the intuition behind the method is clear (especially with Figure 2). The sensitivity analysis and comparisons seem appropriate, though not particularly amazing.  Some smaller comments:  The first paragraph of the introduction seems rather generic. It would be good to quickly focus on the problems the authors are trying to solve (goal-directed exploration) more quickly.  I do wonder how important it is to solve the bipartite matching problem exactly. For example, could the authors have instead sampled 100 different trajectories and taken the max over all time steps and trajectories?  The related works could discuss more work such as [1-7], though some of the work is extremely recent and I do not penalize the authors for not including them.  [1] S. Forestier, Y. Mollard, and P.-Y. Oudeyer. Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning.   [2] A. Péré, S. Forestier, O. Sigaud, and P.-Y. Oudeyer. Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration.   [3] C. Colas, et. al. GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms  [4]  V. Pong, et. al. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning  [5] V. Veeriah. J Oh., and S. Singh. Many-Goals Reinforcement Learning.  [6] R. Zhao, et. al. Maximum Entropy-Regularized Multi-Goal Reinforcement Learning  [7] Kaelbling, Leslie P. Learning to achieve goals.  -- After reading the author feedback, I still find the mathematical results rather weak. Again, the theorem simply says "if the triangle inequality is true for all states, then it is true in expectation, no matter what the (coupling) distribution is. There's no clear explanation for why this assumption would be reasonable, and the only justification provided is that prior related work made this assumption implicitly. Personally, I believe that that it is a reasonable assumption in many applications, and the paper would be strengthened by explicitly discussing this, rather than leaving it to the reader. I'm increasing my score in spite of the distracting mathematical discussion, primarily due to the strong empirical results (performance and visualizations).