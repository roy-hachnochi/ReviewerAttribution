Post rebuttal update: I appreciate the additional explanation for need of overshooting in empirical methods, and the clarity of response regarding stochastic models. The issue I took was with Sec 2.2, that next-step prediction is insufficient to produce belief states, which is only an issue with approximation error when dealing with empirical results. This is not clearly explained in the paper, but clarified much more nicely in the rebuttal. This would cause me to raise my score from a 3 to a 4 for the misunderstanding, but I still do not find this paper worthy of acceptance.  I don't think they are particularly surprising insights, and it seems the sole merit of this paper is an empirical one, and impressive because of performance on complex tasks.Â   In that case, I strongly believe that those environments and their method should be open-sourced. Getting image-based, model-based RL working is not a trivial task -- there are many tricks in the Planet paper to get their final performance, and their results are completely non-reproducible without them, and their code is open sourced. This paper is not useful to practitioners and other researchers without seeing those insights and being able to build on top of their results.   -------------------------------------- While the authors present a detailed list of related work in model-based reinforcement learning, it is unclear what is novel in this work, and what the message is. They experiment with various tricks introduced in prior works like self-supervised losses, overshooting, and memory architectures and report on performance results in a handful of environments.  Originality: There are new environments presented, but using existing techniques to perform a survey over combinations of techniques already shown in other work. Quality: The submission does not provide theoretical justification for their claims, but have significant results showing spread in performance across a variety of loss functions, overshoot lengths, and architectures. Clarity: The introduction is clear in the hypothesis being presented, but the experimental results  Significance: This paper has low significance, as it is testing a hypothesis that stochastic generative models benefit more from overshooting than deterministic ones, while failing to theoretically disprove that next-step predictive models are insufficient for learning belief states. There are only empirical results presented showing that overshooting increases performance, but this does not necessarily mean that next step is insufficient for forming belief states, as increasing the overshoot length also increases the supervision and number of labels being used.