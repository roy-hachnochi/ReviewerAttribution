This paper provides an interesting comparison between different methodological approaches and nicely addresses some interesting theoretical issues associated with model conditioning for  POMDP.  Many of the technical details are original and the presented work seems theoretically sound.  However, the clarity of the paper suffers from the amount of material covered and the whole thing could be better organised. The description of the contribution seemed to be relatively fluid throughout the paper.  However after some re-organisation of the document I think this will makes a solid contribution to the community. Below I highlight specific issues and points of clarification.  In the abstract, the last line, the authors say “In practice, using a expressive generative model in RL is computationally expensive and propose a scheme to reduce this computational burden allowing us to build agents that competitive model three baselines”.  It is not clear where this demonstrated, please clarify.   The authors  confront the fact they do no consider planning algorithms and  a summary of the work is “how good is the belief state at enabling standard RL algorithms”. This seems fine, as the focus is on the beliefs formed by the model, not action per se. However, the comparison to [20] is important because the focus on  decoding from the belief state  is covered in [20].Could the authors say a little more about the works relationship to [20]  A belief-state is defined as the sufficient statistics of future states. This seems ambiguous and potentially misleading. Is it necessarily over future states, is it the sufficient statistics of the state? Is it not a probability distribution over world states? They cite [9,10] saying that a belief state is a vector representation that is sufficient to predict future observations. Again, the original statement potentially misleading.   The difference between belief-state and state, i.e. SimCore starts with a belief state and then predicts a state at some future time. In the table, this is cleared up somewhat, where they say that state initialization is done such that s = b. But why the difference?   Some intuition as to why ConvDRAW + GECO solves conditioning would be nice.    I guess the fact that a map can reconstructed form the LSTM state is not hugely surprising. The authors note that contrastive loss is poor at mapping but good a localisation. I think theme emerging here is between local and global predictions. Could the authors comment on this.   Agent trained using IMPALA, a policy gradient method. This is, presumably, for the agent core, not the simulation core. Authors state running speed decreased 20-40% compared to an agent without a model - why would this be the case? Surely the model is more of a computational burden.  It seems pretty obvious why map construction should fail when there is a task because the environment is only partially sampled. I think this good example why building a map per se is not really helpful for RL in general. I really think the paper would benefit by discussing these ideas in the context of mode-based reinforcement learning.   The voxel environment tin Section 4.3 is interesting but there is little information about this and it is not clear what to conclude. There is also no consideration of where this work sits in the literature. 