1. I found it hard to understand what the "finetuning" method referred in this paper is. L217 explains it to be a method that fine-tunes the weights of the network. Is it the same as the proposed approach (same objective function) but instead of using the weight transform kernels (L130), the original parameters of the supervised network are updated during the unsupervised adaptation process? This is an important baseline for the paper because it shows the importance of using - a) the weight transform kernel; b) keeping the parameters of the landmark model itself fixed. Please clarify this in the author response.  2. What is the performance of the method on the original MPII dataset after the unsupervised domain adaptation process? The authors allude to catastrophic forgetting being a motivation to use the weight transform kernels, but do not show if it actually helps. For such an experiment, it is important to show what happens if a) the weight transform kernels are used on MPII; b) the weight transform kernels are set to identity on MPII.  3. I find the analysis in this paper a bit lacking. For example, rather than the extremes - use the weight transform kernels OR finetune all weights (assuming my understanding in 1 is correct), the authors do not show any approach in between. How about a model that optimizes the same loss as in L170, but does not use the weight transform kernel and only finetunes the last few layers? Although this approach may have catastrophic forgetting, it is a good baseline to demonstrate why the particular design decisions in the paper matter.  4. What are the training hyperparameters for the scratch network? Is it trained for longer than the other methods? With enough data augmentation (example affine transforms of images)?