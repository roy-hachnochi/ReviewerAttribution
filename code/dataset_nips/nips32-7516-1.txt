Originality: To the best of my knowledge, adversarial manipulation of explanations was (foreshadowed by previous research but) new.  The constrained optimization method introduced to generate adversarial manipulations follows closely the ones used for generating class-adversarial examples, but it is also new.  The theoretical analysis is very closely related to ideas of Ghorbani et al. 2017, but it goes quite beyond it.  Quality: The paper is of good quality.  Its best quality is the intuition that fragility of explanations can be effectively exploited.  This is an important message.  The methodological contribution is somewhat simple and limited (e.g. the manipulation algorithm only applies to differentiable explainers like input gradients).  The theoretical analysis does make up for it.  The length of the related work is somewhat surprising, considering the amount of recent work on explainability.  Clarity: All ideas are presented clearly.  Significance: This paper showcases an important phenomenon;  I am confident that it will receive a lot of attention, as it highlights an important phenomenon at the interface between adversarial attacks and explainability.