Update: I've revised the overall score based on the response from the authors.  This is another step to better understandings to distributed SGD approach for deep learning model training. However, the key limitation is on the strong assumptions over the model, which makes the results not meaningful to any practice.  Q1 in Section 2.3 strongly claims that the mapping from weights w to the loss function is a quadratic function. This will never happen in real world.  Moreover, batch normalization is commonly applied in real world deep learning tasks. Obviously, the convergence results do not consider it, although it is out of scope of a theoretical study in this submission.