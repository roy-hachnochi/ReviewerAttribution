EDIT after rebuttal: The paper proposes a new feedback model which is new. However, the algorithmic idea and analysis is not new. This is probably the weakest point of the paper. So it boils down to if the question is itself useful (e.g., has useful applications).   I agree that 7 is a generous score. My primary reason was that this seemed like an interesting direction: to understand how much can the full-feedback be relaxed while getting the same bounds. The stated motivations in the rebuttal are pretty weak; in fact, even if feasible I would not release the number of agents traveling a route due to privacy issues (e.g., if a route is sensitive and telling if > 0 agents choosing a route gives away too much information). On the other hand, I thought that robust bayesian optimization, in the paper, seemed like a reasonable one.  --- This paper considers the no-regret learning dynamics in multi-player game. They consider the setting where one player is using a no-regret algorithm that receives a noisy reward for the chosen action and the actions chosen by the other players (but not their rewards). They further assume a regularity assumption on the rewards that is required by their solution which uses gaussian process (GP) to estimate UCB on the rewards of the other players. The key result is to illustrate that under this restrictive feedback, in many applications they are able to get as fast a convergence rate as receiving full feedback and always faster than bandit feedback. They argue that this is a natural feedback that is received in many applications and full-feedback is restrictive for applications. The propose an algorithm, prove regret bounds and evaluate this on simulated and real-world transportation dataset.  My comments for this paper as follows. I think the feedback model they consider and the goal of the paper is novel. It is interesting to understand how much feedback one really needs to get fast bounds and the trade-off between feedback and convergence rate is a research worth pursuing. The algorithm seems to be a rather simple modification of the usual exponential weights approach after appropriately estimating the rewards of the other players from their actions using standard tools from GP. Thus, the algorithm and hence the analysis seems to be a straight forward marriage between the techniques in these two areas. But, they provide *extensive* experimental evaluation on simulated and real-world transportation datasets. They also show an application in Bayesian optimization previously considered in prior work and compare their algorithms with that in prior work. The lack of significant novelty in theory is made up by the extensive experimental evaluation. As a bonus, this paper is very well-written and easy to read. Weighing the pros and cons, I would lean towards acceptance of this paper.  I had a question to the authors. In the bandit setting and 2-player games, when both players play no-regret algorithms faster convergence rates can be obtained (for instance [1]). Is something similar possible for this model of feedback?   In summary, the pros and cons are as follows:  Pros: - Novel feedback model - Extensive experimental evaluation - Paper extremely well-written  Cons: - Algorithm and analysis is rather straightforward once the problem is correctly setup.  [1] - Fast convergence of regularized learning in games - Syrgkanis, Agarwal, Luo, Schapire, NIPS 2015. 