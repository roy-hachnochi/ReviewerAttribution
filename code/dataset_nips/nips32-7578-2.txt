The paper presents a simple fully differentiable discrete latent variable model for content planning and surface realization for sentence-level paraphrase generation. The discrete latent variables are grounded to the BOW from the target sentences bringing semantic interpretability to the latent model.    The paper is very well written and the proposed approach is thoroughly evaluated on the sentence-level paragraph generation with both quantitative and qualitative analysis.   One of my main concern with the approach is its evaluation on the sentence-level paragraph generation. The need for content planning for sentence-level paragraph generation is rather limited, often we aim to generate the target sentence which is semantically equivalent to the input sentence, there is no need for content selection or content reordering during content planning. It is no surprise that the simple model just as the BOW model is good enough here. The decoder  simply takes an average representation of the bag of the words and generates the target sentence. I am afraid that the presented model will not influence or be useful for more complex text generation tasks such as document summarization or data-to-text generation. In these tasks, they often require more sophisticated content selection and content reordering during content planning.   What are the state of the art results on the Quora and Mscoco datasets? It is not surprising that the Seq2seq-Attn model seems to be doing well as well. The attention scores also learns a semantic interpretation of the information that is used during decoding.  It would have been interesting to see the outputs of the Seq2seq-Attn model along with the LBOW models to understand how is the unsupervised neighbour words learning useful.