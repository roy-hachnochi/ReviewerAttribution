Content: The authors’ introduce a framework for practical DP computation using a secure environment with limited memory and a notion they call “oblivious differential privacy”. They give several algorithms for computation in this framework and show that they can achieve accuracy guarantees close to CDP, but have a trust structure that is more similar to LDP. This is an important area of research as companies grow increasingly wary of holding sensitive user data in the clear. This paper makes some interesting conceptual contributions to this discussion. The statistical tasks they present algorithms for are very common and useful tasks. These tasks are also examples of problems where there is a significant gap in performance between LDP and CDP algorithms.  Originality:  As far as I am aware, ODP is a new concept, although the authors’ mention that it has appeared in concurrent work. The use of oblivious shuffling to hide dummy and fake records in the histogram algorithm is interesting and the proof is non-trivial.  The most interesting aspect of the definition of ODP to me is that it is actually a weaker condition than the algorithm being data oblivious. However, the authors’ don’t really seem to take advantage of this and most of the algorithms rely on heavy machinery from the oblivious computations and streaming algorithms literature. The histogram and heavy hitters algorithms use oblivious shuffling, which I understand (in my limited understanding of oblivious algorithms) to have more overhead than non-oblivious shuffling. Can you get simpler algorithms by leveraging the fact that we don’t require total obliviousness?  The other idea that I haven’t seen much in the literature is performing DP mechanisms with limited memory. This is an interesting direction although again in this paper, it seems to be reduced to using existing streaming algorithms.  Quality: The computations and proofs of the main theorems appear correct. All the definitions are sensible and present interesting ideas. Any assumptions made seems reasonable.   I would like further clarification on how much overhead is required to set up the secure environment. Also, the encryption/decryption step appears to be ignored when discussing the run-time of the oblivious algorithms in the secure environment? How does the overhead in this setting compare to that of the “analyses, shuffle and analyse” framework?  Clarity: The paper is well-written. The motivation and main results are clear.   The diagram in Figure 1 (left) seems incomplete. From my understanding of the set-up, the algorithm can also access encrypted data stored outside the trusted execution environment, provided it is accessed in an oblivious way? This is an important distinction since data stored inside the secure environment does not need to be accessed in an oblivious manner, hence the use of streaming algorithms.  Significance: This work explores bridging the gap between the local DP and central DP. It attempts to simulate the trust model of LDP using a secure computation environment, but also provide accuracy guarantees that are similar to CDP. This is an important area of research that has started to receive significant attention in the literature. The trust model of LDP can be more appealing not only to users, by also to data aggregators as it protects them from unintentional data leakages. However, for many tasks there is a significant gap between accuracy bounds for LDP and CDP algorithms. From a legal perspective, TEEs may protect companies from being liable for protecting users data.  I think this work explores two exciting directions for DP research:  1. The idea of weakening definitions like obliviousness (and, as has occurred elsewhere in the literature, secure multi-party computation) to allow DP leakage is an interesting idea that has a lot of potential. I can see the definition of ODP in this paper propelling a lot of interesting research.  2. The idea of performing DP computations in a secure environment with limited memory. Unlike in the typical streaming setting, this allows for the potential of storing DP statistics outside of the secure environment to be used later. I am curious whether, and to what degree, this additional freedom allows one to improve the accuracy of streaming algorithms for DP. I can see this idea sparking interest in members of the community.  Minor comments:  - When surveying previous works, the authors’ mention recent work on the “analyses, shuffle and analyse” framework. They mention that this work can be seen as only shifting the trust boundary since the user has to trust that the anonymisation primitives are effective. The authors’ allude several times to how the secure computation environment has more safeguards, e.g. they discuss that the users can verify that they are communicating with a specific piece of software. However, I would be interested to hear more about how trusting an anonymisation primitive is different to trusting a secure environment? I don’t see this as a failing of either technique, but would be interested in hearing more about the differences between the trust structures.  