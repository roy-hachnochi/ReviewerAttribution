I found this sentence convincing "replay needs re-training which requires memory" Iterative pruning is a big overhead after learning each task  The authors stated that "Without loss of generality, our work follows a task-based sequential learning setup" It is a standard setup but it has many limitations, not being applied when tasks are not known at test time for example. How could this not limit the generality of the approach? Why gradual pruning, there are many methods for compression. Choice is not discussed extensively. The main components of the methods are: 1- Compression, 2- piggyback(a previous method) and training of released weight. 3-If not enough (no explanation of what this means), add nodes or filter. How is this happening??? 4- The algorithm is a paragraph of text.  5-Comparison to HAT is missing, an icml18 paper that masks neurons instead of parameters.  "Overcoming Catastrophic Forgetting with Hard Attention to the Task" No explanation on how to behave at test time. Are  weights only associated with the test task  activated? How does this limit the method?   