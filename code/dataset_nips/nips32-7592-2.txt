The paper starts out with an attempt to address the difficulty of evaluating a conversational model that is supposed to be able to produce an interesting dialog (with human) in an open-domain chat.  This is indeed a challenge for the research on automated dialog systems.  However the paper soon goes into the addition of an EI regularization layer, which is confusing and somewhat distracts from the main focus on the evaluation system.    In the experiments it is shown that using five conventional metrics with humans doing the scoring, the proposed EI regularization can improve almost each of the 3 known models; yet between the 3 models the ANOVA results do not show significant differences.  Then using the hybrid metric and self-play automation, it is shown that the addition of the EI layer is favorable over the baseline models, and the results correlate well with human judgment using the conventional metrics on interactive sessions.  One critical discussion that is missing is in what specific ways self-play dialog differs from interactions with a real user.  Does self-play assume that the same persona is talking on both sides?   How can one accurately represent the persistent but potentially very different personas represented by real users?  The paper also seems to struggle between two main proposals:  one on the addition of the EI regularization layer, and the other on advocating self-play automated multi-turn evaluation and a hybrid metric.   If the EI proposal is meant to be the main contribution, the arguments about the evaluation metrics and self-play set up are problematic,  because you are showing that with  your proposed evaluation, your proposed modification is better than the baseline.  There is a question on the fairness of the evaluation choices.   On the other hand, if the evaluation system is meant to be the main proposal,  then the choice of the EI regularization could be an unfair demonstration of the effectiveness of the evaluation system.  How are you sure that your proposed evaluation scheme will not contradict human judgment when a different set of alternatives are evaluated?  Either proposal provides some useful but rather incremental contribution to the state of the art in dialog modeling.  Given that, the paper seems to be a better fit for an audience more specialized in dialog modeling.  It is unclear how the broader audience in NeurIPS can be benefited from the work.