## A New distribution on the Simplex with Auto-Encoding Applications  ## Review after author rebuttal  The authors significantly strengthen the paper by addressing the comments of the reviewers. In particular, they extended the experimental section (which I was particularly concerned about) by adding all the baselines that I suggested. IRG is the most competitive baseline which seems to have the same performance. The authors argue that the implementation of the proposed method is simpler that IRG. On the other hand IRG is more general (it is not only about the dirichlet distribution). Based on this, I am increasing my score to 6.  ## Summary  The authors propose a new distribution over the simplex amenable to the re-parameterization trick. To so so, they resort stick-breaking construction sampling the sticks from i.i.d. kumaraswamy distributions. To avoid the influence of the order in which the stick are sampled, they propose to integrate over all possible orders and they resort to MC to estimate this integral. In the experiments, they apply the proposed distribution to approximate the posterior over the labels of a semi-supervised conditional VAE. As baselines they use the original proposal that does not use a prior and using a gaussian-softmax prior. However, the results are incomplete and they fail to compare to other more recent proposals (Gamma-SB-VAE, Kumar-SB-VAE, general reparameterization trick, ...).  ### Details  The main idea of the authors is to used an stick-breaking construction sampling the sticks from i.i.d. kumaraswamy distributions (and idea already published a few years ago) and get rid off the influence of the order in which the sticks are sampled by integrating over all possible orders. This integration is intractable, so they approximate it using plain montecarlo estimates. This has been applied in other contexts before but I believe the application to symmetrize the kumaraswamy-stick-breaking distribution has not done before to the extend of my knowledge. However, it seems somehow straightforward. The results are show in the first part of the paper, however, even thought it is technically correct it seems to me that claimming a "new distribution" it is maybe an oversell.  Nevertheless, the main weakness of the paper is when the try to prove the superiority of this distribution when applied as a prior of a model. The authors choose the semi-supervised conditional autoencoder originally proposed in [1]. In the original proposal, Kingma et al do not use a prior over the conditioning categorical variables. In this paper, the authors propose  to use the symetrized-kumaraswamy-stick-breaking and they show that it improves the performance. The also compare to a gaussian-softmax prior for which it is know that cannot model multi-modal distributions and again the symetrized-kumaraswamy-stick-breaking slightly outperforms this prior. However, the authors do not compare to the most obvious baseline, the kumaraswamy-stick-breaking already proposed in [2]. This baseline is needed to see the actual contribution of the paper which is the approximate integration over all possible orders, not the kumaraswamy-stick-breaking contruction that have been already used in several papers in the literature. Also, in [2] they use a gamma-stick-breaking construction based on an approximation of the inverse CDF.  Finally, there have been some interesting advances that extends the re-parameterization trick to the beta/gamma distribution [3, 4] that are also missing in the experimental section.  Overall, the theoretical contribution is not novel enough and the experimental section is far from complete. It could be a candidate for a workshop paper but it falls below the novelty and quality neurips bar.  ### Minors  * Why the results table seems incomplete? * Why using a beta in the KL term when you could used the symetrized-kumaraswamy-stick-breaking as well?  ### References  [1] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems 27, pages 3581–3589. Curran Associates, Inc., 2014. [2] Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. International Conference on Learning Representations (ICLR), Apr 2017. [3] Francisco R Ruiz, Michalis Titsias RC AUEB, and David Blei. The generalized reparameteriza- tion gradient. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 460–468. Curran Associates, Inc., 2016. [4] Figurnov, Michael, Mohamed, Shakir, and Mnih, Andriy. Implicit  reparameterization  gradients. Neurips, 2018 