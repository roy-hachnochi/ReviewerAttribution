This work proposes a self-supervised learning framework called the Neural Conditioner (NC) which attempts to model all possible conditional distributions P(X_r|X_a) where X_r and X_a denote partial observations (i.e., of a subset of variables) of a random vector X. The NC is learned using adversarial training where a generator estimates the subset of variables to be recovered X_r given the available subset of variables X_a, and a discriminator tries to discriminate between the estimates and true observations. A trained NC is shown to be capable of sampling from conditional distributions that were unseen at training time, including the joint distribution, and also learns features that are useful for downstream tasks.   While the idea of modeling the conditional distribution X_r|X_a has been explored in prior work (Eg: VAEAC), this work differs by (i) using adversarial training, which enables generating more realistic output and (ii) showing that the model is useful for many different tasks such as estimating conditionals (including unseen conditional distributions) ad producing features useful for downstream tasks.   The paper is well written in general. Ideas are well described, and modeling choices look reasonable. I appreciate the authors providing a lot of intuition about the different components of the approach, performing experiments on a toy setup where things are interpretable, and showing comprehensive results on different tasks as well comparing against a strong baseline from prior work (VAEAC).  