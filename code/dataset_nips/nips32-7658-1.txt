-- I enjoyed reading this paper, it is well-written, and tackles an important problem.   -- My main concern is around the overheads added by the proposed algorithms. First of all, it wasn't clear from reading the paper if these algorithms need to be run for each inference query. Please clarify. If so, even though the complexity analysis is given for different components, it will help if this was empirically shown too. For instance, for each of the datasets, and inference tasks, what was the breakdown of the time spent in choosing a configuration, and actually serving a prediction for these tasks?   -- The key motivation of the proposed work is for the edge computing use cases, which are as noted in the paper, latency and privacy sensitive. Thus, the overhead analysis becomes even more important.   -- The paper has mainly used accuracy as the performance metric, and mentioned that it could be any other application-specific performance metric. However, it isn't clear how. Applications may have their own metrics, and they could vary over time as the paper points out too. Hence, it would definitely strengthen the practical aspects of the approach to show this aspect more clearly.     -- nits:  --- What is column 2 in Table 2? The heading says "64-bits". Is this the actual cost computed for 64-bits for each of the operations? Please consider renaming the header for this column if so.  --- Fig. 3 is microscopic :). Please consider devoting more space.    === UPDATE After Reading The Author Response === I would like to thank the authors for a great response, it clarified and satisfied my questions about the overheads and performance metrics. I am happy to maintain my current score of 7.   