Contribution 1:  The main contribution here in my opinion is the conceptual connection between polynomial optimization and adversarial attacks of PTFs, and showing that finding adversarial examples can be used to robustly learn these PTFs via a convex program. The actual algorithmic result is not super interesting as it only holds for degree-1 and degree-2 PTFs---and learning degree-1 PTFs (in other words just a linear classifier) robustly is easy as it is just a max-margin problem.  Contribution 2:  I find the lower bounds to be more interesting that the above upper bounds. As also mentioned by the authors, previous hardness results by Bubeck et al. were for a worst-case cryptographic instance, whereas the bounds in the paper are for a much more natural problem of learning a simple PTF. The only drawback here is that the result only holds for proper learning, whereas the result in Bubeck et al. held for any learner, but this is still a good contribution.  Contribution 3:  The experiments show that the SDP formulation to find attacks on neural networks could be a promising direction of future work. The main bottleneck here is the computational cost. Maybe using non-convex methods such as Burer-Monteiro could help?  Clarity:  The paper is extremely well-written (even the parts I read in the supplementary read very well).    ------Post author response-------  I thank the authors for their detailed response. The response seems adequate, hence I am keeping my previous score of 7. As suggested by Reviewer 1, the authors should clarify the definition of adversarial examples in the next revision of the paper.  I have one small suggestion about the 2-layer neural network results which could help clarify its significance to the readers. The experimental result here could have potential given that it seems to succeed where PGD fails, though the main concern here is the computational cost. It would be good if the authors also compared with other SMT/MIP based methods for exact verification, as these usually do better than PGD but are also slower.