Summary of Contributions:  This work considers the problem of adversarial robustness, where the goal of the learner is to minimize adversarial or robust loss (as defined in Madry et al. 2017, the population risk modified to account for worst-case attacks in a L_infty ball of radius delta around the distribution).  The focus here is on provably efficient robustness, and to this end the authors introduce a notion of approximately robust learning, where an algorithm gamma-approximately robustly learns the class if the robust loss is at most epsilon for radius delta/gamma, when given a labeled sample of size polynomial in 1/eps and the VC dimension of the class.  The definition assumes that the sample is labeled by a function in the class and that function is gamma-robust with error 0.  Results given for this framework: It is computationally hard to approximately robustly learn the class of delta-robust realizable degree-2 PTFs with gamma = O(log^c n) for some constant c. (It is also NP-hard for gamma=1 for even constant eps.)  Degree-2 PTFs can be approximately robustly learned in polynomial time with gamma=O(sqrt{log n}) and halfspaces with gamma=1. Polynomial time algorithms for finding an adversarial example with l_infty distance at most delta * O(sqrt{log n}) when one exists at distance delta, or proof that no such example exists.  An algorithm for finding adversarial examples for 2-layer neural networks (with some additional constraint on the last layer of the network or on the resulting SDP), with gamma~sqrt{log n} delta / m or a certifying that no such example exists with l_infty ball of radius delta around x and has margin > m from the decision boundary.  Experiments for 2-layer neural networks using the algorithm from 4 on a network trained for MNIST.  The experiments suggest that the assumption underlying 4 may hold in practice and the performance of the attack is compared to PGD from Madry et al. In this particular experiment, the attack seems to have advantage over PGD both when delta=.3 and .01, although it is computationally more expensive.             Strengths/Weaknesses The impact of computational efficiency on the tasks of adversarially robust training and adversarial attacks is increasingly important and not yet fully understood; this paper proposes a connection to polynomial optimization and a model for understanding this impact. While it shows several non-trivial results in this model, due to the definition of the model, I’m not sure I understand the implications to the original problem, especially regarding the results for deg-2 PTFs given here.  One limiting aspect of the definition (mostly for the lower bound) is that the learner should also output a deg-2 PTF.  For the upper bounds, I’m not sure I understand the requirement that the target PTF must have robust error =0.  Please correct me if I’ve misunderstood, but it seems like when learning LTFs over {0,1}^n, the learning algorithm need not learn decision lists, since there would be no deg-1 LTF with robust error=0 for even small values of delta. If this is not an unnatural requirement (or if this isn’t actually a requirement for the definition), perhaps the authors can clarify why.  When coupled with the fully realizable/proper requirement, Deg-2 LTFs feels like a fairly restrictive class that may be of less interest to the adversarial attack community.  The lower bound has an additional restriction on the behavior of learning algorithm (see last paragraph below).   The results, especially the lower bound, are still interesting from a theoretical point of view (and we don’t expect to achieve computational hardness without some restriction on the hypothesis class or cryptographic assumption). The results and experiments for 2-layer networks should be of broader interest, but I feel unable to judge how reasonable the required assumptions are (also, there is no robustness result for these networks) or how meaningful the experimental results are.     Overall, the quality and clarity of the work are good.  I found the clarity in section 6 could easily be improved, for example it references a problem (9) that is not shown in the section.  Basically this section is unreadable without referring to the supplementary material.  Regarding Theorem 5.1, Remark 5.2, I didn’t see that this is a requirement from the statement? Unless I misunderstand, you should state this requirement in the Theorem statement, as it is a limitation of the result.  It also is not mentioned in the supplementary material, and seems relevant to the discussion justifying corollary G.2.  To summarize, I think this is interesting theoretical work with some limitations -- the results for 2-layer networks may make a significant difference in this regard, but I am not in a position to assess this.    