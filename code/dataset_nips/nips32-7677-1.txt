In this paper, the authors derive a mean-field theory for the analysis of GAN training when the learning rate is not necessarily small. They show that the training dynamics of a large network is described by a smaller set of parameters that scale as the number of hidden features. The dynamics of the order parameter obey a collection of ODEs while the microscopic parameters follow stochastic dynamics. The source of the noise in the dynamics is the noise in the data (here modeled as a spiked covariance matrix) and the generative model. They provide detailed analysis and simulation of a simple model with two hidden features and a simple quadratic loss function.  Analysis of the temporal evolution of the order parameters reveals several dynamical phases. In one phase the weights of the generator converge to high overlap with that of the data model, signifying a success of feature retrieval by the generator. In another dynamical phase, the weights of the student oscillate around the correct values without ever converging. In the last phase, the system exhibits mode collapsing, where the generator finds only some of the features. They find that the bifurcating parameters between these solutions are the total amount of noise — the mean level of noise in the system, given by the mean of the noise in the data, and the noise level used for the generator.   Interestingly, the macroscopic dynamics of the order parameters reveals an interesting hierarchical interaction when one feature is learned after the other. I think the authors should have focused more on this result, and perhaps compare it to known analysis of simple GAN (I do not know any, but either this is an exciting and new result or the authors should compare it with previous findings).  However, I also have some criticism: - The authors introduce the macroscopic overlap matrix M, which aggregates all the order parameters into one matrix, but it makes it unclear. Different indices range of these matrix means different things, which make it hard to follow. - The result that the macroscopic dynamic is deterministic and accurate at the asymptotic regime with scaling 1/sqrt{N}, and that the microscopic dynamic is stochastic seem trivial to me. These are the starting point for any mean-field theory. While indeed these assumptions can, and often should be proven rigorously, the underlying microscopic noise, and the self-averaging of the teacher and student weights in the model is enough to support this claim.  To conclude, I think this paper shows interesting findings on the training dynamics of GANs. I suggest that the authors focus more on the result they gain by analyzing the dynamics of the order parameters, and discuss further its meaning and applications. The further discussion can replace comparison between the deterministic macroscopic dynamics and the stochastic microscopic one — which seem trivial.  ** update ** Despite the authors' response, I still find the use of the concatenated matrix M confusing. Nevertheless, it may be the better choice of due to the short format limitation. Either way, I still find this work interesting and believe it should be published. I am also satisfied with the authors' response to my comments about the emphasis on the stochastic microscopic dynamics. I agree that given previous works on a statistical analysis of GANs, which I am not well familiar with, it is a point important to make.  I would suggest the authors add a couple of sentences explaining and emphasizing this, for the like of me that find their statements trivial.