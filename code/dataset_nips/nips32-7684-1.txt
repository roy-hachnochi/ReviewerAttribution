Summary: The paper introduces a new method for causal effect estimation by exploiting social network structure to capture possible confounding aspects between treatment and outcome, which can subsequently be adjusted for. It translates a social network into an embedding that can be used as a proxy variables for the actual confounders. It is based on the assumption that closely linked friends in a network are likely to be similar. Performance is evaluated on partially simulated real-world data on various data sets.  The subject is interesting and difficult: many tech giants are eager to exploit and extract all kinds of new information from social networks, and this application could offer a very interesting opportunity. However, despite this intriguing backdrop, the paper itself does not manage to convince at all points.  The main rationale for using the (social) network is that ‘similar people are likely to be friends’ (l..21). This makes intuitive sense, however it does not imply that people that are far apart in a social network are likely to be very dissimilar. As the resulting similarity measures from the network embeddings are subsequently used as proxy for the actual confounding variables, it would be helpful to provide a slightly more in-depth example + explanation of how effective this proxy actually captures the key confounding aspects of the unobserved variables. At the moment we are required to trust in the ‘black box’ (l.136) … which is ok, but I still would like a bit more reassurance to start from. Unfortunately the current evaluations provided in section 6 are not enough due to the lack of a reliable ground-truth.  I applaud the authors for making the assumptions behind their approach very specific (as every principled causal method should do). However, this also lays bare some of the potential weak points behind it.  The translation of the network into a predictive embedding that serves as a good proxy for use in a causal inference procedure is interesting and promising. However, then asking a practitioner to assess the plausibility of this predictive embedded model is not realistic in practice. For a generating model s/he will at least have some notion of possible mechanisms to give a good assessment … but for such an abstract embedded model this is almost impossible unless s/he has a huge amount of experience in judging such models, as already acknowledged by the authors (l.209).  However, a more important issue is that the current assumptions are either not complete or do not seem to guarantee validity. In particular: if the observed joint density of a straightforward model X <- Z -> (T -> Y), (i.e. X good proxy of confounder Z of causal link from T to Y) follows a multivariate Gaussian distribution, then the system satisfies the assumptions, and adjusting on Z (or X) will indeed lead to a good approximation of the causal effect of T on Y. However, there ALSO exists a linear Gaussian model T -> (Z -> Y) + Z -> X, (i.e. X good proxy of partial mediator Z of causal link between T and Y) that matches the exact same distribution, and for that model/system adjusting on Z or X would clearly lead to a wrong causal prediction.  Essentially you require that X and Z are not causally affected by treatment T. But this is an assumption on the underlying model: if satisfied then assumption 1 automatically follows, but not necessarily the other way around.  Furthermore, you start from X as a noisy proxy of the actual confounder Z, but then as stated in l.201, Assumption 2 essentially states that X captures ‘whatever information is relevant to the prediction of the treatment and outcome’. That does not correspond to a noisy observation, but essentially still relies on obtaining full information on the effect of the confounding variable. I was hoping for a bound on the accuracy of the estimated causal effect in relation to the ‘closeness’ of the proxy variable X to the actual confounder Z.   The hope expressed in l.210-213 only holds if both observed/proxy and unobserved/exogenous confounding have the same sign and the second is not stronger than the first, otherwise the unadjusted estimate may actually be closer to the ground truth. (This also holds for the experiment behind Fig.1).  On a final note: I have also had the pleasure of reviewing the closely related ‘Using Text Embeddings for Causal Inference’ which conceptually seemed more interesting than this one.  In conclusion: the problem is interesting and highly relevant, and the approach is promising. However, there is some concern about the actual validity in section 5, and the experiments don’t seem sufficiently rigorous to fully capture a proper evaluation of efficacy of the method. Together with the strong overlap with the other paper mentioned above I find this the weaker one, and hence recommend reject.