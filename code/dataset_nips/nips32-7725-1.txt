The paper focuses on the bayesian optimisation with heavy tailed noise. The goal is to maximize a function f over an action space X in a bandit setting. When chosing the action x, the player observes f(x) + some heavy tailed noise. f has some regularity: it is bounded in some RKHS norm. The authors first present a simple algorithm which is far from being optimal. They give lower bounds for specific kernels: SE and Matern. They then present an algorithm with two different embedding strategies, their regret bounds and compare them empirically. The used methods are very interesting and the black box optimization is a significant problem. The theoretical results are nice. However, I have the general feeling that the authors tend to oversell their results: the algorithm does not seem to be adapted to continuous set of arms and does not seem to have a nice complexity in practice as well. I am not yet convinced of the improvement brought by this work, especially compared to the "bandits with heavy tail" paper. For these reasons, I give a weak reject score to the paper. ------------------------------------------------- Major comments: 1. You first consider the problem with a possibly continuous set of arms. However, the computation of your algorithm scales linearly with the cardinal of this set, meaning that your algorithm does only work for a finite set of arms. Besides losing a lot of interest in the original problem, this is the kind of oversold results I mentioned.  2. Let us now consider only finite set of actions. The bandits setting seems to be a more general formulation than your problem then: no regularity of f is assumed. In this case, you need to compare your results (theoretically and empirically) with what is obtained by the bandits setting. More generally, it is hard to place this paper with the related literature. I am especially eager to know the performance of the "bandits with heavy tail" algorithm in the experiments with real data.   3. The complexity of the algorithm (see page 7) is very large (besides being linear in the cardinality of X) and this would make your algorithm not very practical. Moreover, you claim some complexity if gamma << T. First, what does this mean ? Second, gamma can be a power of T (eg in the Matern kernel according to line 127), so this is an overstatement to me. Also, why isn't the per step space complexity of Nystrom embedding scaling with t (but only with m_t) ? We indeed need to store all x_t' for t'<t to sample the dictionary D_t at each time step. This would give both a space and time per step complexity scaling linearly with t (and with large other multiplicative factors) which I believe is not convenient in practice. This is not denied by the experiments which consider only short horizons (especially on real datasets).  4. The paper is very dense. We feel the authors struggled to make it through 8 pages. I think it would not be too bad to delay some parts to the Appendix: for example Section 3, the QFF embedding or even details of the experiments. In this way, the paper could be easier to understand.  ---------------------------------------------------- Minor comments: 5. page 1 line 5: "belongs to the RKHS induced by a kernel", this is odd. I would rather say "belongs to some RKHS,"  6. typo page 2 line 45: nonparametric (and) assumed ?  7. page 3 line 100: the introduction of the different notations is unclear/uncomplete. What is X_t for example ?  8. page 3 line 114 "we truncate more and more agressively". I would have said "less and less". Am I wrong or is this a typo ?  9. page 3 line 123: you write the constraint |A|=t but this should be |A|=T  10. The different upper bounds theorem are given with probability 1- \delta. Maybe you could then add in expectation (for a well chosen delta) what this gives. This would be useful, especially to compare with the lower bounds.  11.  page 7 line 269: the condition can be rewritten as d(d+1)/2 \nu < \alpha (easier). This seems quite a restrictive condition as well.  12. page 7 line 307: it would still have been interesting to see how QFF performs here.  13. page 8 line 345: I think it is oversold to claim you have solved BO optimally here. This is only the case with regularity with the SE kernel norm.  14. page 8 line 346: I do not really see where you demonstrated the failure of existing methods. You would at least need some empirical results to claim this.  ----------------------------------------- The authors answered to all my major concerns and I agree with their points. I thus decided to raise my score in consequence.