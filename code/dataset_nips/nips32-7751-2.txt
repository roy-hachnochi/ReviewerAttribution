The paper considers three ways to make neural networks be overparameterized and their corresponding landscape analysis, including unit replication, inactive units, and inactive propagation. Both ReLU and smooth activation functions are considered. By employing PAC-Bayesian theory, the paper shows that ReLU achieves better generalization compared with Tanh. The paper is interesting. However, I have the following concerns:  1. What is the definition of "semi-flatness"? It seems not be clearly defined in main context. What is the difference of "semi-flatness" and "flatness"?  2. Employing PAC-Bayesian theory to explain the benefits of flat minima is already shown in previous literature (for example, [1]). The authors may oversell their contribution on generlization error bounds (i.e. section 5.2).   [1] Neyshabur et al. Exploring generalization in deep learning. NeurIPS 2017.  3. In the paper, it seems to show that the method of unit replication method is not good since it introduces saddle points. In contrast, the method of inactive units is good since it gives the embedding semi-flat minima. How the number of added units (replicated or inactive) affects the landscape? For example, how is more inactivate (replicated) units related to the flatness (saddle point)? How does it guide through us to take advantage of specific ways of overparameterization?   =======POST REBUTTAL======== I have read the rebuttal and would like to keep the score.