Summary: The authors propose a novel HRL algorithm (named FEN) for training fair and efficient policies in MARL settings. They design a new type of reward that takes both efficiency and fairness into consideration. FEN contains one meta controller and a few sub-policies. The controller learns to optimize the fair and efficient reward while one sub-policy is trained to optimize external reward (from the environment) and other sub-policies provide diverse but fair behavior. They show their method learns both fair and efficient behavior at the same time and outperforms relevant baselines on 3 different tasks: job scheduling, the Matthew effect, and manufacturing plant. They also show that the agents achieve Pareto efficiency and fairness is guaranteed in infinite-horizon sequential decision making if all agents play optimal policies (that maximize their own fair=efficient reward).  Strengths:  - the paper is generally clear and well-structured - the proposed approach is novel as far as I know - I believe the authors address an important topic in multi-agent reinforcement learning: designing agents that are both efficient and fair at the same time. I think people in the community would be interested in this work and could build on top of it.   - Their method has some theoretical guarantees, which makes it quite appealing. It is also grounded in game theory aspects.  - The approach is thoroughly evaluated on 3 different tasks and shows significant gains in fairness without losing in overall performance.  - The authors do ablation studies to emphasize the importance of using a hierarchical model and also the effectiveness of the Gossip version of the model   Weaknesses:  - the paper is missing some references to other papers addressing fairness in MARL such as Hughes et al. 2018, Freire et al. 2019, and other related work on prosocial artificial agents such as Peysakhovich et al. 2018 etc. - the paper could benefit from comparisons against other baselines using fairness and prosocial ideas such as the ones proposed by Hughes et al. 2018 and Peysakhovich et al 2018  - I've find the use of "decentralized training" to not be entirely correct given that the agents need access to the utility of all (or at least some, in the Gossip version) agents in order to compute their own rewards. this is generally private information and, so I wouldn't consider it fully decentralized. while the Gossipy version of the model that only uses the utilities of neighboring agents helps to relax some of these constraints, the question of how these rewards can be obtains in a setting with truly independent learners remains. Please clarify if there is a misunderstanding on my part.   Other Comments: - it wasn't very clear to me from the paper what happens at test time. is it the same as during training in that the meta-controller picks one of the policies to act? - it would be interesting to look at the behavior of the controller for gaining a better understanding of when it decides to pick the sub-policy that maximizes external reward and when it picks the sub-policies that maximize the fairness reward.  - in particular, it would be valuable how the different types of policies are balanced and what factors influence the trade-off between the sub-policy maximizing external reward and those with diverse fair behavior (i.e. current reward, observation, training stage, environment etc.)