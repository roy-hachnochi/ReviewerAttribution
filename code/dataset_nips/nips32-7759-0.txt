== POST-REBUTTAL ==  Overall, I reckon that the authors have made a strong rebuttal. The contribution of the paper is now clearer in my mind and I reckon that both the theoretical result and experimental result of the paper look solid. As pointed out by other reviewers, I strongly encourage the authors to clarify the overall presentation based on our comments (highlight better the contributions, improve the presentation of experiments and algorithms...).  For these reasons, I am ok to change my score to 7.  == REVIEW BEFORE REBUTTAL ==  The paper is relatively well written and the proposed algorithms are technically sound. The experimental results on the 6 different datasets seem to indicate the performance of the approach. For these reasons I am leaning towards weak accept. However, as a disclaimer, I am not an expert in the domain and was not able to proofread all the proofs of the paper nor appreciate the complete significance of the contribution (hence my low confidence score). Below are some remarks/questions that could improve the paper:  - Highlight better the contribution and significance of the paper: it would be important to add a paragraph in the introduction to highlight better why the proposed algorithm is a significant contribution for machine learning/optimization.  - Equation (8) in Algorithm 1, sparse update: I was not able to understand exactly what was the procedure to obtain \tilde{x}. Do you have an exact algorithm? If yes can you give more details on that procedure (the section 7 in appendix was hard to understand).   - How do you choose s in practice?  - In the experimental section, please provide test accuracy curves as well. As this algorithm has a vocation to be used in Machine Learning one does not really care about getting to machine precision on the training set but we rather care about having good performance on the test set, hence it makes to also report performance on the test set. In particular, it is important to show that the algorithm is also better in the regimes of regularization that performs well in terms of final performance.  - Related work:   The authors should discuss the following related work:  Frank-Wolfe algorithms for saddle point problems, Gauthier Gidel, Tony Jebara, Simon Lacoste-Julien  - L92-93, for the three line proofs, I would still add details for the last line (using the \mu strong convexity) as you are in the constrained case and therefore the gradient at the optimum is not necessarily zero, this is not just a trivial application of the \mu strong convexity of f to me).  - How does the method would compare to FW variants that also enjoy linear convergence rate (e.g. away FW (AFW) or pairwise FW)?   In particular I think that it should be possible to apply the Block coordinate version of AFW presented in the following paper that also enjoys a linear convergence rate theoretically (under specific conditions) and in practice:  Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs, Osokin et al, ICML2016  It would be interesting to compare to that paper and discuss the advantages of your method compared to this work.