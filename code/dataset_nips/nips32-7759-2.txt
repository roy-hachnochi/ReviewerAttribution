This paper proposes a novel method based on a combination of the Frank-Wolfe-like method in [1] (with an extension to sparsity problems) with a greedy coordinate ascent technique. Technical results are adequately sound: I did not encounter a major flaw in the analysis (note that I did not go thoroughly over the proofs in the supplements, but I did skim it). There are a few issues with the complexity analysis of the methods (listed at the end of this review), but these do not change the main advantage of the proposed method (that is avoiding the O(nd) cost of full matrix-vector multiplication with A). I would recommend emphasize, however, that these improved complexity guarantees come at the cost of knowing the sparsity (or rank) of the solution a priori.   I am not particularly impressed by the clarity of this paper. It took me a few reads to understand some of the main contributions. One of the very first questions one would ask when reading the manuscript is "Why do we consider a primal-dual formulation while the template is completely smooth and strongly convex?". This should be clarified in an earlier stage of the paper.  Some concerns: - Line 143: Reported O(d) and O(n) cost of quick selection holds only if you need to choose a single coordinate. As the proposed algorithm requires "s" and "k" coordinates, the costs should be O(d*log(s)) and O(n*log(k)) respectively, and this is using heap. - Line 144: It is not clear to me why the primal variable updates cost is O(s). A naive implementation requires O(d), as you also have a scaling factor (1-eta). - An extension of [1] for the sparse problems (but not to the primal-dual setup) should be included in the baseline methods in the numerical experiments. This would also clarify the significance of your approach. - Line 102: I guess one iteration of PGD costs O(nd) instead of O(d^2). - Line 135: How does "elastic net" fall into your template? It is a non-smooth regularizer (although strongly convex).  Minor concerns: - The title is too generic, the template covered in the paper is kind of specific. It is restricted to ell-1 or trace norm-ball constraints. The optimization templates have strong convexity assumption on both terms. I would recommend using a more descriptive title. - Please consider defining problem parameters like "d" and "n" in the first equation where you introduce the problem template. I would also list the structural assumptions right here at the beginning, such as smoothness and the strong duality of "f" and "g", and that "C" is either ell-1 or trace norm-ball. This would be much more convenient for the readers. - Please avoid using contractions in lines 62 and 340 (doesn't -> does not) - Is "Delta x" in Eq(9) "x_tilde" ? - Eq (6): There are some Frank-Wolfe variants that can solve saddle point problems, although with sublinear rates. I would recommend the authors to have a look at [R1] and [R2, Section 5.4], as these methods might apply for solving (6).   A completely subjective comment: I would not call this algorithm a Frank-Wolfe variant (of course the same goes for the algorithm developed in [1]), despite the similar per-iteration cost. The original Frank-Wolfe algorithm is a successive linear approximation approach. The main geometric intuition behind the original is minimizing a linear approximation of the function at the current estimate at each iteration. This in contrast with the classical first-order methods where we minimize a quadratic approximation. The method introduced in [1] (and used in this submission) minimizes a quadratic approximation with sparsity/rank constraint. This method does not recover the classical Frank-Wolfe algorithm even when we choose the sparsity/rank parameter 1.   [R1] Gidel, Jebara, Lacoste-Julien, "Frank-Wolfe Algorithms for Saddle Point Problems", 2016. [R2] Yurtsever, Fercoq, Locatello, Cevher, "A Conditional Gradient Framework for Composite Convex Minimization with Applications to Semidefinite Programming", 2018.  ========= After author feedback ========= I increase my score to 6 upon the author feedback and the discussions.  I insist, however, that the manuscript requires a major update in terms of the presentation. I also respectfully disagree with the authors' feedback, that the proposed approach is the first primal-dual method for constrained problems. This is a vague claim that needs further specifications and clarifications. This is not even true for the line-up of Frank-Wolfe type algorithms. There is a mass of works that consider a primal-dual formulation of a constrained problem for various reasons. I write down a nonexhaustive list of examples:  - Tran-Dinh, Cevher "A Primal-Dual Algorithmic Framework for Constrained Convex Minimization" - Liu, Liu, Ma, "On the Nonergodic Convergence Rate of an Inexact Augmented Lagrangian Framework for Composite Convex Programming" - Gidel, Pedregosa, Lacoste-Julien "Frank-Wolfe Splitting via Augmented Lagrangian Method" - Yurtsever, Fercoq, Cevher "A Conditional Gradient-Based Augmented Lagrangian Framework" - Silveti-Falls, Molinari, Fadili "Generalized Conditional Gradient with Augmented Lagrangian for Composite Minimization" ... I am not telling that these works are direct competitors for your approach, but these papers design and analyze primal-dual methods for constrained problems (or that also apply for constrained problems as a special case). 