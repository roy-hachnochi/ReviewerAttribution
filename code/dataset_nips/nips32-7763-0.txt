Mixup is originally proposed as a data augmentation / regularization method. In the past few years, it has been widely adopted as an effective method to improve generalization performance. While the mixup method is not new, this work seems to be the first to use it for training well-calibrated models and convincingly demonstrate its applicability and advantage over several alternatives. Specifically, the paper shows on image classification as well as NLP datasets that mixup training significantly improves expected calibration error (ECE) and overconfidence error (OE) on the validation set; that mixup training also leads to much more conservative uncertainty estimations on out-of-distribution inputs; and that the improvement is likely due to training with soft-labels. Given that accurate uncertainty estimation is an important topic for risk-sensitive machine learning applications, this work may lead to significant practical impact.  Overall I find the paper is written with good clarity, the experimental results strong and the impact could be high, hence I vote for acceptance.