In this paper, the authors present an approach for inverting neural network generators.  Instead of doing gradient descent, as is more common in the literature, they do a layerwise inversion.  In the case where the full output of the network is measured (e.g. denoising), then this method can be directly implemented. In the case where only partial measurements are taken (e.g. compressed sensing, pinpointing, super resolution), then this method can not directly be implemented).  The argument relies on the fact that at each layer of the neural network, there are a sufficient number of nonzero activations.  This then provides a set of linear measurements, which can be inverted.  In the noisy case, the exact linear equations are replaced by a linear program (because there is no direct test for whether the ReLU was active or not.  Theorem 3 guarantees approximate recovery if the weights of the network satisfy an L_infinity lower bound, which is satisfied by random weights under a linear amount f expansivity.  The approach they study is easy to understand, the analysis of this paper is original and interesting.  The theoretical work is reasonably complete, as it contains both the L_infinity and the L1 norm error bounds, which require modifications in the formulation.  The scalings the authors obtain are superior to known scalings that work in the gradient descent case, providing theoretical justification to the empirical observation that layerwise inversion can be gradient descent.  The empirical studies nicely complement the theoretical analysis.  Overall, it is a decent contribution to the field.  The paper is mostly clearly written.  The last sentence of the abstract can be clarified (better than what?)   === Comments after reading rebuttal  I was satisfied with the additional experiment the authors reported in the rebuttal.  I am upping by rating to an 8 because this additional experiment I think shows that the method is better on real nets than one might originally expect.