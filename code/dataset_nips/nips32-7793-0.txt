This work builds on the previous work about generalization in RL ([10] in the paper references) by (re-)investigating the classical stochastic regularization approaches in this context. It completes and updates the claims made in [10] by focusing of similar performance based experiments.  Clarity: The method is clearly described in the paper. Significance: The question of generalization in RL is of great interest to the field.  Main comments:  - The paper motivates well the problems one faces when is comes to regularization in RL. The idea of being “selective” when injecting a regularizing noise makes a lot of  convincing. However, the proposed method is more about an ad-hoc mixture of  training modes, with noise injection and without, which makes the notion of selectivity here a bit loose.  - Related to the previous point, since the mixture can be done in several ways, a comparison with other ways in order to justify this choice of the mixture can greatly enhance the paper quality. For example, one can imagine adapting the dropout probability or scheduling it in a curriculum learning fashion. How does your approach relate/compare to this ?   -  In the multiroom environment, it is confusing that your method has the highest variance while the method was presented as to be favoring variance reduction. Also, IBAC seems to have a higher trend than IBAC-SNI above 0.4 x 10^8 frames (IBAC has not stabilized like IBAC-SNI), which makes the fact that IBAC outperforms IBAC-SNI very likely when trained for longer than 10^8 frames. - It is not clear from eq. 7 how SNI circumvents some of the issues listed as motivation for it. For example, how does the roll-out avoid “undesirable actions (potentially ending episodes prematurely) ?  - From the experiments point of view, a deeper analysis could be expected to relate to the issues listed as being specific to regularization in RL. The variance of the terms described in section 3, the nature of the trajectories ... etc ? In other words, it is not clear how SNI deals with the stated issues in section 3.  Minor comments:  - Should the normalization constant Z disappear in the right hand side of eq. 8 ? Also does this equation correspond to the semi-gradient or the exact gradient ?  - Was L^V_{AC} defined in the paper ? and V_\theta(z) ? (I might have missed them)  - In figure 3, can we assume that the curves with no std have very small variance ? Does the averaged curves over more runs for those one change the observations ?  - When you say “outperforms an equal mixing = 0.5 early on in training ”, to which curve are you relating here ? Is it Dropout with \lambda=0.5 ? ------------------------------------------------------------------------ I have read the author response which has fairly addressed most of my concerns so i decided to increase my score. I would expect the authors to introduce more convincing plots of the claimed performances (like the one in the rebuttal) and a clearer justification of their choice of this type of mixture, in a future version of the paper.   