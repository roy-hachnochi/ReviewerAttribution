The main contribution of the paper is to introduce a novel usage of the Information Bottleneck technique for off-policy actor-critic algorithms involving importance sampling and function approximation based critics, which results in state of the art results on the CoinRun challenge domain. An insightful discussion on some of the challenges when applying techniques that improve the generalization of supervised learning techniques to some RL settings.  In general I think the paper is well motivated and supported sufficiently with experimental evidence. I have a couple of presentation concerns but generally think the paper is acceptable.  Presentation: In general the paper is clearly written, however I do think there is a fundamental misunderstanding in the presentation, and think the authors would do well to avoid statements like "in RL the loss function depends on the learned parameters" or phrases like "the RL loss". In IID supervised learning, while one typically talks about having the same loss function (such as square loss) for all examples, with a relatively simple form, the loss function itself is a function of the parameters, so what is said isn't strictly correct. It's overly simplistic to view every single RL algorithm or technique as minimizing some loss function. Some algorithms are expressed in the form of weight update mechanisms, and rely on being contraction mappings or what not to converge, with function approximation being applied heuristically to scale to larger problems. Policy gradient techniques estimate gradient of the expected future return under some parametrized policy, it's not a loss function per-se, it's a goal you can estimate a gradient with respect to and make local improvements. Perhaps I am being overly pedantic, but I think the presentation would be more clear if you explicitly state the setting you are interested in (the empirically successful class of off-policy actor-critic algorithms involving importance sampling and function approximation based critics) and limit your broader statements and intuition to this setting. Too be clear, I have no problem with the motivation or insights behind the algorithmic changes, I just think the broader discussion is a bit sloppy.  More minor points: - In line 66, the expectation should be over q not m. - Use q(\cdot) to talk about the distribution q, not q(m) which is a density. Line 64, just write m ~ q. Similarly for other distributions in the paper.  - I don't understand the comment on line 233 - isn't the mean performance ~0.4? Do you mean that one seed got to a return of 1? - I think the paper could be improved too if it is possible to run larger numbers of seeds. The stderr is somewhat meaningless for such small sample sizes like 5.