Clarity The paper is clearly written.  The main ideas, experiments, and results are described clearly.   Originality The identification of noise issues and the proposed solutions appear original.  The idea of selective noise injection is new to me, and it addresses the issue with noisy gradients.  Although an information bottleneck has been proposed in the past, its evaluation in this setting seems novel.  Significance  This work should be of use to the deep reinforcement learning community.  Techniques such as batch normalization from supervised deep learning do not perform consistently in deep reinforcement learning.  This paper's examination of several regularization methods on Coinrun in section 5.3 provides additional evidence of this phenomenon, and the paper provides potentially useful alternatives.   Quality The technical aspects of the paper are acceptable. The issues are identified, and the proposed solutions are well motivated. The experiments and evaluation is clear.   The presentation could be improved in a few ways as noted below.  line 71: Please don't use r_t for the importance sampling ratio in Equation 1 when it also means the reward in equation 5.  line 95: Was dropout only applied on the last layer in all the experiments in this paper?  Equation 5: The rollout policy should be evaluated at s_t not s_T  Equation 12: The number of parameters that are aliased or underspecified is confusing.  I see lambda (equation 7), lambda_H, and lambda_V.  I also see L^V_{AC} without a definition, but it is perhaps equation 6.  However the IBAC algorithms are referenced without a lambda_H or lambda_v term in the experiments Figures 2,3).  Please clarify the use of these terms.  