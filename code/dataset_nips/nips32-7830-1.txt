This paper prunes the heads for transformer models and empirically asks whether they are needed. The answer seems to be that we don't need many heads for good accuracy. This is an interesting empirical result, but I think there are a few more experiments that should be run to convince the reader that the conclusions are general:   (a) Repeating the analysis with transformer modeled trained with different number of heads.   (b) Repeating the analysis on more datasets, e.g. transformer trained on a different dataset. I understand there was an IWSLT experiment in Sec 6 but it asks slightly different problems than Sec 2-5.   Clarification questions:  - Fig 1. y axis is number of heads, which is a bit confusing. Is it supposed to be frequency of models with a given BLEU/accuracy instead? Or is this plot really binnned by number of heads?   - Fig 3. What is the green line? I don't understand why pruning based on the end metric (BLEU or accuracy) would do worse than the blue line (I_h)? 