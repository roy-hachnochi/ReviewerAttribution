This paper provides four different estimators for expected information gain in a Bayesian optimal experimental design framework, with the objective of using amortized variational inference to reduce the computational cost. The main idea is using variation approximations with shared parameters for either posterior of parameters of interest, or the marginal of outcome given the design. Furthermore, an importance sampling estimator with asymptotic consistency.  Over the idea is interesting and its presentation is neat. Theoretical study of convergence for the proposed estimators and their performance in practice is also provided.  Some minor comments:  1. In the sequential setting, it seems that the implied assumption is independence of designs across different times, as entropy of the prior for parameters is assumed constant w.r.t design [line 169]. Does this hold in practice?  2. Regarding the performance results in Table 2, somehow the lower bias of \mu_{m+l} compared to \mu_{post} seems counter-intuitive, as the former uses two variational approximations. Are parameters shared between q_m and q_l, so that biases cancel out?