This paper brings a powerful dynamical systems perspective to bear on recurrent models. The universal approximation theory and results on incrementally stable models offer theoretical support for why Bai et al. 2018 and others can outperform recurrent models with feedforward/TCN architectures, and the approximation results nicely generalize those of Miller and Hardt 2019.   In particular, the approximation theorem avoids relying on a state-space representation, and the Theorem 4.1 showing recurrent models can have approximately finite memory goes through without the strong exponential contractivity assumption.   The paper is clearly written, and I checked most of the proofs for correctness.  In addition to the results themselves, the paper offers a nice bridge between recurrent models and powerful results, tools, and definitions in dynamical systems and controls that hopefully inspire more interplay between the two areas in the future.  Since the results in the paper are quite general, I would have appreciated instantiating them with a few specific parameterizations. For instance, the paper derives the Miller/Hardt result as a special case of a more powerful result. Are there nice examples where models fail the more restrictive stability condition, but satisfy incremental stability and show approximately finite memory?  Finally, is the exponential dependence on depth in theorem 3.1 inevitable or an artifact of the construction via Hanin and Selke 2018?  Typos: Equation 17: f(\xi), u should be f(\xi, u) and similarly later in the proof.  After rebuttal: Thanks for the authors for their response. I remain a fan of this paper. The specific example given showing a separation between the contractivity condition and the conditions studied in the paper is interesting, and the camera-ready version might benefit from including this and other examples, or at least linking to them the appendix for the interested reader.