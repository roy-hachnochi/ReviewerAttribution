Overall I thought the paper was quite clear: the definitions were precise and given in a timely manner, and the theorems followed in a reasonable order with plenty of motivating / explanatory remarks.  I am less sure about how deep / novel the results are however. On Theorem 3.1 - I think this is a relatively trivial application of [Hanin and Sellke, 2018], and so does not contribute much to the body of ML knowledge? The result of Hanin and Sellke says that "we can well-approximate functions on m variables with a net of width m+1 and some depth" - and Theorem 3.1 says that "if you have a function on an infinite number of variables (i/o map) that only uses approximately local context of m variables, then you can well-approximate it by a (convolutional) net of width m+1 and some depth", which follows in a pretty straightforward manner.   Some other remarks:  Definition of mF*(0): for this to be well-defined, it should be "<= epsilon", not "< epsilon", in defition 2.1 (equation 1).  I suggest sharing same number counter for propositions, theorems, definitions, etc. Otherwise there is a remark, theorem, assumption, proposition, and definition all with the label 3.1, which is slightly confusing.  On definition of time-invariant: there is an annoying condition here on "boundary effects" at t=0. In particular it requires that F(0, x_1, x_2, ...) = 0, F(x_1, x_2, ..) (also: does k>1 follow from k=1?), but recurrent systems (as defined at the beginning of section 4) only satisfy this if they ignore initial sequences of zero, i.e., with the extra conditions that f(\xi, 0) = 0 and g(\xi) = \xi where \xi \in R^n is the initial state. Therefore the time-invariant defition, or the conditions on recurrent models, need to be adjusted.