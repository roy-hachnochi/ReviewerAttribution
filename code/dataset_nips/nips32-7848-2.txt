Summary of main ideas: This paper consists of three theorems. Theorem 3.1 states that a ReLU TCN can $\epsilon$-approximate (in $L^\infty$ sense) any causal and time-invariant i/o map F with (Assumption 3.1) approximately finite memory and (Assumption 3.2) an additional continuity condition. Theorem 4.1 states that a state-space model with (Assumption 4.1) sufficiently smooth transition/output maps, (Assumption 4.2) compact state spaces, and (Assumption 4.3) ``uniformly asymptotically incrementally stability'' generates a causal and time-invariant i/o map F. Theorem 4.2 (and Corollary 4.1) provides detailed estimates of the memory and continuity against the norm $R$ of inputs, the Lipschitz constants $L_f$ (of transition model) and $L_g$ (of observation model), under some assumptions including ``Demidovich criterion,'' which contains the conditions assumed in [Miller and Hardt, 2019] as a special case.  Originality: High. The theorems are new and consistent.  Quality: High. The most technical part (Theorem 3.1) seems to be imported from [Hanin and Sellke, 2018]; but the total message goes beyond the previous work.  Clarity: High. The writing is good.  Significance: Medium. Since this study is heavily motivated by [Miler and Hardt, 2019], I would raise my score if the authors could answer the learnability question: What class of i/o maps a TCN can learn during gradient descent training?  After rebuttal: I appreciate the authors' reply. I agree that the authors are right that the learnability arguments need explicit parameterizations, but surely it is out-of-the-scope in this study. I will raise my score because I am also moved by the R1's strong phrase "I expect to see follow-up work exploiting these definitions."