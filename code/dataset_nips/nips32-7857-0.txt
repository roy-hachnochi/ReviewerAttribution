The paper presents a way to solve the approachibility problem in RL by reduction to a standard RL problem. It casts this problem as a zero-sum game using conic duality, which is solved by a primal-dual technique based on tools from online learning. The proposed algorithm assumes an oracle that approximately solves a standard RL problem. It runs primal-dual iterations, where the dual part of the algorithm updates measurement weights according to the current primal solution obtained from the oracle.   Originality: This work introduces a new problem of finding policy those measurements vectors lies inside a convex target set. The presented approach largely builds on the ideas from Abernethy et al. The high-level algorithm (Algorithm 1) and the performance bound (Theorem 3.1) are known in the literature and are similar to those of Le et al. (Algorithm 1, Proposition 3.1) with the conic duality being used instead of Lagrangian duality. The proposed algorithm is novel to the RL community. The theoretical performance guarantee is established (Theorem 3.3) that relates the approximation errors at its subroutines and the number of rounds to the performance gap.  Quality: The paper is theoretically sound. It supports the proposed algorithm with performance guarantee and the proofs are clearly presented. However, I have doubts regarding the practical feasibility of the algorithm that involves approximately solving an RL problem at each iteration. Even though authors describe computational shortcuts in Section 3.4, the effective running time and required memory resources is not reported in the experiments.  Related concern is about the comparison at a fixed the number of trajectories (Figure 1). If my understanding is correct, the proposed algorithm does much more computation at a fixed number of trajectories than RCPO that only does 1 gradient descent step at each transition. Commenting on experiments, line 284 states that the algorithm achieves a “larger reward faster by the baseline”, but it is unclear if it is due to higher complexity of ApproPO. Moreover, it was not the part of the objective.  Clarity: The paper is well-written, but it might not be straightforward for a reader from RL community. The paper would benefit from better the link/integration with the RL literature; explanation of the policy space convexification as it is not standard; explicitly stating time/space complexity of the algorithm; explicitly stating the two problems studied: the feasibility problem and distance minimization in Section 2 and link them to relevant sections.   Significance: This paper presents a novel approachibility problem that might be of interest to the RL practitioners. The paper introduces to the RL community an approach from Abernethy et al. that combines ideas of conic duality and online learning. Although, the practical feasibility of this algorithm remains a concern, it might encourage other researchers to pursue this direction.  