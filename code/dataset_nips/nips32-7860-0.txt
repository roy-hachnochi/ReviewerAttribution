Summary:  The paper investigates an online convex optimization problem where gradients of loss functions are given combined with a random noise form an unknown symmetric distribution and the norm of the gradients are not bounded. The authors propose an algorithm which achieves state-of-the-art regret bounds in terms of the expected norm in the new noisy settings. The algorithm consists of a reduction to a one-dimensional OCO (coin betting) and an OCO.   Comments:  The theoretical analyses are solid and strong and subsume previous work, e.g., Jun-Orabona-COLT19. However, I feel that the topic and results of this paper looks quite similar to the previous COLT paper. In fact, algorithms are also similar.   I also wonder how much the local differential private setting is related to the noisy OCO setting and I am afraid the connection is not so strong.   Overall, the theoretical results are beyond standard ones, but the similarity to Jun-Orabona-COLT19 makes me feel that the contribution is a bit incremental.  After reading the rebuttal comments, I understand the technical contribution more and raised my score.