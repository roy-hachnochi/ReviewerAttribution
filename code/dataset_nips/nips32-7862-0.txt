The paper studies a linear stochastic bandit problem where the learner observes only a distribution over the context vectors and the true context vector is a sample from this distribution. Authors propose a UCB-based algorithm and show that the uncertainty in the context vector leads to only a small additive regret. They also consider a variant of the problem where the realized context is observed after the learner takes an action. A kernelized version of the problem is also studied, and finally experimental results are reported.   The setting of the current paper is very similar to the setting of the following paper:    “Profile-Based Bandit with Unknown Profiles” by Sylvain Lamprier, Thibault Gisselbrecht, Patrick Gallinari; JMLR 19(53):1-40, 2018.  The above paper studies a less restrictive version of the problem where only a sample from the context distribution is observed. I am concerned about the significance of the current paper in light of this earlier result. Please discuss the above paper in your related work section and explain the differences and similarities with their setting.     Minor comment: The paragraph after equation (1): authors discuss two different notions of regret. They argue that one notion of regret is too difficult and leads to \Omega(T) regret. But in that example, the other notion of regret leads to 0 regret and the problem becomes trivial. You need a better example here. 