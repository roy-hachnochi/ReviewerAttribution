The major point that needs to be clarified for me is the distinction between single play regret and multiple play regret.  - first, regarding the d-lookahead regret, it seems E[R_T^{(d)}] is only defined for policies of a very particular form, that would select every d time step a leaf of the d-lookahead tree. Am I right? The UCB-Z algorithm is not of this type for example. It is a bit weird to have regret notion that only apply to very specific algorithms - second, regarding the distinction between single and multiple play regret, am I right that E[R_T^{(d,m)}]=E[R_T^{(d)}]? That is, you would in principle care about the multiple play lookahead regret, as one should be allowed to be play an arm multiple times during the d-time step on which we optimize.  - then, if I understood correctly, I just don't see the point of single-play regret. From my understanding, it would be defined only for strategies which selects each arm at most once during d steps (and therefore d cannot be larger than K in this case, right?). How do you motivate such a restriction? Currently in the paper, the motivation is mostly "easier to analyze" and I think a practical motivation is also needed.  When thinking of the concrete application of the proposed algorithms for example in a recommendation settings, some questions arise. First, which kernel would be chosen? For each arm, the recovery function is defined on the discrete space [0,...,z_\max]. Typical kernel k(x,x') depend on the euclidian distance between x and x', in that case k(z,z') simply depends on the integer |z-z'|? More importantly, z_\max seems to represent a "characteristic time" after which the mean of an arm doesn't evolve any more. Does it make sense to know the maximum characteristic time of all products? In practice, what type of time scales / values of z_\max would be chosen in a recommendation context?   Some other remarks:  - full-horizon regret: the comparison is with respect to an oracle to would act optimally for maximizing rewards up to horizon T in an MDP. This oracle is called "stationary deterministic", but I don't agree that it is stationary. The optimal solution in finite-horizon MDP is solution to some dynamic programming equations which in this case reduce to a finite backwards induction. As a consequence, the optimal policy depends on the remaining horizon, and is therefore not stationary.  - the analysis is in terms of Bayesian regret assuming recovery functions are drawn from the GP used in the algorithms. It seems the experimental protocol leading to Figure 4 is different: the recovery functions have a parametric forms and the parameters are drawn uniformly: is it equivalent from sampling the function from some GP? what would be the kernel? If not, it is worth mentioning the discrepancy between the kernel used by the algorithms and the "kernel" from which the functions are drawn - Figure 2 and Table 1 are a bit far from where they are needed (in Section 7): would it be possible to re-group all figures on the same page? - the bibliography contains several references to arxiv preprints that have been published by then, e.g. [10,26,28]. Could you please correct that?  