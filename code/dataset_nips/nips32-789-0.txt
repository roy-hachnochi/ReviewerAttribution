The task is novel and methodology uses existing work on computing attribution maps using Taylor series approximation to compute ambiguity maps. The use and analysis of the effects of hardness scores are interesting. The work has a clear motivation (I like Fig 1),  uses sound methodology, and relevant empirical experiments and so it meets the requirements for a NeurIPS-like conference.   Overall, I find the paper clear enough to understand and the figures useful, especially the landing figure and the output figures. Although, I think are many easy fixes that can make the paper much more readable. Here are some suggestions (in chronological order): p. Choose a different visual illusion or remove it: I like the idea of using a visual illusion because it explains the task.  But the current illusion it is not very convincing because--to me-- the individual image regions are not ambiguous but the entire image is. The bird example actually is pretty convincing and real. q. Line 72: "This observation is quantified by the introduction of a procedure to measure the alignment between network insecurities and attribute ambiguity, for datasets annotated with attributes." This is sentence is not clear, at all. r.  From the introduction, it is not clear why we care about difficulty scores for generating deliberative explanations. s. Make the definition of deliberative explanation more visible: The most important part of the beginning of Section 3 is the definition of deliberative explanations, and in your current writeup, the definition is a tiny sentence at the end of second paragraph. Also, the first paragraph can be easily made more concise to make room for  t. It is not clear what the green dots in the figures are and why they are only available for the bird classification dataset. u. a. Line 70: "For example, in Figure 1, insecurity 4 is caused by the presence of “black legs” ....”  How do you know it is not caused by the presence of concept “two pointy sticks”?