Pros:  * The new algorithm scales better with the dimension and eps: their dependence is d^{3/2} / sqrt(eps) vs d^{3/2} / eps for instance for Analyze Gauss. I think this scaling is very interesting, as it shows that when the dimensionality of the data is high (and for reasonably small eps), this method is to be preferred. The authors do not emphasize this very much for some reason, but I think it is a nice property of their algorithm.  * The algorithm, while only subtly different than the on in Kapralov-Talwar, seems substantially more natural: projection does feel to be the more natural operation than subtracting out the eigenvector, for exactly the reasons the authors mention. This is also similar to the intuition behind the private recursive preconditioning in [1]. It would be interesting to see if there is any technical connection between these two methods, as they seem quite similar.  Cons:  * The new algorithm improves upon Analyze Gauss in only a relatively small set of parameter regimes. When n is somewhat large, in particular, the new guarantees are worse than that of Analyze Gauss.  * The assumption that the columns have bounded l2 norm is quite restrictive and somewhat unrealistic. For instance, most natural generative models have columns that would be scaling with the dimension, at least. It would interesting if these methods could also depend more strongly on the geometry of the data points, for instance, if the data is ill-conditioned.  * When the data matrix is low rank (or approximately low rank), it appears that this algorithm gives weak guarantees. In particular, the very first step of estimating the eigenvalues to additive O(1/eps_0) could already plausibly destroy any meaningful low-rank structure. As a result, they can only achieve Frobenius-style approximation guarantees, which don't say much when the matrix is low rank.  Despite these flaws, I still think the paper is interesting enough to merit acceptance.  [1] G. Kamath, J. Li, V. Singhal, J. Ullman. Privately learning high dimensional distributions. COLT 2019.