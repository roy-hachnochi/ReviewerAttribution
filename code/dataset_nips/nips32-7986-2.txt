Originality: The proposed algorithm and analysis are very similar to [Kapralov and Talwar 2013]. The efficient implementation of sampling a vector on unit sphere utilizes a more recent work of [Kent et al 2018]. The main change is algorithmic: instead of subtracting rank-1 matrix from exponential mechanism each step, the new algorithm projects the data so that it is orthogonal to this rank-1 subspace.  Though the algorithm and analysis are similar to [Kapralov and Talwar 2013], it requires non-trivial amount of work to show utility analysis. The outline of proof is similar - proving utility of eigenvectors (seeing the difference of lambda_i(C) and theta^hat C theta^hat) and eigenvalues (bounding tau in Lemma 6). The paper also shows the first empirical comparison of known DP covariance estimation algorithms.  Related work and preliminary section cover main previous work and yet are not overly verbose.   Quality: The paper is well written. The proofs and details are complete. The experiments are extensive and their results are adequately discussed.  Small comments: - Lemma 1 has a utility bound with RHS depending on w^hat_alpha. I would think that RHS should depends only on C,C^hat (which is why you want to estimate covariance). It is a little disappointing - or is there an explanation why depending on regression coefficient is needed? - line 95 formatting typo: the second C^hat is not bold --> should be bold - line 1 of Alg 1: "C_1 = C" is repeated twice - I understand that the main point of theoretical contribution is to have pure-DP (Theorem 1), but wouldn't it be natural to also do advanced composition or others to get (eps,delta)-DP and check if it improves the privacy analysis, compared to other (eps,delta)-DP? At least can include this in Appendix if it does not help. - line 230, forget a space after the full stop.  Clarity: The paper is clear and concise - easy to read. The introduction is concise with good overview of subject, motivation, their algorithm's quick explanation, and contributions. Clear comparison (e.g. Section 3.2) make their result easier to understand in the context.  Small comments: - it may be beneficial to readers why/if tuning eps_i helps the algorithm or not after Corollary 1. It is mentioned as one line on empirical result section (line 317) that it does not change the performance significantly, but is there any intuition why/when/what type of data this may help? - line 300-301: what about eps_0 that is used to estimate the eigenvalues? Same as those eps_i for eigenvectors estimation?  Significance: Though similar to previous algorithm of [Kapralov and Talwar 2013], their new algorithm is significantly different enough with different proven privacy and utility bounds. The empirical results is promising. The only few details missing are running time (how it scale to real datasets and compared to other algorithms), especially to convince readers that Algorithm 2 (only non trivial part of their algorithm, in my opinion) is not hard to implement and is efficient.