The paper conducts a large-scale study of the performance of disentangled representations on upstream abstract reasoning tasks. The abstract reasoning tasks use the methodology of Raven’s progressive matrices but use samples from dSprites and 3dshapes as the skin, with some modifications. Wild Relation Network is used as the upstream model, which would use representations learned by the models under comparison: beta-vAE, FactorVAE, beta-TCVAE, DIP-VAE, and variants of these which improve on it. There are many small bits of useful information in the paper, such as the fact that metrics which measure modularity as opposed to compactness perform better in the upstream task. However, the main conclusion of the paper is that disentangled representations, in general, do enable sample efficient learning in low-sample regimes as compared to learning from scratch.  I wish the analysis could have been clearer and mode space was dedicated to it. I don’t fully understand how gradient-boosted trees or logistic regression were used as points of comparison. The first three pages are not very information-dense and perhaps should be compressed so that we get to the good stuff faster. Similarly, small details about the dataset generation could have been moved to the appendix. However, overall the paper is well-written and my criticism on clarity is minor. The paper tackles a very important question on representation learning and provides interesting new insights about it. 