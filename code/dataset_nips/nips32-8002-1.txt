This paper studies gradient compression methods for distributed optimization. The existing sign-based approach and top-k method relies on all-gather operation, which does not scale well with increasing number of workers. In this paper, a rank-r powerSGD compression method is proposed that allows using all-reduce, which has better scalability than all-gather. The rank-r powerSGD compression utilizes the one-step power iteration and is therefore computationally inexpensive. Extensive experiments are conducted to demonstrate the efficiency of the proposed method. It is good to see the ablation study in the experiment. The paper is well written. Overall, I this is a good paper, but the theoretical parts and experiments can be further strengthened.   It is not clear to me why powerSGD enjoys linearity. In particular, why does Lemma 3 in supplementary material hold? As can be seen in Algorithm 1, the construction of matrix Q involves matrix multiplication between M_i and M_j for i \not= j, and therefore \hat{P}Q^T \not= 1/W\sum_{i=1}^W\hat{P}_iQ_i^T, where \hat{P}_i and Q_i are obtained by performing compression on each i-th worker's gradient. Due to this, I do not see why powerSGD can have linearity.    There is no rigorous convergence analysis of Algorithm 2 and Algorithm 1. Only a short of description of what techniques can be applied in the supplementary material.   A similar setting is considered in [Yu et al., 2018], which proposed a PCA-based compression for all-reduce. It would be good to compare the proposed method with it.   In Section 4.1, an unbiased low-rank approximation without feedback is compared. Why don't we just compare to powerSGD without feedback?   The testing accuracy 94.3% of ResNet18 trained with SGD on CIFAR-10 is much better than the 91.25% result in original paper [He et al., 2016]. I wonder which difference in the experimental setting contributes to such improvement?   Distributed training on a small scale dataset such as CIFAR-10 is not very interesting. It is more convincing to conduct experiment on training a deep ResNet on the ImageNet dataset.   Yu et al. GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training. NIPS 2018. He et al. Deep Residual Learning for Image Recognition. CVPR 2016.   ===================================== I have read the rebuttal, and my score remains the same. I would encourage the authors to take time to study the convergence with k-step power method, and perform the experiments on larger dataset such as ImageNet. 