Summary ------- The authors address a major drawback of constraint-based causal structure learning algorithms (PC algorithm and derivatives), namely, that for finite sample sizes the outputted graphs may be inconsistent regarding separating sets: The final graph may imply different separating sets than those identified in the algorithm. This implies in particular that outputted graphs are not guarenteed to belong to their presumed class of graphical models, for example CPDAGs or PAGs. The main reason is that PC-based methods remove too many true links in the skeleton phase. The authors' solution is based on an iterative application of a modified version of PC until the final graph is consistent with the separating sets. They prove that their solution fixes the problem and demonstrate these improvements with numerical experiments.   Strengths --------- - Improves on an important current shortcoming of the PC algorithm - theoretical results underpinning their solution  Weaknesses ---------- - a bit heavy on constraint-based causal discovery jargon, some explanation for non-experts would help - computational complexity not sufficiently discussed  Quality ------- The algorithm and theory is technically sound. What I miss is a more thorough discussion of the increase in compational complexity since this may be a major impediment in adopting the algorithm.   The introduction could also mention other structure learning approaches that, I believe, don't have the inconsistency issue, e.g., SAT-solver based approaches [2].  One question to clarify for non-experts is whether the presented modification is relevant also for the graph class of DAGs instead of CPDAGs (no Markov ambiguity)? I.e., if only the skeleton phase is needed since all links are oriented due to time-order or other side knowledge.  Also, since this is about finite sample consistency, the authors could discuss how the new algorithm relates to the uniform consistency proof of the PC algorithm [1]?    Clarity ------- Overall the article well-written, but a bit heavy on constraint-based causal discovery jargon. I have listed a number of points below that could be immproved upon.  What would help is a figure and intuitive example of sepset inconsistency. I find a paper much more readable that illustrates a problem and solution on a simple example.   Originality ----------- As far as I know the work is new and related work is cited.  Significance ------------ The new algorithm can be of significant impact since, as the authors also mention, it makes PC-based methods more robust and better interpretable. The question is just whether the increase in computational complexity makes this prohibitive. That's not clear to me from the paper.  Minor further points --------------------  Abstract: - explain separating set for non-experts - "uncover spurious conditional independences" --> "uncover false negatives" ?   Introduction "sampling noise" --> finite sample sizes  - could cite some more related work, what about SAT solver approaches to causal discovery [2]? - "invoked to remove" --> sounds odd -  "achieve a better balance" --> "achieves a better balance"  Section 2.2.1 first paragraph is unclear, could phrase "slicing the available data" better...  Definition 1: Here I would like to see an example of a sepset  inconsistency with the final that gives a non-expert an intuition  Section 2.2.2 S, S', S_0, etc: all a bit confusing. Maybe a different notation would help.  "(i.e. relaxed, majority or conservative rules)" --> difficult to understand for non-experts  Proof of Theorem 4 "discard all" --> "discards all"  "less stringent separating set consistency" --> what does this imply theoretically? The theorem 4 still holds, right?  "the the edges" --> "the edges"  Section 2.3 Here one could summarize the difference in complexity between PC-stable and the proposed algorithm  Section 2.3.3 - "covariance ranges" --> Covariance between what? The SCM is run with the given coefficient ranges and gaussian unit variance errors, no? - Which specific alpha levels were chosen?   References ---------- [1] Kalisch, Markus. 2007. “Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm.” The Journal of Machine Learning Research 8: 613–36. [2] Hyttinen, Antti, Patrik O Hoyer, Frederick Eberhardt, and Matti Järvisalo. 2013. “Discovering Cyclic Causal Models with Latent Variables: A General SAT-Based Procedure.” In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 301–10.  ---------------------- Update after author feedback: The authors convincingly addressed my question regarding computational complexity and they have also found a better definition of orientation-consistency. I also acknowledge that the authors will explain the problem better with an example figure. With these, I would still stick to my previous score "a good submission" (7). I concur with the other reviewers evaluation that the work is more incremental, but in a very solid way. 