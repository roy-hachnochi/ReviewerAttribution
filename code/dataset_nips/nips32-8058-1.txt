 The authors proposed a suite of algorithms for learning the structure of the causal graph under different assumptions (infinite and finite interventional sample, single vs. K intervention, non-manipulable variables). The assumption about the type of underlying causal graphs is quite stringent: a tree with no v-structure. Authors do not provide a compelling real-world example where this assumption makes sense. Nevertheless, this work seems to provide a theoretical insight to the very specific class of problems. Overall the paper is written clearly for readers to follow without any interruptions in general (there are some issues with how the paper is organized and I will talk about this below.) Theorems and their proofs seem clear to me. The impact of theoretical results seems minimal due to its lack of applicability.   Lemma 1 suggests that the ratio between P(Y=y)/P(Y=y|X_i=1) plays an important role to update one’s belief on where the root of the given tree is. Regardless, the central node algorithm (a finite sample case, Algorithm 1) does not make use of such information. Then, the algorithm might not be optimal with respect to the number of total interventional samples (not the number of interventions under noiseless setting). Consider a case where a node X_i is selected by the algorithm based on a centrality measure where the following holds: P(Y=y) is very similar to P(Y=y|X_i=1), in other words, the strength of edges between X_i and all of its neighbors are too weak. In such case, interventional samples will not be able to change the posterior distribution sufficiently compared to samples obtained by intervening on other variables (close to the central node but associated with larger ratios) will. This case suggests that the ratio should be incorporated into a centrality measure in a finite sample case. Although you have shown the sub-optimality of an information greedy algorithm with respect to the number of interventions under noiseless setting, you didn’t show how such an information greedy algorithm works w.r.t. finite-sample case. Section 3.4 partially addresses the problem by defining epsilon, and the number of samples we need to pull under the intervention on a selected node X_i to confidently determine where the root is. (what are estimators “a” in Proposition 2? You didn’t define it.)  Q: Is there a way to incorporate a prior distribution, e.g., Dirichlet distribution, over variables? Is it unnecessary?  ========== - after response: In the finite case, not only weak edges can be left out but also non-edges can be included (false positives). I am keeping my initial score of the paper, which is a weak acceptance.