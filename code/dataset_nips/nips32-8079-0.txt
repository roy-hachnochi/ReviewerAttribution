The paper reads very well and manages to present both the challenges of NAS and the proposed idea in a very understandable form (although English grammar and spelling could be improved).   The paper's main idea is to constrain the search space of NAS to the dilation factor of convolutions, such that the effective receptive field of units in the network can be varied, while keeping the network weights fixed (or at least allowing the weights to be re-used and smoothly varied during the optimization). This idea is very attractive from a computational point of view, since it allows the notoriously expensive NAS process to achieve faster progress by avoiding the need for ImageNet pre-training after every architecture change.   On the flip side, the proposed NATS method only explores part of the potential search space of neural architecture variations. So, the longer-term effect will depend on how restrictive this choice of search space is. I.e., do we lose on potential object detection performance by only exploring the dilation factor of convolutions and would other architecture variations achieve larger improvements? This question is very hard to answer, since (if the paper is correct) no alternative NAS methods are available that could deal with the overhead of ImageNet pre-training.  Apart from this issue, the paper presents a very convincing experimental validation. It demonstrates consistent performance improvements across different backbones (ResNet50, ResNet101, ResNeXt101) and in combination with different detector heads (Faster-RCNN, Mask-RCNN, Cascade-RCNN, RetinaNet). An ablation study explores the influence of different numbers of channel groups (Tab.2), channels per group (Tab.3), and different dilation densities and aspect ratios (Tab.5).   Questions:  - What is the computational cost of performing the optimizations presented in Tab. 4 and 5? Does each NATS row of the table correspond to 20 GPU days worth of computation (L204), or were there larger variations for the different backbone architectures?  - As stated in L193, the architecture transformation search is performed for 25 epochs in total, while the architecture parameters are designed not to be updated for the first 10 epochs to achieve better convergence. I was surprised to see such a relatively low number of epochs here. Although one epoch corresponds to a run over the full COCO training set, I would have expected more epochs to be necessary for optimization. Is this low number a restriction in practice?  - What was actually the outcome of the optimization? I.e., in the optimibzed network architectures, what were the modifications that ended up being selected? Is it possible to derive some general insights into what are good architectural features for object detection? In particular, was there an observable trend in how dilation factors varied across the different layers of the network (and did this change for deeper architectures)?  Update: The rebuttal cleared up my remaining questions. I think this is good work that should be published. In the long run, the point about the necessity of pretraining R5 brought up would indeed be interesting to explore. However, even as it is, the paper already provides a very useful point of reference that future papers on NAS for objection can refer to and compete against. I keep my vote to Accept.