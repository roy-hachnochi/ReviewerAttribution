- Originality: This paper addresses an important problem - NAS for object detection. In particular, it identifies the need to use pre-training as. a key obstacle towards NAS for detection. This is a new problem that has not been considered previously in NAS and object detection research.   UPDATE:   - Quality: This paper has a good structure and a thorough literature survey. However, in some cases it seems to have omitted some related works, especially in object detection. More specifically.   This work claims object detection must use “pre-training”. While this is mostly true, prior works on getting rid of pre-training should be acknowledged. As an example:  [1]: Zhu et al. “ScratchDet: Training Single-Shot Object Detectors from Scratch”, CVPR 19 oral.  In the discussion, the paper also omits that Auto-Deeplab [22] essentially argues that running NAS from scratch on segmentation can essentially match the performance of the use of a pre-trained model. While this may not be the case for object detection (although it is hard to believe so, since segmentation is similar to object detection in many aspects), the current presentation which seems to be asserting “pre-training” as “absolutely necessary” is trivializing the more subtle topic.   - Clarity: The paper has good use of language in general. The motivation, literature survey and experiments section are clear and informative. However, there are major issues in the presentation of methodology:   Line 148-149: the meaning of “i” is not clearly defined, although it is likely to be channel index.  Equation 2: C_i^g and C_{out} are not defined.  Equation 4: “I^g” is not well defined. Also, ind_i is defined using vague language rather than precise mathematical statement which is essential for such a critical part of the presentation.  Equation 5: It is not clear how “I^g” affects “y^g”...This may again due to the lack of precise definition of “I^g”.   In general, with the help of Figure 2, 3 and the simplicity of the method in general, the core idea, namely the paper proposes to assign different dilation rates at a per-channel level is clear. However, the presentation is not clear enough for a reader to re-implement the method.   - Significance:  - The paper addresses an important question. The improvement is convincing suggesting that it is a practical method for NAS in object detection. There are a few advantages in this method that makes it likely to inspire future methods: (1) It is quite efficient, as shown in comparison in Table 1, although efficiency is derived from the continuous relaxation in DARTS and thus not an original contribution. (2) It does not introduce additional FLOPs, since it focuses on dilation operators. (3) It can use pre-trained models, which again stems from the use of dilation.   - There are important claims that are not justified, which is a limiting factor in its significance. Note that the paper propose “channel-wise” search as an important contribution. However, no direct comparison with “path-level” search is provided. This shouldn’t be impossible, as the proposed method is essentially adding degrees of freedom in the architecture parameters so that they are channel wise. The lack of ablations in this regard limits the significance of this work.    UPDATE: The rebuttal fails to convince me that the discussions on pre-training is fair and complete. This is important but not critical to me. The clarification of methodology and comparisons with path-level search are on the other hand critical, and I see a much better case there. I revise my score up to 6. 