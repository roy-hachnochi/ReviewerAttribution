This paper presents an interesting direction towards deeper understanding of speech signals in neural architectures. The methodology adapts an existing approach based on mean field theory to audio and speech recognition tasks  Comparing two random weights might be an overly weak baseline. It would be nice to see a comparison to weights after a short time training to skip past the poorly scaled norms of initial weights that can sometimes occur   It would be nice if the main figures in the paper could include error bars or auxiliary experiments that show whether the findings presented are robust across different training runs of the same neural architecture   I am not sure that other practitioners would be able to implement the metrics used and reproduce the experimental setup with other neural architectures. Even though some information is given in the supplemental material more specific method description would help others continue to use these techniques 