To me, this is an important paper contributing significantly to Bayesian deep leaning. Specifically, the paper bring the idea of “adversarial variational Bayes” to deep Gaussian processes, which is both novel (although someone may argue the idea already appears in variational autoencoders) and important. As pointed out by the authors, the learning of DGP is significantly harder than a shallow GP, even after introducing sparse approximation, and the field is dominated by the mean-field variational inference (which is easy to implement and works robustly in practice, but may lose predictive powers due to the mean-field assumption) and more recently stochastic MCMC such as SGHMC (which promises better results but is hard to tune in practice). All these urge us to bring new and better methods to training DGP or even general Bayesian deep learning models (such as Bayesian neural networks). The idea of “adversarial variational Bayes” or “implicit posterior” is a promising direction to go and the work in this paper demonstrates a significant step. In my opinion, the paper makes the following contributions: (1) The paper introduces the novel idea of “implicit posterior variational inference” to DGP, the methodology is introduced clearly and the method is solid, both theoretically and empirically; (2) The authors introduced some modifications of the architecture tailoring to the problem of DGP itself, such as parameter tying, concatenating the random inputs and the inducing points (and inducing variables); (3) Both theoretical results and the experimental section are complete and convincing. A structure suggestion hopefully to make the paper even better: Although I understand for completeness, the authors give pretty detailed explanations to “implicit posterior variational inference” in Sect 3, however, the parts have some overlaps with [1], for example the idea of introducing T (discriminator) to compute the KL divergence, etc. In contrast, Sect 4 is pretty novel and important for the method to work in DGP, while is quite short. I suggest to shorten Sect 3 a bit (by making more connections to [1], and/or move some parts to the appendix) and give more details in Sect 4 (such as a demo how parameter tying helps comparing to a naïve design). Minor comments: From Section 4, it’s not clear to me epsilon (random inputs) are shared among all the layers or not. [1] Mescheder, Lars, Sebastian Nowozin, and Andreas Geiger. "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. 