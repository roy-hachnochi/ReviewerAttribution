The authors present a novel algorithm with theoretical analysis and empirical results. We have a few comments and suggestions for the work:  The comparison of forward vs reverse KL divergence as the objective criteria resembles the choice of mode vs. mean seeking form of the objective in variational inference, respectively (in this case applied with a Dirichlet distribution). We recommend that the authors refer and make connections to this similar literature.  It would be great if the authors could expand upon the distinction of in-domain and out-of-domain training data in lines 105-106. How are these datasets created and is the purpose of separating the data to improve generalization. How is the optimization performed in practice?  In the algorithm, the authors propose to set the in-domain \beta parameters to large value of 1e2 and the  out-of-domain parameters to small values of 0. How sensitive are the results to these specific choices? The authors also note that the losses were equally weighted using the forward KL divergence and had a large relative weighting \gamma when using the reverse loss. What criteria was used to optimally choose the \gamma parameter?  Lastly, we had a few minor suggestions for the text: using the conventional indicator variable instead of \mathcal{I} may be more clear, and defining all notation (i.e., \pi) in the main text may improve readability. 