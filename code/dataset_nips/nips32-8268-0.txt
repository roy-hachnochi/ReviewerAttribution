The paper gives a clear overview of the “mode-connectivity” or epsilon-mode connectivity phenomenon in two- and arbitrary-layer neural networks, and how overparametrization has been studied as a potential explanation.  Overparametrization is shown to enable dropout in a structural (rather than algorithmic) sense:  if solutions are “dropout-stable”, then there exists a path between them (explicit construction provided) satisfying the eps-mode connectivity property (that the loss stays within eps of the larger of the two loss values).  An equivalence between noise-stability and dropout-stability is then derived using a straightforward dropout algorithm, to extend and expand upon the results from dropout-stability to noise-stability.  Experimental results show that the loss along paths as described for the minima studied is indeed nearly constant, and largely unaffected by dropout.  Finally, a counterexample shows that overparametrization does not guarantee (eps-)mode connectivity.  The paper is well motivated and generally well written, and the settings described are not restrictive.  The path constructions are not overly complicated, but would benefit from being described more in math or pictorially (e.g., the block matrix multiplications where zeros propagate) rather than in paragraphs.  Also, this is partly a function of space constraints, but there is much geometric meaning in the various “cushion” definitions that could be further touched upon (with greater concision elsewhere).