- Predict a target variable based on representations.    - Theory suggests disentanglement doesn't guarantee fairness, but empirical results show a correlation between fairness and disentanglement.    - Fairness here is defined as having a prediction not depend on a sensitive factor s.    - The introduction could be a bit clearer about the definition of fairness that will be used, given the technical nature of the paper.  More particularly, what justifies the demographic parity definition that is used throughout the rest of the paper?  - In Figure 1, assumes a causal graph where the sensitive and target variables are independent, but the observations are generated from a complicated and unknown mixing function.    - Intuitively I can think of a simple case where having entangled latent factors can "drag" along irrelevant factors which are correlated with the sensitive variable.    - Several VAE variants try to improve disentanglement.    - May be useful to also be fair with respect to unobserved variables.    - Experiments show that disentanglement is generally correlated with fairness (Cars3D seems to be an exception)  --  This paper presents an interesting empirical analysis relating fairness to disentanglement of learned representations.