=== UPDATE === After reading the author response, etc., I do agree with reviewer 1 and think the proofs present in the author response makes the math easier to follow (although I was able to do it previously with a non-negligible amount of effort). Regarding Minecart vs. experiments in this paper, I still don't have an intuitive feel for the significance of the empirical results (as I don't specifically work in the MORL space), but I was convinced in the first place. I still think it is a good paper that I would prefer to see accepted. ======  The authors consider the problem of multi-objective reinforcement learning (MORL), proposing a solution that learns a single policy network over the envelope of the convex coverage set of the Pareto frontier wrt a linear combination of preferences. This is an innovation over scalarized deep Q-learning [Mossalam, et al., 2016] (and follow-up work [Abels, et al., ICML19]) in that this previous work sequentially trains Q-networks on corner weights in preference space (thus approximating the CCS), resulting in a set of Q-networks (as opposed to a single-network policy). This is basically accomplished by performing (deep) Q-Learning over a generalized Bellman equation that directly utilizes vectorized rewards such that they generalize to all weight parameters in the CCS — using hindsight replay and homotopy optimization to make learning tractable (and presumably stable). Once this generalized setting is learned, it immediately adapts to any set of weight parameters as a deep learner is learning a function approximation over the entire space. To accommodate scalarizations of the multi-objective problem in the reward, they also introduce a method for learning the weights. Analytical analysis mostly centers around convergence of the generalized Bellman equations and that weight parameters selected from this function are equivalent to learning the single preference function directly. Experiments are conducted on four different domains (including simple and challenging tasks), demonstrating empirical improvements in terms of coverage ratio (ability to recover optimal policies), adaptation error (difference between recovered control frontier and optimal one), and average utility over recent relevant works (including [Abels, et al., ICML19]).  Overall, this seems another step in the deep learning evolution to formulate the problem directly (in this case, by generalizing the Bellman equations) and relying on the expressivity of the function approximator to directly learn the function from data. On one hand, it is sort of surprising it works as one would think that the generalized Bellman equations would be difficult to learn and require really massive datasets and potentially finicky optimization methods (i.e., more than hindsight replay and homotopic optimization — as is sometimes seen the Appendix details) to get this to work. However, I suppose since the CCS is relatively smooth, maybe this isn’t a big issue with the right architecture. Conceptually, it is an appealing framework, has sufficient analytical analysis to demonstrate depth of understanding, and the empirical results are reasonably compelling. Thus, while noting that I have little experience with MORL (and much more with RL in general) and that I do have some criticisms as itemized below, I think this paper is an improvement over recent related works and will likely be well-received and used within this community.  Below, I will evaluate this submission along the requisite dimensions.  === Originality Obviously, MORL is a well-established problem — however the conceptual approach of using the vectorized values directly and generalizing the Bellman equations to derive a single-network solution as opposed to scalarizing the vectorized values and learning multiple networks for different configurations seems a non-negligible step that is more elegant and greater runway for further innovations. Specifically, it is an important distinction that learning the boundaries of the CCS may not generalize well to arbitrary weight parameters at test time as is done in the multi-network case. The solution proposed in this work will almost certainly lead to a better interpolation of unseen weight configurations and the theory shows that it will return optimal Q-networks when actually on the CCS envelope. As I previously state, I am a bit surprised it actually works due to the expressivity of the resulting function, but I was also surprised when first seeing DQN work as well as it does in general, but wish I had more time to really dig into the code to see the details. However, I will view that I want to do this as a positive. Wrt originality, I believe it meets the NeurIPS publication bar.  === Quality The general framework make sense and the theoretical analysis supports it well. For the ‘inverse weight parameters’ problem, it is not entirely clear the MVN assumption of weights makes sense, but seems to work and is fine for an introduction of the framework. Something like a GP or other bayesian optimization procedure seems to make more sense, but this can be follow-up work and isn’t the core contribution of the paper. The empirical results, while sufficiently convincing, offer more room for improvement. Specifically, [Abels, et al. ICML19] use a minecart simulation which seems to be relatively difficult and since this paper makes a direct comparison in general, I am not certain why this wasn’t used. Deep Sea Treasure and Fruit Tree Navigation don’t seem particularly difficult, while dialogue management and SuperMario do. Some discussion here regarding the relative difficulty, convergence times, etc. would be nice. Specific to [Abels, et al., ICML19], while I don’t know of the a better comparison, this isn’t a perfect match as this paper was really about adaptation under non-stationary weights — even if they do use the multi-network formulation. Some discussion in this regard would also be nice. Wrt quality, I also believe it meets the publication bar, although more due to concept and theory than empirical results and not as strongly as originality.  === Clarity For somebody very familiar with RL in general, the paper is easy to follow and reasonably well self-contained. That being said, the Appendices do add quite a bit of material that makes the paper more understandable — but do believe the paper stands as relatively complete on its own. My one conceptual question is lines 44-45, “optimized over the entire space of preferences in a domain” vs. lines 166-168, “same utilities…will only differ when the utility corresponds to a recess in the frontier”. By the definition of CCS, the second statement is true (along with lines 81-82) — is the first statement just referring to the expressivity of the learned function? The construct used for training will certainly bias the portion of the space that is learned. A few small questions regarding details;  what were the values of hyperparameters N_\tau and N_\omega in Algorithm 1 and what was the rate of annealing for homotopy optimization? In any case, this is a relatively simple idea (once revealed) and well-motivated. The theoretical analysis is sufficient and the experiments are well detailed (even better once the Appendices are read).  === Significance I believe the overall conceptual framework is sufficiently significant, the theory is sufficiently well-supported, and there is enough evidence that they actually got this to work. I didn’t spend a ton on time looking over the code and didn’t run, but it ‘appears’ usable just from skimming it. The least well-studied contribution is the ‘recovering linear weights’ component and the ‘2x average improvement’ of line 61 does show preferences can be learned, but is a bit of a straw man on the whole — but this isn’t the core contribution of the paper. Overall, while I am not of this specific community, I think the overall framework is elegant and shown to work. I would expect it to be the starting point for work in this area as it handles vector rewards directly and is conceptually very elegant — fitting into the continuing trend of general frameworks with lots of data.