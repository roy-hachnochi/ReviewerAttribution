***** Post-rebuttal comments ***** Thank you for addressing my queries in your response. I think including a discussion on the contributions over and above Abels et al (2019), as well as the other issues raised, will help improve the final version of the paper.  Summary:  The paper presents a method for learning a preference-conditioned Q value function that can quickly adapt and solve new tasks with different preferences (in the case of linear preferences). A generalised Bellman operator is introduced, and convergence to optimal Q values is proved. This allows for the approach to learn a single Q function over all preferences. At test time, dynamic preferences can be fed into the function to produce the optimal value functions for the corresponding task. Results in both low and high-dimensional domains validate the method, which improves over existing work. I think the paper is extremely well written and the idea as a whole to be the "right" way of tackling the problem. I only have some concerns in terms of how big a contribution it is compared to previous work.  Comments:  I found the paper to be extremely well-written, easy to read and clear in its explanations. I thought the algorithm block was very useful and informative, and I really liked the Super Mario domain. I think Figure 10a is also very useful and authors may wish to consider including it in the main text, space permitting. The only comment I have on clarity is about Figure 2c. It's not quite clear what is going in on the diagram and how it links to (c)'s caption.  Which solutions do scalarised algorithms find in this diagram? In particular, the claim on L115 says that the envelope method leads to better learning as demonstrated in the figure, but it's not clear to me how.   My main concern revolves around the contributions and improvements offered by the paper, in particular compared to Abels et al. The paper claims that only that "scalarized updates are not sample efficient and lead to sub-optimal MORL policies", but I think a more detailed explanation should be provided, especially since they appear to share many similarities. Do the main differences come down to a lack of convergence/optimality guarantees? It seems like the idea of learning a preference-conditioned Q function is that same here as it was in that previous work. Additionally, while the results show the method outperforming the baselines, I find it hard to judge how big an improvement it is. Certainly compared to scalarised, the numbers in Table 1 look close together (the utility in particular), and Figure 13, for example, looks like extremely minimal gains. As such, I struggle to judge whether the improvements are significant or incremental. Including some significance testing on the results would be helpful here.  The paper claims "And to achieve the same level AE the envelope algorithm requires smaller N_omega than the scalarized algorithm. This reinforces our theoretical analysis that the envelope MORL algorithm has better sample efficiency than the scalarized version." I don't understand this paragraph for two reasons. It suggests that there is theory that shows that the sample complexity is lower than that of the scalarised version, but I don't see any theory suggesting that (unless I am missing something). Additionally, picking a larger N_omega has no effect on the number of interactions with the environment, since it is just more omegas being applied to the same (s, a, r, s') transition.  Could the authors please clarify this paragraph?  I have a question regarding the loss functions used when learning. L^A and L^B are defined, where L^A is the standard loss function and L^B is a surrogate one, and the final loss is a mixture of the two.  Given that L^A is the actual DQN loss, why is lambda annealed toward L^B instead of L^A? Intuitively, wouldn't you want L^A to be the final loss optimised, and so anneal lambda in the other direction?   In Algorithm 1, the weights are sampled and y_{ij} is calculated. In that section it says that omega prime is an element of W. What is W?  One separate area of research that may be worth mentioning in relation to this work is composition. There's been some recent work on composing value functions and policies to solve new tasks [1, 2, 3, 4].  In this setting, you could imagine learning separate policies for each objective individually, and the composing them to solve the multi-objective task, where the weights of the composed policy are related to the preferences. Of course this has drawbacks in terms of sample and memory complexity, but it may still be worth discussing.  The link to code that was provided returned "This upload does not exist", and so I was unable to review the code.  Minor corrections:   At the risk of being overly pedantic, mention that s' comes from the trajectory tau in Eq 3. Also, the discount factor is only introduced on L158, but is used much earlier (obviously gamma as the discount factor is common knowledge, but it would still be good to mention it around Eq 1. L136: omega should be bold Caption table 2: learnig -> learning   [1] Todorov, Emanuel. "Compositionality of optimal control laws." Advances in Neural Information Processing Systems. 2009. [2] Hunt, Jonathan J., et al. "Composing Entropic Policies using Divergence Correction." International Conference on Machine Learning. 2019. [3] Van Niekerk, Benjamin, et al. "Composing Value Functions in Reinforcement Learning." International Conference on Machine Learning. 2019. [4] Peng, Xue Bin, et al. "MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies." arXiv preprint arXiv:1905.09808 (2019) 