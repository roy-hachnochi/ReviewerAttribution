The work is novel and  significant. It applies previously established generalization bounds in the form of upper bounds on norms of DNN weights towards providing performance guarantees by instead imposing these bounds as constraints on the parameters. This is an interesting development studying the geometry of parameters W of deep networks, and is a promising direction to pursue as demonstrated by improved performance in experimental results in practice. Authors describe challenges of handling multiple constraints expressed as products of manifolds, as they can be different from the geometry imposed by individual component manifolds. Further they provide the algorithm FG-SGD as a way to modify SGD such that the POM constraints are satisfied. The modification involves projection operation onto the tangent space of the manifold. Rescaling operation so that the upper bound RHS is 1.   Clarity can be improved by avoiding run-on sections and breaking up text into meaningful subsections. clearly stating the purpose of each section, the motivations, findings and conclusions.   ### After rebuttal, review discussion: Suggestions to improve clarity  - Reduce introduction to no more than 1 page - List contributions as done in rebuttal succinctly - Currently, there is some redundancy under - Boundary names of lists - Training DNNs and turn another bulleted list in pages 2 and 3 Please consolidates - Have a separate background/related work section to motivate work  - Have a separate notation section, introducing any notation used throughout paper. It is difficult to parse when new notation is introduced right before it is used or not at all  - Subsections in section 3 e.g.,  -- "In order to ..." should be a new section -- The 2 results should be listed as sub-headings -- How these results are incorporated into algorithm should be listed immediately after  Section 4: - Compress table 1 & 2 captions - Notation to be described separately and not merged in bullets - Derivation of Lines 5,6,7 should be explained in greater detail