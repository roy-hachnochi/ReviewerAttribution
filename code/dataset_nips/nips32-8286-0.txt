EDIT post-rebuttal: Thanks to the authors for answering to my comments. I am still voting for acceptance of this paper.   +++++   This paper is about a software component, called Bayesian Layers, that allows for consistent creation of deep layers that are associated with some form of uncertainty or stochasticity. The paper outlines the design philosophy and principles, shows many examples and concludes with new demonstrations of Bayesian neural network applications.   I find that this work is on a significant topic, since software for Bayesian (deep) learning models significantly lacks behind. Integration and drop-in replacement with traditional architectures seems like the right avenue to pursue, and is a strong motivation point for this approach.   I also think that this work is sufficiently original, related to what one could expect form a software component. I find that the relation to Pyro's random module is strong, perhaps stronger than discussed in the paper, when it comes to the fundamental concepts behind it (for this statement I am assuming that there is no fundamental reason why Pyro couldn't be extended with recent estimators using the current random module's architecture, but I am not an expert). When it comes to practicality though, I find that Bayesian Layers is sufficiently different, more consistent and more extensive. In any case, the community needs more works on this kind of modules. Having said that, comparison to Pyro (e.g. more discussion or side-by-side snippets) would be useful.   Regarding quality, it is in general difficult to criticize a software in the same way as for a mathematical idea: ultimately, in software it's all about trade-offs in functionality. Nevertheless, I find that from the technical viewpoint, the design choices are very reasonable, offer useful features and improve upon past approaches (e.g. Pyro's random module) in certain aspects. The consistent use of layers with traditional architectures and software components makes the approach very useful, readable and scalable. The separation of model and inference is an important aspect for the Bayesian community, and is indeed unfortunate that the default use of Bayesian Layers obscures this separation. However, it is also true that no one design can fulfill all requirements, and the authors admit that this is deliberate to facilitate other important aspects of their framework. Indeed the modularization of inference per layers is a very interesting idea, however for such an important design choice, I feel that the paper does not explain clearly all the reasons why this is beneficial. A more concrete side-by-side demonstration about its advantages would be welcome. As a side note, it is also not entirely clear how the approach in section 2.5 separates in principle model and inference. Could the authors provide a more extended snippet that uses the model and the posterior?   As for presentation, in this type of software paper the usual issue of trading-off details and abstraction is exacerbated. However, I feel that the authors did a good job exposing the correct amount of details. Having said that, the flow between being high- and low-level is not very smooth and at times the paper assumes too deep knowledge of Keras, Tensorflow and recent Bayesian ML methods (e.g. lines 22-40). The intersection of readers being in-depth experts in all the above is smaller than the set of readers in the target audience. Therefore, I suggest a background section listing the basic topics before diving deeper into the details.   The authors use a variety of ML models to demonstrate the concepts. Although this is more difficult to follow, I feel that it is ultimately more instructive, and also demonstrates better the range of applications for Bayesian Layers. Related work is generally discussed thoroughly. Overall, the presentation in this paper is honest (starting with what it is: code), and gets right to the point.   A think that the paper would benefit from the following central discussion: Bayesian models are still not as widely used in practice as DNNs. Some say that this is due to current software capabilities. Is Bayesian Layers a reply to this? In my opinion, it seems that there's more missing, whether in the inference methods, practical tricks or software. It'd be interesting to see a discussion about this here. The paper shows *how* to use Bayesian layers, but not as much  *why*. This is more important than in theoretical papers, because by (some) definition a software paper is about practicality.   More than simply a discussion on the above, it'd be nice to have a demonstration, e.g. with stronger, previously unthought of RL results. However, that would be an impressive bonus, and I understand that it goes beyond the scope of the paper.  