-- Paper Summary --  This paper describes a comprehensive extension to Edward2, built upon Tensorflow, which permits the seamless inclusion of non-deterministic layers for constructing deep probabilistic models. The API is similar to that used for standard neural network layers, but permits the inclusion of uncertainty on a variety of components such as the weights in a layer, any associated parameters in the activation function, as well as the inclusion of Gaussian process layers (for which a variety of formulations are included). This module not only makes it easier for practitioners to construct such probabilistic models, but also exploits the processing power of state-of-the-art TPUs for implementing large-scale models such as the Bayesian transformer described in the paper.   -- Writing/Clarity --  The paper is well-written and methodological in its approach. The related work is properly described, and I found the individual subsection on different layer options to be informative and useful. On the downside, I found the code snippets to be fairly messy - the presentation is quite poor with some instances of overlapping text, and I highly encourage the authors to rethink their presentation. I am also neither a fan of the current side-by-side placement of the figures which is hard to follow at times.  With particular emphasis on the paper title and the first line of the abstract, I am slightly puzzled by the insistence on referring to the module as catering for ‘neural networks’ when it also enables the inclusion of Gaussian process layers, and consequently the construction of deep Gaussian processes. Of course we could debate on the connection between Gaussian processes and infinitely-wide neural networks, but this generalisation appears misleading to me in this context (and possibly also undersells the broader functionally provided by the proposed module).  It may also be helpful to include a more detailed breakdown of the various options available for some of the arguments in the method signatures. For example, the local reparameterisation trick is mentioned in the description of one of the experimental set-ups, but it would be nice to have a more comprehensive list of the available options for each parameter option (perhaps in the appendix). I appreciate that this verges on providing full code documentation, but I think it would be nice to have more of this included in the paper along with a list of associated references for the papers originally proposing the implemented/featured techniques.  Some additional minor complaints: - There are some typing issues in the references where words such as ‘Gaussian’ appear as lower-case. I noted some papers cited here as appearing on ‘arXiv’ which have since been published at either ICLR or ICML 2019 - please double check and update accordingly. - A few typos/preferences: L29: monolothic -> monolithic; L32: ops -> operations; L85: abbreviated last names in citation; L207: In ‘the’ experiments; - The phrase ‘whether it be the weights, pre-activation units, activations’ appears in some form  or another at least 3 times in the first two pages. While I appreciate the authors’ intent to drive the message home, it unintentionally comes across as overly repetitive.   -- Originality and Significance --  The importance of Bayesian inference in practical applications of machine learning has recently become more prevalent, and the work presented here is certainly a step forward in facilitating their use. As the authors clearly illustrate in the supplementary material, implementing Bayesian variations of neural networks and other models using frameworks such as Tensorflow and PyTorch typically require a substantial amount of tweaking (often bordering on ‘hacks’), whereas the proposed module could help abstract away from such complex implementations. Homogenising techniques such as Bayesian neural networks and Gaussian processes under a single framework should also have positive repercussions by encouraging future work on these topics to include broader experimental comparisons of both methods. While reading the paper, I was wondering about the possibility of incorporating the recent work by Sun et al. 2019 in this set-up, so I was very pleased to see this listed as a direction for future work.  I also liked that the authors picked reinforcement learning as a use-case for demonstrating the scope of this work. Given the ballooning popularity of this field, showing how Bayesian Layers enable the extension of these methods to the Bayesian setting should be a great entry point for both ‘Bayesians’ interested in dipping their toes into reinforcement learning, and vice versa.  The related work section is comprehensive and I appreciated the segment dedicated to describing the differences to Pyro, which would be the primary competing mechanism available on PyTorch. However, I was surprised that there was no mention of the MXFusion package (Dai et al., 2018), which completes the trifecta of modular probabilistic modelling by offering an implementation for MXNet. To the best of my understanding, this package bears greater similarity to the work presented here due to the inclusion of Gaussian processes and a similar notion of ‘inference modularity’ relying on variational inference. I would expect to see any connections explored in greater detail given the similar nature of this work. Instead, it is currently conspicuous by its absence. Work by Cutajar et al (2017) on deep Gaussian processes also merits a mention in the discussion on GPs for being one of the first practical instances of exploiting Tensorflow for implementing large-scale DGPs.   -- Technical Quality/Evaluation --  There are little theoretical elements to comment on here, but I found most of the discussion on the implementation details of this module sufficiently clear to follow. Perhaps including some more background information on Edward2 could be useful for readers who aren’t immediately familiar with what it currently provides. As highlighted earlier in my review, the presentation and content of the code snippets could definitely be improved however. I think it’s also important to specify the degree to which this work extends beyond simply building an API around existing functionality and implemented models - at the moment the extent of the contributions are not entirely clear.  Although the experiments showcasing the module are diverse and sufficiently convey the broad scope of their potential use, I feel as though the paper is missing some degree of benchmarking with regards to both scalability across hardware and model parallelisation. While I appreciate that this is not immediately within the scope of this work (which rather relies on the fundamentals of Tensorflow for these inner workings), it would be interesting to see whether a direct comparison against Pyro and MXFusion can be carried out in this regard. In its current format, the experimental evaluation feels fairly isolated in simply showcasing the functionality of the module, but there is little external context.  It is also slightly unclear to me how existing frameworks such as GPflow are positioned in relation to this module - while I appreciate that the scope of GPflow is much greater than the brief appearance of GPs featured here, I am still curious to understand whether say, the DGP of Salimbeni et al. (2017) which can be constructed here, is just as good as the original implementation.   -- Overall recommendation --  I would be hard-pressed to classify this paper as ‘essential reading’, but I also believe that it successfully describes the module in a succinct manner, while also giving potential readers and conference attendees a better incentive for incorporating probabilistic modelling in their workflows. There’s a few disappointing aspects which I highlighted in the review - aside from some other minor issues, the messy inclusion of code snippets is a sign of carelessness when preparing the submission. While effective in showcasing the diversity of model which can be tackled using the proposed module, the experimental section also verges on being more of a ‘demo’ than a critical evaluation. I am currently giving this submission a relatively ‘modest’ score, but would be keen on raising this score following a convincing rebuttal.   ** Post-rebuttal update **  Thank you for your response! The rebuttal targets the majority of concerns listed in the reviews, and also clears up some of the more muddled aspects of the paper. If accepted, there are some points which require particular attention, especially clarifying the similarities to related work (all reviewers had issues with this aspect) and highlighting the contributions further. Papers describing software toolkits are always faced with greater scepticism, which makes it essential to clearly emphasise the contributions of the paper. Likewise, the inclusion of additional comparisons and benchmarks will elevate this from seeming like a standard technical report or documentation to a proper paper. Coupled with the presentation issues highlighted across reviews, I believe there is still some work to do, which is why I am not increasing my score. However, I also think that this work could benefit from the increased attention enabled by NeurIPS, and hopefully encourage more streamlined model implementation and evaluation within the Bayesian community. For this reason, my vote still tends towards accepting this paper.   * With regards to the title, it is ultimately at your discretion whether to change it or not. However, ‘uncertainty-aware functions’ has a nice ring to it!