** update after author response ** The author response is lucid and to the point. My concerns are cleared. So I decide to raise the score to 8.  This paper presents a method AlphaNPI to learn neural programs without groundtruth execution traces. Instead, post-condition tests are used to provide reward signals. AlphaNPI could try different atomic actions and novel subprograms, and reuse subprograms that leads to high rewards. It's verified to generalize well on bubblesort and tower of hanoi.  This could be a very good work. However I feel a few points should be improved: 1. The idea of how a new subprogram is learned and added to M_prog is not clearly presented, which I think is the core contribution of this paper. 2. The experiments are a bit weak. Basically the authors compare the model with its own variants. In addition, as in typical RL papers, the authors should provide the training iterations and reward curves, so that readers will have an idea of its sample efficiency.  3. AlphaNPI uses post-conditions to make sure a learned subprogram brings direct benefits. This is interesting, but I'm also worried that this may be limiting. In more sophisticated problems, not every subprogram can be measured this way. Some subprograms could be utility functions and do not provide direct rewards. It's difficult to define meaningful post-conditions for such subprograms. I guess that's why the authors choose bubblesort and tower of hanois (because they can be decomposed into easier subtasks, and post-conditions can measure whether each subtask is accomplished successfully.) Though, this work could be an interesting initial attempt along this line. 4. How the tree policy vector pi_mcts encodes the tree? It's not explained. 