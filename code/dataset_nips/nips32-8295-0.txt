UPDATE: I thank the authors for their detailed response to reviewer feedback. Overall I believe this is a quality paper, and would be happy to see it published at NeurIPS. This is reflected in my current assessment, which remains unchanged.  Previous studies have contributed three classes of methods for avoiding gradient communication bottlenecks for distributed SGD: (1) communicating updates less frequently, (2) communicating less updates (random k or top k) and (3) communicating quantized updates. There is also a known method for keeping track of induced error in order to compensate later.  The authors build upon these previous works by combining all three methods into a unified algorithm (QLSGD), showing that one can combine their benefits. They additionally show that the error compensation mechanism is more broadly applicable than (2), which is the main context it has been previously used. In order to demonstrate convergence properties of QLSGD, the authors were further required to generalize some tools and statements from prior literature, in some cases with improvements (e.g. getting rid of an unnecessary technical assumption from a NeurIPS paper last year). These generalizations are not stand-out contributions in isolation but do add to the overall strength and quality of the work.  Pros:  The authors are to be commended for combining both a very strong, theoretical analysis with experimental results on a practical distributed SGD problem (ResNet-50 for ImageNet). As far as I can tell their results represent a new state-of-the-art for bits transmitted in a master-based distributed SGD setup.  Cons (all minor points):  The authors focus entirely on worker -> master communication complexity, but never discuss the reverse direction (broadcasting updated network weights). Tackling this issue directly is not necessarily within the scope of this study, but it certainly warrants discussion. Even a 100x improvement for half the communications is "only" a 50% speed-up.  Additionally, the authors should take note of this related work from ICML this year, "Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication" (https://arxiv.org/pdf/1902.00340.pdf). This appears to already address 2/3 of what the authors propose in this study, which does detract slightly from the overall novelty.  I further refer the authors to Ofer Dekel's SysML 2019 keynote, where he describes at length why claims of the form "we achieve X% cost reduction for only Y% loss in accuracy" are flawed / very difficult to interpret. I largely agree with his opinion on the matter.