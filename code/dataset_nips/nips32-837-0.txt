POST-REBUTTAL COMMENTS  Thanks for your response.  Alternative derivation: Sorry, I wasn't thinking of the alternative derivation (using Newton's method) as a contribution. I still don't see that this derivation gives any additional insight into the problem, however.  Partial derivatives: Thanks, I believe some clarifying remarks will make it easier to follow.  Deep learning: The main appeal of this paper is that it presents a general mechanism that can be integrated into any gradient-based learning framework. However, the other reviewers have raised legitimate concerns that there are no experiments which demonstrate the approach in an end-to-end setting, and I agree. While you have shown that the stabilised variant works, it has only been demonstrated on relatively shallow problems.  Tracking experiments: Putting some frames of a video in the training set and other frames in the testing set violates the independence assumption. To me, this means that you are virtually reporting training error, not testing error. This may be sufficient to show that the optimization procedure works. However, you may as well not bother having a testing set.  Related work: Great. The contribution of the paper will be more clear once put in better context.  Taking all of this into account, I will preserve my rating of borderline / lean-to-reject. I like the idea of the paper. However, I think that it needs to be further developed before being published.  Minor: Perhaps "Kegan et al" should be cited as "Samuel et al"?   ORIGINAL REVIEW  # Originality  The overall method is similar to the prior work of Kegan et al. It seems that the main theoretical difference is the introduction of a trust region regularizer to stabilize the solution?  This paper also misses several relevant citations. In particular: * Strelow "General and Nested Wiberg Minimization" (CVPR 2012). This paper extends the Wiberg technique to the optimization of general functions of multiple variables. * Bertinetto et al "Meta-learning with differentiable closed-form solvers" (ICLR 2019). Performs SGD using the derivative of several Newton steps for the solution of a logistic regression problem. * Madry et al. "Towards Deep Learning Models Resistant to Adversarial Attacks" (ICLR 2018). Considers the min-max problem of training a network that is robust to an adversary.  It may also make sense to cite some of: * Amos and Kolter "OptNet: differentiable optimization as a layer in neural networks" (ICML 2017). Uses the solution of a Quadratic Program as an intermediate layer in a deep network. * Finn et al "Model-agnostic meta-learning for fast adaptation of deep networks" (ICML 2017). Performs SGD using the derivative of a gradient step with respect to the parameters of a network. * Jenni and Favaro "Deep Bilevel Learning" (ECCV 2018). Solves a nested optimization problem to optimize for the error on a validation mini-batch using several training mini-batches. * Maclaurin et al. "Gradient-based hyperparameter optimization through reversible learning" (ICML 2015). Back-propagates through 100 iterations of SGD to optimize hyper-parameters. * Ionescu et al. "Matrix backpropagation for deep networks with structured layers" (ICCV 2015). Incorporates matrix-valued functions into back-prop, including the solution of an eigenvalue problem (normalized cuts).  This paper considers a problem which is different or more general than each of these existing works, although it seems that all of the necessary theoretical elements were already existent in this literature.  I like the two problems which are tackled using the proposed method (tracker fusion and 3D pose estimation). This approach is novel for these problems, and they demonstrate its generality.  # Quality  The main issue for this paper is the lack of theoretical novelty.  I'm satisfied with the experimental investigation.  # Clarity  The paper is mostly clear and easy to understand.  I understand why you introduced the final term equal to zero in equation 8 (to arrive at the definition of the partial derivative), but I think it wouldn't hurt to state this explicitly.  The division notation of equation 8 is not familiar to me. You are using a / b to denote the matrix a_i / b_j, such that delta_y / delta_x is the Jacobian? It might be worth mentioning.  I didn't understand what "frames missing at random" meant in the tracking experiment.  # Significance  The ideas in this paper are not novel in general. However, there does not seem to be an existing paper that presents these known results in a general framework for making continuous predictions in deep networks.  # Comments on derivation  I have a minor issue with the derivation in Section 3, but it does not affect the correctness of the final result: I don't think you should equate the RHS of equation 8 to the _total_ derivative d/dw (rather than the partial derivative ∂/∂w) in the RHS of equation 9, since it does not seem to account for the dependence of x* on w? To be precise, I think it is OK to say the RHS is equal to   {∂/∂w -HE(x; w)^-1 ∇E(x; w)} evaluated at x = x*(w) but not that the RHS is equal to   d/dw -HE(x*; w)^-1 ∇E(x*; w)} since x* depends on w. The end result is the same, except that the final expression in eq 9 will be   -HE(x*; w)^-1 {∂/∂w ∇E(x; w)} evaluated at x = x*(w). This is more clear: otherwise it may seem that the derivative d/dw ∇E(x*; w) = 0 since the value ∇E(x*(w); w) = 0 for all w. I guess you were implicitly treating x* as constant when needed and you obtained the correct result. However, this required significantly more effort for me as a reader to understand the mathematics.  While trying to understand the derivation in Section 3, I accidentally re-discovered the technique of Kegan et al. Why not use this method to obtain the result of equation 9? It seems much more clear and simple. For example, the entire derivation could be: Let t(w) = ∇E(x*(w); w). Clearly t(w) = 0 for all w and therefore dt(w)/dw = 0. 0 = dt(w)/dw 0 = d/dw ∇E(x*(w); w) 0 = (∂/∂x ∇E(x; w)) dx*/dw + (∂/∂w ∇E(x; w)) dx*/dw = -HE(x*(w); w)^-1 (∂/∂w ∇E(x; w))_{x = x*(w)} 