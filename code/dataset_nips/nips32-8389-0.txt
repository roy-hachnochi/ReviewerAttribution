This paper is easy to read and well structured. It raises an important privacy issue in naive collaborate learning. The problem statement is well-formalized and solution is well explained. I categorized this paper among the ones with major novelty and high significance.   The following is some the detailed comments: 1- Authors have this assumption that F is twice differentiable. Although they discuss this briefly at some point where they replace ReLU with Sigmoid, but I would like to see deeper discussion regarding this constraint. What are the other common scenarios where attacker needs to replace the network layers? 2- In noisy gradients defense strategy, what is the tradeoff between information leakage and accuracy loss?  3- The concept of `iteration` in their paper is sometimes unclear. For example at the experimental setups when they mention: " ... max iterations 20 and optimize for 1200 iterations and 100 iterations for image and text 129 task respectively." When do you mean the number of iterations `n` in the for-loop in DLG algorithm? and when you mean it as the number of iterations in original distributed training?  4- does lower precision training helps? 8-bits precision training for example? do they protect training data?  5- DLG algorithm, line 4, computes the dummy gradient but it shows it as \Delta{W_t}. Should it be \Delta{W^'_t}? 6- Section 3.2: "to chooses" -> "to choose"    