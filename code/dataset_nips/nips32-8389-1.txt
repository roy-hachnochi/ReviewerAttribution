strength: a. Relevant findings - Given the recent surging interest in federate/collaborative learning, the authors' findings indicate that gradients do capture private information is insightful and relevant b. Elegant approach - The approach, unlike [27] is much simpler and requires weaker assumptions to reconstruct the input data.  Major concerns:  a. Attack model / gradient computation - The authors look at the specific case of reconstructing raw private inputs resulting from gradients resulting from a single iteration computed on a small batch of images. The attack model assumes these are shared to the adversary. - However, participants in collaborative/federated learning scenarios share gradients/updates computed over multiple batches and epochs (see [26], Alg. 1) -- after all, this is communication efficient. In this particular case, I'm skeptical of the effectiveness of the proposed attack. - Consequently, I'm concerned that the attack model (where attacker uses a single gradient) is in a contrived setting. - Moreover, while it could be argued that in some distributed computation models e.g., [14, 19, 23] the gradients from each iteration are indeed communicated -- these models seem to cater towards distributed computation in a cluster, in which the raw data is already possibly present for the adversary to access bypassing the need to use gradients.   b. Missing details / writing I strongly recommend the authors to make more passes to fix typos/gramma and add many missing details that makes the findings unclear: - Implementation:    * L135: CIFAR = CIFAR10 or CIFAR100?     * L138: what is the batch size $N$ used in the experiments?    * L134 / Eq. 4: Do you use all trainable parameters of the Resnet as $\nabla W'$?    * L134: How were these models trained? What are their train/test accuracies? - Results:    * Figure 5: Is the blue line "L2 distance" over all parameters and other lines over parameters of specific layers? Assuming it is, the green and red lines (parameters of layers closer to FC) have lower losses -- so why not use these? How do the leaked images look in this case?    * Figure 3, 4, ..: are qualitative results from a held-out test set that was not used to train $W$?    * Figure 3, 4, ..: How/why were these images chosen? How does the reconstruction look like on set of randomly sampled gradients?    * Figure 7: what are the accuracies of the model when defending with these strategies? Afterall, if the accuracy of $W$ is retained with a prune ratio of 30% (Fig 7d), the attack can be easily defended. - Some unclear statements:    * L119: "... batched data can have many different permutations ... N! satisfactory solutions ..." - How? Won't they still produce the same loss irrespective of the permutation?  Other concerns:  c. Experimental depth I overall find the experimental section somewhat shallow, leaving many questions unanswered:   - Is the model $W$ trained to convergence? Isn't it more interesting to evaluate the attack at various stages of training of $W$? After all, the proposed attack seems relevant primarily at train-time.   - How do the reconstruction results vary with batch size?    - How does size/complexity of $W$ affect effectiveness of the attack?    d. Other simple defenses - Given that "The deep leakage becomes harder when batch size increases." [Table 1], wouldn't this also make for a good defense? - Extending this argument and connecting to the point I raised earlier in (a): wouldn't averaging updates/gradients (computed over multiple batches) instead of gradients on a single batch also prevent reconstruction to a large extent? After all, the former is what's done in federated learning. 