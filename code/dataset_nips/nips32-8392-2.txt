This paper tackles an interesting type of adversarial examples different from the classic Lp adversarial examples.   In the previous literature,  group-equivariant networks have not been extensively evaluated using adversarially chosen, but rather random transformations. This paper provides an interesting angle of spatially Invariance-inducing regularizations, and justify it both theoretically and empirically.   Empirically, the paper showed that regularized methods can achieve ∼ 20% relative adversarial error reduction compared to previously proposed augmentation-based methods (including adversarial training).   One advantage of the paper is the theoretical analysis of spatial adversarial examples, giving insights as to why regularized augmentation is effective. In addition, the results indicating that regularized training is just as effective as specialized architectures is an insightful result. Besides, its theorem about there is no trade-off in natural accuracy for the transformation robust minimizer is interesting.   One major drawback is that there is very little discussion on the empirical results regarding the effectiveness of regularization. Much of the empirical discussion is about training runtime, which isn’t an issue in most cases. Most importantly, the experimental section is hard to follow, and there are not clear takeaways from Table 1 aside from that regularization is better than standard augmentation plus random rotations. A more in-depth analysis of this would be helpful.   It is also unclear why larger datasets such as ImageNet were not used in addition to SVHN and Cifar10. These are two highly specialized datasets, so the results may be biased.