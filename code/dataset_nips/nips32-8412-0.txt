This paper presents compressed sensing style recovery guarantees for solving inverse imaging problems with untrained network priors, as initially investigated (empirically) in [9] and [10]. The work extends RIP-like conditions for exact recovery under a “deep decoder” prior for the image [1] (i.e., assumes the image is in the range of a convolutional neural network of a certain structure). In particular, they prove for a d-dimensional image, n x d Gaussian random matrices satisfy an RIP condition for images in the range of deep decoder network provided n is on the order of the dimension of the initial layer of the network. Using this RIP property they prove that a projected gradient descent algorithm converges linearly to a solution of the optimization problem with the deep decoder prior. The paper also extends this analysis to an algorithm for solving compressive phase retrieval.  This paper makes several excellent contributions. To my knowledge, this is the first paper to give compressed sensing style recovery guarantees for solving inverse problems with deep network priors like those in [9] and [10]. Also, to my knowledge, this is the first work applying these methods to the compressive phase retrieval problem.  The paper is very clearly written. In particular, the proof sketches give useful details for understanding how the main results were obtained, and indicate clearly what earlier results they build off of (namely, ref [6]).  Generally my appraisal is very positive, and I think this paper is a clear accept. However, I see some minor limitations of this work.  One limitation with the present analysis (acknowledged by the authors) is that the algorithm studied is somewhat idealized in that it assumes the projection onto the range of the deep net can be computed exactly. This problem is non-convex, and standard solvers for this subproblem, such as gradient descent, could get stuck in local minima. It would be interesting if the present analysis could be extended to the case where this subproblem is only solved inexactly, perhaps even with one gradient step.   Also, the sampling complexity n = O(k_1(k_2 + k_3 + ... + k_L)log d) seems suboptimal -- is the dependence on the size of intermediate layers k_2, k_3,...k_L truly necessary or an artifact of the proof technique? Some comments on this might be helpful for the reader to understand if the results are order-wise optimal or not.