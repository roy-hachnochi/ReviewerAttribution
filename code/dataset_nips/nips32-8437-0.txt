# Overall Comments: This is a nice paper in the sense that it makes the AE-based models produce really high-fidelity images as good as GAN-based models. In addition, the model also inherit the nice property of AE-based models that it does not suffer from the mode collapse issue. However, it seems to me that the only difference between this paper and the VQ-VAE paper is that this work introduces the hierarchical structure to learn different levels of latent representations and priors. The novelty looks a bit low. In addition, this paper didn't provide any idea about why such a design can make the generative performance better.  The loss function (2) is not a reasonable objective to optimize considering the stop gradient operator. During the optimization procedure, the loss function may increase by taking a step in the gradient directions. This makes the algorithm like a hack and not elegant at all.  # Questions: - Why you call this method VQ-VAE-2 since no variational method is used in the algorithm?  - Is it possible that h_bottom encodes all the information for reconstruction and the algorithm ignores h_top completely? If not, why this cannot happen. In my opinion, h_bottom has size S/4xS/4xD, where S is the original image height/width and D is the number of channels. This is already big enough to remember the training data. Of course there is still a quantization step which can forbid it from remembering the input data. However, it seems to me the capacity is already big enough for h_bottom to perfectly reconstruct the input data.  - Since h_bottom may have enough capacity to remember the training data, could it be that the model is just remembering the training data rather than generating really novel samples?  - Can the algorithm do interpolation like may AE-based generative models can? I cannot find a trivial way to do this. Interpolation is also an important ability of AE-based models.  - The input to the pixelCNN will at least have the size S/4xS/4. This will make the generation phase relative slow. Can the authors provide the comparison of time cost between this algorithm and the corresponding GAN model? 