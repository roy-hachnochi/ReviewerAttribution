The proposed method produces impressive samples of high quality and high diversity, with a novel approach. The text is fairly clear and makes the method easy to grasp on a high level. However, the text is light on details. A detailed architecture description of the whole two-level hiararchy is missing, where the “conditional stacks” (L135, what are those?) should be explained/visualized, and how the different inputs to the decoder are merged, how the masked attention layers are implemented exactly, etc. Also, on a more minor note, Fig. 2a) does not quite align with Algorithm 1 (encoders both have aces to x, one also to etop, decoder has two inputs, no decoder from etop to hbottom), using the same terms in the figure as in the algorithm would help.  Conceptually, the method seems to have some similarities to image compression literature, where the line of work leading to [A] use a hierarchical model, including an auto regressive model for latents, for *lossy* compression, and [B] proposes a hierarchical model similar to [25] for *lossless* compression. Further similarities arise a) from [A] training for MSE + NLL, motivated from a rate-distortion perspective, b) from the fact that both networks could in principle be used for sampling (albeit probably with much worse results, see Fig. 3 of [B]), and c) because NLL for some levels of their hierarchies can be calculated. Given the hierarchical nature of this work, a discussion or even quantitative comparison might be interesting.  The comparison to BigGAN seems thorough and is convincing. I like the proposed ‘critic’.  Refs [A] Minnen, David, et al. "Joint autoregressive and hierarchical priors for learned image compression." NeurIPS 2018. https://arxiv.org/pdf/1809.02736 [B] Mentzer, Fabian, et al. "Practical full resolution learned lossless image compression." CVPR 2019. https://arxiv.org/pdf/1811.12817.pdf  Minor details - Fig. 2 and Fig. 5 miss a label.  - Algorithm 3 mentioned on L138 is nowhere to be found.