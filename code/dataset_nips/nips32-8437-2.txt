Model Summary: The model proposed is based on the VQ-VAE model. The VQ-VAE model is a variational auto-encoder model, with quantized latent variables, and an auto-regressive prior trained to model these latent variables. It is extended using top-down hierarchical latent variables. The second contribution of the paper is to trade-off variability against quality of images by using the confidence of a classifier to reject bad samples.    The paper is well and clearly written. The state of the art and related work is well covered, and good intuitions are given about the model.  Conceptually, the novelty of the proposed approach is limited. The only difference to the VQ-VAE model seems to be the addition of hierarchical top down sampling, which is a standard construction in the VAE literature. On the other hand, the results obtained are excellent in terms of image quality. So the value of the paper is mostly experimental as it scales an existing model further. However, the paper does not put much focus on ablations studies (what is important to scale things up? e.x what's the impact of batch size etc..).   Results: The samples obtained are impressive especially at high resolutions on FFHQ. They  demonstrate that a likelihood model with sufficient capacity can generate compelling photo-realistic images. In particular, the precision-recall curves shown in Figure 5 convincingly show that the model obtains a quality of image close to that of BigGAN, while having better diversity. This is significant: models trained by maximum-likelihood, unlike GANs, are unlikely to drop parts of the training support ('mode-dropping') which makes it harder to produce compelling samples.    However, this paper is not the first to achieve that, so better ablations that clearly show why it works may be desirable. In particular, the size of the model and batch sizes used are presumably significant, and a big part of why this works. If this is what it takes to make maximum likelihood work, it is better to make it evident. Therefore, ablations on Model size could be desirable. How does this model perform compared to existing models when model sizes are comparable? In this respect, the authors have provided architectural details in the Supplementary. Similarly, I would be interested to know how this model performs without quantization.   No BPD measurements (or bounds) are provided in the main paper. I find this choice a little surprising. Given that the model belongs to the family of maximum-likelihood models, it could be desirable to report these values and compare to that literature as well as BigGAN.    Classifier based rejection sampling: This contribution is quite orthogonal. In practice, resampling favours model that over-generalise VS models that mode-drop: the model is no longer penalized for generating bad samples, but has better support. In particular, given enough rejections any model will eventually produce a compelling sample. But it is indeed a simple and nice way to trade-off variability for quality, and to obtain precision recall curves. In the case where the classifier is trained on an other dataset, this uses extra data.  Could you elaborate on the range of thresholds used for classifier based rejection in Figure 5? Are they the same for both VQ-VAE and BigGan? Also, is classifier based rejection used in Table 1?    Minor remarks: In terms of evaluating image quality, showing nearest neighbours in pixel space and vgg-feature space could be considered.  