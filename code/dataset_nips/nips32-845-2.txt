This paper is original and analyzed several tensor network's expressive power.    The paper presents MPS, BM, and LPS to model multivariate probability mass function. For MPS, it is well-known that can be used to represent HMM or other probabilistic models, while BM and LPS are relatively new but straightforward. The learning algorithms are based on minimization of negative log-likelihood, which is very natural and straightforward.   In the paper, 7 proposition are given, which are the main contributions and results. These propositions mostly focus on analyzing the rank relations among several tensor networks.  These results are important and interesting.   Weakness: 1. Although these tensor networks can be used to represent PMF of discrete variables. How these results are useful to machine learning algorithms or analyze the algorithm is not clear. Hence, the significance of this paper is poor.   2. The two experiments are all based on very small dataset either generated or realistic data. The evaluation is performed on KL-divergence or NLL, which only show how good the model can fit the data, rather than generalization performance. How these results are useful for machine learning?  In addition, MPS, BM, LPS are quite similar in the structure. There are many well known tensor models, CP, Tucker, Hirachical Tucker are not compared.  There are also more complicated models like MERA, PEPS.   I have read the authors' rebuttal, they addressed some of questions well. But the generalization is not considered, thus it becomes a standard non-negative tensor factorization problem on the PMF data.  Hence, I will remain the original score. 