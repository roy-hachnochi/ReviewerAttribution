The paper discusses how to solve semi-supervised learning with multi-layer graphs. For single-layer graphs, this is achieved by label regression regularized by Laplacian matrix. For multi-layer, the paper argues that it should use a power mean Laplacian instead of the plain additive sum of Laplacians in each layer. This generalizes prior work including using the harmonic means. Some theoretical discussions follow under the assumptions from Multilayer Stochastic Block Model (MSBM), showing that specificity and robustness trade-offs can be achieved by adjusting the power parameter.  I am not fully understanding or convinced by the Theorem 1. Particularly, it claims zero test error under stochastic models, which is either wrong or suggesting that the method may be impractical. Further, Corollary 1 and most of the experimental results support a negative norm parameter, which is rarely seen.  I also checked the additional descriptions on Krylov subspace methods. Krylov subspace methods are important for sparse matrix factorization. However, I found no original contributions on this part from the authors, besides a (nice) simple mentioning.  === Post rebuttal ===  I re-checked the paper. I agree that the paper seems intuitive and it makes incremental contributions. I will adjust my score to weak accept, with the following notes:  The main contribution is a SSL method that is "provably robust against noise ... as long as one layer is informative and remaining layers are potentially just noise" as a limiting case. The intuition is that when p -> -inf, the smallest eigenvalues of the informative layers will dominate the other presumably larger eigenvalues of the noisy layers. The intuition is further extended to a series of consistency theorems using graph Laplacians in expectation, which is a limiting case given infinite amount of edge observations.  While the intuition for p -> -infty is obvious, I would love to see more discussions around p = -1 case. The p=-1 case is included in Theorem 1, but not discussed in Corollary 1. Particularly, the assumption that "one layer is informative and the remaining layers are potentially just noise" no longer holds. Neither is p=-1 discussed in Section 4.2, because Figure 4 shows equally good results with p from -10 to 1, i.e., not specific to p=-1. The main evidence for p=-1 case is from the real-data experiments, but with insufficient discussions.  The Krylov subspace methods need to be presented more clearly. Does it fundamentally change the orders of complexity or not?