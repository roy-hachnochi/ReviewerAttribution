The authors study online control problems where the dynamics are known, there is no process noise, but the cost functions are changing over time. A main motivating application is LQ tracking. The authors study the problem by looking at dynamic regret, where the online algorithm has to compete with the optimal solution (not just best static controller in hindsight). To compensate for this more difficult objective, the algorithm is assumed to have access to a small prediction lookahead window.  The main result of the work is a new momentum based online control algorithm.  The authors prove that the regret of their proposed algorithm decreases exponentially in the prediction lookahead window size W.  The authors also prove a lower bound that shows that for LQ tracking their regret upper bound is nearly optimal. Experimentally, the authors show their algorithm outperforms MPC from a regret point of view while using similar amount of computation.  This is a nice paper. The focus on dynamic regret is new, and the reduction from LQ controllable form to unconstrained convex optimization is of interest to the online learning community.  The authors should cite the following related paper: Online Control with Adversarial Disturbances. Agarwal et al. ICML 2019.  There are some similarities between Agarwal et al. and this work. For instance, Agarwal et al. also considers time varying costs while allowing for an adversarial disturbance sequence to affect the dynamics. However, they show regret bounds with respect to the best (almost) static policy.  Presentation wise, in Algorithm 1 I would have preferred to first see the algorithm with vanilla gradient descent first before moving to the triple momentum algorithm. The regret bound can be presented for just RHTM, but for simply understanding the algorithm a simpler update rule would be easier to follow.  Minor typo: in lines 155 and 156, it should be x_{t+1} = A x_t + B u_t.