Originality: The first work that trains GAN for spectrogram-to-waveform conversion without distillation.  Quality: This paper suffers from a few critical issues. See comments below.  Clarity: The experiment setting ups can be described with more details. Sec 3.2 and 3.4 is missing important information such as the datasets used for conducting the experiments.  Significance: Although the quality of the proposed model remains unclear because of the previously mentioned critical issues, it’s a significant work because it’s the first GAN-based model for spectrogram-to-waveform conversion which seems to be working at some degree.  Critical issues:  1. It’s significantly over-claimed: 1) claiming state-of-the-art for spectrogram-to-waveform conversion (line 6) with MOS 3.09 is surprising; many previous works are at a much higher level (e.g. 3.96 on the same dataset in https://arxiv.org/pdf/1811.00002.pdf, 4.5 in https://arxiv.org/pdf/1712.05884.pdf); 2) claiming state-of-the-art for text-to-speech synthesis (line 87) without comparison to strong baselines; 3) claiming “autoregressive models can be readily replaced with MelGAN decoder” (line 89, line 228) without necessary experiments (e.g. for TTS, no comparison to WaveNet and WaveRNN, no quantitative evaluation for multispeaker TTS; missing strong baselines) ; 4) claiming likely to work for linguistic-feature-to-waveform generation (footnote 1) without evidence.  2. Some experiment results seem in conflict. The MOS scores in a TTS setting up (3.88 in Table 3) is way higher than the MOS scores in a much easier (groundtruth-)spectrogram-to-waveform conversion setting up (3.09 in Table 2).  Other comments:  1. Table 2: it will be very helpful to include the MOS for groundtruth audio, which serves as an anchor for comparison MOS scores (which is a subjective evaluation score).  2. Table 3: needs stronger baselines, such as Tacotron + WaveNet and Text-to-Mel + WaveNet, especially because the paper is claiming state-of-the-art. Also needs MOS on groundtruth.  3. It’s not clear what datasets are used for experiments in Sec 3.2 and 3.4. It will be helpful to make it clear.  4. Sec. 3.2: are the vocoder models trained on groundtruth spectrogram or predicted spectrogram? For best results, they should be trained on predicted spectrogram (see https://arxiv.org/pdf/1712.05884.pdf).  5. Since spectrogram-to-waveform conversion is a strongly conditioned generative process, it’s interesting to know how much benefit GAN brings in this work. A baseline to compare with is to train only a Generator model with MSE loss (or other simple loss), without using Discriminator.  6. The author seems misunderstanding what VQ-VAE refers to (Sec. 3.4). VQ-VAE is a framework for learning discrete latent representation, which doesn’t include a WaveNet decoder (line 248) or other decoder (it’s typically paired with a decoder). It doesn't have to produce “downsampled” encoding either (line 247).  7. Line 93 “10 times faster than the fastest available model to date” -- please add a reference to the “fastest available model to date” in order to be clear which model is compared to.  8. Two papers should be covered as related work in the paragraph on line 73: https://arxiv.org/pdf/1904.07944.pdf and https://arxiv.org/pdf/1902.08710.pdf.  ==============  Update:  Thanks for the authors' response. The critical issues were addressed. I updated my score accordingly. However, the authors' response didn't address 6, 7, 8 in my detailed review. Please address them in the camera ready version.  A couple more comments regarding the response:  1. Discrepancy in MOS scores between Table 3 and Table 2: comparison before training is converged is generally not trustworthy. Please justify why it's a proper comparison in the camera ready version.  2. The complete failure on the training without Discriminator sounds surprising. It may suggest headroom for improving in the design of the (strongly-conditional) Generator.