The problem is relevant and the method is based on an interesting attention based idea to look at different regions in the image for the task of ZSL   The losses used focus on (i) making each attention map peaky, while making different maps diverse, (ii) embedding based softmax for better prediction and (iii) class center triplet loss which makes the features closer to their respective class centers relative to the other class centers.   Line 190 mentions that the image and parts are sent to “separate backbone networks”, which implies that the network parameters are not shared. If that is the case then the method will have ~3x parameters cf competing methods ie. a significantly higher capacity network overall. What happens when the CNN params are shared? And what happens when the image only baseline has a higher capacity network backbone (which is also then end-to-end finetuned)?   The learning of channel wise attention weights are initialized by clustering the features using k-means, which is shown as an L2 loss minimization approach in eqn.(3) in supplementary section. A detailed ablation study would have been helpful showing the importance of this initialization.    The number of clusters is fixed to be 2 for all the datasets. There is no justification or experiments provided to validate the same. Parametric studies for this should be provided, preferably with network weight sharing and without.   As mentioned in supplementary material, clustering of feature channels is done using “CNN trained for the conventional classification task and extract the coordinates of the peak for each channel”. Is this a diffrent CNN other than that used in the approach? If so then the approach should include the same or else if the clustering is done along with the training of the CNN network embedded in the approach, then it would give erroneous peak channel value for the initial iterations. A more clear explanation is required about the calculation of channel wise attention.    Results for Generalized ZSL, which is a more practical and harder task combining both seen and unseen classes at test time.    Missed one of the relevant approaches in this area: Ji, Zhong, et al. "Stacked semantics-guided attention model for fine-grained zero-shot learning." Advances in Neural Information Processing Systems. 2018.  A more recent paper in the same idea of attention for ZSL (although the paper appeared after NeurIPS’19 submission date, so not considering in this review, only fyi): Xie, Guo-Sen, et al. "Attentive Region Embedding Network for Zero-shot Learning." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.  Minor comments: ‘qualitative’ in place of ‘quantitative’ as mentioned in the title of appendix G  Some of the main contributions are pushed to supplementary section, such as the details of clustering of channels mentioned in appendix A and the inference in appendix D. It would have been better if these contents could have been mentioned in the main paper.  --------------------- Post rebuttal ---------------------  I appreciate the interesting rebuttal. However, I still find that the submission would need more work. - The with and without parameter sharing exposes high variance in results with number of clusters. Since the part definition step is stochastic, I would also do multiple initializations and report variances. - The ZSL results are not quite state of the art (eg many results from CVPR 2018 Feature Generating networks, Xian et al.; there are even better numbers out there now) - The clustering process and its working would still need more analysis and discussion (with experiments)  I would keep my rating. 