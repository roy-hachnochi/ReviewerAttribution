 The paper uses brain activity data (fMRI and MEG) obtained from subjects while reading natural text and computes representations of NN models (ELMO, BERT, etc.) on the same text data. The goal is to see which layers predict brain activity in different areas of the brain, as well as the role of context size for each of the method.  Conclusions: - T-XL increase prediction accuracy with increase of context size  - BERT and T-XL capture context in a way that is relevant to predicting brain activity in their middle layers - Removing the attention at various layers for BERT has similar effects on brain prediction and NLP tasks: lower layer uniform attention for BERT is better in both cases, while at layer 11 it decreases performance in both cases.  While this type of analysis is not completely novel, the observations made are new and very interesting. For the most part the paper is clearly written (See Improvements section for clarification questions). The paper would be stronger if the empirical implications of the observations (the attention removal) would be tested more.  