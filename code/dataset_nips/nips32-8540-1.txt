Review update: Thanks for the response. It addressed my concerns well, and the SM kernel comparison seemed to give consistent results. I'm increasing my score slightly.  ---  This paper proposes functional kernel learning (FKL) a Bayesian nonparametric framework to infer flexible, stationary kernels for Gaussian process (GP) models, under multitask and multidimensional settings, with its latent GP inferred with elliptical slice sampling. The paper includes extensive empirical evidence to support FKL's superior performance over common kernels.  The paper exposites this method in a clear and understandable manner with a step-by-step process. However, from my perspective it can benefit from some improvements on certain theoretical and empirical aspects. Here are my detailed comments on said aspects.  In Section 3.1, the description of the Bochner's theorem is crucial to understanding stationary kernels, but it is slightly inaccurate. Bochner's theorem maps stationary kernels to finite measures, instead of Lebesgue measures (line 78), hence not all spectral measures of stationary kernels have valid spectral densities. For a detailed account of this issue, see Samo and Roberts [1]. The trapezoid rule as a variant of the Darboux sum poses another issue: when the density S(omega) is Lebesgue measurable, the Darboux sum is a good approximator only when S(omega) is continuous almost everywhere (Riemann integrable). However, those issues can be resolved with the fact that a mixture of Gaussian measures is dense in the Banach space of finite measures (see Shen et al.[2]), suggesting Riemann integrable densities are dense for spectral measures.  In Section 3.2, the logarithm of the spectral density is modeled by a Gaussian process, but the quadratic mean function of this GP is without proper justification. It is slightly confusing why a parametrized, quadratic mean function is necessary for spectral density estimation on a compact subset of the frequency domain (eq. 3).  Section 3.3 explores FKL with multidimensional data, which is modeled with a separable product kernel, rendering it inherently restrictive. Theoretically, separable kernels do not support all multidimensional stationary covariances (one example is the multivariate spectral mixture kernel[3]). For complete support over stationary kernels, it is needed to generalize eq. 3 into a multidimensional setting where omega_i are placed on a Cartesian grid. This generalization represents the entirety of stationary functions, but incurs the curse of dimensionality. While product kernels are commonly used in Gaussian process models, it is worth mentioning the capacity of different model specifications.  For inference with FKL, the paper proposes an MCMC based inference scheme using elliptical slice sampling. While the inference is suitable for this task, I think it would benefit from formulating the GP regression for f as Bayesian linear regression instead. The trapzoid rule approximation renders the corresponding Gaussian process degenerate, equivalent to a Bayesian linear regression with trigometric basis expansion.  In Section 5.4, comparisons for GP interpolation are made between FKL and standard kernels (RBF, RBF w/ ARD, Matérn). Standard kernels generally do not perform well when the dataset exhibits clear periodicities, for their spectral measures converge around frequency 0. It would be prudent to at least consider spectral mixture kernel (with sparse GP regression) as a candidate for parametric, flexible kernels.  On a minor note, the placement of Figure 1 can be improved, and the citation style needs to be more consistent.  [1] Samo, Y.-L. K., and Roberts, S.. "Generalized spectral kernels." arXiv preprint arXiv:1506.02236 (2015). [2] Shen, Z., Heinonen, M., and Kaski, S.. (2019). Harmonizable mixture kernels with variational Fourier features. Proceedings of Machine Learning Research, in PMLR 89:3273-3282 [3] Yang, Z., Wilson, A. G., Smola, A., and Song, L.. "A la carte–learning fast kernels." Artificial Intelligence and Statistics. 2015