Summary of Contributions:  UPDATE: I have read the author rebuttal and my review is unchanged.  This work considers the role of interaction in (distribution-free) PAC learning with local differential privacy (LDP).  In this model the learning algorithm only has access to examples that have been locally randomized in a differentially private way; the learner may specify the index of the example and the randomizer.  If the learning algorithm makes all of its queries in advance, it is non-interactive, and if it makes its queries independent of the labels returned by the oracle, it is label-non-adaptive.  The main results of this paper show that PAC learning with a label-non-adaptive LDP algorithm is characterized (up to polynomial factors) by the margin complexity MC(C) of the class, which is the inverse largest margin of separation achievable by linearly separable (into positive and negative examples) embedding of the domain. The lower bound (see 1. below) has interesting consequences regarding the power of interaction for LDP PAC learning.  For example, this implies that any non-interactive (and even label-non-adaptive) algorithm for decision lists requires ~2^Omega(d^⅔) examples; any such algorithm for linear separators requires 2^Omega(d) examples. On the other hand, these classes do have efficient LDP PAC learning algorithms.  The last exponential separation result for PAC learning is due to Kasiviswanathan et al. (2011), who first introduced the notion of LDP; they gave an exponential separation for PAC learning a less natural class that does not hold in the distribution-free setting.  Thus, this work gives the first distribution-free separation and does so for natural and well-studied learning problems.  The characterization via margin complexity is given by the following statements :  Any label-non-adaptive eps-LDP algorithm that PAC learns a class C (closed under negations) requires Omega(MC(C)^{⅔} / e^{eps}) examples. There is a label-non-adaptive eps-LDP algorithm that PAC learns a class C to accuracy 1-alpha with poly(MC(C)/alpha*eps) samples.  In fact the characterization is for the non-interactive statistical query (SQ) model, which then translates to LDP PAC learning through the equivalence proven in Kasiviswanathan et al.  The proof of statement 1 uses a connection between margin complexity and correlational statistical query (CSQ) dimension established by Feldman 2008, which was improved by Kallweit and Simon 2011.  These works show that the ability to construct a small set of universally correlated functions for a class C implies small margin complexity.  This work shows how to construct such a set using a label-non-adaptive LDP algorithm.  The proof of statement 2 is more involved and reformulates the halfspace problem as a stochastic convex optimization one, applies a random projection to the embedding space (which may be large compared to the margin), and shows how to solve the convex optimization with label-non-adaptive queries.   Strengths/Weaknesses The power of interaction for solving problems with local differential privacy is an important topic within differential privacy that is motivated by both theoretical and practical concerns.  This work gives a strong lower bound for learning natural and well-studied classes without interaction; these classes have efficient algorithms with interaction.  On the other hand, Kasiviswanathan gave a strong lower bound for a more contrived class, which only has an efficient algorithm with interaction under the uniform distribution.  Since that work does not give a separation for general PAC learning, I view this result as a significant contribution to our understanding of the power of interaction for local differentially private algorithms.  As a technical contribution, this work makes a simple but nice connection to the notion of margin complexity (admittedly, a good portion of the connection is already made in Feldman 2008), and shows that margin complexity characterizes non-interactive LDP PAC learning for a more permissive notion of non-interaction.  One drawback of this characterization, as noted in the conclusion, is the need for this more permissive definition; another area left open is to show lower bounds for algorithms with bounded interaction.   In addition, the paper is of high quality and is generally well-written.  A few specific comments regarding clarity follow below.  Notes:  Lines 72-73: Smith et al. 2017 do give a natural statistical problem which can be solved with interaction and show an exponential (to my knowledge) lower bound without interaction for a restricted but natural class of algorithms (which include the interactive ones).  In light of this, it seems fair to point to the exposition in 1.2 with a footnote here. Lines 153: If this bound is exponential say this instead of “strong”?   Line 110: It seems one should also cite Kallweit and Simon since that stronger bound, which is proven in a different way than Feldman 2008, is used in the results that have already been stated. Line 152: typo “in the dimension number of queries”   Lemma 2.6: In both sources the corresponding Lemma is stated in terms of the CSQ-dimension, which makes it hard to track down for the unfamiliar reader.  Furthermore, Feldman 2008 states result without any reference to margin complexity.  So, it would be helpful to mention this to the reader or state the location (e.g., see proof of Theorem 5.4 in Feldman 2008).  Line 258: label-non-adaptive?  