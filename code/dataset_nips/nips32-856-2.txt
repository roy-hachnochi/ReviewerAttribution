UPDATE: After reading the author feedback I would like to keep my score and stick to the recommendation of introducing a more intuitive, graphical explanation of the difference between MMP and MAML, similarly to how my question has been answered.  Novelty:  The approach seems to be quiet novel, as well as the convergence results. I am not an expert in this, but I think it should not be difficult to provide similar converge guarantees e.g. for the standard MAML, and their absence is mostly due to lack of interest from the community. However, it is important that finally such results are available.  Quality:  The proposed Meta-MinimatchProx is a sound framework, analysed well both from theoretical and experimental perspectives. I have only a few questions / suggestions to the authors.  I would appreciate more discussion of the parameter lambda and how one should set it. It must have something to do with the (meta-)generalisation performance as it is the strength of the prior in some sense, and if itâ€™s the case it would be interesting to see a graph of the test performance, e.g. on ImageNet, depending on lambda.   Clarity:   The paper is written well and generally easy to follow.   I have only a few comments: 1. The distinction between MAML and Meta-MinimatchProx updates discussed on lines 132-151 could be made clearer and perhaps supplied with a graphical illustration. 2. It looks like function F(w) appears first in eq. 3 and is defined implicitly.   Significance:  The paper makes an interesting contribution to a very important area of meta-learning and, I think, will be recognised by the community. 