 The paper is well-motivated and introduce a novel tunable class of losses for (DNN) classification by replacing the usual logistic loss. The experiments demonstrate the gain obtained by using this biparametric logistic loss and improve AISTATS'19  - mention general deformed logarithm and exponential (integral of a monotonous function) and then introduce its specialization of Eq. 1 Cite the book of Jan Naudts : "Generalised Thermostatistics", Springer.  - explain that parameters are called temperature because of their use in thermostatistics [14]. I think the paper (and notably the abstract) will gain in readibility by not mentioning "temperature" of generalized thermostatistics.  - cite book of Amari 2016 "Information Geometry and Its Applications", mention conformal flattening of Tsallis relative entropy, and escort distributions  - need to state whether domain is open (convex) or not, and whether the Bregman generator of Legendre-type or not  - Should better explain "However, the Tsallis based divergences do not result in proper loss functions" (AISTATS 19 paper)  Minor typos: - Kullback Leibler divergence -> Kullback-Leibler divergence - typo 106 -> Kullback-Leibler (KL) divergence