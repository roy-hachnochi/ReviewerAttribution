The paper is sound and technically correct. There are contributions but I would not term them as significant.   I'm not familiar with standard data sets in weak supervision for sequential data and thus I cannot assess the relevance and soundness of the computational study. The improvement is definitely significant, to the extent that it leans towards "too good to be true."  I think algorithms based on graphical models (variational inference, Gibbs) should be added as benchmarks.   The paper is very well written; easy to understand and without mistakes as far as I'm concerned.   Based on the title I was expecting a stronger connection with classification. After all, the end goal is classification. Computational experiments are definitely about classification and there is the loss function bound stated in 'End Model Generalization.' However, this is a bound on loss and not generalization to unseen data. Besides, the bound is a direct consequence of Theorem. In summary, a true generalization bound would be much more significant.