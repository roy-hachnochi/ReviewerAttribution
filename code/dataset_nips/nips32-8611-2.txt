The paper builds on recent work on orthogonal machine learning, efficient policy learning, and continuous-action policy learning. The theory is solid and it is a reasonable advance with good empirical results.  The results are most similar to Foster and Syrgkanis [9], which makes the present paper less novel and interesting. The big improvement over [9], per the authors, is that they provide an analysis giving a regret guarantee for the unpenalized policy learning approach that depends on the efficient variance of estimating the optimal policy. [9], in contrast, provided a bound that depended on the variance of estimating any policy in the class, and only show a regret bound depending on the variance of estimating the optimal policy for a modified learning procedure that penalizes the second moment.  First, I'm not sure how groundbreaking improving the bound to depend on the efficient variance of the optimal policy instead of an arbitrary policy is. Sure, it's better, but is it of sufficient general interest to merit acceptance?  Second, I'm not sure I buy the authors' claim about the computational issues of variance regularization. Optimizing the the orthogonal machine learning estimate over policies is already not convex and rather intractable (and the authors don't explain how they actually optimize it in section 4!). So adding a variance regularizer does not destroy any convexity. Also, unlike the policy optimization objective, which is difficult to convexify for continuous actions (where for binary actions one usually uses classification surrogate losses), the variance regularization has very nice convexifications via phi-divergences / distributionally robust optimization.  Post-response: I have read the authors' response and I find that it appropriately and sufficiently addresses my questions about comparison to [9] and the tractability of the different optimization problems. But I would suggest the authors to add discussion reflecting the explanations they offered in the response into the text at camera ready.