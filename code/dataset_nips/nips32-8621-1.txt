-- Paper Summary --  Multi-output/multi-task Gaussian processes are highly suitable for several engineering and physical problems where multiple signals are associated with a given input. Although this problem has been thoroughly investigated in set-ups where every task or output is dependent on the same input or input space, it is also possible for related tasks to have different supports, where only aggregate data is available instead. In this work, a flexible model is developed for handling such data whereby each task can be assigned a different likelihood, and the input for each task can be of a different form or have different resolution. The effectiveness of this method is showcased using a synthetic example and two real-world problems.   -- Writing/Clarity --  The paper is well-written - the broad motivation for developing the proposed model is clearly expressed in the opening sections of the paper, while the differences to similar work are succinctly highlighted in both the introduction and related work sections. I appreciated having a dual discussion of related work both before and after presenting the model since the contributions were contextualised better in this manner. I do however think that visualising some of the information in the form of graphical models (for depicting the model) and tables (for showing which features appear in related papers vs. this model - for example using ticks for model features/capabilities) could further clarify the sometimes overly-dense discussion.  Further miscellaneous comments:  - Spotted typos: L3: city, region or countries -> ‘city, region, or country’ or ‘cities, regions, or countries’; L15/122: this is -> that is/i.e.; L24: these types -> this category; L67: single task setting, we .. -> single task setting. We then…; L71/222: ‘Geostatistics’ doesn’t need to be capitalised; L74: mean zero -> zero mean; L91: incorrect punctuation in formula; L111: weight -> weigh; L124: has not a -> does not have a;  L282/292/F3 caption: bias -> biased; L285: consisting ‘of’ observations; L290: available a at -> available at a; L313: demonstrated -> introduced;  - Is there a citation for the variational EM procedure introduced in L183? - The bars in Figure 1 are difficult to interpret properly even when printed in colour; - Some capitalisation is required in the references, e.g. Bayesian, etc. Some references include links whereas others don’t - try to commit to a single format;   -- Originality and Significance --  Due to the applied setting in which the problems investigated in this work typically arise, variations  of this problem have been investigated in several other works, most recently by Law et al. (2018). There are several similar elements to this work, such as the use of stochastic variational inference for scalability and parameter optimisation, but the aforementioned work was primarily focused on count problems having a Poisson likelihood. This submission also places a greater emphasis on multi-task learning, while leveraging the methodology developed in Moreno-Muñoz et al (2018) for obtaining a coregionalisation scheme and heterogeneous predictions for different tasks. Parallels are also drawn to multiple instance learning, although the flexibility of using different likelihoods per task extends beyond the standard regression and classification problems considered in those problems.  In view of the above, I believe that the paper succeeds in proposing and developing a very flexible framework for such problems, which generalises previous disparate work targeting a similar goal. The inclusion of a sensible inducing points framework also permits scalability to large problems, which further reinforces this work’s appeal for application to practical problems. Perhaps one of my complaints in this regard is that the problems chosen in the experiments are fairly conservative in all aspects regarding size, choice of likelihood, and number of tasks. While I appreciate that limiting the analysis to two tasks allows for easier visualisation of the results, I would have liked to see more difficult problems being considered. I am also slightly concerned with the assumption that ‘the correlation between tasks will remain constant over the whole domain’ [L320]. To the best of my understanding, this is very rarely the case in practice, where it is instead highly likely for the degree of correlation to be dependent on the location in the input-space. Given how the authors claim that this should only involve a minor extension of the currently presented model, I would highly encourage the authors to include this extension within the paper itself - this could then be investigated further through additional synthetic experiments.   Given that this methodology is amenable to data with varying quality available at different resolutions/granularity, I would also expect some connection to be made to multi-fidelity modelling, which similarly leverages the availability of abundant lower fidelity data to improve high-fidelity predictions for which data is more scarce. Although I can immediately see the difference between the two set-ups in many aspects, e.g. lack of explicit ordering between tasks, varying support, etc, I believe there is still an interesting discussion to be made here with reference to basic multi-fidelity modelling and more recent state-of-the-art variations.   -- Technical Quality/Evaluation --  The formulation of the model appears to be correct, and I did not spot any particular issues while going through the paper. Given that other competing models are not as flexible as what is being proposed here, the evaluation is consequently quite one-sided whereby the model is mostly considered against a very plain baseline. In this sense, the evaluation tends to seem more like a ‘demo’ at times. Nonetheless, the inclusion of a statistical test for verifying the contribution of the proposed model towards the observed results is a nice touch. However, I think it should still be possible to compare against the framework of Law et al (2018) in an experiment dealing with count data?  It would be nice to have some details on the availability of the code - will this be implemented as an extension to GPy, GPflow or some other widely-used library?   -- Overall recommendation --  I think this a solid paper overall - the work encompasses several recent developments in the literature on multi-task problems, and the proposed model has lots of potential use-cases due to its flexibility, even if these are currently not extensively showcased in the experiments. Nevertheless, I believe this paper should be considered for publication if the authors commit to carrying out further refinements and improvements.  P.S. A paper having similar goals appeared on arXiv shortly following the NeurIPS deadline: “Multi-resolution Multi-task Gaussian Processes”, by Hamelijnck et al. In order to judge this submission fairly, I ignored the aforementioned paper when evaluating this work. Optionally, the authors may choose to briefly comment on the similarities/differences to this work.   ** Post-rebuttal update **  I thank the authors for preparing a detailed rebuttal. On the topic of input-dependent coregionalisation, I would possibly opt for a slightly expanded discussion of the comment provided in the rebuttal rather than discarding its mention entirely. While not essential to the contribution developed in the paper, it remains a logical extension to the work, which is why some form of discussion would be appreciated. Alongside the new experiment summarised in the rebuttal, I also highly encourage the authors to identify more examples which showcase the flexibility of the model as described in the text. While I appreciate that it could be difficult to identify real-world problems which simultaneously showcase all of the model’s capabilities, this concern was also echoed in another review, which is why I believe it should be given due attention. Improving the overall presentation of the paper based on other suggestions provided in the reviews should also simplify the more the cluttered segments of the paper, and I look forward to reading an updated version which implements all of the aforementioned changes.