To build gradient boosted decision trees, best possible feature split should be determined with the cost proportional to the number of examples. Sub-sampling from the training data can be carried out to make this task applicable for big datasets and improve the generalization ability of the model.   This paper presents a non-uniform sampling strategy for stochastic gradient boosting (SGB). Namely,  probabilities which are used for weighted sampling from the original training data are optimized with the objective of maximizing the estimation accuracy of splitting scores in the trees. Also, a closed-form solution is presented for this optimization problem. However, in the solution, the threshold is a parameter that should be tuned, where authors suggested using binary search.    In this algorithm, the examples with gradient lower than a threshold are selected with probability equal to one, and examples with gradients greater than the threshold are selected using the probabilities obtained from the solution of the presented optimization formulation.    The issue addressed in this paper is interesting and optimizing the sampling probabilities in GBDT is understudied as it is also mentioned in the paper.  In my opinion, the author can re-organize the paper to make it more clear. Namely, pseudo-code of the proposed algorithm can be included in the text. Additionally, in the experiments section, there is a lack of explanation and discussion of the results (e.g. Figure 1 doesn't have any explanation). The lack of space can be compensated by eliminating Gradient gradient boosting pseudo-code. Also, the proof for Theorem 1 can move to the supplementary materials.   In the learning time comparison, the tuning time is not included (or mentioned). In MVS, the sampling ratio is tuned, which means that we are fitting a smaller portion of data and consequently training will be faster. At least, this issue should be mentioned in the explanation.  