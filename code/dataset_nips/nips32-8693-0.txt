Though the paper is clearly written and presents an interesting approach for large model training. It also allows a GPU to train models that cannot fit into the GPU on-device memory. However, it completely ignores how fast a model can be trained after rematerialization. Given that it already takes a significant amount of time to train a large model on a single GPU and there are many alternatives to train large models without sacrificing too much on computation, it does not seem like the paper provides strong enough evidence over choosing an alternative approach.   1. The paper claims that the "feasibility" of training large models as the main motivation, but the memory itself is not the only bottleneck and the paper completely ignores the training speed side of the story. Training large models such as Bert on a single GPU already takes more than hundreds of days (e.g., Titan V), without specialized optimization. The evaluation results show that the reduced memory footprint is at the cost of 3-4 times increasing of the schedule length, which leads to at least 3-4 times increase of training time. The actual training time could be prohibitively slow, and sacrificing computation speed would further increase the already unbearable long training time and seems to be equally infeasible. It would be better that the paper reports the actual training time after rematerialization.   2. The main motivation of performing rematerialization is to make it possible to train a large model with limited memory, but the paper fails to discuss many alternatives that address the same problem. For example, existing DL frameworks such as TensorFlow and PyTorch support model parallelism, where it partitions the computation graph of a large model and uses aggregated device memory for training.  On a single node, there are approaches that reduce memory consumption by reusing memory regions [1], or using unified memory that allows training to use both CPU and GPU memory [2, 3].  Given that, it is not clear the advantage of this work as compared with those existing work.  3. A large portion of the proposed technique is from rematerialization techniques in compilers, where tree decomposition is used to optimize register allocation. Algorithm 1 seems to be fairly generic and does not rely too much on any domain knowledge of neural networks. The technical contribution seems to be incremental.   Minor: It seems to be possible that an increased schedule length can lead to much longer actual execution time. Is it possible that although the schedule length increases only 3-4 times, the new schedule may cause the computation to lose potential parallelism opportunities, which may increase the critical path and negatively impact the GPU utilization rate, leading to significantly longer execution time? It would be better to report the actual end-to-end training time.     [1] K. Shirahata, Y. Tomita, and A. Ike. 2016. Memory reduction method for deep neural network training. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing. [2] Chen Meng, Minmin Sun, Jun Yang, Minghui Qiu, and Yang Gu. 2017. Training deeper models by GPU memory optimization on TensorFlow. In Proc. of ML Systems Workshop in NIPS [3] M. Rhu, N. Gimelshein, J. Clemons, A. Zulfiqar, and S. W. Keckler. 2016. vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design. ArXiv e-prints (Feb. 2016). arXiv:1602.08124  ===================== After author response ==================== After reading the author's response, I increased my score from 5 to 6 because although the technique has been applied to other domains, it has a good problem formalization and theoretical analysis of trading computation for memory consumption under the ML/DL context. It might not be a bad thing to release a paper which offers a different line of solving the memory consumption challenges in large-scale model training. That said, it would be better to consider the computation constraint while optimizing the memory usage. 