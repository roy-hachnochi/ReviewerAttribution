The paper proposes a method for improving convergence rates of RL algorithms when one has access to a set of state-only expert demonstrations. The method works by modifying the given MDP so that the episode terminates whenever the agent leaves the set of states that had high-probability under the expert demonstrations. The paper then proves an upper bound on the regret incurred using their algorithm (as compared to the expert) in terms of the regret for the RL algorithm that is used to solve the modified MDP. The paper presents a set of experiments showing that the proposed mechanism can effectively strike a tradeoff between convergence rate and optimality.  The clarity of the exposition is quite high, and the paper is easy to follow. The idea behind the algorithm is not particularly novel. The theoretical results presented in the paper seem significant in that they lend theoretical support to an idea that makes good intuitive sense, however I am not sufficiently familiar with theoretical work in this area to properly judge the quality and significance of the proved results. One quibble I might state is that nothing is proved about the main motivation behind the algorithm, i.e. improved convergence rates. But maybe such proofs are difficult to pull off. The reported experiments are simple and intuitively appealing, however there is much more that could have been done to characterize the proposed method; the most notable omission is the extension to non-tabular environments.  Other comments:  I do not understand Definition 5.1. It seems to be taking the argmin of a boolean-valued function. Is the argmin necessary?  Quibble: Line 36: "terminate the episode with a large penalty when the agent leaves this set" is a bit misleading since in the experiments and theorems below that agents are just given 0 reward when they leave the support supersets.  The recent paper "Off-Policy Deep Reinforcement Learning without Exploration" may be of interest, as it explores similar ideas of constraining policies to the regions explored by the expert, but under somewhat different assumptions.  Another quibble I have (this is maybe more of a personal thing, and as such won't affect my score) is that the title is not very descriptive of the work. First, the "in Training" feels unnecessary; surely the majority of improvements in machine learning are about improving training methods. That leaves us with "Fast Agent Resetting". But isn't it true that the point of the work is not that the proposed resetting procedure is particularly fast, but rather that the proposed resetting policy makes *convergence* fast? Thus if I could title the work myself it would be some refined version of "Improving Convergence Rates in Reinforcement Learning Through Adaptive Resetting". This could also be improved to indicate that the resetting is based on expert demonstrations, i.e. to signal to potential readers that the work relates to Learning from Demonstrations.  The experiments that were shown are tidy and nicely interpretable, but do not go very far in characterizing the proposed method. Indeed a single tabular environment was tested. It would be a big improvement to see this method applied in any kind of non-tabular environment, and this doesn't seem like such a big ask given that the paper describes this as "straightforward" in the final paragraph.  Another thing I would like to see tested is how performance scales as one changes the number of expert trajectories provided; in the Q-learning / actor-critic examples, 1000 expert trajectories are provided, which seems like a lot compared to the complexity of the environment.  Typos: Line 52 "general falls" Line 72 should be "model predictive control" rather than "model-predictive control" Line 163 should be \rho_{\pi_d} instead of \rho_{\pi_e} Line 164 missing bar on absolute value for \hat{S}