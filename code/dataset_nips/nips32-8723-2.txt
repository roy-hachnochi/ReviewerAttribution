Post Rebuttal:  The rebuttal did a nice job of changing my mind. In particular, I was glad to see the method work in continuous spaces and with sub-optimal demonstrations.  ------------  This is a well-written paper that is reasonable clear. It introduces the novel ideas of e-stops, which reduces the exploration required, and that such e-stops can be learned from a sequence of observations. The core idea of the paper is that when the agent goes off the rails and is in a bad part of the state space, it should be reset, rather than spending time/samples exploring this area.  The main drawbacks of the paper are * the method only works in discrete MDPs * it's not clear how well the method works relative to other imitation learning methods * it's not clear how well the method will work when the demonstrated trajectories are poor   "We argue that such interventions can... (b) address function approximation convergence issues introduced by covariate shift.": Is this shown? The results are only in discrete domains and I don't think function approximation is used at all.  "We emphasize that our technique for incorporating observations applies to any reinforcement learning algorithm.": This statement seemed to over claim - doesn't it only work on discrete MDPs?  It wasn't clear to me why the method introduced in [5] shouldn't be empirically compared to the proposed method. More generally, there are many methods that combine imitation learning from state-only demonstrations with RL - why are these methods not relevant as empirical baselines? There is a general paragraph at the end of section 2 saying why some existing methods cannot be applied - is the argument that *none* of the existing methods apply in the paper's empirical setting?  Section 4 makes thet claim that "This modified MDP M_\hat{S} [defined in equation (5)] has an optimal policy which achieves at least the same reward as \pi_d on the true MDP M". Doesn't this depend on the quality of the support superset? For instance, if the support superset guides the agent to a sub-optimal terminal state, and does not allow the agent to reach the optimal terminal state, won't the agent in M_\hat{S} be prohibited from finding the optimal terminal state?  In the empirical study, I was expecting to see an experiment where the demonstrations were suboptimal (e.g., travels left and right on the upper edge of the environment). Will the agent still be able to learn a near-optimal policy in this case?  Figure 2 right: What is the x-axis? Is it really floating point operations? If so, why is this a reasonable metric?  I like that the authors tested their method with value iteration, Q-learning, and actor-critic methods.