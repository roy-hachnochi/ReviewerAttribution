Note that a denser version of feature hashing was already considered in [29], under the name "Multiple Hashing". This is an alternative to sparse JL, seen as a preprocessing step that expands each dimension to s dimensions (scaling by 1/sqrt(s)) before applying feature hashing. This defines essentially the same mapping (only difference is whether a set or multiset of s dimensions is selected). Note that this decreases the L_infty/L_2 ratio by exactly a factor sqrt(s), so combining this with [23] yields lower bounds f(m,eps,p) that are exactly sqrt(s) times the bounds in [23] and not far from your tight bounds. This consequence of previous work should be acknowledged in your paper.  Even though the L_infty/L_2 ratio has been used in the past to give bounds on the performance, it does not *characterise* the performance of feature hashing. For example, feature hashing handles a single large entry in a vector well, even though it may send L_infty/L_2 through the roof. It would be nice to have a more fine-grained understanding that has predictive power on any given vector.