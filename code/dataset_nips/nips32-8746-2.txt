1) First, the reviewer thinks that, this algorithm combines both momentum and SARAH estimator in (Nguyen et al, 2017). So, it would be better if the authors rigorously discuss this.  2) On page 1, the authors write \nabla{F}(x) = 0 is a criterion that used in stochastic methods? I think it should be in some clear sense such as expectation, probability 1, etc.   3) I am not sure if [27] provides a lower bound complexity for expectation problems. It only considered a finite sum problem. So far I have not seen any results on lower bound complexity for the expectation problems under standard assumptions, i.e., smoothness and bounded variance.  4) The authors may be missing some recent works based on SARAH estimators such as SPIDER, SpiderBoost, and ProxSARAH which achieve optimal rate in the finite-sum case, and the best-known rate in the expectation case. It should be useful if these works are mentioned. Note that some of these methods can work with single-sample, and still achieve optimal rates.  5)  I don't think it is proper to claim that STORM achieves optimal rate. First, it relies on an additional assumption: bounded gradient apart from other two assumptions. Second, I have not seen any paper studying the lower bound complexity for the expectation problems. So, it is a bit unclear if this complexity is optimal.  6) It is not clear about the assumptions in Section 3. It relies on the smoothness and bounded gradients with probability 1, which seems unclear to me to where it is used. Everything in the proof is relied on expectation.  7) The authors also claim that (page 2 and conclusion), Algorithm 1 resolves the tuning parameters, but this is not true. It also relies on several parameters, at least three: k, c, w. 