This paper explores an interesting question: if we are allowed certain control over the input to a pre-trained language model, can we get it to return an arbitrary sentence? The control given is a vector z associated with the sentence, which is added as a bias to the hidden state at each timestep.  In forward estimation, gradient descent is used to find the optimal z to "bias" the decoder towards a given sentence. In backward estimation, a given z is decoded to find the MAP sentence it encodes (which is intractable in general, so the authors use beam search).  The authors analyze the "effective dimensionality" of a sentence space given a recoverability threshold tau; that is, what's the smallest dimension such that at most a tau-fraction of sentence fail to be encoded? Interestingly, larger models seem to require lower dimensional inputs to recover sentences. The authors only check three sizes of model, but there is a convincing trend here.  I want to like this paper; I'm interested in just how much can be memorized by big LMs, and I feel like this is getting at an important piece of the puzzle. But this feels like one pretty limited experiment and I'm not sure there's enough there for a strong NeurIPS paper.  One technical quibble: I wonder if the results would be different if rather than a random matrix, smaller vectors were still bucketed into K buckets, the model attended over buckets, and those vectors were projected up after attention. I don't think this is equivalent to the matrix multiply version (having a softmax introduces a nonlinearity and fundamentally changes the model). There seems to be a phase transition always around 2x model dimension, which indicates to me that somehow having 4 vectors to look at really gives the model more to work with in some way.  The core experiment is good and gives some interesting results. Perhaps it makes sense that in the limit, we should be able to provide a lower-dimensional indicator of a sentence and recover it, though I'm not sure what the limit of this process is, since I have no idea what the intrinsic dimensionality of language is.  The major caveat in this work is whether the geometry of the z space has been meaningfully studied here. There are several limitations:  (1) This assumes that all we care about are generating sentences in-domain. Gigaword is a much narrower corpus than, e.g., what BERT is trained on, and moreover much of the interest in pre-training is transfer to new settings. It seems disingenuous to pitch the model as encoding "arbitrary" sentences and then they're from the training distribution.  (2) English is assumed to be a proxy for all languages.  (3) More minor, but it seems like the vast majority of such models have moved to some type of generation more closely conditioned on the input (attention of various forms), so I'm not sure how practically useful this insight is for model designers.  I think there's room in an 8-page paper to cover issues #1 and #2 in some depth. The authors don't necessarily need to go in this direction, but I want to see something more.  Overall, I like the direction a lot, but I feel like I can't strongly endorse this as NeurIPS until there's a stronger conclusion that can be drawn.  One presentation note: Section 3.2 could be written more efficiently. I was confused by the lack of detail in lines 106-130, but then this was largely recapitulated on the following page with additional details.  =====================  Thanks for the nice response. I've raised my score to a 6 in light of the out-of-domain results provided; this comparison with random sentences is intriguing and tells us that what we can memorize are arbitrary *sentences* and not arbitrary strings. However, I still have an overall feeling that I'm not quite sure how to contextualize these results based on what's in the paper.