This work borrows from recent research on certifying robustness properties of neural networks on image datasets to Linfinity norm perturbations, and extends this to certification against geometric attacks. More specifically, previous works used sound approximations on the outputs of non-linear activations to certify against worst-case Linfinity based perturbations. Accumulating these approximations gives a (potentially loose) output region that can be certified as robust.  This work extends and develops techniques to handle geometric based attacks such as rotation and scaling.  Overall, I found this paper to be well written. I particularly appreciated the running example style employed in this paper for exposition. I list my suggestions and concerns below:  1. Why is the input always assumed to be perturbed with Linfinity noise? There is no justification for this in the text as far as I can see, its inclusion is unnecessary and somewhat confusing given the main contribution is to defend against other kinds of perturbations.   2. It is surprising that the networks tested have non-negligible robustness to geometric attacks (even the undefended networks). Engstrom et al [1] (and others) have shown that simple attacks such rotating the input usually causes a misclassification, why are the verified networks here seemingly robust to these attacks?   3. The networks verified are extremely small. How difficult would it be to scale to larger datasets / networks? It seems that due to the optimization & sampling mechanism employed this would suffer more than related work (such as IBP [2] which only requires two passes through the network to propagate linear bounds). For example, to reduce sampling error to <= 0.5% takes over 45s - this seems like it has a scalability issue.  4. Adversarial attacks are commonly restricted to small epsilon values for the Linfinity norm since this is highly likely to preserve semantic meaning of the input. Where in the paper do you define a similarity metric under geometric attacks? For example, an a horizontally flipped two digit can be interpreted as a five - yet your certification scheme would not exclude such cases.   [1] Engstrom et al. A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations [2] Gowal et al. On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models