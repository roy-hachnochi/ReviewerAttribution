# Originality This work is primarily built on Hindsight Experience Replay (HER), Behavioural Cloning (BC), and Generative Adversarial Imitation Learning (GAIL). It combined these three works by additionally proposed a new expert relabeling technique. Besides, by replacing the action by the next state they can train the algorithm with state-only demonstrations. Although the novelty of this paper is a little incremental, it combined all the stuff in a reasonable fashion.   # Quality In general, the method proposed in this paper is technically and experimentally sound. On both sides, however, I still have a few questions. First, in Section 4.2, you claimed that the key motivating insight behind the idea of relabeling the expert is that ''if we have the transitions (s_t, a_t, s_{t+1}, g) in a demonstration, we can also consider the transition (s_t, a_t, s_{t+1}, g'=s_{t+k})''. Did you have a rigorously mathematical proof of this statement under which condition it is the case? Because it is well known that a globally optimal policy is not necessary to be locally optimal at each step or each sub-trajectory. Second, on the experimental part, do you have any explanation about why GAIL+HER w/wo ER is better than BC+HER w/wo ER in continuous four rooms but is worse in fetch pick&place?  # Clarity The paper is generally well written. However, in order to be more reader-friendly, the authors had better reorganise the layout to keep texts and their corresponding figures/tables/algorithms on the same paper as much as possible. For example, Algorithm 1 is presented on Page 1 but its first mention is on Page 5. In addition, the paper has some minor typos: e.g., is able -> is able to (line 57); U R -> R (line 5 of Algorithm 1); form -> from (line 109); etc.  # Significance The problem explored in this paper is important and the authors proposed a natural but reasonable solution to it. Built on this, I believe there are still some other directions worth exploring. For instance, as we seen in Figure 4, BC and GAIL with ER and without ER perform quite differently in the two tasks. There must be some reason behind this phenomenon. Is it task-specific or method-specific? If method-specific, what causes the difference? etc. All this should be of great interest and of assistance to the community.  