The paper is very well written and organized and its contributions are quite original as it proposes a novel coarse-ID method for robust model-based reinforcement learning in which both exploration AND exploitation are optimized jointly (which was not the case in previous similar works). The method proposed to solve the robust Reinforcement Learning problem is all the more original as it does not rely on Stochastic Dynamic Programming, but rather on Semidefinite Programming.  Concerning clarity, the only element that is not clear for me is related to equation (1) in page 2: do you consider in the system model some uncertainty in the measurements of the states x ? For example, it is said in the supplemental material that the velocity of the servo-motor of your second experiment is estimated using a high pass-filter, and is hence not perfectly known. If it is modeled, is it included in the process noise w or how do you deal with it ? This seems important as it is rather common not to have access to perfect measurements in controlled systems.  Furthermore, in page 3 you restrict the study to static-gain policies. I believe that it would be useful for non-expert readers to comment on how restrictive this assumption is. You could say for example whether it is a common assumption and give one or two examples of systems which can be properly controlled that way.  I also believe that the authors should comment the approximations made in section 4.1 leading to problem (7), on how the solutions of this convex surrogate problem are close or not to those of the original problem (3). Indeed, this seems to be an important element of the theoretical analysis here, as the method proposed allows to solve (7), while previous methods (DP) seem to solve a gridded version of problem (3). <--- I apologize for this comment. I reckon after reading the authors rebuttal that this question was indeed treated in the manuscript and that I just did not see it. --->  Moreover, no comparisons are made what so ever between the proposed approach and the classical Dynamic Programming approach (or other known Optimal Control methods). For example, in page 7, a complexity analysis of the RRL method proposed is given, but it is never compared to the complexity of DP. This seems crucial as the authors justify RRL in page 3 by the fact that DP is "computationally intractable for systems of even modest dimension ». As far as I’m aware, semidefinite programming problem also suffers heavily from dimensionality, which can indeed be seen in the complexity presented in page 7. In the same direction, experimental results in section 5 never compare RRL to SoTA methods. Also, how much time do the computations take ? Again, the motivation for not solving (3) with a known method seems to be that known methods are not efficient/tractable, and yet no results points toward the conclusion that the proposed method is efficient/practical in higher dimension.  Also concerning experimental results: 1. In figure 2a, what happened in the experiments corresponding to the outliers of the RRL results plot? These seem to be worth investigating/commenting as the RRL method is supposed to deliver robust solutions. 2. In figure 2c, shouldn’t greedy method interpolate between RRL and Nominal, as it is first equal to Nominal than shifted towards RRL ? 3. More experiments with more real-word data would make the paper way more convincing.  All in all, the ideas in this submission seem quite interesting and it could likely be significant for other researchers in the field, but this is somehow mitigated by the lack of experimental and theoretical comparisons to SoTA methods supporting the claimed strength of the new approach proposed.  -----  I have read the authors rebuttal and I am mostly satisfied by their clarifications. I am hence increasing my score under the condition that they add indeed the new experiments and complexity analysis.  PS: I don't understand however why the execution times that the authors are willing to add to their V2 was not included in the rebuttal.