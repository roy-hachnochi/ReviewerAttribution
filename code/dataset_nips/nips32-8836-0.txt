Authors propose DRUM, an end-to-end differentiable rule-based inference method which can be used for mining rules via backprop, and extracting rules from data. Their approach is quite interesting - it can be trained from positive examples only, without negative sampling (this is currently a burden for representation learning algorithms targeting knowledge graphs).  In DRUM, paths in a knowledge graph are represented by a chain of matrix multiplications (this idea is not especially novel - see [1]). For mining rules, authors start from a formulation of the problem where each rule is associated with a confidence weight, and try to maximise the likelihood of training triples by optimising an end-to-end differentiable objective. However, the space of possible rules (and thus the number of parameters as confidence scores) is massive, so authors propose a way of efficiently approximating the rule scores tensor using with another having a lower rank (Eq. (1)), and by using a recurrent neural network conditioned on the rule.  My main concern of this paper is that it does not compare at all with some very relevant work for end-to-end differentiable rule mining, e.g. [2, 3, 4].  Finally, in Tab. 4, authors claim that "The results on FB15k-237 are also very close to SOTA", while from the table it's quite clear they are not (but authors even use bold numbers for their results). Also, in those results, authors evaluate on WN18: this does not seem fair, since [5] shows that one can get WN18 with an extremely simple rule-based baseline, capturing patterns like has_part(X, Y) :- part_of(Y, X).   Rules in Table 7 look fairly interpretable, but also do the ones in [2].   Contributions: - Clever end-to-end differentiable rule-based inference method that allows to learn rules via backprop. - Results seem quite promising in comparison with Neural-LP, but there is loads of work in this specific area it would be great to compare with. - Very clearly written paper.    [1] https://arxiv.org/abs/1506.01094 [2] https://arxiv.org/pdf/1705.11040 [3] https://arxiv.org/abs/1807.08204 [4] https://arxiv.org/abs/1711.04574 [5] https://arxiv.org/abs/1707.01476 