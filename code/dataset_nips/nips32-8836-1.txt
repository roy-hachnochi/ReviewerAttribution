- A comparison with neural link prediction methods ComplEx, TransE or ConvE is good, but not timely anymore. I think by now you have to include the following state-of-the-art methods:   - M3GM -- Pinter and Eisenstein. Predicting Semantic Relations using Global Graph Properties. EMNLP 2018.   - ComplEx-N3 -- Lacroix et al. Canonical Tensor Decomposition for Knowledge Base Completion. 2018.   - HypER -- Balazevic et al. Hypernetwork Knowledge Graph Embeddings. 2018.   - TuckER -- Balazevic et al. TuckER: Tensor Factorization for Knowledge Graph Completion. 2019.   Claims of state-of-the-art performance (L252) do not hold. TuckER, HypER and ComplEx-N3 outperform DRUM on FB15k-237 and all three as well as the inverse model of ConvE outperform DRUM on WN18! Moreover, instead of WN18, I would encourage the authors to use the harder WN18-RR (see Dettmers et al. Convolutional 2d knowledge graph embeddings. AAAI 2018. for details). - It is true that some prior differentiable rule induction work (references [24] and [19] in the paper) was jointly learning entity embeddings and rules. However, at test time one can simply reinitialize entity embeddings randomly so that predictions are solely based on rules. I think that could be a fair and worthwhile comparison to the approach presented here. - How does your approach relate to the random walk for knowledge base population literature?   - Wang. (2015). Joint Information Extraction and Reasoning: A Scalable Statistical Relational Learning Approach   - Gardner. (2014). Incorporating vector space similarity in random walk inference over knowledge bases.   - Gardner. (2013). Improving Learning and Inference in a Large Knowledge-base using Latent Syntactic Cues.   - Wang. (2013). Programming with personalized pagerank: a locally groundable first-order probabilistic logic.    - Lao. (2012). Reading the web with learned syntactic-semantic inference rules.   - Lao. (2011). Random walk inference and learning in a large scale knowledge base.  Other comments/questions: - L66: "if each variable in the rule appears at least twice" -- I think you meant if every variable appears in at least two atoms. - L125: What is R in |R|? The set of relations? - L189: What's the rationale behind using L different RNNs instead of one shared one? - L271: How are the two annotators selected? - L292: Extending the approach to incorporate negative sampling should be trivial, no?   UPDATE: I thank the authors for their response. I am increasing my score by one.