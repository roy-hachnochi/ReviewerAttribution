*** REBUTTAL RESPONSE *** Thank you for the sincere rebuttal. I've changed my review to marginally above accept. I look forward to revisions for clarity and to the expanded empirical evaluations as mentioned in the rebuttal.  *** ORIGINAL REVIEW *** As observed in previous studies certain similarity matching objectives leads to a biologically plausible (local) hebbian/anti-hebbian learning algorithm. This paper introduces a "deep" and "structured" similarity matching objective and derive the corresponding local hebbian/anti-hebbian learning algorithm.  The objective is structured in the sense that constraints are introduced on which input and output neurons pairs should contribute to the matching objective. In particular for image inputs local spatial structure is imposed, akin to convolutional filters. The structured objective is a generalization and becomes the unstructured objective in the limit of no constraints.   The similarity matching objective is deep in the sense that its the sum of similarity measures over multiple layers of a multi-layer neural network.   The paper performs experiments on a toy dataset and on labeled faces in the wild. The main results are the filters learned by the network, which resemble a clear hierarchy of face parts, although it's not clear how the filters were visualized exactly.  w.r.t. originality: In a sense the deep and structured extensions seem to be "just" imported and brought together from previous studies in sparse coding (no small feat to be sure). It is however very interesting to see what this deep, structured hebbian/anti-hebbian learning is actually optimizing for.  w.r.t. quality and clarity: I think the paper is very hard to follow and has several quality issues. In my opinion it could be a lot better. The introduction and abstract are fairly well written, and explains the idea pretty well. However in the following sections much of the math is poorly introduced and hard to follow. For instance W and L are not defined in eq. 1 and y_i is not defined in eq. 2. eq. 2 is given as self-evident, but isn't shown until eq. 9. A "leak term" is mentioned on line 77 which is never defined. I couldn't follow the parts on regularization in eq. 3, and 4 where several results are given as self-evident. At this point in the text the link between eq. 1, 2 and 3 have not yet been made, but the regularization discussion mentions these links as self-evident and makes extensive claims. I'd suggest the authors either take the time and space needed to actually introduce and explain the equations, or simply give them as proven, give the relevant reference and explain what they mean. If you read the referenced papers it becomes more and more clear what the author means by their math, but the paper is simply not understandable as a self contained unit in my opinion. After having spent 2 days trying to understand section 2 I gave up, and skimmed the rest of the math, and tried to understand the idea instead. Maybe I'm just not intelligent enough, although I think I'm fairly close to the average reader, and I think I was more patient than the average reader would be. I suggest significant editing of the manuscript, with a focus on how understandable it is to the average reader and what parts of the math are truly significant to the idea. Nitpick: It's "The credit assignment problem is ..." not "Credit assignment problem is ..."  The experimental results looks good, but are presented without much analysis. The paper introduces several hyper-parameters, e.g. the gamma parameter which determines feedback (top down) strength, the regularization functions, the structural constraints, etc. but doesn't examine how these affect the results or whether they're important. The results are not compared to any other methods and it's not exactly clear how the experiments were performed. I'd not be able to reproduce the experiments.  I want to like this paper, since I think the subject is fascinating, and the direction important, but in the current form I can't recommend it for publication.