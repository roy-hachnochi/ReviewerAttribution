Summary =======  The authors investigate the capability of graph convolutional networks (GCN) to approximate graph moments.  Herein a graph moments is a polynomial functions of the graph's adjacency matrix.   In a theoretical section they analyze necessary conditions on GCNs to be capable of learning  such moments. The main insight here is that:  (1) Exact depth configuration, i.e., a deep enough architecture selection, is necessary to learn a graph moment of certain order (degree of the polynomial).  (2) Residual connections allow to learn also moments of lower order than the maximum order which can be achieved with the selected architecture's depth.   In the light of this considerations the authors propose a unified residual architecture combining three different message passing paradigms.   Experimentally, the authors conduct an ablation study where they evaluate the impact of depth, wideness, and choice of activation function on the performance of regressing different graph moments.  The theoretical findings support the results of those experiments.   In another experimental part the authors evaluate similar architectural design choices in a classification setup on synthetic data (i.e., random graph generation).    Originality ===========  The theoretical result are interesting and original.   Quality =======  Except for one issue the overall quality of this work is good, could, however, by polished a little more to improve readability and clarity (see below).   The main concern is the lack of related work concerning graph moments. Are they introduced by the authors? If not, state where they are originally defined! Do they have application so far? Given that this work is built around graph moments this has to be handled, one way or the other.  Clarity =======  Notation: There should be done some improvement concerning the mathematical notation. The main concern is a lack of explicit introduction of notational convenience which makes things unnecessarily hard to understand, e.g.,   1) p3 l112 what is M_p? shouldn't this be M_p(A) 2) sometimes you use an index such as in f(A)_i, I think you mean the i-th coordinate function? 3) The moments are matrix valued, right? 4) p3 l92 why two times \hat{A}? 5) (other things of similar nature)   Contribution: It seems that the theoretical contribution is the core of this work.  However, the proposed unified layer (using 3 message passing paradigms simultaneously) and the introduction of residual connections are very strongly/(over?) stated and even get an experimental section.  But I do not understand the motivation to present this here as contribution.  For me, it is not clear how the theoretical findings motivate to gather several message passing paradigms in one *module*.   Significance ============  The work is potentially significant. Even more if the importance of moments for graphs could be stated more clearly (see Quality --> related work comment). 