>Originality: Q: Are the tasks or methods new? A: The task is not new, the method is new. Q: Is the work a novel combination of well-known techniques? A: The work is a novel combination of well-known techniques. Q: Is it clear how this work differs from previous contributions? A: Yes. Q: Is related work adequately cited? A: Mostly, except one very related work is missing.  >Quality: Q: Is the submission technically sound? A: I think so. Q: Are claims well supported by theoretical analysis or experimental results? A: Most of the claims are supported by evidence, however there is no evaluation of the claimed contribution of "hypothetical inference". Q: Is this a complete piece of work or work in progress? A: It is a work in progress: there are some important missing experiments. Q: Are the authors careful and honest about evaluating both the strengths and weaknesses of their work? A: It seems so, except there's no clear attempt made to analyze weaknesses of the approach.  >Clarity: Q: Is the submission clearly written? A: The submission is mostly clear, although there are some technical ambiguities. Q: Is it well organized? (If not, please make constructive suggestions for improving its clarity.) A: Yes. Q: Does it adequately inform the reader? (Note: a superbly written paper provides enough information for an expert reader to reproduce its results.) A: I think some readers might be able to implement a version of this approach. The training algorithm is a complex mixture of many different components.  >Significance: Q: Are the results important? A: The results indicate the model is a feasible approach for modeling multi-agent vehicle motion. Q: Are others (researchers or practitioners) likely to use the ideas or build on them? A: I think the graphical model presented holds promise for others to extend. Q: Does the submission address a difficult task in a better way than previous work? A: The approach has strengths with respect to previous work (tractable graphical model allows for exact inference) but they're accompanied by weaknesses (training algorithm is complex). Q: Does it advance the state of the art in a demonstrable way? A: It demonstrably advances the state-of-the-art on the NGSIM dataset, which is somewhat simplistic given its simple context. Q: Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach? A: It sounds like new data (on CARLA) will be released.  Below are my major comments / concerns (unordered):  -- It's unclear what some of the columns in Table 1 mean. Does context mean including past trajectories, image/LIDAR information, or both? (After reading further beyond the table, it appears to be both, but the Table should be clear when it is presented) Isn't DESIRE [21] variational, since it uses a cVAE? (Related: why is "variational" an important attribute of the model to list in the table?). Does "inter-rollouts" mean "interacting rollouts"? What about "inter. encodings"? What is "hypothetical"? This table is almost more confusing than enlightening. The meaning of the attributes should be clear, and each attribute should be meaningful.  -- Equation 5, 6 derivations are unclear (although I am fairly sure they're correct). I didn't follow the derivation of (5), at least not by using the mentioned log derivative trick. The derivation of Eq6 RHS was cryptic as well; I didn't see where Jensen's could be applied. However (6) is derived, please make sure the derivation is clear, and put in appendix if space doesn't allow. Here's my derivation of the RHS of (6) directly, which might be simpler (dropping X, \theta dependence), as it doesn't rely on Jensen's or the log-derivative trick.  log p(y) = sum_z q(z|y) log p(y) = sum_z q(z|y) log [ p(y,z)/p(z|y) ] = sum_z q(z|y) log [ p(y,z) ] + H(q,p)  = sum_z q(z|y) log [ p(y,z) ] + KL(q,p) + H(q) >= sum_z q(z|y) log [ p(y,z) ] + H(q)  (since KL >= 0)  -- L193 "possible to evaluate the exact log-likelihoods of trajectories" It would be helpful here to clarify that [if I'm correct] something like Eq(4) is used to evaluate log-likelihoods. It's not clear what precise units and coordinate system NLL is calculated in (cross-entropy depends on the coordinate system of the trajectories). Can the exact NLL metric be given analytically? Is the NLL computed for each agent's trajectory individually, or the joint (I'm assuming the joint)? Is there some normalization going on in the metric (e.g. by number of agents or by timesteps)?  -- There's some related uncited work that I think the authors should be aware of [A]. Although [A] is too recent to expect quantitative comparison (since it appeared within a month of the beginning of the NeurIPS submission deadline), the work shares many similarities that merit discussion, as it overlaps with the submission's stated contributions: 1) interative and parallel step-wise rollouts for all agents 2) ability to perform "hypothetical inference" / planning. [A] demonstrated using their graphical model and likelihood function to condition on goal states of one agent, and plan the latent variables to infer how other agents might respond, and showed that this inference can improve forecasting performance.   -- There should be some sample quality metric on the CARLA data. The common one (referred to as minADE or minMSD) is computed by fixing a budge of samples, e.g. 10 or 20, and reporting the smalled MSD to the ground-truth sample.  -- There's no comparison to any other state-of-the-art methods on the collected CARLA dataset. That makes the CARLA experiments much less informative w.r.t. to prior work.  -- "Hypothetical inference" was stated as a capability of the model, but there's no qualitative or quantitative results that provide evidence of this capability.  -- Are the latent modes for each agent fairly robust to variation in scene context and the latent modes for the other agents? There's only a single qualitative example that the modes are semantically meaningful (Fig4c). Is there a metric (aside from human verification) that could be used to roughly measure whether the meaning of the modes retains semantic meaning across scenes and variation in other agents? If this claim isn't that integral to the paper, it could be removed.  -- It's unclear what the context is in the NGSIM experiments. Is the visual context used at all? Is it just past trajectory context?  Below are my minor comments / concerns (unordered):  The title is not very informative -- it's hard to predict much about the contents of the paper (even the data domain!) from the title. Please make it more informative. It's not clear 1) that the problem is multi-agent 2) the domain is contextual vehicle motion prediction.  L105 I'm not familiar with a "Discrete(K)" distribution. Perhaps the authors meant "Categorical(K)".  Minor suggestion, use "\big(" and "\big)" for outer parentheses, e.g. after the second equality in (5).  L171 Is it really a KL? The non-RNN terms of Eq(7) RHS look like a cross-entropy -- isn't it d/\theta_z H(p(z|y,x;\theta') , p(z|x;\theta_z) ?  The T and Hz of the modelled trajectories in CARLA are not clear.  [A] "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings." Rhinehart et al. arXiv preprint arXiv:1905.01296  