The paper proposes a new loss function for value function learning. This loss function is the Bellman error measured in a norm based on an ISPD kernel. The motivation is that this loss can be optimized from data without divergence (compared to projected Bellman equation), and can be estimated from data in a consistent way (compared to residual gradient). The method is shown to outperform various baselines when combined with PCL for policy optimization.  This is a strong and well-written work, and I enjoyed reading it. The contribution is a fresh perspective on approximating value functions, and it is nice to see that this creative turn away from the literature (which is heavily based on the projected Bellman equation) leads to strong results, both theoretically and empirically.  The empirical evaluation is convincing, and the example in Figure 1 clearly illustrates the benefit of the proposed approach.  The 3.2 relates the loss function to an error in the value function. However, this error is measured w.r.t. a dual kernel that I found hard to interpret. Can the authors relate it to a more meaningful quantity? E.g., measuring error in L-p norms in very intuitive, and I wonder whether these results can be related in some way.  One issue that wasn’t discussed was the computation time. For N data samples, each gradient step would require O(N^2) computations, which can be limiting for big-data regimes. I wonder if the authors can comment on this.  Minor comments:  66: this notation is a bit strange - what if there are multiple transitions from a state? 66: the parametric V_theta is used before it’s introduced a bit later on.