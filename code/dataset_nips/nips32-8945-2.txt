***Post Rebuttal*** I read the other reviews and the author feedback. The authors clarified my few concerns, thus, I confirm my positive score.  The paper is written in good English and reads very well. I think that this work represents a valuable contribution for NeurIPS and I vote for the acceptance. The proposed objective function, although not unbiased, is consistent, differently from the squared TD error and it can be more easily estimated, avoiding the double sample approach. The analysis of the proposed objective is fairly deep. I liked the connection with the L2 loss and with TD methods. The experimental evaluation succeded in showing the advantages of the approach over state-of-the-art methods, although the presentation of the results can be improved. I also checked the proofs in the appendix and everything seems correct to me. Here are my detailed comments.  - line 66: here the value function depends on the parameter \theta, but the parametric value function is introduced only at line 69 - Figures 1, 2, and 3: there are no labels on the y-axis - Plots are not very readable, especially when the paper is printed in greyscale. I suggest to use different line styles or to introduce markers to differentiate the curves. - Figure 1b: I think that a y scale up to 10^{-30} is not necessary, 10^{-10} would be sufficient. Shouldn't the 0 tick on the y-axis be 10^{0} = 1? - Figure 1c: although they diverge, the light blue and green curves are plotted for very few iterations - Figures 2c and 2d: I did not fully get the meaning of these plots. Can the authors clarify better? -line 277-278: "report the best performance of these four methods". What do the authors mean by "best performance"? Maybe, the best hyperparameter configuration?   ***Typos*** - line 36: blank space before comma - line 264: Fig. 2(c&d) -> Fig. 2 (c&d) - line 273: TD0 -> TD(0) - line 279: bettere -> better - line 280 and caption of Figure 4: Trust PCL -> Trust-PCL - line 472 and 498: footnote should be after the full stop