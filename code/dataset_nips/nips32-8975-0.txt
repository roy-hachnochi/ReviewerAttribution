POST-REBUTTAL: authors have answered my questions satisfactorily, hence I am increasing my score to 7.  I enjoyed reading this clearly written piece.  It's a little bit of a misnomer to call this method "batch normalization", since it does not normalize the "spread", but only the "center". This is justified by the authors describing distributions over SPD matrices (sec 3.2). (Side question: what's the relationship between eq (10) and Wishart distribution? Are they the same, but one with the Riemannian metric? Wishart is also a maximum entropy distribution over SPD.) However, maximum entropy distribution depends not only on the space, but also which sufficient statistics one choses. I do not see an inherent reason why variance-like quantity should be ignored. I suggest changing the name of the method to "batch centering" to avoid confusion.  Since traditional BN improves on learning rate, I wish to see learning curves with this method.  How does this method compare (final performance, and convergence speed) to a naive BN the projection to closest point on the manifold? (perhaps this is a stupid question, but please explain)  Also, the computational complexity of this method for forward and backward pass are not obvious. Can you include them? 