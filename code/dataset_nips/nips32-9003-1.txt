# Originality  The work presents a well thought-out extension of RMs to POMDPs, and a learning algorithm to jointly learn them with state of the art model-free methods. The manuscript contains a sufficient literature review, and makes it clear about the distinctions and improvements from previous work.   # Quality  The overall quality of the work is high. The presented framework and learning method seem theoretical sound (at least under a quick scrutiny), and the experimental results are sound - albeit tiny in content and, possibly, analysis. That said, the authors are careful about their claims, and seem pretty forthcoming regarding the weaknesses of LRM.   # Clarity  The paper is very well written, and mostly clear w.r.t. its notation and presentation. The manuscript and its supplementary materials are dense of both theoretical and implementation details. I feel relatively confident myself and most other RL researchers could have a good attempt at reproducing the method and the experimental settings purely based on the writeup, however I hope that the authors will nonetheless release the code as declared in the manuscript.   # Significance  Reward Machines have yet to become part of the common set of tools used by RL researchers and practitioners, primarily because of how recent they are, and the relatively simple settings that have been used to evaluate them so far. This work doesn't fix this particular issue, however it demonstrates that for a class of common environment settings - i.e. partially observable, sparse-reward - they have the potential of being a good way to incorporate priors about the task structure into policy learning and inference, which is an extremely important topic of research in the RL community.  