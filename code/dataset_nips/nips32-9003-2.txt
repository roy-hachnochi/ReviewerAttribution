The authors apply Reward Machines (RMs) [26] to POMDP problems. While RMs is a automate-based representation of the reward function and originally is proposed for decomposing problems into subproblems, it is used for a kind of belief state representation.  While RMs were given in the previous work, the paper proposes a method of learning RMs from data. The task as learning RMs is formulated as a discrete optimization problem. The authors also show the effectiveness of the proposed method with numerical experiments. The proposed method had much better performance than A3c, ACER, and PPO with LSTM. However, I have the following concerns.  Concerns: [A] It appears to be non-trivial to prepare the labeling function L. It would be useful to give examples of the labeling function in several POMDP tasks. [B] Numerical experiments are weak. It would be required to experiment with various POMDP benchmark tasks or practical tasks. The baseline methods seem to be biased. I think that the proposed method can be regarded as a kind of the model-based approach for POMDP. So model-based method for POMDP like [*] should be included in baseline methods. [C] I could not follow Proof sketch of Theorem 4.3, especially the last sentence. I feel that there is some discrepancy between Definition 4.1 for the perfect RM and the setting of the objective value in (LRM), which is the sum over log(|N_*|).   Minor issue: * In page 13, Theorem 3.* should be Theorem 4.*?   [*] Doshi-Velez, Finale P.; Pfau, David; Wood, Frank; Roy, Nicholas. Bayesian Nonparametric Methods for Partially-Observable Reinforcement Learning, IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume: 37, Issue: 2 , 2015.  