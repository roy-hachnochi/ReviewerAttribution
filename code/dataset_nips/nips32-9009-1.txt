The authors present a self-supervised approach for visual representation learning. The approach is based on maximizing the mutual information between different "views" of the same data. In particular, the model is tasked with predicting features across augmented variations of each input, and across multiple scales. The training is based on a contrastive loss (InfoNCE) which itself a lower bound on the mutual information (if the positive and negative instances to contrast are sampled carefully). Using a large number of stabilisation and regularization tricks, the model can outperform existing algorithms on a variety of standard benchmarks. Most notably, it outperforms Alexnet trained end-to-end (under the linear evaluation protocol) and sets the new state-of-the-art on ImageNet.  The paper is in general well written, but the clarity of exposition can be improved (details below). In contrast to CPC, the model can compute all necessary feature vectors in one forward pass which makes it more computationally attractive. On the other hand, the number of tricks required to make the training stable is stunning. Nevertheless, my score is based on the extremely strong empirical performance and view this work as an instance of "move fast and break things" and the necessary understanding of the success of such methods will follow at a later point. My score is not higher because this work doesn't even cite several highly relevant, information-theoretically backed frameworks for multi-view learning (e.g. [1, 2]).  I have several questions and suggestions in the "improvements" section on the basis of which I will consider updating the score.  [1] https://homes.cs.washington.edu/~sham/papers/ml/info_multi.pdf [2] https://homes.cs.washington.edu/~sham/papers/ml/regression_cca.pdf  ======== Thanks a lot for the strong rebuttal. The new results are impressive and the added ablation tests quantify the importance of each method. I would urge the authors to establish a connection to [1,2], as it provides some theoretical understanding beyond the proposed multi-view approach.