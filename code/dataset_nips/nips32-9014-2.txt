The paper is well-written. The introduced approach is interesting, but the experimental results are not very impressive.   1) How amenable this traditionally difficult saddle point objective would be comparing to VI based approaches on larger datasets?  2) The marginal constraints can also be enforced by matrix operations. It would be interesting to contrast the two approaches.  3) The numbers in Table 1 and Figure 2 seem inconsistent. The figure suggests individual marginals of LBP are better. Is this due to the figure being a single sample or some other thing I am missing?  4) Given the sizes of the dataset considered I wonder whether some architecture of inference networks are better than others (the authors mentioned Transformers and RNN). It would be good to see the results across them.  More questions about experiment details:  1) The form of discrepancy used for constraining the marginals is not clear. Did you use total variation distance, KL divergence or some other discrepancy measure?  2) In the appendix, the authors state that they have treated the random seed of the model also as a hyper-parameter. While it is good to see runs across different seeds, it is not clear to me why the seed has been treated as a hyperparameter.   3) I tried to run the provided code on ptb with the settings similar to that in the paper, and did not get numbers similar to the reported numbers. A glance at the code seems to suggest that the authors probably ran over 100k seeds. This coupled with the previous point makes me uncertain about the stability/reproducibility of the work. I would like the authors to provide the exact parameters with which they got these results.  =========== I have read and considered the authors' response and the other reviews.  