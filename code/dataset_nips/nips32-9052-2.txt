Many of the gains come from a more thorough approach to analyzing the language (e.g. synsets etc) and new finer labels.  A somewhat unfair characterization of this work might be that its gains come primarily from “cleaning up” the data.  I’m surprised that there is no benefit from additional fine-tuning of BERT/ResNet and would appreciate a bit more insight into the design choices that were made regarding the modeling (and/or ablations to this end). 