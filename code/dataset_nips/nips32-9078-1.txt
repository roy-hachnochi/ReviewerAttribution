1.  The primary message of the manuscript - that using a much larger dataset whose properties are somewhat different than those of the testing data provides improved performance of deep 3d convolutional networks - is more interesting to researchers in the application field than to ML researchers.  2.  The use of 3d convolution for protein 3d structures is not new (e.g. ref [17]) as is the overall architecture for partner specific prediction (ref [23]).  3.  The fact that the model is successful despite being presented with examples that do not reflect the unbound structures, by itself does not suggest by itself that the model has learned something about protein flexibility.  In related work on interface prediction using ML, other authors have observed that performance remains quite good even for "hard" examples that exhibit conformational change upon binding.  This kind of statement requires some kind of justification.  Minor comments:  1.  The references are numbered in the Reference list but are cited by author in the manuscript itself.  2.  ref [17] is published in Bioinformatics.  3.  "Xue et al. (2015) demonstrated that partner-specific interface predictors yield much higher performance."  That is a review paper, so they did not demonstrate that.  Better to cite ref [32] for that.  4.  "Due to the homogenous, local, and hierarchical structure of proteins, we selected a three-dimensional convolutional neural network as SASNetâ€™s underlying model" From this description it sounds like you should have chosen graph convolution, which is also rotation/translation invariant.  5.  "In Figure 3B, we see that the dataset size tests yield consistently increasing performance, reflecting the high degree of scalability of our model, and implying that further performance gains could be obtained with larger dataset sizes."  Figure 3B does not imply what is being said here.  Although it does not seem like performance has saturated, that is a possibility.  Furthermore, increasing performance does not reflect scalability. 