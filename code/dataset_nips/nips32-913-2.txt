Applying the idea of Word Mover's Distance to topic distribution representation of documents is interesting.  While the overall idea is good, the evaluation is not good enough.  Although WMD is far inefficient compared with the proposed method, its performance is better presented in the experiment. Instead, WMD-T20 is taken as a baseline.  It is not clear why 20 is determined.  While each of the topic is represented by top 20 words, the number of topics are decided as 70.  So, other size of truncated WMD model should be compared. According to Figure 4 (a), WMD-T20 attains lower error than the proposed method on GloVe.  It shows a quite different performance on word2vec.  Why does this happen? If GloVe is used throughout the experiments, what results are obtained in Figure 5?  In the definition of HOTT on page 3, why do you need to use delta-function?  Since \overline{d^i} is defined as a distribution, a definition like W_1( \overline{d^1}, \overline{d^2} ) looks to be enough.  Or, please make the formulas more clearly either they are distributions on topics or words. 