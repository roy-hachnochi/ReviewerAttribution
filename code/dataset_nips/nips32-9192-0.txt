* Update after rebuttal: After reading the other reviews and the authors' response, I have decided to maintain my score. I appreciate the clarifications about the MNIST-SVHN protocol and the authors' willingness to add pixel-level generation and MVAE-on-CUB results as requested. On the other hand, my questions about the likelihood results remained unaddressed, and I still feel it is misleading that the qualitative visual results in Fig. 7 (via feature look-up) are presented as 'reconstruction' and 'generation'. (There may have been a misunderstanding about what I meant by 'missing modalities'â€”when individual samples may not have all modalities available, rather than a modality being unobserved for the entire dataset. I realise my wording was not very clear, so I am disregarding this point.)  Summary ------------ The authors posit that a good generative model for multi-modal data should be able to achieve latent factorisation, coherent joint and cross generation, and synergy between modalities. They propose a deep generative model based on variational autoencoders (VAE) and mixture-of-experts (MoE), which satisfies these criteria and sidesteps some limitations of existing models. One branch for each modality and a shared latent space.  Originality ------------- The formulation of the desiderata for multi-modal generative modelling data feels quite original. The related work section is detailed and fairly comprehensive, clearly delineating the authors' contribution. In particular, it differs from the closest related method (MVAE; Wu & Goodman, 2018) in significant and non-trivial aspects.  Quality --------- - It is unclear why the authors limit their scope to cases wherein all modalities are fully available during training (lines 65-66), as this could be a deal-breaker for some applications. Could the training objective not be adapted somehow to allow missing modalities? There should be some discussion of this limitation and/or of potential solutions.  - There is no discussion of the likelihood results at the end of Sec. 4.3. What is the message here? What should we have expected to see? Do these numbers confirm the authors' hypotheses? If the expected difference was between the third and fourth columns ('p(xm|xm,xn)' vs. 'p(xm|xm)'), then the effect was minuscule... Also, the sentence describing the calculation of these results is quite confusing (lines 276-279), and this is not helped by the table's lack of a title and caption---which makes it even harder to interpret the results.  - The experiment with the Caltech-UCSD Birds (CUB) dataset are quite preliminary. The authors apply their model on the outputs of a pre-trained deep feature extractor, on the grounds of avoiding blurriness of generated images. They then report 'generated samples' as the nearest-neighbour training images in this feature space. This is a really understated limitation, and for example the visual results shown in Fig. 7 can be very misleading to less attentive readers.  - Although I am not deeply familiar with recent NLP literature, the caption generation results feel fairly unconvincing in terms of text quality. There are also several inconsistencies in language->language and image->language generation, so the authors' claim of 'quality and coherence' (Fig. 7 caption) of the sampled captions appears quite subjective. It would be useful to show many more examples of these five tasks (e.g. in the supplement) to give readers a better picture of what the model is able to learn from this data.  - Why is there no comparison with the MVAE baseline (Wu & Goodman, 2018) on CUB?  Clarity -------- - Generally the exposition is very clear and pleasurable to read.  - I believe it could be helpful for readers if the authors added to lines 112-123 a brief intuitive description of their MoE formulation. For example, my understanding of the mixture-of-experts is that evidence can come from *either* modality, rather than the product-of-experts' assumption that *all* modalities should agree.  - The initial description of the MMVAE objective in lines 124-136 felt a bit convoluted to me, and I am not sure if it is currently adding much value in the main text. In fact, the jump from IWAE straight to the explanation starting from line 137 seems a lot more intuitive to begin with. Since the authors seemed pressed for space, I would actually suggest moving the initial formulation and surrounding discussion to the supplement.  - It would be interesting to briefly justify the choice of factorised Laplace distributions for priors, posteriors and likelihoods, and to clarify how the constraint on the scales is enforced during training.  - In the interest of squeezing in more content, the authors seem to have tampered with some spacing elements: the abstract's lateral margins and the gaps above section headings are visibly reduced. Some additional space could be spared e.g. by deleting the superfluous Sec. 4.1 heading and rephrasing to get rid of dangling short lines of text (e.g. 257, 318, 334).  - Consider increasing the spacing around inline figures (Figs. 1, 3, 5, and 6), as it currently feels very tight and makes the surrounding text a bit harder to read.  - The authors could clarify that MMVAE refers to multi-modal MoE VAE (or MoE multi-modal VAE?), to better distinguish it from Wu & Goodman (2018)'s multi-modal VAE (MVAE).  - The multiple likelihood samples (N=9) in Fig. 4 do not seem to provide any additional information or insight, especially given the heavy speckle noise in the generated images. Here it might be clearer to display just the predicted mean for each sampled latent code.  - The description of the quantitative analysis protocol for MNIST-SVHN (lines 258-267) needs some further clarifications. Is it classifying digits? Are the SVHN and MNIST accuracies computed by feeding through only data from the corresponding modality? To which model does the 94.5% on line 267 refer, and what are the implications of this result?  - Minor fixes:   * Line 18: 'lack *of* explicit labels'   * Line 19: 'provided' => 'provide'   * Line 25: '[...] them (Yildirim, 2014).'   * Lines 41, 43, 63, 72, 145: 'c.f.' => 'cf.'   * Line 64: 'end-to-end'   * Line 166: 'targetted' => 'targeted'   * Line 205: '..' => '.'   * Line 214: 'recap' sounds a bit informal   * Line 238: 'generative' => 'generate'   * Line 293: 'euclidean' => 'Euclidean'   * Ref. Kalchbrenner et al. (2014): author list is messed up  - The authors cite no fewer than 17 arXiv preprints, most of which were subsequently published and have been available for a while. Consider searching for the updated references and citing the published works instead.  Significance ---------------- The authors formalisation of criteria for multi-modal generative modelling is inspiring. The model and inference algorithm could also raise interest and invite extensions. While there are several issues with the evaluation on the more complex CUB dataset, the MNIST-SVHN experiments are interesting and informative, and suggest improvements over the MVAE baseline.