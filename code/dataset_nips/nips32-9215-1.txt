Very nice work. Some minor comments:   1. As in so many NIPS and ICML papers I review these days, the scholarship is somewhat shoddy. There's a vast literature of work on mirror descent based RL, including mirror-descent based safe policy gradient (Thomas et al, NIPS 2013), mirror-descent based sparse Q-learning (Mahadevan and Liu, UAI 2012), mirror-descent based gradient TD (Liu et al, UAI 2015, JAIR 2019), and many more articles in this vein. Please do a more thorough literature review.   2. From the results of Thomas et al., NIPS 2013, one can reinterpret your learning algorithm (Section 3) as doing natural gradient updates. Please read Thomas et al., and summarize your ideas from this perspective.   3. Following Mahadevan and Liu's work on sparse Q-learning, can you provide any sparsity guarantees on your learned policies (which would help in their interpretability).   4. Table 1 needs to be augmented with some of these earlier mirror-descent based approaches. 