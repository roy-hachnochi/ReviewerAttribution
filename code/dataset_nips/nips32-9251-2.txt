This paper is relevant since hashing is important for high-dimensional data, and the proposed method is a simple variant of HBE. The idea of introducing sparseness in estimation is a popular technique in many problems, so the methodology itself is not new. But, adopting it to this setting is a nice attempt, and the space saving should be valuable in practice. The theoretical part is not that strong, perhaps because the idea is simple. Two questions:  1) why is the sampling probability chosen as \delta=1/(n\sqrt\tau)? I didn’t find a clear explanation in the paper. 2) In the theorems and lemmas, by ‘’there exists a data structure…’’ you just refer to the proposed one, right?   Post Rebuttal: After reading the rebuttal and re-reading the paper, I am willing to raise the score from 5 to 6.  