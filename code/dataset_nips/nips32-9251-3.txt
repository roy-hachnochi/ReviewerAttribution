Overall the paper is an average paper but clearly written.  This paper proposes an improvement of Charikar's approach to achieve sublinear kernel density estimation with linear space and linear time preprocessing. Experimental results focus mainly on Laplacian (L1 variant in the main submission and L2 variant added in supplement).  The key observation for achieving linear space is to modify the previous HBE approach so that each hash table stores each point in the dataset with constant probability - in this way, the superlinear storage cost is overcome.  However, my main complaint is in the experimental results. First of all, the authors need to clarify what is meant by "typical kernel value" - I understand the median nearest neighbor distance is estimated and then subsequently scaled to obtain the bandwidth value used for the kernel.  By "typical", does it mean 50 % of the dataset (or any other threshold)? What about the distribution of the raw kernel values themselves and the skewness/kurtosis? From the plots, it is impossible to judge how these factors are accounted to achieve scalability. I think it would be better for the authors to present a more complete picture by showing the scalability as the bandwidth of the kernel is varied.  In summary:  Originality: builds upon the previous hashing-based approach by Charikar et al., though the modification is trivial. Quality: average - I would like to have seen more experimental results. Clarity: The paper is clearly written - though it may be good idea for the authors to correct minor punctuation errors and run-on sentences. Significance: space improvement over the previous approach is novel but the experimental results need to be supplemented for demonstrating a stronger message.