This paper presents a novel outlier measure and an outlier detection algorithm to estimate the proposed measure.  The tree-based measure designed can handle data sets with attributes with different nature (categorical, numerical, binary, etc.) and different scales (for numerical attributes).  The authors propose a random forest-like algorithm to estimate the outlier measure.  They also conduct experiments on 12 data sets where the proposed algorithm achieves the best performance on 6 data sets.    The problem studied in this paper is interesting.  Outlier detection is a classical task with various applications.  Finding outliers in large-scale, heterogeneous data sets remains a challenging problem.    One of the major contributions is the proposed outlier measure.  The authors provide an outlier measure PIDScore defined based on inverse density (i.e. sparsity).  They also point out how this measure unifies binary attributes and numerical attributes and achieves scale invariance for numerical attributes.  I find the outlier measure definition simple yet quite intuitive.    The algorithm provided is a heuristic estimate of the PIDScore.  The algorithm aims to generate subcubes with large variance in terms of sparsity.  However, the authors do not provide detailed explanation on why this is desired.  The authors do not provide theoretical analysis on how well the algorithm can approximate the PIDScore either.  Generally, the clarity of the algorithm needs improvement.    The experimental results show that the algorithm outperforms baselines in some data sets but shows lower (sometimes substantially worse, e.g. on Vowels) performance on other data sets.  Do authors have detailed analysis on why the algorithm does not work well on some data sets?  The authors provide relatively thorough comparison against iForest, which is insightful.  However, the authors may also want to include in-depth comparative analysis of the algorithm against other baselines such as PCA and KNN, especially on data sets where they achieve better performance.    Detailed comments: 1) Line 17, 66, ...: heterogenous -> heterogeneous? 2) Line 168: it in -> it is 3) Line 177: We do not how -> We do not know how  4) Does the algorithm work for numerical attributes with unbounded values?   %=== After Rebuttal ===%  Thanks the authors for their responses.  There are something in the responses that I think is worth being included in the paper.   1) I think the authors could consider to include their comparison between kNN/PCA to their methods into the final paper as it provides a more complete picture to the readers.   2) I also think it is reasonable to include some other possible options other than the proposed algorithm, and why the proposed algorithm is chosen.  This can provide better understanding of the connection between the algorithm and the proposed method, and could also be inspiring for future research.   3) I suggest the authors to also investigate some earlier literature of grid-based clustering, which might be relevant.    With these points addressed, I believe the paper will be more solid.    