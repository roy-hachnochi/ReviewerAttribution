This work is outside by immediate research area, but I found it interesting. I saw examples along the lines of those shown in Figure 3 at least fifteen years ago illustrating the finite capacity of ANNs and the importance of architecture. In the particular context considered here, this is shown very explicitly in the context of the input layer when using ReLU activations. The thresholding nature of these activation functions indeed allow a precise characterization of the behavior. I find it difficult to believe that experts in the field are not aware of this potential behavior but the paper develops the idea considerably and seems interesting.  It seemed to me that one possible use of the developed algorithm would be to provide a post hoc assessment of the adequacy of a trained neural network: given the trained network is there a collision polytope of sufficiently diverse inputs  that one would be unhappy to depend upon the network? I was surprised not to see this in the discussion.  The example in Section 3.1 seems to be somewhat simpler than the state of the art in this area: does one find similar behavior with more complex conv nets with appropriate input features?  l216 founding -> found?  It might be interesting to comment on the extent to which the findings might transfer approximately to other activation functions -- sigmoidal and tanh activations, for example, map large parts of the state space to values very close to +/-1.  Details: l83: the intersection of halfspaces is in general a convex polytope, but can easily be empty. I'd have liked to see some discussion of if/when that situation might arise. l87 infinitely colliding examples -> infinitely many colliding examples?  I thank the reviewers for their response, particularly developing some possible applications on the work, which seem to me to strengthen the work somewhat, and for providing numbers for a more sophisticated network.