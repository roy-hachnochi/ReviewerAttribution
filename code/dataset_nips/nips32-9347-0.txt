The paper targets the problem of measuring the representation power of Graph neural networks (GNNs), an interesting and important topic, that has become popular recently (partially due to two prominent works (Xu et al. ICLR 2019, Morris et al. AAAI 2019)).  There are three main contributions:  1. Establishing the equivalence between two methods for measuring GNN representation power: (i) their ability to approximate permutation invariant functions (ii) their ability to distinguish non-isomorphic graphs. Although not very surprising, this is a nice observation.  2. The authors further suggest reformulating these GNN representation power measures in the language of sigma algebras: Given a class of GNN models, one can naturally associate a sigma algebra (defined on the domain of these models). The authors show that these sigma algebras are an equivalent way to measure representation power of GNNs, for instance, the inclusion of sigma algebras originating from two models is equivalent to saying one model is more powerful than the other. This is a potentially useful observation. I would see it as a stronger contribution if the authors could identify a case in which using the sigma algebra formulation is useful (e.g., easier than using the other representation power measures.)  3. The authors propose a variant of the 2-Graph G-invariant neural networks (Maron et al., ICLR 2019). This variant is shown to be more powerful than many popular graph learning models such as message-passing networks and the 2-G-invariant networks mentioned above. I think this is a very good direction although the experimental section lacks proper comparison on multiple datasets.  Here are a few additional comments/reservations: 1. Clarity: in general the paper is well structured but there are several places that can be improved. For example definition 3 is long and hard to parse, the high-level idea behind  Lemmata 3-4. Similarly, definition 5 is too long. Illustrations might help. 2. Section 4: can the authors discuss the case of infinite K? 3. I am missing a more thorough evaluation of the Ring-GNN. 4. Figure1: can the authors clearly write how each arrow is obtained? 5. The long proofs in the appendix are hard to follow. Can the authors try to make them more accessible? (maybe by using illustrations)?  In general, this paper presents two potentially useful theoretical observations on measuring the expressive power of GNNs and proposes a strong GNN variant that looks promising.  -------------------------------------------------------------------------- Post Rebuttal -------------------------------------------------------------------------- After reading all other reviews and author response I remain on the positive side. The new experimental results strengthen the paper. The SVD part, as other reviewers pointed out, should be changed. I think that the authors should compare to other eigenvalue based methods, or provide results without the SVD part.