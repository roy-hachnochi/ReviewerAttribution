Pros:  1, The theoretical analysis on the connection between the universal approximation of permutation invariant functions and the capacity limitation of GNNs is fundamental.   2, The introduction of sigma-algebra formulation of expressiveness is very novel and interesting.  3, The synthetic experiments on circular skip links graphs are very impressive and interesting.  Cons:  1, Although I like the theoretical results on the universal approximation and limitations of GNNs, it is a bit disappointing that the newly proposed Ring-GNN does not have any guarantee on the universal approximation, if I understood correctly.  2, To me, it seems that the reason why Ring GNN outperforms other models in the Circular Skip Link graphs is that the construction of the adjacency matrix at different layers uses graph information at  different scales. For example, the J + 1 layer adjacency matrix is min(A^2^J, 1). On this perspective, the discussion and comparison with [1] is necessary to me since you can easily treat the exponent of adjacency matrix as a hyperparameter and learn spectral filters in [1]. Moreover, the full SVD is avoided in [1] which breaks the complexity down to O(kN^2) where k is the number of top eigenvalues you want.  3, Details of the model are sparse. Since the space is limited, I suggest authors to trim some of the remarks and lemmas which does not directly help understanding the main results. Instead, you can use the space to properly introduce the G-invariant network which will significantly help readers understand the model in section 5. For example, in definition 5, what is the function rho used in Ring GNN?   4, The computational complexity of Ring GNN is quite high which may be a bottleneck for practical use.  5, The performance of the proposed Ring-GNN on real IMDB datasets is considerably worse than GIN and other baselines. Do you have any idea why this is the case? Also, please provide the reference for the IMDB datasets in the text.  6, The problem setting of the theoretical analysis is different from the practical setting where node feature is presented. Specifically, a graph neural network is not a function which maps an n by n adjacency matrix to scalar but a function which maps a tuple of an n by n adjacency matrix and an n by d node feature to scalar/vector. I wonder how the analysis applies to such a setting.  [1] Liao, R., Zhao, Z., Urtasun, R. and Zemel, R.S., 2019. Lanczosnet: Multi-scale deep graph convolutional networks. arXiv preprint arXiv:1901.01484.  ============================================================= Thanks for the response! The clarification on Ring-GNN and node feature helps me understand the contribution better.  However, I disagree about the comparison with LanczosNet.  First, the randomized starting vector would not be a problem if you just fix it for every run which does not hurt the theoretical guarantee of Lanczos algorithm in general (smart choice of starting vector leads to better convergence).  Second, the functional form of min(A^2^J, 1) will not make a big difference since if the learnable spectral filter (essentially a MLP) in LanczosNet takes the A^2^J as input, min( , 1) can be approximated by the filter in an arbitrarily accurate manner. At last, your example of "hexagon” and “2 disconnected triangles" can be distinguished by LanczosNet. The reason is the eigenvalues/spectrum of the adjacency matrices are different. One is [-2, -1,  2, -1,  1,  1] and the other is [-1,  2, -1, -1,  2, -1]. LanczosNet not only takes the node feature and propagates but also takes the spectrum as input and extract feature from the spectrum which means the final representation of these graphs are different.  Overall, lacking a comparison with GNNs which also leverage the eigenvalues significantly degrades the contribution on the proposed algorithm. On the other hand, if you can remove the SVD part, i.e., every GNNs do not use eigenvalues explicitly, then the comparison seems more fair and convincing.  Therefore, I would like to keep my original score.