Having read the other reviews and the authors' response, I am willing to downgrade my score a tad (due to Reviewer 1's points). But that's still a good score! I didn't quite understand their response to my review; I don't believe I said anything about different size receptive fields.  This paper presents an incredibly simple idea that is effective. Given this, I am not sure whether it has been done before (I don’t know of any other papers that do this exact thing, but I am willing to be corrected).  Hence I am unsure of the originality.  The idea is simple: surround modulation is a pervasive feature of the visual system. A similar surround will reduce the response of a neuron, indicating that nearby neurons sensitive to the same pattern are inhibiting its response.  This is implicated in a large number of phenomena in visual neuroscience, which are listed in the paper. There are at least three potential mechanisms for it, one of which is lateral connections with a difference-of-gaussians (DOG), or center-surround shape. Neurons responsive to the same stimulus nearby enhance response, and a little farther away inhibit it.   In this paper, the authors choose to implement the lateral inhibition, DOG idea. This is implemented as a convolution of a DOG linear filter on the activation maps of half of the first layer of convolutions (before ReLU, I believe). Other variants are also tested, including applying it to all of the initial filters, applying it to the input image instead of the first layer of convolutions, and applying it to the first maxpooling layer.   As a control experiment, another layer is added above the first layer, both with and without the ReLU nonlinearity.   Using a subset of ImageNet, they show that this model learns more quickly and achieves higher performance than the control networks. Two of the variants also perform better than an equivalent convnet. Here, the paper would be clearer by associating the three variants with the labels used in Table 1, although it was clear to me.  They then test the robustness of this model against different lighting conditions using the NORB dataset. The results are quite convincing, in that the network with the DOG convolution is much more robust to the lighting changes than a network without it, by around 15%. My guess is that one should attribute most of this to the DOGs providing a kind of contrast normalization.  They also show that this network is more robust to occlusion in MNIST when compared to a standard convnet.  Finally, they show that the network activations are relatively sparse, in both lifetime and population senses, and that the activations are made more independent by this manipulation.   The paper is very well-written and clear. Lines 62-77 could be considerably shortened, considering the audience is very familiar with these concepts.  I think this paper is significant (modulo my lack of knowledge of it being done before), because it is a simple idea that appears to aid in learning, classification performance, and robustness. I also like the fact that it’s biologically inspired.   Line 51: insert "are" between "and" and "unlikely."  Line 93: reference 39 is also appropriate here (with 37 and 60).  Line 23 of supplementary material: separation is misspelled. 