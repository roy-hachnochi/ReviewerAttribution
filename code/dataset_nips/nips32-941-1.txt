Main Ideas The high level motivation is to combine the strengths of supervised and unsupervised methods for auxiliary task learning. The authors present a meta-learning algorithm that automatically determines labels of auxiliary tasks without manual labels. They study this method in the context of classification.  Relation to Prior Work This is a straightforward application of gradient-based meta-learning. The formulation of tailoring the learning of the label-generator to the learning progress of the multi-task learner is an elegant formualtion of an iterative optimization procedure.  Quality     - strengths: the authors conducted a thorough analysis comparing MAXL with several baselines.     - weakness: It would strengthen the paper to show an experiment that analyzes the weighting coefficient lambda on the entropy term. The authors state the collapsing class problem, but do not show an experiment highlighting why the problem is important. The number of auxiliary classes per primary class seems to be a hyperparameter; it would be informative if the authors could provide an analysis for how to choose this hyperparameter, as according to Figure 3 the choice of hyperparameter has a non-trivial effect on generalization performance.  Clarity     - strengths: the paper is very well written and motivated  Originality     - strengths: the proposed method seems to be novel  Significance     - strengths: MAXL can be in principled be applied to any classification task as long as the number (but not the identity) of auxiliary tasks is pre-defined.     - weakness: While MAXL provides an improvement over single task learning as shown in Table 1, the improvement seems marginal. It would be informative for the authors to include a discussion for why MAXL could not improve generalization performance beyond one percentage point in all of the classification tasks.