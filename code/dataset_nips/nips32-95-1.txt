From what I can tell, the observation that the adversarial perturbation is coupled with only the first layer and the exploitation of this to create the YOLO method is novel and an interesting contribution to the literature that could potentially inspire a lot of follow-up work.  Theorem 1 requires that f is twice continuously differentiable, but in the experiments the authors use ResNets, which have ReLU activation functions. How do the discontinuities in ReLU affect the theory?  A more extensive and detailed experiments section in the main text would be good.  Some additional comments/questions: -- It'd be grammatically more correct to call it "You Propagate Only Once" :) -- Shouldn't Theorem 2 be a Lemma? -- In the description of PGD, it should be eta^r, not eta^m (last line). -- Figures 3a and b are hard to see, they should be made bigger.  -- Consider using a log scale for the y-axis in Figure 3a. -- It's generally better to indicate if the percentages given in tables are test accuracy or error, even if it appears obvious. -- There are a number of typos in the manuscript, for instance: "are the network parameters (line 28); "exploits"(caption of Figure 1); "min-max" (l. 66); "needs" (l. 67); "adversarial" (l. 87); "Contributions" (l. 88); rewrite the sentence starting on line 109, for instance "... is a game where one player tries to maximize, the other to minimize, a payoff functional"; "could significantly" (l. 159); "on the Hamiltonian" (l. 204); "Small CNN" (caption of Figure 3a). -- There's an unresolved reference on line 29 in the supplementary material.