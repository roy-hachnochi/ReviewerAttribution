This paper proposes a new algorithm for computing PGD attack, and applies the proposed algorithm to adversarial training. The experiments show that the computational cost is reduced by 2/3, and achieves the similar performance.   The key insight of this paper comes from a dynamical system view of back propagation algorithm as a recurrent network. This insight, however, actually dates back to 1980's. For example, Le Cun 1988 has discussed this view in Section 3.1  http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf  Moreover, the proposed algorithm is not the only way to approximate the backpropogation algorithm. There have been some truncation heuristics in existing literature, e.g.,   https://arxiv.org/abs/1705.08209.   https://arxiv.org/abs/1807.03396  Williams and Zipser (1992), Gradient-Based Learning Algorithms for Recurrent Networks and Their Computation Complexity.   Williams and Peng (1990), An Efficien Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories,  These truncation heuristics can reduce the computational cost of computing adversarial attacks. The authors did not provide any experiments to justify their method is better than these heuristics.  The application to adversarial training is somehow a novel selling point, since there are not many existing works discussing the importance of truncation in adversarial training.  The authors claim that the training time is reduced by 2/3~4/5. However, this highly depends on the implementation and hidden factors, e.g., library used. All experiments are based on one single realization. I did not see any details on multiple realizations and standard errors. Therefore, more experiments are needed.