Novelty ====== This paper introduces a clear new module into the crowded space of contextual models for semantic segmentation. In addition to the value of the core idea, the authors explain how to differentiate their tree filtering operator,  enabling end-to-end training. Finally, they also propose a technically interesting way to reduce the complexity of computing their tree filtering module down to linear in the number of pixels.    Experiments ========== The experiments are thorough, with lots of comparisons to the state-of-the-art and a clear ablation study. However, the improvements over the state-of-the-art are very minor: +0.6% mIoU over PSP [12] on VOC12 val (table 3), +0.2% mIoU over DenseASPP [32] on Cityscapes test (table 5), and +0.1% mIoU over ExFuse [36] on VOC12 test with MS-COCO pretrain (table 6). In the light of these numbers, I find the results oversold: the abstract states "leading performance on VOC 2012 (86.3% mIoU) and Cityscapes (80.8% mIoU)", exactly referring to the last two results I listed in this review (+0.1% and +0.2%). Moreover, the conclusion states "superiority of the proposed method on VOC12 and Cityscapes". These claims are not justified by such a small delta. It would be small for any system, but especially for neural networks, due to the inherent randomness of their SGD training procedure. The one significant result I could find is +1.3% on VOC12 test set without MS-COCO pretraining, but that's not what is sold in the abstract.  The ablation studies in Tables 1 and 2 show the impact of the proposed new module starting from a system with no context at all. The effect is nice (+2.1% on ResNet101 on VOC12), but the true comparison is: how much more does this new context module bring compared to previous ones? And the cleanest answer I could find is in Table 3: +0.6% over PSP [3] and +0.7% over NL [12]. That is a small effect.   Quality of writing ============= The quality of writing is good, but not great. There are many missing or oddly places articles, incorrect verb conjugations, and so on.   Summary ======= This paper offers good novelty and it is technically interesting. However, the results are underwhelming and the comparisons to the state-of-the-art are very oversold.    Reaction to rebuttal =============== The authors have provided some reply to my critique that the improvements over the state-of-the-art are small, mainly with one new experiment and by promising to include dilated convolutions to improve performance of their backbone network. Most importantly, they promised to revise their claims in the final version. This is very important. If the delta improvements remain as small as they were in the submission version, then their claims must be toned down. In the light of the rebuttal, I keep my original score and trust the authors to keep their promise.  