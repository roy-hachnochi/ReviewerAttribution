Thank you for the opportunity to review this interesting and well-written manuscript. I
have focused on statistical aspects, and have the following comments
1) At the start of the results, there needs to be more discussion on the included
populations of the trials and whether they are broadly similar across them. This is crucial to
the understanding of whether these trials and their populations are in some sense
exchangeable, such that the consistency assumption is likely to hold (in advance of data
analysis). i.e. that it is sensible to combine these trials in a network meta-analysis. When I
look at the table of study characteristics I see trials from different countries, different
settings (eg primary and secondary care), different age range (some all ages, other upto
just 45)and different lengths of follow-up (eg 12 and 18 months), which do raise concerns
about the distribution of potential effect modifiers being different for studies that give
direct and indirect evidence.
2) The authors say in the results section that: “Direct evidence was therefore available for
nine of the 10 possible comparisons. One RCT was a cluster-randomised trial. We therefore
used the cluster size and the intra-cluster correlation coefficient to reduce the size of the
trial to its “effective sample size” before we carried out any data pooling” –the authors
should explain this approach in the methods section.
3) In addition to the heat plot, it would useful to have a statistical measure or test of
whether there is any evidence of inconsistency across the network MA overall. I do not
find the heat plot easy to conclude whether there is inconsistency or not. The consistency
assumption can also be examined across the whole network using design-by-treatment

interaction models, which allow an overall significance test for inconsistency. (of course this
may have low power; see introduction by Riley RD, Jackson D, Salanti G, et al. Multivariate
and network meta-analysis of multiple outcomes and multiple treatments: rationale,
concepts, and examples. BMJ 2017;358:j3932)
4) The authors say the “The P-score is the probability of each treatment being ranked as
best in the network analysis” – this is not correct, or at least it not quite exact. Readers
may interpret this as the probability of being ranked first, when actually the P-score has
the same interpretation as the SUCRA, which is the area under a line plot of the cumulative
probability over ranks (from most effective to least effective) and is just the mean rank
scaled to be between 0 and 1. I think the mean rank is more interpretable. Could this also
be provided? Also the SUCRA graphs are needed, and a graph giving the probability of each
rank (See the aforementioned Riley et al. paper for examples)
5) “We performed a network meta-analysis using the frequentist model, with the statistical
package “netmeta”” – we need more details than just referring to a stats package, as there
are many options therein. In particular, what estimation method was used (REML), were
random treatment effects assumed, was the heterogeneity assumed the same for each
treatment effect, what was used in the pooling (RRs or the raw numbers of events/total),
etc? Also whether a two-stage or one-stage meta-analysis modelling approach was used,
and whether the confidence intervals accounted for uncertainty in the estimate of
tau-squared, for example using the Hartung-Knapp method?
https://www.ncbi.nlm.nih.gov/pubmed/30067315
6) I-squared is a poor direct measure of heterogeneity, and we should rather be seeing the
estimate(s) of tau-squared, the actual between-study variance estimate(s). (Rucker G,
Schwarzer G, Carpenter JR, et al. Undue reliance on I(2) in assessing heterogeneity may
mislead. BMC Med Res Methodol 2008;8:79)
7) How were multiple (i.e. 2 or more) treatment effects from the same study handled in
the analysis (as they are correlated due to the common control group)?
8) In the results, there is a major emphasis on the P-score, but we do not get told the
magnitude of treatment effects and their CIs. This needs to be clearer in the text as well as
the tables.
9) For the outcome of being asymptomatic (see Figure 2) we have wide CIs that include
the null, and so we might even want further evidence to clarify that there is a benefit for
any of these treatments, before ranking them. Whereas there is clearer evidence that
test-treat has significantly fewer endoscopies than other approaches, it less clear for other
outcomes. Actually the authors already hint at this when they say “but none of the
strategies was significantly less effective than “test and treat”, or more effective than each
other, on direct or indirect comparison” – so is the following conclusion actually justified:
- ““test and treat” is likely to be the most effective first-line strategy for the management
of uninvestigated dyspepsia in primary care”
CIs might be even wider following a Bayesian approach or when using the Hartng-Knapp
correction
10) The last follow-up time is the key time-point, but as mentioned this varies across trials.
This will lead to heterogeneity in the RRs and make it harder to interpret. This should be
discussed. I assume hazard ratios were not available?
11) Table 3 and 4– I am not sure the words ‘league table’ are correct or needed – simply
say summary treatment effect estimates from the network meta-analysis. As mentioned,
we need all the treatment effects for all pairs.

12) Usually we are shown the direct evidence results too for each treatment effect; a forest
plot for each would be welcome, to reveal study specific results. Indeed, I cannot see study
specific results in this article at all.
13) None of the studies were at low risk of bias. Thus, in addition to the summary results
from the NMA having wide CIs that include the null, this perhaps gives extra credence for
not making strong conclusions about the best treatments from these network
meta-analysis results.
14) The abstract does not explain what the P-score is, and I think readers will confuse it
with a p-value. Suggest that the authors define it or, better still, rather give the mean
rank. As mentioned, the abstract should also discuss the uncertainty of the ranking, i.e.
that comparisons of key treatments have wide CIs
15) The authors say “We used a RR of remaining symptomatic at the final point of
follow-up” – but earlier say “We extracted all endpoints at a minimum of 12 months, even
for RCTs providing effectiveness data at other time points. We did this to ensure as much
homogeneity as possible between individual trial results, and to avoid overestimating the
effectiveness of one management strategy relative to another”
- thus taking the final point of follow-up does not seem sensible. Why not choose
time-points that were similar to each? Eg do a separate meta-analysis at 12 months, and
then at 2 years etc? This is the best way to improve homogeneity, rather than taking the
final follow-up times.
I thank the authors for considering my comments and I hope they are useful going
forward.
Best wishes, Prof Richard Riley
