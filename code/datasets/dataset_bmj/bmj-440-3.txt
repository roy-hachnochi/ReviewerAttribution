Bellou et al. present a systematic review of prognostic models for COPD. The paper is highly
descriptive providing an overview the characteristics of the models. The authors have done
an extraordinary work in identifying and summarizing the existing COPD models and they
are to be congratulated for the enormous amount of work put this endeavor. The paper
adheres to recently developed reporting guidelines in the field of prognostic models and also
applies novel “risk of bias” tools. These are important strengths.
The paper is in general clearly written although it is “dense” and somewhat difficult to follow
at places. This is somewhat understandable because the scope of the paper was (very?)
broad. I.e. the authors describe models that predict any outcome among COPD patients. The
manuscript is methodologically rigorous and of high quality.
My major comment has to do with the “so, what” question. As expected, most of the COPD
models are not externally validated. The lack of external validation has been documented in
various areas in the field of risk prediction. Based on the characteristics of the existing
models, it looks like most of them are not ready for prime-time use in clinical practice – and
indeed the current practice seems to be that their use is limited. I think it would be helpful
to know how many of the externally validated models were successfully validated? Supp
Table 3 lists “External validation studies of 41 prognostic models for COPD-related outcomes
in independent populations”. Are these all successful validations?
My other point has to do with the heterogeneous set of outcomes (and populations). The
authors deemed eligible any outcome, any clinical setting, any type of data source, and any
type of predictors. Although this is helpful for presenting the overall status of the field, it
does not allow for a more granular presentation of the model characteristics. E.g.
distributions of age, gender etc.; clinical characteristics of the populations; ascertainment of
outcomes, predictors, etc. The current focus of the manuscript is on the statistical aspects of
the models rather than the clinical characteristics.
That being said, the manuscript also focuses on aspects of model performance (AUC, HL
tests, etc.). Although these are important issues, they are of limited value for
decision-making. For the latter, absolute risks are more helpful. The authors present in the
table different aspects of model presentation, some of them alluding to absolute risks. It
would be helpful if they elaborated on these issues and also highlight which aspects of model
presentation are helpful for different stakeholders making decisions – some may value
absolute risks, others may just value adequate classification.
Minor points:
- The authors excluded cross-sectional studies. Why? Their inclusion/exclusion criteria do not
seem to differentiate between incident and prevalent outcomes. [minor to moderate]
- Although cross-sectional models were not eligible, three such models are mentioned in 2-3
places in the results (although not examined in detail)
- I think the outcomes predicted by the eligible models should appear earlier to help readers
put things in clinical context. Right now, there are “buried” after the methodological aspects
of the models. A figure showing these outcomes could help.
- It would be also helpful to report the model selection strategies for the eligible models.
- Risk of bias: these new tools might be helpful, but they rely on first principles rather on
empirical evidence of their performance. Perhaps this should be acknowledged. For example,
RCTs have “moderate” risk while prospective cohort studies have “low risk”. Often, an RCT
cohort (especially the control arm) can be used for model development. The problem arises
when this control arm is not “sampled” to be representative. But the same can happen with
a cohort study: any model is not just of “low value” just because it was developed/validated
in a cohort study regardless of the characteristics of this cohort (including its suitability for
the prediction question at hand). Similarly, if the goal of prediction is related to

heterogeneity of treatment effects, an RCT is probably more appropriate than an cohort
where treatment data may be incomplete.