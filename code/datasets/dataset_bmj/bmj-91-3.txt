I have struggled mightily with my recommendation for this paper, as I simultaneously believe it provides
extremely important preliminary evidence for the effectiveness of social distancing —support which is
desperately needed as social distancing measures are challenged by those who wish to dismiss the
dangers of COVID-19 as “fake news”— but also believe it relies on data that (through no fault of the
authors) is fundamentally flawed.
The authors correctly point out that most data concerning the effectiveness of social distancing from
covid-19 comes from modeling. The greatest strength of this study is its reliance not upon modeling,
which is subject to potentially biasing input and algorithms, but on actual data.
While the use of some modeling techniques (e.g. interrupted time series analysis) remained necessary
to, for example, establish “controls” specific for each country, the primary data input reflected actual
testing results as collected in each country. Unfortunately, this is also the study’s greatest weakness.
Due most directly to the failure to implement a coordinated, consistent testing strategy both globally
and, in most cases, regionally or nationally, rates of positive testing results might well reflect changing
testing practices rather than actual effects on incidence. This is true not only for “total diagnosed
cases”, but also for positive result incidence rates. For example, in the U.S. early shortages of testing
kits led covid-19 testing to be restricted in many areas to only those showing obvious overt symptoms,
or known to have been exposed to others who tested positive. Once testing expanded beyond these
individuals that we had strong independent reason to believe would test positive, we should naturally

expect the incidence ratio of positive test results to lower. Because very little coordination —let alone
supervision and consistency in application — of testing strategies occurred in the U.S., it is nearly
impossible to know how we might account for variable testing practices in any analysis of positive result
incidence. As illustrated by the early testing restrictions just described, there is little consistency in
testing practices even within local testing sites, let alone between such sites. To this day, testing data in
the U.S. remains a mess, subject to political manipulation resulting in the resignation of state health
officials who refuse to “cook the results”, and with newspapers reporting that even CDC data is failing to
properly sort antibody testing from active case testing, completely corrupting the utility of data for
scientific purposes. The WHO has faced similar accusations of subverting epidemiology to political
pressures internationally, further calling into question the accuracy and utility of data collected globally.
Perhaps the strongest scientific conclusion to be drawn from this work (in terms of definitive
knowledge), then, is the illustration of what it might have been possible to know had better coordination
of testing taken place. While data supporting the effectiveness of social distancing is extremely
important, maintaining trust between the public and scientific advisors is even more important. We
must be careful, then, to not mislead or overplay convenient findings, but instead acknowledge the
limitations of what conclusions we can draw. Only by acknowledging our failures in systematic testing
and data collection can we learn from our mistakes and avoid repeating these. Glossing over the flawed
nature of data in order to support a desired conclusion risks violating the trust of the public. I want to
be clear that these criticisms are not directed toward the study authors, who have done admirable work,
but toward the shameful politicization of the global public health infrastructure, which has resulted in
corrupted data.
Nonetheless, the study provides strong -if not definitive for the reasons described above—support for
social distancing. The fact that effectiveness is maintained over so many different data collection
mechanisms and locations (individual countries) is, to me, strongly suggestive of social distancing
effectiveness. In addition, the examination of specific country-by-country social distancing strategies is
—by itself— a valuable resource in pandemic response planning. In essence, I view this study’s results
as I would preliminary data for a grant: what data we have is strongly suggestive of a conclusion, but it
lacks the quality, rigor and consistency needed to definitively rely on its conclusions (thus the need to do
the study one is seeking a grant for). I suspect the study’s conclusions are correct...but we cannot
know this from data collected (by countries, not the authors!) so un-systematically and without care to
detail.
In short, this data is very helpful and suggestive, but would be much more helpful were there better
implementation of testing (specifically, consistent and standardized) such that more reliable conclusions
might be drawn.
Because I believe the study is of such great importance, because I think the authors did such a good job
with the flawed data they have to work with, and because I believe the results are highly suggestive and
conclusions probably correct (though not definitively proven due to unavoidable use of flawed data), I
strongly recommend publication of this important work, but would also strongly suggest it be
accompanied by an independent commentary that highlights the limitations of conclusions that are
based on data that is flawed (through no fault of the authors: they have done the best that can be done
with the flawed data available) — and how this emphasizes the need for greater coordination, and
consistent, systematic implementation of both interventions and, perhaps most importantly, testing
strategies so that legitimate conclusions may inform future response as well as refine current efforts.
This limitation is of sufficient importance to merit its being highlighted in an accompanying commentary,
rather than lost in the minutiae of other study limitations.
