derivation versus validation
assessing the probable clinical course of children presenting
to their care is one of the day to day dilemmas facing
the practising paediatrician. with accurate prognostication,
invasive or expensive treatment may be targeted
towards those most likely to benefit. there is little point in
expending vital resources on, or administering invasive
treatments to, children who are likely to recover without
such intervention. children with certain characteristics
may tend to do better or worse than others. for example,
younger children or those with specific clinical signs might
be expected to deteriorate more rapidly. past experience
often acts as a guide for the experienced paediatrician.
algorithms (or prognostic models) can be developed to
provide a means of transferring expert knowledge to the
novice. for example, apgar scores are routinely used to
assess the health of newborn babies, and apache scores
can be used as a measure of prognostication among admissions
to paediatric intensive care.1 in this issue, brogan and
raffles2 present an algorithm for identifying children
presenting to a&e with fever and petechiae who are at
increased risk of significant bacterial sepsis. it is interesting
to note that of the many prognostic models that are
published each year,3 4 relatively few are sufficiently
validated and fewer still find their way into clinical
practice.5
determining which patients will benefit from treatment
is only one use of prognostic algorithms. they also provide
a means of informing parents of the likely outcome and can
be used in research to give baseline measures of severity in
different groups.
derivation of prognostic models
an experienced clinician accumulates knowledge via the
patients that he or she has cared for from initial presentation
through to final outcome. what has happened to these
cases will inform the future decisions that the clinician
makes. the process can be formalised by documenting
information on newly referred patients who are then
tracked until their outcome is known. when a suitably
sized sample has been collected, statistical techniques may
be used to build an algorithm designed to predict poor
outcome in future patients.
the most common way of deriving a prognostic model
from existing data is via regression analyses. the development
of prognostic models is extensively covered in many
medical statistics texts. from the competing models one
may be chosen that is both practical and statistically
acceptable. in particular, models may be preferred on the
grounds that they require only routinely collected and reliable
data and do not have substantially less predictive ability
than alternative models that require invasive or
non-routine data. decisions may need to be made quickly
and algorithms should be simple and user friendly.
one problem with the development of algorithms for
prognostication is that derivation will be driven by the
available dataset. quirks individual to that dataset may
appear to be prognostic and be encapsulated in the predictive
algorithm. for example, time between symptoms and
presentation may be unrelated to outcome but by chance
those patients with the poorest outcome in the available
dataset tended to present soon after the first appearance of
symptoms. a prognostic model designed to be applied at
presentation which incorporates time from symptoms as a
factor would under diagnose those presenting late.
unusual features or extreme but random differences
between prognostic groups within the development dataset
may not be replicated elsewhere, leading to the creation of
a non-transportable model. where there are many
variables competing for inclusion this problem may be
extreme. hence, analyses that are not pre-specified but are
data dependent are liable to give a better fit than is
obtained when the model is applied elsewhere. the problem
is further compounded when cut off points for
continuous variables are selected to give the best prognostication
based on the development dataset.6 for example,
respiratory rate was dichotomised at 40, 50, 60, 70, and
80/min to identify the cut off point giving the best sensitivity
and specificity for predicting hypoxia in acutely ill
infants.7 although arbitrary thresholds for continuous
variables are not generally recommended,5 they are often
preferred because of the simplicity that they confer on the
final algorithm.
the converse problem is that of under fitting. important
prognostic variables may not be identified in the derivation
dataset and there are several reasons this could occur. random
or chance variation may mean that the values of a
truly prognostic variable are not significantly different
between prognostic groups within the available dataset. for
example, suppose time from first symptoms to presentation
is predictive of outcome but that, in the development dataset,
it just happens that the patients with the worse
outcome were unusual in that they tended to present early.
alternatively prognostic factors may not be identified
because the patient set used for derivation is limited in
some way. for example, age may be highly prognostic but
does not enter into the algorithm because the model was
derived from data collected only from children in a very
narrow age range.
methods of model checking are available with most statistical
computer packages and tend to be well covered in
most regression texts. however, these methods merely
detect whether the chosen model adequately describes the
trends in the dataset on which it has been developed. they
cannot inform on whether the model is suitable for use in
clinical practice or an accurate description of the
population trends.
validation
the actual evidence that the application of a
prognostic model alters medical practice and
improves the outcome of patients has to be
established additionally which is in accordance with
phase iv studies of diagnostic tests.8
model validation is the process whereby the derived (or
fitted) model is shown to be suitable for the purpose for
which it was developed. it addresses the question of
whether the model has wider applicability. as the aim of
most published papers is to present results that will be
generally useful, these are questions of major importance
that cannot be overlooked. surprisingly few medical statis-
tics textbooks discuss techniques for model validation and
most do not even mention its importance or relevance to
model interpretation.
in addition to being user friendly and statistically sound,
a prognostic algorithm needs to be generalisable to be
clinically useful. a model based on variables which have
low reliability will not tend to be valid in a sample other
than that for which it was developed. one of the reasons
published algorithms may not find their way into standard
practice is the lack of evidence that they are applicable to
patients from establishments other than where they were
developed. generalisability needs to be established by testing
the prognostic algorithm in numerous and diverse settings.
9 in the words of a recent article10 on the subject of
model validation, “usefulness is determined by how well a
model works in practice, not by how many zeros there are
in the associated p-values.”
techniques for model validation
it is well recognised that deriving and validating a model on
the same dataset will by definition lead to over optimistic
estimates of the model’s accuracy. an alternative approach
is to split the dataset into two parts, one part for derivation
and the other for validation. a major drawback is that the
precision of the fitted parameters will clearly be reduced as
only a portion of the dataset is used for model derivation.11
a variety of methods have been advocated for dividing the
dataset. automated procedures exist for choosing two
halves that are homogeneous but these will also clearly give
over optimistic estimates of model validity. if some
measure of internal validity is required then it is
recommended that the data are split in a non-random way.
data from different time periods could be used3 and this is
the same as validation using a more recent cohort or prospective
validation using an algorithm derived from a
retrospective dataset. alternatively, one of the less arbitrary
“leave one out” approaches may be used.12 13
external validity
the model should be externally validated by assessing its
applicability to data collected at another centre or by
different individuals. altman and royston10 present a series
of examples of models that have been derived, internally
validated, and then validated elsewhere. they note that
authors tend to confirm the validity of their own models
but that others are less successful at doing so. this finding
could be the result of a form of publication bias; if the
authors’ internal validation was weak it is doubtful that
they would attempt to publish the results. alternatively
there may be real differences between centres and the
model, while being internally valid, is not transportable
and hence of limited use.
statistical versus clinical validity
in general authors show no appreciation of a
distinction between statistically and clinically valid
models.10
a model that is statistically valid will yield unbiased
predictions when applied to new datasets. this quality
however does not necessarily mean that the model has
clinical validity. to be clinically valid the model must be
accurate enough to serve the purpose for which it was
developed. for example, abnormal white cell might be significantly
associated with increased risk of significant bacterial
sepsis among children presenting to a&e with rash.
this finding, even if replicated across hospitals and hence
statistically valid, may be of little clinical relevance if only a
few extra children with poor outcome are identified as a
result. there are different clinical implications if application
of the algorithm means that an additional child in 50
or an additional child in four with poor outcome is identified
early. consideration must of course also be given to
whether there are other children who achieve a worse outcome,
perhaps because treatment is withheld that would
normally have been administered, as a result of applying
the algorithm. similarly a statistically invalid model is not
necessarily clinically invalid. the algorithm may not be
consistent in the extent to which it identifies children with
poor outcome but it may always identify enough to warrant
its implementation on a regular basis. scores need to be
reliable enough for the purpose they were developed for,
even if this reliability is relatively low.14 there will be a trade
off between the additional workload entailed in applying
the model and the likely benefit derived by the patients.
conclusion
simple diagnostic models may be more portable than
more complex models.15
when kennedy et al made the above observation they were
commenting on the statistical aspects of deriving a model
which is then used elsewhere, but it is of course true in
more ways than one. models based only on factors which
are highly related to outcome will be more likely to be
similarly predictive in another setting. complex models
incorporating multiple factors, for some of which any predictive
value may be highly specific to that dataset, are less
likely to be similarly predictive elsewhere. from a practical
viewpoint simple models are more likely to be readily
incorporated into clinical practice with minimal disruption.
model derivation and validation are two separate and
important parts of the same process, the identification of
clinically useful models. it is to be remembered that the
final test of a model should always be whether it is accurate
and generalisable enough for the purpose for which it was
derived.

<|EndOfText|>

abstract
background: minimisation can be used within treatment trials to ensure that prognostic factors
are evenly distributed between treatment groups. the technique is relatively straightforward to
apply but does require running tallies of patient recruitments to be made and some simple
calculations to be performed prior to each allocation. as computing facilities have become more
widely available, minimisation has become a more feasible option for many. although the technique
has increased in popularity, the mode of application is often poorly reported and the choice of input
parameters not justified in any logical way.
methods: we developed an automated package for patient allocation which incorporated a
simulation arm. we here demonstrate how simulation of data can help to determine the input
parameters to be used in a subsequent application of minimisation.
results: several scenarios were simulated. within the selected scenarios, increasing the number
of factors did not substantially adversely affect the extent to which the treatment groups were
balanced with respect to the prognostic factors. weighting of the factors tended to improve the
balance when factors had many categories with only a slight negative effect on the factors with
fewer categories. when interactions between factors were included as minimisation factors, there
was no major reduction in the balance overall.
conclusion: with the advent of widely available computing facilities, researchers can be better
equipped to implement minimisation as a means of patient allocation. simulations prior to study
commencement can assist in the choice of minimisation parameters and can be used to justify those
selections.
background
most medical researchers are aware that it is necessary to
perform a randomised controlled trial to effectively establish
the usefulness of a new treatment. the aim is that
treatments are compared on similar groups of patients.
completely random allocation of patients to treatments
does not, however, ensure that the patient groups are similar
with respect to prognostic factors. for example, purely
by chance one of the treatment groups may have been
allocated older or more severely ill patients. if such an
imbalance in prognostic factors has occurred then it may
be difficult to attribute any differences to treatment, the
analyses will require adjustment and the study will have
less power.
minimisation [1-3] is a dynamic allocation procedure that
ensures treatment groups are similar with respect to a
series of pre-specified prognostic factors. as patients are
recruited to the trial they are allocated to the treatment
group that will 'minimise' the differences in the distribution
of those factors between the groups. one pitfall of
this process is that the allocation is not random and hence
could be predicted. this problem is addressed by randomising
each patient but weighting the randomisation
towards the minimisation favoured treatment group for
that individual. by introducing weighted randomisation,
the individual is more likely to be allocated to receive the
preferred treatment, but is not guaranteed to do so. there
is a trade-off between the size of the weighting used and
the ability of the researcher to predict the next allocation.
to apply minimisation requires only simple algebra but
this may be problematic for the clinician with limited
time and resources. the advent of greater access to computing
technology has led to an increase in the usage of
minimisation, which has previously been implemented
relatively rarely. published trials increasingly cite the use
of minimisation for patient allocation. for example, falk
et al [4] minimized patients to receive immediate or
delayed radiotherapy with groups balanced according to
clinician, histology, presence of metastases and who performance
status. pal et al [5] used minimisation to ensure
even distribution within age groups (2–12 or 13–18
years) and whether or not there was cerebral impairment
when conducting a trial of phenobarbital versus phenytoin
for seizure control amongst epileptic children in rural
india. minimisation can similarly be used for allocation
within cluster randomised controlled trials when confounding
factors are applicable at the cluster level. for
example, hilton et al [6] performed a cluster randomised
trial of intervention to lessen individuals' cardiovascular
risk with general practices allocated to intervention or
control using minimisation for jarman score, ratio of
patient to practice nurse hours per week and fundholding
status.
the benefits of minimisation have been debated recently
[7,8] and a recent review of the usage of minimisation recommended
'its wider adoption in the conduct of randomized
controlled trials' [9].
the technique has not been without its critics [10] as well
as advocates, but it is acknowledged to be relatively simple
to implement and perform comparably to more complex
models where prognostic factors are non-numeric in
basis [11]. a common criticism is that minimisation concentrates
only on marginal distributions of prognostic factors
and may not ensure that the interactions are similar
between groups. for example, although there may be similar
numbers of males, females, disease state positive and
negative in the treatment groups post allocation, all of the
positive males may receive one treatment and all of the
positive females the other. if there is an interaction
between disease state and gender on outcome, then this
difference between the treatment arms may be problematic.
however, the likelihood of imbalance is easily countered
via a 4-category minimisation variable: male/
positive, male/negative, female/positive, female/negative.
the usefulness of minimisation as an allocation procedure
within randomised controlled trials is therefore
established and will continue to be recognised and utilised.
however, it has been commonplace for published
studies that have used minimisation to give little or no
information as to how the process has been implemented.
often they cite the minimisation variables but do not state
what metric has been used to determine the preferred allocation
group, whether and what randomisation weightings
have been used for the allocations, whether and how
the factors have been weighted, whether interactions have
been accounted for or, where applicable, how cut-points
for continuous prognostic factors were selected. the
researcher who wishes to embark on a trial using minimisation
as the means of patient allocation often has many
questions to ask. several parameters need to be selected
for each trial to which minimisation is to be applied.
these parameters and, from our experience within the statistical
consultancy, the most common associated questions
related to the choice of each, are given below:
1. number of factors
• how many factors can be balanced simultaneously?
• how does the balance for an individual factor change as
more factors are incorporated?
• in particular, what is the effect on other factors of adding
a single, many-categoried factor such as centre?
2. number of categories for each factor
• how does choice of number of categories (for continuous
prognostic factors) affect the allocation process?
3. whether and how to weight the factors
• how should the factors be weighted?
4. the randomisation weighting to use
• how much should the randomisation be weighted in
favour of the preferred group?
they also want to know
• with a chosen randomisation weighting and given
number of minimisation factors, how big a discrepancy
can be expected for a specified sample size?
and
• how does inclusion of interactions between prognostic
factors change all this?
there is minimal information available in the published
literature to inform researchers when addressing the
above questions.
we have developed a simple computer package to perform
minimisation allocations subject to selected values
of these 4 input parameters. we have also incorporated a
simulation element that allows the researcher to investigate
the size of discrepancies in allocation of prognostic
factors between treatment groups subject to variation in
the input parameters.
we here present the results of some simulations for a
hypothetical proposed trial and show how this process
can assist the researcher in deciding on the parameter values
to be used in their trial. subject to our chosen criteria
for model comparison, we examine the extent to which
varying the minimisation parameters may influence the
equalisation of prognostic factors between treatment
groups.
methods
for each new patient to be allocated, the process of minimisation
considers the imbalance in selected prognostic
factors and weights the randomisation of the next patient,
according to his or her characteristics, in favour of the
treatment that will make the treatment groups most similar
with respect to the prognostic factors. to formalize this
process, assume a trial where patients are allocated to one
of two treatments, t1 and t2. suppose there are m prognostic
factors and cj is the number of categories for the jth
prognostic factor (j = 1,...,m). let anj be the value that the
nth patient takes for the jth prognostic factor (anj∈
{1,2,...,cj}) and let dj be a measure of the difference
between the numbers of patients allocated to each of the
two treatments who are in category anj after allocation of
the (n-1)st patient. assume dj positive if more patients in
category anj are currently receiving t2, negative if more are
receiving t1 and zero if both treatments currently contain
equal numbers of patients at this level for the jth confounder.
therefore dn-1 = {d1, d2, ..., dm} is the vector of
differences between the treatment groups with respect to
the m prognostic factors at the levels seen in the nth
patient prior to allocation of that patient. using minimisation,
according to the size and direction of dn-1, randomisation
of the nth patient will be weighted towards the
treatment group that will make dn numerically smaller
according to some chosen metric ie. the differences
between the treatment groups will be minimised with
respect to the prognostic factors.
for example, consider a study where there are m = 4 minimisation
criteria: gender, age (under/over 18), residency
status of patient (in/out) and severity of disease (mild/
moderate/severe). suppose that 34 patients have been
recruited and allocated to one of 2 treatment arms (17 per
arm) and that they are distributed amongst the minimisation
criteria categories as follows:
suppose that the 35th (n = 35) patient to be allocated (n =
35) is an adult male in-patient with mild disease. prior to
his allocation the numbers of patients falling into these
categories is 8+3+7+4 = 22 in treatment arm t1 and
9+5+7+3 = 24 in treatment arm t2. hence d34 = {8-9, 3-
5, 7-7, 4-3} = {-1, -2, 0, 1} is the vector of differences
between the treatment groups with respect to the 4 prognostic
factors at the levels seen in the 35th patient prior to
allocation of that patient. since there are fewer similar
patients in t1, randomisation of the 35th patient should
be weighted towards this group.
choice of metric to minimise dn
the simplest algorithm for allocation of the nth patient is
to weight the randomisation in favour of t1 if ,
in favour of t2 if and use simple randomisation
(p = 1/2, where p is the probability with which the patient
is allocated to the preferred treatment) if . this
dj
j
m
>
= σ
0
1
dj
j
m
>
= σ
0
1
dj
j
m
>
= σ
0
1
table 1
treatment arm:
t1 t2
gender:
male 8 9
female 9 8
age:
under 18 14 12
over 18 3 5
residency status:
in patient 7 7
out patient 10 10
severity of disease:
mild 4 3
moderate 12 11
severe 1 3
prognostic factors.
the prognostic factors can be given different weightings
(wj) according to their relative importance and
used to determine the allocation. there are a variety of
ways that the wj may be chosen. one potential system that
seems reasonable is to weight according to the number of
categories. for example, if 100 patients are allocated to
two treatment groups then we would expect 25 within
each category of a binary variable for each treatment
group, and 10 within each category of a 5-category variable
for each treatment (assuming, without loss of generality,
equal probability of the categories occurring within
each variable). a maximum difference of the same absolute
magnitude between the two treatment allocations for
any of the variable categories would probably be more
clinically relevant for the 5-category than for the binary
variable:
the absolute (unweighted) differences are identical (= 10)
but for the binary variable t2 contains 50% more patients
within the first category, for the 5-category variable there
are 200% more relative to t1. the deviations from
expected are 20% (5/25) and 50% (5/10) respectively.
weighting the absolute differences (= 10) by the number
of categories gives weighted differences of 20 (10 × 2) and
50 (= 10 × 5) for the 2 and 5 category variables respectively.
the extent to which randomisation is more likely to
favour one treatment over the other depends on the size
of the randomisation weighting used. with p = 1/2 (simple
randomisation) there is no preference for either group.
when p = 1, the patient automatically receives the preferred
treatment, there is no random element and allocation
is said to be deterministic i.e. the researcher could
predict the group allocation of the next patient if they
knew the previous allocations. selecting a value 1/2 <p <
1 will bias allocation in favour of the preferred treatment
whilst retaining an element of randomness so that patient
allocation cannot be predicted. more extreme values of p
(greater bias) will lead to better balancing of prognostic
factors between treatment groups.
there must be a trade-off between introducing sufficient
randomisation weighting (p large enough) and not allowing
allocation of the next patient to be predicted (p not
close to 1).
quantification of balance
the maximum absolute difference between the patients
allocated to each of the categories within a given factor
summarises how well that factor has been balanced by the
minimisation process and is used as a summary measure
of balance for that factor. in our simulations we summarise
across factors with the same number of categories. for
example, if we minimize according to 3 binary variables
and one 15-category factor then there will be 2 summary
measures of balance achieved for each simulated dataset:
the maximum absolute differences allocated to (1) any of
the binary variables i.e. the 6 categories which constitute
these 3 factors and (2) any category within the 15 category
factor.
as previously noted [11] it is the behaviour of an individual
design that is of interest to the researcher rather than
the average over many applications. of interest is the
value below which discrepancies are likely to occur for the
majority of applications of minimisation with given criteria.
an individual investigator is more likely to want to
know the maximum discrepancy that s/he can reasonably
expect with chosen allocation parameters rather than
what the average will be for everyone using those parameters.
for these reasons we prefer the 95th centile of the
simulated distributions as more realistic and relevant
measures than the means or medians which have previously
been used to describe the success of a selected allocation
process. the error in estimating the 95th centile is
about 1 1/2 times the error when estimating the mean
[12] but this difference becomes unimportant if a large
number of simulations are taken.
the 95th centiles of the distributions of simulated values
are used to compare allocation schemes with varying
input parameters (sample size, randomisation weighting,
number and type of variables, weighting of variables).
simulation options
it was estimated that 200 simulations would allow quantification
of the 95th centile of the distribution to within ±
0.2 standard deviations of that distribution with 95% confidence.
increasing the number of simulations to 2500 or
5000 would increase this precision substantially to ± 0.06
and 0.04 respectively. a further doubling of the number of
simulations (to 10000) would only further increase precision
by less than 0.01 standard deviations. hence it was
wjdj
j
m
= σ
1
table 2
t1 t2 absolute difference t1 t2 absolute difference
20 30 10 5 15 10
25 25 0 10 10 0
10 10 0
10 10 0
10 10 0
decided that 5000 simulations would be adequate for a
comparison of minimisation criteria. for each scenario
5000 simulations were performed using a fortran program
incorporating nag subroutines for random selection
of patient characteristics.
measure of balance
the 95th centiles of the distributions of maximum absolute
differences for each factor type were recorded and
classified according to the number of potential categories
within which each individual could fall. these differences
are expressed as the proportionate difference from that
expected by multiplying by the number of categories for
that factor and dividing by the total sample size.
randomisation weighting
all simulations were repeated for p set at 1/2 (simple randomisation)
and also for p taken to be 0.67, 0.75, 0.8,
0.83, 0.88, 0.91, 0.95, 0.97, 0.99 and 0.999, values which
equate respectively to allocation to the preferred treatment
being 2, 3, 4, 5, 7, 10, 20, 30, 100 and 1000 times as
likely as to the alternative.
weighting of variables
simulations were performed with prognostic factors both
unweighted (wj = 1; j = 1,...,m) and with weights equal to
the number of categories of the prognostic factor.
to address the types of questions posed by potential
researchers we here simulate several different scenarios:
firstly, we investigate the effect on balance of increasing
the number of factors. models with 1, 2, 3, 4, 5, 10, 20 and
30 binary prognostic factors are compared. the sample
size for each simulation was set at 500. since all factors are
binary, weighting will not change the results and these
models are only presented unweighted.
secondly, we chose a study with three binary, one 3-category
and one 4-category (total 5) prognostic factors to
simulate. this scenario was chosen as being typical of the
problems we were encountering in the local consultancy
and not dissimilar to published studies citing minimisation
criteria which commonly have several binary criteria
and one or more multiple category confounders [4,6]. we
investigated the extent to which balance was a function of
sample size by simulating the scenario with each simulation
based on samples of 40 and of 500 patients. for sample
size of 500, we also considered the balance achieved
when all 2-way interactions were used as the minimisation
criteria (total of 10 interactions between the 5 confounders).
finally, we generated simulations of
allocations for a sample size of 500 based on 6 minimisation
factors: the original 5 plus an additional 15- category
confounder.
practicalities of application
the practicalities of application require some degree of
automation. we have developed a package for clinical
usage (simin) which both utilises simulations to facilitate
the process of specification selection and provides a userfriendly
front-end for the subsequent allocations within
trial. the fortran programs which generate the simulations
are embedded in this package. the purpose of this
paper is to illustrate the types of patterns that can easily be
determined via simulation and to show how this might
assist the researcher, leading to more informed parameter
selection and enhanced reporting of the decision process.
results
all results were equivalent for randomisation weights of
100 and 1000. hence, only the results for 100 are shown
in the figures.
increasing the number of factors
figure 1 shows the results of the simulations as the
number of binary factors is set at 1, 5, 20 and 30. the output
for 2, 3, 4 and 10 binary factors are not shown on the
figure. as expected, the proportionate change from
expected falls as the randomisation weighting is
increased. the largest fall occurs after the introduction of
any randomisation weighting i.e. between values of 1
(simple randomisation) and 2. as the randomisation
weighting is increased to 5 in favour of the preferred treatment
there are further declines and less so between 5 and
30. after 30 the proportionate change appears to have
reached an asymptote. the differences in proportionate
changes with increasing number of factors are approximately
constant with changing randomisation weights.
weighting the variables
figure 2 shows the 95th centiles of the distributions of proportionate
changes for the 5 prognostic factor (3 binary, 1
3-category and 1 4-category) scenario with a sample size
of 500. the dotted lines show the results when the prognostic
factors are weighted according to the number of categories.
again, any degree of randomisation weighting is associated
with the largest fall in change from expected. proportionate
differences increase with the number of categories.
weighting of the prognostic factors has the effect of
increasing the discrepancies for the binary factors but
reducing them for the factors with more categories.
changing the sample size
figure 3 shows the 95th centiles of the distributions when
a sample of only 40 patients is allocated using 5 minimisation
factors (3 binary, 1 3-category and 1 4-category).
the proportionate differences are much larger with the
smaller sample size (c.f. figure 2).
interactions
figure 4 shows 95th centiles of the distributions of proportionate
changes when minimisation is used to allocate
500 patients to 2 groups with the aim of obtaining even
distribution of all 2-way interactions of 5 confounders (3
binary, 1 3-category and 1 4-category). if the distributions
sof the interactions are similar, then the marginal distributions
of the factors will be also.
the differences are increased from the non-interaction
model (figure 2) approximately 2-fold for the lower randomisation
weights (and when simple randomisation is
used) up to approximately 5-fold for large randomisation
weighting. since the proportionate changes are smaller for
larger randomisation weights, the absolute difference in
the proportionate changes falls with increasing randomisation
weights. it should be noted, however, that the interaction
factors have more categories and the discrepancies
are about the same in terms of the numbers of individuals
allocated.
the effect of weighting of the prognostic factors is similar
to previous models. when factors are weighted according
to the number of categories this has the effect of reducing
the proportionate discrepancies for the factors with more
categories (3 × 4 and 2 × 4 interactions) whilst having the
opposite effect for the factors with fewer categories (2 × 2
and 2 × 3 interactions).
ifnicgrueraesi n1g the number of binary factors
increasing the number of binary factors. 95th centiles of the distributions of proportionate changes from expected for
randomisation weights from 1 to 100 obtained from 5000 simulations and a simulated sample size of 500. the number of minimisation
variables is increased from 1 (black line) to 5 (dark green), 20 (lime green) and to 30 (red).
adding a factor with many categories
figure 5 shows the results when samples of 500 are simulated
using 6 confounders (3 binary, 1 3-category, 1 4-category
and 1 15-category). the inclusion of the 15-category
variable has had little influence on the proportionate
changes for the other factors in the unweighted model
(figure 2). when the factors are weighted the proportionate
changes for the 4-category factor are increased in the
expanded model as opposed to having decreased previously.
weighting of the prognostic factors has most effect
on the extent to which the 15-category factor has a similar
distribution for the 2 treatment groups.
discussion
in this paper we have simulated some common treatment
trial scenarios and compared the results in terms of the
distribution of discrepancies between treatment groups.
whilst only a selection of potential scenarios can be
shown, we have illustrated how prior investigation helps
quantify the sensitivity of minimisation to the choice of
input parameters (randomisation weights, weighting of
prognostic factors, number and type of factors). assuming
that numerically small differences between the groups are
of little practical clinical importance, then we can make
several useful statements regarding the selected scenarios:
sfaigmuprlee s2ize 500, 5 prognostic factors (3 binary, 1 3-category, 1 4-category)
sample size 500, 5 prognostic factors (3 binary, 1 3-category, 1 4-category). 95th centiles of the distributions of proportionate
changes from expected for randomisation weights from 1 to 100 obtained from 5000 simulations. dotted lines
show results when prognostic factors are weighted according to the number of categories. results for binary factors shown in
black, 3-category in blue and 4-category in purple.
• the number of factors to be taken into account can be
increased quite substantially without severely affecting
the overall balance.
• weighting of the factors had most effect on the factors
with many categories and was not highly detrimental to
the factors with fewer categories (in keeping with the findings
of weir and lees [13]).
• including interaction terms in the minimisation did not
greatly increase the overall discrepancies.
• even weighting the randomisation by a small amount in
favour of the preferred treatment had a large effect on the
equality of the distribution of prognostic factors between
treatments.
• increasing the randomisation weights above 5 had little
effect on the extent to which the treatment groups were
similar.
note that for a different number or type of minimisation
factors the above statements may not hold. they are not
meant to be universally true. these are statements about
one particular hypothesised scenario to show how infor-
sfaigmuprlee s3ize 40, 5 prognostic factors (3 binary, 1 3-category, 1 4-category)
sample size 40, 5 prognostic factors (3 binary, 1 3-category, 1 4-category). 95th centiles of the distributions of proportionate
changes from expected for randomisation weights from 1 to 100 obtained from 5000 simulations. dotted lines
show results when prognostic factors are weighted according to the number of categories. results for binary factors shown in
black, 3-category in blue and 4-category in purple.
mation relating to that scenario can be easily generated via
simulations.
we believe that prior simulation according to expected
sample size will be useful for clinicians embarking on a
randomised controlled trial for which prognostic factors
exist and should be equalised between treatment groups.
simulation will help quantify the effect of different input
parameters on the expected discrepancies. it may assist in
the choice of randomisation weighting utilised and the
trade-off between minimizing for more criteria and/or
increasing the categories of minimisation where prognostic
factors are continuous. however, it should be noted
that the decision of which variables to include in the minimisation
process should be informed primarily by the
clinical importance of variables and their impact on outcome.
vaughan reed and wickham [14] give further discussion
to the choice of cut-points for continuous
prognostic factors when performing minimisation. all
minimisation variables should be adjusted for in the final
analyses [1,9] and it is therefore important that only necessary
variables are included to avoid over-parameterisation
of the models. there is a trade-off between
incorporating too many variables/unnecessarily increasing
the number of categories used and allowing imbalance
in important prognostic factors. the results of the
mfiignuimreiz i4ng the difference of the interactions
minimizing the difference of the interactions. 95th centiles of the distributions of proportionate changes from expected
for randomisation weights from 1 to 100 obtained from 5000 simulations. all 10 2-way interactions from 5 prognostic factors
(3 binary, 1 3-category, 1 4-category) used as minimisation criteria. dotted lines show results when prognostic factors are
weighted according to the number of categories. results for the 3 2 × 2 interaction terms shown in black, for the 3 2 × 3 interactions
in dark green, the 3 2 × 4 in lime green and the 1 3 × 4 interaction in red.
simulation exercises can assist in the process but cannot
be used as the sole, or even main, decision criteria for
inclusion of a particular variable.
in theory, the process of minimisation is relatively simple
to undertake [15] but in practice we feel that clinicians do
not find it easy to keep running totals and perform
weighted randomisations. automation of the process (in
addition to telephone allocation where available) reduces
the propensity for conscious or unconscious interference
with the allocation procedure. given the importance of
allocation concealment, this is an additional benefit of
employing minimisation. furthermore, simulated models
are not necessarily straightforward to generate. consequently,
we have developed a software package (simin)
for clinical usage which enables not only easy minimisation
but also generates simulations that may be useful
prior to the commencement of the study to help in determining
which input parameters to use. it is important that
the person performing the allocations is independent of
the trial team. having a quick and simple to use automated
system makes it easier to enrol suitable individuals
for this task.
at study commencement it should be possible to justify
the number and type of minimisation variables, their
faidgduinrge a5 15-category factor
adding a 15-category factor. 95th centiles of the distributions of proportionate change from expected for randomisation
weights from 1 to 100 obtained from 5000 simulations. dotted lines show results when prognostic factors are weighted
according to the number of categories. results for binary factors shown in black, 3-category in blue, 4-category in purple and
15-category in green.
weighting and the choice of randomisation weighting.
simulation enables estimation of the discrepancies anticipated
and their probability. for example, suppose a treatment
trial is estimated to require 40 patients to be
allocated to the new or standard treatments to obtain a
reasonable power to detect differences in outcome of clinical
importance. potential confounders are sex (male/
female), age (under/over 18), whether the individual is an
in- or out-patient, severity of disease (mild/moderate/
severe) and ethnicity (4 categories). all categories of all
minimisation variables are expected to be equally likely.
(this latter criterion may not realistic but the density functions
can be easily adjusted within the simulations.) if a
randomisation weight of 2 (p = 0.67) is used then the proportionate
change from expected of patients allocated to
new and standard treatment is expected to be less than or
equal to 0.35 for sex, age group and patient status (in- or
out- patient), less than or equal to 0.45 for disease severity
and less than or equal to 0.6 for ethnicity for 95% of random
patient samples (figure 3). these are equivalent to
absolute difference of 7, 6 and 6 patients respectively. a
statement such as this could be incorporated into the protocol
in a similar way to having a power calculation. i.e.
the protocol could state, "sex, age (under/over 18), residency
status of patient (in/out), severity of disease (mild/
moderate/severe) and ethnicity (4 categories) were
included as factors in the minimisation. factors were
unweighted and a randomisation weighting of 2 was used
for the allocations. it was estimated that the discrepancy
between patients allocated to the 2 treatment groups
would not exceed 7, 6 or 6 for the binary variables, disease
state and ethnicity respectively with probability 0.95."
frequently, very little information is given on randomisation
weights, weighting of variables or even the categories
used in description of minimisation procedures.
whilst it should be possible to justify the minimisation
criteria, the precise details of the allocation process should
not be widely divulged until after the trial has completed.
the less information that is accessible, the less chance
there is of recruitment being biased by knowledge of the
likely allocation of future patients. we recommend that
the details of and justification for the allocation process
being employed are documented and given in the final
trial reports. however, we also recommend that these
details are not revealed to the research team during the
trial apart from where this is essential.
the international conference on harmonisation e9
guidelines [16] discuss the importance of minimising bias
in the design of trials, with 'bias' defined as "the systematic
tendency of any factors associated with the design,
conduct, analysis and interpretation of the results of clinical
trials to make the estimate of a treatment effect deviate
from its true value." these guidelines also state that
"good design should generally aim to achieve the same
distribution of subjects to treatments within each centre
and good management should maintain this design
objective." the use of dynamic allocation of patients to
treatments is one way to achieve these aims. in this paper
we have investigated some of the issues that arise in the
practical application of one allocation technique the use
of which has risen sharply in recent years.
we have performed simulations using the most common
scenario of 2 treatment groups. a similar process could be
done where there are more treatment groups and this
option has been incorporated into our software.
we have simulated datasets under different minimisation
criteria to show how outcomes may vary as the input
parameters are changed and suggest that this sort of
approach should become standard practice. previously it
has been noted that simulations can be usefully employed
prior to study commencement to determine the best allocation
method to use [9,11,13]. these studies have used
relatively complex models to compare not only minimisation
parameters but also alternative approaches such as
stratification. in some cases they have incorporated existing
data [13]. our results are in keeping with the findings
of these more detailed studies. what we have aimed to
show in this paper is how a relatively simple automatic
algorithm, made available in package form, can be used to
assist clinicians when they have decided to utilise minimisation
and need to determine the optimal parameters. the
package simplifies the practicalities of the process and
hence may make this the preferred allocation method
even when there are few prognostic factors to be taken
into account and stratification is also a feasible option. it
is important that researchers justify the choices they make
with regards to the procedure for allocating patients. the
arguments for this are not dissimilar to the argument for
giving a power calculation or describing other details of
the study protocol.
conclusion
the use of minimisation as a means of patient allocation
is increasing. decisions need to be made regarding the
precise mode of implementation. choice of input parameters
may influence the extent to which the process is successful
in ensuring equality of prognostic factors between
treatment groups. we show how a simple automated
package that we have developed locally can be used to
allow researchers to investigate the effects of varying the
input parameters prior to study commencement. the
advent of the wide availability of computing technology
makes minimisation a more realistic choice for many
researchers. it is important that they utilise the technique
most effectively. we have shown how simulations can be
used prior to study commencement to ensure that the
minimisation has a reasonable chance of providing comparable
treatment groups.

<|EndOfText|>

fear or favour? statistics in pathology
statistics have a role to play in most areas of
medical research including the field of pathology.
we have come a long way since 1954 when
the british medical journal published excerpts
from a debate held by the study circle on
medical statistics as to whether the then growing
influence of statistics in medicine was, in
fact, welcome.1 one speaker declared that,
“medicine was an art, statistics a science; he
conceded that the latter had its uses, but when
it came to mixing science and art, statistics was
as out of place as a skillet in a crown derby
tea-service.” he concluded that “statistics
might be all very well for the elite but were a
menace to the mob.” someone else “referred
darkly to the deliberate misuse of statistics,
fostered—for what purpose ?—by statisticians
themselves. statistical publications, he said,
could be recognised by the prolixity of their
tables. in his view no papers should contain any
tables at all.” the debate concluded with the
motion that the influence of statistics should be
welcomed in all branches of medicine and this
was carried by a narrow majority on a show of
hands.
in the intervening 45 years there has been a
mushrooming of statistical literature designed
to assist the medical researcher, with numerous
articles highlighting misuses of statistics and
giving pointers towards improvement. there
has been a growing understanding that statisticians
are concerned with the whole process of
research, from study design through to final
conclusions, and are not merely purveyors of p
values and analytical methodology. the more
recent evidence based medicine movement has
served to further publicise this recognition. as
h g wells predicted, “statistical thinking will
one day be as necessary for efficient citizenship
as the ability to read and write.”
the lessons have been numerous and only
the major developments are reviewed here.
emphasis has increasingly been placed on
identifying a well defined and answerable
research question before undertaking any
study. this seemingly obvious prerequisite may
be the hardest part, finding the right question
often being more troublesome than finding the
right answer. in the words of einstein, “the
formulation of a problem is often more essential
than its solution which may be merely a
matter of mathematical or experimental skill.”
the incorporation of suitable control groups
and some quantification, before starting a
study, of the necessary sample size required to
conclusively answer the research question have
been stressed. the need for some formal statistical
comparison of the results, usually resulting
in a p value, has also been encouraged. an
informal review of the publications in this journal
over the last 20 years shows that this latter
point has indeed been taken on board. the
majority of the papers published in jcp in
1978 contained little or no formal statistical
analysis: during the whole of that year only 38
papers contained any p values and most of
those had only one. by contrast, in the current
editions most submissions have at least some
formal analysis and usually contain several p
values. when interpreting the clinical value of
results statisticians have, in more recent years,
stressed the importance of quantifying the size
of any effect, rather than merely relying on a
significance level as given by a p value. to this
end, the current guidelines for this journal state
that “95% confidence intervals should be used
wherever appropriate.” this has certainly led
to a dramatic increase in usage. during the
whole of 1978 there was only one confidence
interval presented in jcp, compared with the
current situation where they are to be found in
most issues.
so, in common with most others, this journal
has seen a secular trend in the use of statistics,
and the statistical quality of published research
is undoubtedly superior to that seen 20 years
ago. as we enter the new millennium is there
anything that can be done to assist yet further
improvements?
one potential barrier to such improvement is
the dearth of statistical guidance aimed specifically
at pathologists. it may seem strange to
suggest that the wealth of published statistical
literature is not directly applicable to research
in the field of pathology. every discipline tends
to use certain types of study design and forms
of statistical analyses more than others. the
necessary information is out there but it may be
somewhat off-putting to have to delve through
a mountain of irrelevant material to find it. by
identifying the areas of main interest we can
considerably reduce the ground that must be
covered to gain the necessary knowledge to
produce good quality research that answers
useful questions in our particular area. for
example, the majority of medical statistics texts
aimed at the non-statistician urge us to
perform randomised controlled trials, anything
else being considered inferior, yet these are
hardly ever used by and are largely irrelevant to
pathologists.
most of the research questions addressed by
studies in this journal concern the comparison
between two or more previously defined groups
(for example, diseased and healthy, different
diseased groups, or those at different stages of
the same disease); assessing the reliability/
validity/reproducibility of measurements; predicting
time to death/recovery/relapse/infection
or using new measurements to improve diagnostic
accuracy.
the rest of this paper will focus on two
aspects of medical statistics that are relevant to
all of these scenarios yet are still widely misunderstood
or poorly presented. the aim is to
provide a further learning brick to build on the
improvements that have already been seen over
the last two decades.
choice of sample(s)
individuals
when we perform studies we are trying to find
out what happens in the population. for example,
do two measuring instruments give the
same readings when applied to patients in disease
group x? can we accurately and consistently
measure y? can we predict survival or
time to relapse from p, q, and z? we cannot
measure the whole population, so we observe a
subset or sample of individuals and from these
infer what we think happens in the population
as a whole. statistical analyses are used to make
this inference.
studies can be irretrievably ruined by the
biased choice of samples, particularly if we are
unaware of the size and direction of any bias.
however, choice of sample appears to receive
little thought and is often made according for
convenience rather than representativeness.
commonly all available samples from a given
laboratory or hospital may be included. while
this is all that may be feasible within the time
and practicality constraints imposed on the
researchers, there should be some attempt to
identify whether this sample is in fact representative
of the population of interest. for
example, does this hospital tend to get referred
patients at all stages of this disease or is it
biased towards the more symptomatic? is the
area which this laboratory/hospital serves
socially and ethnically representative? given
this kind of information, the reader can decide
whether the results are likely to apply to their
own population.
quite commonly it is a subgroup of patients
over a certain time frame who are included,
these being chosen according to availability of
blocks, tissue samples, or data. in this case
there needs to be some discussion of the representativeness
of the subgroup. for example,
were those with available tissue more severely
ill? do they tend to have different underlying
diseases? were data more carefully recorded in
the more unusual diagnostic cases?
control groups which consist of “healthy
volunteers” may be used. this group should
ideally be similar to the disease group except
for the presence of disease. it is of interest to
know precisely how this group has been
recruited to help determine whether it constitutes
a reasonable comparison group. for
example, sometimes laboratory staff or patients
admitted for reasons unrelated to the present
study interest are used as controls. in fact
either of these might be considered unsuitable.
the former, laboratory staff, may be younger
than the study disease group and the latter may
not be entirely normal with respect to the study
measures.
volunteers, whether from the disease or the
control group, may differ from non-volunteers,
and it will usually be impossible to assess the
extent of that difference. it is best to avoid
advertising for volunteers as a means of
recruitment. if 50% of those approached to
participate refuse then at least we have some
measure of how representative the final sample
is.
when the sample is to consist of a subset of
some larger group of eligible individuals, then
this subset should be randomly chosen, that is
in a manner unbiased by the characteristics of
the individuals and in a non-systematic way.
random selections must be made using either
tables or suitable software and this should be
made explicit in the description of the sample
selection process.
specimens within individuals
having identified individuals to be included in
the study, there may be the further selection of
a particular specimen to be analysed. this has
been shown2 to be “the most observer dependent
and therefore most subjective step.” many
studies state that “representative sections” or
“systematically selected areas” were chosen,
but precisely how this was done is not made
explicit. if there is a system it should be clearly
outlined so that others can make comparable
choices. if selection is random, the methodology
should be specified.
to summarise, while the ideal of comparable
and representative samples from the study
groups concerned (for example, disease x and
healthy controls, disease x and disease y) is
rarely attained, we can improve published
studies by giving full details of the selection
process for both the individuals and specimens
from those individuals. the representativeness
of these should be discussed to enable readers
to identify applicability and potentially confounding
variables.
a further and related point is that comparability
can be improved by ensuring that those
who make the assessments are blind to the
study group. where several assessors are
involved then there should be high inter-rater
reliability.
size of sample(s)
samples are used to estimate population
effects. it is intuitively obvious that a larger
sample will give a more precise estimation of
the population value. for example, if 20% of
the population display trait x then in a sample
of 10 from this population we would not be
surprised to find anywhere between 0 and 5
individuals with the trait (0–50%). if we
sample 100 individuals we would not be
surprised to observe anywhere between 13 and
29 (13–29%) with the trait; more extreme
numbers may lead us to doubt whether the true
prevalence of x is actually 20%. the thinking
is similar when we present confidence intervals
with sample estimates. from our sample we
estimate the population value (for example, the
mean or proportion; the difference in means or
proportions between normal and diseased
individuals or the median survival in different
groups). we do not expect this estimate to be
exact, although we know that the larger the
statistics in pathology 17
sample the more precise it will be.a confidence
interval gives the range of population values (or
differences) that our sample(s) are compatible
with: 95% confidence intervals give the range
within which we are 95% confident the
population value lies. clearly the addition of a
confidence interval facilitates the clinical
interpretation of the results and highlights any
limitations caused by sample size. most statistical
packages now give confidence intervals as
standard; details of calculation can be found
elsewhere.3 4
the power of a study is its ability to detect a
difference of a given size.5 for example,
suppose qrz in the population of individuals
with disease k is on average 10 and always
varies between 5 and 20 compared with an
average of 20 and range of 10–35 for normal
individuals. (note that the ranges are not symmetric
around the averages and this is to stress
the idea that this thinking applies not only to
normally or symmetrically distributed data.)
we may by chance randomly sample disease k
patients with a tendency to higher values and
normal controls with a tendency to lower
values and hence there will be no significant
difference in the average values in the samples.
we may therefore wrongly conclude that there
is no difference in average qrz between the
groups. of course this will not always happen
and how often it does depends on both the
sample sizes (the larger the samples the more
closely they tend to approximate their respective
population means) and the variability of
the measurements in the populations (if disease
k measures are mostly between 8 and 12 and
the normals between 18 and 22, then it will be
less likely to happen than if the values of qrz
are more evenly spread across the range in each
group). the power of the study, usually
expressed as a percentage, tells us how often a
given difference will be detected for a certain
variability and sample size. as the variability of
a measure is fixed (that is, it exists in the population
and there is nothing we can do about it
except perhaps choose a more homogeneous
population which will change the research
question), the aim is to choose a sample size
that will detect a clinically important difference
with reasonable power. power is usually set at
80% or above. a value of 80% means that four
times out of five the study will detect the difference
if it exists.
it is now accepted as standard practice that
all published randomised controlled trials
should include some statement regarding the
power of the study.6 it is less well recognised
that a similar proviso would benefit the presentation
of all studies, including non-randomised
and single group descriptive studies. such a
policy safeguards the researcher against wasting
time with a sample that is too small to give
conclusive answers. it also serves to assist
interpretation where results are nonsignificant,
in which case we want to know the
power that the study had to detect a difference.
the combination of no power calculations, p
value reporting, and interpretation with little or
no use of confidence intervals is a recipe for
potential disaster.
kirkwood7 gives a good overview of the most
commonly used power and precision calculations.
other useful references8 are for ordinal
variables,9 10 reliability coefficients,4 11–13 survival
analyses,14 15 and for testing equivalence16
17 (which requires larger samples than to
show a difference).
conclusion
if we are to have a new year’s resolution for statistics
in pathology let it be that we will use
unbiased sample selection methods which are
fully reported, perform power calculations, and
present results with confidence intervals. in
this way we can ensure that we are selecting
and interpreting our data without fear or favour
of being misled by biased samples or mystical p
values. conclusions based solely on the latter
should be d valued forthwith.

<|EndOfText|>

research is only worth doing if it provides useful
information. medical research usually consists
of studying groups of individuals with the
aim of answering a predefined research question.
most commonly in the field of sexually
transmitted infections (sti), the prevalence of
a virus or abnormality is to be estimated or
prevalences compared, either over time or
between different groups of people. alternatively,
several therapies may be compared
within a randomised controlled trial. one
question that arises at the start of any study is
“how many individuals should be included in
this study?” there are several ways of answering
this question. the number included may be
based on practical issues—for instance, the
length of time available to the researcher
together with the time taken to recruit, treat,
and test each individual and the expected
patient accrual rate. these factors will vary
from researcher to researcher and between different
sources of patients—for example, accrual
rates will differ between different hospitals.
to use such variable quantities to
determine the number needed to effectively
answer a given research question is clearly
flawed. ethically it is wrong to either underrecruit
or overrecruit. on the one hand we may
be left with insufficient numbers to conclusively
answer the question. on the other hand,
if we overrecruit, then the best scenario is not
only do we waste time, but also we subject
more individuals than necessary to any inconvenience
associated with being studied. in the
worst scenario, we may be allowing individuals
to receive inferior treatment after sufficient
numbers have been recruited to ensure that the
best treatment is known.
many researchers associate sample size
calculation purely with randomised controlled
trials. most of the studies presented in this
journal do not fall into this category. however
sample size estimation before study commencement
is important for all types of study,
including prevalence studies and observational
comparisons. this article highlights the need
for consideration of study size over and above
issues of feasibility and practicality. information
is presented on how to determine an
appropriate sample size for the most commonly
used study designs within the field of sti.
why size matters
a prevalence study finds that 25% of women in the
outer hebrides have hsv-2 antibodies.
we cannot interpret this information without
knowing the numbers this figure was based on.
for example, one out of four (25%) is a much
less precise estimate than 100 out of 400
(25%). a proper presentation of the results
would include a confidence interval. the 95%
confidence interval for the first scenario is (0.6,
80.6%) and for the second (21.0, 29.5%). for
each of the examples, these are the ranges
within which we are 95% confident the
population prevalence lies. as we would
expect, with a much larger sample size, in the
latter scenario we can make a more precise
statement about the likely population prevalence.
20% of pregnant outer hebrideans have
hsv-2 antibodies compared with 20% of inner
hebrideans.
if the above statement were true and we randomly
sampled and tested 30 individuals in
each group, then we would expect to see about
six antibody positive individuals in each group.
however, we would not be unduly surprised to
find four individuals with antibodies in one of
the groups and eight in the other.
however, we can quantify how likely, in the
absence of a difference, we are to falsely
conclude from our study that there is one. the
statistical significance of a study is the probability
that we will falsely identify a difference
when none exists. the larger the study, the less
likely this is to happen.
20% of pregnant outer hebrideans have
hsv-2 antibodies compared with 10% of mainland
scots.
if the above statement were true and we randomly
sampled and tested 30 individuals in
each group, then we would expect to see about
six antibody positive individuals in the first
group and three in the latter. however, we
would not be unduly surprised to find four
individuals with antibodies in each of the
groups.
key messages
+ sample size is an important issue for all
contributors of studies to this journal.
+ the interpretation of study results depends
on the sample size included that
study.
+ sample size formulas are given for the
most common scenarios encountered in
the field of sti.
question: how do we know that our study
will not indicate there are differences when
none exist?
answer:we don’t.
however, we can quantify how likely we are
to find a difference of a given size if it exists.
the power of a study, usually represented as a
percentage, is the ability of a study of a given
size to detect a difference of a given magnitude.
the larger the difference the smaller the
number needed.
simple sums for sample sizing
choosing the best sample size is not a precise
art. equations exist for calculating the sample
sizes needed to obtain a specified precision or
to identify differences of a given size. the latter
of these are called power calculations. sample
size calculation relies on “guesstimates” of
unknown quantities and hence obtained sizes
are by definition unlikely to be correct. the
extent to which sample size determination is
influenced by erroneous estimates can be
investigated by trying out different guesstimates
in the formulas. below are details of
sample size calculation for the most common
scenarios in sti studies.
(i) the approximate number of individuals
required to estimate a prevalence within ± e%
is given by the formula:
the number required depends on the prevalence
the study is designed to estimate!
note that if the prevalence is x% then the
sample size required is the same as if estimating
a prevalence of (100 − x)%.
for example, if the true prevalence is 25%
and we want to estimate this prevalence to
within ±5% then
individuals are required.
of course we do not know this prevalence
before we undertake the study. our guesstimate
of 25% may be inaccurate. if the true
prevalence is actually 35%, then
individuals would be required to give the same
level of precision. if only 300 individuals are
included (based on the guesstimate of 25%)
then the prevalence will be estimated less precisely
than anticipated.
figure 1 shows the numbers required to
detect various prevalences to within ±1%. for
example, to estimate a prevalence of 25% with
this precision will require approximately 7500
individuals. (note that the same number would
be required to estimate 75% —that is, 100 −
25%, with the same precision.)
if a prevalence needs to be estimated with
greater precision then the sample size must be
increased, for less precision smaller sample
sizes are required.
to estimate prevalences more or less precisely,
the estimates given for ±1% can be
divided by the required precision squared. for
example:
(a) to estimate to within ±2% the numbers
given for 1% need to be divided by 22 (=4), this
curve is shown on the figure. to estimate 25%
to within ±2% requires approximately 7500/4
or 1875 individuals.
(b) to estimate to within ±0.5% the
numbers given for 1% need to be divided by
0.52 (= 0.25). part of this curve is shown on the
figure. to estimate 25% to within ±0.5%
requires approximately 7500/0.25 or 30 000
individuals.
(ii) if the prevalences within different groups
are p1% and p2%, the approximate numbers of
individuals required to detect this difference
with 80% power at the 5% significance level
are:
table 1 shows n for selected p1% and p2%.
for greater power the sample sizes need to
be increased. similarly, larger samples are
required to detect smaller differences between
prevalences.
for 90% power, the samples need to be
increased by about one third.
for example, if 10% of hebrideans and 20%
of mainland scots have hsv-2 infection, then
200 hebridean and 200 mainland scots need
question: how do we know that our study
will not fail to identify a difference of clinical
importance that truly exists?
answer:we don’t.
figure 1 number need to estimate a single prevalence to
within plus or minus 1%, 2%, and 0.5% with 95%
confidence.
10 000
9000
8000
estimate to within:
± 0.5%
± 1%
± 2%
7000
6000
5000
4000
3000
2000
1000
0
50
percentage prevalence
number needed
0 5 10 15 20 25 30 35 40 45
table 1 number required in each group to detect
differences with 80% power at the 5% significance level
given that group prevalences are p1% and p2%
prevalence
(p1%):
prevalence (p2%):
10 20 30 40 50 60 70
5 440 74 33 19 12 8 5
10 200 60 30 17 11 7
20 296 80 37 20 12
30 360 92 40 21
40 392 96 40
50 392 92
60 360
to be tested to detect this difference with 80%
power at the 5% significance level. for 90%
power a total of 266 (= 200 ´ 1.33) will need to
be tested in each group.
as before, the sample size estimate is based
on guesstimates of the study outcome. for the
purposes of sample size determination, the
magnitude of the difference in prevalences
between groups may be chosen on the basis of
what is a clinically important difference that we
want to detect. for example, the best available
evidence may suggest that the hebrideans only
have 5% fewer hsv-2 infections but we decide
to undertake a study to detect a difference of
10% since this could represent a clinically
important difference (in terms of allocating
resources, etc), whereas 5% would not.
often there is more than one factor to
consider. for example, outer hebrideans may
be older than the average scottish person and it
would be of interest to know whether the
difference in prevalence of hsv-2 can be
explained by the difference in ages of the
groups. sample size estimation in these scenarios
is more complex and will depend on the
extent to which the factors are related.1
more of one than the other
when comparisons are made between two or
more groups, power will be maximised (for a
given overall total number enrolled) if each
group is of the same size. to accommodate an
imbalance in numbers while retaining the same
power, the total sample size needs to be
increased accordingly.2
ones that got away
calculations give the estimated numbers required
for statistical analysis. sometimes it will
be necessary to recruit many more to ensure
that sufficient are obtained for that analysis. on
the basis of the expected refusal and compliance
rates, the study should be designed so that
sufficient are contacted/approached/recruited
to account for these losses.
ones that never happen
in some studies, individuals are recruited and
monitored for variable lengths of time with the
aim of comparing times to some defined event,
such as death or infection, between subgroups
of individuals. the nature of the outcome is
that not all individuals will have the event and
a straightforward comparison of percentages
dying or becoming infected is invalidated
because of the variable follow up times.
similarly, comparing average times to death/
infection would also be invalid. a special type
of analysis is required3 and the sample size will
be based on the number of individuals for
whom the event occurs.4 so for a rare event
larger numbers will be required. extending the
length of follow up may be used to increase the
numbers of events occurring.
ones that flock together
if treatments are allocated on a group basis, the
effective sample size will not equal the total
number of individuals. adjustment must be
made according to the extent to which
individual outcomes are influenced by the fact
that they form part of a homogeneous group.5 6
for example, the introduction of trained
specialists within randomly selected clinics7 to
identify whether these specialists have an effect
on adverse outcomes. individuals from the
same clinic may be more alike, perhaps because
of social and ethnic similarities, than those
from different clinics in terms of their tendency
to be recorded as having a problem even before
any intervention. a study that randomises by
clinic will effectively be putting groups of similar
individuals into the same arm of the trial en
bloc. a similar situation occurs when individuals
within the same family, ward/hospital, etc,
are jointly allocated to treatments.
summary
in conclusion, this short article has addressed
the issues surrounding the determination of
appropriate study size. only the sample size
equations relating to the most common study
types in the field of sti are presented. for
more complex but common situations, the
important issues to consider are highlighted.
many other sample size equations exist—for
example, when the outcome is continuous,
such as cd4 count, or more than two groups
are compared. several useful references are
given in the further reading section. this
article raises awareness of the need to perform
studies of the correct size for the given purpose
and informs the researcher in sexually transmitted
infections of the particular points that
they may need to consider.

<|EndOfText|>

summary
cross-sectional covariate-related reference ranges are widely used in clinical medicine to put individual
observations in the context of population values. usually, such reference ranges are created from data
sets of independent observations. if multiple measurements per individual are available, then ignoring the
within-person correlation between repeats will lead to overestimation of centile precision. furthermore, if
abnormal measurements have triggered more frequent assessment, the data set will be biased thus producing
biased centiles. where multiple measures per individual exist, the methods commonly used are either
randomly or systematically to select one observation per individual or to model individual trajectories and
combine these. the first of these approaches may result in discarding a large proportion of the available
data and may itself cause bias and the latter requires the form of the changes within individuals to be
characterized. we have developed an approach to the modeling of the median, spread, and skew across
individuals using maximum likelihood, which can incorporate correlations between dependent observations.
heavily biased data sets are simulated to illustrate how the methodology can eliminate the biases
inherent in the data collection process and produce valid centiles plus estimates of the within-person correlations.
the “select one per individual” approach is shown to be liable to bias and to produce less precise
centiles.we recommend that the maximum likelihood method incorporating correlations be used with existing
data sets. furthermore, this is a potentially more efficient approach to be considered when planning
the future collection of data solely for the purposes of creating cross-sectional covariate-related reference
ranges.
keywords: age-related reference ranges; correlated measurements; dependence; serial measures; unbiased;
z-scores.
1. introduction
covariate-adjusted reference ranges may be used to assess individuals at a single point in time (crosssectional)
or to monitor changes within individuals over time (velocity or conditional). most commonly,
the covariate used is age. if an individual presents for diagnosis/assessment and repeat measurements are
available, then it will generally be advisable to utilize all of these. however, there are many occasions
on which only a single measurement is available and this needs to be evaluated against population values
using a covariate-adjusted cross-sectional reference range.
the methodologies for constructing population-based covariate-adjusted cross-sectional reference centiles
are now well established and were recently reviewed by the world health organization (borghi and
others, 2006, for the world health organization [who] multicentre growth reference study group).
these methodologies commonly assume that the measurements used for construction are independent. if
the data set contains serial measurements from individuals, then these will be correlated within person and
hence the independence assumption is not satisfied. one approach has been to circumvent the problem
by systematically or randomly selecting one observation per individual to create a data set of independent
measurements (e.g. kurmanavicius, wright, royston, wisser and others, 1999; kurmanavicius, wright,
royston, zimmermann and others, 1999; wade and ades, 1994). however, this is wasteful of the data
and may lead to bias.
this paper is concerned specifically with the construction of cross-sectional reference ranges using
serial measurements from individuals. the need for any marginal analyses to include assumptions about
the form of the correlation has been well documented within other applications (diggle and others, 2002).
the laird–ware model (1982) gives a general framework for modeling which allows for variable spacing
of observations and varying structures between individuals. the estimation of parameters for this model
form has received much coverage (davidian and giltinan, 1995; hand and crowder, 1996; vonesh and
chinchilli, 1997; lindsey, 1999; diggle and others, 2002). however, very little of the available literature
applies specifically to the reference range problem. within the majority of texts, characterization of the
average pattern is of primary importance followed by estimation of the covariance/correlation structure
between repeats within individuals where these exist. when reference ranges are to be constructed, the
estimation of any skewness and the spread of values at each covariate are at least as important as quantification
of the median. this shift of emphasis is necessary as it is usually the extreme centiles that are
of most clinical use. by contrast, estimation of the covariance/correlation structure is generally of little or
no direct interest in this scenario.
the who review recommended the use of methodologies that model the covariate-related changes
in distributional features and then combine these to obtain centiles. commonly, the underlying distribution
is assumed to be some transformation of the normal distribution and the kurtosis, skew, spread,
and median are modeled. the form of the models used for the distributional features and the mode of
identifying the best fit parameters vary according to the specific method chosen (cole, 1988, cole and
green, 1992; wright and royston, 1997). previously, we used this methodology within a maximum likelihood
framework with exponential models to create age-related centiles for cd4 counts (wade and ades,
1994), randomly selecting one measurement per child. we subsequently extended the approach by incorporating
suitable correlation structure into the likelihood and thus additionally modeled the correlation
between repeats from the same individual as smooth functions of age and time (wade and ades, 1998),
hence utilizing the entire data set. this extension may be viewed as a generalization of models developed
to identify trends in longitudinal data with explicit modeling of the serial correlation (diggle, 1988).
despite a strong correlation structure, incorporation did not substantially alter the fitted centiles. this
finding was not unexpected because measurements were made at ages defined within a strict protocol,
and hence, frequency of measuring was independent of previous measurements. a similar approach that
has been used is systematically, as opposed to randomly, to select one measurement per individual. for
example, kurmanavicius, wright, royston, wisser and others (1999) and kurmanavicius, wright,
royston, zimmermann and others (1999) used only the first of serial measurements made during pregnancy
to create cross-sectional centiles for fetal biometry.
while it is generally appreciated that incorporation of dependent observations without adjustment for
correlations will lead to overestimation of centile precision, the propensity for bias invalidating the centiles
has received little discussion. when the number and/or timing of observations are related to outcome,
for example, when an abnormal measurement is likely to trigger more frequent assessment for clinical
purposes, then the incorporation of correlations may have a large impact on the centiles by reducing
or removing the bias inherent in the collection process. the problem is one of informative observation
times, whereby future measurement frequency is related to the values of existing measurements for that
individual (lin and others, 2004).
the who review (borghi and others, 2006) identified the following 2 approaches that incorporated
correlated measurements into the construction of cross-sectional reference ranges. laird and ware (1982)
proposed 2-stage random-effects models, while goldstein (1986) proposed a more general framework of
multilevel models which could be parameterized to allow for complex covariance structures and multiple
explanatory variables. marginal distributions obtained from these models would identify cross-sectional
patterns of change (pan and goldstein, 1997). while these models are flexible and present a solution to
the specific problem posed here, they require explicit characterization of a common underlying form for
expected trajectories within individuals. goldstein and others (1994) recognized that the methodology
for conditional (longitudinal) references can theoretically yield cross-sectional references. the second
approach cited by the who review was our previously described maximum likelihood method (wade
and ades, 1998) requiring characterization only of population changes irrespective of how individual
trajectories vary.
in this paper, we illustrate how biases may be removed and precision increased via appropriate modeling
even for heavily biased simulated data sets. we compare the precision with which centiles are
estimated when correlations are incorporated versus the alternative systematic or random “select one”
approach. the methodology is illustrated by application to serial fetal ultrasound measurements collected
at the university hospital in zurich (uhz) and previously modeled using only a subset of the
data (kurmanavicius, wright, royston, wisser and others, 1999; kurmanavicius, wright, royston,
zimmermann and others, 1999).
2. methods
2.1 statistical methods
in previous papers, we have demonstrated the use of splines, fractional polynomials, and exponentials
within the maximum likelihood methodology. any data collection protocol can be accommodated, as
can any amount of variation between the number and timing of measurements per individual. formal
significance tests are easily performed between nested models and confidence intervals constructed for the
model parameters and/or the centiles (wade and ades, 1994, 1998). thompson and fatti (1997) extended
the methodology to create multivariate centile charts. in the analyses presented in this paper, we assume
that a transformation of the normal distribution is appropriate at each covariate value and we model the
changes in the skewness, spread, and median. we maximize the likelihood incorporating a correlation
structure between repeated measurements within the same individual.
we used fortran programs incorporating numerical algorithms group subroutines, which are available
within the supplementary material, available at biostatistics online. an alternative would be to use generalized
additive models for location, scale, and shape (gamlss; rigby and stasinopoulis, 2005). the
gamlss command in r can be used to fit centiles with incorporation of random effects for individuals to
account for serial correlation.
2.2 simulations
full details and results from the simulations are given in the supplementary material, available at biostatistics
online. the features and findings were as follows.
underlying models were assumed so that median and spread were both increasing with gestational age,
as this would be typical in most applications. simulations were based around a scenario often encountered
during pregnancy where measurements are made between 15 and 40 weeks and abnormally low values
trigger additional repeated measurements. repeated measurements within an individual were generated
with an exponentially decaying correlation function.
the extent of bias in the data sets was dependent on how the frequency of repeat measurements was
determined. fitted centiles based on an assumption of independence were heavily biased. the extent of
the correlations between repeats was typically underestimated within the correlation models, although the
centiles obtained were not biased. both precision and accuracy were improved for the correlation model
compared to that assuming independence.
with “select one”, the precision of the centile estimates was reduced and the centile estimates were
biased. this latter finding shows that selecting a subset of independent measurements does not necessarily
yield unbiased centiles. at later gestations, there were more measurements from those fetuses previously
with abnormally low values and hence there was more chance of selecting biased assessments in this
gestational age range.
3. application to ultrasound data set
3.1 the data set
ultrasound measurements were taken from clinic records of pregnant women examined at the uhz, where
routine examinations were performed at 11–13, 18–21, and 28–32 weeks of gestation. high-risk pregnancies
were examined at shorter intervals, every 2 or 3 weeks until delivery. referrals at later gestations from
other ultrasound centers were also included. a relatively common reason for such referral would be suspected
intrauterine growth retardation (iugr) due to placental insufficiency which manifests after 25–28
weeks of gestation with reduced growth of the fetal abdomen. such women then undergo repeat tests until
a definitive diagnosis is made. small values of abdominal circumference (ac) indicate potential iugr,
whereas biparietal diameter (bpd), a measure of skull size, is not expected to be affected by iugr. the
only measurements excluded from the data set were for fetuses found to have a congenital abnormality.
the original analysis used the first fetal measurements made between 12 and 42 weeks from 6557
pregnant women (6557 bpds and 5807 acs). fractional polynomials were used to model age-related
changes in the mean and standard deviation, and shapiro–francia w test was used to check the normality
of the z-scores. for these 2 fetal measurements, a linear cubic in age for the mean and linear model for the
standard deviation were found to be suitable (kurmanavicius, wright, royston, wisser and others, 1999;
kurmanavicius, wright, royston, zimmermann and others, 1999).
the current data set, which has expanded since its use in 1999, consists of information from 12 480
women measured between 1 and 28 times. a total of 48 005 bpds and 45 352 acs are included. hence,
any select one approach would utilize only about 25% of the available measurements. since the purpose
was to illustrate the effects of this modeling, we used the same model forms as kurmanavicius and others
had previously (linear-cubic model for the mean, linear model for the standard deviation, and no skew)
and estimated only their parameters. we allowed the correlation between repeats from the same individual
to fall as the time between those repeats increased and characterized this as a 2-parameter exponential
model (ρ1e−ρ2diff). hence, we estimated 5 parameters for the independence and select one models
(3 for the mean and 2 for the standard deviation) and an additional 2 (for the correlation structure) for
the correlation incorporated models. we compare the fitted centiles with those previously presented by
kurmanavicius, wright, royston, wisser and others (1999) and kurmanavicius, wright, royston,
zimmermann and others (1999).
3.2 fitted models
figures 1(a) and (b) show the 5th, 50th, and 95th centiles from the independence models, the exponentially
correlated models, and those previously presented by kurmanavicius, wright, royston, wisser and
others (1999) and kurmanavicius, wright, royston, zimmermann and others (1999). for both ac and
bpd, taking into account the correlations reduces the centile range at earlier gestations. this pattern
is compatible with a greater frequency of measurement of fetuses with extreme values. the centiles fitted
by kurmanavicius, wright, royston, wisser and others (1999) and kurmanavicius, wright, royston,
zimmermann and others (1999) lie between the correlation and independence models at early gestations
but become increasingly like the independence centiles at later ages.
the effect of incorporating correlations on the centiles at later gestations differs between ac and
bpd. the increased ac 5th centile beyond 30 weeks of gestation may be explained by the inclusion of
late referrals to uhz for suspected iugr. by contrast, bpd is a skull measurement, large values of which
may be of greater concern near to term (40 weeks of gestation). the pattern of differences shown in figure
1(b) suggests that the fetuses with larger bpd were more likely to be measured more frequently in the last
5 weeks of pregnancy. incorporation of correlations reduces the effect of these larger measurements, and
the centiles based on the correlation model are lower.
figures 1(a) and (b) show how incorporation of correlations can remove the selection bias inherent
in clinical data sets. the patterns observed were not anticipated but with hindsight have clinically valid
fig. 1. estimated 5th, 50th, and 95th centiles. the solid lines show the estimated centiles when all measures are
assumed independent and the thick dashed lines when correlations between repeats are incorporated. the lighter
dashed lines show the fitted centiles as presented by kurmanavicius, wright, royston, wisser and others (1999) and
kurmanavicius, wright, royston, zimmermann and others (1999). (a) ac obtained using 45 352 ultrasound measurements
from 12 480 pregnancies. (b) bpd obtained using 48 005 ultrasound measurements from 12 226 pregnancies.
explanations. the results show that the adjustments for correlation will not be uniform in either direction
or quantity even for seemingly highly related measurements, that is, different assessments of growth
from the same ultrasounds in the same group of women. it is perhaps surprising that the centiles based
on the first measurement from each fetus (kurmanavicius, wright, royston, wisser and others, 1999;
kurmanavicius, wright, royston, zimmermann and others, 1999) were more akin to those obtained from
the data set of all measurements. however, this is compatible with the finding of the simulation study.
selecting an independent subset does not necessarily remove bias as late referrals to uhz are probably
atypical.
4. discussion
our simulations and application demonstrate that incorporation of a correlation structure within the fitting
algorithm is to be preferred to the select one approach. although select one is computationally simpler, our
analysis shows that the precision and accuracy of the centiles may be severely affected. the simulations
were developed to represent typical clinical scenarios. the irremovable bias obtained was not expected,
although easily explained with hindsight. it is important to note that such biases may occur in any data set
and will yield biased centiles if select one is used.
simulations are necessarily limited by choice of the parameters and assumptions incorporated. in
particular, we assumed very similar correlation structures within the data generation and fitting phases.we
do know that if the correlation structure is severely misspecified, then this will lead to invalid estimation.
the comparison with the independence model of zero correlation between repeats within individuals is an
extreme case and clearly illustrates this point. there must therefore be some degree of misspecification
that can lead to invalidation of the centiles. for all cases where we modeled the correlation structure, we
assumed exponential decline with increasing time, a reasonable assumption for clinical applications. the
extent of any decline was estimated, and estimates were often biased. however, the model did allow for
any extent of decline (including zero), and the simulations illustrate that biased parameter estimates do
not necessarily lead to biased centile estimates. when constructing cross-sectional reference ranges from
correlated data, estimation of the correlation structure is not of direct interest, but rather is a means toward
the important end of obtaining unbiased estimates of centiles.
the data set we describe consists of longitudinal measurements with informative observation times.
techniques that specifically model the timing mechanism could be employed and would give additional
information. however, this would necessitate the specification of the conditional distribution of an observation
given the history of the process and the centile estimates may not be robust to misspecification
(lin and others, 2004). for the purposes of creating cross-sectional covariate-related reference ranges, the
biases in the measurement process that we wish to eliminate are a function of the clinical scenario. our
simulations show that the method we present is capable of producing unbiased centiles from messy data
sets of the form likely to be found in clinical practice. the extent and direction of centile adjustment when
correlations are incorporated may yield information about the nature of biases inherent in the data set.
despite the numerous advantages of incorporating some form of correlation, the technique has not
been widely used, perhaps because of a mistaken belief that the added complexity of modeling is not
warranted. in this paper, we have shown that simpler methods applied to data with informative observation
times lead to invalid centile estimation. the process of selecting the first observation per individual,
as previously used by kurmanavicius, wright, royston, wisser and others (1999) and kurmanavicius,
wright, royston, zimmermann and others (1999), may lead to a biased solution and necessitates discarding
a large proportion of the data. the resulting reduction in precision will be a function of the percentage
of data discarded and the extent of the within-individual correlation. the reduction may not be uniform
across the age range since selecting the first measurement from each pregnancy will lead to greater loss
of precision at later gestations where there will be fewer women presenting. the uhz received referrals
for suspected problems, based on abnormal ultrasound measurements, from other hospitals. while fetuses
subsequently found to have a congenital abnormality were excluded, biases are still likely to remain. the
simulation results show that incorporating correlation structure into the modeling has the capacity to reduce
biases such as these in estimating centiles. the extent of the bias reduction will depend on the degree
to which the correlation structure has been adequately modeled.
if data are to be collected specifically for the purposes of creating cross-sectional reference ranges,
then the most precise centiles for a given total number of observations will be obtained when these observations
are independent. however, there may be a trade-off between the recruitment cost per individual
and the cost of following individuals serially (goldstein, 1979). if subsequent measurements for a recruited
individual are easier and/or cheaper to obtain than measurements from new recruits, then some
consideration should be given at the design stage to the most efficient way to proceed. it may be ethically
easier to justify serial collection from a smaller pool of prospective subjects, or the pool may be
limited (e.g. children born to hiv-1-infected mothers). often, the remit is to produce both cross-sectional
and conditional or velocity references (borghi and others, 2006). in this case, the optimal approach will
be to incorporate serial observations into the cross-sectional references using appropriate adjustment for
correlation within individuals. the precision with which centiles are estimated under differing correlation
structures, model forms, and/or sample sizes can be readily compared using simulations to identify the
most appropriate recruitment method to use.

<|EndOfText|>

our one-day introduction to sample size course has run 20 times since 2012. comprehensive
written notes are given for use in conjunction with custom made excel sheets to perform
calculations and these are openly available via a weblink. immediate feedback for the course is
generally excellent. to gauge the extent to which course materials had ongoing usage, and whether
post leaving the course perception of the material and its’ usefulness remained, a short survey was
emailed to all 383 course participants from the 20 courses. attendees noted ongoing usage of the
course materials, with over two thirds of respondents subsequently referring to these frequently or
occasionally. even the most conservative estimate showed a substantial proportion still gaining
direct benefit after 5 years.
background
for 10 years the ucl centre for applied statistics courses (casc) has presented 1 to 5
day courses primarily aimed at non-statisticians in the workplace. attendees often aim to leave a
course equipped to undertake their own research, as well as having an improved understanding of
published research. the one-day introduction to sample size estimation has proved highly popular
and runs on average three or four times per year with an audience of up to 20 participants. numbers
are restricted by the fact that teaching takes place within a cluster room and participants are
directed to custom made excel sheets to perform calculations. the excel sheets were originally
available via cds given to students but are now accessed via a web link to which there is open
access (http://www.ucl.ac.uk/ich/short-courses-events/about-stats-courses/samplesize ). a folder of
comprehensive course notes is given to each course participant detailing all course material and
usage of the excel sheets with examples.
all casc courses request feedback from students via opinio, a web-based tool used to
gather ratings and comments. this feedback, collected shortly after the day of the course, is
consistently positive. there remains the question of how useful the participants find the training
and material on an ongoing basis. the sample size course is unique in that the excel sheets are
presented and their usage integral to the estimation processes introduced to the students.
furthermore, the excel sheets remain available to the students for as long as they wish to use them.
therefore, the sample size course, with its very specific estimation tools as well as written notes,
was ideal for beginning an investigation into the ongoing potential benefits of our training we
decided to contact attendees to the sample size courses held over the last five years to evaluate
whether they continued to use the excel sheets and whether the course had in fact been of direct
relevance to their work.
the aim of this survey was to address the research questions:
i) did participants of the one day sample size course utilise the information and tools given in
the course in their work life?
ii) was usage related to time since attendance at the course?
data collection
an online survey was developed asking whether individuals had referred back to the course
material and whether they had continued to use the excel spreadsheets. both of these questions had
the response options: ‘yes frequently’, ‘yes, occasionally’, ‘rarely’ and ‘no’. the individuals
were also asked whether what they had learnt on the sample size course had been of direct
relevance and use to their work, with response options ‘yes’ or ‘no’. all questions had space for
comments and there was an additional overall comment box at the end asking for any other
suggestions that may help improve the one-day introduction to sample size course. potential
participants were sent an email requesting that they complete the web survey (via a link), which
would consist of only 3 questions. by making the survey short and informing of length in the
accompanying email, it was hoped to maximize the response rate (galesic and bosniak, 2009).
the survey was sent via email to all 383 participants of the 20 one day courses held
between 2012 and april 2017. there were 46 emails (12%) returned undeliverable with the
proportion falling over time from 17/74 (23%) attendees at the three courses in 2012 to only one of
the 39 (2.5%) in the two courses in the first half of 2017. since the survey was anonymous, and we
did not wish to alienate future participants by repeated emails, no additional requests were made to
those who may not have responded.
results
from the 337 emails that were successfully sent, responses were received from 62 (18.4%).
most of the respondents (39/62, 63%) completed the form in a minute or less and over 90% in less
than 5 minutes (56), two outliers were 26 and 13 minutes. over 80% (50) replied within 48 hours,
and all but one within eight days. response rates were not dependent on time since course
attendance being between 16 and 20% for all but two years, which were 2014 (13.9%) and 2013
(25%).
two people skipped the question about whether what they had learnt on the one-day
sample size course had been of direct relevance to their work. of the 60 that submitted a completed
survey form, the overwhelming response was ‘yes’ (55/60, 91.7%) and this was consistent across
years. the five who did not find the course relevant, rarely (3) or never (2) used the course material
and also rarely (1) or never (4) returned to use the excel sheets.
all individuals answered the usage questions. most referred back to the course material and
the excel spreadsheets occasionally or frequently. there was a strong correspondence between
responses from the two questions (table 1). two individuals occasionally referred back to the
course material but did not use the excel spreadsheets, a single individual made occasional use of
the spreadsheets but did not refer to the course material.
table 1: ongoing usage of course material and excel spreadsheets responses
have you returned to the excel spreadsheets and used these?
yes, frequently yes,
occasionally
rarely no total
have you referred back to the
course material?
yes, frequently
yes, occasionally
rarely
no
total
7
1 0 0 8
3
24 6 2 35
0
3 6 6 15
0
1 0 3 4
10 29 12 11 62
when analysed according to year of attendance, there was no consistent pattern of more
usage either if the course was more recent nor with increasing time since receiving (figure 1).
figure 1: temporal changes in responses to usage questions
comments
a total of 53 comments were left by 34 individuals. comments attached to the usage
questions were most often given by those who responded ‘rarely’ or ‘no’.
there were six comments following the question about usage of course material with half
from individuals who cited rare usage but explained their lack of need rather than problems with
the material. one person who said they never referred back to the course material commented that
they disliked using excel, whereas another person who rarely used the course material stated
difficulties following the concepts. the final comment was from someone who occasionally
referred back to the course material (“2 colleagues have asked me to help them with a power
calculation and one colleague borrowed the course file”).
there were 8 comments given following the question about usage of the spreadsheets,
including the person who disliked excel and reiterated their previous comment. two of those
commenting on this question stated that they had gone on to use stata (occasional user) or g power
(never user). two individuals said they occasionally used the excel sheets and gave positive
comments. the remaining three comments were very positive despite recording rare (“however, i
think that this material it's very useful for working with”) or no (“i think it will come useful when i
need to calculate sample size“ and “while i am very glad to have them and the process of
reviewing them certainly helped understanding, the design of of (stet) research does require the
calculations used the spread sheet“)
of the 10 comments left after the relevance and usefulness in work question, eight were
from individuals who answered positively and the only minor criticism was that stata may have
been better to use on the course(“it was interesting but using more traditional software such as
stata would have been far more helpful.”). two comments were from the five who said the course
was not of direct relevance and use to their work. they said, “i typically use a repeated measure
design, which was not covered in the workshop” and “it will be useful when i get to design my own
study and as i have been developing a proteomics method i have not needed to use the course
info”.
27 of the 62 respondents left an overall comment at the end of the survey. of these 16
made a comment that was purely positive with half of these specifically mentioning the usefulness
of the excel sheets. there was only one negative comment, from someone who attended the course
in 2014, rarely used material or excel sheet and did not find the course relevant to their work they
said they “did not learn anything as it was not explained”. however, a review of the opinio
feedback from 2014 did not yield any such negativity. three comments raised the issue of
alternative software, with 2 of these also praising the excel sheets. one user of gpower mentioned
the need to quote software used for publication, “i currently use g*power3 as it is more flexible
than the spreadsheets. although the spreadsheets were useful i wonder if sometime spent on
recognised software would be useful. i say this because, as part of getting published i have had to
quote the software i used and the parameters i put in to generate the sample size and using
g*power3 the reviewers can check my work”. five comments were from individuals who would
like more advanced topics covered: these were sample size calculations for repeat measures (2
comments), non-inferiority trials (these are given a brief mention in the course), unspecified
‘epidemiological studies’ and multiple groups (such as 5 ethnic groups). despite each formula
included in the course having at least one fully worked through example of usage, one participant
wanted more. their comment was “more worked examples with answers would be useful when
returning to the spreadsheets to be sure of using the correct one and inputting the correct figures”.
the final comment felt the course might be shorter, but seemed generally positive, “was good,
could be shorter, or just a half-day for more advanced users of excel/stats -overall it was very
useful, if anything, it highlighted an often neglected issue in science ”.
discussion
a substantial proportion of those responding to the survey continued to refer to the course
material and/or to use the online excel spreadsheets for calculation. some of those that did not
currently use the material still cited the potential usefulness and intended to do so. almost all felt
that the course had been of direct relevance and useful to their work. there was no evidence of
these findings being related to time since course attendance.
strengths and limitations
the response rate was low at 18.1%, but this is to be expected with a one shot survey of
this type. online survey response rates below 10% are not uncommon and rates are declining due
to the volume that individuals currently receive (van mol, 2017). those responding may have been
biased towards those who did engage with the material. conversely though they may have been
biased towards those who found the course not very helpful. however, low response does not
necessarily infer bias (fosnacht et al, 2017).
we had access to email addresses of course attendees over the last five years. all
presentations of the course were led by the same teacher and the content of the course remained
similar throughout. we were asking about current usage and so we could evaluate the long term
effect of the course. the numbers of non-deliverable emails declined over time as would be
expected, but the response rates within those whose emails which were delivered did not show a
temporal pattern.
if we consider the numbers responding positively from those where an email was delivered
(ie. assume a worse-case scenario that those who did not respond rarely or never used the materials
or spreadsheets) there remains 14% (95% ci (7.3, 25.3%)) of those who attended the course five
years ago still gaining direct benefit occasionally or frequently from the course materials and the
excel sheets. we can consider this a very conservative lower limit.
conclusion
this study gives evidence of a long term benefit of attendance at the one day sample size
course, with over two thirds of respondents who had attended the course stating that they
subsequently referred to the course materials frequently or occasionally. even the most
conservative estimate shows a substantial proportion of attendees continuing to use the course
material directly in their work after 5 years.

<|EndOfText|>

abstract
why might the average paediatrician need to get involved in understanding
statistics? what do they need to know? are there simple rules that can be followed
in determining the appropriate analyses? where can help be found?
these are the questions that we aim to answer in this short review of
how to design and analyze research studies.
keywords statistical-analyses; study-design; critical-appraisal
introduction
in the modern day there is a plethora of information available at the
practicing clinicians’ fingertips. between 100 and 150 paediatric
journals produce on average about 150,000 publications related to
child health annually and these, together with research presented in
journals not solely confined to paediatric medicine, contribute
roughly 2 million paediatric related articles within pubmed online
resource. in the us there are currently around 30,000 clinical trials
being undertaken on children and these are mainly initiated by
universities and institutes rather than drug companies.
so, with all this happening, why is there any need to be able to
understand the research process and statistics?why not just leave it
to the experts to do the work and provide the results?
the problem is that not all published, or even all peer-reviewed,
information is good information. studies are not always optimally
performed and analyses may be sub-standard. anyone wishing to
use the available literature to inform practice needs to be able to
critically appraise content and not just skim abstracts. there is
usually no malicious intent with badly performed research, but
nonetheless it frequently exists and the reader needs to be able to
identify potential flaws as well as useful and applicable research.
even if a study has been well designed, executed and presented,
there will need to be verification that it is indeed generalisable to
any context the reader may wish to apply the results in.
to be fully commensurate with all aspects of medical statistics
from research design through to analyses and interpretation of
results however is too much of a daunting task for the majority of
practicing clinicians. thankfully, such in-depth knowledge is not
necessary to be able to discern the good from the bad and to gain
useful insights into the quality of publications, nor indeed to
perform ones own research study. what is necessary is to understand
the basics of design and also the thinking that underlines
statistical analyses. this, plus the awareness to recognize where
a fairly basic analysis may not be appropriate and it is time to enlist
more expert help, should stand most readers in good stead.
what do we want to know?
whether reviewing the work of others or aiming to undertake our
own research, the first thing we need to do is to establish the
correct research question.
sometimes data is collected primarily for clinical purposes
and research a by-product i.e. individuals’ have information
documented for their own benefit, but these individual bits may
be combined within a research project. alternatively, data may
be collected specifically to address the research question posed.
in the latter case data may be obtained that can more accurately
address the research question.
it is generally useful to consider a pico breakdown of the
research question:
p e patient, problem or population
i e intervention or exposure of interest
c e comparison
o e outcome
for example, we may be interested in how to cure nits or find
the risk factors for early onset asthma but these questions are too
loose and it is not easy to see how they may be answered via
a single study. by contrast, the questions:
“in primary school children, does combing with conditioner
post shampooing (as opposed to just shampooing) lead to less
infestations of nits?”
and
“are family history and introduction of solid food prior to
6 months associated with onset of asthma before age 10?”
are clearly answerable with obvious pico elements.
note that not all elements occur in all research questions. for
example, “what is the prevalence of asthma in 5 year olds?” is an
answerable well-defined question that has a ‘p’ (5 year olds) and
an ‘o’ (asthma yes/no), but no i or c.
it is, however, useful to consider in each case whether there
should be a p, i, c and o and what these are.
design, design, design!
design is of over-riding importance in any study. with even the
most sophisticated elegant analyses, if basic data collection set
up is less than ideal, then the conclusions must be tempered to
take account of this.
there are oftenmany differentways to address the same research
question. if you are undertaking your own research, then youwould
want to ensure that the most efficient and valid feasible design is
used. if you are evaluating published research, then the decision to
be made is whether the design they have used, even if not optimal,
can usefully address the research question posed. in either case, it is
worth remembering that it may be impossible to implement the best
theoretical design due to practical, financial or ethical limitations.
not all designs fit into a neat framework. figure 1 shows the
most commonly cited designs where two groups of individuals
are to be compared. they are split into observational studies,
where the researcher does not change things but merely observes
what is happening, and experimental where there is some
manipulation of individuals for the study purposes.
when observing individuals there are various approaches that
can be taken:
i) consider past habits etc. that are associated with current
outcomes (case-control).
ii) consider associations in the present time (cross-sectional).
iii) classify individuals and follow forward in time to observe
outcomes (cohort).
with experimental studies the researcher gets to decide who
goes into which group. the split can be done systematically (nonrandomized)
or randomly (randomized controlled trial). a before
and after trial is a form of non-randomized experimental study
which is rarely recommended as there will be no measure of what
would have happened in the absence of treatment. randomized
controlled trials (rct) where individuals are consented to the study
and then randomly allocated to a treatment arm are generally
considered the most effective way to show causal relationships. a
crossover (or within person) rct is where each individual receives
both treatments in random order. a crossover trial is less likely to
have hidden confounders but will only be appropriate to investigate
a treatment giving short term relief of chronic symptoms.
sometimes the associations between behavior and outcomes
of individuals are considered on a grouped basis:
if the study is observational then this form of study is often
known as ecological. for example, an ecological study compared
childhood cancer rates between those born in hospitals with
differing intramuscular vitamin k policies. no evidence was found
of an association and this counteracted previous suggestions that
administration of vitamin k may increase childhood cancer risk.
if the study is a rct, then the groups will be randomized en
masse to different treatment arms and this is known as a cluster
randomized trial. for example, villages were randomized to
different educational programs in a bid to reduce infant mortality in
rural areas of nepal. the outcome for babies born in villages which
received the new educational program were compared to those in
the villages which did not receive additional input (controls).
note that the main named study types are given here but
there are plenty of studies that do not easily pigeonhole into
these mainstays and yet are equally valid.
is there anything that might get in the way of any
comparisons?
the majority of studies are designed to address comparative
research questions. for example: do children with sickle cell
anaemia have different scd163 and/or plasma haptoglobin levels?
does giving antenatal corticosteroids (ac) reduce respiratory
disorders in late preterm infants?
whether the study is observational or experimental, things we
are not interested in may get in the way. they do this by having
different distributions in the groups being compared and also
being associated with outcome. when a variable behaves like
this it is known as a confounder.
often studies are designed specifically to avoid confounding.
groups may be chosen to be similar with respect to the potential
confounder(s), for example age and sex matched pairs, stratified
randomization. a problem is that there may be many potential
confounders, some of which are not even known, and it is not
possible to consciously correct for all. a strength of randomized
trials is that in a large enough trial all factors will be evened out
between the groups and hence there will not be confounding.
for example to address the question as to whether ac reduces
respiratory disorders in late preterm infants, we need two groups
of preterm infants: those given ac or not. there are several ways
in which these two groups can be chosen relating to different
types of study. i.e.:
a) an ecological study: outcomes compared between hospitals
with different ac policies.
b) a case-control study: babies with and without respiratory
disorders compared with respect to ac history.
c) a cohort study: ac and not ac groups classified then followed
to see who develops respiratory disorders.
d) a randomized controlled trial of ac versus not ac.
in designs (a)e(c), the observational studies, there is far more
capacity for confounding and a causal relationship cannot be
proved. with (d) there is less chance of confounding and it is
possible to infer causality if a difference is found.
how generalisable are any results?
generalisability is an issue whether the study is observational or
experimental. a random sample of the target population (of the
research question) should ideally be drawn whatever the study
design. it is worth remembering that such an ideal sample is rarely
obtained. for example, individuals were randomly sampled from
those registered with the two largest unions in hong kong to
investigate associations between lifestyle and obesity; schools were
randomly selected in the athens region when assessing the validity
of a food questionnaire amongst school children; different doses of
vitamin d were compared in a rct in schoolchildren in taleghan
near tehran. although these may all provide suitably representative
samples, we should consider whether they actually are
generalisable outside of the groups the selection was from (unionregistered
individuals in hong kong, schoolchildren in the athens
region or taleghan). very often there is no need to worry with rcts
in particular since what works in one sub-population will probably
also work elsewhere. for example, if a rct shows that a preparation
is beneficial to asthmatics in liverpool, then it will probably
also be beneficial to asthmatics in the rest of the country (and
possibly the world), so long as our population of asthmatics is
similar. with observational studiesmore caution may be warranted
as there is greater potential for confounding and effects may not be
constant across different communities.
similarly, we should be happy that any sampling is representative
on a time basis. for example, if a drug is trialed exclusively
in summer, will the effects be similar in winter? when dealing
with children there are often differences in diet and exercise during
term time and outside which may confound the results if not
properly adjusted for. weekends and weekday habits also often
differ: the greek children were asked to complete food questionnaires
on two consecutive weekdays and one weekend day.
if comparisons are made over a long time period then causal
agents may be very difficult to infer as there may be underlying
improvements for all kinds of related reasons. for example,
survival rates increased and levels of severe neurodevelopmental
impairment decreased substantially between 1998 and 2003 for
inborn infants with birthweight below 1000 g, but it would be
hard to pinpoint one single causal factor.
summaries
for any quantitative study, information is recorded for each individual
and the values combined within the study groups to provide
summaries of similarities and differences. the appropriate form of
summary depends on whether the particular piece of information is
numeric (such as height, weight, scd163) or categoric (such as
family history: yes/no, severity of disease: mild/moderate/severe,
gender:male/female).acategoric variable with only two categories
(eg. yes/no, male/female) is known as binary.
numeric data should be summarized as either mean and standard
deviation or median and range or inter-quartile range. the
median is the 50th centile, with half the values being higher than
this and half lower, whatever the distribution. the inter-quartile
range is the 25the75th centiles i.e. the ‘middle half’ of the data if we
chop of the top and bottom quarters. if the distribution of the
variable is skew (tailing off in one direction), then the mean and
standard deviation should not be used.
for example, height does not have a skew distribution since
there are as many short as there are tall people whereas incomes
do have a skew distribution as there is a minimum level of earning
and relatively few very high earners. for height, the mean and
median will be approximately equal and either provides a good
summary of average, the standard deviation can also be used. for
income levels, the mean will not be representative of average
earnings as it will be skewed by the few high earners, whereas the
median and inter-quartile range will give a useful summary.
differences between groups can be quantified as the difference
in means or medians as appropriate.
categoric data can be summarized using proportions who fall
into one category and differences between groups quantified as
either the absolute or relative difference. these are sometimes
known as the arr (absolute risk reduction) and rr (relative
risk) respectively. for example, a recent study showed antenatal
corticosteroid treatment at 34e36 weeks gestation did not reduce
the incidence of respiratory morbidity (36/143 ¼ 25% corticosteroid
group; 30/130 23% placebo group). hence arr¼ 25e23
¼ 2% and rr¼ 25/23 ¼ 1.09. respiratory distress syndrome
(rds) incidences were also similar (2/143 ¼ 1.4% and 1/130 ¼
0.77% respectively, yielding an arr of 0.6% and rr of 1.40/0.77
¼ 1.82).
note that no difference in absolute terms is given by zero,
whereas for relative comparisons, no difference is defined as 1.
an approximation to the rr which is often used is the odds
ratio (or), for which a value of 1 similarly means no difference
between groups.
how many is enough?
it is important to study enough individuals to address the
research question but unethical to waste time and subject more
individuals to scrutiny than is necessary. all studies should have
some stated rationale for the proposed numbers to be included
prior to study commencement.
there are many different sample size formulae and the
correct one to use depends on the nature of the outcome
(numeric or categoric) and the purpose of the study (to identify
a difference between groups or to estimate a quantity with
sufficient precision). each formula will require you to provide
several pieces of information, such as the size of difference
that you want to detect, or the precision required. note
however, that there is a circular argument to sample size
estimation which may initially appear confusing. calculation is
also based on outcome and if you had all the information to do
the calculation properly you wouldn’t need to do the study!
hence, these calculations can only ever be educated guesswork
but nonetheless it is always worth guessing. some web
links which contain suitable applications are given at the end
of this article.
analyses
figure 2 shows when to use the seven most commonly cited
tests for comparing a single outcome between two groups. the
aim is either to compare the proportion in one category of
a categoric outcome (left hand branch) or the mean (right hand
branch, left side) or median (right hand branch, right side)
between the groups. if the groups are paired (for example, age
and sex matched pairs of individuals in the two groups or
a crossover treatment trial yielding paired measurements within
person) then the appropriate test is mcnemars (categoric
outcome), paired t-test or wilcoxon’s test. if there is no pairing
then the appropriate test to compare the groups is chi-square or
fishers exact (categoric), two sample t-test or mannewhitney u
test respectively.
for example:
chi-square test was used to compare respiratory morbidity
between ac and placebo treated groups (36/143 vs 30/130, p ¼
0.69). the lownumbers with rds (2/143 and 1/130) meant that
fishers exact test was used for this comparison (p¼0.54).
a two sample t-test would be used to compare scd163 levels
between patients with and without sickle cell anaemia.
mannewhitney u test would be used to compare plasma
haemoglobin levels which have a skew distribution.
if each child with sickle cell anaemia had a matched control
selected from their class at school, then the data would consist
of matched pairs and the appropriate tests would be a paired
t-test and wilcoxon paired test for the scd163 and plasma
haemoglobin comparisons respectively.
if the aim is to make a comparison after adjustment for other
factors or to build a predictive model, then regression will be
appropriate. the type of regression to use depends on the nature
of the outcome. table 1 gives an overview of what type of
regression to use when.
for example:
to compare respiratory morbidity rates between those given
ac or not, adjusting for gestational age and gender, we would
use a logistic regression since the outcome (respiratory
morbidity) is binary (yes/no).
poisson regression is used for discrete (count) data and results
are given as relative risks (rr). for example, a recent study
showed that there was a significant decrease in mortality in
the delivery room for babies with an estimated ga of 22
weeks between 1998 and 2008: rr ¼ 0.47 (95% ci (0.24 to
0.93)).
linear regression would be used to compare scd163 between
sickle cell children and controls taking into account age and
weight.
the extent to which onset of childhood asthma (yes/no) is
associated with family history and early diet and infections can
be investigated using a logistic regression model.
not the whole story
papers that only cite p-values as the results of statistical analyses,
although commonplace, are flawed. it is not possible to ascertain
the clinical impact of a finding using the p-value alone. the
p-value merely shows how compatible the data obtained is with
the hypothesis of no difference (average difference or arr ¼ 0,
rr or or ¼ 1), but not how likely other scenarios might be.
estimates of effect size should always be given and presented
with confidence intervals. a 95% confidence interval gives the
range of population scenarios that the sample data is compatible
with 95% confidence.
for example:
neither respiratorymorbidity norrds were significantly different
in the trial of corticosteroid versus placebo (rr 1.09, p¼0.69 and
1.82, p ¼ 0.54 respectively). however, confidence intervals for
therrreveal the range ofvalueswhich the trial is compatible with
and paint a different picture. the 95%confidence intervals for the
rr are (0.72, 1.66) formorbidity and (0.17, 19.8) for rds.we are
farmore confident in excluding large differences inmorbidity but
for rds the interval is very wide (due to the small number of
events) and almost 20-fold differences cannot be excluded even
though the difference is statistically non-significant.
filling the gaps
rarely is a study as simple as the textbook examples often given
where a single outcome is compared between two or more groups
of well-defined individuals. more commonly, there is a whole host
of information collected from each participant that will be
combined to give an overall picture. some pieces of information
may correlate with others to give a more comprehensive overview
of the issues. although this is a good thing to do and may lend
validity to the ‘main’ question, the collection of large amounts of
information carries with it pitfalls. most notably, there is greater
capacity for some individuals to have incomplete data. unfortunately
most statistical analyses require each individual to supply
values for all variables included and so it is necessary to either fill
the gaps or exclude those individuals with missing data.
figure 3 gives a guide to useful and not so useful techniques that
can be used. replacing the missing values by some substitute with
no allowance for potential error in doing so is always wrong (left
hand branch) since this will lead to overly precise results and
possibly also bias. if the data are missing completely at random,
then removing any individuals with any missing data (listwise
deletion) will not introduce bias but precision may be less than it
needs to be. ‘missing at random’ means that the missing data could
be inferred from that available. for example, if we have the age and
gender of a child then we would be able to make a reasonable guess
at their height. for data missing at random, or missing completely
at random, the preferred option is multiple imputation and this will
give unbiased results with the best possible precision. multiple
imputation techniques are now available in most large packages
and details can be found in the references at the end of this article.
particular issues for paediatricians
mostly problems are the same as for any discipline. when
dealing with frontline clinical medicine, as opposed to the laboratory
scientist, there are the additional issues to face around
recruitment of people who may choose not to be part of the study
or may agree and then not provide all required data. they may
not provide valid information (for example, they may be inaccurate
in reporting gestation and self-reported smoking histories
have long been known to be open to bias) or adhere to preferred
time-scales (for example, a scheduled annual follow up may need
to be undertaken much earlier or later due to illness or holidays).
when dealing with children there are the added problems of
whether ethics require assent from the child and/or consent
from the parent/guardians, plus the aging process may mean
that they are not a homogenous group. the latter is evidenced
in the need for age (or growth) related standards to interpret
individual data. for example, it is usual to interpret childhood
lung function measurements with reference to the height of the
child. hence adjustment is often necessary for comparing
groups of children unless those groups are only within a relatively
small age range (in which case the results would not be
generalisable outside of the range). this infers a need for more
complex analyses.
making sense of it all!
in this short article we have given an overviewof some issues facing
the paediatrician either trying to interpret published research or
undertaking their own study. it cannot be comprehensive but aims
to provide a starting point. further recommended reading is given
below.
to summarize, we hope that:
p aediatricians have become more
i nformed after reading this article
c ompared to those who have not read it, with the longer term
o utcome of better understanding of published research.