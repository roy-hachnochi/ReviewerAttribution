summary points
patients and healthcare professionals require clinical prediction models
to accurately guide healthcare decisions
larger sample sizes lead to the development of more robust models
data should be of sufficient quality and representative of the target
population and settings of application
it is better to use all available data for model development (ie, avoid data
splitting), with resampling methods (such as bootstrapping) used for
internal validation
when developing prediction models for binary or time-to-event outcomes,
a well known rule of thumb for the required sample size is to ensure at
least 10 events for each predictor parameter
the actual required sample size is, however, context specific and depends
not only on the number of events relative to the number of candidate
predictor parameters but also on the total number of participants, the
outcome proportion (incidence) in the study population, and the expected
predictive performance of the model
we propose to use such information to tailor sample size requirements
to the specific setting of interest, with the aim of minimising the potential
for model overfitting while targeting precise estimates of key parameters
our proposal can be implemented in a four step procedure and is
applicable for continuous, binary, or time-to-event outcomes
the pmsampsize package in stata or r allows researchers to implement
the procedure
clinical prediction models are needed to inform diagnosis and
prognosis in healthcare.1-3 well known examples include the
wells score,4 5 qrisk,6 7 and the nottingham prognostic index.8 9
such models allow health professionals to predict an individual’s
outcome value, or to predict an individual’s risk of an outcome
being present (diagnostic prediction model) or developed in the
future (prognostic prediction model). most prediction models
are developed using a regression model, such as linear regression
for continuous outcomes (eg, pain score), logistic regression
for binary outcomes (eg, presence or absence of pre-eclampsia),
or proportional hazards regression models for time-to-event
data (eg, recurrence of venous thromboembolism).10 an equation
is then produced that can be used to predict an individual’s
outcome value or outcome risk conditional on his or her values
of multiple predictors, which might include basic characteristics
such as age, weight, family history, and comorbidities; biological
measurements such as blood pressure and biomarkers; and
imaging or other test results. supplementary material s1 shows
examples of regression equations.
developing a prediction model requires a development dataset,
which contains data from a sample of individuals from the target
population, containing their observed predictor values (available
at the intended moment of prediction11) and observed outcome.
the sample size of the development dataset must be large
enough to develop a prediction model equation that is reliable
when applied to new individuals in the target population. what
constitutes an adequately large sample size for model
development is, however, unclear,12 with various blanket “rules
of thumb” proposed and debated.13-17 this has created confusion
about how to perform sample size calculations for studies aiming
to develop a prediction model.
in this article we provide practical guidance for calculating the
sample size required for the development of clinical prediction
models, which builds on our recent methodology papers.13-16 18
we suggest that current minimum sample size rules of thumb
are too simplistic and outline a more scientific approach that
tailors sample size requirements to the specific setting of interest.
we illustrate our proposal for continuous, binary, and
time-to-event outcomes and conclude with some extensions.
moving beyond the 10 events per variable
rule of thumb
in a development dataset, the effective sample size for a
continuous outcome is determined by the total number of study
participants. for binary outcomes, the effective sample size is
often considered about equal to the minimum of the number of
events (those with the outcome) and non-events (those without
the outcome); time-to-event outcomes are often considered
roughly equal to the total number of events.10 when developing
prediction models for binary or time-to-event outcomes, an
established rule of thumb for the required sample size is to
ensure at least 10 events for each predictor parameter (ie, each
β term in the regression equation) being considered for inclusion
in the prediction model equation.19-21 this is widely referred to
as needing at least 10 events per variable (10 epv). the word
“variable” is, however, misleading as some predictors actually
require multiple β terms in the model equation—for example,
two β terms are needed for a categorical predictor with three
categories (eg, tumour grades i, ii, and iii), and two or more β
terms are needed to model any non-linear effects of a continuous
predictor, such as age or blood pressure. the inclusion of
interactions between two or more predictors also increases the
number of model parameters. hence, as prediction models
usually have more parameters than actual predictors, it is
preferable to refer to events per candidate predictor parameter
(epp). the word candidate is important, as the amount of model
overfitting is dictated by the total number of predictor
parameters considered, not just those included in the final model
equation.
the rule of at least 10 epp has been widely advocated perhaps
as a result of its simplicity, and it is regularly used to justify
sample sizes within published articles, grant applications, and
protocols for new model development studies, including by
ourselves previously. the most prominent work advocating the
rule came from simulation studies conducted in the 1990s,19-21
although this work actually focused more on the bias and
precision of predictor effect estimates than on the accuracy of
risk predictions from a developed model. the adequacy of the
10 epp rule has often been debated. although the rule provides
a useful starting point, counter suggestions include either
lowering the epp to below 10 or increasing it to 15, 20, or even
50.10 22-26 these inconsistent recommendations reflect that the
required epp is actually context specific and depends not only
on the number of events relative to the number of candidate
predictor parameters but also on the total number of participants,
the outcome proportion (incidence) in the study population, and
the expected predictive performance of the model.13-17 this
finding is unsurprising as sample size considerations for other
study designs, such as randomised trials of interventions, are
all context dependent and tailored to the setting and research
question. rules of thumb have also been advocated in the
continuous outcome setting, such as two participants per
predictor,27 but these share the same concerns as for 10 epp.16
sample size calculation to ensure precise
predictions and minimise overfitting
recent work by van smeden et al13 14 and riley et al15 16 describe
how to calculate the required sample size for prediction model
development, conditional on the user specifying the overall
outcome risk or mean outcome value in the target population,
the number of candidate predictor parameters, and the
anticipated model performance in terms of overall model fit
(r2
). these authors’ approaches can be implemented in a four
step procedure. each step leads to a sample size calculation,
and ultimately the largest sample size identified is the one
required. we describe these four steps, and, to aid general
readers, provide the more technical details of each step in the
figures.
step 1: what sample size will produce a
precise estimate of the overall outcome risk
or mean outcome value?
fundamentally, the sample size must allow the prediction
model’s intercept to be precisely estimated, to ensure that the
developed model can accurately predict the mean outcome value
(for continuous outcomes) or overall outcome proportion (for
binary or time-to-event outcomes). a simple way to do this is
to calculate the sample size needed to precisely estimate (within
a small margin of error) the intercept in a model when no
predictors are included (the null model).15figure 1 shows the
calculation for binary and time-to-event outcomes, and we
generally recommend aiming for a margin of error of ≤0.05 in
the overall outcome proportion estimate. for example, with a
binary outcome that occurs in half of individuals, a sample size
of at least 385 people is needed to target a confidence interval
of 0.45 to 0.55 for the overall outcome proportion, and thus an
error of at most 0.05 around the true value of 0.5. to achieve
the same margin of error with outcome proportions of 0.1 and
0.2, at least 139 and 246 participants, respectively, are required.
for time-to-event outcomes, a key time point needs to be
identified, along with the anticipated outcome event rate. for
example, with an anticipated event rate of 10 per 100 person
years of the entire follow-up, the sample size must include a
total of 2366 person years of follow-up to ensure an expected
margin of error of ≤0.05 in the estimate of a 10 year outcome
probability of 0.63, such that the expected confidence interval
is 0.58 to 0.68.
for continuous outcomes, the anticipated mean and variance of
outcome values must be prespecified, alongside the anticipated
percentage of variation explained by the prediction model (see
supplementary material s2 for details).16
step 2: what sample size will produce
predicted values that have a small mean error
across all individuals?
in addition to predicting the average outcome value precisely
(see step 1), the sample size for model development should also
aim for precise predictions across the spectrum of predicted
values. for binary outcomes, van smeden et al use simulation
across a wide range of scenarios to evaluate how the error of
predicted outcome probabilities from a developed model
depends on various characteristics of the development dataset
sampled from a target population.14 they found that the number
of candidate predictor parameters, total sample size, and
outcome proportion were the three main drivers of a model’s
mean predictive accuracy. this led to a sample size formula
(fig 2) to help ensure that new prediction models will, on
average, have a small prediction error in the estimated outcome
probabilities in the target population (as measured by the mean
absolute prediction error, mape). the calculation requires the
number of candidate predictor parameters and the anticipated
outcome proportion in the target population to be prespecified.
for example, with 10 candidate predictor parameters and an
outcome proportion of 0.3, a sample size of at least 461
participants and 13.8 epp is required to target a mean absolute
error of 0.05 between observed and true outcome probabilities
(see fig 2 for calculation). the calculation is available as an
interactive tool (https://mvansmeden.shinyapps.io/beyondepv/
) and applicable to situations with 30 or fewer candidate
predictors. ongoing work aims to extend to larger numbers of
candidate predictors and also to time-to-event outcomes.
for continuous outcomes, accurate predictions across the
spectrum of predicted values require the standard deviation of
the residuals to be precisely estimated.10 16 supplementary
material s3 shows that to target a less than 10% multiplicative
error in the estimated residual standard deviation, the required
sample size is simply 234+p, where p is the number of predictor
parameters considered.
step 3: what sample size will produce a small
required shrinkage of predictor effects?
our third recommended step is to identify the sample size
required to minimise the problem of overfitting.28 overfitting
is when a developed model’s predictions are more extreme than
they ought to be for individuals in a new dataset from the same
target population. for example, an overfitted prediction model
for a binary outcome will give a predicted outcome probability
too close to 1 for individuals with a higher than the average
outcome probability and too close to 0 for individuals with a
lower than the average outcome probability. overfitting notably
occurs when the sample size is too small. in particular, when
the number of candidate predictor parameters is large relative
to the number of participants in total (for continuous outcomes)
or to the number of participants with the outcome event (for
binary or time-to-event outcomes). a consequence of overfitting
is that a developed model’s apparent predictive performance (as
observed in the development dataset itself) will be optimistic
(ie, too high), and its actual predictive performance in new data
from the same target population will be lower (ie, worse).
shrinkage (also known as penalisation or regularisation)
methods deal with the problem of overfitting by reducing the
variability in the developed model’s predictions such that
extreme predictions (eg, predicted probabilities close to 0 or 1)
are pulled back toward the overall average.29-34 however, there
is no guarantee that shrinkage will fully overcome the problem
of overfitting when developing a prediction model. this is
because the shrinkage or penalty factors (which dictate the
magnitude of shrinkage required) are also estimated from the
development dataset and, especially when the sample size is
small, are often imprecise and so fail to tackle the magnitude
of overfitting correctly in a particular application.30 furthermore,
a negative correlation tends to occur between the estimated
shrinkage required and the apparent performance of a model.
if the apparent model performance is excellent simply by chance,
the required shrinkage is typically estimated too low.30 thus,
ironically, in those situations when overfitting is of most concern
(and thus shrinkage is most urgently needed), the prediction
model developer has insufficient assurance in selecting the
proper amount of shrinkage to cancel the impact of overfitting.
riley et al therefore suggest identifying the sample size and
number of candidate predictors that correspond to a small
amount of desired shrinkage (≤10%) during model
development.15 16 the sample size calculation (fig 3) requires
the researcher to prespecify the number of candidate predictor
parameters and, for binary or time-to-event outcomes, the
anticipated outcome proportion or rate, respectively, in the target
population. in addition, a (conservative) value for the anticipated
model performance is required, as defined by the cox-snell r
squared statistic (r2
cs
).15 35 the anticipated value of r2
cs
 is
important because it reflects the signal:noise ratio, which has
an impact on the estimation of multiple parameters and the
potential for overfitting. when the signal:noise ratio is
anticipated to be high (eg, r2
cs
 is close to 1 for a prediction
model with a continuous outcome), true patterns are easier to
detect and so overfitting is less of a concern, such that more
predictor parameters can be estimated. however, when the
signal:noise ratio is low (ie, r2
cs
 is anticipated to be close to 0),
true patterns are harder to identify and there is more potential
for overfitting, such that fewer predictor parameters can be
estimated reliably.
in the continuous outcome setting, r2
cs
 is simply the coefficient
of determination r2
, which quantifies the proportion of the
variance of outcome values that is explained by the prediction
model and thus is between 0 and 1. for example, when
developing a prediction model for a continuous outcome with
up to 30 predictor parameters and an anticipated r2
cs
 of 0.7, a
sample size of 206 participants is required to ensure the expected
shrinkage is 10% (see supplementary material s4 for full
calculation). this corresponds to about seven participants for
each predictor parameter considered.
the r2
cs
 statistic generalises to non-continuous outcomes and
allows sample size calculations to minimise the expected
shrinkage when developing a prediction model for binary and
time-to-event outcomes (fig 3). for example, when developing
a new logistic regression model with up to 20 candidate predictor
parameters and an anticipated r2
cs
 of at least 0.1, a sample size
of 1698 participants is required to ensure the expected shrinkage
is 10% (see fig 3 for full calculation). if the target setting has
an outcome proportion of 0.3, this corresponds to an epp of
25.5. the required sample size and epp are sensitive to the
choice of r2
cs
, with lower anticipated values of r2
cs
 leading to
higher required sample sizes. therefore, a conservative choice
of r2
cs
 is recommended (fig 4).
as in sample size calculations for randomised trials evaluating
intervention effects, external evidence and expert opinion are
required to inform the values that need specifying in the sample
size calculator. figure 4 provides guidance for specifying r2
cs
.
importantly, unlike for continuous outcomes when r2
cs
 is
bounded between 0 and 1, the r2
cs
 is bounded between 0 and
max(r2
cs
) for binary and time-to-event outcomes. the max(r2
cs
)
denotes the maximum possible value of r2
cs
, which is dictated
by the overall outcome proportion or rate in the development
dataset and is often much less than 1. supplementary material
s5 shows the calculation of max(r2
cs
). for logistic regression
models with outcome proportions of 0.5, 0.4, 0.3, 0.2, 0.1, 0.05,
and 0.01, the corresponding max(r2
cs
) values are 0.75, 0.74,
0.71, 0.63, 0.48, 0.33, and 0.11, respectively. thus the
anticipated r2
cs
 might be small, even for a model with potentially
good performance.
step 4: what sample size will produce a small
optimism in apparent model fit?
the sample size should also ensure a small difference in the
developed models apparent and optimism adjusted values of
r
2
nagelkerke
 (ie, r2
cs
/max(r2
cs
)), as this is a fundamental overall
measure of model fit.10 38 the apparent r2
nagelkerke
 value is simply
the model’s observed performance in the same data as used to
develop the model, whereas the optimism adjusted r2
nagelkerke
value is a more realistic (approximately unbiased) estimate of
the model’s fit in the target population. the sample size
calculations are shown in supplementary material s6 for
continuous outcomes and in figure 5 for binary and time-to-event
outcomes. as before, they require the user to specify the
anticipated r2
cs
 and the max(r2
cs
), as described in figure 4. for
example, when developing a logistic regression model with an
anticipated r2
cs
 of 0.2, and in a setting with an outcome
proportion of 0.05 (such that the max(r2
cs
) is 0.33), 1079
participants are required to ensure the expected optimism in the
apparent r2
nagelkerke
 is just 0.05 (see figure 5 for calculation).
recommendations and software
box 1 summarises our recommended steps for calculating the
minimum sample size required for prediction model
development. this involves four calculations for binary
outcomes (b1 to b4), three for time-to-event outcomes (t1 to
t3), and four for continuous outcomes (c1 to c4). to implement
the calculations, we have written the pmsampsize package for
stata and r. the software calculates the sample size needed to
meet all the criteria listed in box 1 (except b2, which is available
at https://mvansmeden.shinyapps.io/beyondepv/), conditional
on the user inputting values of required parameters such as the
number of candidate predictors, the anticipated outcome
proportion in the target population, and the anticipated r2
cs
. the
calculations are especially helpful when prospective data
collection (eg, new cohort study) are required before model
development; however, they are also relevant when existing
data are available to guide the number of predictors that can be
considered.
box 1 recommendations for calculating the sample size
needed when developing a clinical prediction model for
continuous, binary, and time-to-event outcomes
to increase the potential for developing a robust prediction model,
the sample size should be at least large enough to minimise
model overfitting and to target sufficiently precise model
predictions
binary outcomes
for binary outcomes, ensure the sample size is enough to:
estimate the overall outcome proportion with sufficient precision
(use equation in figure 1) (b1)
target a small mean absolute prediction error (use equation in
figure 2, if number of predictor parameters is ≤30) (b2)
target a shrinkage factor of 0.9 (use equation in figure 3) (b3)
target small optimism of 0.05 in the apparent r2
nagelkerke (use
equation in figure 5) (b4)
time-to-event outcomes
for time-to-event outcomes, ensure the sample size is enough
to:
estimate the overall outcome proportion with sufficient precision
at one or more key time-points in follow-up (use equation in figure
1) (t1)
target a shrinkage factor of 0.9 (use equation in figure 3) (t2)
target small optimism of 0.05 in the apparent r2
nagelkerke (use
equation in figure 5) (t3)
continuous outcomes
for continuous outcomes, ensure the sample size is enough to:
estimate the model intercept precisely (see supplementary
material 1) (c1)
estimate the model residual variance with sufficient precision
(see supplementary material 2) (c2)
target a shrinkage factor of 0.9 (use equation in figure 3) (c3)
target small optimism of 0.05 in the apparent r2
nagelkerke (use
equation in figure 5) (c4)
these approaches require researchers to specify the anticipated
overall outcome risk or mean outcome value in the target
population, the number of candidate predictor parameters, and
the anticipated model performance in terms of overall model fit
(r2
cs
). when the choice of values is uncertain, we generally
recommend being conservative and so taking those values (eg,
smallest r2
cs
) that give larger sample sizes
when an existing dataset is already available (such that sample
size is already defined), the calculations can be used to identify
if the sample size is sufficient to estimate the overall outcome
risk or the mean outcome value, and how many predictor
parameters can be considered before overfitting becomes a
concern
applied examples
we now illustrate the recommendations in box 1 by using three
examples.
example 1: binary outcome
north et al developed a model predicting pre-eclampsia in
pregnant women based on clinical predictors measured at 15
weeks’ gestation,43 including vaginal bleeding, age, previous
miscarriage, family history, smoking, and alcohol consumption.
the model included 13 predictor parameters and had a c statistic
of 0.71. emerging research aims to improve this and other
pre-eclampsia prediction models by including additional
predictors (eg, biomarkers and ultrasound measurements).
as the outcome is binary, the sample size calculation for a new
prediction model needs to examine criteria b1 to b4 in box 1.
this requires us to input the overall proportion of women who
will develop pre-eclampsia (0.05) and the number of candidate
predictor parameters (assumed to be 30 for illustration). for an
outcome proportion of 0.05, the max(r2
cs
) value is 0.33 (see

supplementary material s5). if we assume, conservatively, that
the new model will explain 15% of the variability, the
anticipated r2
cs
 value is 0.15×0.33=0.05. now we can check
criteria b1, b3, and b4 by typing in stata:
pmsampsize, type(b) rsquared(0.05) parameters(30)
prevalence(0.05)
this indicates that at least 5249 women are required,
corresponding to 263 events and an epp of 8.75. this is driven
by criterion b3, to ensure the expected shrinkage required is
just 10% (to minimise the potential overfitting). to check
criterion b2 in box 1, we can apply the formula in figure 2. this
suggests that 544 women are needed to target a mean absolute
error in predicted probabilities of ≤0.05. this is much lower
than the 5249 women needed to meet criterion b3.
if recruiting 5249 women is impractical (eg, because of time,
cost, or practical constraints for data collection), the sample size
required can be reduced by identifying a smaller number of
candidate predictors (eg, based on existing evidence from
systematic reviews44). for example, with 20 rather than 30
candidate predictors, the required sample size to meet all four
criteria is at least 3500 women and 175 events (still 8.75 epp).
example 2: time-to-event outcome
many prognostic models are available for the risk of a recurrent
venous thromboembolism (vte) after cessation of treatment
for a first vte.45 for example, the model of ensor et al included
predictors of age, sex, site of first clot, d-dimer level, and the
lag time from cessation of treatment until measurement of
d-dimer (often around 30 days).46 the model’s c statistic was
0.69 and the adjusted r2
cs
 was 0.051 (corresponding to 8% of
the total variation). emerging research aims to extend such
models by including additional predictors.
the sample size required for a new model must at least meet
criteria t1 to t3.15 this requires us to input a key time point for
prediction of vte recurrence risk (eg, two years), alongside
the number of candidate predictor parameters (n=30), the
anticipated mean follow-up (2.07 years), and outcome event
rate (0.065, or 65 vte recurrences for every 1000 person years
of follow-up), and the conservative value of r2
cs
 (0.051), with
all chosen values based on ensor et al.46 now criteria t1 to t3
can be checked, for example by typing in stata:
pmsampsize, type(s) rsquared(0.051) parameters(30) rate(0.065)
timepoint(2) meanfup(2.07)
this indicates that at least 5143 participants are required,
corresponding to 692 events and an epp of 23.1. this is
considerably more than 10 epp, and is driven by a desired
shrinkage factor (criterion t2) of only 10% to minimise
overfitting based on just 8% of variation explained by the model.
if the number of candidate predictor parameters is lowered to
20, the required sample size is reduced to 3429 (still an epp of
23.1).
example 3: continuous outcome
hudda et al developed a prediction model for fat free mass in
children and adolescents aged 4 to 15 years, including 10
predictor parameters based on height, weight, age, sex, and
ethnicity.47 the model is needed to provide an estimate of an
individual’s current fat mass (=weight minus predicted fat free
mass). on external validation, the model had an r2
cs
 of 0.90.
let us assume that the model will need updating (eg, in 10 years
owing to changes in the population behaviour and environment),
and that an additional 10 predictor parameters (and thus a total
of 20 parameters) will need to be considered in the model
development.
the sample size for a model development dataset must at least
meet the four criteria of c1 to c4 in box 1. this requires us to
specify the anticipated r2
cs
 (0.90), number of candidate predictor
parameters (n=20), and mean (26.7 kg) and standard deviation
(8.7 kg) of fat free mass in the target population (taken from
hudda et al47). for example, in stata, after installation of
pmsampsize (type: ssc install pmsampsize), we can type:
pmsampsize, type(c) rsquared(0.9) parameters(20)
intercept(26.7) sd(8.7)
this returns that at least 254 participants are required, and so
12.7 participants for each predictor parameter. the sample size
of 254 is driven by the number needed to precisely estimate the
model standard deviation (criterion c3), as only 68 participants
are needed to minimise overfitting (criteria c1 and c2).
extensions and further topics
ensuring accurate predictions in key
subgroups
alongside the criteria outlined in box 1, a more stringent task
is to ensure model predictions are accurate in key subgroups
defined by particular values or categories of included
predictors.48 one way to tackle this is to ensure predictor effects
in the model equation are precisely estimated, at least for key
subgroups of interest.15 16 for binary and time-to-event outcomes,
the precision of a predictor’s effect depends on its magnitude,
the variance of the predictor’s values, the predictor’s correlation
with other predictors in the model, the sample size, and the
outcome proportion or rate in the study.49-51 for continuous
outcomes, it depends on the sample size, the residual variance,
the correlation of the predictor with other included predictors,
and the variance of the predictor’s values.48 52-55 note that for
important categorical predictors large sample sizes might be
needed to avoid separation issues (ie, where no events or
non-events occur in some categories),13 and potential bias from
sparse events.56
sample size considerations when using an
existing dataset
our proposed sample size calculations (ie, based on the criteria
in box 1) are still useful in situations when an existing dataset
is already available, with a specific number of participants and
predictors. firstly, the calculations might identify that the dataset
is too small (for example, if the overall outcome risk cannot be
estimated precisely) and so the collection of further data is
required.57 58 secondly, the calculations might help identify how
many predictors can be considered before overfitting becomes
a concern. the shrinkage estimate obtained from fitting the full
model (including all predictors) can be used to gauge whether
the number of predictors could be reduced through data
reduction techniques such as principal components analysis.10
this process should be done blind to the estimated predictor
effects in the full model, as otherwise decisions about predictor
inclusion will be influenced by a “quick look” at the results
(which increases the overfitting).
sample size requirements when using
variable selection
further research on sample size requirements with variable
selection is required, especially for the use of more modern
penalisation methods such as the lasso (least absolute shrinkage
and selection operator) or elastic net.33 59 such methods allow
shrinkage and variable selection to operate simultaneously, and
they even allow the consideration of more predictor parameters
than number of participants or outcome events (ie, in high
dimensional settings). however, there is no guarantee such
models solve the problem of overfitting in the dataset at hand.
as mentioned, they require penalty and shrinkage factors to be
estimated using the development dataset, and such estimates
will often be hugely imprecise. also, the subset of included
predictors might be highly unstable60-63; that is, if the prediction
model development was repeated on a different sample of the
same size, a different subset of predictors might be selected and
important predictors missed (especially if sample size is small).
in healthcare the final set of predictors is a crucial consideration,
owing to their cost, time, burden (eg, blood test, invasiveness),
and measurement requirements.
larger sample sizes might be needed when
using machine learning approaches to
develop risk prediction models
an alternative to regression based prediction models are those
based on machine learning methods, such as random forests and
neural networks (of which “deep learning” methods are a special
case).64 when the focus is on individualised outcome risk
prediction, it has been shown that extremely large datasets might
be needed for machine learning techniques. for binary outcomes,
machine learning techniques could need more than 10 times as
many events for each predictor to achieve a small amount of
overfitting compared with classic modelling techniques such
as logistic regression, and might show instability and a high
optimism even with more than 200 epp.26 a major cause of this
problem is that the number of predictor (“feature”) parameters
considered by machine learning approaches will usually far
exceed that for regression, even when the same set of predictors
is considered, particularly because they routinely examine
multiple interaction terms and categorise continuous predictors.
therefore, machine learning methods are not immune to sample
size requirements, and actually might need truly “big data” to
ensure their developed models have small overfitting, and for
their potential advantages (eg, dealing with highly non-linear
relations and complex interactions) to reach fruition. the size
of most medical research datasets is better suited to using
regression (including penalisation and shrinkage approaches),65
especially as regression also leads to a transparent model
equation that facilitates implementation, validation, and
graphical displays.
sample size for model updating
when an existing prediction model is updated, the existing
model equation is revised using a new dataset. the required
sample size for this dataset depends on how the model is to be
updated and whether additional predictors are to be included.
in our worked examples, we assumed that all parameters in the
existing model will be re-estimated using the model updating
dataset. in that situation, the researcher can still follow the
guidance in box 1 for calculating the required sample size, with
the total predictor parameters the same as in the original model
plus those new parameters required for any additional predictors.
sometimes, however, only a subset of the existing model’s
parameters is to be updated.66 67 in particular, to deal with
calibration-in-the-large, researchers might only want to revise
the model intercept (or baseline survival), while constraining
the other parameter estimates to be the same as those in the
existing model. in this case the required sample size only needs
to be large enough to estimate the mean outcome value or
outcome risk precisely (ie, to meet criteria c1, b1, or t1 in box
1). even if researchers also want to update the existing predictor
effects, they might decide to constrain their updated values to
be equal to the original values multiplied by a constant. then,
the sample size only needs to be large enough to estimate one
predictor parameter (ie, the constant) for the existing predictors,
plus any new parameters the researchers decide to add. such
model updating techniques therefore reduce the sample size
needed (to meet the criteria in box 1) compared with when every
predictor parameter is re-estimated without constraint.
conclusion
patients and healthcare professionals require clinical prediction
models to accurately guide healthcare decisions.1
 larger sample
sizes lead to more robust models being developed, and our
guidance in box 1 outlines how to calculate the minimum sample
size required. clearly, the more data for model development
the better; so if larger sample sizes are achievable than our
guidance suggests, use it! of course, any data collected should
be of sufficient quality and representative of the target
population and settings of application.68 69
after data collection, careful model building is required using
appropriate methods.1 3 10 in particular, we do not recommend
data splitting (eg, into model training and testing samples), as
this is inefficient and it is better to use all the data for model
development, with resampling methods (such as bootstrapping)
used for internal validation.70 71 sometimes external information
might be used to supplement the development dataset further.72-74
lastly, sample size requirements when externally validating an
existing prediction model require a different approach, as
discussed elsewhere.75-78


<|EndOfText|>

quantifying the impact of different
approaches for handling continuous
predictors on the performance of a
prognostic model
continuous predictors are routinely encountered when developing a prognostic model. investigators, who are
often non-statisticians, must decide how to handle continuous predictors in their models. categorising continuous measurements into two or more categories has been widely discredited, yet is still frequently done because of
its simplicity, investigator ignorance of the potential impact and of suitable alternatives, or to facilitate model
uptake. we examine three broad approaches for handling continuous predictors on the performance of a prognostic model, including various methods of categorising predictors, modelling a linear relationship between the
predictor and outcome and modelling a nonlinear relationship using fractional polynomials or restricted cubic
splines. we compare the performance (measured by the c-index, calibration and net benefit) of prognostic
models built using each approach, evaluating them using separate data from that used to build them. we show
that categorising continuous predictors produces models with poor predictive performance and poor clinical
usefulness. categorising continuous predictors is unnecessary, biologically implausible and inefficient and
should not be used in prognostic model development.
keywords: prognostic modelling; continuous predictors; dichotomisation
1. introduction
categorising continuous measurements in regression models has long been regarded as problematic
(e.g., biologically implausible particularly when dichotomising), highly inefficient and unnecessary
[1–5]. in the context of clinical prediction, investigators developing new prognostic models frequently
categorise continuous predictors into two or more categories [6]. categorisation is often carried out with
no apparent awareness of its consequences or in a misguided attempt to facilitate model interpretation,
use and uptake. however, categorisation causes a loss of information, and therefore reduces the statistical power to identify a relationship between a continuous measurement and patient outcome [7]. as
studies developing new prognostic models are already generally quite small, it seems unwise to discard
any information [6,8]. despite the numerous cautions and recommendations not to categorise continuous
measurements, there have been relatively few quantitative assessments of the impact of the choice of
approach for handling continuous measurements on the performance of a prognostic model [1,9].
4124
research article
determining the functional form of a variable is an important, yet often overlooked step in modelling
[10]. it is often insufficient to assume linearity, and categorising continuous variables should be avoided.
alternative approaches that allow more flexibility in the functional form of the association between
predictors and outcome should be considered, particularly if they improve model fit and model predictions
[11–13]. two commonly used approaches are fractional polynomials and restricted cubic splines [14,15].
however, few studies have examined how the choice of approach for handling continuous variables affects
the performance of a prognostic model. two single case studies demonstrated that dichotomising continuous predictors resulted in a loss in information and a decrease in the predictive ability of a prediction
model [1,16]. however, neither study was exhaustive, and both studies only examined the impact on
model performance using the same data that the models were derived from. in a more recent study, nieboer
and colleagues compared the effect of using log transformations, fractional polynomials and restricted cubic splines against retaining continuous predictors as linear on the performance of logistic-based prediction
models, but they did not also examine models that categorised one or more continuous predictors [9].
when developing a new prognostic model, investigators can broadly choose to (i) dichotomise, or
more generally categorise, a continuous predictor using one or more cut-points; (ii) leave the predictor
continuous but assume a linear relationship with the outcome; or (iii) leave the predictor continuous but
allow a nonlinear association with the outcome, such as by using fractional polynomials or restricted
cubic splines. how continuous measurements are included will affect the generalisability and transportability of the model [17]. a key test of a prognostic model is to evaluate its performance on an entirely
separate data set [18]. it is therefore important that the associations are appropriately modelled, to
improve the likelihood that the model will predict sufficiently well using data with different case-mix.
the aim of this article is to quantitatively illustrate the impact of the choice of approach for handling
continuous predictors on the apparent performance (based on the development data set) and validation
performance (in a separate data set) of a prognostic model.
2. methods
2.1. study data: the health improvement network
the health improvement network (thin) is a large database of anonymised electronic primary care
records collected at general practice surgeries around the united kingdom (england, scotland, wales
and northern ireland). the thin database contains medical records on approximately 4% of the united
kingdom population. clinical information from over 2 million individuals (from 364 general practices)
registered between june 1994 and june 2008 form the data set. the data have previously been used in
the external validation of a number of risk prediction models, including those considered in this study
[19–26]. there are some missing data for the predictors of systolic blood pressure, body mass index
and cholesterol. for simplicity and convenience, we have used an imputed data set used in published
external validation studies (details on the imputation strategy are reported elsewhere [20,27]).
2.2. prognostic models
we used cox regression to develop prognostic models that predict the 10-year risk of cardiovascular disease and 10-year risk of hip fracture. we split the thin database geographically by pulling out two
cohorts: general practices from england and general practices from scotland. data from the england
cohort were used to develop the models, whilst data from scotland were used to validate the model. using
data for 1,803,778 patients (men and women) from england (with 80,880 outcome events) to develop cardiovascular prognostic models. model performance was evaluated on 110,934 patients from scotland
(with 4688 outcome events). similarly, 980,465 women (with 7721 outcome events) in the thin database from england were used to develop hip fracture models. the model performance was evaluated
on data from 61,563 women from scotland (with 565 outcome events). all data come from the same underlying computer system used to collect the patient level data, and thus splitting this large database geographically is a weak form of external validation (sometimes referred to as narrow validation), although
given the health systems in the two countries are ultimately the same the case-mix is not dissimilar. in this
instance, the validation is closer to an assessment of reproducibility than transportability [18].
for convenience and simplicity, the models were developed using a subset of variables that were
chosen from the total list of predictors contained in the qrisk2 [28] and qfracture [29] risk prediction
models. the cardiovascular disease models used age (continuous), sex (binary), family history of cardiovascular disease (binary), serum cholesterol (continuous), systolic blood pressure (continuous), body
4125
mass index (continuous) and treated hypertension (binary). the hip fracture models used age (continuous), body mass index (continuous), townsend score (categorical), diagnosis of asthma (binary) and
prescription of tricyclic antidepressants (binary). table i shows the distribution of the model variables
in the thin data set for both outcomes.
2.3. resampling strategy
a resampling study was performed to examine the impact of different strategies for handling continuous
predictors in the development of prognostic models on the performance of the model when evaluated in
a separate data set.
two hundred samples were randomly drawn (with replacement) from the thin data set so that the
number of events in each sample was fixed at 25, 50, 100 and 2000. individuals in each sample were chosen by sampling a constant fraction of those who experienced the event and those who did not, according
to the overall proportion of events in the thin data set, for the corresponding prognostic model outcome
(cardiovascular disease or hip fracture). models were developed for each sample as described in sections
2.2 and 2.4. model performance (section 2.5) was evaluated on the same data used to derive the model, to
give the apparent performance, and on separate data, for validation performance (section 2.1).
2.4. approaches for handling continuous predictors
we considered three broad approaches for handling continuous predictors:
1. categorise each continuous predictor into equally sized groups, using the median value of the predictor to form two groups, the tertile values to form three groups, the quartile values to form four groups
or the quintile values to form five groups. we further categorised the age predictor by grouping individuals into 5-year or 10-year age intervals. we also categorised the continuous predictors based on
‘optimal’ cut-points, using a cut-point that minimised the p-value from the logrank test for over 80%
of the observations (the lower and upper 10% of observations removed) [30].
2. model each continuous predictor by assuming that it has a linear relationship with the outcome.
3. model each continuous predictor by assuming that it has a non-linear relationship with the outcome,
using fractional polynomials [14] and restricted cubic splines [15]. fractional polynomials are a set of
flexible power transformations that describe the relationship between a continuous predictor and the
outcome. fractional polynomials of degree one (fp1) and two (fp2) are defined as
fp1ð þ¼ x β1xp
;
fp2ð þ¼ x β1xp1 þ β2xp2 ; p1≠p2
β1xp1 þ β2xp2 lnx; p1 ¼ p2 ¼ p

where the powers p, p1, p2 ∈ s = {2, 1,0.5, 0, 0.5, 1, 2, 3} and x0= ln x. we used the multivariable
fractional polynomial (mfp) procedure in r to model potentially nonlinear effects while performing
table i. characteristics of the individuals in the thin data set, used as predictors in the developed prognostic models; sd: standard deviation.
variable cardiovascular disease hip fracture
development
(england)
(n = 1,803,778)
validation
(scotland)
(n = 110,934)
development
(england)
(n = 980,465)
validation
(scotland)
(n = 61,563)
mean age in years (sd) 48.6 (14.1) 48.9 (14.0) 50.8 (15.3) 50.1 (14.9)
men 922,913 (51.2%) 62,321 (56.2%) — —
family history of cardiovascular disease 74,668 (4.1%) 4,268 (3.8%) — —
mean serum cholesterol (sd) 5.5 (1.2) 5.5 (1.2) — —
mean systolic blood pressure (sd) 131.8 (20.3) 131.6 (20.2) — —
mean body mass index (sd) 26.3 (4.4) 26.3 (4.5) 26.1 (4.9) 26.5 (5.1)
treated for hypertension 96,634 (5.4%) 6,223 (5.6%) — —
diagnosis of asthma — — 84,279 (8.6%) 5,207 (8.5%)
history of falls — — 26,124 (2.7%) 1,016 (1.7%)
prescription of tricyclic antidepressants — — 51,849 (5.3%) 3,528 (5.7%)
4126
variable selection [31]. mfp formally tests for deviations from linearity using fractional polynomials. it
incorporates a closed test procedure that preserves the ‘familywise’ nominal significance level. the
default fp2 with significance level of 0.05 was chosen to build the prognostic models. the restricted cubic splines method places knots along the predictor value range and between two adjacent knots, where
the association between the predictor and the outcome is modelled using a cubic polynomial. beyond the
outer two knots, the relationship between the predictor and outcome is modelled as a linear association.
three to five knots are often sufficient to model the complex associations that are usually based on the
percentiles of the predictor values. we used the rcs function in the rms package in r [32]. prior to
conducting the simulations, preliminary analyses suggested that four degrees of freedom should be used
to model each continuous predictor with fractional polynomials (i.e., fp2 models) and that three knots
should be used in the restricted cubic splines approach. the models with the highest degrees of freedom
(df) were those categorising the continuous predictors into fifths (cvd: df = 19, hip fracture: df = 11),
model using fractional polynomials also had a maximum potential degrees of freedom also of 19 and
11 for the cvd and hip fracture models respectively. the fewest degrees of freedom were used by
the models that assumed a linear relationship with the outcome, those dichotomising (cvd: df = 7;
hip fracture: df = 5), see supplementary table 1.
figure 1 shows how the relationship between the continuous predictors (age, systolic blood pressure, body mass index and total serum cholesterol) and cardiovascular disease when estimated,
assuming linearity, nonlinearity (fractional polynomials and restricted cubic splines) and using
categorisation (supplementary figure 1 shows the corresponding relationship for continuous age
and body mass index with hip fracture). age is one of the main risk factors for many diseases,
including cardiovascular disease [33] and hip fractures [34]; we therefore examined models whereby
all continuous predictors apart from age were assumed linear, whilst age was included using the
approaches described above.
all of the continuous predictors in the development and validation data sets were centred and scaled
before modelling. scaling and centring reduce the chance of numerical underflow or overflow, which
can cause inaccuracies and other problems in model estimation [13]. the values used for transformation
were obtained from the development data set and were also applied to the validation data set.
2.5. performance measures
model performance was assessed in terms of discrimination, calibration, overall model performance and
clinical utility. discrimination is the ability of a prognostic model to differentiate between people with
different outcomes, such that those without the outcome have a lower predicted risk than those with
the outcome. the survival models in this study were based on the time-to-event. discrimination was thus
evaluated using harrell’s c-index, which is a generalisation of the area under the receiver operating characteristic curve for binary outcomes (e.g. logistic regression) [35,36].
we graphically examined the calibration of the models at a single time point, 10 years, using the val.
surv function in the rms library in r. for each random sample, hazard regression with linear splines was
used to relate the predicted probabilities from the models at 10 years to the observed event times (and
censoring indicators). the actual event probability at 10 years was estimated as a function of the estimate
event probability at 10 years. we investigated the influence of the approach used to handle continuous
predictors on calibration by overlaying plots of observed outcomes against predicted probabilities for
each of the 200 random samples.
we evaluated the clinical usefulness, or net benefit, of the models using decision curve analysis [37].
net benefit is the difference between the number of true-positive results and the number of false-positive
results, weighted by a factor that gives the cost of a false-positive relative to a false-negative result [38],
and is defined as:
net benefit ¼ true positives
n  false positives
n
pt
1  pt
 :
the numbers of true and false positives were estimated using the kaplan–meier estimates of the
percentage surviving at 10 years among those with calculated risks greater than the threshold probability.
n is the total number of people. pt is the threshold on the probability scale that defines high risk and is
used to weight false positives to false negative results. to calculate the net benefit for survival time data
4127
subject to censoring [39], we defined x = 1 if an individual had a predicted probability from the
model ≥ pt (the threshold probability), and x = 0 otherwise. s(t) is the kaplan–meier survival probability
at time t (t = 10 years) and n is the number of individuals in the data set. the number of true positives is
given by [1(s(t) | x = 1)] × p(x = 1) × n and the number of false positives by (s(t) | x = 1) × p(x = 1) × n. a
decision curve was produced by plotting across a range of pt values.
3. results
tables ii and iii show the mean (standard deviation) of the c-index over the 200 samples (each containing 25, 50, 100 and 2000 events) for the models produced by each approach that predict the hip fracture
and cardiovascular outcomes, respectively. for the hip fracture models, there was a large difference of
0.1 between the mean c-index produced by the approaches that did not categorise the continuous predictors and the approach that dichotomised the continuous predictors at the median. the same c-index difference was observed in the apparent performance (i.e. the development performance) and the
geographical validation performance. a similar pattern was observed for other performance measures
including the d-statistic [40], r2 [41] and brier score (see supplementary tables 2–7). the differences
in all the measures (c-index, d-statistic, r2 and brier score [42]) all favoured the approaches that kept
the variables continuous. the approaches that assumed a linear relationship between the predictor and
outcome and the approaches that used either fractional polynomials or restricted cubic splines had similar results for all of the performance measures. the variability (as reflected in the sd) in the four model
performance metrics was small relative to the mean value for all of the methods of handling continuous
predictors.
similar patterns in model performance were observed in the cardiovascular models. for example, a
difference of 0.055 in the c-index between the approaches that assumed a linear or nonlinear relationship
between the predictor and outcome and the approach that dichotomised at the median predictor value
was observed in both the apparent performance and geographical validation.
figure 2 shows the geographical validation calibration plots for the cardiovascular models. the
models using fractional polynomials or restricted cubic splines were better calibrated than the models
produced by other approaches, including the models that assumed a linear relationship between the
predictor and outcome. a similar pattern was observed in the hip fracture models, although there was
more similarity between the linear and nonlinear approaches (see supplemental figure 7).
figure 3 shows the distribution of the predicted 10-year cardiovascular risk for a subset of the
approaches. the approaches that kept the measurements continuous (linear, fractional polynomial and
restricted cubic splines) showed similar predicted risk spreads. the approaches that categorised the
continuous predictors showed a noticeably wide predicted risk spread in those who did not experience
the outcome, which was widest when the predictor was dichotomised, and showed a noticeable narrow
spread for those who did experience the outcome. as the number of categories increases the spread of
predicted risk in those who did not have an event decreases, whilst the spread of predicted risk in those
who did have an event increases.
models developed with smaller sample sizes (25, 50 and 100 events) showed higher performance
measure values when they were evaluated using the development data set than the models developed
figure 2. calibration plots of cardiovascular disease risk in the validation cohort (2000 events).
4131
with 2000 events. this pattern was observed for both the hip fracture and cardiovascular disease models,
regardless of the how continuous predictors were included in the models. in contrast, lower values were
observed in the geographical validation of models developed with smaller sample sizes than the models
developed with 2000 events, suggesting overfitting. similarly, models developed with fewer events
showed worse calibration on the geographical validation data than the models developed with 2000
events. the models developed by categorising continuous predictors showed the greatest variability in
performance.
table iv shows the clinical consequences of the different strategies for handling the continuous predictors in the cardiovascular models, using dichotomising at the median predictor value as the reference
method. similar findings were observed in the hip fracture models. over a range of probability thresholds (e.g. 0.09 to 0.2), an additional net 5 to 10 cardiovascular disease cases per 1000 were found during
geographical validation if models that implemented fractional polynomials or restricted cubic splines
were used, rather than models that dichotomised all of the continuous predictors at the median without
conducting any unnecessary treatment. for models that categorised continuous predictors, more additional cases per 1000 found as the number of categories increases. the models that used fractional
figure 3. boxplot of the predicted cardiovascular disease risks in the validation cohort (2000 events).
table iv. range of additional net cases per 1000 found when using each approach for handling continuous
predictors, compared with categorising all of the continuous predictors at the median. models developed using
2000 outcome events. range of thresholds 0.09 to 0.2 for cardiovascular disease and 0.01 to 0.05 for hip
fracture.
model cardiovascular disease hip fracture
development validation development validation
linear 6 to 11 5 to 10 4 to 6 5 to 7
fractional polynomials 7 to 11 6 to 10 4 to 6 5 to 7
restricted cubic splines 7 to 11 6 to 10 4 to 6 5 to 7
5-year age categories 6 to 10 6 to 9 4 to 6 4 to 7
10-year age categories 6 to 10 5 to 9 2 to 5 3 to 6
thirds 2 to 6 1 to 5 1 to 3 1 to 3
fourths 4 to 8 3 to 7 1 to 4 2 to 5
fifths 5 to 9 4 to 8 2 to 5 3 to 6
4132
4124–4135
polynomials or restricted cubic splines, or that assumed a linear relationship between the predictor and
outcome all showed a higher net benefit, over a range of thresholds (0.09 to 0.2), than the categorising
approaches (see supplementary figure 9).
4. discussion
we examined the impact of the choice of approach for handling continuous predictors when developing
a prognostic model and evaluating it on a separate data set. we developed models using data sets of
varying size to illustrate the influence of sample size. the predictive ability of the models was evaluated
on a large geographical validation data set, using performance measures that have been recommended
for evaluation and reporting [43,44].
we have demonstrated that categorising continuous predictors, particularly dichotomising at the median value of the predictor, produces models that have substantially weaker predictive performance than
models produced with alternative approaches that retain the predictor on a continuous scale [1]. it is not
surprising that categorising continuous predictors leads to poor models, as it forces an unrealistic, biologically implausible and ultimately incorrect (step) relationship onto the predictor and discard information. it may appear to be sensible to use two or more quantiles (or similar apparently meaningful cutpoints, such as a particular age) to categorise an outcome into three or more groups, but this approach
does not reflect the actual predictor–outcome relationship. individuals whose predictor values are similar
but are either side of a cut-point are assigned different levels of risk, which has clear implications on how
the model will be used in clinical practice. the fewer the cut-points, the larger the difference in risk
between two individuals with similar predictor values immediately either side of a cut-point. we
observed only small differences between the methods that retained the variables as continuous (assuming linearity or nonlinearity). larger differences would be expected if the relationship between a continuous variable and the outcome were markedly nonlinear.
continuous predictors are often categorised as the approach is intuitive, simple to implement and
researchers may expect it to improve model use and uptake. however, categorising comes at a substantial cost to the predictive performance. we believe that this cost is detrimental and counterproductive, as the resulting models have weak predictive accuracy and are unpopular for clinical use.
if ease of use is required, we recommend leaving predictors as continuous during modelling and
instead simplifying the final model, using a points system to allow finer calculation of an individual’s risk [45–47].
we focused on exploring the differences in the predictive performance of models produced by each
approach for handling continuous predictors. focusing on a fixed and small number of variables, we
circumvented issues around variable selection procedures for which we anticipate the differences in performance to be more marked. using restricted cubic splines on very small data sets may produce wiggly
functions, and fractional polynomials can result in poorly estimated tails. both problems lead to unstable
models that have poor predictive ability when evaluated on separate data in a geographical validation.
we observed very little difference in model performance when using either fractional polynomials or restricted cubic splines. similarly, when a data set is small, the centile values for dichotomising or
categorising continuous predictor values may vary considerably from sample to sample, again leading
to unstable models and poor predictive performance. we did, however, also examine the influence of
smaller sample sizes on model performance and observed greater variability in model performance in
small samples with few outcome events than in larger samples. furthermore, the apparent performance
on smaller data sets was higher compared to using larger data set (which decreased as sample size
increased), but at the expense of poorer performance in the geographical validation, where recent guidance suggests that a minimum of 100 events are required [48].
systematic reviews have highlighted numerous methodological flaws, including small sample size,
missing data and inappropriate statistical analyses in studies that describe the development or validation of prognostic models [6,8,49–52]. contributing to why many studies have methodological shortcomings is because it is generally easy to create a (poorly performing) prognostic model. a model
that has been developed using poor methods is likely to produce overoptimistic and misleading
performance. as noted by andrew vickers, ‘we have a data set, and a statistical package, and
add the former to the latter, hit a few buttons and voila, we have another paper’ [53]. it is therefore
both unsurprising and fortunate that most prognostic models never get used. however, some of these
poorly performing or suboptimal prognostic models will undoubtedly be used, which may have undesirable clinical consequences. we have clearly demonstrated that categorising continuous predictor
4133
4124–4135
outcomes is an inadequate approach. investigators who wish to develop a new prognostic model for
use on patients that is fit for purpose should follow the extensive methodological guidance on model
building that discourages categorisation and use an approach that keeps the outcome continuous
[11–13].



<|EndOfText|>

abstract
prediction models are developed to aid health care providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision making. however, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes. this article describes how the tripod statement was developed. an extensive list of items based on a review of the literature was created, which was reduced after a web-based survey and revised during a 3-day meeting in june 2011 with methodologists, health care professionals, and journal editors. the list was refined during several meetings of the steering group and in e-mail discussions with the wider group of tripod contributors. the resulting tripod statement is a checklist of 22 items, deemed essential for transparent reporting of a prediction model study. the tripod statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. the tripod statement is best used in conjunction with the tripod explanation and elaboration document. to aid the editorial process and readers of prediction model studies, it is recommended that authors include a completed checklist in their submission (also available at www.tripod-statement.org).

editors’ note: in order to encourage dissemination of the tripod statement, this article is freely accessible on the annals of internal medicine web site (www.annals.org) and will be also published in bjog, british journal of cancer, british journal of surgery, bmc medicine, british medical journal, circulation, diabetic medicine, european journal of clinical investigation, european urology, and journal of clinical epidemiology. the authors jointly hold the copyright of this article. an accompanying explanation and elaboration article is freely available only on www.annals.org; annals of internal medicine holds copyright for that article.

background
in medicine, patients with their care providers are confronted with making numerous decisions on the basis of an estimated risk or probability that a specific disease or condition is present (diagnostic setting) or a specific event will occur in the future (prognostic setting) (figure 1). in the diagnostic setting, the probability that a particular disease is present can be used, for example, to inform the referral of patients for further testing, initiate treatment directly, or reassure patients that a serious cause for their symptoms is unlikely. in the prognostic setting, predictions can be used for planning lifestyle or therapeutic decisions based on the risk for developing a particular outcome or state of health within a specific period [1,2]. such estimates of risk can also be used to risk-stratify participants in therapeutic clinical trials [3,4].

figure 1
figure1
schematic representation of diagnostic and prognostic prediction modeling studies. the nature of the prediction in diagnosis is estimating the probability that a specific outcome or disease is present (or absent) within an individual, at this point in time—that is, the moment of prediction (t = 0). in prognosis, the prediction is about whether an individual will experience a specific event or outcome within a certain time period. in other words, in diagnostic prediction the interest is in principle a cross-sectional relationship, whereas prognostic prediction involves a longitudinal relationship. nevertheless, in diagnostic modeling studies, for logistical reasons, a time window between predictor (index test) measurement and the reference standard is often necessary. ideally, this interval should be as short as possible and without starting any treatment within this period.

full size image
in both the diagnostic and prognostic setting, estimates of probabilities are rarely based on a single predictor [5]. doctors naturally integrate several patient characteristics and symptoms (predictors, test results) to make a prediction (see figure 2 for differences in common terminology between diagnostic and prognostic studies). prediction is therefore inherently multivariable. prediction models (also commonly called “prognostic models,” “risk scores,” or “prediction rules” [6]) are tools that combine multiple predictors by assigning relative weights to each predictor to obtain a risk or probability [1,2]. well-known prediction models include the framingham risk score [7], ottawa ankle rules [8], euroscore [9], nottingham prognostic index [10], and the simplified acute physiology score [11].

figure 2
figure2
similarities and differences between diagnostic and prognostic prediction models.

full size image
prediction model studies
prediction model studies can be broadly categorized as model development [12], model validation (with or without updating) [13] or a combination of both (figure 3). model development studies aim to derive a prediction model by selecting the relevant predictors and combining them statistically into a multivariable model. logistic and cox regression are most frequently used for short-term (for example, disease absent vs. present, 30-day mortality) and long-term (for example, 10-year risk) outcomes, respectively [12–17]. studies may also focus on quantifying the incremental or added predictive value of a specific predictor (for example, newly discovered) to a prediction model [18].

figure 3
figure3
types of prediction model studies covered by the tripod statement. d = development data; v = validation data.

full size image
quantifying the predictive ability of a model on the same data from which the model was developed (often referred to as apparent performance) will tend to give an optimistic estimate of performance, owing to overfitting (too few outcome events relative to the number of candidate predictors) and the use of predictor selection strategies [19]. studies developing new prediction models should therefore always include some form of internal validation to quantify any optimism in the predictive performance (for example, calibration and discrimination) of the developed model. internal validation techniques use only the original study sample and include such methods as bootstrapping or cross-validation. internal validation is a necessary part of model development [2]. overfitting, optimism, and miscalibration may also be addressed and accounted for during the model development by applying shrinkage (for example, heuristic or based on bootstrapping techniques) or penalization procedures (for example, ridge regression or lasso) [20].

after developing a prediction model, it is strongly recommended to evaluate the performance of the model in other participant data than was used for the model development. such external validation requires that for each individual in the new data set, outcome predictions are made using the original model (that is, the published regression formula) and compared with the observed outcomes [13,14]. external validation may use participant data collected by the same investigators, typically using the same predictor and outcome definitions and measurements, but sampled from a later period (temporal or narrow validation); by other investigators in another hospital or country, sometimes using different definitions and measurements (geographic or broad validation); in similar participants but from an intentionally different setting (for example, model developed in secondary care and assessed in similar participants but selected from primary care); or even in other types of participants (for example, model developed in adults and assessed in children, or developed for predicting fatal events and assessed for predicting nonfatal events) [13,15,17,21,22]. in case of poor performance, the model can be updated or adjusted on the basis of the validation data set [13].

reporting of multivariable prediction model studies
studies developing or validating a multivariable prediction model share specific challenges for researchers [6]. several reviews have evaluated the quality of published reports that describe the development or validation prediction models [23–28]. for example, mallett and colleagues [26] examined 47 reports published in 2005 presenting new prediction models in cancer. reporting was found to be poor, with insufficient information described in all aspects of model development, from descriptions of patient data to statistical modeling methods. collins and colleagues [24] evaluated the methodological conduct and reporting of 39 reports published before may 2011 describing the development of models to predict prevalent or incident type 2 diabetes. reporting was also found to be generally poor, with key details on which predictors were examined, the handling and reporting of missing data, and model-building strategy often poorly described. bouwmeester and colleagues [23] evaluated 71 reports, published in 2008 in 6 high-impact general medical journals, and likewise observed an overwhelmingly poor level of reporting. these and other reviews provide a clear picture that, across different disease areas and different journals, there is a generally poor level of reporting of prediction model studies [6,2–27,29]. furthermore, these reviews have shown that serious deficiencies in the statistical methods, use of small data sets, inappropriate handling of missing data, and lack of validation are common [6,23–27,29]. such deficiencies ultimately lead to prediction models that are not or should not be used. it is therefore not surprising, and fortunate, that very few prediction models, relative to the large number of models published, are widely implemented or used in clinical practice [6].

prediction models in medicine have proliferated in recent years. health care providers and policy makers are increasingly recommending the use of prediction models within clinical practice guidelines to inform decision making at various stages in the clinical pathway [30,31]. it is a general requirement of reporting of research that other researchers can, if required, replicate all the steps taken and obtain the same results [32]. it is therefore essential that key details of how a prediction model was developed and validated be clearly reported to enable synthesis and critical appraisal of all relevant information [14,33–36].

reporting guidelines for prediction model studies: the tripod statement
we describe the development of the tripod (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) statement, a guideline specifically designed for the reporting of studies developing or validating a multivariable prediction model, whether for diagnostic or prognostic purposes. tripod is not intended for multivariable modeling in etiologic studies or for studies investigating single prognostic factors [37]. furthermore, tripod is also not intended for impact studies that quantify the impact of using a prediction model on participant or doctors’ behavior and management, participant health outcomes, or cost-effectiveness of care, compared with not using the model [13,38].

reporting guidelines for observational (the strengthening the reporting of observational studies in epidemiology [strobe]) [39], tumor marker (reporting recommendations for tumour marker prognostic studies [remark]) [37], diagnostic accuracy (standards for the reporting of diagnostic accuracy studies [stard]) [40], and genetic risk prediction (genetic risk prediction studies [grips]) [41] studies all contain many items that are relevant to studies developing or validating prediction models. however, none of these guidelines are entirely appropriate for prediction model studies. the 2 guidelines most closely related to prediction models are remark and grips. however, the focus of the remark checklist is primarily on prognostic factors and not prediction models, whereas the grips statement is aimed at risk prediction using genetic risk factors and the specific methodological issues around handling large numbers of genetic variants.

to address a broader range of studies, we developed the tripod guideline: transparent reporting of a multivariable prediction model for individual prognosis or diagnosis. tripod explicitly covers the development and validation of prediction models for both diagnosis and prognosis, for all medical domains and all types of predictors. tripod also places much more emphasis on validation studies and the reporting requirements for such studies. the reporting of studies evaluating the incremental value of specific predictors, beyond established predictors or even beyond existing prediction models [18,42], also fits entirely within the remit of tripod (see the accompanying explanation and elaboration document [43]).

developing the tripod statement
we convened a 3-day meeting with an international group of prediction model researchers, including statisticians, epidemiologists, methodologists, health care professionals, and journal editors (from annals of internal medicine, bmj, journal of clinical epidemiology, and plos medicine) to develop recommendations for the tripod statement.

we followed published guidance for developing reporting guidelines [44] and established a steering committee (drs. collins, reitsma, altman, and moons) to organize and coordinate the development of tripod. we conducted a systematic search of medline, embase, psychinfo, and web of science to identify any published articles making recommendations on reporting of multivariable prediction models (or aspects of developing or validating a prediction model), reviews of published reports of multivariable prediction models that evaluated methodological conduct or reporting and reviews of methodological conduct and reporting of multivariable models in general. from these studies, a list of 129 possible checklist items was generated. the steering committee then merged related items to create a list of 76 candidate items.

twenty-five experts with a specific interest in prediction models were invited by e-mail to participate in the web-based survey and to rate the importance of the 76 candidate checklist items. respondents (24 of 27) included methodologists, health care professionals, and journal editors. (in addition to the 25 meeting participants, the survey was also completed by 2 statistical editors from annals of internal medicine).

the results of the survey were presented at a 3-day meeting in june 2011, in oxford, united kingdom; it was attended by 24 of the 25 invited participants (22 of whom had participated in the survey). during the 3-day meeting, each of the 76 candidate checklist items was discussed in turn, and a consensus was reached on whether to retain, merge with another item, or omit the item. meeting participants were also asked to suggest additional items. after the meeting, the checklist was revised by the steering committee during numerous face-to-face meetings, and circulated to the participants to ensure it reflected the discussions. while making revisions, conscious efforts were made to harmonize our recommendations with other reporting guidelines, and where possible we chose the same or similar wording for items [37,39,41,45,46].

tripod components
the tripod statement is a checklist of 22 items that we consider essential for good reporting of studies developing or validating multivariable prediction models (table 1). the items relate to the title and abstract (items 1 and 2), background and objectives (item 3), methods (items 4 through 12), results (items 13 through 17), discussion (items 18 through 20), and other information (items 21 and 22). the tripod statement covers studies that report solely development [12,15], both development and external validation, and solely external validation (with or without updating), of a prediction model [14] (figure 1). therefore, some items are relevant only for studies reporting the development of a prediction model (items 10a, 10b, 14, and 15), and others apply only to studies reporting the (external) validation of a prediction model (items 10c, 10e, 12, 13c, 17, and 19a). all other items are relevant to all types of prediction model development and validation studies. items relevant only to the development of a prediction model are denoted by d, items relating solely to a validation of a prediction model are denoted by v, whereas items relating to both types of study are denoted d;v.

table 1 checklist of items to include when reporting a study developing or validating a multivariable prediction model for diagnosis or prognosis*
full size table
the recommendations within tripod are guidelines only for reporting research and do not prescribe how to develop or validate a prediction model. furthermore, the checklist is not a quality assessment tool to gauge the quality of a multivariable prediction model.

an ever-increasing number of studies are evaluating the incremental value of specific predictors, beyond established predictors or even beyond existing prediction models [18,42].the reporting of these studies fits entirely within the remit of tripod (see accompanying explanation and elaboration document [43].

the tripod explanation and elaboration document
in addition to the tripod statement, we produced a supporting explanation and elaboration document [43] in a similar style to those for other reporting guidelines [47–49]. each checklist item is explained and accompanied by examples of good reporting from published articles. in addition, because many such studies are methodologically weak, we also summarize the qualities of good (and the limitations of less good) studies, regardless of reporting [43]. a comprehensive evidence base from existing systematic reviews of prediction models was used to support and justify the rationale for including and illustrating each checklist item. the development of the explanation and elaboration document was completed after several face-to-face meetings, teleconferences, and iterations among the authors. additional revisions were made after sharing the document with the whole tripod group before final approval.

role of the funding source
there was no explicit funding for the development of this checklist and guidance document. the consensus meeting in june 2011 was partially funded by a national institute for health research senior investigator award held by dr. altman, cancer research uk, and the netherlands organization for scientific research. drs. collins and altman are funded in part by the medical research council. dr. altman is a member of the medical research council prognosis research strategy (progress) partnership. the funding sources had no role in the study design, data collection, analysis, preparation of the manuscript, or decision to submit the manuscript for publication.

discussion
many reviews have showed that the quality of reporting in published articles describing the development or validation of multivariable prediction models in medicine is poor [23–27,29]. in the absence of detailed and transparent reporting of the key study details, it is difficult for the scientific and health care community to objectively judge the strengths and weaknesses of a prediction model study [34,50,51]. the explicit aim of this checklist is to improve the quality of reporting of published prediction model studies. the tripod guideline has been developed to support authors in writing reports describing the development, validation or updating of prediction models, aid editors and peer reviewers in reviewing manuscripts submitted for publication, and help readers in critically appraising published reports.

the tripod statement does not prescribe how studies developing, validating, or updating prediction models should be undertaken, nor should it be used as a tool for explicitly assessing quality or quantifying risk of bias in such studies [52]. there is, however, an implicit expectation that authors have an appropriate study design and conducted certain analyses to ensure all aspects of model development and validation are reported. the accompanying explanation and elaboration document describes aspects of good practice for such studies, as well as highlighting some inappropriate approaches that should be avoided [43].

tripod encourages complete and transparent reporting reflecting study design and conduct. it is a minimum set of information that authors should report to inform the reader about how the study was carried out. we are not suggesting a standardized structure of reporting, rather that authors should ensure that they address all the checklist items somewhere in their article with sufficient detail and clarity.

we encourage researchers to develop a study protocol, especially for model development studies, and even register their study in registers that accommodate observational studies (such as clinicaltrials.gov) [53,54]. the importance of also publishing protocols for developing or validating prediction models, certainly when conducting a prospective study, is slowly being acknowledged [55,56]. authors can also include the study protocol when submitting their article for peer review, so that readers can know the rationale for including individuals into the study or whether all of the analyses were prespecified.

to help the editorial process; peer reviewers; and, ultimately, readers, we recommend submitting the checklist as an additional file with the report, indicating the pages where information for each item is reported. the tripod reporting template for the checklist can be downloaded from [57].

announcements and information relating to tripod will be broadcast on the tripod twitter address (@tripodstatement). the enhancing the quality and transparency of health research (equator) network [58] will help disseminate and promote the tripod statement.

methodological issues in developing, validating, and updating prediction models evolve. tripod will be periodically reappraised, and if necessary modified to reflect comments, criticisms, and any new evidence. we therefore encourage readers to make suggestions for future updates so that ultimately, the quality of prediction model studies will improve.

<|EndOfText|>

minimum sample size for developing a multivariable
prediction model: part ii - binary
and time-to-event outcomes
when designing a study to develop a new prediction model with binary or
time-to-event outcomes, researchers should ensure their sample size is adequate
in terms of the number of participants (n) and outcome events (e) relative to
the number of predictor parameters (p) considered for inclusion. we propose
that the minimum values of n and e (and subsequently the minimum number
of events per predictor parameter, epp) should be calculated to meet the following three criteria: (i) small optimism in predictor effect estimates as defined by
a global shrinkage factor of ≥0.9, (ii) small absolute difference of ≤ 0.05 in the
model's apparent and adjusted nagelkerke's r2, and (iii) precise estimation of
the overall risk in the population. criteria (i) and (ii) aim to reduce overfitting
conditional on a chosen p, and require prespecification of the model's anticipated cox-snell r2, which we show can be obtained from previous studies. the
values of n and e that meet all three criteria provides the minimum sample
size required for model development. upon application of our approach, a new
diagnostic model for chagas disease requires an epp of at least 4.8 and a new
prognostic model for recurrent venous thromboembolism requires an epp of at
least 23. this reinforces why rules of thumb (eg, 10 epp) should be avoided.
researchers might additionally ensure the sample size gives precise estimates of
key predictor effects; this is especially important when key categorical predictors have few events in some categories, as this may substantially increase the
numbers required.
keywords
binary and time-to-event outcomes, logistic and cox regression, multivariable prediction model,
pseudo r-squared, sample size, shrinkage
1 introduction
statistical models for risk prediction are needed to inform clinical diagnosis and prognosis in healthcare.1-3 for example,
they may be used to predict an individual's risk of having an undiagnosed disease or condition (“diagnostic prediction
model”), or to predict an individual's risk of experiencing a specific event in the future (“prognostic prediction model”).
they are typically developed using a multivariable regression framework, such as logistic or cox (proportional hazards)
regression, which provides an equation to estimate an individual's risk based on their values of multiple predictors (such
as age and smoking, or biomarkers and genetic information). well-known examples are the wells score for predicting
the presence of a pulmonary embolism4,5
; the framingham risk score and qrisk2,6,7 which estimate the 10-year risk
of developing cardiovascular disease (cvd); and the nottingham prognostic index, which predicts the 5-year survival
probability of a woman with newly diagnosed breast cancer.8,9
researchers planning or designing a study to develop a new multivariable prediction model must consider sample size
requirements for their development data set. our related paper considered this issue for prediction models of a continuous
outcome using linear regression.10 here, we focus on binary and time-to-event outcomes, such as the risk of already having
a pulmonary embolism, or the risk of developing cvd in the next 10 years. in this situation, the effective sample size is
often considered to be the number of outcome events (eg, the number with existing pulmonary embolism, or the number
diagnosed with cvd during follow-up). in particular, a well-used “rule of thumb” for sample size is to ensure at least
10 events per candidate predictor (variable),11-13 where “candidate” indicates a predictor in the development data set that
is considered, before any variable selection, for inclusion in the final model. note that, if a predictor is categorical with
three of more categories, or continuous and modelled as a nonlinear trend, then including the predictor will require two
or more parameters being included in the model. therefore, we refer to events per predictor parameter (epp) here, rather
than events per variable.
the 10 epp rule has generated much debate. some authors claim that the epp can sometimes be lowered below 10.14
in contrast, harrell generally recommends at least 15 epp,15 and others identify situations where at least 20 epp or up
to 50 epp are required.16-19 however, a concern is that any blanket rule of thumb is too simplistic, and that the number
of participants required will depend on many intricate aspects, including the magnitude of predictor effects, the overall
outcome risk, the distribution of predictors, and the number of events for each category of categorical predictors.16 for
example, courvoisier et al20 concluded that “there is no single rule based on epp that would guarantee an accurate
estimation of logistic regression parameters.” a new sample size approach is needed to address this.
in this article, we propose the sample size (n) and number of events (e) in the model development data set must, at
the very least, meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global
shrinkage factor of ≥0.9, (ii) small absolute difference of ≤ 0.05 in the model's apparent and adjusted nagelkerke's r2, and
(iii) precise estimation of the overall risk or rate in the population (or similarly, precise estimation of the model intercept
when predictors are mean centred). the values of n and e (and subsequently epp) that meet all three criteria provide the
minimum values required for model development. criteria (i) and (ii) aim to reduce the potential for a developed model to
be overfitted to the development data set at hand. overfitting leads to model predictions that are more extreme than they
ought to be when applied to new individuals, and most notably occurs when the number of candidate predictors is large
relative to the number of outcome events. a consequence is that a developed model's apparent predictive performance
(as observed in the development data set itself) will be optimistic, and its performance in new data will usually be lower.
therefore, it is good practise to reduce the potential for overfitting when developing a prediction model,15 which criteria
(i) and (ii) aim to achieve. in addition, criterion (iii) aims to ensure that the overall risk (eg, by a key time point for
prediction) is estimated precisely, as fundamentally, before tailoring predictions to individuals, a model must be able to
reliably predict the overall or mean risk in the target population.
the article is structured as follows. section 2 introduces our proposed criterion (i), for which key concepts of a global
shrinkage factor and the cox-snell r2 are introduced.21 the latter needs to prespecified to utilise our sample size formula,
and so in section 3, we suggest how realistic values of the cox-snell r2 can be obtained in advance of any data collection,
eg, by using published information from an existing model in the same field, including values of the c statistic or alternative r2 measures. extension to criteria (ii) and (iii) is then made in section 4. section 5 then provides two examples,
which demonstrate our sample size approach for diagnostic and prognostic models. section 6 raises a potential additional
criteria to consider: ensuring precise estimates of key predictor effects, to help ensure precise predictions across the entire
spectrum of predicted risk. section 7 concludes with discussion.
2 sample size required to minimise overfitting of predictor
effects
to adjust for overfitting during model development (and thereby improve the model's predictive performance in
new individuals), statistical methods for penalisation of predictor effect estimates are available, where regression
coefficients are shrunk toward zero from their usual estimated value (eg, from standard maximum likelihood
estimation).22-26 van houwelingen notes that “ … shrinkage works on the average but may fail in the particular unique
problem on which the statistician is working.”22 therefore, it is important to minimise the potential for overfitting during
model development, and this criterion forms the basis of our first sample size calculation. our approach is motivated
by the concept of a global shrinkage factor (a measure of overfitting), and so we begin by introducing this, before then
deriving a sample size formula.
2.1 concept of a global shrinkage for logistic and cox regression
the concept of shrinkage (penalisation) was outlined in our accompanying paper,10 and is explained in detail
elsewhere.1,15,27 here, we focus on using a global shrinkage factor (s), sometimes referred to as a uniform shrinkage
factor. consider a logistic regression model has been fitted using standard maximum likelihood estimation (ie, traditional
and unpenalised estimation). subsequently, s can be estimated (eg, using bootstrapping,28 or via a closed-form solution;
see section 2.2) and applied to the estimated predictor effects, so that the revised model is
𝑙𝑛 ( pi
1 − pi
)
= 𝛼∗ + s
(
𝛽̂
1x1i + 𝛽̂
2x2i + 𝛽̂
3x3i +···)
. (1)
here, pi is the outcome probability for the ith individual, the 𝛽̂ terms denote the original predictor effect estimates
(ln odds ratios) from maximum likelihood, and 𝛼* is the intercept that has been re-estimated (after shrinkage of predictor
effects) to ensure perfect calibration-in-the-large, such that, the overall predicted risk still agrees with the overall observed
risk in the development data set (for details on how to do this, we refer to the works of harrell15 and steyerberg1
). similarly, after fitting a proportional hazards (cox) regression model using standard maximum likelihood, the model can be
revised using
hi(t) = h0(t)
∗ exp (
s
(
𝛽̂
1x1i + 𝛽̂
2x2i + 𝛽̂
3x3i +···)) , (2)
where hi(t) is the hazard rate of the outcome over time (t) for the ith individual and ho(t)
* is the baseline hazard function
re-estimated (after shrinkage of predictor effects) to ensure the predicted and observed outcome rates agree for the development data set as whole. compared to the original (nonpenalised) models, the revised models (1) and (2) will shrink
predicted probabilities away from zero and one, toward the overall mean outcome probability in the development data set.
example of a global shrinkage factor
van diepen et al developed a prognostic model for 1-year mortality risk in patients with diabetes starting dialysis.29
they use a logistic regression framework, with backwards selection to choose predictors in a dataset of 394 patients with
84 deaths by 1 year, and the estimated model is shown in table 1. to examine overfitting, the authors use bootstrapping to
estimate a global shrinkage factor of 0.903, indicating that the original model was slightly overfitted to the data. therefore,
a revised prediction model was produced by multiplying the original 𝛽̂ coefficients (ln odds ratios) from the original
logistic regression model by a global shrinkage factor of s = 0.903.
table 1 example of global shrinkage applied to a prognostic model for 1-year mortality risk in patients with diabetes starting
dialysis29
developed (unpenalised) model final (penalised) model adjusted for overfitting
intercept 𝜶 𝜶 ̂ *
1.962 1.427
predictor 𝜷
̂ s𝜷
̂ = 0.903𝜷
̂
age (years) 0.047 0.042
smoking 0.631 0.570
macrovascular complications 1.195 1.078
duration of diabetes mellitus (years) 0.026 0.023
karnofsky scale −0.043 −0.039
haemoglobin level (g/dl) −0.186 −0.168
albumin level (g/l) −0.060 −0.054
2.2 expressing sample size in terms of a global shrinkage factor
bootstrapping is an excellent way to calculate the shrinkage factor postestimation, but (as it is a resampling method) is
not useful for us in advance of data collection. an alternative approach to calculating a global shrinkage factor is to use
the closed form “heuristic” shrinkage factor of van houwelingen and le cessie,23 defined by
svh = 1 − p
lr , (3)
where p is the total number of predictor parameters for the full set of candidate predictors (ie, all those considered for
inclusion in the model) and lr is the likelihood ratio (chi-squared) statistic for the fitted model defined as
lr = −2 (ln lnull − ln lmodel) , (4)
where ln lnull is the log-likelihood of a model with no predictors (eg, intercept-only logistic regression model), and ln lmodel
is the log-likelihood of the final model. in our related paper on linear regression, we used the copas shrinkage estimate that
is similar to equation (3), but with p replaced by p + 2. in our experience, svh performs better for generalised linear models
than the copas estimate, with svh further from 1 and closer to the corresponding estimate obtained from bootstrapping.
copas also notes that, unlike for linear regression, a formal justification for replacing p by p + 2 in equation (2) has not
been proved for logistic regression.30
hence, we use equation (3) as our shrinkage estimate (ie, our measure of overfitting) for logistic and cox regression
models, which now motivates our sample size approach to meet criterion (i). first, let us re-express the right-hand side
of equation (3) in terms of sample size (n), number of candidate predictor parameters (p), and the cox-snell generalised
r2.
21 the latter is also known as the maximum likelihood r2, the likelihood ratio r2, or magee's r2,
31 and it provides a
generalisation (eg, to logistic and cox regression models) of the well-known proportion of variance explained for linear
regression models. let us use r2
cs_app to denote the apparent (“app”) estimate of a prediction model's cox-snell (“cs”) r2
performance as obtained from the model development data set. it can be shown (eg, see the works of magee31 or hendry
and nielsen32) that the lr statistic can be expressed in terms of the sample size (n) and r2
cs_app as follows:
lr = −n ln (
1 − r2
cs_app)
. (5)
this leads to the cox-snell generalised definition of the apparent r2 expressed in terms of the lr value for any regression
model, including logistic and cox regression
r2
cs_app = 1 − exp (−lr
n
)
. (6)
applying equation (5) within equation (3), the van houwelingen and le cessie shrinkage factor becomes
svh = 1 + p
n ln (
1 − r2
cs_app) . (7)
2.3 criterion (i): calculating sample size to ensure a shrinkage factor ≥ 0.9
equation (7) provides a closed-form solution for the expected shrinkage conditional on n, p, and r2
cs_app. therefore, if
we could specify a realistic value for r2
cs_app in advance of our study starting, we could identify values of n and p that
correspond to a desired shrinkage factor (eg, 0.9), thus informing the required sample size. however, a major problem is
that r2
cs_app is a postestimation measure of model fit, whereas for a sample size calculation, this needs to be specified in
advance of collecting the data when designing a new study. furthermore, due to overfitting in the model development
data set, the observed r2
cs_app is generally an upwardly biased (optimistic) estimate of the cox-snell r2 as it is estimated
in the same data used to develop the model. thus, in new data, the actual cox-snell r2 peformance is likely to be lower.
therefore, we need to re-express svh in terms of r2
cs_adj, an adjusted (approximately unbiased) estimate of the model's
expected r2
cs performance in new individuals from the same population. in other words, r2
cs_adj is a modification of r2
cs_app
to adjust for optimism (caused by overfitting) in the model development data set. for generalised linear models such as
logistic regression, mittlboeck and heinzl suggest that r2
cs_adj can be obtained by33
r2
cs_adj = svhr2
cs_app (8)
as the expected value of this r2
cs_adj corresponds to the underlying population value.33 by rearranging equation (8), we
can express r2
cs_app in terms of r2
cs_adj
r2
cs_app =
r2
cs_adj
svh
. (9)
applying equation (9) within equation (7), we can now express svh in terms of r2
cs_adj, rather than r2
cs_app
svh = 1 + p
n ln (
1 − r2
cs_adj
svh ). (10)
finally, a simple rearrangement of equation (10) leads to a closed-form solution for the required sample size to develop
a prediction model conditional on p, svh and r2
cs_adj
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ). (11)
for example, for developing a new logistic regression model based on up to 20 candidate predictor parameters with an
anticipated r2
cs_adj of at least 0.1, then to target an expected shrinkage of 0.9, we need a sample size of
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ) = 20
(0.9 − 1) ln (
1 − 0.1
0.9
) = 1698,
and thus 1698 individuals.
2.4 translating the calculated sample size to the number of events and epp
it may be surprising that the overall outcome proportion (or overall outcome rate) is not directly included in the right-hand
side of the sample size equation (11), especially because the total number of events, e, (which depends on the outcome
proportion or rate) is often considered the effective sample size for binary and time-to-event outcomes.15 however, the
outcome proportion (rate) is indirectly accounted for in the sample size calculation via the chosen r2
cs_adj, as the maximum value of r2
cs_adj for the intended population of the model depends on the overall outcome proportion (rate) for
that population. as the outcome proportion decreases, the maximum value of r2
cs decreases. this is explained further in
section 3.4. therefore, after n is derived from the sample size equation (11), e can be obtained by combining the calculated
n with the outcome proportion (rate) for the intended population. similarly, epp can be obtained.
for example for binary outcomes, e = n𝜙 and epp = n𝜙/p, where 𝜙 is the overall outcome proportion in the target
population (ie, the overall prevalence for diagnostic models, or the overall cumulative incidence by a key time point
for prognostic models). in our aforementioned hypothetical example, where 1698 subjects were needed based on an
r2
cs_adj of 0.1 and svh of 0.9, then if the intended setting has 𝜙 of 0.1 (ie, overall outcome risk is 10%), the required
e = 1698 × 0.1 = 169.8. with 20 predictor parameters, the required epp = (1698 × 0.1)/20 = 8.5. however, if the intended
setting has 𝜙 of 0.3, then e = 509.4 and epp = 25.5. the big change in epp is because, although the chosen value of r2
cs_adj
is fixed at 0.1, the maximum value of r2
cs is much higher for the setting with the higher outcome proportion.
we can explain this further using nagelkerke's “proportion of total variance explained”,34 which is calculated as
r2
cs_adj∕ max(r2
cs). if two models have the same r2
cs_adj (say at 0.1, as in the aforementioned examples), then nagelkerke's
measure of predictive performance will be lower for the model whose setting has a higher outcome proportion, as the
max(r2
cs) is larger in that setting. models with lower performance have larger overfitting concerns,22 and therefore require
larger epp to minimise overfitting than models with high performance. hence, explaining why epp was larger when 𝜙
was 0.3 compared with 0.1 in the aforementioned example. this highlights that a blanket rule of thumb (such as at least
10 epp) is unlikely to be sensible to meet criterion (i), as the actual epp depends on the setting/population of interest
(which dictates the overall outcome proportion or rate) and expected model performance.
3 how to prespecify r𝟐
cs_adj based on previous information
our sample size proposal in equation (11) requires researchers to provide a value for the model's r2
cs_adj, that is, to prespecify the anticipated cox-snell r2 value if the model was applied to new individuals. how should this be done? we
recommend using r2
cs_adj values from previous prediction model studies for the same (or similar) population, considering
the same (or similar) outcomes and time points of interest. for example, the researcher could consult systematic reviews
of existing models and their performance, which are also increasingly available,35 or registries that record the prediction
models available in a particular field.36
often, a new prediction model is developed specifically to update or improve upon the performance of an existing model,
by using additional predictors. then, the existing model's r2
cs_adj could be used as a lower bound for the new model's
anticipated r2
cs_adj. in this situation, if the apparent cox-snell estimate, r2
cs_app, is available in an article describing the
development of the existing model, then its r2
cs_adj can be derived using equation (8) as long as the study's n and p can
also be obtained. in addition, as in van diepen et al's example (table 1), a global shrinkage factor may be reported directly
for an existing model development study, and if so, r2
cs_adj can be derived from a simple rearrangement of equation (10),
again as long as the study's n and p are also available.
note that, if r2
cs_app is available from an external validation study of an existing model, there is no need for adjustment
(ie, r2
cs_app = r2
cs_adj), as the validation dataset provides a direct estimate of the model's performance in new individuals
(free from overfitting concerns as there is no model development therein).
other options to obtain r2
cs_adj from the existing literature are now described. for guidance on choosing an r2
cs_adj value
in the absence of any prior information, please see our discussion.
3.1 using the lr statistic to derive the cox-snell r𝟐
adj
if the r2
cs_app or r2
cs_adj is not available in the publication of an existing model, the lr value may be reported, which would
allow r2
cs_app to be derived using equation (6), then svh for the model derived using equation (7) (assuming the model's
n and p are also provided), and finally r2
cs_adj using equation (8).
sometimes the log-likelihood of the final model (lnlmodel) is reported, but not the lr value itself. in this situation, the
researcher should calculate ln lnull based on other information in the article, and then calculate lr using equation (4),
thus allowing r2
cs_app and r2
cs_adj to be derived using equations (6) and (8), respectively. for example, in a logistic
regression model, the loglnull value can be calculated using
ln lnull = e ln (e
n
)
+ (n − e)ln (
1 − e
n
)
, (12)
where e is the total number of outcome events. of course, this assumes e and n are actually available in the article.
similarly, for an exponential survival model (equivalent to a poisson model with ln (survival time) as an offset), the
ln lnull can be calculated using
ln lnull = e ln(𝜆) + 𝜆t = e ln (e
t
)
+ e (13)
as long as 𝜆 (the constant hazard rate), e (the total number of events), and t (the total time at risk, eg, total person-years)
are available in the article. note that, for survival models, packages such as sas and stata usually add a constant to the
reported log-likelihood to ensure it remains the same value regardless of the time scale used. for example, stata adds the
sum of the ln (survival times) for the noncensored individuals to the reported ln lmodel and ln lnull, and so this constant
must be either consistently used or consistently removed in each of ln lmodel and ln lnull when deriving the lr value.
3.2 using other pseudo-r2 statistics to derive r𝟐
cs_adj
sometimes other pseudo-r2 statistics are reported for logistic and survival models, rather than the cox-snell version
specified in equation (6). in particular, because r2
cs_app has a maximum value less than 1, nagelkerke's r2 is sometimes
reported,34 which divides r2
cs_app by the maximum value defined by 1 − exp (2 ln lnull
n
)
, as follows:
r2
nagelkerke_app =
r2
cs_app
max (
r2
cs_app) =
r2
cs_app
1 − exp (2 ln lnull
n
). (14)
recall that ln lnull is derivable from other information, eg, using equations (12) or (13) for logistic and exponential
(poisson) models, respectively. when nagelkerke's r2, ln lnull, and n are available, the r2
cs_app can be calculated by
rearranging equation (14) to give
r2
cs_app = r2
nagelkerke_app (
1 − exp (2 ln lnull
n
)) , (15)
and then r2
cs_adj calculated via equation (8).
another measure sometimes reported is mcfadden's r2 37
r2
mcfadden_app = 1 − ln lmodel
ln lnull
. (16)
as ln lnull is often obtainable (see previous equation), when r2
mcfadden_app is reported, we can rearrange equation (16) to
obtain ln lmodel, and subsequently derive the lr statistic using equation (4), the cox-snell r2
cs_app from equation (6), svh
from equation (7) (assuming the model's n and p are also provided), and finally r2
cs_adj via equation (8).
for proportional hazards survival models, o'quigley et al suggested to modify r2
cs_app by replacing n with the number
of events (e)
38
r2
óquigley_app = 1 − exp (−lr
e
)
. (17)
therefore, if r2
óquigley_app and e were reported, the lr value could be found using
lr = −e ln (
1 − r2
óquigley_app)
, (18)
and subsequently, r2
cs_app can be obtained using equation (6), svh using equation (7), and finally r2
cs_adj using
equation (8).
another measure increasingly being reported for survival models is royston's measure of explained variation,39 which
is given by
r2
royston_app =
r2
óquigley_app
r2
óquigley_app +
(𝜋2
6
) (1 − r2
óquigley_app). (19)
when r2
royston_app is reported it can be used to obtain r2
óquigley_app by rearranging equation (19) as
r2
óquigley_app =
−𝜋2
6
r2
royston_app
(
1 − 𝜋2
6
)
r2
royston_app − 1
. (20)
this subsequently allows lr, r2
cs_app, svh and then r2
cs_adj to be derived as explained previously. a similar measure
to r2
royston is royston and sauerbrei's r2
d,
40 which can be derived from their proposed d statistic (the ln(hazard ratio)
comparing two groups defined by the median value of the model's risk score in the population of application)
r2
d_app =
𝜋
8
d2
𝜋2
6 + 𝜋
8
d2
. (21)
in examples shown by royston,39 r2
royston_app and r2
d_app are reasonably similar, and thus, we tentatively suggest r2
d_app
as
a proxy for r2
royston_app when only r2
d_app (or d) is reported; though, we recognise that further research is needed on the
link between r2
d_app
and r2
royston.
table 2 predicted values of the d statistic
and r2
d from equation (23) for selected values of
the c statistic (values taken from table 1 in the
work of jinks et al41)
c dr𝟐
d cdr𝟐
d
0.50 0 0 0.72 1.319 0.294
0.52 0.11 0.003 0.74 1.462 0.338
0.54 0.221 0.011 0.76 1.61 0.382
0.56 0.332 0.026 0.78 1.765 0.427
0.58 0.445 0.045 0.80 1.927 0.470
0.60 0.560 0.070 0.82 2.096 0.512
0.62 0.678 0.099 0.84 2.273 0.552
0.64 0.798 0.132 0.86 2.459 0.591
0.66 0.922 0.169 0.88 2.652 0.627
0.68 1.05 0.208 0.90 2.857 0.661
0.70 1.182 0.25 0.92 3.070 0.692
3.3 using values of the c statistic to derive r𝟐
cs_adj
jinks et al also proposed the following equation, based on empirical evidence, for predicting royston's d (and thus
subsequently r2
d_app) when only the c statistic is reported for a survival model41
d = 5.50(c − 0.5) + 10.26(c − 0.5)
3
. (22)
table 2 provides values of d (and corresponding values of r2
d_app from equation (21)) predicted from equation (22) for
selected values of the c statistic, as taken from the work of jinks et al.41 thus, if only the c statistic is reported, we can
use equation (22) to predict royston's d statistic and calculate r2
d_app (using equation (21)) as a proxy to r2
royston_app, and
then r2
óquigley_app, lr, r2
cs_app and finally r2
cs_adj computed sequentially using the equations given previously.
further evaluation of the performance of jinks' formula is required, eg, using simulation and across settings with different cumulative outcome incidences. indeed, based on figure 5 in the work of jinks et al,41 the potential error in the
predictions of d appears to increase as c increases, and is about +/− 0.25 when c is 0.8. nevertheless, equation (22)
serves as a good starting point and works well in our applied example (see section 5.2.1). further research is also needed
to ascertain how to predict r2
cs from other measures, such as somer's d statistic.
3.4 the anticipated value of r𝟐
cs_adj may be small
it is important to emphasise that the cox-snell, r2
cs, values for logistic and survival models are usually much lower than
for linear regression models, with values often less than 0.3. a key reason is that (unlike for linear regression) the r2
cs_app
has a maximum value less than 1, defined by
max(
r2
cs_app)
= 1 − exp(2 ln lnull
n
)
. (23)
this is because ln lnull is itself bounded for binary and time-to-event outcomes (see equations (12) and (13)). for example,
for a logistic regression model with an outcome proportion of 50%, using equation (12) and an arbitrary sample size of
100, we have
ln lnull = e ln (e
n
)
+ (n − e)ln (
1 − e
n
)
= 50 ln ( 50
100
)
+ (100 − 50)ln (
1 − 50
100
)
= −69.315,
and therefore, using equation (23),
max(
r2
cs_app)
= 1 − exp(2 ln lnull
n
)
= 1 − exp(−69.315
100
)
= 0.75.
however, for an outcome proportion of 5%, the max(r2
cs_app) is 0.33, and for an outcome proportion of 1%, the max(r2
cs_app)
is 0.11. therefore, especially in situations where the outcome proportion is low, researchers should anticipate a model
with a (seemingly) low r2
cs_app value, and subsequently a low r2
cs_adj value.
low values of r2
cs_app or r2
cs_adj do not necessarily indicate poor model performance. consider the following three
examples. first, poppe et al used a cox regression to develop a model (“predict-cvd”) to predict the risk of future
cvd events within two years in patients with atherosclerotic cvd,42 and directly report an r2
cs_app of 0.04. however,
the corresponding c statistic is 0.72, which shows discriminatory magnitude typical of many prognostic models used in
practice. second, hippisley-cox and coupland use the qresearch database to produce three models (qdiabetes) that
estimates the risk of future diabetes in a general population.43 in their validation of their “model a,” there were 27 311
incident cases of diabetes recorded in 1 322 435 women (3.77 cases per 1000 person-years) during follow-up, and the
reported r2
royston_app was 0.505. using the approach described previously to convert r2
royston to lr, this leads to a r2
cs_app of
0.02; however, the corresponding d statistic of 2.07 and c statistic of 0.89 are large. third, in a risk prediction model for
venous thromboembolism (vte) in women during the first 6 weeks after delivery,44 r2
cs_app was 0.001 due to the extremely
low event risk (7.2 per 10 000 deliveries), but the model still had important discriminatory ability as the corresponding c
statistic was 0.70.
4 additional sample size criteria
criterion (i) focuses on shrinkage of predictor effects, which is a multiplicative measure of overfitting (ie, on the relative
scale). harrell suggests to also evaluate overfitting on the absolute scale and to check key model parameters are estimated
precsiely.15 we now address this with two further criteria.
4.1 criterion (ii): ensuring a small absolute difference in the apparent
and adjusted r𝟐
nagelkerke
our second criterion for minimum sample size is to ensure a small absolute difference (𝛿) between the model's apparent
and adjusted proportion of variance explained. we suggest using nagelkerke's r2 for this purpose as, unlike the cox-snell
r2 value, it can range between 0 and 1, and so a small difference (say ≤ 0.05) can be ubiquitously defined. based on
equation (14), the difference in the apparent and adjusted nagelkerke's r2 can be defined as
r2
nagelkerke_app − r2
nagelkerke_adj =
r2
cs_app
max (
r2
cs_app) −
r2
cs_adj
max (
r2
cs_app)
=
r2
cs_adj
svh
− r2
cs_adj
max (
r2
cs_app)
=
r2
cs_adj (1 − s𝑉 𝐻 )
svh max (
r2
cs_app), (24)
where max(r2
cs_app) = 1 − exp (2 ln lnull
n
)
, as shown in equation (23).
therefore, to meet sample size criterion (ii) and ensure the difference is less than a small value (say, 𝛿), we require
r2
csadj
(1 − svh)
svh max (
r2
csapp) ≤ 𝛿. (25)
we generally recommend 𝛿 is ≤ 0.05, such that the optimism is nagelkerke's percentage of variation explained is ≤ 5%.
rearranging equation (25), we find that
(1 − svh)
svh
≤
𝛿 max (
r2
csapp)
r2
csadj
,
and therefore,
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp). (26)
equation (26) allows the researcher to calculate the required svh to satisfy criterion (ii), conditional on prespecifying the
model's anticipated r2
cs_adj (as they did for criterion (i)) and also the value of max(r2
csapp
) as outlined for equation (23).
then, sample size equation (11) can be used to derive the sample size needed to satisfy criterion (ii). this is only necessary
when the calculated value of svh from equation (26) is larger than that chosen for criterion (i), as then the sample size
required to meet criterion (ii) will be larger than that for criterion (i).
for example, consider the development of a logistic regression model with anticipated r2
cs_adj of at least 0.1, and in a
setting with the outcome proportion of 5%, such that the max(r2
cs_app) is 0.33. then, to ensure 𝛿 is ≤ 0.05, we require
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp) = 0.1
0.1 + (0.05 × 0.33) = 0.858.
therefore, svh must be at least 0.86 to meet criterion (ii). as this is lower than the recommended value of at least 0.90 to
meet criterion (i), no further work is required. however, had the anticipated r2
cs_adj been 0.2, then
svh ≥ 0.2
0.2 + (0.05 × 0.33) = 0.924.
as this is higher than 0.90, we would need to reapply sample size equation (11) using 0.924, rather than 0.90, to obtain a
sample size that meets both criteria (i) and (ii).
4.2 criterion (iii): ensure precise estimate of overall risk (model intercept)
for logistic and time-to-event models, it is fundamental that the available sample size can precisely estimate the overall
risk in the population by key time-points of interest. one way to examine this is to calculate the margin of error in outcome
proportion estimates (𝜙̂) for a null model (ie, no predictors included). for example, for a binary outcome, an approximate
95% confidence interval for the overall outcome proportion is
𝜙̂ ± 1.96√
𝜙̂(1 − 𝜙̂)
n .
therefore, the absolute margin of error (𝛿) is 1.96√𝜙̂(1−𝜙̂)
n , which leads to
n =
(1.96
𝛿
)2
𝜙̂(1 − 𝜙̂) . (27)
this is largest when the outcome proportion is 0.5. we require 96 individuals to ensure a margin of error ≤ 0.1 when the
true value is 0.5.15 however, we recommend a more stringent margin of error ≤ 0.05, which, when the outcome proportion
is 0.5, requires
n =
(1.96
0.05
)2
0.5(1 − 0.5) = 384.2,
and thus, 385 participants (and hence, about 193 events) are required. if the outcome proportion is 0.1, then we require
139 subjects to ensure a margin of error ≤ 0.05, whilst an outcome proportion of 0.2 requires 246 subjects.
these sample sizes aim to ensure precise estimation of the overall risk in the population of interest. strictly speaking,
we are more interested in precise estimation of the mean risk in an actual model including multiple predictors. if we
centre predictors at their mean value, then the model's intercept is the logit risk for an individual with mean predictor
values. the corresponding risk for this individual will often be very similar (though not identical) to the mean risk in
the overall population. furthermore, the variance of the estimated risk for this individual will be approximately 𝜙̂(1−𝜙̂)
n .*
*as obtained by inversing the information matrix x'v−1x and replacing individual variances defined by pi(1-pi) with a constant variance defined by
𝜙̂(1 − 𝜙̂).
thus, it follows that equation (27) is also a good approximation to the sample size required to precisely estimate the mean
risk in a model containing predictors centred at their mean.
for time-to-event data, we could consider the precision of the estimated cumulative incidence (outcome risk) at a key
time point of interest. a simple (and therefore practical) approach is to assume an exponential survival model, for which
the estimated cumulative incidence function is f(t) = 1 − exp(−𝜆̂ t), where 𝜆̂ is the estimated rate (number of events per
person-year). an approximate 95% confidence interval for the estimated f(t) is 1−exp (
−
(
𝜆̂ ± 1.96√𝜆̂
t
)
t
)
, where t is
the total person-years of follow-up. therefore, to ensure a small absolute margin of error, such that the lower and upper
bounds of the confidence interval are ≤ 𝛿 (eg, 0.05) of the true value, we must ensure both the following are satisfied:
− exp (
−
(
𝜆̂ + 1.96√
𝜆̂
t
)
t
)
+ exp(−𝜆̂ t) ≤ 𝛿
− exp(−𝜆̂ t) + exp (
−
(
𝜆̂ − 1.96√
𝜆̂
t
)
t
)
≤ 𝛿.
(28)
for example, for a constant event rate of 0.10 (10 events per 100 person-years), then by 10 years, the outcome risk is f(10)
= 1 − exp (−0.1 × 10) = 0.632. then, 2366 person-years of follow-up (and thus 0.1 × 2366 ≈ 237 events) are needed to
provide a confidence interval, which has a maximum absolute error of 0.05 from the true value. that is,
1 − exp (
−
(
𝜆̂ ± 1.96√
𝜆̂
t
)
t
)
= 1 − exp (
−
(
0.10 ± 1.96√ 0.10
2366)
10)
= 0.582 to 0.676.
thus, equation (28) is satisfied, as both the lower and upper bounds are ≤ 0.05 of the true value of 0.632. more generally, to
avoid assuming simple survival distributions like the exponential, harrell suggests using the dvoretzky-kiefer-wolfowitz
inequality to estimate the probability of a chosen margin of error anywhere in the estimated cumulative incidence
function.15,45
5 worked examples
to summarise our sample size approach for researchers, we provide a step-by-step guide in figure 1. the sample size (and
corresponding number of events and epp) that meets criteria (i) to (iii) provides the minimum sample size required for
model development. we now present two worked examples to illustrate our approach.
5.1 a diagnostic prediction model for chronic chagas disease
our first example considers the minimum sample size required for developing a diagnostic model for predicting a binary
outcome (disease: yes or no). brasil et al developed a logistic regression model containing 14 predictor parameters for
predicting the risk of having chronic chagas disease in patients with suspected chagas disease.46 upon external validation
in a cohort of 138 participants containing 24 with chagas disease, the model had an estimated c statistic of 0.91 and an
r2
nagelkerke_app of 0.48. consider that a researcher wants to update this model and improve the predictive performance. our
sample size approach can be applied as follows.
5.1.1 steps 1 and 2: identifying values for p, r𝟐
cs_adj, and max(r𝟐
cs_app)
assume that the researcher has identified (eg, based on recent studies) 10 additional predictor parameters that they wish
to add to the original model. thus, in total, the number of predictor parameters, p, is 24. the next step is to identify a
sensible value for the anticipated cox-snell r2
adj. to achieve this, we can convert the r2
nagelkerke_app value for brasil's existing
model into a r2
cs_app value. assume the disease prevalence is 17.4%, as in the brasil validation study, and use equation (12)
to calculate the log-likelihood for the null model in brasil's validation study
ln lnull = e ln (e
n
)
+ (n − e)ln (
1 − e
n
)
= 24 ln ( 24
138
)
+ (138 − 24)ln (
1 − 24
138
)
= −63.761.
figure 1 summary of the steps involved in calculating the minimum sample size required for developing a multivariable prediction
model for binary or time-to-event outcomes
hence, the max(r2
csapp
) = 1 − exp (2 ln lnull
n
)
= 1 − exp (2×−63.761
138 )
= 0.60. now, we can use equation (15) to obtain
r2
cs_app = r2
nagelkerke_app (
max (
r2
csapp)) = 0.48 × 0.60 = 0.288.
this apparent cox-snell value of 0.288 can be directly used as an estimate of the model's r2
cs_adj, as it was obtained in a
different data set to that used for model development. therefore no adjustment is needed, because r2
cs_app= r2
cs_adj here.
5.1.2 step 3: criterion (i) - ensuring a global shrinkage factor of 0.9
let us assume 0.288 is a lower bound for the r2
cs_adj of our new model. we now use equation (11) to estimate the sample
size required to ensure an expected shrinkage factor (svh = 0.90) conditional on a number of predictor parameters (p= 24)
n = p
(svh − 1)ln (
1 − r2
csadj
svh ) = 24
(0.90 − 1)ln (
1 − 0.288
0.90 ) = 622.31.
thus, 623 participants are required to meet criterion (i).
5.1.3 step 4: criterion (ii) - ensuring a small absolute difference in the apparent
and adjusted r𝟐
nagelkerke
to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of 0.05 or less
in the apparent and adjusted r2
nagelkerke. using equation (26), we obtain
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp) = 0.288
0.288 + (0.05 × 0.60) = 0.906.
this is more stringent than the 0.90 assumed for criterion (i). therefore, we need to reapply equation (11) to estimate the
sample size required conditional on svh = 0.906 (rather than 0.90)
n = p
(svh − 1)ln (
1 − r2
csadj
svh ) = 24
(0.906 − 1) ln (
1 − 0.288
0.906 ) = 667.41.
therefore, 668 subjects are required to meet criterion (ii), exceeding the 623 subjects required for criterion (i).
5.1.4 step 5: criterion (iii) - ensure precise estimate of overall risk (model intercept)
assuming the prevalence of chagas disease is 17.4% (as observed from the brasil validation study), then to ensure we
estimate this with a margin of error ≤ 0.05, we require (using equation (27))
n =
(1.96
0.05
)2
0.174 (1 − 0.174) = 220.85
and thus 221 subjects. this is far fewer than the sample size required to meet criteria (i) and (ii).
5.1.5 step 6: minimum sample size that ensures all criteria are met
the largest sample size required was 668 subjects to meet criterion (ii), and so this provides the minimum sample size
required for developing our new model. it corresponds to 668 × 0.174 = 116.2 events, and an epp of 116.2/24 = 4.84,
which is considerably lower than the “epp of at least 10” rule of thumb.
5.2 a prognostic model to predict a recurrence of vte
our second example considers the sample size required to develop a prognostic model with a time-to-event outcome.
ensor et al developed a prognostic time-to-event model for the risk of a recurrent vte following cessation of therapy for
a first vte.47 the sample size was 1200 participants, with a median follow-up of 22 months, a total of 2483 person-years
of follow-up, and 161 (13.42% of) individuals had a vte recurrence by end of follow-up.47 the model included predictors
of age, gender, site of first clot, d-dimer level, and the lag time from cessation of therapy until measurement of d-dimer
(often around 30 days). these predictors corresponded to six parameters in the model, which was developed using the
flexible parametric survival modelling framework of royston and parmar48 and royston and lambert.49 although ensor's
model performed well on average, the model's predicted risks did not calibrate well with the observed risks in some
populations.47 therefore, new research is needed to update and extend this model, eg, by including additional predictors.
we now identify suitable sample sizes to inform such research.
5.2.1 steps 1 and 2: identifying values for p, r𝟐
cs_adj and max(r𝟐
cs_app)
assume that there are 25 potential predictor parameters for inclusion in the new model, and thus, p = 25. we next need
to identify suitable values for 𝑅2
cs_adj and max(𝑅2
cs_app).
calculating max(r𝟐
cs_app)
for the ensor model, r2
cs_app was not reported but we should expect it to be quite small because the maximum value
of r2
cs_app is low. for example, assuming (for simplicity) an exponential survival model was fitted to the ensor data, then
using equation (13), we have
ln lnull = e ln (e
t
)
+ e = 161 ln(161∕2483) + 161 = −279.47,
and therefore, using equation (23),
max (
r2
cs_app)
= 1 − exp (2 ln lnull
n
)
= 1 − exp (−2 × 279.47
1200
)
= 0.37.
thus, max(r2
cs_app) is considerably less than 1.
obtaining a sensible value for r𝟐
cs_adj from the study authors
as r2
cs_app was not reported for the ensor model, we need to obtain it. we contacted the original authors who told us
their model's r2
cs_app was 0.056 in the development data set. thus, let us use this value to derive r2
adj from equation (8).
based on ensor's sample size of 1200, and six predictor parameters, we obtain
r2
cs_adj = svhr2
cs_app =
⎛
⎜
⎜
⎜
⎝
1 + p
n ln (
1 − r2
cs_app)
⎞
⎟
⎟
⎟
⎠
r2
cs_app =
(
1 +
6
1200 ln (1 − 0.056)
)
0.056 = 0.051.
hence, when developing a new model in this field, we could assume 0.051 is a lower bound for the expected r2
cs_adj of the
new model. this corresponds to nagelkerke's proportion variation explained of r2
cs_adj∕max(r2
cs_app) ≈ 0.051/0.37 = 0.14
(or 14%).
calculating a sensible value for r𝟐
cs_adj from other reported information
for illustration, we also consider how r2
cs_app could have been estimated indirectly from other available information.
the model's reported c statistic was 0.69, and so we can use equation (22) to predict the corresponding d statistic
d = 5.50 (c − 0.5) + 10.26(c − 0.5)
3 = 5.50 (0.69 − 0.5) + 10.26(0.69 − 0.5)
3 = 1.115.
the corresponding r2
d_app can be derived from equation (21)
r2
d_app =
𝜋
8
d2
𝜋2
6 + 𝜋
8
d2
=
𝜋
8
1.1152
𝜋2
6 + 𝜋
8
1.1152
= 0.229.
taking r2
d_app as a proxy for r2
royston_app, we can then use equation (20) to obtain
r2
óquigley_app =
−𝜋2
6
r2
royston_app
(
1 − 𝜋2
6
)
r2
royston_app − 1
= −𝜋2
6
0.229
(
1 − 𝜋2
6
)
0.229 − 1
= 0.328.
next, we can use r2
óquigley_app and the number of reported events (e = 161) to derive the lr statistic from equation (18)
lr = −e ln (
1 − r2
ó
quigleyapp)
= −161 ln (1 − 0.328) = 64.05.
using equation (6), this corresponds to
r2
cs_app = 1 − exp (−lr
n
)
= 1 − exp (−64.05
1200
)
= 0.052.
thus, based on using the reported c statistic, an indirect estimate of the r2
cs_app is 0.052 for the ensor model. this is
reassuringly close to the estimate of 0.056 provided directly by the study authors.
5.2.2 step 3: criterion (i) - ensuring a global shrinkage factor of 0.9
equation (11) can now be applied to derive the required sample size to meet criterion (i). using an r2
cs_adj of 0.051, for a
model with 25 predictor parameters and a targeted expected shrinkage of 0.9, the sample size required is
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ) = 25
(0.9 − 1) ln (
1 − 0.051
0.9
) = 4285.5
and thus 4286 participants.
5.2.3 step 4: criterion (ii) - ensuring a small absolute difference in the apparent
and adjusted r𝟐
nagelkerke
to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of 0.05 or less
in the apparent and adjusted r2
nagelkerke. recall, assuming an exponential model for simplicity, we calculated that the
max(r2
csapp
) = 0.37. then, using equation (26), we obtain
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp) = 0.051
0.051 + (0.05 × 0.37) = 0.73.
this is less stringent than the 0.90 assumed for criterion (i), and so no further sample size calculation is required to meet
criterion (ii).
5.2.4 step 5: criterion (iii) - ensure precise estimate of overall risk
assuming a simple exponential model, we can check the width of the confidence interval for the overall risk at a particular
time point based on the sample size identified, using the approach outlined in section 4.2. ensor et al47 reported an overall
vte recurrence rate of 161/2483 = 0.065, with an average follow-up of 2.07 years. therefore, assuming 𝜆 is 0.065 in our
new study, and that a predicted risk at 2 years is of key interest, an exponential survival model would give the cumulative
incidence of f(2)=1− exp (−0.065 × 2)=0.122. based on the calculated sample size of 4286 participants from criterion (i),
and thus an estimated 4286×2.07 = 8872 person-years of follow-up, the 95% confidence interval would be
1 − exp (
−
(
𝜆̂ ± 1.96√
𝜆̂
t
)
t
)
= 1 − exp (
−
(
0.065 ± 1.96√0.065
8872 )
2
)
= 0.113 to 0.131.
this is reassuringly narrow, and satisfies equation (28) as both the lower and upper bounds are well within an error of
0.05 of the true value of 0.122.
5.2.5 step 6: minimum sample size that ensures all criteria are met
the largest sample size required was 4286 participants to meet criterion (i), which therefore provides the minimum sample
size required for developing our new model. this assumes the new cohort will have a similar follow-up, censoring rate,
and event rate to that reported by ensor et al, where the mean follow-up per person was 2.07 years, 13.42% of individuals
had a vte recurrence by end of follow-up, and the event rate was 0.065.47
then, the required 4286 participants corresponds to about 4286 × 2.07 = 8872 person-years of follow-up, and
8872 × 0.065 ≈ 577 outcome events, and thus an epp of 577/25 ≈ 23. this is over twice the “epp of at least 10” rule of
thumb. figure 2 shows that an epp of 10 only ensures a shrinkage factor of 0.79, which would reflect relatively large
overfitting.
5.2.6 what if the sample size is not achievable?
if a researcher was restricted in their total sample size, for example, by the time and cost of a new cohort study, then a
sample size of 4286 may not be practical. in this situation, we do not recommend reducing sample size by decreasing
sc below 0.9 (as this would reflect larger overfitting) or by assuming a larger r2
cs_adj value (as this is anticonservative
for criterion (i)). rather, to ensure an svh of 0.9 (ie, an expected shrinkage of 10%), the researcher should lower p by
reducing the number of candidate predictors. for example, predictors could be prioritised based on previous evidence (eg,
systematic reviews). after data collection, unsupervised learning techniques such as principal component analysis may
be useful, which are blinded to the outcome data. figure 3 shows how changing p changes the required sample size to
meet criterion (i). for example, if a researcher was restricted to a sample size of about 2000 participants, then they would
need to reduce p to 12 to ensure an expected shrinkage of 0.90. this is because, for an svh of 0.9 and r2
cs_adj of 0.051, the
sample size required is
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ) = 12
(0.9 − 1) ln (
1 − 0.051
0.9
) = 2057
figure 2 events per predictor parameter required to achieve various expected shrinkage (svh) values for a new prediction model of
venous thromboembolism recurrence risk with an assumed r2
cs_adj of 0.051 [colour figure can be viewed at wileyonlinelibrary.com]
figure 3 sample size required (based on equation (11)) for a particular number of predictor parameters (p) to achieve a particular value
of expected shrinkage (svh), for a new prediction model of venous thromboembolism recurrence risk with an assumed r2
cs_adj of 0.051
[colour figure can be viewed at wileyonlinelibrary.com]
and so now close to 2000. figure 3 also shows how larger values of svh require larger sample sizes; in particular, the
increase in sample size required is substantial when moving from svh of 0.90 to 0.95. values of svh < 0.9 lead to lower
sample sizes, but come at the cost of larger expected overfitting, and so are not recommended. therefore, targeting a value
of svh of 0.9 would seem a pragmatic choice.
6 potential additional criterion: precise estimates of
predictor effects
ideally, predictions should also be precise across the entire spectrum of predicted values, not just at the mean. this is challenging to achieve, but is helped by ensuring the sample size will give precise estimates of the effects of key predictors;50
hence, this may form a further criterion for researchers to check (ie, in addition to criteria (i) to (iii)). briefly, for a particular predictor of a binary or time-to-event outcome, the sample size required to precisely estimate its association with
the outcome (ie, an odds ratio or hazard ratio) depends on the assumed magnitude of this effect, the variability of the
predictor's values across subjects, the predictor's correlation with other predictors in the model, and the overall outcome
proportion in the study.51-53 ideally, we want to ensure a sample size that gives a precise confidence interval around the
predictor's effect estimate.54 however, this is taxing, as closed-form solutions for the variance of adjusted log odds ratio
or hazard ratios, from logistic and cox regression, respectively, are nontrivial. one solution is to use simulation-based
evaluations.54,55 however, perhaps a more practical option is to utilise readily available power-based sample size calculations that calculate the sample size required to detect (based on statistical significance) a predictor's effect for a chosen
type i error level (eg, 0.05) and power.51-53,56 as such sample size calculations are likely to be less stringent than those
based on confidence interval width (especially for predictors with large effect sizes), we might use a high power, say of
95%, in the calculation.
checking sample size for predictor effects will be laborious with many predictors, and so it may be practical to focus
on the subset of key predictors with smallest variance of their values, as these predictors will have the least precision.
in particular, when there are important categorical predictors but with few subjects and/or outcome events in some categories, substantially larger sample sizes may be needed to avoid separation issues (ie, no event or nonevents in some
categories).57 in addition, any predictors whose effect is small (and thus harder to detect), but still important, may warrant
special attention.
for example, returning to the vte prediction model from section 5.2, a key predictor in the original model by ensor et al
was age,47 with an adjusted log hazard ratio of −0.0105. although this is close to zero, as age is on a continuous scale, the
impact of age on outcome risk is potentially large; for example, it corresponds to an adjusted hazard ratio of 0.66 comparing
two individuals aged 40 years apart. based on the results presented by ensor et al,47 the standard deviation of age was 15.21
and the overall outcome occurrence by end of follow-up was 13.5%. based on these values, and assuming other included
predictors explain 20% of the variation in age, then the sample size approach of hsieh and lavori52 suggests 4718 subjects
are required to have 95% power to detect a prognostic effect for age. this is larger than the 4286 subjects required to meet
criterion (i), and so, to be extra stringent beyond criteria (i) to (iii), the researcher might raise the recommended sample
size to 4718 subjects, if possible.
7 discussion
sample size calculations for prediction models of binary and time-to-event outcomes are typically based on blanket rules
of thumb, such as at least 10 epp, which generates much debate and criticism.14,16,57 in this article, building on our related
work for linear regression,10 we have proposed an alternative approach that identifies the sample size, events and epp
required to meet three key criteria, which minimise overfitting whilst ensuring precise estimates of overall outcome risk.
criterion (i) aims to ensure the optimism of predictor effect estimates is small, as defined by a global shrinkage factor of
≥ 0.9. this idea extends the work of harrell who suggests that, after a model is developed, if the shrinkage estimate “falls
below 0.9, for example, we may be concerned with the lack of calibration the model may experience on new data.”15 our
premise is the same, except we focused on calculating the expected shrinkage before data collection, to inform sample
size calculations for a new study. criterion (ii) extends this idea to ensure the optimism is small on the r2
nagelkerke scale,
such that there is a difference of ≤ 5% in the apparent and adjusted percentage of variation explained by the model. lastly,
criterion (iii) ensures the sample size will precisely estimate the overall outcome risk, which is fundamental.
by utilising the model's anticipated cox-snell r2, the sample size calculations are essentially tailored to the model and
setting at hand, because the cox-snell r2 reflects many factors including the outcome proportion (ie, outcome prevalence
or cumulative incidence) and the overall fit (performance) of the model. it therefore better reflects the trait of a particular
model and setting at hand rather than a blanket epp rule.16 in our examples, the sample sizes required often differed
considerably from an epp of 10, reinforcing the idea that this rule is too simplistic.57 indeed, the required epp was much
higher (23) in our second example than our first (4.8), illustrating the problem with a blanket epp rule trying to cover all
situations.14,16-18
section 3 also showed how to obtain a realistic value for cox-snell r2 based on previous models to make our proposal
more achievable in practice. if no previous prediction model exists for the outcome and setting of interest, then information might be used from studies in a related setting or using a different but similar outcome definition or time points to
those intended for the new model. information can also be borrowed from predictor finding studies (eg, studies aiming
to estimate the prognostic effect of a particular predictor adjusted for other predictors58). typically, these studies apply
multivariable modelling, and although mainly focused on predictor effect estimates, they often report the c statistic and
pseudo-r2 values.
further research is needed to help researchers when there are no existing studies or information to identify a sensible
value of the expected cox-snell r2. medical diagnosis and prediction of health-related outcomes are, generally speaking,
low signal-to-noise ratio situations. it is not uncommon in these situations to see r2
nagelkerke values in the 0.1 to 0.2 range.
therefore, in the absence of any other information, we suggest that sample sizes be derived assuming the value of r2
cs_adj
corresponds to an r2
nagelkerke of 0.15 (ie,
r2
csadj
max(r2
csadj) = 0.15). an exception is when predictors include “direct” (mechanistic)
measurements, such as including the baseline version of the binary or ordinal outcome (eg, including smoking status at
baseline when predicting smoking status at 1 year), or direct measures of the processes involved (eg, including physiologic
function of patients in intensive care when predicting risk of death within 48 hours). then, in this special situation,
an r2
nagelkerke = 0.5 may be a more appropriate default choice.
the rule of having an epp of at least 10 stems from limited simulation studies examining the bias and precision of
predictor effects in the prediction model.11-13 jinks et al41 alternatively developed sample size formulae for a time-to-event
prediction model based on the d statistic.40 they suggest to predefine the d statistic that would be expected, and then,
based on a desired significance or confidence interval width, their formulae provide the number of events required to
achieve this. however, their method does not account for the number of candidate predictors and does not consider the
potential for overfitting when developing a model. our sample size calculations address this, and are meant to be used
before any data collection. in situations where a development data set is already available, containing a specific number
of participants and predictors, our criteria could be used to identify whether a reduction in the number of predictors
is needed before starting model development. indeed, harrell already illustrated this concept by using the shrinkage
estimate from the full model (including all predictors) to gauge whether the number of predictors should be reduced via
data reduction techniques.15 ideally, this should be done blind to the estimated predictor effects (ie, just calculate the
shrinkage factor for the full model, but do not observe the predictor effect estimates and associated p-values), as otherwise
decisions about predictor inclusion are influenced by a “quick look” at the effect estimates from the full model results.
similarly, when planning to use a predictor selection method (such as backwards selection) during model development,
researchers should define p as the total number of parameters due to all predictors considered (screened), and not just
the subset that are included in the final model.59 as harrell notes,15 the value of p should be honest.
section 6 also highlighted the potential additional requirement to ensure precise estimates of key predictor effects.
in particular, special attention may be given to those predictors with strong predictive value (and thus most influential
to the predicted outcome risk), especially if the variance in their values is small, or when events or nonevents in some
categories of the predictor are rare, as this leads to larger sample sizes. for example, van smeden et al highlighted that
“separation” between events and nonevents is an important consideration toward the required sample size, which occurs
when a single predictor (or a linear combination of multiple predictors) perfectly separates all events from all nonevents,
and thus causes estimation difficulties.57 this may lead to substantially larger epp to resolve the issue (eg, so that all
categories of a predictor have both events and nonevents). for such reasons, we labelled our criteria (i) to (iii) proposal
as the “minimum” sample size required.
further research should identify how our sample size criteria relates to that of the work of van smeden et al, who focused
on sample size in regards to the mean squared error in predictions from the model.60 specifically, they use simulation
to evaluate the characteristics that influence the mean squared prediction error of a logistic model, and identify that the
outcome proportion and number of predictors are important,60 in addition to total sample size. this leads to a sample size
equation to minimise root mean-squared prediction error in a new model development study. harrell also suggested using
simulation to inform sample size, and illustrates this for a logistic regression model with a single predictor.15 for example,
one could simulate a very large dataset from an assumed prediction model, and quantify the mean square (prediction)
error and mean absolute (prediction) error of a model developed from this data set. then, repeat this process each time
removing an individual at random, until a sample size is identified below which the mean squared (prediction) error is
unacceptable.
in summary, we have proposed criteria for identifying the minimum sample size required when developing a prediction
model for binary or time-to-event outcomes. we hope this, and our related paper,10 encourages researchers to move away
from rules of thumb, and to rather focus on attaining sample sizes that minimise overfitting and ensure precise estimates
of overall risk within the model and setting of interest. we are currently writing software modules to implement the
approach.

<|EndOfText|>

bstract
objective
to review and appraise the validity and usefulness of
published and preprint reports of prediction models
for diagnosing coronavirus disease 2019 (covid-19)
in patients with suspected infection, for prognosis of
patients with covid-19, and for detecting people in
the general population at increased risk of becoming
infected with covid-19 or being admitted to hospital
with the disease.
design
living systematic review and critical appraisal by the
covid-precise (precise risk estimation to optimise
covid-19 care for infected or suspected patients in
diverse settings) group.
data sources
pubmed and embase through ovid, arxiv, medrxiv,
and biorxiv up to 5 may 2020.
study selection
studies that developed or validated a multivariable
covid-19 related prediction model.
data extraction
at least two authors independently extracted data
using the charms (critical appraisal and data
extraction for systematic reviews of prediction
modelling studies) checklist; risk of bias was
assessed using probast (prediction model risk of
bias assessment tool).
results
14 217 titles were screened, and 107 studies
describing 145 prediction models were included. the
review identified four models for identifying people at
risk in the general population; 91 diagnostic models for
detecting covid-19 (60 were based on medical imaging,
nine to diagnose disease severity); and 50 prognostic
models for predicting mortality risk, progression
to severe disease, intensive care unit admission,
ventilation, intubation, or length of hospital stay. the
most frequently reported predictors of diagnosis and
prognosis of covid-19 are age, body temperature,
lymphocyte count, and lung imaging features. flulike symptoms and neutrophil count are frequently
predictive in diagnostic models, while comorbidities,
sex, c reactive protein, and creatinine are frequent
prognostic factors. c index estimates ranged from 0.73
to 0.81 in prediction models for the general population,
from 0.65 to more than 0.99 in diagnostic models,
and from 0.68 to 0.99 in prognostic models. all
models were rated at high risk of bias, mostly because
of non-representative selection of control patients,
exclusion of patients who had not experienced the
event of interest by the end of the study, high risk of
model overfitting, and vague reporting. most reports
did not include any description of the study population
or intended use of the models, and calibration of the
model predictions was rarely assessed.
conclusion
prediction models for covid-19 are quickly entering
the academic literature to support medical decision
making at a time when they are urgently needed. this
review indicates that proposed models are poorly
reported, at high risk of bias, and their reported
for numbered affiliations see
end of the article
what is already known on this topic
the sharp recent increase in coronavirus disease 2019 (covid-19) incidence has
put a strain on healthcare systems worldwide; an urgent need exists for efficient
early detection of covid-19 in the general population, for diagnosis of covid-19 in
patients with suspected disease, and for prognosis of covid-19 in patients with
confirmed disease
viral nucleic acid testing and chest computed tomography imaging are standard
methods for diagnosing covid-19, but are time consuming
earlier reports suggest that elderly patients, patients with comorbidities (chronic
obstructive pulmonary disease, cardiovascular disease, hypertension), and
patients presenting with dyspnoea are vulnerable to more severe morbidity and
mortality after infection
what this study adds
four models identified patients at risk in the general population (using proxy
outcomes for covid-19)
ninety one diagnostic models were identified for detecting covid-19 (60
were based on medical images; nine were for severity classification); and 50
prognostic models for predicting, among others, mortality risk, progression to
severe disease
proposed models are poorly reported and at high risk of bias, raising concern
that their predictions could be unreliable when applied in daily practice
performance is probably optimistic. hence, we do not
recommend any of these reported prediction models
for use in current practice. immediate sharing of well
documented individual participant data from covid-19
studies and collaboration are urgently needed to
develop more rigorous prediction models, and
validate promising ones. the predictors identified in
included models should be considered as candidate
predictors for new models. methodological guidance
should be followed because unreliable predictions
could cause more harm than benefit in guiding
clinical decisions. finally, studies should adhere to
the tripod (transparent reporting of a multivariable
prediction model for individual prognosis or
diagnosis) reporting guideline.
systematic review registration
protocol https://osf.io/ehc47/, registration https://
osf.io/wy245.
readers’ note
this article is a living systematic review that will
be updated to reflect emerging evidence. updates
may occur for up to two years from the date of
original publication. this version is update 2 of
the original article published on 7 april 2020 (bmj
2020;369:m1328), and previous updates can be
found as data supplements (https://www.bmj.com/
content/369/bmj.m1328/related#datasupp).
introduction
the novel coronavirus disease 2019 (covid-19)
presents an important and urgent threat to global
health. since the outbreak in early december 2019 in
the hubei province of the people’s republic of china,
the number of patients confirmed to have the disease
has exceeded 8 963 350 in 188 countries, and the
number of people infected is probably much higher.
more than 468 330 people have died from covid-19
(up to 22 june 2020).1
 despite public health responses
aimed at containing the disease and delaying the
spread, several countries have been confronted with a
critical care crisis, and more countries could follow.2-4
outbreaks lead to important increases in the demand
for hospital beds and shortage of medical equipment,
while medical staff themselves could also get infected.
to mitigate the burden on the healthcare system,
while also providing the best possible care for patients,
efficient diagnosis and information on the prognosis of
the disease is needed. prediction models that combine
several variables or features to estimate the risk of people
being infected or experiencing a poor outcome from the
infection could assist medical staff in triaging patients
when allocating limited healthcare resources. models
ranging from rule based scoring systems to advanced
machine learning models (deep learning) have been
proposed and published in response to a call to share
relevant covid-19 research findings rapidly and openly
to inform the public health response and help save
lives.5
 many of these prediction models are published in
open access repositories, ahead of peer review.
we aimed to systematically review and critically
appraise all currently available prediction models for
covid-19, in particular models to predict the risk of
developing covid-19 or being admitted to hospital with
covid-19, models to predict the presence of covid-19
in patients with suspected infection, and models to
predict the prognosis or course of infection in patients
with covid-19. we included model development and
external validation studies. this living systematic
review, with periodic updates, is being conducted
by the covid-precise (precise risk estimation to
optimise covid-19 care for infected or suspected
patients in diverse settings) group in collaboration
with the cochrane prognosis methods group.
methods
we searched pubmed and embase through ovid,
biorxiv, medrxiv, and arxiv for research on covid-19
published after 3 january 2020. we used the publicly
available publication list of the covid-19 living
systematic review.6
 this list contains studies on
covid-19 published on pubmed and embase through
ovid, biorxiv, and medrxiv, and is continuously
updated. we validated whether the list is fit for
purpose (online supplementary material) and further
supplemented it with studies on covid-19 retrieved
from arxiv. the online supplementary material
presents the search strings. additionally, we contacted
authors for studies that were not publicly available
at the time of the search,7 8 and included studies that
were publicly available but not on the living systematic
review6
 list at the time of our search.9-12
we searched databases repeatedly up to 5 may 2020
(supplementary table 1). all studies were considered,
regardless of language or publication status (preprint
or peer reviewed articles; updates of preprints are
only included and reassessed after publication in a
peer reviewed journal). we included studies if they
developed or validated a multivariable model or
scoring system, based on individual participant level
data, to predict any covid-19 related outcome. these
models included three types of prediction models:
diagnostic models for predicting the presence or
severity of covid-19 in patients with suspected
infection; prognostic models for predicting the course
of infection in patients with covid-19; and prediction
models to identify people at increased risk of covid-19
in the general population. no restrictions were made
on the setting (eg, inpatients, outpatients, or general
population), prediction horizon (how far ahead the
model predicts), included predictors, or outcomes.
epidemiological studies that aimed to model disease
transmission or fatality rates, diagnostic test accuracy,
and predictor finding studies were excluded. starting
with the second update, retrieved records were initially
screened by a text analysis tool developed by artificial
intelligence to prioritise sensitivity (supplementary
material). titles, abstracts, and full texts were screened
for eligibility in duplicate by independent reviewers
(pairs from lw, bvc, mvs) using eppi-reviewer,13 and
discrepancies were resolved through discussion.
data extraction of included articles was done by
two independent reviewers (from lw, bvc, gsc, tpad,
mch, gh, kgmm, rdr, es, ljms, ews, kies, cw,
al, jm, tt, jaad, kl, jbr, lh, cs, ms, mch, ns, nk,
smjvk, jcs, pd, clan, rw, gpm, it, jyv, dld, jw, fsvr,
ph, vmtdj, and mvs). reviewers used a standardised
data extraction form based on the charms (critical
appraisal and data extraction for systematic reviews
of prediction modelling studies) checklist14 and
probast (prediction model risk of bias assessment
tool) for assessing the reported prediction models.15 we
sought to extract each model’s predictive performance
by using whatever measures were presented. these
measures included any summaries of discrimination
(the extent to which predicted risks discriminate
between participants with and without the outcome),
and calibration (the extent to which predicted risks
correspond to observed risks) as recommended in
the tripod (transparent reporting of a multivariable
prediction model for individual prognosis or diagnosis)
statement.16 discrimination is often quantified by
the c index (c index=1 if the model discriminates
perfectly; c index=0.5 if discrimination is no better
than chance). calibration is often quantified by the
calibration intercept (which is zero when the risks are
not systematically overestimated or underestimated)
and calibration slope (which is one if the predicted
risks are not too extreme or too moderate).17 we
focused on performance statistics as estimated from
the strongest available form of validation (in order
of strength: external (evaluation in an independent
database), internal (bootstrap validation, cross
validation, random training test splits, temporal
splits), apparent (evaluation by using exactly the
same data used for development)). any discrepancies
in data extraction were discussed between reviewers,
and remaining conflicts were resolved by lw and mvs.
the online supplementary material provides details
on data extraction. we considered aspects of prisma
(preferred reporting items for systematic reviews and
meta-analyses)18 and tripod16 in reporting our article.
patient and public involvement
it was not possible to involve patients or the public in
the design, conduct, or reporting of our research. the
study protocol and preliminary results are publicly
available on https://osf.io/ehc47/ and medrxiv.
results
we retrieved 14209 titles through our systematic
search (of which 9306 were included in the present
update; supplementary table 1, fig 1). two additional
unpublished studies were made available on request
(after a call on social media). we included a further
six studies that were publicly available but were not
detected by our search. of 14217 titles, 275 studies were
retained for abstract and full text screening (of which
76 in the present update). one hundred seven studies
describing 145 prediction models met the inclusion
criteria (of which 56 papers and 79 models added in
the present update, supplementary table 1).7-12 19-119
these studies were selected for data extraction and
critical appraisal (table 1, table 2, table 3, and table 4).
primary datasets
forty five studies used data on patients with covid-19
from china (supplementary table 2), six from
italy,32 39 72 74 76 79 three from brazil,69 81 109 three from
france,71 77 110 three from the united states,96 108 112 two
from south korea,6380 one from belgium,82 one from
the netherlands,95 one from the united kingdom,75
one from israel,67 one from mexico,70 and one from
singapore.40 twenty two studies used international data
(supplementary table 2) and two studies used simulated
data.35 41 three studies used proxy data to estimate
covid-19 related risks (eg, medicare claims data from
2015 to 2016).8 90 113 twelve studies were not clear on
the origin of covid-19 data (supplementary table 2).
based on 59 studies that reported study dates,
data were collected between 8 december 2019 and
21 april 2020. four studies reported median followup time (4.5, 8.4, 15, and 18 days),20 37 83 108 while
another study reported a follow-up of at least five
days.42 some centres provided data to multiple studies
and several studies used open github120 or kaggle121
data repositories (version or date of access often
unspecified), and so it was unclear how much these
datasets overlapped across our identified studies
(supplementary table 2). one study25 developed
prediction models for use in paediatric patients. the
median age in studies on adults varied from 34 to 68
years, and the proportion of men varied from 35% to
75%, although this information was often not reported
at all (supplementary table 2).
among the studies that developed prognostic models
to predict mortality risk in people with confirmed or
suspected infection, the percentage of deaths varied
between 1% and 59% (table 3). this wide variation is
partly because of substantial sampling bias caused by
studies excluding participants who still had the disease
at the end of the study period (that is, they had neither
recovered nor died).7 21-23 44 96 98 100 additionally, length
of follow-up could have varied between studies (but was
rarely reported), and there might be local and temporal
variation in how people were diagnosed as having
covid-19 or were admitted to the hospital (and therefore
recruited for the studies). among the diagnostic model
studies, only nine reported on the prevalence of
covid-19 and used a cross sectional or cohort design;
the prevalence varied between 17% and 79% (table 2).
because 58 diagnostic studies used either case-control
sampling or an unclear method of data collection, the
prevalence in these diagnostic studies might not have
been representative of their target population.
table 1, table 2, and table 3 give an overview of the
145 prediction models reported in the 107 identified
studies. supplementary table 2 provides modelling
details and box 1 discusses the availability of models
in a format for use in clinical practice.
models to predict risks of covid-19 in the general
population
we identified four models that predicted risk of
covid-19 in the general population. three models
from one study used hospital admission for non-
tuberculosis pneumonia, influenza, acute bronchitis,
or upper respiratory tract infections as proxy outcomes
in a dataset without any patients with covid-19.8
among the predictors were age, sex, previous hospital
admissions, comorbidity data, and social determinants
of health. the study reported c indices of 0.73,
0.81, and 0.81. a fourth model used deep learning
on thermal videos from the faces of people wearing
facemasks to determine abnormal breathing (not covid
related) with a reported sensitivity of 80%.90
diagnostic models to detect covid-19 in patients
with suspected infection
we identified 22 multivariable models to diagnose
covid-19. most models targeted patients with
suspected covid-19. reported c index values ranged
between 0.65 and 0.99. a few models also evaluated
calibration and reported good results.69 78 117 the most
frequently used diagnostic predictors (at least 10
times) were flu-like signs and symptoms (eg, shiver,
fatigue), imaging features (eg, pneumonia signs on
computed tomography scan), age, body temperature,
lymphocyte count, and neutrophil count (table 2).
nine studies aimed to diagnose severe disease in
patients with covid-19: eight in adults with covid-19
with reported c indices between value of 0.80 and 0.99,
and one in paediatric patients with reported perfect
performance.25 predictors of severe covid-19 used more
than once were comorbidities, liver enzymes, c reactive
protein, imaging features, and neutrophil count.
sixty prediction models were proposed to support
the diagnosis of covid-19 or covid-19 pneumonia (and
some also to monitor progression) based on images.
most studies used computed tomography images
or chest radiographs. others used spectrograms of
cough sounds53 and lung ultrasound.73 the predictive
performance varied widely, with estimated c index
values ranging from 0.81 to more than 0.99.
prognostic models for patients with diagnosis of
covid-19
we identified 50 prognostic models (table 3) for patients
with a diagnosis of covid-19. the intended use of these
models (that is, when to use them, and for whom) was
often not clearly described. prediction horizons varied
between one and 30 days, but were often unspecified.
of these models, 23 estimated mortality risk and eight
aimed to predict progression to a severe or critical state
(table 3). the remaining studies used other outcomes
(single or as part of a composite) including recovery,
length of hospital stay, intensive care unit admission,
intubation, (duration of) mechanical ventilation,
and acute respiratory distress syndrome. one study
used data from 2015 to 2019 to predict mortality and
prolonged assisted mechanical ventilation (as a noncovid-19 proxy outcome).113
additional records identified through other sources
articles excluded
not a prediction model development or validation study
epidemiological model to estimate disease transmission or case fatality rate
commentary, editorial or letter
methods paper
duplicate article
77
27
18
33
13
records screened
records identified through database searching
records excluded
articles assessed for eligibility
studies included in review (with 145 models)
168
107
13 942
14 217
14 209 8
diagnostic models
(including 9 severity models
and 60 imaging studies)
prognostic models
(including 23 for mortality,
8 for progression to
severe or critical state)
models to identify subjects
at risk in general population
275
4 91 50
fig 1 | prisma (preferred reporting items for systematic reviews and meta-analyses) flowchart of study inclusions and
exclusions
the most frequently used prognostic factors (for
any outcome, included at least 10 times) included
comorbidities, age, sex, lymphocyte count, c reactive
protein, body temperature, creatinine, and imaging
features (table 3).
studies that predicted mortality reported c indices
between 0.68 and 0.98. some studies also evaluated
calibration.7 67 116 when applied to new patients, the
model by xie et al yielded probabilities of mortality
that were too high for low risk patients and too low
for high risk patients (calibration slope >1), despite
excellent discrimination.7
 the mortality model by
zhang et al also showed miscalibrated (overfitted and
underestimated) risks at external validation,116 while
the model by barda et al showed underfitting.67
the studies that developed models to predict
progression to a severe or critical state reported c
indices between 0.73 and 0.99. three of these studies
also reported good calibration, but this was evaluated
internally (eg, bootstrapped)88 or in an unclear way.83 119
reported c indices for other outcomes varied
between 0.72 and 0.96. singh et al and zhang et al
also evaluated calibration externally (in new patients).
singh showed that the epic deterioration index
overestimated the risk or a poor outcome, while the
poor outcome model by zhang et al underestimated
the risk of a poort outcome.108 116
risk of bias
all studies were at high risk of bias according to
assessment with probast (table 1, table 2, and table
3), which suggests that their predictive performance
when used in practice is probably lower than that
reported. therefore, we have cause for concern that
the predictions of the proposed models are unreliable
when used in other people. box 2 gives details on
common causes for risk of bias for each type of model.
fifty three of the 107 studies had a high risk of bias
for the participants domain (table 4), which indicates
that the participants enrolled in the studies might not
be representative of the models’ targeted populations.
unclear reporting on the inclusion of participants
prohibited a risk of bias assessment in 26 studies.
fifteen of the 107 studies had a high risk of bias for
the predictor domain, which indicates that predictors
were not available at the models’ intended time of
use, not clearly defined, or influenced by the outcome
measurement. one diagnostic imaging study used a
simple scoring rule and was scored at low predictor
risk of bias. the diagnostic model studies that used
medical images as predictors in artificial intelligence
were all scored as unclear on the predictor domain.
the publications often lacked clear information on the
preprocessing steps (eg, cropping of images). moreover,
complex machine learning algorithms transform
images into predictors in a complex way, which makes
it challenging to fully apply the probast predictors
section for such imaging studies. most studies used
outcomes that are easy to assess (eg, death, presence
of covid-19 by laboratory confirmation). nonetheless,
there was cause for concern about bias induced by the
outcome measurement in 19 studies, for example due
to the use of subjective or proxy outcomes (eg, non
covid-19 severe respiratory infections).
all but one of these studies50 were at high risk
of bias for the analysis domain (table 4). many
table 1 | overview of prediction models for use in the general population
study; setting; and outcome predictors in final model
sample size: total
no of participants for
model development
set (no with outcome)
predictive performance on validation
overall risk
of bias using
probast
type of
validation*
sample size: total
no of participants for
model validation (no
with outcome)
performance* (c index,
sensitivity (%), specificity
(%), ppv/npv (%),
calibration slope, other
(95% ci, if reported))
general population
original review
decaprio et al8
; data from us
general population; hospital
admission for covid-19
pneumonia (proxy events)†
age, sex, number of previous
hospital admissions,
11 diagnostic features,
interactions between age and
diagnostic features
1.5 million (unknown) training test split 369 865 (unknown) c index 0.73 high
decaprio et al8
; data from us
general population; hospital
admission for covid-19
pneumonia (proxy events)†
age and ≥500 features
related to diagnosis history
1.5 million (unknown) training test split 369 865 (unknown) c index 0.81 high
decaprio et al8
; data from us
general population; hospital
admission for covid-19
pneumonia (proxy events)†
≥500 undisclosed features,
including age, diagnostic
history, social determinants
of health, charlson
comorbidity index
1.5 million (unknown) training test split 369 865 (unknown) c index 0.81 high
update 2
jiang et al90; data from
china, respiratory patients
versus healthy volunteers;
detection of respiratory
diseases such as covid-19 infrared/thermal video of face unknown training test split not applicable sensitivity 80, ppv 90 high
npv=negative predictive value; ppv=positive predictive value; probast=prediction model risk of bias assessment tool.
*performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is
reported. apparent performance is the performance observed in the development data.
†proxy events used: pneumonia (except from tuberculosis), influenza, acute bronchitis, or other specified upper respiratory tract infections (no patients with covid-19 pneumonia in data).
setting; and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported)) diagnosis original review feng et al10; data from china, patients presenting at fever clinic; suspected covid-19 pneumonia age, temperature, heart rate, diastolic blood pressure, systolic blood pressure, basophil count, platelet count, mean corpuscular haemoglobin content, eosinophil count, monocyte count, fever, shiver, shortness of breath, headache, fatigue, sore throat, fever classification, interleukin 6 132 (26) temporal validation 32 (unclear) c index 0.94 high lopez-rincon et al35; data from international genome sequencing data repository, target population unclear; covid-19 diagnosis specific sequences of base pairs 553 (66) 10-fold cross validation not applicable c index 0.98, sensitivity100, specificity 99 high meng et al12; data from china, asymptomatic patients with suspected covid-19; covid-19 diagnosis age, activated partial thromboplastin time, red blood cell distribution width sd, uric acid, triglyceride, serum potassium, albumin/globulin, 3-hydroxybutyrate, serum calcium 620 (302) external validation 145 (80) c index 0.87‡ high song et al31; data from china, inpatients with suspected covid-19; covid-19 diagnosis fever, history of close contact, signs of pneumonia on ct, neutrophil to lymphocyte ratio, highest body temperature, sex, age, meaningful respiratory syndromes 304 (73) training test split 95 (18) c index 0.97 (0.93 to 1.00) high update 1 martin et al41; simulated patients with suspected covid-19; covid-19 diagnosis unknown not applicable external validation only (simulation) not applicable sensitivity 97, specificity 96 high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis age, sex, temperature, heart rate, systolic blood pressure, diastolic blood pressure, sore throat 292 (49) leave-one-out cross validation not applicable c index 0.65 (0.57 to 0.73) high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis sex, temperature, heart rate, respiration rate, diastolic blood pressure, sore throat, sputum production, shortness of breath, gastrointestinal symptoms, lymphocytes, neutrophils, eosinophils, creatinine 292 (49) leave-one-out cross validation not applicable c index 0.88 (0.83 to 0.93) high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis sex, temperature, heart rate, respiration rate, diastolic blood pressure, sputum production, gastrointestinal symptoms, chest radiograph or ct scan suggestive of pneumonia, neutrophils, eosinophils, creatinine 292 (49) leave-one-out cross validation not applicable c index 0.88 (0.83 to 0.93) high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis sex, covid-19 case contact, travel to wuhan, travel to china, temperature, heart rate, respiration rate, diastolic blood pressure, sore throat, sputum production, gastrointestinal symptoms, chest radiograph or ct scan suggestive of pneumonia, neutrophils, eosinophils, creatinine, sodium 292 (49) leave-one-out cross validation not applicable c index 0.91 (0.86 to 0.96) high wang et al43; data from china, patients with suspected covid-19; covid-19 pneumonia epidemiological history, wedge shaped or fan shaped lesion parallel to or near the pleura, bilateral lower lobes, ground glass opacities, crazy paving pattern, white blood cell count 178 (69) external validation 116 (68) c index 0.85, calibration slope 0.56 high
wu et al45; data from china, inpatients
with suspected covid-19; covid-19
diagnosis
lactate dehydrogenase, calcium, creatinine, total protein,
total bilirubin, basophil, platelet distribution width, kalium,
magnesium, creatinine kinase isoenzyme, glucose
108 (12) training test split 107 (61) c index 0.99, sensitivity 100,
specificity 94
high
update 2
batista et al69; data from brazil,
inpatients with suspected covid-19
admitted to the emergency care
department; covid-19 diagnosis
age, sex, haemoglobin, platelets, red blood cells, mean
corpuscular haemoglobin concentration, mean corpuscular
haemoglobin, red cell distribution width , mean corpuscular
volume, leukocytes, lymphocytes, monocytes, basophils,
eosinophils and c reactive protein
234 (102) training test split 31 (unknown) c index 0.85, sensitivity 68,
specificity 85
high
table 2 | overview of prediction models for diagnosis of covid-19
(continued) and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported)) brinati et al74; data from italy, inpatients with suspected covid-19; covid-19 diagnosis age, aspartate aminotransferase, lymphocytes, lactodehydrogenase, pcr, wbc count, eosinophils, alanine transaminase, neutrophils, gamma-glutamyltransferase, monocytes, basophils, alkaline phosphatase, platelets 279 (102) training test split 56 (20) c index 0.84, sensitivity 92, specificity 65 high brinati et al74; data from italy, inpatients with suspected covid-19; covid-19 diagnosis age, aspartate aminotransferase, lymphocytes, lactodehydrogenase, pcr, wbc count, eosinophils, alanine transaminase, neutrophils, gamma-glutamyltransferase, monocytes, basophils, alkaline phosphatase, platelets 279 (102) training test split 56 (20) sensitivity 95, specificity 75, ppv 86 high
chen et al78; data from china, inpatients
with suspected covid-19; covid-19
diagnosis
total number of mixed ggo in peripheral area, tree-in-bud,
offending vessel augmentation in lesions, respiration, heart
ratio, temperature, wbc count, cough, fatigue, lymphocyte count
98 (51) training test split 38 (19) c index 0.94 (0.87 to 1.00),
sensitivity 74, specificity 79
high
diaz-quijano et al81; data from brazil,
inpatients with suspected covid-19;
covid-19 diagnosis
age, days after reporting first confirmed case in federal unit,
fever, cough, sore throat, diarrhoea, coryza, chills, pulmonary
manifestation, other signs, hiv, kidney diseaes, trip outside
brazil up to 14 days before onset
1243 (541) external validation
(new centres, brazil)
4192 (785) c index 0.73 (0.71 to 0.75),
sensitivity 46, specificity 80
high
kurstjens et al95; data from the
netherlands, inpatients with suspected
covid-19; covid-19 diagnosis
age, sex, crp, ld, ferritin, absolute neutrophil count, absolute
lymphocyte count, chest radiograph
375 (276) external (unclear) 592 (393) c index 0.91 (0,89 to 0,94) high
mei et al101; data from china; inpatients
with suspected covid-19; covid-19
diagnosis
age, sex, ct imaging, exposure history, symptoms (present or
absent of fever, cough and/or sputum), wbc counts, neutrophil
count, percentage neutrophils, lymphocyte counts, percentage
lymphocytes
534 (242) training test split 279 (134) c index 0.92 (0.89 to 0.95),
sensitivity 84 (77 to 90),
specificity 83 (76 to 89),
ppv 81.9 (76 to 87),
npv 85 (79 to 90)
high
menni et al102; data from uk and usa,
suspected covid-19; covid-19 diagnosis
age, sex, loss of smell and taste, severe or significant persistent
cough, severe fatigue, skipped meals
12 510 (5162) external validation
(new centres, usa)
2763 (726) c index 0.76 (0.74 to 0.78),
sensitivity 66 (62 to 69),
specificity 83 (82 to 85),
ppv 58 (55 to 62),
npv 87 (86 to 89)
high
soares et al109; data from brazil;
patients with suspected infection
presenting at triage centre; covid-19
diagnosis
age, red blood cells, mean corpuscular volume, mean
corpuscular haemoglobin concentration, mean corpuscular
haemoglobin, red blood cell distribution width, leukocytes,
basophils, monocytes, lymphocytes, platelets, mean platelet
volume, creatinine, potassium, sodium, crp
599 (81) repeated 10-fold
cross validation
not applicable c index 0.87 (0.86 to 0.88),
sensitivity 70 (67 to 73),
specificity 86 (85 to 87),
npv 95 (94 to 95),
ppv 45 (43 to 47)
high
tordjman et al110; data from france;
suspected patients; covid-19 diagnosis
eosinophils, lymphocytes, neutrophils, basophils 100 (50) external validation
(new centres,
france)
300 (208) c index 0.89 (0.85 to 0.93),
sensitivity 80, specificity 85,
ppv 92
high
zhao et al117; data from china;
inpatients with suspected covid-19;
covid-19 diagnosis
fever, chest ct, crp, pct, wbc 547 (unknown) training test split 275 (unknown) c index 0.97 (0.96 to 0.97) high
diagnostic severity classification
original review
yu et al25; data from china, paediatric
inpatients with confirmed covid-19;
severe disease (yes/no) defined based
on clinical symptoms
direct bilirubin, alanine transaminase 105 (8) apparent
performance only
not applicable f1 score 1.00 high
update 1
zhou et al46; data from china, inpatients
with confirmed covid-19; severe
pneumonia
age, sex, onset-admission time, high blood pressure, diabetes,
chd, copd, white blood cell counts, lymphocyte, neutrophils,
alanine transaminase, aspartate aminotransferase, serum
albumin, serum creatinine, blood urea nitrogen, crp
250 (79) training test split 127 (38) c index 0.88 (0.94 to 0.92),
sensitivity 89, specificity 74
high
table 2 | continued
and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported)) update 2 benchoufi et al71; data from france, inpatients with suspected or confirmed covid-19; lung injury severity (pathologic vs normal) lung ultrasound scores for 8 quadrants in a global score 90 (unknown) internal validation by resampling (bootstrap) not applicable c index 0.93, sensitivity 95, specificity 83 high
chassagnon et al77; data from france,
inpatients with confirmed covid-19;
severe covid-19
unclear 50 (unknown) external validation
(new centres,
france)
130 (unknown) c index 0.80, sensitivity 69,
specificity 79
high
li et al97; data from china, target
population unclear; severe covid-19
portion of infection, average infection hounsfield unit, a
measure of radio density
196 (32) apparent
performance only
not applicable c index 0.97 (0.94 to 0.98),
sensitivity 94 (87 to 98),
specificity 88 (85 to 91)
high
lyu et al99; data from china, target
population unclear; severe/critical
covid-19 pneumonia
unclear 51 (39) apparent
performance only
not applicable c index 0.99 (0.88 to 1.00),
sensitivity 90, specificity 100
high
lyu et al99; data from china , target
population unclear; critical covid-19
pneumonia
unclear 39 (24) apparent
performance only
not applicable c index 0.92 (0.73 to 0.99),
sensitivity 92, specificity 87
high
wang et al114; data from china,
inpatients with confirmed covid-19;
severe covid-19
neutrophil-to-lymphocyte ratio, red cell volume distribution
width
45 (10) apparent
performance only
not applicable c index 0.94 (0.90 to 0.97),
sensitivity 90, specificity 85,
ppv 52, npv 96
high
zhu et al118; data from china, inpatients
with confirmed covid-19; severe
covid-19
peripheral blood cytokine il-6, crp, hypertension 127 (16) apparent
performance only
not applicable c index 0.90 (0.83 to 0.97),
sensitivity 100 (79 to 100),
specificity 66 (56 to 75)
high
diagnostic imaging
original review
barstugan et al32; data from italy,
patients with suspected covid-19;
covid-19 diagnosis
not applicable 53 (not applicable) cross validation not applicable sensitivity 93, specificity 100 high
chen et al27; data from china, people
with suspected covid-19 pneumonia;
covid-19 pneumonia
not applicable 106 (51) training test split 27 (11) sensitivity 100, specificity 82 high
gozes et al26; data from china and
us,§ patients with suspected covid-19;
covid-19 diagnosis
not applicable 50 (unknown) external validation
with chinese cases
and us controls
unclear c index 0.996 (0.989 to 1.000) high
jin et al11; data from china, us, and
switzerland,¶ patients with suspected
covid-19; covid-19 diagnosis
not applicable 416 (196) training test split 1255 (183) c index 0.98, sensitivity 94,
specificity 95
high
jin et al33; data from china, patients
with suspected covid-19; covid-19
pneumonia
not applicable 1136 (723) training test split 282 (154) c index: 0.99, sensitivity 97,
specificity 92
high
li et al34; data from china, patients with
suspected covid-19; covid-19 diagnosis
not applicable 2969 (400) training test split 353 (68) c index 0.96 (0.94 to 0.99),
sensitivity 90 (83 to 94),
specificity 96 (93 to 98)
high
shan et al29; data from china, people
with confirmed covid-19; segmentation
and quantification of infection regions in
lung from chest ct scans
not applicable 249 (not applicable) training test split 300 (not applicable) dice similarity coefficient
91.6%**
high
shi et al36; data from china, target
population unclear; covid-19
pneumonia
five categories of location features from imaging: volume,
number, histogram, surface, radiomics
2685 (1658) fivefold cross
validation
not applicable c index 0.94 high
table 2 | continued
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported))
wang et al30; data from china, target
population unclear; covid-19 diagnosis
not applicable 259 (79) internal, other
images from same
people
not applicable c index 0.81 (0.71 to 0.84),
sensitivity 83, specificity 67
high
xu et al28; data from china, target
population unclear; covid-19 diagnosis
not applicable 509 (110) training test split 90 (30) sensitivity 87, ppv 81 high
song et al24; data from china, target
population unclear; diagnosis of
covid-19 v healthy controls
not applicable 123 (61) training test split 51 (27) c index 0.99 high
song et al24; data from china, target
population unclear; diagnosis of
covid-19 v bacterial pneumonia
not applicable 131 (61) training test split 57 (27) c index 0.96 high
zheng et al38; data from china, target
population unclear; covid-19 diagnosis
not applicable unknown temporal validation unknown c index 0.96 high
update 1
abbas et al47; data from repositories
(origin unspecified), target population
unclear; covid-19 diagnosis
not applicable 137 (unknown) training test split 59 (unknown) c index 0.94, sensitivity 98,
specificity 92
high
apostolopoulos et al48; data from
repositories (us, italy); patients with
suspected covid-19; covid-19 diagnosis
not applicable 1427 (224) tenfold cross
validation
not applicable sensitivity 99, specificity 97 high
bukhari et al49; data from canada and
us; patients with suspected covid-19;
covid-19 diagnosis
not applicable 223 (unknown) training test split 61 (17) sensitivity 98, ppv 91 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; percentage lung
opacity
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.98 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; percentage high
lung opacity
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.98 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; severity score
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.97 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; lung opacity score
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.97 high
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal”
not applicable unknown fivefold cross
validation
not applicable c index 0.99 high
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal” and viral
pneumonia
not applicable unknown fivefold cross
validation
not applicable c index 0.98 high
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal”
not applicable unknown fivefold cross
validation
not applicable c index 0.998 high
table 2 | continued
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported))
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal” and viral
pneumonia
not applicable unknown fivefold cross
validation
not applicable c index 0.99 high
fu et al51; data from china, target
population unclear; covid-19 diagnosis
not applicable 610 (100) external validation 309 (50) c index 0.99, sensitivity 97,
specificity 99
high
gozes et al52; data from china, people
with suspected covid-19; covid-19
diagnosis
not applicable 50 (unknown) external validation 199 (109) c index 0.95 (0.91 to 0.99) high
imran et al53; data from unspecified
source, target population unclear;
covid-19 diagnosis
not applicable 357 (48) twofold cross
validation
not applicable sensitivity 90, specificity 81 high
li et al54; data from china, inpatients
with confirmed covid-19; severe and
critical covid-19
severity score based on ct scans not applicable external validation
of existing score
78 (not applicable) c index 0.92 (0.84 to 0.99) high
li et al55; data from unknown origin,
patients with suspected covid-19;
covid-19
not applicable 360 (120) training test split 135 (45) c index 0.97 high
hassanien et al56; data from repositories
(origin unspecified), people with
suspected covid-19; covid-19 diagnosis
not applicable unknown training test split unknown sensitivity 95, specificity 100 high
tang et al57; data from china, patients
with confirmed covid-19; covid-19
severe v non-severe
not applicable 176 (55) threefold cross
validation
not applicable c index 0.91, sensitivity 93,
specificity 75
high
wang et al42; data from china, inpatients
with suspected covid-19; covid-19
not applicable 709 (560) external validation
in other centres
508 (223) c index (average) 0.87 high
zhang et al58; data from repositories
(origin unspecified), people with
suspected covid-19; covid-19
not applicable 1078 (70) twofold cross
validation
not applicable c index 0.95, sensitivity 96,
specificity 71
high
zhou et al59; data from china, patients
with suspected covid-19; covid-19
diagnosis
not applicable 191 (35) external validation
in other centres
107 (57) c index 0.92, sensitivity 83,
specificity 86
high
update 2
angelov et al64; data from unknown
origin; covid-19 diagnosis
not applicable unknown apparent
performance only
not applicable c index 0.89, sensitivity 89,
ppv 90
high
arpan et al65; data from repositories
(multiple countries); covid-19 diagnosis
not applicable 3516 (80) training test split 424 (19) c index >0.99, sensitivity 100,
ppv 94
high
bai et al66; data from china and us,
target population unclear; covid-19
diagnosis
not applicable 830 (377) training test split 119 (42) c index 0.95, sensitivity 95 (83
to 100) , specificity 96 (90 to
99)
high
bassi et al68; data from italy, target
population unclear, covid-19 diagnosis
not applicable unknown training test split unknown sensitivity 98, ppv 98 high
borghesi et al72; data from italy,
target population unclear, ; severity of
covid-19 pneumonia
sum score for lung abnormalities based on chest radiograph not applicable external validation
only
100 (unknown) agreement, kappa 0.82 (0.79
to 0.86)
high
born et al73; data from repositories
(origin unspecified), target population
unclear, covid-19 diagnosis
not applicable 64 (37) fivefold cross
validation
not applicable c index 0.94 (0.82 to 1.00),
sensitivity 96, specificity 79
high
table 2 | continued
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported))
castiglioni et al76; data from italy,
inpatients suspected of covid-19;
covid-19 diagnosis
not applicable 500 (250) temporal validation 110 (36) c index 0.81 (0.73 to 0.87),
sensitivity 80 (72 to 86),
specificity 81 (73 to 87), ppv 89
(82 to 94), npv 66 (57 to 75)
high
guiot et al82; data from belgium,
inpatients suspected of covid-19;
covid-19 diagnosis
30 radiomics features 727 (unknown) training test split 165 (unknown) c index 0.94 (0.88 to 1.00) ,
sensitivity 79, specificity 91, ppv
54, npv 97
high
hu et al86; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 629 (313) training test split 201 (104) c index 92 (84 to 100),
sensitivity 86, specificity 85
high
islam et al87; data from unknown origin,
inpatients suspected of covid-19;
covid-19 diagnosis
not applicable 16130 (98) unknown origin 210 (10) sensitivity 80 high
kana et al91; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 5092 (161) external validation,
different repository
(unknown origin)
600 (200) sensitivity 100, specificity 100 high
karim et al92; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable unknown unknown unknown severe inconsistencies in
reported performance: data not
extracted
high
khan et al93; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 1300 (284) training test split 221 (30) sensitivity 100, ppv 97 high
kumar et al94; data from usa, china and
italy, target population unclear; covid-19
diagnosis; covid-19 diagnosis
not applicable unknown apparent
performance only
not applicable c index 0.997, sensitivity 100,
specificity 100
high
kumar et al94; data from usa, china and
italy, target population unclear; covid-19
diagnosis; covid-19 diagnosis
not applicable unknown apparent
performance only
not applicable c index 0.998, sensitivity 100,
specificity 100
high
moutounet-cartan103; data from
repositories, target population unclear;
covid-19 pneumonia
not applicable 325 (125) training test split 98 (unknown) sensitivity 88 high
ozturk et al104; data from repositories,
target population unclear; covid-19
pneumonia
not applicable 1127 (127) fivefold cross
validation
not applicable sensitivity 85, specificity 92
ppv 90
high
rahimzadeh et al105; data from
repositories, target population unclear;
covid-19 pneumonia
not applicable 633 (149) fivefold cross
validation
not applicable sensitivity 81, specificity 100,
ppv 35
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 320 (160) training test split 80 (40) sensitivity 100, specificity 98,
ppv 96¶¶
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 320 (160) training test split 80 (40) sensitivity 100, specificity 98,
ppv 96¶¶
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 320 (160) training test split 80 (40) sensitivity 100, specificity 98,
ppv 96¶¶
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 480 (160) training test split 120 (40) sensitivity 98, specificity 99.
ppv 96
high
table 2 | continued
research
studies had small sample sizes (table 1, table 2,
table 3), which led to an increased risk of overfitting,
particularly if complex modelling strategies were
used. three studies did not report the predictive
performance of the developed model, and four
studies reported only the apparent performance
(the performance with exactly the same data used to
develop the model, without adjustment for optimism
owing to potential overfitting). only 13 studies
assessed calibration,7 12 22 43 50 67 69 78 83 108 116 117 119
but the method to check calibration was probably
suboptimal in two studies.12 119
twenty five models were developed and externally
validated in the same study (in an independent dataset,
excluding random training test splits and temporal
splits).7 12 26 42 43 51 52 59 67 77 81 83 84 91 95 100 102 110 112 113
116 119 however, in 11 of these models, the datasets
used for the external validation were likely not
representative of the target population,7 12 26 42 59
91 100 102 116 and in one study, data from before the
covid-19 crisis were used.113 consequently, predictive
performance could differ if the models are applied in
the targeted population. in one study, commonly used
performance statistics for prognosis (discrimination,
calibration) were not reported.42 gozes,52 fu,51
chassagnon,77 hu,84 kurstjens,95 and vaid112 had
satisfactory predictive performance on an external
validation set, but it is unclear how the data for the
external validation were collected (eg, whether the
patients were consecutive), and whether they are
representative. wang,43 barda,67 guo,83 tordjman,110
and gong119 obtained satisfactory discrimination on
probably unbiased validation datasets, but each of
these had fewer than the recommended number of
events for external validation (100).137 138 diaz-quijano
externally validated a diagnostic model in a large
registry with reasonable discrimination, but many
patients had to be excluded because no polymerase
chain reaction (pcr) testing was performed.81
one study presented a small external validation
(27 participants) that reported satisfactory predictive
performance of a model originally developed for
avian influenza h7n9 pneumonia. however, patients
who had not recovered at the end of the study period
were excluded, which again led to a selection bias.23
another study was a small scale external validation
study (78 participants) of an existing severity score for
lung computed tomography images with satisfactory
reported discrimination.54 three studies validated
existing early warning or severity scores to predict
in-hospital mortality or deterioration.85 96 108 they
had satisfactory discrimination but less than the
recommended number of events for validation137 138 or
unclear sample sizes, excluded patients who remained
in hospital at the end of the study period, or had an
unclear study design.
discussion
in this systematic review of prediction models related
to the covid-19 pandemic, we identified and critically
appraised 107 studies that described 145 models.
study; setting; and outcome predictors in final model
sample size: total
no of participants for
model development
set (no with outcome)
predictive performance on validation
overall risk
of bias using
probast
type of
validation*
sample size: total
no of participants
for model validation
(no with outcome)
performance* (c index,
sensitivity (%), specificity (%),
ppv/npv (%), calibration slope,
other (95% ci, if reported))
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 640 (160) training test split 160 (40) sensitivity 82, specificity 93,
ppv 96
high
singh et al107; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable unknown twentyfold cross
validation
not applicable sensitivity 91, specificity 89 high
ucar et al111; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable unknown training test split unknown sensitivity 100, specificity 100,
ppv 99
high
wu et al115; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 300 (150) training test split 400 (200) sensitivity 95 (91 to 98),
specificity 93 (89 to 97)
high
table 2 | continued
chd=coronary heart disease; copd=chronic obstructive pulmonary disease; covid-19=coronavirus disease 2019; crp=c reactive protein; ct=computed tomography; ggo=ground glass opacities; npv=negative predictive value; ppv=positive predictive
value; probast=prediction model risk of bias assessment tool; pcr=polymerase chain reaction; wbc=white blood cells.
*performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is reported. apparent performance is the performance observed in the
development data.
‡calibration plot presented, but unclear which data were used.
§the development set contains scans from chinese patients, the testing set contains scans from chinese cases and controls, and us controls.
¶data contain mixed cases and controls. chinese data and controls from us and switzerland.
**describes similarity between segmentation of the ct scan by a medical doctor and automated segmentation.
§§pearson correlation between the predicted and ground truth scores for patients with lung abnormalities.
¶¶performance similar for models with different non-cases (healthy, bacterial pneumonia, and viral pneumonia).
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported)) prognosis original review bai et al9; data from china, inpatients at admission with mild confirmed covid-19; deterioration into severe/critical disease (period unspecified) combination of demographics, signs and symptoms, laboratory results and features derived from ct images 133 (54) unclear not applicable c index 0.95 (0.94 to 0.97) high
caramelo et al19; data from china, target population
unclear; mortality (period unspecified)††
age, sex, presence of any comorbidity (hypertension,
diabetes, cardiovascular disease, chronic respiratory
disease, cancer)††
unknown not reported not applicable not reported high
lu et al20; data from china, inpatients at admission
with suspected or confirmed covid-19; mortality
(within 12 days)
age, crp 577 (44) not reported not applicable not reported high
qi et al21; data from china, inpatients with confirmed
covid-19 at admission; hospital stay >10 days
6 features derived from ct images‡‡ (logistic
regression model)
26 (20) fivefold cross
validation
not applicable c index 0.92 high
qi et al21; data from china, inpatients with confirmed
covid-19 at admission; hospital stay >10 days
6 features derived from ct images‡‡ (random forest) 26 (20) 5 fold cross
validation
not applicable c index 0.96 high
shi et al37; data from china, inpatients with
confirmed covid-19 at admission; death or severe
covid-19 (period unspecified)
age (dichotomised), sex, hypertension 478 (49) validation in less
severe cases
66 (15) not reported high
xie et al7; data from china, inpatients with confirmed
covid-19 at admission; mortality (in hospital)
age, ldh, lymphocyte count, spo2 299 (155) external validation
(other chinese
centre)
130 (69) c index 0.98 (0.96 to 1.00),
calibration slope 2.5 (1.7 to
3.7)
high
yan et al22; data from china, inpatients suspected of
covid-19; mortality (period unspecified)
ldh, lymphocyte count, high sensitivity crp 375 (174) temporal
validation, selecting
only severe cases
29 (17) sensitivity 92, ppv 95 high
yuan et al23; data from china, inpatients with
confirmed covid-19 at admission; mortality (period
unspecified)
clinical scorings of ct images (zone, left/right,
location, attenuation, distribution of affected
parenchyma)
not applicable external validation
of existing model
27 (10) c index 0.90 (0.87 to 0.93) high
update 1
huang et al60; data from china, inpatients with
confirmed covid-19 at admission; severe symptoms
three days after admission
underlying diseases, fast respiratory rate >24/min,
elevated crp level (>10 mg/dl), elevated ldh level
(>250 u/l)
125 (32) apparent
performance only
not applicable c index 0.99 (0.97 to 1.00),
sensitivity 91, specificity 96
high
pourhomayoun et al61; data from 76 countries,
inpatients with confirmed covid-19; in-hospital
mortality (period unspecified)
unknown unknown 10-fold cross
validation
not applicable c index 0.96, sensitivity 90,
specificity 97
high
sarkar et al44; data from several continents
(australia, asia, europe, north america), inpatients
with covid-19 symptoms; death v recovery (period
unspecified)
age, days from symptom onset to hospitalisation,
from wuhan, sex, visit to wuhan
80 (37) apparent
performance only
not applicable c index 0.97 high
wang et al42; data from china, inpatients with
confirmed covid-19; length of hospital stay
age and ct features 301 (not
applicable)
not reported not applicable not reported high
zeng et al62; data from china, inpatients with
confirmed covid-19; severe disease progression
(period unspecified)
ct features 338 (76) cross validation
(number of folds
unclear)
not applicable c index 0.88 high
zeng et al62; data from china, inpatients with
confirmed covid-19; severe disease progression
(period unspecified)
ct features and laboratory markers 338 (76) cross validation
(number of folds
unclear)
not applicable c index 0.88 high
update 2
al-najjar et al63; data from south korea, target
population unclear; recovery from covid-19 (period
unspecified)
birth year (age), sex, country, group, infection reason,
confirmed date
466 (40) training test split 193 (14) sensitivity 43, specificity 98 high
table 3 | overview of prediction models for prognosis of covid-19
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported))
al-najjar et al63; data from south korea, target
population unclear; mortality (period unspecified)
age, sex, country, region, infection reason, confirmed
date
463 (25) training test split 191 (7) sensitivity 86, specificity 100 high
barda et al67; data from israel, patients with
confiremed covid-19; mortality (period unspecified)
age, sex, pack years, copd, number of wheezing/
dyspnea diagnoses, albumin, red cell distribution
width, c reactive peptide, urea, lymphocyte, chloride,
creatinine, high density lipoprotein, duration of
hospital admissions, count of hospital admissions,
count of ambulance rides, count of sulfonamide
dispenses, count of anticholinergic dispenses, count
of glucocorticoid dispenses, chronic respiratory
disease, cardiovascular disease, diabetes, malignancy,
hypertension
735,000 (8251) other (specify in
column cl) january
29 to april 8
inclusion, follow-up
until april 22 2020
(covid cases for
external validation)
3176 (87) c index 0.94 (0.92 to 0.96) ,
sensitivity 90 (83 to 96), ppv
17 (14 to 21)
high
bello-chavolla et al70; data from mexico, confirmed
covid-19 patients presenting at gp; 30-day mortality
age, pregnancy, diabetes, obesity, pneumonia, ckd,
copd, immunosuppression
12424 (1137) training test split 3105 (297) c index 0.80, somer’s d 0.60 high
carr et al75; data from united kingdom, inpatients
with confirmed covid-19; progression to severe
covid-19 (period unspecified)
age, national early warning score (news) 2, crp,
neutrophil, egfr, albumin
452 (159) temporal validation 256 (59) c index 0.73, sensitivity 46,
specificity 87
high
chassagnon et al77; data from france, inpatients with
confirmed covid-19; composite, 4-day intubation or
mortality
unclear 383 (84) external validation
(new centres,
france)
95 (26) sensitivity 88, specificity 74 high
colombi et al79; data from italy, inpatients with
confirmed covid-19; icu admission or in-hospital
(period unspecified)
age, cardiovascular comorbidities, median platelet
count, crp, visual assessment of well aerated lung %
236 (108) apparent
performance only
not applicable c index 0.86 (0.81 to 0.90),
sensitivity 72 (63 to 80),
specificity 81 (73 to 88) ppv 70
(61 to 78), npv 78 (72 to 83)
high
colombi et al79; data from italy, inpatients with
confirmed covid-19; icu admission or in-hospital
mortality (period unspecified)
age, cardiovascular comorbidities, median platelet
count, ldh, crp, software assessment of well aerated
lung absolute volume, adipose tissue
236 (108) apparent
performance only
not applicable c index 0.86 (0.81 to 0.90),
sensitivity 75 (66 to 83),
specificity 81 (73 to 88), ppv
70 (61 to 78), npv 78 (72 to
83)
high
das et al80; data from south korea, inpatients with
confirmed covid-19; icu admission or in-hospital
mortality (period unspecified)
age, sex, province, date of diagnosis, place of
exposure to covid-19
3022 (61) training test split 604 (12) c index 0.97 high
gong et al119; data from china, target population
unclear; 15-day progression to severe covid-19
age, direct bilirubin, red cell distribution width, blood
urea nitrogen, crp, lactate dehydrogenase, albumin
189 (28) external validation
(new centres,
china)
165 (40) c index 0.85 (0.79 to 0.92),
sensitivity 78, specificity 78
high
guo et al83; data from china, inpatients with
confirmed covid-19; 14-day progression to severe
covid-19
age, chronic illness, neutrophil to lymphocyte ratio,
crp, d-dimer
818 (24) external validation
(new centres,
china)
320 (38) c index 0.78 (0.70 to 0.87) high
hu et al84; data from china inpatients with confirmed
covid-19;in-hospital mortality (period unspecified)
age, high-sensitivity crp, lymphocyte count, d-dimer 183 (68) external validation
(new centres,
china)
64 (31) c index 0.88, sensitivity 84,
specificity 79
high
hu et al85; data from china, inpatients with
confirmed covid-19; in-hospital mortality (period
unspecified)
modified early warning score (mews): heart rate,
systolic blood pressure, respiratory rate, body
temperature, consciousness
not applicable external validation
only
105 (19) c index 0.68 (0.58 to 0.77),
sensitivity 68, specificity 65,
ppv 30, npv 90
high
hu et al85; data from china, inpatients with
confirmed covid-19; in-hospital mortality (period
unspecified)
rapid emergency medicine score (rems): mean
arterial pressure, pulse rate, respiratory rate, oxygen
saturation, gcs, age
not applicable external validation
only
105 (19) c index 0.84 (0.76 to 0.91),
sensitivity 89, specificity 70,
ppv 40, npv 97
high
 ji et al88; data from china, inpatients with confirmed
covid-19; 10-day progression to severe covid-19
comorbidity, age, lymphocyte count, lactate
dehydrogenase
208 (40) internal validation
by resampling
(bootstrap)
not applicable c index 0.91 (0.86 to 0.94),
sensitivity 95 (83 to 99),
specificity 78 (71 to 84)
high
table 3 | continued
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported)) jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 50% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 80% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 70% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 70% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 70% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 80% high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) age, serum blood urea nitrogen, emergency severity index, red cell distribution width, absolute neutrophil count, serum bicarbonate, glucose unknown leave-one-out cross validation not applicable c index 0.83 high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) sofa score not applicable external validation only unclear c index 0.73 high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) curb-65 score not applicable external validation only unclear c index 0.74 high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) sofa+ score not applicable external validation only unclear c index 0.83 high liu et al98; data from china, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) age, underlying disease status, helper t cells, helper t cells and suppressor t cells ratio 340 (30) apparent performance only not applicable mcfadden pseudo r-squared 0.35 high
mcrae et al100; data from china, inpatients with
confirmed covid-19; in-hospital mortality (period
unspecified)
age, sex, cardiac troponin i, crp, procalcitonin,
myoglobin
160 (43) new centres in
china, case series
12 (unknown) c index 0.94 (0.89 to 0.99) high
singh et al108; data from usa, inpatients with
confirmed covid-19; icu-level care, mechanical
ventilation or in-hospital mortality (period
unspecified)
epic deterioration index unknown external validation
only
174 (61) c index 0.76 (0.68 to 0.84),
sensitivity 39 ppv 80
high
table 3 | continued
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported))
vaid et al112; data from usa, inpatients with
confirmed covid-19; intubation, discharge to hospice
care or mortalit (period unspecified)
sex, race, ethnicity, age, hypertension, atrial
fibrillation, coronary artery disease, heart failure,
stroke, chronic kidney disease, diabetes, asthma,
copd, cancer, heart rate, pulse, oximetry, respiration
rate, temperature, systolic blood pressure, diastolic
blood pressure, body weight, sodium, potassium,
creatinine, lactate, white blood cells, lymphocyte
percentage, haemoglobin, red blood cell distribution
width, platelets, alanine, aminotransferase,
aspartate, aminotransferase, albumin, total bilirubin,
prothrombin time, partial thromboplastin time, pco2,
ph, crp, ferritin, d-dimer, creatinine phosphokinase,
lactate dehydrogenase, procalcitonin, troponin i
1225 (37) external validation,
new centres (usa)
1830 (unknown) c index 0.84, sensitivity 86,
specificity 82
high
vazquez guillamet et al113; data from usa, target
population unclear; in-hospital mortality (period
unspecified)
age, immunosuppression, copd, congestive heart
failure, bmi, sex, time to mechanical ventilation
(days), length of hospital stay prior to hospital
admission, pao2/fio2, glasgow coma scale, maximum
heart rate, maximum respiratory rate, minimum mean
arterial blood pressure, maximum temperature,
minimum albumin, minimum ph
2122 (429) external validation,
new centres (usa)
1175 (154) c index 0.81, ppv 55, npv 89 high
vazquez guillamet et al113; data from usa, target
population unclear; mechanical ventilation >96 hours
age, immunosuppression, copd, congestive heart
failure, bmi, sex, time to mechanical ventilation
(days), length of hospital stay before hospital
admission, pao2/fio2, glasgow coma scale, maximum
heart rate, maximum respiratory rate, minimum mean
arterial blood pressure, maximum temperature,
minimum albumin, minimum ph
2167 (158) training test split 1063 (96) c index 0.81 high
vazquez guillamet et al113; data from usa, target
population unclear; mechanical ventilation >96 hours
age, immunosuppression, copd, congestive heart
failure, bmi, sex, time to mechanical ventilation
(days), length of hospital stay prior to hospital
admission, pao2/fio2, glasgow coma scale, maximum
heart rate, maximum respiratory rate, minimum mean
arterial blood pressure, maximum temperature,
minimum albumin, minimum ph
1169 (141) training test split 619 (90) c index 0.78 high
zhang et al116; data from china and united kingdom,
inpatients with confirmed covid-19; in hospital
mortality (period unspecified)
age, sex, neutrophil count, lymphocyte count, platelet
count, crp, creatinine
653 (20) external validation
(new centres,
different country)
226 (77) c index 0.75, sensitivity 23,
specificity 95, ppv 69, npv 71
high
zhang et al116; data from china, inpatients with
confirmed covid-19; ards, intubation or ecmo, icu
admission, in hospital mortality (period unspecified)
age, sex, chronic lung disease, diabetes mellitus,
malignancy, cough, dyspnoea, immunocompromised,
hypertension, heart disease, chronic renal disease,
fever, fatigue, diarrhoea
768 (72) repeated five-fold
cross validation
not applicable c index 0.80, sensitivity 9,
specificity 99 ppv 53, npv 91
high
zhang et al116; data from china and united kingdom,
inpatients with confirmed covid-19; ards, intubation
or ecmo, icu admission, in hospital mortality (period
unspecified)
age, sex, neutrophil count, lymphocyte count, platelet
count, crp, creatinine
653 (58) external validation
(new centres,
different country)
226 (97) c index 0.72, sensitivity 40,
specificity 85, ppv 67, npv 65
high
ards=acute respiratory distress syndrome; bmi=body mass index; copd=chronic obstructive pulmonary disease; covid-19=coronavirus disease 2019; crp=c reactive protein; ct=computed tomography; ecmo=extracorporal membrane oxygenation;
icu=intensive care unit; ldh=lactate dehydrogenase; npv=negative predictive value; pao2/fio2=the ratio of arterial oxygen partial pressure to fractional inspired oxygen; pco2=partial pressure of carbon dioxide; ppv=positive predictive value;
probast=prediction model risk of bias assessment tool; sofa=sequential organ failure assessment score; spo2=oxygen saturation; na+=sodium; k+=potassium.
table 3 | continued
*performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is reported. apparent performance is the performance observed in the
development data.
††outcome and predictor data were simulated.
‡‡wavelet-hlh_gldm_smalldependencelowgraylevelemphasis, wavelet-lhh_glcm_correlation, wavelet-lhl_glszm_graylevelvariance, wavelet-llh_glszm_sizezonenonuniformitynormalized, wavelet-llh_glszm_smallareaemphasis, wavelet-llh_
glcm_correlation.
***each model uses a different predictive algorithm.
these prediction models can be divided into three
categories: models for the general population to
predict the risk of having covid-19 or being admitted to
hospital for covid-19; models to support the diagnosis
of covid-19 in patients with suspected infection; and
models to support the prognostication of patients with
covid-19. all models reported moderate to excellent
predictive performance, but all were appraised to
have high risk of bias owing to a combination of
poor reporting and poor methodological conduct
for participant selection, predictor description, and
statistical methods used. models were developed
on data from different countries, but the majority
used data from china or public international data
repositories. with few exceptions, the available sample
sizes and number of events for the outcomes of interest
were limited. this is a well known problem when
building prediction models and increases the risk of
overfitting the model.139 a high risk of bias implies
that the performance of these models in new samples
will probably be worse than that reported by the
researchers. therefore, the estimated c indices, often
close to 1 and indicating near perfect discrimination,
are probably optimistic. the majority of studies
developed new models, only 27 carried out an external
validation, and calibration was rarely assessed.
we reviewed 57 studies that used advanced
machine learning methodology on medical images to
diagnose covid-19, covid-19 related pneumonia, or to
assist in segmentation of lung images. the predictive
performance measures showed a high to almost perfect
ability to identify covid-19, although these models and
their evaluations also had a high risk of bias, notably
because of poor reporting and an artificial mix of
patients with and without covid-19. therefore, we do
not recommend any of the 145 identified prediction
models to be used in practice.
challenges and opportunities
the main aim of prediction models is to support
medical decision making. therefore, it is vital to
identify a target population in which predictions
serve a clinical need, and a representative dataset
(preferably comprising consecutive patients) on which
the prediction model can be developed and validated.
this target population must also be carefully described
so that the performance of the developed or validated
model can be appraised in context, and users know
which people the model applies to when making
predictions. unfortunately, the studies included in our
systematic review often lacked an adequate description
of the study population, which leaves users of these
models in doubt about the models’ applicability.
although we recognise that all studies were done
under severe time constraints, we recommend that
any studies currently in preprint and all future studies
should adhere to the tripod reporting guideline16 to
improve the description of their study population and
their modelling choices. tripod translations (eg, in
chinese and japanese) are also available at https://
www.tripod-statement.org.
authors
risk of bias
participants predictors outcome analysis
hospital admission in general population
original review
decaprio et al8 high low high high
update 2
jiang et al90 high unclear high high
diagnosis
original review
feng et al10 low unclear high high
lopez-rincon et al35 unclear low low high
meng et al12 high low high high
song et al31 high unclear low high
update 1
martin et al41 high high high high
sun et al40 low low unclear high
wang et al43 low unclear unclear high
wu et al45 high unclear low high
update 2
batista et al69 unclear unclear low high
brinati et al74 unclear unclear low high
chen et al78 high high low high
diaz-quijano et al81 high high low high
kurstjens et al95 unclear low high high
mei et al101 high unclear unclear high
menni et al102 high unclear unclear high
soares et al109 unclear unclear low high
tordjman et al110 low unclear unclear high
zhao et al117 high high unclear high
diagnosis of severity
original review
yu et al25 unclear unclear unclear high
update 1
zhou et al46 unclear low high high
update 2
benchoufi et al71 high low low high
chassagnon et al77 low low low high
li et al97 unclear unclear unclear high
lyu et al99 low unclear unclear high
wang et al114 unclear high low high
zhu et al118 low low high high
diagnostic imaging
original review
barstugan et al32 unclear unclear unclear high
chen et al27 high unclear low high*
gozes et al26 unclear unclear high high
jin et al11 high unclear unclear high†
jin et al33 high unclear high high*
li et al34 low unclear low high
shan et al29 unclear unclear high high†
shi et al36 high unclear low high
wang et al30 high unclear low high
xu et al28 high unclear high high
song et al24 unclear unclear low high
zheng et al38 unclear unclear high high
update 1
abbas et al47 high unclear unclear high
apostolopoulos et al48 high unclear high high
bukhari et al49 unclear unclear unclear high
chaganti et al50 high unclear low unclear
chowdhury et al39 high unclear unclear high
fu et al51 high unclear unclear high
gozes et al52 high unclear unclear high
imran et al53 high unclear unclear high*
li et al54 low low unclear high
li et al55 high unclear high high*
hassanien et al56 unclear unclear unclear high*
tang et al57 unclear unclear high high
table 4 | risk of bias assessment (using probast) based on four domains across 107
studies that created prediction models for coronavirus disease 2019
a better description of the study population could
also help us understand the observed variability
in the reported outcomes across studies, such as
covid-19 related mortality and covid-19 prevalence.
the variability in prevalence could in part be reflective
of different diagnostic standards across studies. note
that the majority of diagnostic models use viral nucleic
acid test results as the gold standard, which may have
unacceptable false negative rates.
covid-19 prediction problems will often not present
as a simple binary classification task. complexities
in the data should be handled appropriately. for
example, a prediction horizon should be specified for
prognostic outcomes (eg, 30 day mortality). if study
participants have neither recovered nor died within
that time period, their data should not be excluded
from analysis, which most reviewed studies have done.
instead, an appropriate time to event analysis should
be considered to allow for administrative censoring.17
censoring for other reasons, for instance because of
quick recovery and loss to follow-up of patients who
are no longer at risk of death from covid-19, could
necessitate analysis in a competing risk framework.140
a prediction model applied in a new healthcare
setting or country often produces predictions that
are miscalibrated141 and might need to be updated
before it can safely be applied in that new setting.17
this requires data from patients with covid-19 to be
available from that system. instead of developing and
updating predictions in their local setting, individual
participant data from multiple countries and healthcare
systems might allow better understanding of the
generalisability and implementation of prediction
models across different settings and populations. this
approach could greatly improve the applicability and
robustness of prediction models in routine care.142-146
the evidence base for the development and validation
of prediction models related to covid-19 will quickly
increase over the coming months. together with the
increasing evidence from predictor finding studies147-153
and open peer review initiatives for covid-19
related publications,154 data registries120 121 155-157
are being set up. to maximise the new opportunities
and to facilitate individual participant data metaanalyses, the world health organization has released a
new data platform to encourage sharing of anonymised
covid-19 clinical data.158 to leverage the full potential
of these evolutions, international and interdisciplinary
collaboration in terms of data acquisition, model
building and validation is crucial.
study limitations
with new publications on covid-19 related prediction
models rapidly entering the medical literature, this
systematic review cannot be viewed as an up-to-date list of
all currently available covid-19 related prediction models.
also, 87 of the studies we reviewed were only available
as preprints. these studies might improve after peer
review, when they enter the official medical literature; we
will reassess these peer reviewed publications in future
updates. we also found other prediction models that are
authors
risk of bias
participants predictors outcome analysis
wang et al42 low unclear unclear high
zhang et al58 high unclear high high
zhou et al59 high unclear high high*
update 2
angelov et al64 high unclear high high
arpan et al65 unclear unclear unclear high
bai et al66 high unclear high high
bassi et al68 high unclear high high
borghesi et al72 high unclear unclear high
born et al73 high unclear unclear high
castiglioni et al76 unclear unclear low high
guiot et al82 high unclear low high
hu et al86 high unclear high high
islam et al87 high unclear high high
kana et al91 high unclear high high*
karim et al92 high unclear high high
khan et al93 high unclear high high*
kumar et al94 high unclear unclear high*
moutounet-cartan103 unclear unclear unclear high
ozturk et al104 high unclear unclear high
rahimzadeh et al105 high unclear unclear high
rehman et al106 high unclear unclear high
singh et al107 high unclear unclear high
ucar et al107 high unclear unclear high
wu et al115 high unclear unclear high
prognosis
original review
bai et al9 low unclear unclear high
caramelo et al19 high high high high
lu et al20 low low low high
qi et al21 unclear low low high
shi et al37 high high high high
xie et al7 low low low high
yan et al22 low high low high
yuan et al23 low high low high
update 1
huang et al60 unclear unclear unclear high
pourhomayoun et al61 low low unclear high
sarkar et al44 high high high high
wang et al42 low low low high
zeng et al62 low low low high
update 2
al-najjar et al63 unclear unclear unclear high
barda et al67 low low high high
bello-chavolla et al70 unclear unclear low high
carr et al75 low low low high
chassagnon et al77 low low low high
colombi et al79 high unclear unclear high
das et al80 low low low high
gong et al119 low low high high
guo et al83 low high unclear high
hu et al84 high low low high
hu et al85 low unclear low high
ji et al88 low low low high
jiang et al89 unclear unclear unclear high
levy et al96 low low low high
liu et al98 low low low high
mcrae et al100 high high high high
singh et al108 low unclear high high
vaid et al112 unclear high high high
vazquez guillamet et al113 high low unclear high
zhang et al116 low unclear unclear/low‡ high
probast=prediction model risk of bias assessment tool.
*risk of bias high owing to calibration not being evaluated. if this criterion is not taken into account, analysis risk
of bias would have been unclear.
†risk of bias high owing to calibration not being evaluated. if this criterion is not taken into account, analysis risk
of bias would have been low.
‡zhang et al evaluated two outcomes: death (low risk of bias) and a composite poor outcome (unclear risk of
bias).
table 4 | continued
currently being used in clinical practice without scientific
publications,159 and web risk calculators launched for
use while the scientific manuscript is still under review
(and unavailable on request). these unpublished models
naturally fall outside the scope of this review of the
literature.160 as we have argued extensively elsewhere,161
transparent reporting that enables validation by
independent researchers is key for predictive analytics,
and clinical guidelines should only recommend publicly
available and verifiable algorithms.
implications for practice
all 145 reviewed prediction models were found to have
a high risk of bias, and evidence from independent
external validation of the newly developed models is
currently lacking. however, the urgency of diagnostic
and prognostic models to assist in quick and efficient
triage of patients in the covid-19 pandemic might
encourage clinicians and policymakers to prematurely
implement prediction models without sufficient
documentation and validation. earlier studies have
shown that models were of limited use in the context of
a pandemic,162 and they could even cause more harm
than good.163 therefore, we cannot recommend any
model for use in practice at this point.
the current oversupply of insufficiently validated
models is not useful for clinical practice. future studies
should focus on validating, comparing, improving,
and updating promising available prediction models,
rather than developing new ones.17 for example,
diaz-quijano developed and externally validated a
diagnostic model using brazilian surveillance data
with reasonable discrimination, but many patients had
to be excluded because no pcr testing was performed,
hence this model needs further validation.17 two other
models to diagnose covid-19 also showed promising
discrimination at external validation in small
unselected cohorts.43 110 an externally validated model
that used computed tomography based total severity
scores showed good discrimination between patients
with mild, common, and severe-critical disease.54
two models to predict progression to severe covid-19
within two weeks showed promising discrimination
when validated externally on unselected cohorts.83 119
another model discriminated well between survivors
and non-survivors among confirmed cases, but the
prediction horizon was not specified, and the study had
many missing values for key parameters.67 because
reporting in each of these studies was insufficiently
detailed and the validation was in datasets with fewer
than 100 events in the smallest outcome category,
validation in larger, international datasets is needed.
such external validations should assess not only
discrimination, but also calibration and clinical utility
(net benefit).141 146 163 owing to differences between
healthcare systems (eg, chinese and european) in
when patients are admitted to and discharged from
hospital, as well as the testing criteria for patients
with suspected covid-19, we anticipate most existing
models will be miscalibrated, but this can usually be
solved by updating and adjustment to the local setting.
when creating a new prediction model, we
recommend building on previous literature and expert
opinion to select predictors, rather than selecting
predictors in a purely data driven way.17 this is especially
important for datasets with limited sample size.164
based on the predictors included in multiple models
identified by our review, we encourage researchers to
consider incorporating several candidate predictors.
common predictors include age, body temperature,
lymphocyte count, and lung imaging features. flulike signs and symptoms and neutrophil count are
frequently predictive in diagnostic models, while
comorbidities, sex, c reactive protein, and creatinine
are frequently reported prognostic factors. by pointing
to the most important methodological challenges and
issues in design and reporting of the currently available
models, we hope to have provided a useful starting
point for further studies aiming to develop new models,
or to validate and update existing ones.
this living systematic review has been conducted in
collaboration with the cochrane prognosis methods
group. we will update this review and appraisal
continuously to provide up-to-date information for
healthcare decision makers and professionals as more
international research emerges over time.
box 1: availability of models in format for use in clinical practice
several studies presented their models in a format for use in clinical practice.
however, because all models were at high risk of bias, we do not recommend their
routine use before they are properly externally validated.
models to predict risk of developing coronavirus disease 2019 (covid-19) or of hospital
admission for covid-19 in general population
the “covid-19 vulnerability index” to detect hospital admission for covid-19
pneumonia from other respiratory infections (eg, pneumonia, influenza) is available
as an online tool.8122
diagnostic models
several sum scores,31 95 110 117 and model equations81 102 are available to support the
diagnosis. graphical diagnostic aids include nomograms43 78 117 and a decision tree.74
the “covid-19 diagnosis aid” app is available on ios and android devices to diagnose
covid-19 in asymptomatic patients and those with suspected disease.12 additionally,
online tools are available.10 45 74 95 123-125 classification in terms of disease severity can
be done using a published equation.114 a decision tree to detect severe disease for
paediatric patients with confirmed covid-19 is also available in an article.25
diagnostic models based on images
five artificial intelligence models to assist with diagnosis based on medical images
are available through web applications.24 27 30 73 91 126-130 one model is deployed in 16
hospitals, but the authors do not provide any usable tools in their study.33 two papers
includes a severity scoring system to classify patients based on images.5472
prognostic models
to assist in the prognosis of mortality, a nomogram,7
 a decision tree,22 a score
system,70 online tools,80 84 96 98 131-134 and a computed tomography based scoring rule
are available in the articles.23 other online tools predict in-hospital death and the
need for prolonged mechanical ventilation,113 135 or in-hospital death and a composite
of poor outcomes.116 136 additionally nomograms,88 119 sumscores83 88 and a model
equation60 are available to predict progression to severe covid-19.
several studies made their code available on github.8 11 34 35 38 47 55 65-68 70 73 86 92 98 101
104 105 109 seventy four studies did not include any usable equation, format, code, or
reference for use or validation of their prediction model.
conclusion
several diagnostic and prognostic models for covid-19
are currently available and they all report moderate
to excellent discrimination. however, these models
are all at high risk of bias, mainly because of nonrepresentative selection of control patients, exclusion
of patients who had not experienced the event of
interest by the end of the study, and model overfitting.
therefore, their performance estimates are probably
optimistic and misleading. the covid-precise group
does not recommend any of the current prediction
models to be used in practice. future studies aimed
at developing and validating diagnostic or prognostic
models for covid-19 should explicitly address the
concerns raised. sharing data and expertise for the
validation and updating of covid-19 related prediction
models is urgently needed.

<|EndOfText|>

risk factors for the progression of fnger interphalangeal joint
osteoarthritis: a systematic review
abstract
progressive hand interphalangeal joint (ipj) osteoarthritis is associated with pain, reduced function and impaired quality of
life. however, the evidence surrounding risk factors for ipj osteoarthritis progression is unclear. identifying risk factors for
ipj osteoarthritis progression may inform preventative strategies and early interventions to improve long-term outcomes for
individuals at risk of ipj osteoarthritis progression. the objectives of the study were to describe methods used to measure the
progression of ipj osteoarthritis and identify risk factors for ipj osteoarthritis progression. medline, embase, scopus,
and the cochrane library were searched from inception to 19th february 2020 (prospero crd42019121034). eligible
studies assessed potential risk factor/s associated with ipj osteoarthritis progression. risk of bias was assessed using a modifed quips tool, and a best evidence synthesis was performed. of eight eligible studies, all measured osteoarthritis progression radiographically, and none considered symptoms. eighteen potential risk factors were assessed. diabetes (adjusted
mean diference between 2.06 and 7.78), and larger fnger epiphyseal index in males (regression coefcient β=0.202) and
females (β=0.325) were identifed as risk factors (limited evidence). older age in men and women showed mixed results;
13 variables were not risk factors (all limited evidence). patients with diabetes and larger fnger epiphyseal index might
be at higher risk of radiographic ipj osteoarthritis progression, though evidence is limited and studies are biased. studies
assessing symptomatic ipj osteoarthritis progression are lacking.
keywords hand interphalangeal joint · osteoarthritis · risk factors · disease progression
introduction
osteoarthritis is one of the leading causes of worldwide disability [1], and, in the usa alone, carries a cost of $10 billion just from economic loss [2]. hand osteoarthritis is one
of the most common types of radiographic osteoarthritis [1].
hand osteoarthritis also presents in a younger population
than osteoarthritis at other joints, with a prevalence of 3% in
men and 8% in women aged 45–64 years [3, 4]. it is considered a chronic disease, with some cases progressing and the
prevalence increasing to 5% in men and 9% in women aged
65–74 years [5]. symptomatic treatment for progressive
hand osteoarthritis is limited, with patients often requiring
surgical management, such as arthrodesis or arthroplasty [6].
measuring progressive hand osteoarthritis is difcult,
with no consensus for defning or quantifying worsening
of disease [7]. the osteoarthritis research society international (oarsi) 2006 task force described hand osteoarthritis progression as being joint specifc, whereby osteoarthritis
in one hand joint evolves independently from other hand
joints [8]. however, analysis from a large cohort study suggests there are patterns of symmetry, osteoarthritis clustering
by row (across distal interphalangeal joints (dipjs) or across
proximal interphalangeal joints (pipjs)), and clustering by
ray (within a fnger) also exist [9]. there is also poor correlation between radiographic and symptomatic disease [10, 11].
rheumatology international
conference abstract presentations some data from this
manuscript has been published as a conference abstract:
shah k et al (2020) a systematic review of risk factors and
diagnostic methods for hand interphalangeal joint osteoarthritis
progression. osteoarthr cartil 28(s86–s527): s426.
electronic supplementary material the online version of this
article (https://doi.org/10.1007/s00296-020-04687-1) contains
supplementary material, which is available to authorized users.
rheumatology international
1 3
the aetiology for the progression of hand osteoarthritis
is also poorly understood, and therefore identifying patients
at highest risk for needing surgical management is limited.
when managing hip and knee osteoarthritis surgically,
shared decision making between clinicians and patients has
been shown to be benefcial [12]. in the hand, a better understanding of whether a patient is at increased risk of progressive disease would help to inform shared decision making. in
particular, it would enable earlier investigations, more personalised treatment pathways, and targeted interventions for
prevention and treatment. these priorities have been highlighted by the recent commission on the future of surgery
[13]. similarly, being able to identify patients with osteoarthritis who will not progress will prevent the over-investigation and excessive medical treatment of these patients. a
review has found that abnormal scintigraphy scans, higher
australian/canadian hand osteoarthritis index (auscan)
scores, number of osteoarthritis joints at baseline, more pain,
and nodal osteoarthritis were risk factors for the progression
of radiographic or clinical hand osteoarthritis [14]. however,
this review combined interphalangeal joint (ipj) and base of
thumb [frst carpometacarpal joint (cmcj)] osteoarthritis
under the umbrella of ‘hand osteoarthritis.’ finger ipj and
frst cmcj osteoarthritis are now thought to be diferent
subsets of the disease, with diferent risk factors, pathophysiology and patterns of progression [15].
therefore, the primary aim of this systematic review is to
identify risk factors for the progression of fnger ipj osteoarthritis. the secondary aim is to describe the measurements
used to defne the progression of ipj osteoarthritis.
methods
the reporting of this systematic review followed the
preferred reporting items for systematic reviews and
meta-analysis (prisma) statement [16]. the protocol was prospectively registered on prospero [17]
(crd42019121034).
search strategy
the search strategy was constructed with the assistance
of a specialist health-care librarian. the search was conducted in four electronic databases: (1) medline by ovid,
(2) embase by ovid, (3) scopus, (4) the cochrane library.
the search string included a range of search terms for (1)
hands and fngers, (2) osteoarthritis, and (3) progression,
and was amended for each database (electronic supplementary material 1). the picos tool [18] was used to
frame the search strategy as follows: population: adults
with ipj osteoarthritis, intervention/prognostic factor:
potential risk factor(s) for ipj osteoarthritis progression,
comparison: no exposure to the risk factor(s), outcome:
progression of ipj osteoarthritis, study type: quantitative
methodology. the search was conducted on 17th october 2018 and duplicates were removed. the search was
updated on 19th february 2020. the reference lists of all
eligible articles were manually assessed for additional
studies. rayyan qcri tool was used to import all papers
[19].
two groups of reviewers (group 1: ks, group 2: xy and
jcel) independently screened titles and abstracts for eligibility. any articles with insufcient title or abstract information were referred for full text review. articles for which the
full text was not available were requested directly from the
authors. any disagreements in eligibility assessment was
resolved at a consensus meeting by a third reviewer (srf).
study eligibility criteria
studies were considered eligible if they (a) included participants with evidence of radiographic or clinical ipj osteoarthritis at baseline; (b) the participants were followed up
for at least 1 year (as it has been shown that progression
of radiographic hand osteoarthritis can be detected over
a 1 year time frame [20]); (c) ipj osteoarthritis (separate
from frst cmcj osteoarthritis) progression was measured
at follow-up, using radiographic and/or symptomatic criteria
(ipj osteoarthritis progression was defned as an increase
in radiographic or symptomatic criteria/score at follow-up
compared to baseline); (d) the association between a potential risk factor and the progression of ipj osteoarthritis was
investigated at follow-up.
case reports were excluded. letters to editors might
contain important information about studies, such as new
information or discussions of further weaknesses of original studies [21]. therefore, letters to editors which exist in
the context of original studies, included in our review, were
examined to inform the risk of bias assessment and as additional sources of information [22]. conference abstracts are
considered to have high variability in terms of data reliability, accuracy and detail, and therefore these were excluded
[21, 23].
studies of infammatory arthritis, erosive arthritis, with
participants under the age of 18 years (to avoid confounding
by juvenile arthritis), and studies where ipj osteoarthritis
results could not be separated from other joints including
the frst cmcj, and were not provided on request of the
corresponding author within 2 months were excluded. animal, cadaver, and cell studies were excluded. articles not
in english, and articles which lacked accessible full texts
(online or in paper copy throughout the uk, or after requesting them from the corresponding author with no reply within
2 months) were excluded.
rheumatology international
1 3
data extraction
one reviewer (ks) independently extracted participant
demographics (e.g. age and sex), study characteristics
(e.g. study design), the potential risk factor/s assessed,
effect measure and size/s and the definition/s used to
measure osteoarthritis progression. a potential risk factor
was defined as any factor investigated for an association
with ipj osteoarthritis progression.
if data was reported at multiple time points, results
from all time points were extracted. if articles or supplementary material did not contain sufficient data, the
corresponding author was contacted to request additional
data, with a 2 month turnaround policy. for any articles
which reported data from a study described in detail
elsewhere, the source of the data was retrieved and data
extracted as appropriate. data extraction was input into a
microsoft excel file and cross-checked by a second independent reviewer (xy).
risk of bias assessment
two independent reviewers (ks and xy) rated the risk of
bias of included studies using a modified version of the
quality in prognosis studies (quips) risk of bias tool
[24] (electronic supplementary material 2). the following five domains were assessed: (1) study participation,
(2) study attrition, (3) prognostic factor measurement, (4)
outcome measurement, (5) statistical analysis and reporting [24]. we excluded the domain assessing ‘confounding
factors’, as confounders can themselves be considered to
be prognostic factors, and thus the term ‘confounders’ is
a misnomer in prognostic factor studies [25]. as there is
currently limited established literature in the field of ipj
osteoarthritis progression, any ‘confounder’ identified in
the literature was treated as a potential risk factor for this
review. each domain was given an overall score of ‘low’,
‘moderate’ or ‘high’ risk of bias (electronic supplementary material 2). the overall risk of bias of a study was
classified by examining the risk of bias in each of the
five domains. if one or more domains were classified as
having high risk of bias, then this study was classified as
having an overall high risk of bias [24, 26–28]. if three or
more domains were classified as having a moderate risk
of bias, then this study was classified as having an overall
moderate risk of bias [24, 26, 27, 29]. if all domains were
classified as having a low risk of bias, or less than three
domains had a moderate risk of bias, then this study was
classified as having an overall low risk of bias [28, 30].
any disagreement between reviewers was discussed at a
consensus meeting with a third reviewer (srf).
analysis and best evidence synthesis
risk factors for all defnitions of ipj osteoarthritis progression were identifed, followed by a subgroup analysis for
dipj and pipj separately. if studies were homogenous with
regard to study populations, potential risk factors assessed,
efect measures used, and measurements of ipj osteoarthritis progression, a pooled meta-analysis was considered
using review manager software [31], and the grading of
recommendations, assessment, development and evaluation (grade) approach was used to assess the quality
of evidence [32].
if studies were heterogeneous, we chose not to report
effect measures of different types and instead used a
qualitative narrative summary. the association between
a potential risk factor and ipj osteoarthritis progression
was categorised as:
(a) a risk factor: positive efect measure.
(b) not a risk factor: negative efect measure; or, no statistical association.
(c) conficting evidence: efect measures not in the same
direction.
a best evidence synthesis was used to summarise the
data for each potential risk factor assessed [33–36]. the
criteria were applied sequentially. if multiple analyses
were performed within one study, the consistent fndings
approach described below was applied to the study to
decide whether it showed consistent or mixed evidence.
this was then used to calculate the overall best evidence
synthesis across studies.
(a) consistent evidence:≥ 75% of studies reported the
same direction of efect (either positive or negative/no
association).
(b) mixed evidence:<75% of analyses reported the same
direction of efect.
if consistent evidence was found, the strength of evidence was assessed:
(i) strong evidence:>2 studies with low risk of bias.
(ii) moderate evidence: 1 study with low risk of bias. and
1 other study; or:>2 studies with moderate or high
risk of bias.
(iii) limited evidence with low risk of bias: 1 study with
low risk of bias.
(iv) limited evidence:≤2 studies with moderate or high
risk of bias.
rheumatology international
1 3
results
studies included
combining results from the search in october 2018 and
the updated search in february 2020, 25, 739 titles were
identifed through the search strategy, with 13,346 remaining after removal of duplicates. after screening titles and
abstracts, the full text of 32 articles was evaluated, and eight
articles met the inclusion criteria (fig. 1). no additional articles were found by reviewing the reference lists of eligible
studies.
study characteristics
eight prospective cohort studies were included [37–44]
(table 1). five studies included men and women [40–44],
whilst three studies included only men [37–39]. the smallest
study included 177 participants [37], whilst the largest study
included 5560 participants [40]. the shortest follow-up
period was a mean of 2.28 years [39] and the longest followup was reported as a mean (standard deviation) of 23.5 (3.3)
years [37].
risk of bias
seven studies were rated as having overall high risk of bias
[37–39, 41–44], and one study was of moderate risk of bias
[40] (table 2). ‘study participation’ was of high risk of
bias in four studies due to studies not adequately reporting
recruitment periods and places of recruitment [37, 39, 41,
44]. in the ‘study attrition’ domain, plato et al. and kallman
et al. did not clearly report response rates and reasons for
participants with loss to follow-up [37, 39], whilst haugen
et al. and marshall et al. had less than 80% response rates
and also did not report reasons for loss to follow-up [41,
42]. when assessing the ‘statistical analysis and reporting’ domain, it was found that plato et al., busby et al., and
kalichman et al. did not provide efect measures, but only
reported p values or stated whether results were ‘signifcant
or not signifcant’ [38, 39, 43].
fig. 1 preferred reporting
items for systematic reviews
and meta-analysis (prisma)
fowchart of study selection
screening inclusion eligibilit
y idenficaon
records aer duplicates removed
(n = 13,346)
abstracts/tles screened
(n = 13,346)
records excluded
(n = 13,314)
full-text arcles assessed for
eligibility
(n = 32)
studies included in systemac
review
(n = 8)
combined records idenfied
through database searching on
17th oct 2018 and 19th feb 2020
(n= 25,739)
medline (n = 7,515),
embase (n = 8,474),
scopus (n = 8,391),
cochrane (n = 1,359)
full-text arcles excluded
(n = 24)
-ineligible study design (n=15)
-ineligible study outcome (n=9)
rheumatology international
1 3
table 1 characteristics of studies investigating risk factors for the progression of fnger interphalangeal joint osteoarthritis
authors population length of follow-up
(years)
age (years)
(mean)
female (%) inclusion criteria exclusion criteria n (n) criteria for ipj
oa progression
risk factor
assessed
plato et al. [35] white middle
class volunteers
participated in
the blsa in the
usa
group 1: 0–3, group 2:
4–7, group 3: 8–11,
group 4: 12–16
ns 0 ns ns 478 (ns) increase by≥1
grade from the
highest kl
[41] grade at
baseline in any
dipj
older age in men
kallman et al.
[33]
white middle
class volunteers who
participated in
the blsa in the
usa
≥20
(age<60 years);≥14
(age≥60 years)
ns 0 ns maximum kl
score (4) at
baseline (per
patient); not
specifed
177 (177) increase by≥1
grade from the
highest kl [41,
42] grade at
baseline in any
pipj
older age in men
busby et al. [34] white middle
class volunteers who
participated in
the blsa in the
usa
5–16.3 ns 0 ns joints with kl
score of 4 at
baseline
386 (ns) outcome 1:
increase by≥1
grade from the
highest kl
[42] grade at
baseline in any
ipj (dipj and
pipj assessed
separately)
outcome 2:
increase in
number of
ipjs with kl
[42] grade≥2
(dipj and pipj
assessed separately)
older age in men
kalichman et al.
[39]
chuvashians; village; randomly
recruited
8 men: 45.3,
women: 49.7
52 ns ns 263 (263) increase in
number of
ipjs with kl
[42] grade≥2
(dipj and pipj
assessed separately)
alcohol, anthropometric features,
familial relationship, gender
(female), older
age in men, older
age in women,
smoking
kalichman et al.
[40]
chuvashians; village; randomly
recruited
8 men: 47.4,
women: 50.9
46 ns bone disease,
amenorrhoea,
hormone
replacement
therapy, steroids
557 (513) increase by≥1
grade in a
cumulative kl
[38] sum score
(2nd, 3rd and
4th, pipjs)
epiphyseal index
(larger)
rheumatology international
1 3
blsa baltimore longitudinal study of aging, bmi body mass index, casha clinical assessment studies of the hand, cask clinical assessment studies of the knee, dipj distal interphalangeal joint, gp general practice, ipj interphalangeal joint, kl kellgren–lawrence atlas, pipj proximal interphalangeal joint, n number at baseline, n number at follow-up, ns not specifed, oa
osteoarthritis, usa united states of america, x-rays plain flm radiographs
table 1 (continued)
authors population length of follow-up
(years)
age (years)
(mean)
female (%) inclusion criteria exclusion criteria n (n) criteria for ipj
oa progression
risk factor
assessed
hoeven et al. [36] rotterdam 10 men: 67.5,
women: 68.6
58 ≥55 years, living
for≥1 year
in ommoord,
knee, hip, hand
x-rays
no x-rays,
rheumatoid,
fractures
5650 (2442) increase by≥1
kl [41] grade
in≥1 ipj, if≥1
ipj had kl
[41] grade≥2
at baseline
(dipj and pipj
assessed separately)
atherosclerosis
haugen et al. [37] usa; hospital
study sites
4 58.4 58 ns systemic infammatory arthritis,
bilateral end
stage knee oa,
inability to walk
without aids,
contraindication
to mri
994 (994) increase by≥1
grade in a
cumulative
modifed kl
[42, 53] sum
score (dipj and
pipj assessed
together)
alcohol (higher
intake), bmi
(higher)—at
age 25, bmi
(higher)—current, smoking,
waist circumference (higher)
marshall et al.
[38]
from casha
and cask
cohorts; gp
community
7 60.5 60 age 50–69 years
at baseline,
reported hand
pain in last
month
infammatory
arthritis, all
hand joints
afected
with kl≥2
at baseline,
deaths/untraceable/address
unknown,
severe/terminal
illness
706 (388) outcome 1:
increase by≥1
grade in a
cumulative
kl [43] sum
score (dipj and
pipj assessed
together)
outcome 2:
increase in
number of
ipjs with kl
[43] grade≥2
(dipj and
pipj assessed
together)
bmi (higher)—
current, diabetes
type 2/impaired
fasting glucose,
dyslipidaemia,
hypertension,
number of
metabolic factors
(higher)
rheumatology international
1 3
measurements for the progression of fnger
interphalangeal joint osteoarthritis
all studies assessed osteoarthritis radiographically, using
a version of the kellgren and lawrence (kl) classifcation
[45, 46] (table 1). three studies measured ipj osteoarthritis progression as a≥1 grade increase from the highest kl
grade at baseline [37–39]; three studies measured it as an
increase in the total number of ipjs with kl grade≥2 [38,
42, 43]; one study measured progression as a≥1 grade kl
increase in a≥1 ipj [40]; and three studies measured it as≥1
grade increase in a cumulative kl sum score [41, 42, 44].
no studies measured osteoarthritis progression through a
deterioration in symptomatic scoring.
risk factors for the progression of fnger
interphalangeal joint osteoarthritis
eighteen potential risk factors were assessed, most commonly in one study only (efect measures shown in electronic supplementary material 3). for potential risk factors
assessed by more than one study, due to heterogeneity in the
defnitions of the risk factor/s, statistical tests, and osteoarthritis defnitions, a best evidence synthesis was performed.
three risk factors were identifed: diabetes type 2/impaired
fasting glucose (ifg) [42]; and larger epiphyseal index (ei)
in males [44], and in females [44] (all with limited evidence)
(table 3). older age in men [37–39, 43] and in women [43]
showed mixed results (table 3).
diabetes type 2/impaired fasting glucose
(ifg)
marshall et al. assessed diabetes type 2/ifg compared to not
having these conditions in a total of 474 participants [42]
(efect measures shown in electronic supplementary material 3). in a complete case analysis, these conditions were
associated with an increase by≥1 grade in a cumulative kl
[47] sum score for all ipjs [adjusted mean diference (95% confdence interval) 7.78 (1.13–14.43)] [42]. however, there was
no association following multiple imputation [4.50 (− 0.26
to 9.25)] [42]. diabetes type 2/ifg was associated with an
increase in the number of ipjs with kl [47] grade≥2 following multiple imputation and complete case analysis [2.06
(0.25–3.87) and 3.35 (1.08–5.62), respectively] [42].
large fnger epiphyseal index (ei)
kalichman et al. investigated larger ei in 177 participants
[44] (electronic supplementary material 3). a positive
association was found in both males (multiple regression
coefcient, β=0.202; 95% ci not reported) and females
(β=0.325; 95% ci not reported), between larger ei and ipj
osteoarthritis progression (measured as an increase by≥1
grade in a cumulative kl [38] sum score for pipjs in the
assessed digits).
dipj and pipj subgroup analysis
in the dipj subgroup analysis, eight potential risk factors
were assessed, and only older age in women was found to be
a risk factor (correlation coefcient 0.20) [43] (limited evidence) (table 3). in the pipj subgroup analysis, 11 potential
risk factors were assessed, and larger ei in males (β=0.202;
95% ci not reported) and females (β=0.325; 95% ci not
reported) were identifed as risk factors [44] (limited evidence for both) (table 3).
discussion
osteoarthritis is one of the largest health-care burdens, and
radiographic hand osteoarthritis is highly prevalent, afecting
more than one out of fve adult americans [48]. osteoarthritis is considered to be progressive in some cases. however,
table 2 risk of bias for
studies assessing potential risk
factors for the progression of
fnger interphalangeal joint
osteoarthritis, assessed using a
modifed quality in prognosis
studies (quips) tool
a
biases from modifed quality in prognosis studies (quips) tool: (1) study participation; (2) study attrition; (3) prognostic factor measurement; (4) outcome measure; (5) statistical analysis and reporting
authors biasesa overall risk of bias
1 2 3 4 5
plato et al. [35] high high moderate moderate high high
kallman et al. [33] high high low low moderate high
busby et al. [34] moderate moderate low moderate high high
kalichman et al. [39] moderate low moderate moderate high high
kalichman et al. [40] high high moderate low moderate high
hoeven et al. [36] moderate low moderate low moderate moderate
haugen et al. [37] high high moderate moderate moderate high
marshall et al. [38] moderate high low moderate low high
rheumatology international
1 3
table 3 potential risk factors for the progression of fnger interphalangeal joint osteoarthritis, assessed using a best evidence synthesis
consistent evidence for a risk factor consistent evidence for not being a risk factor mixed evidence
strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence
using all defnitions of ipj osteoarthritis progression
diabetes/impaired
fasting glucose
[38]
higher alcohol
intake [37, 39]
older age in men
[33–35, 39]
a
larger epiphyseal
index in females
[40]
anthropometric
features [39]
older age in women
[39]
a
larger epiphyseal
index in males
[40]
atherosclerosis
[36]
larger bmi—at
age 25 years
[37]
larger bmi—current [37, 38]
dyslipidaemia
[38]
familial relationship [39]
gender (female)
[39]
gender (male)
[39]
hypertension [38]
higher number of
metabolic factors [38]
smoking [37, 39]
larger waist
circumference
[37]
in dipjs only
older age in
women [39]
higher alcohol
intake [39]
gender (female)
[39]
anthropometric
features [39]
older age in men
[34, 39]
atherosclerosis
[36]
familial relationship [39]
gender (male)
[39]
in pipjs only
larger epiphyseal
index in females
[40]
higher alcohol
intake [39]
older age in men
[33–35, 39]
a
larger epiphyseal
index in males
[40]
anthropometric
features [39]
older age in women
[39]
atherosclerosis
[36]
rheumatology international
1 3
there is no unifed method to measure the progression of
hand osteoarthritis, and ipj osteoarthritis is now considered
to be a diferent disease subset from frst cmcj osteoarthritis. as ipj osteoarthritis progresses, it can be treated surgically, and there are currently no disease-modifying drugs.
risk factors which increase the chance of ipj osteoarthritis
progression in patients have been studied in the literature.
we identifed eight studies (seven high risk of bias) investigating potential risk factors for the progression of fnger ipj
osteoarthritis [37–44]. all studies measured osteoarthritis
progression radiographically, using a version of the kl classifcation system [45, 46]. our review found that patients
with diabetes/ifg [42], and both male and females with a
larger fnger ei [44], are at increased risk of ipj osteoarthritis progression (limited evidence), whilst older age in men
[37–39, 43] and in women [43] showed mixed evidence.
results were largely similar when dipj and pipj osteoarthritis when assessed separately.
the kl classifcation system [45, 46] was used to measure osteoarthritis progression by all studies [37–44, 49]. the
kl classifcation system [45, 46] is a sensitive method for
measuring the progression of radiographic hand osteoarthritis over a 1-year time frame [20]. all of the studies included
in this review were longitudinal studies, and the shortest
follow-up period had a mean of 2.28 years [39]. therefore,
all studies would have adequately detected any radiographic
ipj osteoarthritis progression. however, the defnitions of
each measure of progression varied across studies. some
studies measured an increase in kl grade [37–40], whilst
others measured it as an increase in the number of joints
with a particular kl grade [38, 40, 42, 43] and still other
studies measured it as an increase in a cumulative kl sum
score [41, 42, 44] (which is dependent on either an increase
in kl grade of already afected joints, or an increase in the
number of joints with a particular kl grade). the sensitivity of the kl classifcation system [45, 46] in detecting ipj
osteoarthritis progression measured in these diferent ways
has not yet been investigated. additionally, potential risk
factors that occur at a localised joint level (such as joint
trauma) could also be risk factors for isolated ipj osteoarthritis progression. however, localised risk factors were
not assessed by studies in this review. further research is
required to understand whether there are any joint-specifc
risk factors for ipj osteoarthritis progression, and whether
these might cause osteoarthritis to progress at one joint independently of other ipjs.
diabetes/ifg was found to be a risk factor for ipj osteoarthritis progression [42]. marshall et al. suggest diabetes/
ifg might be a risk factor for the progression of osteoarthritis due to hyperglycaemia [42]. hyperglycaemia has been
shown to induce reactive oxygen species and the production
of cytokines, which result in joint infammation and in the
production of proteolytic enzymes that degrade cartilage
[50]. however, in a delphi study consisting of a panel of
hand surgeons, the use of diabetic medication and abnormal
fasting glucose were not identifed as risk factors for fnger
ipj osteoarthritis progression [51]. this suggests that though
diabetes/ifg might have a relationship with ipj osteoarthritis on a molecular level, in a clinical context the efect is not
yet well recognised. our results also found that larger fnger
ei is a risk factor for ipj osteoarthritis progression [44].
in hip and knee osteoarthritis, larger cross-sectional areas
in the femoral neck and proximal femoral shaft and in the
tibial plateau, respectively, have also been described [52,
53]. additionally, in knee osteoarthritis, a loss of articular
cartilage coupled with larger bone epiphyseal area results
in a change of loading and force across a joint, further contributing to the progression of osteoarthritis [54]. however,
in the hands, and particularly the fnger ipjs, the load across
the joint is much lower, suggesting that there might be other
mechanisms which contribute to the relationship between
ei and ipj osteoarthritis progression. given the limited evidence reported in our systematic review, further high-quality
studies are needed to assess this relationship.
table 3 (continued)
consistent evidence for a risk factor consistent evidence for not being a risk factor mixed evidence
strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence
familial relationship [39]
gender (female)
[39]
gender (male)
[39]
smoking [39]
bmi body mass index, dipj distal interphalangeal joints, ipj interphalangeal joint, pipj proximal interphalangeal joints
a

<|EndOfText|>

sample size considerations for the
external validation of a multivariable
prognostic model: a resampling study
after developing a prognostic model, it is essential to evaluate the performance of the model in samples independent from those used to develop the model, which is often referred to as external validation. however, despite its importance, very little is known about the sample size requirements for conducting an external validation. using a
large real data set and resampling methods, we investigate the impact of sample size on the performance of six published prognostic models. focussing on unbiased and precise estimation of performance measures (e.g. the c-index,
d statistic and calibration), we provide guidance on sample size for investigators designing an external validation
study. our study suggests that externally validating a prognostic model requires a minimum of 100 events and
keywords: prognostic model; sample size; external validation
1. introduction
prognostic models are developed to estimate an individual’s probability of developing a disease or outcome in the future. a vital step toward accepting a model is to evaluate its performance on similar individuals separate from those used in its development, which is often referred to as external validation or
transportability [1,2]. however, despite the widespread development of prognostic models in many areas
of medicine [3–5], very few been externally validated [6–8].
to externally validate a model is to evaluate its predictive performance (calibration and discrimination)
using a separate data set from that used to develop the model [9]. it is not repeating the entire modelling
process on new data, refitting the model to new ‘validation’ data, or fitting the linear predictor (prognostic
index) from the original model as a single predictor to new data [9]. it is also not necessarily comparing the
similarity in performance to that obtained during the development of the prognostic model. whilst in some
instances a difference in the performance can be suggestive of deficiencies in the development study, the
performance in the new data may still be sufficiently good enough for the model to be potentially useful.
the case-mix (i.e., the distribution of predictors included in the model) will influence the performance
of the model [10]. it is generally unlikely that the external validation data set will have an identical casemix to the data used for development. indeed, it is preferable to use a slightly different case-mix in
external validation to judge model transportability. successful external validation studies in diverse
settings (with different case-mix) indicate that it is more likely that the model will be generalizable to
plausibly related, but untested settings [11].
despite the clear importance of external validation, the design requirements for studies that attempt to
evaluate the performance of multivariable prognostic models in new data have been little explored
[7,12,13]. published studies evaluating prognostic models are often conducted using sample sizes that
are clearly inadequate for this purpose, leading to exaggerated and misleading performance of the
prognostic model [7]. finding such examples is not difficult [14–16]. for example, a modified
thoracoscore, to predict in-hospital mortality after general thoracic surgery, was evaluated using 155
patients, but included only eight events (deaths). a high c-index value was reported, 0.95 (95% confidence interval 0.91 to 0.99) [16]. in the most extreme case, a data set with only one outcome event
was used to evaluate a prognostic model [14]. in this particular study, an absurd value of the c-index
was reported, 1.00 (95% confidence interval 1.00 to 1.00)[sic]. concluding predictive accuracy, and thus
that the model is fit for purpose, on such limited data is nothing but misleading.
the only guidance for sample size considerations that we are aware of is based on a hypothesis testing
framework (i.e. to detect pre-specified changes in the c-statistic) and recommends that models developed
using logistic regression are evaluated with a minimum of 100 events [12]. however, a recent systematic
review evaluating the methodological conduct of external validation studies found that just under half of
the studies evaluated models on fewer than 100 events [7].
it is therefore important to provide researchers with appropriate guidance on sample size considerations when evaluating the performance of prognostic models in an external validation study. when
validating a prognostic model, investigators should clearly explain how they determined their study size,
so that their findings can be placed in context [17,18]. our view is that external validation primarily
concerns the accurate (unbiased) estimation of performance measures (e.g., the c-index). it does not necessarily include formal statistical hypothesis testing, although this may be useful in some situations.
therefore sample size considerations should be based on estimating performance measures that are sufficiently close to the true underlying population values (i.e., unbiased) along with measures of uncertainty that are sufficiently narrow (i.e., precise estimates) so that meaningful conclusions on the
model’s predictive accuracy in the target population can be drawn [9,19].
the aim of this article is to examine sample size considerations for studies that attempt to externally
validate prognostic models and to illustrate that many events are required to provide reasonable estimates of model performance. our study uses published prognostic models (qrisk2 [20], qdscore
[21] and the cox framingham risk score [22]) to illustrate sample size considerations using a resampling
design from a large data set (>2 million) of general practice patients in the uk.
the structure of the paper is as follows. section 2 describes the clinical data set and the prognostic
models. section 3 describes the design of the study, the assessment of predictive performance and the
methods used to evaluate the resampling results. section 4 presents the results from the resampling
study, which are then discussed in section 5.
2. data set and prognostic models
2.1. study data: the health improvement network
the health improvement network (thin) is a large database of anonymized primary care records
collected at general practice surgeries around the uk. the thin database currently contains medical
records on approximately 4% of the uk population. clinical information from over 2 million individuals
(from 364 general practices) registered between june 1994 and june 2008 form the data set. the data
have previously been used in the external validation of a number of prognostic models (including those
considered in this study) [23–30]. there are missing data for various predictors needed to use the prognostic models. for simplicity, we have used one of the imputed data sets from the published external
validation studies, where details on the imputation strategy can be found [23,24].
2.2. prognostic models
at the core of the study are six sex-specific published models for predicting the 10-year risk of developing cardiovascular disease (cvd) (qrisk2 [20], and cox framingham [22]) and the 10-year risk of developing type 2 diabetes (qdscore [21]). all six prognostic models are all predicting time-to-event
outcomes using cox regression. none of these models were developed using thin, but thin has previously been used to evaluate their performance in validation studies [23,24].
qrisk2 was developed using 1.5 million general practice patients aged between 35 and 74 years
(10.9 million person years of observation) contributing 96 709 cardiovascular events from the
qresearch database [20]. separate models are available for women (41 042 cvd events) and
men (55 667 cvd events), containing 13 predictors, 8 interactions and fractional polynomial terms
for age and body mass index (www.qrisk.org).
215
cox framingham was developed using 8491 framingham study participants aged 30 to 74 years contributing 1274 cardiovascular events [22]. separate models are available for women (456 cvd events)
and men (718 cvd events), each containing 7 predictors.
qdscore was developed on 2.5 million general practice patients aged between 25 and 79 years (16.4
million person years of observation) contributing 72 986 incident diagnoses of type 2 diabetes from the
qresearch database [21]. separate models are available for women and men, each containing 12 predictors, 3 interactions and fractional polynomial terms for age and body mass index (www.qdscore.org).
3. methods
3.1. resampling strategy
a resampling strategy was applied to examine the influence of sample size (more specifically, the number of events) on the bias and precision in evaluating the performance of published prognostic models.
samples were randomly drawn (with replacement) from the thin data set so that the number of
events in each sample was fixed at 5, 10, 25, 50, 75, 100, 150, 200, 300, 400, 500 or 1000 by stratified
sampling according to the outcome ensuring that the proportion of events in each sample was the same
as the overall proportion of events in the thin data set (table i). the sample sizes for each prognostic
model at each value of number of events can be found in the supporting information. for each scenario
(i.e., for each sample size), 10 000 samples (denoted b) were randomly drawn and performance measures were calculated for each sample.
3.2. performance measures
the performance of the prognostic models was quantified by assessing aspects of model discrimination
(the c-index [31] and d statistic [32]), calibration [9,33], and other performance measures (r2
d [34], r2
oxs
[35] and the brier score for censored data [36]).
discrimination is the ability of a prognostic model to differentiate between people with different outcomes, such that those without the outcome (e.g., alive) have a lower predicted risk than those with the
outcome (e.g., dead). for the survival models used within this study, which are time-to-event based,
discrimination is evaluated using harrell’s c-index, which is a generalization of the area under the
receiver operating characteristic curve for binary outcomes (e.g., logistic regression) [31,37]. harrell’s
c-index can be interpreted as the probability that, for a randomly chosen pair of patients, the patient
who actually experiences the event of interest earlier in time has a lower predicted value. the c-index
and its standard error were calculated using the rcorr.cens function in the rms library in r.
we also examined the d statistic, which can be interpreted as the separation between two survival
curves (i.e., a difference in log hr) for two equal size prognostic groups derived from cox regression
[32]. it is closely related to the standard deviation of the prognostic index (pi = β1x1+ β2x2+⋯+ βkxk),
which is a weighted sum of the variables (xi) in the model, where the weights are the regression coefficients (βi). d is calculated by ordering the values from the prognostic index, transforming them using
expected standard normal order statistics, dividing the result by κ ¼ ffiffiffiffiffiffiffi
8=π p ≃1:596 and fitting this in a
single term cox regression. d and its standard error are given by the coefficient and standard error in
the single term cox regression model.
table i. ‘true’ values based on the entire thin validation cohort.
number of
individuals
number of
events (%)
performance measure
c-index d statistic r2
d ρ2
oxs brier
score
calibration
slope
qrisk2 [20,51] women 797,373 29,507 (3.64) 0.792 1.650 0.394 0.668 0.052 0.948
men 785,733 42,408 (5.40) 0.775 1.530 0.359 0.607 0.075 1.000
cox framingham [22] women 797,373 29,507 (3.64) 0.756 1.435 0.330 0.553 0.055 0.919
men 785,733 42,408 (5.40) 0.759 1.452 0.335 0.554 0.084 1.001
qdscore [21,23] women 1,211,038 32,200 (2.66) 0.810 1.872 0.456 0.731 0.041 0.875
men 1,185,354 40,786 (3.44) 0.800 1.760 0.425 0.687 0.053 0.869
216
the calibration slope was calculated by estimating the regression coefficient in a cox regression
model with the prognostic index (the linear predictor) as the only covariate. if the slope is <1, discrimination is poorer in the validation data set (regression coefficients are on average smaller than the development data set), and conversely, it is better in the validation data set if the slope is >1(regression
coefficients are on average larger than the development data set) [9,33]. we also examined the calibration of the models over the entire probability range at a single time point (at 10 years) using the val.surv
function in the rms library in r, which implements the hare function from the polspline package for
flexible adaptive hazard regression [38,39]. in summary, for each random sample, hazard regression
using linear splines are used to relate the predicted probabilities from the models at 10 years to the
observed event times (and censoring indicators) to estimate the actual event probability at 10 years as
a function of the estimate event probability at 10 years. to investigate the influence of sample size on
calibration, for each event size, plots of observed outcomes against predicted probabilities were drawn
and overlaid for each of the 10 000 random samples.
we examined two r2
-type measures [40,41] (explained variation [32] and explained randomness
[35]) and the brier score [42]. royston and sauerbrei’s r2
d is the proportion of the that is explained
by the prognostic model [32,34] and is given by
r2
d ¼ d2=κ2
σ2 þ d2=κ2
where d is the value of the d statistic [32], σ2= π2
/6≃1.645 and κ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
8=π≃1:596 p . the measure of
explained randomness, ρ2
k of o’quigley et al. [35] is defined as
ρ2
oxs ¼ 1  exp 2
k lβ  l0
   
where k is the number of outcome events, and lb and l0 are the log partial likelihoods for the prognostic
model and the null model respectively. standard errors of ρ2
k were calculated using the nonparametric
bootstrap (200 bootstrap replications).
the brier score for survival data is a measure of the average discrepancy between the true disease
status (0 or 1) and the predicted probability of developing the disease [36,43], defined as a function
of time t>0:
bs tð þ ¼ 1
n ∑
n
i¼1
s t ^ð þ jxi
2
i tð þ i ≤ t; δi ¼ 1
ĝ tð þi
þ
1  s t ^ð þ jxi
 2
i tð þ i > t
ĝð þt
" #
where ŝ(· |xi) is the predicted probability of an event for individual i; ĝ is the kaplan–meier estimate of
the censoring distribution, which is based on the observations (ti, 1δi),δi is the censoring indicator and
i denotes the indicator function. [36,43,44]. the brier score is implemented in the function sbrier from
the package ipred in r.
3.3. evaluation
the objective of our study was to evaluate the impact of sample size (more precisely the number of
events) on the accuracy, precision and variability of model performance. we examined the sample size
requirements using the guidance by burton et al. [45]. we calculated the following quantities for each of
the performance measures over the b simulations (defined in the preceding section):
• percentage bias, which is the relative magnitude of the raw bias to the true value, defined as
^θ
-
 θ

=θ

.
• standardized bias, which is the relative magnitude of the raw bias to the standard error, defined as
^θ
-
 θ

=se ^θ
 	 
. a standardized bias of 25 percent implies that the estimate lies one quarter of
a standard error below the true value.
• root mean square error, which incorporates both measures of bias and variability of the estimate,
defined as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1
b ∑
b
i¼1
^θi  θ
	 
2
s .
217
• estimated coverage rate of the 95% confidence interval for the c-index, d statistic, r2
d and ρ2
oxs, which
indicate the proportion of times that a confidence interval contains the true value (θ). an acceptable
coverage should not fall outside of approximately two standard errors of the nominal coverage probability ð þp ; se pð þ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
pð þ 1  p =b p [46].
• average width of the confidence interval, defined as 1=b ∑
b
i¼1
2z1α=2se ^θi
  	 
 .
the true values (θ) of the performance measures were obtained using the entire thin data set for each
model (table i). ^θ
-
¼ ∑
b
i¼1
^θi=b, where b is the number of simulations performed and ^θi is the performance
measure of interest for each of the i = 1,…,b= 10 000 simulations. the empirical standard error, se ^θ
	 
,
is the square root of the variance of over all b-simulated ^θ values. if, for the d statistic andr2
d, the modelbased standard error is valid, then its mean over the 10 000 simulations should be close to the empirical
standard error se ^θ
	 
.
4. result
figure 1 presents the empirical values, with boxplots overlaid, for the c-index, d statistic, r2
d, ρ2
oxs brier
score and calibration slope for qrisk2 (women), describing pure sampling variation. as expected, considerable variation in the sample values for each of the six performance measures are observed when the
number of events is small. thus, inaccurate estimation of the true performance is more likely in studies
with low numbers of events.
the mean percent bias, standardized bias and rmse of the performance measures are displayed
graphically in figure 2. for all of the models, the mean percent bias of both the c-index and brier score
are within 0.1% when the number of events reaches 50. at 50 events, the average bias of the d statistic,
r2
d and calibration slope is within 2% of the true value. the mean standardized bias for all of the models
and performance measures drops below 10% once the number of events increases to 75–100.
because of the skewness in bias at small values of number of events, the median percent bias and standardized bias of the performance measures are also presented (supporting information). for all of the
performance measures, the median bias drops below 1% as the number of events reaches 100. similarly,
figure 1. empirical performance of qrisk2 (women), measured using the c-index, d statistic, r2
d, ρ2
oxs, brier
score and calibration slope.
218
the median standardized bias drops below 10% for all of the performance measures and models when the
number of events approaches 100.
as expected, the rmse decreases as the number of events increases for all six performance measures
(figure 2). the same pattern is observed for all six prognostic models.
coverage of the confidence intervals for the c-index, d statistic and r2
d are displayed in figure 3.
acceptable coverage of the c-index at the nominal level of 95 percent is achieved as the number of
events approaches and exceeds 200. however, the d statistic confidence interval exhibits over-coverage
regardless of sample size. there is under-coverage of r2
d at less than 25 events and over-coverage as the
number of events increases (for four of the six prognostic models examined). the mean widths of the
95% confidence intervals for all of the models are displayed in figure 3. a steep decrease is observed
in the mean width for all models as the number of events approaches 50–100. within this range, the
decrease in mean width becomes smaller with more events. a similar pattern is observed in the width
variability, as shown in figure 4 for qrisk2 (women).
figure 2. mean percent, standardized bias and rmse of the c-index, d statistic, r2
d, ρ2
oxs, brier score and calibration slope.
219
the effect of sample size on the performance of the hazard regression assessment of calibration of
qrisk2 (women) is described in figure 5. for each panel (i.e., each event size), 10 000 calibration lines
have been plotted and a diagonal (dashed) line going through the origin with slope 1 has been
superimposed, which depicts perfect calibration. furthermore, we have overlaid a calibration line using
the entire thin data set to judge convergence of increasing event size. for data sets with 10 or fewer
numbers of events, the ability to assess calibration was poor. for predicted probabilities greater than
0.2, there was modest to substantial variation between the fitted calibration curves, which decreased
as the number of events increased. the calibration line (blue line) using the entire thin data set shows
overestimation towards the upper tail of the distribution, whilst some overestimation is captured, from
event sizes in excess of 100, the true magnitude of overestimation in using qrisk2 (women) in the
thin data set is not fully captured even when the number of events reach 1000. calibration plots for
two of the five prediction models (qrisk2 men and cox framingham women) show similar patterns,
figure 3. coverage rates and 95% confidence interval widths for the c-index, d statistic, r2
d, ρ2
oxs and calibration
slope. [bootstrap standard errors for ρ2
oxs based on 1000 simulations and 200 bootstrap replications].
220
whilst for the remaining three models accurate assessment of calibration is achieved when the number of
events reach 100 (data not shown).
figure 6 displays the proportion of simulations in which the performance estimates are within 0.5, 2.5,
5 and 10% of the true performance measure as the number of events increases. fewer events are required
to obtain precise estimates of the c-index than of the other performance measures. for example, at 100
events, over 80% of simulations yield estimates of the c-index within 5% of the true value and over 60%
of simulations yield values within 2.5% of the true value. considerably more events are required for the
d statistic, r2
d, brier score and calibration slope.
4.1. additional analyses
as observed in figure 3, coverage of the d statistic is larger than the nominal 95% level regardless of
the number of events. similarly, r2
d coverage tends to be larger than the nominal 95% level as the
number of events increases. therefore, we carried out further analyses to investigate the model-based
standard error and the nonparametric bootstrap standard error of the d statistic and r2
d [47]. the
results are shown in table ii.
figure 4. width of the 95% confidence interval of the c-index, d statistic r2
d , ρ2
oxs and calibration slope
(qrisk2 women). [bootstrap standard errors for ρ2
oxs based on 1000 simulations and 200 bootstrap
replications].
221
the results from the additional simulations indicate that the model-based standard error is
overestimated. there is good agreement between the empirical and bootstrap standard errors, with coverage using the bootstrap standard errors close to the nominal 95 percent (table iii).
5. discussion
external validation studies are a vital step in introducing a prognostic model, as they evaluate the performance and transportability of the model using data that were not involved in its development
[2,48]. the performance of a prognostic model is typically worse when evaluated on samples independent of the sample used to develop the model [49]. therefore, the more external validation studies that
demonstrate satisfactory performance, the more likely the model will be useful in untested populations,
and ultimately, the more likely it will be used in clinical practice. however, despite their clear importance, multiple (independent) external validation studies are rare. many prognostic models are only subjected to a single external validation study and are abandoned if that study gives poor results. other
investigators then proceed in developing yet another new model, discarding previous efforts, and the
cycle begins again [2]. however, systematic reviews examining methodological conduct and reporting
have shown that many external validation studies are fraught with deficiencies, including inadequate
sample size [7,49]. the results from our study indicate that small external validation studies are unreliable, inaccurate and possibly biased. we should avoid basing the decision to discard or recommend a
prognostic model on an external validation study with a small sample size.
an alternative approach that could be used to determine an appropriate sample size for an external
validation study is to focus on the ability to detect a clinically relevant deterioration in model performance [12]. whilst this approach may seem appealing, it requires the investigator to pre-specify a
performance measure to base this decision on and to justify the amount of deterioration that will indicate
a lack of validation. neither of these conditions are necessarily straightforward, particularly when the
figure 5. calibration plots for qrisk2 (women). the red dashed line denoted perfect prediction. the blue line
is the model calibration using the entire data set.
222
case-mix is different or the underlying population in the validation data set is different to that from
which the model was originally developed [50]. we take the view that a single external validation is generally insufficient to warrant widespread recommendation of a prognostic model. the case-mix in a
development sample does not necessarily reflect the case-mix of the intended population for which
the model is being developed, as studies developing a prognostic model are rarely prospective and
typically use existing data collected for an entirely different purpose. a prognostic model should be
evaluated on multiple validation samples with different case-mixes from the sample used to develop
the model, thereby allowing a more thorough investigation into the performance of the model, possibly
using meta-analysis methods.
a strength of our study is the use of large data sets, multiple prognostic models and evaluating seven
performance measures (c-index, d statistic, r2
d, ρ2
oxs, brier score, calibration slope and calibration plots).
figure 6. proportion of estimates within 0.5, 2.5, 5, 1 and 0% of the true value for qrisk2 (women).
223
we also showed that the analytical standard error for the d statistic (and r2
d) are too large, but could be
rectified by calculating bootstrap standard errors.
fundamental issues in the design of external validation studies have received little attention. existing
studies examining the sample size requirements of multivariable prognostic models have focused on
models developed using logistic regression [12,13]. adopting a hypothesis testing framework,
vergouwe and colleagues suggested that a minimum of 100 events and 100 non-events are required for
external validation of prediction models developed using logistic regression [12]. peek and colleagues
examined the influence of sample size when comparing multiple prediction models, including examining
the accuracy of performance measures, and concluded that a substantial sample size is required [13]. our
study took the approach that the sample size of an external validation study should be guided by the
premise of producing accurate and precise estimates of model performance that reasonably reflect the true
underlying population estimate. despite the differences taken in approach, our recommendations coincide.
our study focused on prognostic models predicting time-to-event outcomes, whilst we don’t expect any
discernable differences, further studies are required to evaluate models predicting binary events. we suggest that externally validating a prognostic model requires a minimum of 100 events, preferably 200 or
more events.