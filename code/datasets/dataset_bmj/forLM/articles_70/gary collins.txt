statistical issues in the development of covid‚Äê19 prediction
models
to the editor,
clinical prediction models to aid diagnosis, assess disease severity, or
prognosis have enormous potential to aid clinical decision making
during the coronavirus disease 2019 (covid‚Äê19) pandemic. a living
systematic review has, so far, identified 145 covid‚Äê19 prediction
models published (or preprinted) between 3 january and 5 may
2020. despite the considerable interest in developing covid‚Äê19
prediction models, the review concluded that all models to date, with
no exception, are at high risk of bias with concerns related to data
quality, flaws in the statistical analysis, and poor reporting, and none
are recommended for use.1 disappointingly, the recent study by yang
et al2 describing the development of a prediction model to identify
covid‚Äê19 patients with severe disease is no different. the study has
failed to report important information needed to judge the study
findings, but numerous methodological problems are apparent.2
our first point relates to the sample size. the sample size requirements in a prediction model study are largely influenced by the
number of individuals experiencing the event to be predicted (in
yang's study, those with mild covid‚Äê19 disease, as this is the smaller
of the two outcome categories). using published sample size formulae for developing prediction models,3 based on information reported in the yang study (40 predictors, outcome prevalence of
0.489), the minimum sample size in the most optimistic scenario
would be 538 individuals (264 events). to precisely estimate the
intercept alone requires 384 individuals (188 events). the study by
yang included 133 individuals, where 65 had the outcome of mild
disease, substantially lower than required.
developing a prediction model with a small sample size and a
large number of predictors will result in a model that is overfit, including unimportant or spurious predictors, and overestimating the
regression coefficients. this means that the model will appear to fit
the data (used in its development) too well‚Äîleading to a model that
has poor predictive accuracy in new data. an important step in all
model development studies is to carry out an internal validation of
the model building process (using either bootstrapping or cross‚Äê
validation), whereby the overestimation in regression coefficients can
be determined and shrunk as well as estimating the optimism in
model performance.4 this important step is absent in the study of
yang, who reported an area under the curve of 0.8842 in the same
data used to develop their model‚Äîthis will almost certainly be substantially overestimated.
another concern is the actual model. the final model contains seven
predictors and the authors have fully reported this permitting individualized prediction. however, an obvious and major concern is the
regression coefficient reported for procalcitonin, with a value of
48.8309 and accompanying odds ratio with a confidence interval of
‚Äú>999.999 (>999.999, >999.999)‚Äù (sic). this is clearly nonsensical, and to
put it bluntly, makes the model unusable. the reason for the large regression value (standard error and confidence interval) is due to an issue
called separation.
5 this occurred because there was little or no overlap in
the procalcitonin values between individuals with mild and severe disease. the statistical software used by the authors, sas, will report odds
ratios as greater than 999 when this occurs. instead of retaining this in
the model as is, one preferred approach would be to use firth's correction, available in both sas and r5. the authors used the model to
develop an early warning score‚Äîthis score has not been presented by
the authors‚Äîand we caution against such an approach with a preference for alternative formats that permit estimation of absolute risk.6
other concerns include the handling of missing data. while the
authors mention discarding observed values with more than 20%
missing‚Äîit is unclear whether individuals were omitted, or whether
entire predictors were omitted. regardless, one can only assume a
complete‚Äêcase analysis was conducted in preference for more suitable approaches using multiple imputations.7 finally, we note the
use of univariate screening, whereby predictors are omitted based on
the lack of statistical association. this approach is largely discredited,
as predictors can be spuriously retained or omitted.8
we urge the authors and other investigators developing
(covid‚Äê19) prediction models to read the transparent reporting of a
multivariable prediction model for individual prognosis or diagnosis
(tripod) statement (www.tripod-statement.org) for key information
to report when describing their study so that readers have the
minimal information required to judge the quality of the study.9 the
accompanying tripod explanation and elaboration paper describes
the rationale of the importance of transparent reporting, examples of
good reporting, but also discusses methodological considerations.10
until improved methodological standards are adopted, we should not
expect prediction models to benefit patients, and should consider the
possibility that they might do more harm than good.


<|EndOfText|>

risk factors for the progression of fnger interphalangeal joint
osteoarthritis: a systematic review
abstract
progressive hand interphalangeal joint (ipj) osteoarthritis is associated with pain, reduced function and impaired quality of
life. however, the evidence surrounding risk factors for ipj osteoarthritis progression is unclear. identifying risk factors for
ipj osteoarthritis progression may inform preventative strategies and early interventions to improve long-term outcomes for
individuals at risk of ipj osteoarthritis progression. the objectives of the study were to describe methods used to measure the
progression of ipj osteoarthritis and identify risk factors for ipj osteoarthritis progression. medline, embase, scopus,
and the cochrane library were searched from inception to 19th february 2020 (prospero crd42019121034). eligible
studies assessed potential risk factor/s associated with ipj osteoarthritis progression. risk of bias was assessed using a modifed quips tool, and a best evidence synthesis was performed. of eight eligible studies, all measured osteoarthritis progression radiographically, and none considered symptoms. eighteen potential risk factors were assessed. diabetes (adjusted
mean diference between 2.06 and 7.78), and larger fnger epiphyseal index in males (regression coefcient Œ≤=0.202) and
females (Œ≤=0.325) were identifed as risk factors (limited evidence). older age in men and women showed mixed results;
13 variables were not risk factors (all limited evidence). patients with diabetes and larger fnger epiphyseal index might
be at higher risk of radiographic ipj osteoarthritis progression, though evidence is limited and studies are biased. studies
assessing symptomatic ipj osteoarthritis progression are lacking.
keywords hand interphalangeal joint ¬∑ osteoarthritis ¬∑ risk factors ¬∑ disease progression
introduction
osteoarthritis is one of the leading causes of worldwide disability [1], and, in the usa alone, carries a cost of $10 billion just from economic loss [2]. hand osteoarthritis is one
of the most common types of radiographic osteoarthritis [1].
hand osteoarthritis also presents in a younger population
than osteoarthritis at other joints, with a prevalence of 3% in
men and 8% in women aged 45‚Äì64 years [3, 4]. it is considered a chronic disease, with some cases progressing and the
prevalence increasing to 5% in men and 9% in women aged
65‚Äì74 years [5]. symptomatic treatment for progressive
hand osteoarthritis is limited, with patients often requiring
surgical management, such as arthrodesis or arthroplasty [6].
measuring progressive hand osteoarthritis is difcult,
with no consensus for defning or quantifying worsening
of disease [7]. the osteoarthritis research society international (oarsi) 2006 task force described hand osteoarthritis progression as being joint specifc, whereby osteoarthritis
in one hand joint evolves independently from other hand
joints [8]. however, analysis from a large cohort study suggests there are patterns of symmetry, osteoarthritis clustering
by row (across distal interphalangeal joints (dipjs) or across
proximal interphalangeal joints (pipjs)), and clustering by
ray (within a fnger) also exist [9]. there is also poor correlation between radiographic and symptomatic disease [10, 11].
rheumatology international
conference abstract presentations some data from this
manuscript has been published as a conference abstract:
shah k et al (2020) a systematic review of risk factors and
diagnostic methods for hand interphalangeal joint osteoarthritis
progression. osteoarthr cartil 28(s86‚Äìs527): s426.
electronic supplementary material the online version of this
article (https://doi.org/10.1007/s00296-020-04687-1) contains
supplementary material, which is available to authorized users.
rheumatology international
1 3
the aetiology for the progression of hand osteoarthritis
is also poorly understood, and therefore identifying patients
at highest risk for needing surgical management is limited.
when managing hip and knee osteoarthritis surgically,
shared decision making between clinicians and patients has
been shown to be benefcial [12]. in the hand, a better understanding of whether a patient is at increased risk of progressive disease would help to inform shared decision making. in
particular, it would enable earlier investigations, more personalised treatment pathways, and targeted interventions for
prevention and treatment. these priorities have been highlighted by the recent commission on the future of surgery
[13]. similarly, being able to identify patients with osteoarthritis who will not progress will prevent the over-investigation and excessive medical treatment of these patients. a
review has found that abnormal scintigraphy scans, higher
australian/canadian hand osteoarthritis index (auscan)
scores, number of osteoarthritis joints at baseline, more pain,
and nodal osteoarthritis were risk factors for the progression
of radiographic or clinical hand osteoarthritis [14]. however,
this review combined interphalangeal joint (ipj) and base of
thumb [frst carpometacarpal joint (cmcj)] osteoarthritis
under the umbrella of ‚Äòhand osteoarthritis.‚Äô finger ipj and
frst cmcj osteoarthritis are now thought to be diferent
subsets of the disease, with diferent risk factors, pathophysiology and patterns of progression [15].
therefore, the primary aim of this systematic review is to
identify risk factors for the progression of fnger ipj osteoarthritis. the secondary aim is to describe the measurements
used to defne the progression of ipj osteoarthritis.
methods
the reporting of this systematic review followed the
preferred reporting items for systematic reviews and
meta-analysis (prisma) statement [16]. the protocol was prospectively registered on prospero [17]
(crd42019121034).
search strategy
the search strategy was constructed with the assistance
of a specialist health-care librarian. the search was conducted in four electronic databases: (1) medline by ovid,
(2) embase by ovid, (3) scopus, (4) the cochrane library.
the search string included a range of search terms for (1)
hands and fngers, (2) osteoarthritis, and (3) progression,
and was amended for each database (electronic supplementary material 1). the picos tool [18] was used to
frame the search strategy as follows: population: adults
with ipj osteoarthritis, intervention/prognostic factor:
potential risk factor(s) for ipj osteoarthritis progression,
comparison: no exposure to the risk factor(s), outcome:
progression of ipj osteoarthritis, study type: quantitative
methodology. the search was conducted on 17th october 2018 and duplicates were removed. the search was
updated on 19th february 2020. the reference lists of all
eligible articles were manually assessed for additional
studies. rayyan qcri tool was used to import all papers
[19].
two groups of reviewers (group 1: ks, group 2: xy and
jcel) independently screened titles and abstracts for eligibility. any articles with insufcient title or abstract information were referred for full text review. articles for which the
full text was not available were requested directly from the
authors. any disagreements in eligibility assessment was
resolved at a consensus meeting by a third reviewer (srf).
study eligibility criteria
studies were considered eligible if they (a) included participants with evidence of radiographic or clinical ipj osteoarthritis at baseline; (b) the participants were followed up
for at least 1 year (as it has been shown that progression
of radiographic hand osteoarthritis can be detected over
a 1 year time frame [20]); (c) ipj osteoarthritis (separate
from frst cmcj osteoarthritis) progression was measured
at follow-up, using radiographic and/or symptomatic criteria
(ipj osteoarthritis progression was defned as an increase
in radiographic or symptomatic criteria/score at follow-up
compared to baseline); (d) the association between a potential risk factor and the progression of ipj osteoarthritis was
investigated at follow-up.
case reports were excluded. letters to editors might
contain important information about studies, such as new
information or discussions of further weaknesses of original studies [21]. therefore, letters to editors which exist in
the context of original studies, included in our review, were
examined to inform the risk of bias assessment and as additional sources of information [22]. conference abstracts are
considered to have high variability in terms of data reliability, accuracy and detail, and therefore these were excluded
[21, 23].
studies of infammatory arthritis, erosive arthritis, with
participants under the age of 18 years (to avoid confounding
by juvenile arthritis), and studies where ipj osteoarthritis
results could not be separated from other joints including
the frst cmcj, and were not provided on request of the
corresponding author within 2 months were excluded. animal, cadaver, and cell studies were excluded. articles not
in english, and articles which lacked accessible full texts
(online or in paper copy throughout the uk, or after requesting them from the corresponding author with no reply within
2 months) were excluded.
rheumatology international
1 3
data extraction
one reviewer (ks) independently extracted participant
demographics (e.g. age and sex), study characteristics
(e.g. study design), the potential risk factor/s assessed,
effect measure and size/s and the definition/s used to
measure osteoarthritis progression. a potential risk factor
was defined as any factor investigated for an association
with ipj osteoarthritis progression.
if data was reported at multiple time points, results
from all time points were extracted. if articles or supplementary material did not contain sufficient data, the
corresponding author was contacted to request additional
data, with a 2 month turnaround policy. for any articles
which reported data from a study described in detail
elsewhere, the source of the data was retrieved and data
extracted as appropriate. data extraction was input into a
microsoft excel file and cross-checked by a second independent reviewer (xy).
risk of bias assessment
two independent reviewers (ks and xy) rated the risk of
bias of included studies using a modified version of the
quality in prognosis studies (quips) risk of bias tool
[24] (electronic supplementary material 2). the following five domains were assessed: (1) study participation,
(2) study attrition, (3) prognostic factor measurement, (4)
outcome measurement, (5) statistical analysis and reporting [24]. we excluded the domain assessing ‚Äòconfounding
factors‚Äô, as confounders can themselves be considered to
be prognostic factors, and thus the term ‚Äòconfounders‚Äô is
a misnomer in prognostic factor studies [25]. as there is
currently limited established literature in the field of ipj
osteoarthritis progression, any ‚Äòconfounder‚Äô identified in
the literature was treated as a potential risk factor for this
review. each domain was given an overall score of ‚Äòlow‚Äô,
‚Äòmoderate‚Äô or ‚Äòhigh‚Äô risk of bias (electronic supplementary material 2). the overall risk of bias of a study was
classified by examining the risk of bias in each of the
five domains. if one or more domains were classified as
having high risk of bias, then this study was classified as
having an overall high risk of bias [24, 26‚Äì28]. if three or
more domains were classified as having a moderate risk
of bias, then this study was classified as having an overall
moderate risk of bias [24, 26, 27, 29]. if all domains were
classified as having a low risk of bias, or less than three
domains had a moderate risk of bias, then this study was
classified as having an overall low risk of bias [28, 30].
any disagreement between reviewers was discussed at a
consensus meeting with a third reviewer (srf).
analysis and best evidence synthesis
risk factors for all defnitions of ipj osteoarthritis progression were identifed, followed by a subgroup analysis for
dipj and pipj separately. if studies were homogenous with
regard to study populations, potential risk factors assessed,
efect measures used, and measurements of ipj osteoarthritis progression, a pooled meta-analysis was considered
using review manager software [31], and the grading of
recommendations, assessment, development and evaluation (grade) approach was used to assess the quality
of evidence [32].
if studies were heterogeneous, we chose not to report
effect measures of different types and instead used a
qualitative narrative summary. the association between
a potential risk factor and ipj osteoarthritis progression
was categorised as:
(a) a risk factor: positive efect measure.
(b) not a risk factor: negative efect measure; or, no statistical association.
(c) conficting evidence: efect measures not in the same
direction.
a best evidence synthesis was used to summarise the
data for each potential risk factor assessed [33‚Äì36]. the
criteria were applied sequentially. if multiple analyses
were performed within one study, the consistent fndings
approach described below was applied to the study to
decide whether it showed consistent or mixed evidence.
this was then used to calculate the overall best evidence
synthesis across studies.
(a) consistent evidence:‚â• 75% of studies reported the
same direction of efect (either positive or negative/no
association).
(b) mixed evidence:<75% of analyses reported the same
direction of efect.
if consistent evidence was found, the strength of evidence was assessed:
(i) strong evidence:>2 studies with low risk of bias.
(ii) moderate evidence: 1 study with low risk of bias. and
1 other study; or:>2 studies with moderate or high
risk of bias.
(iii) limited evidence with low risk of bias: 1 study with
low risk of bias.
(iv) limited evidence:‚â§2 studies with moderate or high
risk of bias.
rheumatology international
1 3
results
studies included
combining results from the search in october 2018 and
the updated search in february 2020, 25, 739 titles were
identifed through the search strategy, with 13,346 remaining after removal of duplicates. after screening titles and
abstracts, the full text of 32 articles was evaluated, and eight
articles met the inclusion criteria (fig. 1). no additional articles were found by reviewing the reference lists of eligible
studies.
study characteristics
eight prospective cohort studies were included [37‚Äì44]
(table 1). five studies included men and women [40‚Äì44],
whilst three studies included only men [37‚Äì39]. the smallest
study included 177 participants [37], whilst the largest study
included 5560 participants [40]. the shortest follow-up
period was a mean of 2.28 years [39] and the longest followup was reported as a mean (standard deviation) of 23.5 (3.3)
years [37].
risk of bias
seven studies were rated as having overall high risk of bias
[37‚Äì39, 41‚Äì44], and one study was of moderate risk of bias
[40] (table 2). ‚Äòstudy participation‚Äô was of high risk of
bias in four studies due to studies not adequately reporting
recruitment periods and places of recruitment [37, 39, 41,
44]. in the ‚Äòstudy attrition‚Äô domain, plato et al. and kallman
et al. did not clearly report response rates and reasons for
participants with loss to follow-up [37, 39], whilst haugen
et al. and marshall et al. had less than 80% response rates
and also did not report reasons for loss to follow-up [41,
42]. when assessing the ‚Äòstatistical analysis and reporting‚Äô domain, it was found that plato et al., busby et al., and
kalichman et al. did not provide efect measures, but only
reported p values or stated whether results were ‚Äòsignifcant
or not signifcant‚Äô [38, 39, 43].
fig. 1 preferred reporting
items for systematic reviews
and meta-analysis (prisma)
fowchart of study selection
screening inclusion eligibilit
y idenficaon
records aer duplicates removed
(n = 13,346)
abstracts/tles screened
(n = 13,346)
records excluded
(n = 13,314)
full-text arcles assessed for
eligibility
(n = 32)
studies included in systemac
review
(n = 8)
combined records idenfied
through database searching on
17th oct 2018 and 19th feb 2020
(n= 25,739)
medline (n = 7,515),
embase (n = 8,474),
scopus (n = 8,391),
cochrane (n = 1,359)
full-text arcles excluded
(n = 24)
-ineligible study design (n=15)
-ineligible study outcome (n=9)
rheumatology international
1 3
table 1 characteristics of studies investigating risk factors for the progression of fnger interphalangeal joint osteoarthritis
authors population length of follow-up
(years)
age (years)
(mean)
female (%) inclusion criteria exclusion criteria n (n) criteria for ipj
oa progression
risk factor
assessed
plato et al. [35] white middle
class volunteers
participated in
the blsa in the
usa
group 1: 0‚Äì3, group 2:
4‚Äì7, group 3: 8‚Äì11,
group 4: 12‚Äì16
ns 0 ns ns 478 (ns) increase by‚â•1
grade from the
highest kl
[41] grade at
baseline in any
dipj
older age in men
kallman et al.
[33]
white middle
class volunteers who
participated in
the blsa in the
usa
‚â•20
(age<60 years);‚â•14
(age‚â•60 years)
ns 0 ns maximum kl
score (4) at
baseline (per
patient); not
specifed
177 (177) increase by‚â•1
grade from the
highest kl [41,
42] grade at
baseline in any
pipj
older age in men
busby et al. [34] white middle
class volunteers who
participated in
the blsa in the
usa
5‚Äì16.3 ns 0 ns joints with kl
score of 4 at
baseline
386 (ns) outcome 1:
increase by‚â•1
grade from the
highest kl
[42] grade at
baseline in any
ipj (dipj and
pipj assessed
separately)
outcome 2:
increase in
number of
ipjs with kl
[42] grade‚â•2
(dipj and pipj
assessed separately)
older age in men
kalichman et al.
[39]
chuvashians; village; randomly
recruited
8 men: 45.3,
women: 49.7
52 ns ns 263 (263) increase in
number of
ipjs with kl
[42] grade‚â•2
(dipj and pipj
assessed separately)
alcohol, anthropometric features,
familial relationship, gender
(female), older
age in men, older
age in women,
smoking
kalichman et al.
[40]
chuvashians; village; randomly
recruited
8 men: 47.4,
women: 50.9
46 ns bone disease,
amenorrhoea,
hormone
replacement
therapy, steroids
557 (513) increase by‚â•1
grade in a
cumulative kl
[38] sum score
(2nd, 3rd and
4th, pipjs)
epiphyseal index
(larger)
rheumatology international
1 3
blsa baltimore longitudinal study of aging, bmi body mass index, casha clinical assessment studies of the hand, cask clinical assessment studies of the knee, dipj distal interphalangeal joint, gp general practice, ipj interphalangeal joint, kl kellgren‚Äìlawrence atlas, pipj proximal interphalangeal joint, n number at baseline, n number at follow-up, ns not specifed, oa
osteoarthritis, usa united states of america, x-rays plain flm radiographs
table 1 (continued)
authors population length of follow-up
(years)
age (years)
(mean)
female (%) inclusion criteria exclusion criteria n (n) criteria for ipj
oa progression
risk factor
assessed
hoeven et al. [36] rotterdam 10 men: 67.5,
women: 68.6
58 ‚â•55 years, living
for‚â•1 year
in ommoord,
knee, hip, hand
x-rays
no x-rays,
rheumatoid,
fractures
5650 (2442) increase by‚â•1
kl [41] grade
in‚â•1 ipj, if‚â•1
ipj had kl
[41] grade‚â•2
at baseline
(dipj and pipj
assessed separately)
atherosclerosis
haugen et al. [37] usa; hospital
study sites
4 58.4 58 ns systemic infammatory arthritis,
bilateral end
stage knee oa,
inability to walk
without aids,
contraindication
to mri
994 (994) increase by‚â•1
grade in a
cumulative
modifed kl
[42, 53] sum
score (dipj and
pipj assessed
together)
alcohol (higher
intake), bmi
(higher)‚Äîat
age 25, bmi
(higher)‚Äîcurrent, smoking,
waist circumference (higher)
marshall et al.
[38]
from casha
and cask
cohorts; gp
community
7 60.5 60 age 50‚Äì69 years
at baseline,
reported hand
pain in last
month
infammatory
arthritis, all
hand joints
afected
with kl‚â•2
at baseline,
deaths/untraceable/address
unknown,
severe/terminal
illness
706 (388) outcome 1:
increase by‚â•1
grade in a
cumulative
kl [43] sum
score (dipj and
pipj assessed
together)
outcome 2:
increase in
number of
ipjs with kl
[43] grade‚â•2
(dipj and
pipj assessed
together)
bmi (higher)‚Äî
current, diabetes
type 2/impaired
fasting glucose,
dyslipidaemia,
hypertension,
number of
metabolic factors
(higher)
rheumatology international
1 3
measurements for the progression of fnger
interphalangeal joint osteoarthritis
all studies assessed osteoarthritis radiographically, using
a version of the kellgren and lawrence (kl) classifcation
[45, 46] (table 1). three studies measured ipj osteoarthritis progression as a‚â•1 grade increase from the highest kl
grade at baseline [37‚Äì39]; three studies measured it as an
increase in the total number of ipjs with kl grade‚â•2 [38,
42, 43]; one study measured progression as a‚â•1 grade kl
increase in a‚â•1 ipj [40]; and three studies measured it as‚â•1
grade increase in a cumulative kl sum score [41, 42, 44].
no studies measured osteoarthritis progression through a
deterioration in symptomatic scoring.
risk factors for the progression of fnger
interphalangeal joint osteoarthritis
eighteen potential risk factors were assessed, most commonly in one study only (efect measures shown in electronic supplementary material 3). for potential risk factors
assessed by more than one study, due to heterogeneity in the
defnitions of the risk factor/s, statistical tests, and osteoarthritis defnitions, a best evidence synthesis was performed.
three risk factors were identifed: diabetes type 2/impaired
fasting glucose (ifg) [42]; and larger epiphyseal index (ei)
in males [44], and in females [44] (all with limited evidence)
(table 3). older age in men [37‚Äì39, 43] and in women [43]
showed mixed results (table 3).
diabetes type 2/impaired fasting glucose
(ifg)
marshall et al. assessed diabetes type 2/ifg compared to not
having these conditions in a total of 474 participants [42]
(efect measures shown in electronic supplementary material 3). in a complete case analysis, these conditions were
associated with an increase by‚â•1 grade in a cumulative kl
[47] sum score for all ipjs [adjusted mean diference (95% confdence interval) 7.78 (1.13‚Äì14.43)] [42]. however, there was
no association following multiple imputation [4.50 (‚àí 0.26
to 9.25)] [42]. diabetes type 2/ifg was associated with an
increase in the number of ipjs with kl [47] grade‚â•2 following multiple imputation and complete case analysis [2.06
(0.25‚Äì3.87) and 3.35 (1.08‚Äì5.62), respectively] [42].
large fnger epiphyseal index (ei)
kalichman et al. investigated larger ei in 177 participants
[44] (electronic supplementary material 3). a positive
association was found in both males (multiple regression
coefcient, Œ≤=0.202; 95% ci not reported) and females
(Œ≤=0.325; 95% ci not reported), between larger ei and ipj
osteoarthritis progression (measured as an increase by‚â•1
grade in a cumulative kl [38] sum score for pipjs in the
assessed digits).
dipj and pipj subgroup analysis
in the dipj subgroup analysis, eight potential risk factors
were assessed, and only older age in women was found to be
a risk factor (correlation coefcient 0.20) [43] (limited evidence) (table 3). in the pipj subgroup analysis, 11 potential
risk factors were assessed, and larger ei in males (Œ≤=0.202;
95% ci not reported) and females (Œ≤=0.325; 95% ci not
reported) were identifed as risk factors [44] (limited evidence for both) (table 3).
discussion
osteoarthritis is one of the largest health-care burdens, and
radiographic hand osteoarthritis is highly prevalent, afecting
more than one out of fve adult americans [48]. osteoarthritis is considered to be progressive in some cases. however,
table 2 risk of bias for
studies assessing potential risk
factors for the progression of
fnger interphalangeal joint
osteoarthritis, assessed using a
modifed quality in prognosis
studies (quips) tool
a
biases from modifed quality in prognosis studies (quips) tool: (1) study participation; (2) study attrition; (3) prognostic factor measurement; (4) outcome measure; (5) statistical analysis and reporting
authors biasesa overall risk of bias
1 2 3 4 5
plato et al. [35] high high moderate moderate high high
kallman et al. [33] high high low low moderate high
busby et al. [34] moderate moderate low moderate high high
kalichman et al. [39] moderate low moderate moderate high high
kalichman et al. [40] high high moderate low moderate high
hoeven et al. [36] moderate low moderate low moderate moderate
haugen et al. [37] high high moderate moderate moderate high
marshall et al. [38] moderate high low moderate low high
rheumatology international
1 3
table 3 potential risk factors for the progression of fnger interphalangeal joint osteoarthritis, assessed using a best evidence synthesis
consistent evidence for a risk factor consistent evidence for not being a risk factor mixed evidence
strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence
using all defnitions of ipj osteoarthritis progression
diabetes/impaired
fasting glucose
[38]
higher alcohol
intake [37, 39]
older age in men
[33‚Äì35, 39]
a
larger epiphyseal
index in females
[40]
anthropometric
features [39]
older age in women
[39]
a
larger epiphyseal
index in males
[40]
atherosclerosis
[36]
larger bmi‚Äîat
age 25 years
[37]
larger bmi‚Äîcurrent [37, 38]
dyslipidaemia
[38]
familial relationship [39]
gender (female)
[39]
gender (male)
[39]
hypertension [38]
higher number of
metabolic factors [38]
smoking [37, 39]
larger waist
circumference
[37]
in dipjs only
older age in
women [39]
higher alcohol
intake [39]
gender (female)
[39]
anthropometric
features [39]
older age in men
[34, 39]
atherosclerosis
[36]
familial relationship [39]
gender (male)
[39]
in pipjs only
larger epiphyseal
index in females
[40]
higher alcohol
intake [39]
older age in men
[33‚Äì35, 39]
a
larger epiphyseal
index in males
[40]
anthropometric
features [39]
older age in women
[39]
atherosclerosis
[36]
rheumatology international
1 3
there is no unifed method to measure the progression of
hand osteoarthritis, and ipj osteoarthritis is now considered
to be a diferent disease subset from frst cmcj osteoarthritis. as ipj osteoarthritis progresses, it can be treated surgically, and there are currently no disease-modifying drugs.
risk factors which increase the chance of ipj osteoarthritis
progression in patients have been studied in the literature.
we identifed eight studies (seven high risk of bias) investigating potential risk factors for the progression of fnger ipj
osteoarthritis [37‚Äì44]. all studies measured osteoarthritis
progression radiographically, using a version of the kl classifcation system [45, 46]. our review found that patients
with diabetes/ifg [42], and both male and females with a
larger fnger ei [44], are at increased risk of ipj osteoarthritis progression (limited evidence), whilst older age in men
[37‚Äì39, 43] and in women [43] showed mixed evidence.
results were largely similar when dipj and pipj osteoarthritis when assessed separately.
the kl classifcation system [45, 46] was used to measure osteoarthritis progression by all studies [37‚Äì44, 49]. the
kl classifcation system [45, 46] is a sensitive method for
measuring the progression of radiographic hand osteoarthritis over a 1-year time frame [20]. all of the studies included
in this review were longitudinal studies, and the shortest
follow-up period had a mean of 2.28 years [39]. therefore,
all studies would have adequately detected any radiographic
ipj osteoarthritis progression. however, the defnitions of
each measure of progression varied across studies. some
studies measured an increase in kl grade [37‚Äì40], whilst
others measured it as an increase in the number of joints
with a particular kl grade [38, 40, 42, 43] and still other
studies measured it as an increase in a cumulative kl sum
score [41, 42, 44] (which is dependent on either an increase
in kl grade of already afected joints, or an increase in the
number of joints with a particular kl grade). the sensitivity of the kl classifcation system [45, 46] in detecting ipj
osteoarthritis progression measured in these diferent ways
has not yet been investigated. additionally, potential risk
factors that occur at a localised joint level (such as joint
trauma) could also be risk factors for isolated ipj osteoarthritis progression. however, localised risk factors were
not assessed by studies in this review. further research is
required to understand whether there are any joint-specifc
risk factors for ipj osteoarthritis progression, and whether
these might cause osteoarthritis to progress at one joint independently of other ipjs.
diabetes/ifg was found to be a risk factor for ipj osteoarthritis progression [42]. marshall et al. suggest diabetes/
ifg might be a risk factor for the progression of osteoarthritis due to hyperglycaemia [42]. hyperglycaemia has been
shown to induce reactive oxygen species and the production
of cytokines, which result in joint infammation and in the
production of proteolytic enzymes that degrade cartilage
[50]. however, in a delphi study consisting of a panel of
hand surgeons, the use of diabetic medication and abnormal
fasting glucose were not identifed as risk factors for fnger
ipj osteoarthritis progression [51]. this suggests that though
diabetes/ifg might have a relationship with ipj osteoarthritis on a molecular level, in a clinical context the efect is not
yet well recognised. our results also found that larger fnger
ei is a risk factor for ipj osteoarthritis progression [44].
in hip and knee osteoarthritis, larger cross-sectional areas
in the femoral neck and proximal femoral shaft and in the
tibial plateau, respectively, have also been described [52,
53]. additionally, in knee osteoarthritis, a loss of articular
cartilage coupled with larger bone epiphyseal area results
in a change of loading and force across a joint, further contributing to the progression of osteoarthritis [54]. however,
in the hands, and particularly the fnger ipjs, the load across
the joint is much lower, suggesting that there might be other
mechanisms which contribute to the relationship between
ei and ipj osteoarthritis progression. given the limited evidence reported in our systematic review, further high-quality
studies are needed to assess this relationship.
table 3 (continued)
consistent evidence for a risk factor consistent evidence for not being a risk factor mixed evidence
strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence strong
evidence
moderate
evidence
limited evidence
with low risk of
bias
limited evidence
familial relationship [39]
gender (female)
[39]
gender (male)
[39]
smoking [39]
bmi body mass index, dipj distal interphalangeal joints, ipj interphalangeal joint, pipj proximal interphalangeal joints
a

<|EndOfText|>

minimum sample size for developing a multivariable
prediction model: part ii - binary
and time-to-event outcomes
when designing a study to develop a new prediction model with binary or
time-to-event outcomes, researchers should ensure their sample size is adequate
in terms of the number of participants (n) and outcome events (e) relative to
the number of predictor parameters (p) considered for inclusion. we propose
that the minimum values of n and e (and subsequently the minimum number
of events per predictor parameter, epp) should be calculated to meet the following three criteria: (i) small optimism in predictor effect estimates as defined by
a global shrinkage factor of ‚â•0.9, (ii) small absolute difference of ‚â§ 0.05 in the
model's apparent and adjusted nagelkerke's r2, and (iii) precise estimation of
the overall risk in the population. criteria (i) and (ii) aim to reduce overfitting
conditional on a chosen p, and require prespecification of the model's anticipated cox-snell r2, which we show can be obtained from previous studies. the
values of n and e that meet all three criteria provides the minimum sample
size required for model development. upon application of our approach, a new
diagnostic model for chagas disease requires an epp of at least 4.8 and a new
prognostic model for recurrent venous thromboembolism requires an epp of at
least 23. this reinforces why rules of thumb (eg, 10 epp) should be avoided.
researchers might additionally ensure the sample size gives precise estimates of
key predictor effects; this is especially important when key categorical predictors have few events in some categories, as this may substantially increase the
numbers required.
keywords
binary and time-to-event outcomes, logistic and cox regression, multivariable prediction model,
pseudo r-squared, sample size, shrinkage
1 introduction
statistical models for risk prediction are needed to inform clinical diagnosis and prognosis in healthcare.1-3 for example,
they may be used to predict an individual's risk of having an undiagnosed disease or condition (‚Äúdiagnostic prediction
model‚Äù), or to predict an individual's risk of experiencing a specific event in the future (‚Äúprognostic prediction model‚Äù).
they are typically developed using a multivariable regression framework, such as logistic or cox (proportional hazards)
regression, which provides an equation to estimate an individual's risk based on their values of multiple predictors (such
as age and smoking, or biomarkers and genetic information). well-known examples are the wells score for predicting
the presence of a pulmonary embolism4,5
; the framingham risk score and qrisk2,6,7 which estimate the 10-year risk
of developing cardiovascular disease (cvd); and the nottingham prognostic index, which predicts the 5-year survival
probability of a woman with newly diagnosed breast cancer.8,9
researchers planning or designing a study to develop a new multivariable prediction model must consider sample size
requirements for their development data set. our related paper considered this issue for prediction models of a continuous
outcome using linear regression.10 here, we focus on binary and time-to-event outcomes, such as the risk of already having
a pulmonary embolism, or the risk of developing cvd in the next 10 years. in this situation, the effective sample size is
often considered to be the number of outcome events (eg, the number with existing pulmonary embolism, or the number
diagnosed with cvd during follow-up). in particular, a well-used ‚Äúrule of thumb‚Äù for sample size is to ensure at least
10 events per candidate predictor (variable),11-13 where ‚Äúcandidate‚Äù indicates a predictor in the development data set that
is considered, before any variable selection, for inclusion in the final model. note that, if a predictor is categorical with
three of more categories, or continuous and modelled as a nonlinear trend, then including the predictor will require two
or more parameters being included in the model. therefore, we refer to events per predictor parameter (epp) here, rather
than events per variable.
the 10 epp rule has generated much debate. some authors claim that the epp can sometimes be lowered below 10.14
in contrast, harrell generally recommends at least 15 epp,15 and others identify situations where at least 20 epp or up
to 50 epp are required.16-19 however, a concern is that any blanket rule of thumb is too simplistic, and that the number
of participants required will depend on many intricate aspects, including the magnitude of predictor effects, the overall
outcome risk, the distribution of predictors, and the number of events for each category of categorical predictors.16 for
example, courvoisier et al20 concluded that ‚Äúthere is no single rule based on epp that would guarantee an accurate
estimation of logistic regression parameters.‚Äù a new sample size approach is needed to address this.
in this article, we propose the sample size (n) and number of events (e) in the model development data set must, at
the very least, meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global
shrinkage factor of ‚â•0.9, (ii) small absolute difference of ‚â§ 0.05 in the model's apparent and adjusted nagelkerke's r2, and
(iii) precise estimation of the overall risk or rate in the population (or similarly, precise estimation of the model intercept
when predictors are mean centred). the values of n and e (and subsequently epp) that meet all three criteria provide the
minimum values required for model development. criteria (i) and (ii) aim to reduce the potential for a developed model to
be overfitted to the development data set at hand. overfitting leads to model predictions that are more extreme than they
ought to be when applied to new individuals, and most notably occurs when the number of candidate predictors is large
relative to the number of outcome events. a consequence is that a developed model's apparent predictive performance
(as observed in the development data set itself) will be optimistic, and its performance in new data will usually be lower.
therefore, it is good practise to reduce the potential for overfitting when developing a prediction model,15 which criteria
(i) and (ii) aim to achieve. in addition, criterion (iii) aims to ensure that the overall risk (eg, by a key time point for
prediction) is estimated precisely, as fundamentally, before tailoring predictions to individuals, a model must be able to
reliably predict the overall or mean risk in the target population.
the article is structured as follows. section 2 introduces our proposed criterion (i), for which key concepts of a global
shrinkage factor and the cox-snell r2 are introduced.21 the latter needs to prespecified to utilise our sample size formula,
and so in section 3, we suggest how realistic values of the cox-snell r2 can be obtained in advance of any data collection,
eg, by using published information from an existing model in the same field, including values of the c statistic or alternative r2 measures. extension to criteria (ii) and (iii) is then made in section 4. section 5 then provides two examples,
which demonstrate our sample size approach for diagnostic and prognostic models. section 6 raises a potential additional
criteria to consider: ensuring precise estimates of key predictor effects, to help ensure precise predictions across the entire
spectrum of predicted risk. section 7 concludes with discussion.
2 sample size required to minimise overfitting of predictor
effects
to adjust for overfitting during model development (and thereby improve the model's predictive performance in
new individuals), statistical methods for penalisation of predictor effect estimates are available, where regression
coefficients are shrunk toward zero from their usual estimated value (eg, from standard maximum likelihood
estimation).22-26 van houwelingen notes that ‚Äú ‚Ä¶ shrinkage works on the average but may fail in the particular unique
problem on which the statistician is working.‚Äù22 therefore, it is important to minimise the potential for overfitting during
model development, and this criterion forms the basis of our first sample size calculation. our approach is motivated
by the concept of a global shrinkage factor (a measure of overfitting), and so we begin by introducing this, before then
deriving a sample size formula.
2.1 concept of a global shrinkage for logistic and cox regression
the concept of shrinkage (penalisation) was outlined in our accompanying paper,10 and is explained in detail
elsewhere.1,15,27 here, we focus on using a global shrinkage factor (s), sometimes referred to as a uniform shrinkage
factor. consider a logistic regression model has been fitted using standard maximum likelihood estimation (ie, traditional
and unpenalised estimation). subsequently, s can be estimated (eg, using bootstrapping,28 or via a closed-form solution;
see section 2.2) and applied to the estimated predictor effects, so that the revised model is
ùëôùëõ ( pi
1 ‚àí pi
)
= ùõº‚àó + s
(
ùõΩÃÇ
1x1i + ùõΩÃÇ
2x2i + ùõΩÃÇ
3x3i +¬∑¬∑¬∑)
. (1)
here, pi is the outcome probability for the ith individual, the ùõΩÃÇ terms denote the original predictor effect estimates
(ln odds ratios) from maximum likelihood, and ùõº* is the intercept that has been re-estimated (after shrinkage of predictor
effects) to ensure perfect calibration-in-the-large, such that, the overall predicted risk still agrees with the overall observed
risk in the development data set (for details on how to do this, we refer to the works of harrell15 and steyerberg1
). similarly, after fitting a proportional hazards (cox) regression model using standard maximum likelihood, the model can be
revised using
hi(t) = h0(t)
‚àó exp (
s
(
ùõΩÃÇ
1x1i + ùõΩÃÇ
2x2i + ùõΩÃÇ
3x3i +¬∑¬∑¬∑)) , (2)
where hi(t) is the hazard rate of the outcome over time (t) for the ith individual and ho(t)
* is the baseline hazard function
re-estimated (after shrinkage of predictor effects) to ensure the predicted and observed outcome rates agree for the development data set as whole. compared to the original (nonpenalised) models, the revised models (1) and (2) will shrink
predicted probabilities away from zero and one, toward the overall mean outcome probability in the development data set.
example of a global shrinkage factor
van diepen et al developed a prognostic model for 1-year mortality risk in patients with diabetes starting dialysis.29
they use a logistic regression framework, with backwards selection to choose predictors in a dataset of 394 patients with
84 deaths by 1 year, and the estimated model is shown in table 1. to examine overfitting, the authors use bootstrapping to
estimate a global shrinkage factor of 0.903, indicating that the original model was slightly overfitted to the data. therefore,
a revised prediction model was produced by multiplying the original ùõΩÃÇ coefficients (ln odds ratios) from the original
logistic regression model by a global shrinkage factor of s = 0.903.
table 1 example of global shrinkage applied to a prognostic model for 1-year mortality risk in patients with diabetes starting
dialysis29
developed (unpenalised) model final (penalised) model adjusted for overfitting
intercept ùú∂ ùú∂ ÃÇ *
1.962 1.427
predictor ùú∑
ÃÇ sùú∑
ÃÇ = 0.903ùú∑
ÃÇ
age (years) 0.047 0.042
smoking 0.631 0.570
macrovascular complications 1.195 1.078
duration of diabetes mellitus (years) 0.026 0.023
karnofsky scale ‚àí0.043 ‚àí0.039
haemoglobin level (g/dl) ‚àí0.186 ‚àí0.168
albumin level (g/l) ‚àí0.060 ‚àí0.054
2.2 expressing sample size in terms of a global shrinkage factor
bootstrapping is an excellent way to calculate the shrinkage factor postestimation, but (as it is a resampling method) is
not useful for us in advance of data collection. an alternative approach to calculating a global shrinkage factor is to use
the closed form ‚Äúheuristic‚Äù shrinkage factor of van houwelingen and le cessie,23 defined by
svh = 1 ‚àí p
lr , (3)
where p is the total number of predictor parameters for the full set of candidate predictors (ie, all those considered for
inclusion in the model) and lr is the likelihood ratio (chi-squared) statistic for the fitted model defined as
lr = ‚àí2 (ln lnull ‚àí ln lmodel) , (4)
where ln lnull is the log-likelihood of a model with no predictors (eg, intercept-only logistic regression model), and ln lmodel
is the log-likelihood of the final model. in our related paper on linear regression, we used the copas shrinkage estimate that
is similar to equation (3), but with p replaced by p + 2. in our experience, svh performs better for generalised linear models
than the copas estimate, with svh further from 1 and closer to the corresponding estimate obtained from bootstrapping.
copas also notes that, unlike for linear regression, a formal justification for replacing p by p + 2 in equation (2) has not
been proved for logistic regression.30
hence, we use equation (3) as our shrinkage estimate (ie, our measure of overfitting) for logistic and cox regression
models, which now motivates our sample size approach to meet criterion (i). first, let us re-express the right-hand side
of equation (3) in terms of sample size (n), number of candidate predictor parameters (p), and the cox-snell generalised
r2.
21 the latter is also known as the maximum likelihood r2, the likelihood ratio r2, or magee's r2,
31 and it provides a
generalisation (eg, to logistic and cox regression models) of the well-known proportion of variance explained for linear
regression models. let us use r2
cs_app to denote the apparent (‚Äúapp‚Äù) estimate of a prediction model's cox-snell (‚Äúcs‚Äù) r2
performance as obtained from the model development data set. it can be shown (eg, see the works of magee31 or hendry
and nielsen32) that the lr statistic can be expressed in terms of the sample size (n) and r2
cs_app as follows:
lr = ‚àín ln (
1 ‚àí r2
cs_app)
. (5)
this leads to the cox-snell generalised definition of the apparent r2 expressed in terms of the lr value for any regression
model, including logistic and cox regression
r2
cs_app = 1 ‚àí exp (‚àílr
n
)
. (6)
applying equation (5) within equation (3), the van houwelingen and le cessie shrinkage factor becomes
svh = 1 + p
n ln (
1 ‚àí r2
cs_app) . (7)
2.3 criterion (i): calculating sample size to ensure a shrinkage factor ‚â• 0.9
equation (7) provides a closed-form solution for the expected shrinkage conditional on n, p, and r2
cs_app. therefore, if
we could specify a realistic value for r2
cs_app in advance of our study starting, we could identify values of n and p that
correspond to a desired shrinkage factor (eg, 0.9), thus informing the required sample size. however, a major problem is
that r2
cs_app is a postestimation measure of model fit, whereas for a sample size calculation, this needs to be specified in
advance of collecting the data when designing a new study. furthermore, due to overfitting in the model development
data set, the observed r2
cs_app is generally an upwardly biased (optimistic) estimate of the cox-snell r2 as it is estimated
in the same data used to develop the model. thus, in new data, the actual cox-snell r2 peformance is likely to be lower.
therefore, we need to re-express svh in terms of r2
cs_adj, an adjusted (approximately unbiased) estimate of the model's
expected r2
cs performance in new individuals from the same population. in other words, r2
cs_adj is a modification of r2
cs_app
to adjust for optimism (caused by overfitting) in the model development data set. for generalised linear models such as
logistic regression, mittlboeck and heinzl suggest that r2
cs_adj can be obtained by33
r2
cs_adj = svhr2
cs_app (8)
as the expected value of this r2
cs_adj corresponds to the underlying population value.33 by rearranging equation (8), we
can express r2
cs_app in terms of r2
cs_adj
r2
cs_app =
r2
cs_adj
svh
. (9)
applying equation (9) within equation (7), we can now express svh in terms of r2
cs_adj, rather than r2
cs_app
svh = 1 + p
n ln (
1 ‚àí r2
cs_adj
svh ). (10)
finally, a simple rearrangement of equation (10) leads to a closed-form solution for the required sample size to develop
a prediction model conditional on p, svh and r2
cs_adj
n = p
(svh ‚àí 1) ln (
1 ‚àí r2
cs_adj
svh ). (11)
for example, for developing a new logistic regression model based on up to 20 candidate predictor parameters with an
anticipated r2
cs_adj of at least 0.1, then to target an expected shrinkage of 0.9, we need a sample size of
n = p
(svh ‚àí 1) ln (
1 ‚àí r2
cs_adj
svh ) = 20
(0.9 ‚àí 1) ln (
1 ‚àí 0.1
0.9
) = 1698,
and thus 1698 individuals.
2.4 translating the calculated sample size to the number of events and epp
it may be surprising that the overall outcome proportion (or overall outcome rate) is not directly included in the right-hand
side of the sample size equation (11), especially because the total number of events, e, (which depends on the outcome
proportion or rate) is often considered the effective sample size for binary and time-to-event outcomes.15 however, the
outcome proportion (rate) is indirectly accounted for in the sample size calculation via the chosen r2
cs_adj, as the maximum value of r2
cs_adj for the intended population of the model depends on the overall outcome proportion (rate) for
that population. as the outcome proportion decreases, the maximum value of r2
cs decreases. this is explained further in
section 3.4. therefore, after n is derived from the sample size equation (11), e can be obtained by combining the calculated
n with the outcome proportion (rate) for the intended population. similarly, epp can be obtained.
for example for binary outcomes, e = nùúô and epp = nùúô/p, where ùúô is the overall outcome proportion in the target
population (ie, the overall prevalence for diagnostic models, or the overall cumulative incidence by a key time point
for prognostic models). in our aforementioned hypothetical example, where 1698 subjects were needed based on an
r2
cs_adj of 0.1 and svh of 0.9, then if the intended setting has ùúô of 0.1 (ie, overall outcome risk is 10%), the required
e = 1698 √ó 0.1 = 169.8. with 20 predictor parameters, the required epp = (1698 √ó 0.1)/20 = 8.5. however, if the intended
setting has ùúô of 0.3, then e = 509.4 and epp = 25.5. the big change in epp is because, although the chosen value of r2
cs_adj
is fixed at 0.1, the maximum value of r2
cs is much higher for the setting with the higher outcome proportion.
we can explain this further using nagelkerke's ‚Äúproportion of total variance explained‚Äù,34 which is calculated as
r2
cs_adj‚àï max(r2
cs). if two models have the same r2
cs_adj (say at 0.1, as in the aforementioned examples), then nagelkerke's
measure of predictive performance will be lower for the model whose setting has a higher outcome proportion, as the
max(r2
cs) is larger in that setting. models with lower performance have larger overfitting concerns,22 and therefore require
larger epp to minimise overfitting than models with high performance. hence, explaining why epp was larger when ùúô
was 0.3 compared with 0.1 in the aforementioned example. this highlights that a blanket rule of thumb (such as at least
10 epp) is unlikely to be sensible to meet criterion (i), as the actual epp depends on the setting/population of interest
(which dictates the overall outcome proportion or rate) and expected model performance.
3 how to prespecify rùüê
cs_adj based on previous information
our sample size proposal in equation (11) requires researchers to provide a value for the model's r2
cs_adj, that is, to prespecify the anticipated cox-snell r2 value if the model was applied to new individuals. how should this be done? we
recommend using r2
cs_adj values from previous prediction model studies for the same (or similar) population, considering
the same (or similar) outcomes and time points of interest. for example, the researcher could consult systematic reviews
of existing models and their performance, which are also increasingly available,35 or registries that record the prediction
models available in a particular field.36
often, a new prediction model is developed specifically to update or improve upon the performance of an existing model,
by using additional predictors. then, the existing model's r2
cs_adj could be used as a lower bound for the new model's
anticipated r2
cs_adj. in this situation, if the apparent cox-snell estimate, r2
cs_app, is available in an article describing the
development of the existing model, then its r2
cs_adj can be derived using equation (8) as long as the study's n and p can
also be obtained. in addition, as in van diepen et al's example (table 1), a global shrinkage factor may be reported directly
for an existing model development study, and if so, r2
cs_adj can be derived from a simple rearrangement of equation (10),
again as long as the study's n and p are also available.
note that, if r2
cs_app is available from an external validation study of an existing model, there is no need for adjustment
(ie, r2
cs_app = r2
cs_adj), as the validation dataset provides a direct estimate of the model's performance in new individuals
(free from overfitting concerns as there is no model development therein).
other options to obtain r2
cs_adj from the existing literature are now described. for guidance on choosing an r2
cs_adj value
in the absence of any prior information, please see our discussion.
3.1 using the lr statistic to derive the cox-snell rùüê
adj
if the r2
cs_app or r2
cs_adj is not available in the publication of an existing model, the lr value may be reported, which would
allow r2
cs_app to be derived using equation (6), then svh for the model derived using equation (7) (assuming the model's
n and p are also provided), and finally r2
cs_adj using equation (8).
sometimes the log-likelihood of the final model (lnlmodel) is reported, but not the lr value itself. in this situation, the
researcher should calculate ln lnull based on other information in the article, and then calculate lr using equation (4),
thus allowing r2
cs_app and r2
cs_adj to be derived using equations (6) and (8), respectively. for example, in a logistic
regression model, the loglnull value can be calculated using
ln lnull = e ln (e
n
)
+ (n ‚àí e)ln (
1 ‚àí e
n
)
, (12)
where e is the total number of outcome events. of course, this assumes e and n are actually available in the article.
similarly, for an exponential survival model (equivalent to a poisson model with ln (survival time) as an offset), the
ln lnull can be calculated using
ln lnull = e ln(ùúÜ) + ùúÜt = e ln (e
t
)
+ e (13)
as long as ùúÜ (the constant hazard rate), e (the total number of events), and t (the total time at risk, eg, total person-years)
are available in the article. note that, for survival models, packages such as sas and stata usually add a constant to the
reported log-likelihood to ensure it remains the same value regardless of the time scale used. for example, stata adds the
sum of the ln (survival times) for the noncensored individuals to the reported ln lmodel and ln lnull, and so this constant
must be either consistently used or consistently removed in each of ln lmodel and ln lnull when deriving the lr value.
3.2 using other pseudo-r2 statistics to derive rùüê
cs_adj
sometimes other pseudo-r2 statistics are reported for logistic and survival models, rather than the cox-snell version
specified in equation (6). in particular, because r2
cs_app has a maximum value less than 1, nagelkerke's r2 is sometimes
reported,34 which divides r2
cs_app by the maximum value defined by 1 ‚àí exp (2 ln lnull
n
)
, as follows:
r2
nagelkerke_app =
r2
cs_app
max (
r2
cs_app) =
r2
cs_app
1 ‚àí exp (2 ln lnull
n
). (14)
recall that ln lnull is derivable from other information, eg, using equations (12) or (13) for logistic and exponential
(poisson) models, respectively. when nagelkerke's r2, ln lnull, and n are available, the r2
cs_app can be calculated by
rearranging equation (14) to give
r2
cs_app = r2
nagelkerke_app (
1 ‚àí exp (2 ln lnull
n
)) , (15)
and then r2
cs_adj calculated via equation (8).
another measure sometimes reported is mcfadden's r2 37
r2
mcfadden_app = 1 ‚àí ln lmodel
ln lnull
. (16)
as ln lnull is often obtainable (see previous equation), when r2
mcfadden_app is reported, we can rearrange equation (16) to
obtain ln lmodel, and subsequently derive the lr statistic using equation (4), the cox-snell r2
cs_app from equation (6), svh
from equation (7) (assuming the model's n and p are also provided), and finally r2
cs_adj via equation (8).
for proportional hazards survival models, o'quigley et al suggested to modify r2
cs_app by replacing n with the number
of events (e)
38
r2
oÃÅquigley_app = 1 ‚àí exp (‚àílr
e
)
. (17)
therefore, if r2
oÃÅquigley_app and e were reported, the lr value could be found using
lr = ‚àíe ln (
1 ‚àí r2
oÃÅquigley_app)
, (18)
and subsequently, r2
cs_app can be obtained using equation (6), svh using equation (7), and finally r2
cs_adj using
equation (8).
another measure increasingly being reported for survival models is royston's measure of explained variation,39 which
is given by
r2
royston_app =
r2
oÃÅquigley_app
r2
oÃÅquigley_app +
(ùúã2
6
) (1 ‚àí r2
oÃÅquigley_app). (19)
when r2
royston_app is reported it can be used to obtain r2
oÃÅquigley_app by rearranging equation (19) as
r2
oÃÅquigley_app =
‚àíùúã2
6
r2
royston_app
(
1 ‚àí ùúã2
6
)
r2
royston_app ‚àí 1
. (20)
this subsequently allows lr, r2
cs_app, svh and then r2
cs_adj to be derived as explained previously. a similar measure
to r2
royston is royston and sauerbrei's r2
d,
40 which can be derived from their proposed d statistic (the ln(hazard ratio)
comparing two groups defined by the median value of the model's risk score in the population of application)
r2
d_app =
ùúã
8
d2
ùúã2
6 + ùúã
8
d2
. (21)
in examples shown by royston,39 r2
royston_app and r2
d_app are reasonably similar, and thus, we tentatively suggest r2
d_app
as
a proxy for r2
royston_app when only r2
d_app (or d) is reported; though, we recognise that further research is needed on the
link between r2
d_app
and r2
royston.
table 2 predicted values of the d statistic
and r2
d from equation (23) for selected values of
the c statistic (values taken from table 1 in the
work of jinks et al41)
c drùüê
d cdrùüê
d
0.50 0 0 0.72 1.319 0.294
0.52 0.11 0.003 0.74 1.462 0.338
0.54 0.221 0.011 0.76 1.61 0.382
0.56 0.332 0.026 0.78 1.765 0.427
0.58 0.445 0.045 0.80 1.927 0.470
0.60 0.560 0.070 0.82 2.096 0.512
0.62 0.678 0.099 0.84 2.273 0.552
0.64 0.798 0.132 0.86 2.459 0.591
0.66 0.922 0.169 0.88 2.652 0.627
0.68 1.05 0.208 0.90 2.857 0.661
0.70 1.182 0.25 0.92 3.070 0.692
3.3 using values of the c statistic to derive rùüê
cs_adj
jinks et al also proposed the following equation, based on empirical evidence, for predicting royston's d (and thus
subsequently r2
d_app) when only the c statistic is reported for a survival model41
d = 5.50(c ‚àí 0.5) + 10.26(c ‚àí 0.5)
3
. (22)
table 2 provides values of d (and corresponding values of r2
d_app from equation (21)) predicted from equation (22) for
selected values of the c statistic, as taken from the work of jinks et al.41 thus, if only the c statistic is reported, we can
use equation (22) to predict royston's d statistic and calculate r2
d_app (using equation (21)) as a proxy to r2
royston_app, and
then r2
oÃÅquigley_app, lr, r2
cs_app and finally r2
cs_adj computed sequentially using the equations given previously.
further evaluation of the performance of jinks' formula is required, eg, using simulation and across settings with different cumulative outcome incidences. indeed, based on figure 5 in the work of jinks et al,41 the potential error in the
predictions of d appears to increase as c increases, and is about +/‚àí 0.25 when c is 0.8. nevertheless, equation (22)
serves as a good starting point and works well in our applied example (see section 5.2.1). further research is also needed
to ascertain how to predict r2
cs from other measures, such as somer's d statistic.
3.4 the anticipated value of rùüê
cs_adj may be small
it is important to emphasise that the cox-snell, r2
cs, values for logistic and survival models are usually much lower than
for linear regression models, with values often less than 0.3. a key reason is that (unlike for linear regression) the r2
cs_app
has a maximum value less than 1, defined by
max(
r2
cs_app)
= 1 ‚àí exp(2 ln lnull
n
)
. (23)
this is because ln lnull is itself bounded for binary and time-to-event outcomes (see equations (12) and (13)). for example,
for a logistic regression model with an outcome proportion of 50%, using equation (12) and an arbitrary sample size of
100, we have
ln lnull = e ln (e
n
)
+ (n ‚àí e)ln (
1 ‚àí e
n
)
= 50 ln ( 50
100
)
+ (100 ‚àí 50)ln (
1 ‚àí 50
100
)
= ‚àí69.315,
and therefore, using equation (23),
max(
r2
cs_app)
= 1 ‚àí exp(2 ln lnull
n
)
= 1 ‚àí exp(‚àí69.315
100
)
= 0.75.
however, for an outcome proportion of 5%, the max(r2
cs_app) is 0.33, and for an outcome proportion of 1%, the max(r2
cs_app)
is 0.11. therefore, especially in situations where the outcome proportion is low, researchers should anticipate a model
with a (seemingly) low r2
cs_app value, and subsequently a low r2
cs_adj value.
low values of r2
cs_app or r2
cs_adj do not necessarily indicate poor model performance. consider the following three
examples. first, poppe et al used a cox regression to develop a model (‚Äúpredict-cvd‚Äù) to predict the risk of future
cvd events within two years in patients with atherosclerotic cvd,42 and directly report an r2
cs_app of 0.04. however,
the corresponding c statistic is 0.72, which shows discriminatory magnitude typical of many prognostic models used in
practice. second, hippisley-cox and coupland use the qresearch database to produce three models (qdiabetes) that
estimates the risk of future diabetes in a general population.43 in their validation of their ‚Äúmodel a,‚Äù there were 27 311
incident cases of diabetes recorded in 1 322 435 women (3.77 cases per 1000 person-years) during follow-up, and the
reported r2
royston_app was 0.505. using the approach described previously to convert r2
royston to lr, this leads to a r2
cs_app of
0.02; however, the corresponding d statistic of 2.07 and c statistic of 0.89 are large. third, in a risk prediction model for
venous thromboembolism (vte) in women during the first 6 weeks after delivery,44 r2
cs_app was 0.001 due to the extremely
low event risk (7.2 per 10 000 deliveries), but the model still had important discriminatory ability as the corresponding c
statistic was 0.70.
4 additional sample size criteria
criterion (i) focuses on shrinkage of predictor effects, which is a multiplicative measure of overfitting (ie, on the relative
scale). harrell suggests to also evaluate overfitting on the absolute scale and to check key model parameters are estimated
precsiely.15 we now address this with two further criteria.
4.1 criterion (ii): ensuring a small absolute difference in the apparent
and adjusted rùüê
nagelkerke
our second criterion for minimum sample size is to ensure a small absolute difference (ùõø) between the model's apparent
and adjusted proportion of variance explained. we suggest using nagelkerke's r2 for this purpose as, unlike the cox-snell
r2 value, it can range between 0 and 1, and so a small difference (say ‚â§ 0.05) can be ubiquitously defined. based on
equation (14), the difference in the apparent and adjusted nagelkerke's r2 can be defined as
r2
nagelkerke_app ‚àí r2
nagelkerke_adj =
r2
cs_app
max (
r2
cs_app) ‚àí
r2
cs_adj
max (
r2
cs_app)
=
r2
cs_adj
svh
‚àí r2
cs_adj
max (
r2
cs_app)
=
r2
cs_adj (1 ‚àí sùëâ ùêª )
svh max (
r2
cs_app), (24)
where max(r2
cs_app) = 1 ‚àí exp (2 ln lnull
n
)
, as shown in equation (23).
therefore, to meet sample size criterion (ii) and ensure the difference is less than a small value (say, ùõø), we require
r2
csadj
(1 ‚àí svh)
svh max (
r2
csapp) ‚â§ ùõø. (25)
we generally recommend ùõø is ‚â§ 0.05, such that the optimism is nagelkerke's percentage of variation explained is ‚â§ 5%.
rearranging equation (25), we find that
(1 ‚àí svh)
svh
‚â§
ùõø max (
r2
csapp)
r2
csadj
,
and therefore,
svh ‚â•
r2
csadj
r2
csadj
+ ùõø max (
r2
csapp). (26)
equation (26) allows the researcher to calculate the required svh to satisfy criterion (ii), conditional on prespecifying the
model's anticipated r2
cs_adj (as they did for criterion (i)) and also the value of max(r2
csapp
) as outlined for equation (23).
then, sample size equation (11) can be used to derive the sample size needed to satisfy criterion (ii). this is only necessary
when the calculated value of svh from equation (26) is larger than that chosen for criterion (i), as then the sample size
required to meet criterion (ii) will be larger than that for criterion (i).
for example, consider the development of a logistic regression model with anticipated r2
cs_adj of at least 0.1, and in a
setting with the outcome proportion of 5%, such that the max(r2
cs_app) is 0.33. then, to ensure ùõø is ‚â§ 0.05, we require
svh ‚â•
r2
csadj
r2
csadj
+ ùõø max (
r2
csapp) = 0.1
0.1 + (0.05 √ó 0.33) = 0.858.
therefore, svh must be at least 0.86 to meet criterion (ii). as this is lower than the recommended value of at least 0.90 to
meet criterion (i), no further work is required. however, had the anticipated r2
cs_adj been 0.2, then
svh ‚â• 0.2
0.2 + (0.05 √ó 0.33) = 0.924.
as this is higher than 0.90, we would need to reapply sample size equation (11) using 0.924, rather than 0.90, to obtain a
sample size that meets both criteria (i) and (ii).
4.2 criterion (iii): ensure precise estimate of overall risk (model intercept)
for logistic and time-to-event models, it is fundamental that the available sample size can precisely estimate the overall
risk in the population by key time-points of interest. one way to examine this is to calculate the margin of error in outcome
proportion estimates (ùúôÃÇ) for a null model (ie, no predictors included). for example, for a binary outcome, an approximate
95% confidence interval for the overall outcome proportion is
ùúôÃÇ ¬± 1.96‚àö
ùúôÃÇ(1 ‚àí ùúôÃÇ)
n .
therefore, the absolute margin of error (ùõø) is 1.96‚àöùúôÃÇ(1‚àíùúôÃÇ)
n , which leads to
n =
(1.96
ùõø
)2
ùúôÃÇ(1 ‚àí ùúôÃÇ) . (27)
this is largest when the outcome proportion is 0.5. we require 96 individuals to ensure a margin of error ‚â§ 0.1 when the
true value is 0.5.15 however, we recommend a more stringent margin of error ‚â§ 0.05, which, when the outcome proportion
is 0.5, requires
n =
(1.96
0.05
)2
0.5(1 ‚àí 0.5) = 384.2,
and thus, 385 participants (and hence, about 193 events) are required. if the outcome proportion is 0.1, then we require
139 subjects to ensure a margin of error ‚â§ 0.05, whilst an outcome proportion of 0.2 requires 246 subjects.
these sample sizes aim to ensure precise estimation of the overall risk in the population of interest. strictly speaking,
we are more interested in precise estimation of the mean risk in an actual model including multiple predictors. if we
centre predictors at their mean value, then the model's intercept is the logit risk for an individual with mean predictor
values. the corresponding risk for this individual will often be very similar (though not identical) to the mean risk in
the overall population. furthermore, the variance of the estimated risk for this individual will be approximately ùúôÃÇ(1‚àíùúôÃÇ)
n .*
*as obtained by inversing the information matrix x'v‚àí1x and replacing individual variances defined by pi(1-pi) with a constant variance defined by
ùúôÃÇ(1 ‚àí ùúôÃÇ).
thus, it follows that equation (27) is also a good approximation to the sample size required to precisely estimate the mean
risk in a model containing predictors centred at their mean.
for time-to-event data, we could consider the precision of the estimated cumulative incidence (outcome risk) at a key
time point of interest. a simple (and therefore practical) approach is to assume an exponential survival model, for which
the estimated cumulative incidence function is f(t) = 1 ‚àí exp(‚àíùúÜÃÇ t), where ùúÜÃÇ is the estimated rate (number of events per
person-year). an approximate 95% confidence interval for the estimated f(t) is 1‚àíexp (
‚àí
(
ùúÜÃÇ ¬± 1.96‚àöùúÜÃÇ
t
)
t
)
, where t is
the total person-years of follow-up. therefore, to ensure a small absolute margin of error, such that the lower and upper
bounds of the confidence interval are ‚â§ ùõø (eg, 0.05) of the true value, we must ensure both the following are satisfied:
‚àí exp (
‚àí
(
ùúÜÃÇ + 1.96‚àö
ùúÜÃÇ
t
)
t
)
+ exp(‚àíùúÜÃÇ t) ‚â§ ùõø
‚àí exp(‚àíùúÜÃÇ t) + exp (
‚àí
(
ùúÜÃÇ ‚àí 1.96‚àö
ùúÜÃÇ
t
)
t
)
‚â§ ùõø.
(28)
for example, for a constant event rate of 0.10 (10 events per 100 person-years), then by 10 years, the outcome risk is f(10)
= 1 ‚àí exp (‚àí0.1 √ó 10) = 0.632. then, 2366 person-years of follow-up (and thus 0.1 √ó 2366 ‚âà 237 events) are needed to
provide a confidence interval, which has a maximum absolute error of 0.05 from the true value. that is,
1 ‚àí exp (
‚àí
(
ùúÜÃÇ ¬± 1.96‚àö
ùúÜÃÇ
t
)
t
)
= 1 ‚àí exp (
‚àí
(
0.10 ¬± 1.96‚àö 0.10
2366)
10)
= 0.582 to 0.676.
thus, equation (28) is satisfied, as both the lower and upper bounds are ‚â§ 0.05 of the true value of 0.632. more generally, to
avoid assuming simple survival distributions like the exponential, harrell suggests using the dvoretzky-kiefer-wolfowitz
inequality to estimate the probability of a chosen margin of error anywhere in the estimated cumulative incidence
function.15,45
5 worked examples
to summarise our sample size approach for researchers, we provide a step-by-step guide in figure 1. the sample size (and
corresponding number of events and epp) that meets criteria (i) to (iii) provides the minimum sample size required for
model development. we now present two worked examples to illustrate our approach.
5.1 a diagnostic prediction model for chronic chagas disease
our first example considers the minimum sample size required for developing a diagnostic model for predicting a binary
outcome (disease: yes or no). brasil et al developed a logistic regression model containing 14 predictor parameters for
predicting the risk of having chronic chagas disease in patients with suspected chagas disease.46 upon external validation
in a cohort of 138 participants containing 24 with chagas disease, the model had an estimated c statistic of 0.91 and an
r2
nagelkerke_app of 0.48. consider that a researcher wants to update this model and improve the predictive performance. our
sample size approach can be applied as follows.
5.1.1 steps 1 and 2: identifying values for p, rùüê
cs_adj, and max(rùüê
cs_app)
assume that the researcher has identified (eg, based on recent studies) 10 additional predictor parameters that they wish
to add to the original model. thus, in total, the number of predictor parameters, p, is 24. the next step is to identify a
sensible value for the anticipated cox-snell r2
adj. to achieve this, we can convert the r2
nagelkerke_app value for brasil's existing
model into a r2
cs_app value. assume the disease prevalence is 17.4%, as in the brasil validation study, and use equation (12)
to calculate the log-likelihood for the null model in brasil's validation study
ln lnull = e ln (e
n
)
+ (n ‚àí e)ln (
1 ‚àí e
n
)
= 24 ln ( 24
138
)
+ (138 ‚àí 24)ln (
1 ‚àí 24
138
)
= ‚àí63.761.
figure 1 summary of the steps involved in calculating the minimum sample size required for developing a multivariable prediction
model for binary or time-to-event outcomes
hence, the max(r2
csapp
) = 1 ‚àí exp (2 ln lnull
n
)
= 1 ‚àí exp (2√ó‚àí63.761
138 )
= 0.60. now, we can use equation (15) to obtain
r2
cs_app = r2
nagelkerke_app (
max (
r2
csapp)) = 0.48 √ó 0.60 = 0.288.
this apparent cox-snell value of 0.288 can be directly used as an estimate of the model's r2
cs_adj, as it was obtained in a
different data set to that used for model development. therefore no adjustment is needed, because r2
cs_app= r2
cs_adj here.
5.1.2 step 3: criterion (i) - ensuring a global shrinkage factor of 0.9
let us assume 0.288 is a lower bound for the r2
cs_adj of our new model. we now use equation (11) to estimate the sample
size required to ensure an expected shrinkage factor (svh = 0.90) conditional on a number of predictor parameters (p= 24)
n = p
(svh ‚àí 1)ln (
1 ‚àí r2
csadj
svh ) = 24
(0.90 ‚àí 1)ln (
1 ‚àí 0.288
0.90 ) = 622.31.
thus, 623 participants are required to meet criterion (i).
5.1.3 step 4: criterion (ii) - ensuring a small absolute difference in the apparent
and adjusted rùüê
nagelkerke
to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of 0.05 or less
in the apparent and adjusted r2
nagelkerke. using equation (26), we obtain
svh ‚â•
r2
csadj
r2
csadj
+ ùõø max (
r2
csapp) = 0.288
0.288 + (0.05 √ó 0.60) = 0.906.
this is more stringent than the 0.90 assumed for criterion (i). therefore, we need to reapply equation (11) to estimate the
sample size required conditional on svh = 0.906 (rather than 0.90)
n = p
(svh ‚àí 1)ln (
1 ‚àí r2
csadj
svh ) = 24
(0.906 ‚àí 1) ln (
1 ‚àí 0.288
0.906 ) = 667.41.
therefore, 668 subjects are required to meet criterion (ii), exceeding the 623 subjects required for criterion (i).
5.1.4 step 5: criterion (iii) - ensure precise estimate of overall risk (model intercept)
assuming the prevalence of chagas disease is 17.4% (as observed from the brasil validation study), then to ensure we
estimate this with a margin of error ‚â§ 0.05, we require (using equation (27))
n =
(1.96
0.05
)2
0.174 (1 ‚àí 0.174) = 220.85
and thus 221 subjects. this is far fewer than the sample size required to meet criteria (i) and (ii).
5.1.5 step 6: minimum sample size that ensures all criteria are met
the largest sample size required was 668 subjects to meet criterion (ii), and so this provides the minimum sample size
required for developing our new model. it corresponds to 668 √ó 0.174 = 116.2 events, and an epp of 116.2/24 = 4.84,
which is considerably lower than the ‚Äúepp of at least 10‚Äù rule of thumb.
5.2 a prognostic model to predict a recurrence of vte
our second example considers the sample size required to develop a prognostic model with a time-to-event outcome.
ensor et al developed a prognostic time-to-event model for the risk of a recurrent vte following cessation of therapy for
a first vte.47 the sample size was 1200 participants, with a median follow-up of 22 months, a total of 2483 person-years
of follow-up, and 161 (13.42% of) individuals had a vte recurrence by end of follow-up.47 the model included predictors
of age, gender, site of first clot, d-dimer level, and the lag time from cessation of therapy until measurement of d-dimer
(often around 30 days). these predictors corresponded to six parameters in the model, which was developed using the
flexible parametric survival modelling framework of royston and parmar48 and royston and lambert.49 although ensor's
model performed well on average, the model's predicted risks did not calibrate well with the observed risks in some
populations.47 therefore, new research is needed to update and extend this model, eg, by including additional predictors.
we now identify suitable sample sizes to inform such research.
5.2.1 steps 1 and 2: identifying values for p, rùüê
cs_adj and max(rùüê
cs_app)
assume that there are 25 potential predictor parameters for inclusion in the new model, and thus, p = 25. we next need
to identify suitable values for ùëÖ2
cs_adj and max(ùëÖ2
cs_app).
calculating max(rùüê
cs_app)
for the ensor model, r2
cs_app was not reported but we should expect it to be quite small because the maximum value
of r2
cs_app is low. for example, assuming (for simplicity) an exponential survival model was fitted to the ensor data, then
using equation (13), we have
ln lnull = e ln (e
t
)
+ e = 161 ln(161‚àï2483) + 161 = ‚àí279.47,
and therefore, using equation (23),
max (
r2
cs_app)
= 1 ‚àí exp (2 ln lnull
n
)
= 1 ‚àí exp (‚àí2 √ó 279.47
1200
)
= 0.37.
thus, max(r2
cs_app) is considerably less than 1.
obtaining a sensible value for rùüê
cs_adj from the study authors
as r2
cs_app was not reported for the ensor model, we need to obtain it. we contacted the original authors who told us
their model's r2
cs_app was 0.056 in the development data set. thus, let us use this value to derive r2
adj from equation (8).
based on ensor's sample size of 1200, and six predictor parameters, we obtain
r2
cs_adj = svhr2
cs_app =
‚éõ
‚éú
‚éú
‚éú
‚éù
1 + p
n ln (
1 ‚àí r2
cs_app)
‚éû
‚éü
‚éü
‚éü
‚é†
r2
cs_app =
(
1 +
6
1200 ln (1 ‚àí 0.056)
)
0.056 = 0.051.
hence, when developing a new model in this field, we could assume 0.051 is a lower bound for the expected r2
cs_adj of the
new model. this corresponds to nagelkerke's proportion variation explained of r2
cs_adj‚àïmax(r2
cs_app) ‚âà 0.051/0.37 = 0.14
(or 14%).
calculating a sensible value for rùüê
cs_adj from other reported information
for illustration, we also consider how r2
cs_app could have been estimated indirectly from other available information.
the model's reported c statistic was 0.69, and so we can use equation (22) to predict the corresponding d statistic
d = 5.50 (c ‚àí 0.5) + 10.26(c ‚àí 0.5)
3 = 5.50 (0.69 ‚àí 0.5) + 10.26(0.69 ‚àí 0.5)
3 = 1.115.
the corresponding r2
d_app can be derived from equation (21)
r2
d_app =
ùúã
8
d2
ùúã2
6 + ùúã
8
d2
=
ùúã
8
1.1152
ùúã2
6 + ùúã
8
1.1152
= 0.229.
taking r2
d_app as a proxy for r2
royston_app, we can then use equation (20) to obtain
r2
oÃÅquigley_app =
‚àíùúã2
6
r2
royston_app
(
1 ‚àí ùúã2
6
)
r2
royston_app ‚àí 1
= ‚àíùúã2
6
0.229
(
1 ‚àí ùúã2
6
)
0.229 ‚àí 1
= 0.328.
next, we can use r2
oÃÅquigley_app and the number of reported events (e = 161) to derive the lr statistic from equation (18)
lr = ‚àíe ln (
1 ‚àí r2
oÃÅ
quigleyapp)
= ‚àí161 ln (1 ‚àí 0.328) = 64.05.
using equation (6), this corresponds to
r2
cs_app = 1 ‚àí exp (‚àílr
n
)
= 1 ‚àí exp (‚àí64.05
1200
)
= 0.052.
thus, based on using the reported c statistic, an indirect estimate of the r2
cs_app is 0.052 for the ensor model. this is
reassuringly close to the estimate of 0.056 provided directly by the study authors.
5.2.2 step 3: criterion (i) - ensuring a global shrinkage factor of 0.9
equation (11) can now be applied to derive the required sample size to meet criterion (i). using an r2
cs_adj of 0.051, for a
model with 25 predictor parameters and a targeted expected shrinkage of 0.9, the sample size required is
n = p
(svh ‚àí 1) ln (
1 ‚àí r2
cs_adj
svh ) = 25
(0.9 ‚àí 1) ln (
1 ‚àí 0.051
0.9
) = 4285.5
and thus 4286 participants.
5.2.3 step 4: criterion (ii) - ensuring a small absolute difference in the apparent
and adjusted rùüê
nagelkerke
to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of 0.05 or less
in the apparent and adjusted r2
nagelkerke. recall, assuming an exponential model for simplicity, we calculated that the
max(r2
csapp
) = 0.37. then, using equation (26), we obtain
svh ‚â•
r2
csadj
r2
csadj
+ ùõø max (
r2
csapp) = 0.051
0.051 + (0.05 √ó 0.37) = 0.73.
this is less stringent than the 0.90 assumed for criterion (i), and so no further sample size calculation is required to meet
criterion (ii).
5.2.4 step 5: criterion (iii) - ensure precise estimate of overall risk
assuming a simple exponential model, we can check the width of the confidence interval for the overall risk at a particular
time point based on the sample size identified, using the approach outlined in section 4.2. ensor et al47 reported an overall
vte recurrence rate of 161/2483 = 0.065, with an average follow-up of 2.07 years. therefore, assuming ùúÜ is 0.065 in our
new study, and that a predicted risk at 2 years is of key interest, an exponential survival model would give the cumulative
incidence of f(2)=1‚àí exp (‚àí0.065 √ó 2)=0.122. based on the calculated sample size of 4286 participants from criterion (i),
and thus an estimated 4286√ó2.07 = 8872 person-years of follow-up, the 95% confidence interval would be
1 ‚àí exp (
‚àí
(
ùúÜÃÇ ¬± 1.96‚àö
ùúÜÃÇ
t
)
t
)
= 1 ‚àí exp (
‚àí
(
0.065 ¬± 1.96‚àö0.065
8872 )
2
)
= 0.113 to 0.131.
this is reassuringly narrow, and satisfies equation (28) as both the lower and upper bounds are well within an error of
0.05 of the true value of 0.122.
5.2.5 step 6: minimum sample size that ensures all criteria are met
the largest sample size required was 4286 participants to meet criterion (i), which therefore provides the minimum sample
size required for developing our new model. this assumes the new cohort will have a similar follow-up, censoring rate,
and event rate to that reported by ensor et al, where the mean follow-up per person was 2.07 years, 13.42% of individuals
had a vte recurrence by end of follow-up, and the event rate was 0.065.47
then, the required 4286 participants corresponds to about 4286 √ó 2.07 = 8872 person-years of follow-up, and
8872 √ó 0.065 ‚âà 577 outcome events, and thus an epp of 577/25 ‚âà 23. this is over twice the ‚Äúepp of at least 10‚Äù rule of
thumb. figure 2 shows that an epp of 10 only ensures a shrinkage factor of 0.79, which would reflect relatively large
overfitting.
5.2.6 what if the sample size is not achievable?
if a researcher was restricted in their total sample size, for example, by the time and cost of a new cohort study, then a
sample size of 4286 may not be practical. in this situation, we do not recommend reducing sample size by decreasing
sc below 0.9 (as this would reflect larger overfitting) or by assuming a larger r2
cs_adj value (as this is anticonservative
for criterion (i)). rather, to ensure an svh of 0.9 (ie, an expected shrinkage of 10%), the researcher should lower p by
reducing the number of candidate predictors. for example, predictors could be prioritised based on previous evidence (eg,
systematic reviews). after data collection, unsupervised learning techniques such as principal component analysis may
be useful, which are blinded to the outcome data. figure 3 shows how changing p changes the required sample size to
meet criterion (i). for example, if a researcher was restricted to a sample size of about 2000 participants, then they would
need to reduce p to 12 to ensure an expected shrinkage of 0.90. this is because, for an svh of 0.9 and r2
cs_adj of 0.051, the
sample size required is
n = p
(svh ‚àí 1) ln (
1 ‚àí r2
cs_adj
svh ) = 12
(0.9 ‚àí 1) ln (
1 ‚àí 0.051
0.9
) = 2057
figure 2 events per predictor parameter required to achieve various expected shrinkage (svh) values for a new prediction model of
venous thromboembolism recurrence risk with an assumed r2
cs_adj of 0.051 [colour figure can be viewed at wileyonlinelibrary.com]
figure 3 sample size required (based on equation (11)) for a particular number of predictor parameters (p) to achieve a particular value
of expected shrinkage (svh), for a new prediction model of venous thromboembolism recurrence risk with an assumed r2
cs_adj of 0.051
[colour figure can be viewed at wileyonlinelibrary.com]
and so now close to 2000. figure 3 also shows how larger values of svh require larger sample sizes; in particular, the
increase in sample size required is substantial when moving from svh of 0.90 to 0.95. values of svh < 0.9 lead to lower
sample sizes, but come at the cost of larger expected overfitting, and so are not recommended. therefore, targeting a value
of svh of 0.9 would seem a pragmatic choice.
6 potential additional criterion: precise estimates of
predictor effects
ideally, predictions should also be precise across the entire spectrum of predicted values, not just at the mean. this is challenging to achieve, but is helped by ensuring the sample size will give precise estimates of the effects of key predictors;50
hence, this may form a further criterion for researchers to check (ie, in addition to criteria (i) to (iii)). briefly, for a particular predictor of a binary or time-to-event outcome, the sample size required to precisely estimate its association with
the outcome (ie, an odds ratio or hazard ratio) depends on the assumed magnitude of this effect, the variability of the
predictor's values across subjects, the predictor's correlation with other predictors in the model, and the overall outcome
proportion in the study.51-53 ideally, we want to ensure a sample size that gives a precise confidence interval around the
predictor's effect estimate.54 however, this is taxing, as closed-form solutions for the variance of adjusted log odds ratio
or hazard ratios, from logistic and cox regression, respectively, are nontrivial. one solution is to use simulation-based
evaluations.54,55 however, perhaps a more practical option is to utilise readily available power-based sample size calculations that calculate the sample size required to detect (based on statistical significance) a predictor's effect for a chosen
type i error level (eg, 0.05) and power.51-53,56 as such sample size calculations are likely to be less stringent than those
based on confidence interval width (especially for predictors with large effect sizes), we might use a high power, say of
95%, in the calculation.
checking sample size for predictor effects will be laborious with many predictors, and so it may be practical to focus
on the subset of key predictors with smallest variance of their values, as these predictors will have the least precision.
in particular, when there are important categorical predictors but with few subjects and/or outcome events in some categories, substantially larger sample sizes may be needed to avoid separation issues (ie, no event or nonevents in some
categories).57 in addition, any predictors whose effect is small (and thus harder to detect), but still important, may warrant
special attention.
for example, returning to the vte prediction model from section 5.2, a key predictor in the original model by ensor et al
was age,47 with an adjusted log hazard ratio of ‚àí0.0105. although this is close to zero, as age is on a continuous scale, the
impact of age on outcome risk is potentially large; for example, it corresponds to an adjusted hazard ratio of 0.66 comparing
two individuals aged 40 years apart. based on the results presented by ensor et al,47 the standard deviation of age was 15.21
and the overall outcome occurrence by end of follow-up was 13.5%. based on these values, and assuming other included
predictors explain 20% of the variation in age, then the sample size approach of hsieh and lavori52 suggests 4718 subjects
are required to have 95% power to detect a prognostic effect for age. this is larger than the 4286 subjects required to meet
criterion (i), and so, to be extra stringent beyond criteria (i) to (iii), the researcher might raise the recommended sample
size to 4718 subjects, if possible.
7 discussion
sample size calculations for prediction models of binary and time-to-event outcomes are typically based on blanket rules
of thumb, such as at least 10 epp, which generates much debate and criticism.14,16,57 in this article, building on our related
work for linear regression,10 we have proposed an alternative approach that identifies the sample size, events and epp
required to meet three key criteria, which minimise overfitting whilst ensuring precise estimates of overall outcome risk.
criterion (i) aims to ensure the optimism of predictor effect estimates is small, as defined by a global shrinkage factor of
‚â• 0.9. this idea extends the work of harrell who suggests that, after a model is developed, if the shrinkage estimate ‚Äúfalls
below 0.9, for example, we may be concerned with the lack of calibration the model may experience on new data.‚Äù15 our
premise is the same, except we focused on calculating the expected shrinkage before data collection, to inform sample
size calculations for a new study. criterion (ii) extends this idea to ensure the optimism is small on the r2
nagelkerke scale,
such that there is a difference of ‚â§ 5% in the apparent and adjusted percentage of variation explained by the model. lastly,
criterion (iii) ensures the sample size will precisely estimate the overall outcome risk, which is fundamental.
by utilising the model's anticipated cox-snell r2, the sample size calculations are essentially tailored to the model and
setting at hand, because the cox-snell r2 reflects many factors including the outcome proportion (ie, outcome prevalence
or cumulative incidence) and the overall fit (performance) of the model. it therefore better reflects the trait of a particular
model and setting at hand rather than a blanket epp rule.16 in our examples, the sample sizes required often differed
considerably from an epp of 10, reinforcing the idea that this rule is too simplistic.57 indeed, the required epp was much
higher (23) in our second example than our first (4.8), illustrating the problem with a blanket epp rule trying to cover all
situations.14,16-18
section 3 also showed how to obtain a realistic value for cox-snell r2 based on previous models to make our proposal
more achievable in practice. if no previous prediction model exists for the outcome and setting of interest, then information might be used from studies in a related setting or using a different but similar outcome definition or time points to
those intended for the new model. information can also be borrowed from predictor finding studies (eg, studies aiming
to estimate the prognostic effect of a particular predictor adjusted for other predictors58). typically, these studies apply
multivariable modelling, and although mainly focused on predictor effect estimates, they often report the c statistic and
pseudo-r2 values.
further research is needed to help researchers when there are no existing studies or information to identify a sensible
value of the expected cox-snell r2. medical diagnosis and prediction of health-related outcomes are, generally speaking,
low signal-to-noise ratio situations. it is not uncommon in these situations to see r2
nagelkerke values in the 0.1 to 0.2 range.
therefore, in the absence of any other information, we suggest that sample sizes be derived assuming the value of r2
cs_adj
corresponds to an r2
nagelkerke of 0.15 (ie,
r2
csadj
max(r2
csadj) = 0.15). an exception is when predictors include ‚Äúdirect‚Äù (mechanistic)
measurements, such as including the baseline version of the binary or ordinal outcome (eg, including smoking status at
baseline when predicting smoking status at 1 year), or direct measures of the processes involved (eg, including physiologic
function of patients in intensive care when predicting risk of death within 48 hours). then, in this special situation,
an r2
nagelkerke = 0.5 may be a more appropriate default choice.
the rule of having an epp of at least 10 stems from limited simulation studies examining the bias and precision of
predictor effects in the prediction model.11-13 jinks et al41 alternatively developed sample size formulae for a time-to-event
prediction model based on the d statistic.40 they suggest to predefine the d statistic that would be expected, and then,
based on a desired significance or confidence interval width, their formulae provide the number of events required to
achieve this. however, their method does not account for the number of candidate predictors and does not consider the
potential for overfitting when developing a model. our sample size calculations address this, and are meant to be used
before any data collection. in situations where a development data set is already available, containing a specific number
of participants and predictors, our criteria could be used to identify whether a reduction in the number of predictors
is needed before starting model development. indeed, harrell already illustrated this concept by using the shrinkage
estimate from the full model (including all predictors) to gauge whether the number of predictors should be reduced via
data reduction techniques.15 ideally, this should be done blind to the estimated predictor effects (ie, just calculate the
shrinkage factor for the full model, but do not observe the predictor effect estimates and associated p-values), as otherwise
decisions about predictor inclusion are influenced by a ‚Äúquick look‚Äù at the effect estimates from the full model results.
similarly, when planning to use a predictor selection method (such as backwards selection) during model development,
researchers should define p as the total number of parameters due to all predictors considered (screened), and not just
the subset that are included in the final model.59 as harrell notes,15 the value of p should be honest.
section 6 also highlighted the potential additional requirement to ensure precise estimates of key predictor effects.
in particular, special attention may be given to those predictors with strong predictive value (and thus most influential
to the predicted outcome risk), especially if the variance in their values is small, or when events or nonevents in some
categories of the predictor are rare, as this leads to larger sample sizes. for example, van smeden et al highlighted that
‚Äúseparation‚Äù between events and nonevents is an important consideration toward the required sample size, which occurs
when a single predictor (or a linear combination of multiple predictors) perfectly separates all events from all nonevents,
and thus causes estimation difficulties.57 this may lead to substantially larger epp to resolve the issue (eg, so that all
categories of a predictor have both events and nonevents). for such reasons, we labelled our criteria (i) to (iii) proposal
as the ‚Äúminimum‚Äù sample size required.
further research should identify how our sample size criteria relates to that of the work of van smeden et al, who focused
on sample size in regards to the mean squared error in predictions from the model.60 specifically, they use simulation
to evaluate the characteristics that influence the mean squared prediction error of a logistic model, and identify that the
outcome proportion and number of predictors are important,60 in addition to total sample size. this leads to a sample size
equation to minimise root mean-squared prediction error in a new model development study. harrell also suggested using
simulation to inform sample size, and illustrates this for a logistic regression model with a single predictor.15 for example,
one could simulate a very large dataset from an assumed prediction model, and quantify the mean square (prediction)
error and mean absolute (prediction) error of a model developed from this data set. then, repeat this process each time
removing an individual at random, until a sample size is identified below which the mean squared (prediction) error is
unacceptable.
in summary, we have proposed criteria for identifying the minimum sample size required when developing a prediction
model for binary or time-to-event outcomes. we hope this, and our related paper,10 encourages researchers to move away
from rules of thumb, and to rather focus on attaining sample sizes that minimise overfitting and ensure precise estimates
of overall risk within the model and setting of interest. we are currently writing software modules to implement the
approach.

<|EndOfText|>

prognostic factors are associated with
the risk of future health outcomes in
individuals with a particular health
condition or some clinical start point
(eg, a particular diagnosis). research
to identify genuine prognostic factors is
important because these factors can
help improve risk stratification,
treatment, and lifestyle decisions, and
the design of randomised trials.
although thousands of prognostic
factor studies are published each year,
often they are of variable quality and
the findings are inconsistent.
systematic reviews and meta-analyses
are therefore needed that summarise
the evidence about the prognostic
value of particular factors. in this
article, the key steps involved in this
review process are described.
systematic reviews and meta-analyses are common
in the medical literature, routinely appearing in
specialist and general medical journals, and forming
the cornerstone of cochrane. the majority of systematic
reviews focus on summarising the benefit of one or more
therapeutic interventions for a particular condition.
however, they are also important for summarising
other evidence, such as the accuracy of screening and
diagnostic tests,1
 the causal association of risk factors
for disease onset, and the prognostic ability of bespoke
factors and biomarkers. prognostic evidence arises from
prognosis studies, which aim to examine and predict
future outcomes (such as death, disease progression,
side effects or medical complications like pre-eclampsia)
in people with a particular health condition or start point
(such as those developing a certain disease, undergoing
surgery, or women who are pregnant).
the progress (prognosis research strategy)
framework defines four types of prognosis research
objectives: (a) to summarise overall prognosis (eg,
overall risk or rate) of health outcomes for groups with a
particular health condition2
; (b) to identify prognostic
factors associated with changes in health outcomes3
;
(c) to develop, validate, and examine the impact of
prognostic models for individualised prediction of
such outcomes4
; and (d) to identify predictors of an
individual‚Äôs response to treatment
summary points
‚Ä¢‚ÄÇ primary studies to identify prognostic factors are abundant, but often findings are inconsistent and quality is variable. systematic
reviews and meta-analyses are urgently needed to identify, evaluate, and summarise prognostic factor studies and their findings.
‚Ä¢‚ÄÇ a clear review question should be defined using the picots system (population, index prognostic factor, comparator prognostic factors,
outcome, timing, setting), and a transparent search undertaken for eligible articles. broad search strings may be required, leading to a
large number of articles to screen.
‚Ä¢‚ÄÇ a data extraction phase is needed to obtain the relevant information from each study. a modification of charms (checklist for critical
appraisal and data extraction for systematic reviews of prediction modelling studies) can be used for prognostic factors (charms-pf).
‚Ä¢‚ÄÇ the quips tool (quality in prognostic factor studies) can be used to examine each study‚Äôs risk of bias. unfortunately, many primary
studies may have a high risk of bias because of poor design standards, conduct, and analysis. applicability of a study should also be
checked.
‚Ä¢‚ÄÇ if appropriate, meta-analysis can be used to combine prognostic effect estimates (such as hazard ratios, risk ratios, or odds ratios)
across studies to produce an overall summary of a factor‚Äôs prognostic effect. between-study heterogeneity should be expected and
accounted for.
‚Ä¢‚ÄÇ ideally separate meta-analyses should be performed for unadjusted and adjusted prognostic effect estimates; adjusted estimates are
important to examine a factor‚Äôs independent prognostic value over and above (that is, after adjustment for) other prognostic factors.
‚Ä¢‚ÄÇ separate meta-analyses may also be required for each method of measurement (for factors and outcomes), each approach to handling
continuous factors, and each type of estimate (such as hazard ratios or odds ratios).
‚Ä¢‚ÄÇ publication bias is a major threat to the validity of meta-analyses of prognostic factor studies based on published evidence, and may
cause small-study effects (asymmetry on a funnel plot).
‚Ä¢‚ÄÇ remark (reporting recommendations for tumour marker prognostic studies) and prisma (preferred reporting items for systematic
reviews and meta-analyses) can be used to guide the reporting of the systematic review and meta-analysis of prognostic factor studies;
the degree of confidence in the summary results from the review may be examined by use of adapted forms of grade (grades of
recommendation, assessment, development, and evaluation) for interventions and diagnostic test accuracy studies.
‚Ä¢‚ÄÇ availability of individual participant data from primary prognostic factor studies may alleviate many of the challenges.
.5 each objective requires specific methods and tools for conducting
a systematic review and meta-analysis. two recent
articles provided a guide to undertaking reviews and
meta-analysis of prognostic (prediction) models.6 7 in
this article, we focus on prognostic factors.
a prognostic factor is any variable that is associated
with the risk of a subsequent health outcome
among people with a particular health condition.
different values or categories of a prognostic factor
are associated with a better or worse prognosis
of future health outcomes. for example, in many
cancers, tumour grade at the time of histological
examination is a prognostic factor because it is
associated with time to disease recurrence or death.
each grade represents a group of patients with a
different prognosis, and the risk or rate (hazard) of the
outcome increases with higher grades. many routinely
collected patient characteristics are prognostic,
such as sex, age, body mass index, smoking status,
blood pressure, comorbidities, and symptoms. many
researched prognostic factors are biomarkers, which
include a diverse range of blood, urine, imaging,
electrophysiological, and physiological variables.
prognostic factors have many potential uses,
including aiding treatment and lifestyle decisions,
improving individual risk prediction, providing novel
targets for new treatment, and enhancing the design
and analysis of randomised trials.3
 this motivates
so-called ‚Äúprognostic factor research‚Äù to identify
genuine prognostic factors (sometimes also called
‚Äúpredictor finding studies‚Äù8
).9
 although thousands of
such studies are published each year, often they are
of variable quality and have inconsistent findings.
systematic reviews and meta-analyses are therefore
urgently needed to summarise the evidence about
the prognostic value of particular factors.10-12 in this
article, we provide a step-by-step guide on conducting
such reviews. our aim is to help readers, healthcare
providers, and researchers understand the key
principles, methods, and challenges of reviews of
prognostic factor studies.
step 1: defining the review question
the first step is to define the review question. a
review of prognostic factor studies falls within the
second objective of the progress framework2
because it aims to summarise the prognostic value
of a particular factor (or each of multiple factors) for
relevant health outcomes and time points in people
with a specific health condition (eg, disease). some
reviews are broad; for example, riley and colleagues
aimed to identify any prognostic factor for overall and
disease free survival in children with neuroblastoma
or ewing‚Äôs sarcoma.13 other reviews have a narrower
focus; for example, hemingway and colleagues aimed
to summarise the evidence on whether c reactive
protein (crp) is a prognostic factor for fatal and nonfatal events in patients with stable coronary disease.14
this crp review is used as an example throughout this
article.
charms (checklist for critical appraisal and
data extraction for systematic reviews of prediction
modelling studies) provides guidance for formulating
a review question (table 1 in the article by moons and
colleagues15). although charms was developed15
and refined6
 for reviews of prediction model studies,
it can also be used to define and frame the question
for reviews of prognostic factor studies. charms15 and
subsequent improvements6
 propose a modification
of the traditional pico system (population, index
intervention, comparison and outcome) used in
systematic reviews of therapeutic intervention studies.
the modification is called picots, because it also
considers timing and setting (box 1). in the context
of prognostic factor reviews, the ‚Äúp‚Äù of population
and the ‚Äúo‚Äù of outcome remain largely the same as in
the original pico system, but the ‚Äúi‚Äù refers to index
prognostic factors and the ‚Äúc‚Äù refers to other prognostic
factors that can be considered as comparators in some
way. for example, the aim may be to compare the
prognostic ability of a certain index factor with one
or more other (that is, comparator) prognostic factors;
or to investigate the adjusted prognostic value of a
particular index factor over and above (adjusted for)
other (that is, comparator) prognostic factors. if the
only aim is to summarise the unadjusted prognostic
effect of a particular index factor, which is not
generally recommended, then no comparator factor is
being considered. the ‚Äút‚Äù denotes timing and refers
to two concepts of time. firstly, at what time point the
prognostic factors under review are to be measured or
assessed (that is, the time point at which prognosis
information is required); and secondly, over what time
period the outcomes are predicted by these factors. the
‚Äús‚Äù of setting refers to the setting or context in which
the index prognostic factors are to be used because
the prognostic ability of a factor may change across
healthcare settings.
an important component of reviews of prognostic
factors is whether unadjusted or adjusted estimates
of the index prognostic factors will be summarised, or
both. we recommend that reviewers primarily focus
on adjusted prognostic factor estimates because they
reveal whether a certain index factor contributes
independently to the prediction of the outcome over and
above (that is, after adjustment for) other prognostic
factors. in particular, for each clinical scenario there
are often so-called ‚Äúestablished‚Äù or ‚Äúconventional‚Äù
prognostic factors that are always measured.
therefore, for prognostic factors under review, it is
important to understand whether they contribute
additional (sometimes called ‚Äúindependent‚Äù)
prognostic information to the routinely measured
ones. this means that reviewers need adjusted (and
not unadjusted or crude) prognostic effect estimates
to be estimated and reported in primary prognostic
factor studies. such adjusted prognostic estimates
are typically derived from a multivariable regression
model containing the established prognostic factors
plus each index prognostic factor of interest.
for example, consider a logistic regression of a
binary outcome including three adjustment factors (a1
,
a2
, and a3
) and one new index prognostic factor (x1
),
which is expressed as:
ln(p/(1‚àíp)) = Œ±+Œ≤1
a1
+Œ≤2
a2
+Œ≤3
a3
+Œ≤4
x1
here, ‚Äúp‚Äù is the probability of the outcome. after
estimation of all the unknown parameters (that is, Œ±,
Œ≤1
, Œ≤2
, Œ≤3
, Œ≤4
), of key interest is the estimated Œ≤4
. this
parameter provides the adjusted prognostic effect of
the index prognostic factor and reveals its independent
contribution to the prediction of the outcome over and
above the prognostic effects of the other (established
comparator) factors a1
, a2
, and a3
 combined.
the need to focus on adjusted prognostic effects is
no different from (systematic reviews of) aetiological
studies, in which the focus is on estimating the
association of a certain causal risk factor after
adjustment for other risk factors. in such causal
research, these factors are usually referred to as
‚Äúconfounders‚Äù rather than as ‚Äúother prognostic
table 1 | charms-pf checklist of key items to be extracted from primary studies of prognostic factors, based on additions and modifications of the
original charms checklist for primary studies of prediction models15
domain and key items general applicability risk of bias
source of data:
source of data (eg, cohort, case control, randomised trial, or registry data) x x x
participants:
participant eligibility and recruitment method (eg, consecutive participants, location, number of centres, setting, inclusion and
exclusion criteria)
x x x
participant description x x
details of treatments received (if relevant) x x
study dates x x
outcomes to be predicted:
definition and method for measurement of outcomes x x
was the same outcome definition (and method for measurement) used in all participants? x
types of outcomes (eg, single or combined endpoints)? x x
were the outcomes assessed without knowledge of the candidate prognostic factors (that is, blinded)? x
were candidate prognostic factors part of the outcome (eg, when using a panel or consensus outcome measurement)? x
time of outcome occurrence or summary of duration of follow-up x x x
prognostic factors (index and comparator prognostic factors):
number and type of prognostic factors (eg, obtained from demographics, patient history, physical examination, additional testing,
disease characteristics)
x x
definition and method for measurement of prognostic factors x x
timing of prognostic factor measurement (eg, at patient presentation, diagnosis, treatment initiation, at the end of surgery) x x
were prognostic factors assessed blinded for outcome, and for each other (if relevant)? x
handling of prognostic factors in the analysis (eg, continuous, linear, non-linear transformations or categorised) x
sample size:
was a sample size calculation conducted and, if so, how? x
number of participants and number of outcomes or events x
number of outcomes or events in relation to the number of candidate prognostic factors (events per variable) x
missing data:
number of participants with any missing value (in the prognostic factors and outcomes) x x
number of participants with missing data for each prognostic factor of interest x
details of attrition (loss to follow-up) and, for time-to-event outcomes, number of censored observations (ideally in each category
for those categorical prognostic factors of interest)
x
handling of missing data (eg, complete case analysis, imputation, or other methods) x
analysis:
modelling method (eg, linear, logistic, cox, parametric survival, competing risks) regression) x x
how modelling assumptions were checked; in particular, for time-to-event outcomes and the analysis of hazard ratios, the method
for assessing non-proportional hazards (non-constant hazard ratios over time)
x
method for selection of prognostic factors for inclusion in multivariable modelling (eg, all candidate prognostic factors considered,
preselection of established prognostic factors, retain only those significant from univariable analysis)
x
method for selection or exclusion of prognostic factors (including those of interest and those used as adjustment factors) during
multivariable modelling (eg, backward or forward selection, or full model approach including all factors regardless), and criteria
used for any selection or exclusion (eg, p value, akaike information criterion)
x
method of handling each continuous prognostic factor (eg, dichotomisation, categorisation, linear, non-linear), including values
of any cutpoints used and their justification; for non-linear trends, the method of identifying non-linear relationships (eg, splines,
fractional polynomials)
x
results:
unadjusted and adjusted prognostic effect estimates (eg, risk ratios, odds ratios, hazard ratios, mean differences) for each prognostic factor of interest, and the corresponding 95% confidence interval (or variance or standard error). details of any non-linear
relationships and whether modelling assumptions hold; in particular, for time-to-event outcomes, any evidence of non-proportional
hazards (non-constant hazard ratios) for each prognostic factor of interest
x x x
for each extracted adjusted prognostic effect estimate of interest, the set of adjustment factors used x x x
interpretation and discussion:
interpretation of presented results x x
comparison with other studies, discussion of generalisability, strengths and limitations x x
charms=checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies. charms-pf enables reviewers to describe, assess (eg, for applicability or risk of
bias), and summarise (individually and within a meta-analysis) primary studies.
factors,‚Äù which is the term typically used for prognosis
research. the crude (unadjusted) prognostic effect of
some index factors may completely disappear after
adjustment and is therefore rather uninformative,
especially because prognostication in healthcare is
rarely based on a single prognostic factor but rather on
the information from multiple prognostic factors.4
this article focuses on systematic reviews to
summarise prognostic factor effect estimates.
some primary studies may also evaluate an index
factor‚Äôs added value in terms of improvement in risk
classification and clinical use (eg, measures such as
net reclassification improvement and net benefit),
and change in prediction model performance (eg, by
calculating the change in the concordance index, also
known as the c statistic or area under the receiver
operating characteristics (roc) curve).17-20 however,
this is beyond the scope of this article, and we refer the
reader to other relevant sources.6 21 22
application to crp review
crp is widely studied for its prognostic value in
patients with coronary disease. however, there is
uncertainty whether crp is useful because us and
european clinical practice guidelines recommend
measurement but clinical practice varies widely.
this uncertainty motivated the systematic review by
hemingway and colleagues,14 with the corresponding
picots system presented in box 1. no studies were
excluded on the basis of methodological standards,
sample size, duration of follow-up, publication year, or
language of publication.
 van der harst 2006
 blankenberg 2001
 speidl 2002
 kinjo 2005
 espinola-klein 2007
 hoffmeister 2005
 momiyama 2009
 sabatine 2007
 haim 2007
 palazzuoli 2006
 lee 2006
 brodov 2009
 minoretti 2006
overall
6.81 (0.69 to 67.47)
4.50 (1.27 to 16.03)
3.73 (1.06 to 13.15)
6.92 (2.01 to 23.74)
1.28 (0.64 to 2.57)
1.25 (0.64 to 2.43)
2.30 (1.31 to 4.03)
1.62 (1.00 to 2.62)
1.67 (1.08 to 2.61)
1.54 (1.05 to 2.27)
1.91 (1.32 to 2.76)
1.31 (1.02 to 1.69)
1.41 (1.11 to 1.78)
1.65 (1.34 to 2.04)
0.5 1 1 2 5 0
study adjusted risk ratio
(95% ci)
adjusted risk ratio
(95% ci)
2
9
2
11
7
9
6
8
4
2
3
9
14
no of adjustment factors
additional to core set
0.55
1.72
1.74
1.81
4.99
5.33
6.96
8.66
9.73
11.46
12.05
17.08
17.91
100.00
weight
(%)
fig 1 | forest plot showing the study specific estimates and meta-analysis summary
result of the adjusted prognostic effect (risk ratio) of c reactive protein taken from
the review of hemingway and colleagues14; all studies were adjusted for a core set of
existing prognostic factors (age, sex, smoking status, diabetes, obesity, and lipids),
plus up to 14 other prognostic factors. meta-analysis results shown are based on
a random effects meta-analysis model with dersimonian and laird estimation of
the between-study variances. the summary result is identical to hemingway and
colleagues,14 but the confidence interval is wider because we used the hartung-knapp
approach to account for uncertainty in variance estimates.16 although ‚Äúrisk ratio‚Äù is
used, the estimates actually correspond to a mixture of risk ratios, odds ratios, and
hazard ratios
box 1: six items (picots) defining the question for systematic reviews of prognostic factor studies, based on charms (checklist for critical
appraisal and data extraction for systematic reviews of prediction modelling studies)6 15 and applied to a review of the adjusted prognostic
value of c reactive protein (crp)14
‚Ä¢ population: define the target population forwhich prognostic factors under revieware to be used. for example, crp review: patientswith stable
coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of acute coronary syndrome atleasttwo
weeks before prognostic factor (crp) measurement.
‚Ä¢ index prognostic factor: define the factors forwhich prognostic value is under review. for example, crp review: crpwas the single biomarker
reviewed for its prognostic value.
‚Ä¢ comparator prognostic factors: comparator prognostic factors can be considered in a reviewin variousways. for example, the aim could be to
compare the prognostic ability of a certain index factorwith two or more other (thatis, comparator) prognostic factors; or to reviewthe adjusted
prognostic value of a particular index factor‚Äîthatis, over and above (adjusted for, independent of) other existing (thatis, comparator) prognostic
factors. ifthe only aim is to summarise the unadjusted prognostic effect of a particular index factor, then no comparator factor is being considered.
for example, crp review: the focuswas on the adjusted prognostic value of crp‚Äîthatis, its prognostic effect after adjusting for existing (comparator)
prognostic factors. in particular, adjustmentfor the following conventional prognostic factorswas of interest: age, sex, smoking status, obesity,
diabetes, and one or more lipid variables (from total cholesterol, lowdensity lipoprotein cholesterol, high density lipoprotein cholesterol,
triglycerides) and inflammatory markers (fibrinogen, interleukin 6,white cell count).
‚Ä¢ outcome: define the outcomes forwhich the prognostic ability ofthe factor(s) under revieware of interest. for example, crp review: outcome events
were defined as coronary (coronary death, sudden cardiac death, acute non-fatal myocardial infarction, primary percutaneous coronary intervention,
unplanned emergency admissionswith unstable angina), cardiovascular (when coronary eventswere reported in combinationwith heartfailure,
stroke, or peripheral arterial disease), and all cause mortality.
‚Ä¢ timing: define firstly atwhattime points the prognostic factors (index and comparators) are to be used (thatis, the time point of prognostication),
and secondly overwhattime period the outcomes are predicted by these factors. for example, crp review: the crp measurement had to be done at
leasttwoweeks after diagnosis and all follow-up information on the outcomes (alltime periods)was extracted from the studies.
‚Ä¢ setting: define the intended setting and role ofthe prognostic factors under review. for example, crp review: crp measurementwas studied in
primary and secondary care to provide prognostic information about patients diagnosedwith coronary heart disease; this information may be useful
for healthcare professionals treating and managing such patients.
step 2: searching for and selection of eligible studies
the next step is to identify primary studies that are
eligible for review; studies that address the review
question defined in step 1 following the picots
framework. unfortunately, it is more difficult to identify
prognostic factor studies than randomised trials of
interventions. prognosis studies do not tend to be
indexed (‚Äútagged‚Äù) because a taxonomy of prognosis
research is not widely recognised. moreover, compared
with studies of interventions, there is much more
variation in the design of prognostic factor studies (eg,
data from cohort studies, randomised trials, routine
care registries, and case-control studies can all be
used), patient inclusion criteria, prognostic factor and
outcome measurement, follow-up time, methods of
statistical analysis, and adjustment of (and number of)
other prognostic factors (covariates). between-study
heterogeneity is therefore the rule rather than the
exception in prognostic factor research. it is essential
that systematic reviews of prognostic factor studies
define the study inclusion and exclusion criteria
based on the picots structure (step 1) because this
determines the study search and selection strategy.
typically, broad search and selection filters are
required that combine terms related to prognosis
research (such as prognostic, predict, predictor, factor,
independent) with domain or disease specific terms
(such as the name of prognostic factors and the targeted
disease or patient population).23 a broad search comes
at the (often considerable) expense of retrieving many
irrelevant records. geersing and colleagues24 validated
various existing search strategies for prognosis studies
and suggested a generic filter for identifying studies of
prognostic factors,23 25 26 which extended the work of
ingui, haynes, and wong.23 25 26 when tested in a single
review of prognostic factors, this generic filter had a
number needed to read of 569 to identify one relevant
article, emphasising the difficulty in targeting prognostic
factor articles. the number needed to read could be
considerably reduced when specific factors or populations
are added to the filter. even then, care is needed to be
inclusive because multiple terms are often used for the
same meaning; for example, biomarker mycn is also
referred to as n-myc and nmyc, among other terms.13
once the search is complete, each potentially
relevant study must be screened for its applicability to
the review question. because of the heterogeneity in
prognostic factor studies, during this study selection
phase more deviations from the defined picots (in
step 1) are possible (far greater than what is typically
encountered during the selection of randomised
intervention studies). the applicability of this primary
study selection should firstly be based on title and
abstract screening, followed by full text screening,
both ideally done by two researchers independently.
any discrepancies should be resolved through
discussion, potentially with a third reviewer. to check
if any relevant articles have been missed, it is helpful to
share the list of identified articles with researchers in
the field to examine the reference lists of these articles
and to perform a citation search.
application to crp review
hemingway and colleagues included any prospective
observational study that reported risk of subsequent
events among patients with stable coronary disease
in relation to measured crp values.14 eligible studies
had to include patients with stable coronary disease,
defined as clinically diagnosed angina pectoris or
angiographic disease, or a history of previous acute
coronary syndrome at least 2 weeks before crp
measurement. hemingway and colleagues searched
medline between 1966 and 25 november 2009 and
embase between 1980 and 17 december 2009, using
a search string containing terms for coronary disease,
prognostic studies, and crp. the search identified
1566 articles, of which 83 fulfilled the inclusion
criteria. if specific terms for crp had not been included
in the search string, then the total number of identified
articles would have far exceeded 1566.
step 3: data extraction
the next step is to extract key information from each
selected study. data extraction provides the necessary
data from each study, which enables reviewers to
examine their (eventual) applicability to the review
question and their risk of bias (see step 4). this step
also provides the information required for subsequent
qualitative and quantitative (meta-analysis) synthesis
of the evidence across studies. the charms checklist
gives explicit guidance (table 2 in the article by moons
and colleagues15) about which key items across 11
domains should be extracted from primary studies
of prediction models, and for what reason (that is,
to provide general information about the primary
study, to guide risk of bias assessment, or to assess
applicability of the primary study to the review
question). based on our experience of conducting
systematic reviews of prognostic factor studies, we
modified the original charms checklist for prediction
model studies to make it suitable for data extraction
in reviews of prognostic factors (here referred to as
charms-pf; table 1). this basically means that three
domains typically addressing multivariable prediction
modelling aspects were combined to one overall
analysis domain, while other domain names and key
items were slightly reworded or extended. reasons for
extraction of each key item are similar to charms for
prediction models. because we developed the original
charms checklist, a wider consensus of the charmspf content was not considered necessary.
reviewers should extract fundamental information
from the primary prognostic factor studies, such as the
dates, setting, study design, definitions of start points,
outcomes, follow-up length, and prognostic factors;
reviewers will often find large heterogeneity in this
information across studies. the extracted information
can be summarised in tables of study characteristics.
in addition, more specific information is needed to
properly assess applicability and risk of bias (see step
4), such as methods used to measure prognostic factors
and outcomes, handling missing data, attrition (loss to
follow-up), and whether estimated associations of the
prognostic factors under review were adjusted for other
prognostic factors. this information also enhances the
potential for meta-analysis and the presentation and
interpretation of subsequent summary results (see
steps 5-8).
to enable meta-analysis of prognostic factor
studies, the key elements to extract are estimates,
and corresponding standard errors or confidence
intervals, of the prognostic effect for each factor
of interest; for example, the estimated risk ratio or
odds ratio (for binary outcomes), hazard ratio (for
time-to-event outcomes), or mean difference (for
continuous outcomes). as most prognostic factor
studies consider time-to-event outcomes (including
censored observations and different follow-up lengths
for patients), hazard ratios are often the most suitable
effect measure. a concern is that hazard ratios may not
be constant over time, and therefore any evaluations
of non-proportional hazards (that is, non-constant
hazard ratios for the prognostic factors of interest)
should also be extracted; however, such information is
rarely reported in sufficient detail.
unfortunately, many prognostic factor studies do
not adequately report estimated prognostic effect
measures or their precision. for this reason, methods
are available to restore the missing information upon
data extraction. in particular, parmar and colleagues28
and tierney and colleagues29 describe how to obtain
unadjusted hazard ratio estimates (and their variances)
when they are not reported directly. for example,
under assumptions, the number of outcomes (events)
and an available p value (eg, from a log rank test or
cox regression) can be used to indirectly estimate the
unadjusted hazard ratio between two groups defined
by a particular factor (eg, ‚Äúhigh‚Äù versus ‚Äúnormal‚Äù
levels). perneger and colleagues30 report how to derive
unadjusted hazard ratios from survival proportions,
and p√©rez and colleagues suggest using a simulation
approach.31 even with such indirect estimation
methods, not all results can be obtained. for example,
in a systematic review of 575 studies investigating
prognostic factors in neuroblastoma,32 the methods of
parmar and colleagues were used to obtain 204 hazard
ratio estimates and their confidence intervals; but this
represented only 35.5% of the potential evidence.
although indirect estimation methods help retrieve
unadjusted prognostic factor effect estimates, they
often have limited value for obtaining adjusted effect
table 2 | quips tool (quality in prognostic factor studies), which can be used to classify risk of bias of prognostic factor studies
domains signalling items risk of bias ratings
1. study participation (a) adequate participation in the study by eligible persons
(b) description of the target population or population of interest
(c) description of the baseline study sample
(d) adequate description of the sampling frame and recruitment
(e) adequate description of the period and place of recruitment
(f) adequate description of inclusion and exclusion criteria
high: the relationship between the pf and outcome is very likely to be
different for participants and eligible non-participants
moderate: the relationship between the pf and outcome may be
different for participants and eligible non-participants
low: the relationship between the pf and outcome is unlikely to be
different for participants and eligible non-participants
2. study attrition (a) adequate response rate for study participants
(b) description of attempts to collect information on participants who
dropped out
(c) reasons for loss to follow-up are provided
(d) adequate description of participants lost to follow-up
(e) there are no important differences between participants who completed
the study and those who did not
high: the relationship between the pf and outcome is very likely to be
different for completing and non-completing participants
moderate: the relationship between the pf and outcome may be
different for completing and non-completing participants
low: the relationship between the pf and outcome is unlikely to be
different for completing and non-completing participants
3. prognostic factor
measurement
(a) a clear definition or description of the pf is provided
(b) method of pf measurement is adequately valid and reliable
(c) continuous variables are reported or appropriate cutpoints are used
(d) the method and setting of measurement of pf is the same for all study
participants
(e) adequate proportion of the study sample has complete data for the pf
(f) appropriate methods of imputation are used for missing pf data
high: the measurement of the pf is very likely to be different for
different levels of the outcome of interest
moderate: the measurement of the pf may be different for different
levels of the outcome of interest
low: the measurement of the pf is unlikely to be different for different
levels of the outcome of interest
4. outcome measurement (a) a clear definition of the outcome is provided
(b) method of outcome measurement used is adequately valid and reliable
(c) the method and setting of outcome measurement is the same for all
study participants
high: the measurement of the outcome is very likely to be different
related to the baseline level of the pf
moderate: the measurement of the outcome may be different related
to the baseline level of the pf
low: the measurement of the outcome is unlikely to be different
related to the baseline level of the pf
5. adjustment for other
prognostic factors
(a) all other important pfs are measured
(b) clear definitions of the important pfs measured are provided
(c) measurement of all important pfs is adequately valid and reliable
(d) the method and setting of pf measurement are the same for all study
participants
(e) appropriate methods are used to deal with missing values of pfs, such
as multiple imputation
(f) important pfs are accounted for in the study design
(g) important pfs are accounted for in the analysis
high: the observed effect of the pf on the outcome is very likely to be
distorted by another factor related to pf and outcome
moderate: the observed effect of the pf on outcome may be distorted
by another factor related to pf and outcome
low: the observed effect of the pf on outcome is unlikely to be
distorted by another factor related to pf and outcome
6. statistical analysis and
reporting
(a) sufficient presentation of data to assess the adequacy of the analytic
strategy
(b) strategy for model building is appropriate and is based on a conceptual
framework or model
(c) the selected statistical model is adequate for the design of the study
(d) there is no selective reporting of results
high: the reported results are very likely to be spurious or biased
related to analysis or reporting
moderate: the reported results may be spurious or biased related to
analysis or reporting
low: the reported results are unlikely to be spurious or biased related
to analysis or reporting
pf=prognostic factor. some wording from hayden and colleagues27 has been modified to be consistent with the terminology used in this article.7
estimates. furthermore, even when multiple studies
provide the adjusted prognostic effect of a particular
factor, the set of adjustment factors will usually differ
across studies, which complicates the interpretation of
subsequent meta-analysis results. we recommend that
reviewers predefine the core set of prognostic factors for
the outcome of interest (eg, age, sex, smoking status,
disease stage) that represents the desired ‚Äúminimal‚Äù
set of adjustment factors. an agreed process among
health professionals and researchers in the field could
be required to define this set. for example, a list of
established prognostic factors could be identified that
are routinely used within current prognostication of
the clinical population of interest.
it may also be necessary to standardise the extracted
estimates to ensure they all relate to the same scale and
direction in each study. in particular, the direction of the
prognostic effect will need standardising if one study
compares the hazard rate in a factor‚Äôs ‚Äúhigh‚Äù versus
‚Äúnormal‚Äù group, whereas another study compares
the hazard rate in the factor‚Äôs ‚Äúnormal‚Äù versus ‚Äúhigh‚Äù
group. when the outcome is defined differently across
studies, approaches to convert effect measures on
different outcome scales could be useful.33 also, to deal
with different cutpoint levels for values of a particular
factor,34 the prognostic effects of ‚Äúhigh‚Äù versus
‚Äúnormal‚Äù could be converted to prognostic effects
relating to a 1 unit increase in the factor. this requires
assumptions about the underlying distribution of the
factor. such an approach was used by hemingway and
colleagues.14 of concern, however, is that the actual
distribution of a prognostic factor may be unknown (or
even vary across studies). finally, it is also possible to
derive standardised effect estimates by standardising
the corresponding regression coefficients.35
application to crp review
hemingway and colleagues extracted background
information such as year of study start, number of
included patients, mean age, baseline coronary morbidity
(eg, proportion with stable angina), average levels of
biomarker at baseline, method of crp measurement,
follow-up duration, and number and type of events. basic
information was often missing. for example, nearly a fifth
of studies did not report the method of measurement, and
only a quarter gave the number of patients included in
the analyses and reasons for dropout. prognostic effect
estimates for crp were extracted in terms of the reported
risk ratio, odds ratio, or hazard ratio (labelled generally
as ‚Äúrisk ratio‚Äù in this article), and 95% confidence
intervals. these effect estimates were then converted to a
standardised scale comparing the highest third with the
lowest third of the (log transformed) crp distribution.
if available, separate prognostic effect estimates were
extracted for different degrees of adjustment for other
prognostic factors.
step 4: evaluating applicability and risk of bias of
primary studies
once eligible studies are identified and data are
extracted, an important next step is to assess the
applicability and risk of bias (quality) of each study in
the review. as for steps 2 and 3, ideally this is done by
two reviewers, independently, with any discrepancies
resolved. applicability refers to the extent to which a
selected study (in step 2) matches the review question
in terms of the population, timing, prognostic factors,
and outcomes (endpoints) of interest. just because
a study is eligible for inclusion does not mean it is
free from applicability concerns. some aspects of a
study may be applicable (eg, correct condition at start
point, with prognostic factors of interest evaluated)
but not others (eg, incorrect population or setting,
inappropriate outcome definition, different follow-up
time, lack of adjustment for conventional prognostic
factors). applicability is typically first assessed during
title and abstract screening, and then during this step,
so that it is based on full text screening and determined
by picots (step 1) and inclusion and exclusion criteria
of studies (step 2).
risk of bias refers to the extent to which flaws in the
study design or analysis methods could lead to bias in
estimates of the prognostic factor effects. unfortunately,
based on growing empirical evidence from systematic
reviews examining methodology quality, many primary
studies will be at high risk of bias.8 32 36-44 for prognostic
factor studies, hayden and colleagues developed the
quips checklist (quality in prognostic factor studies)
for examining risk of bias across six domains27:
study participation, study attrition, prognostic factor
measurement, outcome measurement, adjustment for
other prognostic factors, and statistical analysis and
reporting. table 2 shows the signalling items within
these domains to help guide reviewers in making low,
unclear, or high risk of bias classifications. additional
guidance may be found in general tools examining the
quality of observational studies,45 46 and the remark
guideline (reporting recommendations for tumour
marker prognostic studies) for reporting of primary
prognostic factor studies.47 48
we recommend that users first operationalise
criteria to assess the signalling items and domains
for the specific review question. for example, with
the study participation and attrition domains,
this includes defining a priori the most important
characteristics that could indicate a systematic bias
in study recruitment (study participation domain) and
loss to follow-up (study attrition domain). defining
these characteristics ahead of time will facilitate
assessment and consensus related to the importance of
potential differences that could influence the observed
association between the index prognostic factors and
outcomes of interest. definitions of sufficiently valid
and reliable measurement of the index prognostic
factors and outcomes should also be specified at
the protocol stage. similarly, the core set of other
(adjustment) prognostic factors that are deemed
necessary for the primary studies to have adjusted for,
should be predefined to facilitate judgment related to
risk of bias in domain 5.
overall assessment of the six risk of bias domains is
undertaken by considering the risk of bias information
from the signalling items for each domain, rated as
low, moderate, and high risk of bias. occasionally, item
information needed to assess the bias domains is not
available in the study report. when this occurs, other
publications that may have used the same dataset
(which often occurs in prognostic studies based on
large existing cohorts) should be consulted and study
authors should be contacted for additional information.
an informed judgment about the potential risk of bias
for each bias domain should be made independently
by two reviewers, and discussed to reach consensus.
each of the six domains needs to be rated and reported
separately because this will inform readers, flag
improvements needed for subsequent primary studies,
and facilitate future meta-epidemiological research.
we recommend defining studies with an overall ‚Äúlow
risk of bias‚Äù as those studies where all, or the most
important domains (as determined a priori), are rated
as having low (or low to moderate) risk of bias.
application to crp review
hemingway and colleagues assessed the quality of
included studies by the quality of their reporting
on 17 items derived from the remark guideline.48
the median number of study quality items reported
was seven of a possible 17, and standards did not
change between 1997 and 2009. only two studies
referred to a study protocol, with none referring to a
statistical analysis plan. hemingway and colleagues
noted that this ‚Äúmakes it difficult to know what the
specific research objectives were at the start of cohort
recruitment, at the time of crp measurement, or
at the onset of the statistical analysis.‚Äù14 only two
studies reported the time elapsed between first lifetime
presentation with coronary disease and assessment of
crp and this raised applicability concerns.
step 5: meta-analysis
meta-analysis of prognostic factor studies aims to
summarise the (adjusted) prognostic effect of each
factor of interest. in addition to missing estimates,
challenges for the meta-analyst include (a) having
different types of prognostic effect measures (eg, odds
ratios and hazard ratios), which are not necessarily
comparable30; (b) estimates without standard errors,
which is a problem because meta-analysis methods
typically weight each study by (a function of) their
standard error; (c) estimates relating to various time
points of the outcome occurrence or measurement;
(d) different methods of measurement for prognostic
factors and outcomes; (e) various sets of adjustment
factors; and (f) different approaches to handling
continuous prognostic factors (eg, categorisation,
linear, non-linear trends), including the choice of
cutpoint value when dichotomising continuous values
into ‚Äúhigh‚Äù and ‚Äúnormal‚Äù groups. many of these issues
lead to substantial heterogeneity and if a meta-analysis
is performed, summary results cannot be directly
interpreted.
generally, meta-analysis results will be most
interpretable, and therefore useful, when a separate
meta-analysis is undertaken for groups of ‚Äúsimilar‚Äù
prognostic effect measures. in particular, we suggest
considering a meta-analysis for:
‚Ä¢ hazard ratios, odds ratios, and risk ratios separately
‚Ä¢	unadjusted and adjusted associations separately
‚Ä¢	prognostic factor effects at distinct cutpoints (or groups
of similar cutpoints) separately
‚Ä¢	prognostic factor effects corresponding to a linear trend
(association) separately
‚Ä¢	prognostic factor effects corresponding to non-linear
trends separately
‚Ä¢	each method of measurement (for factors and
outcomes) separately.
ideally a meta-analysis of adjusted results should
ensure that all included estimates are adjusted for the
same set of other prognostic factors. this situation is
unlikely and so a compromise could be to ensure that
all adjusted estimates in the same meta-analysis have
adjusted for at least a (predefined) minimum set of
adjustment factors (that is, a core set of established
prognostic factors).
even when adhering to this guidance, unexplained
heterogeneity is likely to remain because of other
reasons (eg, differences in length of follow-up or in
treatments received during follow-up). therefore, if a
meta-analysis is performed, a random effects approach
is essential to allow for unexplained heterogeneity
across studies (box 2), as previously described in the
bmj.
53 this approach provides a summary estimate of
the average prognostic effect of the index factor and
the variability in effect across studies. also potentially
useful are meta-analysis methods to estimate the
trend (eg, linear effect) of a prognostic factor that has
been grouped into three or more categories within
studies (with each category compared with the
reference category). these methods generally model
the estimated prognostic effect sizes in each category
as a function of ‚Äúexposure‚Äù level (eg, midpoint or
median prognostic factor value in the category) and
account for within-study correlation and betweenstudy heterogeneity.54-58 to apply these methods,
some additional knowledge of the factor‚Äôs underlying
distribution is usually needed to help define the
‚Äúexposure‚Äù level because the chosen value can have an
impact on the results.56
application to crp review
hemingway and colleagues14 applied a random effects
meta-analysis to combine 53 adjusted prognostic effect
estimates for crp from studies that adjusted for at least
one of six conventional risk factors (age, sex, smoking
status, diabetes, obesity, and lipids). the summary
meta-analysis result was a risk ratio of 1.97 (95%
confidence interval 1.78 to 2.17), which gives the
average prognostic effect of crp (for those in the top v
bottom third of crp distribution), and suggests larger
crp values are associated with higher risk. although
there was substantial between-study heterogeneity,
nearly all estimates were in the same direction (that is,
risk ratio >1). when restricting meta-analysis to just the
13 studies that adjusted for at least all six conventional
prognostic factors, the summary risk ratio decreased to
1.65 (95% confidence interval 1.39 to 1.96), and the
between-study heterogeneity reduced. using the study
specific estimates given by hemingway and colleagues,
we updated this meta-analysis (fig 1), obtaining the
same summary result but a wider confidence interval
(1.34 to 2.04) through the hartung-knapp approach.16
step 6: quantifying and examining heterogeneity
for all meta-analyses, when there is large
heterogeneity across included studies, it may be better
not to synthesise the study results, but rather display
the variability in estimates on a forest plot without
showing an overall pooled estimate. when a metaanalysis is performed in the face of heterogeneity, it
is important to quantify and report the magnitude of
heterogeneity itself; for example, through the estimate
of (the between-study variance),62 or an approximate
95% prediction interval indicating the potential true
prognostic effect of a factor in a new population.53 63
subgroup analyses and meta-regression can be used
to examine or explore the causes of heterogeneity. a
subgroup analysis performs a separate meta-analysis
for categories defined by a particular characteristic,
such as those with a low risk of bias, those with a
follow-up of less than one year or of at least one year,
or those set in countries in europe. a better approach
is meta-regression, which extends the meta-analysis
equation shown in box 2 by including study level
covariates,64 and allows a formal comparison of metaanalysis results across groups defined by covariates
(eg, low risk of bias studies v studies at higher risk
of bias). unfortunately, subgroup analyses and metaregression are often problematic. there will often be
few studies per subgroup and low power to detect
genuine causes of heterogeneity. furthermore, study
level confounding will be rife so that it is difficult to
disentangle the associations for one covariate from
another. for example, studies with a low risk of bias
may also have a different length of follow-up or a
particular cutpoint level compared with studies at
higher risk of bias.
application to crp review
hemingway and colleagues reported that metaregression identified four study level covariates that
explained some between-study heterogeneity in the
prognostic effect of crp: definition of comparison
group, number of adjustment factors, the (log) number
of events, and the proportion of patients with stable
coronary disease (reflecting study size).14 studies
originally reporting unequal crp groups had stronger
effects than those reporting crp on a continuous scale.
for each additional adjustment factor, the summary
risk ratio decreased by 3%. the summary risk ratio
was smaller among studies with more than the median
number of outcome events, and smaller among studies
confined to stable coronary disease. there was no
evidence that the crp effect differed according to the
number of quality items reported by a study, or by the
type of prognostic effect measure provided (that is, risk
ratio, odds ratio, or hazard ratio).
step 7: examining small-study effects
the term ‚Äúsmall-study effects‚Äù refers to when there is
a systematic difference in prognostic effect estimates
for small studies and large studies.65 a particular
concern is when small studies (especially those
that are exploratory because these often evaluate
many potential prognostic factors with relatively few
outcome events) show larger prognostic effects than
larger studies. this difference may be due to chance or
heterogeneity, but a major threat here is publication
bias and selective reporting, which are endemic in
prognosis research.36-38 such reporting biases lead to
smaller studies, with (statistically) significant or larger
prognostic factor effect estimates being more likely to
be published or reported in sufficient detail, and thus
included in a meta-analysis, than smaller studies with
non-significant or smaller prognostic effect estimates.
this bias is a potential concern for unadjusted and
adjusted prognostic effects. a primary study usually
estimates an unadjusted prognostic effect for each of
multiple prognostic factors, but study authors may
only report effects that are statistically significant. in
addition, adjusted results are often only reported for
prognostic factors that retain statistical significance in
univariable and multivariable analysis. a consequence
is that meta-analysis results will be biased, with
larger summary prognostic effects than in reality, and
potentially some factors being deemed to have clinical
value when actually they do not.
the evidence for small-study effects is usually
considered on a funnel plot, which shows the study
estimates (x axis) against their precision (y axis). a
funnel plot is usually recommended if there are 10
box 2: explanation of a random effects meta-analysis of prognostic factor
effect estimates
the true prognostic effect of a factor is likely to vary from study to study; therefore
assuming a common (fixed) prognostic effectis not sensible. if yi
and var(yi
) denote the
prognostic effect estimate (eg, ln(hazard ratio), ln(odds ratio), ln(risk ratio), or mean
difference) and its variance in study i, then a generalrandom effects meta-analysis
model can be specified as:
yi
~n(Œº,var(yi
)+œÑ2
).
mostresearchers use either restricted maximum likelihood or the approach of
dersimonian and laird to estimate this model,
49 but other options are available,
including a bayesian approach.50of key interestis the estimate of Œº,which reveals
the summary (average) prognostic effect ofthe index prognostic factor of interest. the
standard deviation ofthis prognostic factor effect across studies is denoted by œÑ, and
non-zero values suggestthere is between-study heterogeneity. confidence intervals
for ¬µ should ideally accountfor uncertainty in estimated variances (in particular œÑ),51
andwe have found the approach ofhartung-knapp to be robustfor this purpose in
most settings.16 52 when synthesising prognostic effects on the log scale, the summary
results and confidence intervals require back transformation (using the exponential
function) to the original scale.
advanced multivariate meta-analysis methods are also available to handle multiple
cutpoints,59 multiple methods of measurement,59 or different adjustmentfactors in
prognostic factor studies.60 an introduction to multivariate meta-analysis has been
published in the bmj.
61
or more studies.65 the plot should ideally show a
symmetric, funnel like shape, with results from larger
studies at the centre of the funnel and smaller studies
spanning out in both directions equally. asymmetry
will arise if there are small-study effects, with a
greater proportion of smaller studies in one particular
direction. statistical tests for asymmetry in risk, odds
and hazard ratios can be used, such as peter‚Äôs and
debray‚Äôs test.66 67 contour enhanced funnel plots also
show the statistical significance of individual studies,
and ‚Äúmissing‚Äù studies are perhaps more likely to fall
within regions of non-significance if publication bias
was the cause of small-study effects. an example is
shown in figure 2.
as mentioned, small-study effects may also arise
due to heterogeneity. therefore, it is difficult to
disentangle publication bias from heterogeneity in a
single review. for example, if smaller studies used an
analysis with fewer adjustment factors, then this may
cause larger prognostic factor effects in such studies,
rather than it being caused by publication bias. a
multivariate meta-analysis could reduce the impact
of small-study effects by ‚Äúborrowing strength‚Äù from
related information.61
a related concern is that smaller prognostic factor
studies are generally at higher risk of bias than larger
studies. smaller studies tend to be more exploratory
in nature and typically based on a convenient
sample, often examining many (sometimes hundreds
of) potential prognostic factors, with relatively
few outcome events. this design leads to spurious
(due to chance) and potentially biased (due to poor
estimation properties68) prognostic effect estimates,
which are more prone to selective reporting. in
contrast, larger studies are often confirmatory studies
focusing on one or a few prognostic factors, and are
more likely to adopt a protocol driven and prospective
approach, with clearer reporting regardless of their
findings.3
 therefore, larger studies are less likely to
identify spurious prognostic factor effect estimates.
it is helpful to examine small-study effects (potential
publication bias) when restricting analysis to the
subset of studies at low risk of bias. if this approach
resolves previous issues of small-study effects in the
full meta-analysis, then it gives even more credence
to focus conclusions and recommendations on the
meta-analysis results based only on the higher quality
studies.
application to crp review
figure 2 shows a funnel plot of the study estimates
from the crp meta-analysis shown in figure 1. there is
clear asymmetry, which shows the strong potential for
publication bias. there was an insufficient number of
studies considered at low risk of bias to evaluate smallstudy effects in a subset of higher quality studies.
step 8: reporting and interpretation of results
as with all research studies, clear and complete reporting
is essential for reviews of prognostic factor studies.
most of the reporting guidelines of prisma (preferred
reporting items for systemic reviews and meta-analyses)
and moose (meta-analysis of observational studies
in epidemiology) will be relevant,69 70 and should be
complemented by remark,47 48 which was aimed
at primary prognostic factor studies. more specific
guidance for reporting systematic reviews of prognostic
factor studies is under development.
interpretation and translation of summary metaanalysis results is an important final step. the
guidance in the previous steps is the essential input
for this step. discussion is necessary on whether and
how the prognostic factors identified may be useful
in practice (that is, translation of results to clinical
practice), and what further research is necessary.
ideally impact studies (eg, randomised trials that
compare groups which do and do not use a prognostic
factor to inform clinical practice) are needed before
strong recommendations for clinical practice are
made; however, these studies are rare and outside the
scope of the review framework outlined in this article.
to interpret the certainty (confidence) of the summary
results of a review of intervention effectiveness, grade
(grades of recommendation, assessment, development,
and evaluation) was developed. this approach assesses
the overall quality of and certainty in evidence for
the summary estimates of the intervention effects by
addressing five domains: risk of bias, inconsistency,
imprecision, indirectness, and publication bias. the
grade domains can be assessed using the information
obtained by the tools and methods described in the
above steps. however, it is not known whether these
domains, developed for reviews of interventions,
are equally applicable to assessing the certainty of
summary results of systematic reviews of prognostic
factor studies. compared with reviews of intervention
studies, allowing for heterogeneity (the inconsistency
domain) might be more acceptable in reviews of
in (rr)
standard error of in (rr)
studies
p < 1% 1% < p < 5% 5% < p < 10% p > 10%
1.2
0.8
0.6
0.2
0.4
0
1.0
-4 -2 0 2 4
fig 2 | evidence of funnel plot asymmetry (small-study
effects) in the c reactive protein meta-analysis shown
in figure 1. the smaller studies (with higher standard
errors) have risk ratio (rr) estimates mainly to the right
of the larger studies, and therefore give the largest
prognostic effect estimates. a concern is that this is due
to publication bias, with ‚Äúmissing‚Äù studies potentially
falling to the left side of the larger studies and in the
lighter shaded regions denoting non-significant rr
estimates prognostic factor studies because of the inevitable
heterogeneity caused by study differences in methods
of measurement, adjustment factors, and statistical
analysis methods, among others. furthermore, the
threat of selective reporting or publication bias in
reviews of prognostic factor studies may be more
severe than in reviews of intervention studies because
of the problems of exploratory studies, poor reporting,
and biased analysis methods.
there is limited empirical evidence for using the
existing domains to grade the certainty of summary
estimates of prognostic factor studies, although a first
attempt has been made71; in addition, an assessment
has been performed on grading the certainty of evidence
of summary estimates of overall prognosis studies.72
reviewers need to be especially cautious when
comparing the adjusted prognostic value of multiple
index factors, for example, to conclude whether the
summary adjusted hazard ratio for prognostic factor a
is larger than that for factor b. usually different sets of
studies will be available for each index factor, and so
the comparison will be indirect and potentially biased.
moreover, the studies evaluating factor a may often
have used different sets of adjustment factors (other
prognostic factors) than those evaluating factor b. it
will be rare to find studies on different index factors
that used exactly the same set of adjustment factors. we
therefore recommend reviewers restrict comparisons
(of the adjusted prognostic value) of two or more index
factors to those studies that at least used a similar,
minimally required set of adjustment factors.73 even
then, due to different scales and distributions of each
factor (eg, continuous or binary), a simple comparison
of the prognostic effect sizes (eg, hazard ratio for factor
a v hazard ratio for factor b) may not be straightforward.
application to crp review
the meta-analysis results suggest crp is a prognostic
factor for the risk of death and non-fatal cardiovascular
events, even when only including the largest studies
that adjusted for all six conventional prognostic
factors. in their discussion, hemingway and colleagues
downgraded the meta-analysis findings because of a
strong concern about the quality and reliability of the
underlying evidence.14 the absence of prespecified
protocols, poor and potentially biased reporting,
and strong potential for publication bias prevented
the authors from making firm conclusions about
whether crp has prognostic value after adjustment
for established prognostic factors. they state that
the concerns ‚Äúexplicitly challenge the statement for
healthcare professionals made by the centers for
disease control that measuring crp is both ‚Äòuseful‚Äô
and ‚Äòindependent‚Äô as a marker of prognosis.‚Äù74
summary
in this article, we described the key steps and methods
for conducting a systematic review and meta-analysis
of prognostic factor studies. current reviews are often
limited by the quality and heterogeneity of primary
studies.75 76 we expect the prevalence of such reviews
to grow rapidly, especially as cochrane has recently
embarked on prognosis reviews (see also the cochrane
prognosis methods group website www.methods.
cochrane.org/prognosis).77 our guidance will help
researchers to write grant applications for reviews of
prognostic factor studies, and to develop protocols
and conduct such reviews. protocols of prognostic
factor reviews should be published ideally at the same
time as the review is registered, for example within
prospero, the international prospective register of
systematic reviews (www.crd.york.ac.uk/prospero/),
or the cochrane database.77 our guidance will also
allow readers and healthcare providers to better judge
reports of prognostic factor reviews.
finally, we note that some of the limitations described
(eg, use of different cutpoint values across studies)
could be alleviated if the individual participant data
were obtained from primary prognostic factor studies78
rather than being extracted from study publications;
although, this may not solve all problems (eg, quality
of original study, availability of different adjustment
factors).79 further discussion on individual participant
data meta-analysis of prognostic factor studies is given
elsewhere.80

<|EndOfText|>

sample size considerations for the
external validation of a multivariable
prognostic model: a resampling study
after developing a prognostic model, it is essential to evaluate the performance of the model in samples independent from those used to develop the model, which is often referred to as external validation. however, despite its importance, very little is known about the sample size requirements for conducting an external validation. using a
large real data set and resampling methods, we investigate the impact of sample size on the performance of six published prognostic models. focussing on unbiased and precise estimation of performance measures (e.g. the c-index,
d statistic and calibration), we provide guidance on sample size for investigators designing an external validation
study. our study suggests that externally validating a prognostic model requires a minimum of 100 events and
keywords: prognostic model; sample size; external validation
1. introduction
prognostic models are developed to estimate an individual‚Äôs probability of developing a disease or outcome in the future. a vital step toward accepting a model is to evaluate its performance on similar individuals separate from those used in its development, which is often referred to as external validation or
transportability [1,2]. however, despite the widespread development of prognostic models in many areas
of medicine [3‚Äì5], very few been externally validated [6‚Äì8].
to externally validate a model is to evaluate its predictive performance (calibration and discrimination)
using a separate data set from that used to develop the model [9]. it is not repeating the entire modelling
process on new data, refitting the model to new ‚Äòvalidation‚Äô data, or fitting the linear predictor (prognostic
index) from the original model as a single predictor to new data [9]. it is also not necessarily comparing the
similarity in performance to that obtained during the development of the prognostic model. whilst in some
instances a difference in the performance can be suggestive of deficiencies in the development study, the
performance in the new data may still be sufficiently good enough for the model to be potentially useful.
the case-mix (i.e., the distribution of predictors included in the model) will influence the performance
of the model [10]. it is generally unlikely that the external validation data set will have an identical casemix to the data used for development. indeed, it is preferable to use a slightly different case-mix in
external validation to judge model transportability. successful external validation studies in diverse
settings (with different case-mix) indicate that it is more likely that the model will be generalizable to
plausibly related, but untested settings [11].
despite the clear importance of external validation, the design requirements for studies that attempt to
evaluate the performance of multivariable prognostic models in new data have been little explored
[7,12,13]. published studies evaluating prognostic models are often conducted using sample sizes that
are clearly inadequate for this purpose, leading to exaggerated and misleading performance of the
prognostic model [7]. finding such examples is not difficult [14‚Äì16]. for example, a modified
thoracoscore, to predict in-hospital mortality after general thoracic surgery, was evaluated using 155
patients, but included only eight events (deaths). a high c-index value was reported, 0.95 (95% confidence interval 0.91 to 0.99) [16]. in the most extreme case, a data set with only one outcome event
was used to evaluate a prognostic model [14]. in this particular study, an absurd value of the c-index
was reported, 1.00 (95% confidence interval 1.00 to 1.00)[sic]. concluding predictive accuracy, and thus
that the model is fit for purpose, on such limited data is nothing but misleading.
the only guidance for sample size considerations that we are aware of is based on a hypothesis testing
framework (i.e. to detect pre-specified changes in the c-statistic) and recommends that models developed
using logistic regression are evaluated with a minimum of 100 events [12]. however, a recent systematic
review evaluating the methodological conduct of external validation studies found that just under half of
the studies evaluated models on fewer than 100 events [7].
it is therefore important to provide researchers with appropriate guidance on sample size considerations when evaluating the performance of prognostic models in an external validation study. when
validating a prognostic model, investigators should clearly explain how they determined their study size,
so that their findings can be placed in context [17,18]. our view is that external validation primarily
concerns the accurate (unbiased) estimation of performance measures (e.g., the c-index). it does not necessarily include formal statistical hypothesis testing, although this may be useful in some situations.
therefore sample size considerations should be based on estimating performance measures that are sufficiently close to the true underlying population values (i.e., unbiased) along with measures of uncertainty that are sufficiently narrow (i.e., precise estimates) so that meaningful conclusions on the
model‚Äôs predictive accuracy in the target population can be drawn [9,19].
the aim of this article is to examine sample size considerations for studies that attempt to externally
validate prognostic models and to illustrate that many events are required to provide reasonable estimates of model performance. our study uses published prognostic models (qrisk2 [20], qdscore
[21] and the cox framingham risk score [22]) to illustrate sample size considerations using a resampling
design from a large data set (>2 million) of general practice patients in the uk.
the structure of the paper is as follows. section 2 describes the clinical data set and the prognostic
models. section 3 describes the design of the study, the assessment of predictive performance and the
methods used to evaluate the resampling results. section 4 presents the results from the resampling
study, which are then discussed in section 5.
2. data set and prognostic models
2.1. study data: the health improvement network
the health improvement network (thin) is a large database of anonymized primary care records
collected at general practice surgeries around the uk. the thin database currently contains medical
records on approximately 4% of the uk population. clinical information from over 2 million individuals
(from 364 general practices) registered between june 1994 and june 2008 form the data set. the data
have previously been used in the external validation of a number of prognostic models (including those
considered in this study) [23‚Äì30]. there are missing data for various predictors needed to use the prognostic models. for simplicity, we have used one of the imputed data sets from the published external
validation studies, where details on the imputation strategy can be found [23,24].
2.2. prognostic models
at the core of the study are six sex-specific published models for predicting the 10-year risk of developing cardiovascular disease (cvd) (qrisk2 [20], and cox framingham [22]) and the 10-year risk of developing type 2 diabetes (qdscore [21]). all six prognostic models are all predicting time-to-event
outcomes using cox regression. none of these models were developed using thin, but thin has previously been used to evaluate their performance in validation studies [23,24].
qrisk2 was developed using 1.5 million general practice patients aged between 35 and 74 years
(10.9 million person years of observation) contributing 96 709 cardiovascular events from the
qresearch database [20]. separate models are available for women (41 042 cvd events) and
men (55 667 cvd events), containing 13 predictors, 8 interactions and fractional polynomial terms
for age and body mass index (www.qrisk.org).
215
cox framingham was developed using 8491 framingham study participants aged 30 to 74 years contributing 1274 cardiovascular events [22]. separate models are available for women (456 cvd events)
and men (718 cvd events), each containing 7 predictors.
qdscore was developed on 2.5 million general practice patients aged between 25 and 79 years (16.4
million person years of observation) contributing 72 986 incident diagnoses of type 2 diabetes from the
qresearch database [21]. separate models are available for women and men, each containing 12 predictors, 3 interactions and fractional polynomial terms for age and body mass index (www.qdscore.org).
3. methods
3.1. resampling strategy
a resampling strategy was applied to examine the influence of sample size (more specifically, the number of events) on the bias and precision in evaluating the performance of published prognostic models.
samples were randomly drawn (with replacement) from the thin data set so that the number of
events in each sample was fixed at 5, 10, 25, 50, 75, 100, 150, 200, 300, 400, 500 or 1000 by stratified
sampling according to the outcome ensuring that the proportion of events in each sample was the same
as the overall proportion of events in the thin data set (table i). the sample sizes for each prognostic
model at each value of number of events can be found in the supporting information. for each scenario
(i.e., for each sample size), 10 000 samples (denoted b) were randomly drawn and performance measures were calculated for each sample.
3.2. performance measures
the performance of the prognostic models was quantified by assessing aspects of model discrimination
(the c-index [31] and d statistic [32]), calibration [9,33], and other performance measures (r2
d [34], r2
oxs
[35] and the brier score for censored data [36]).
discrimination is the ability of a prognostic model to differentiate between people with different outcomes, such that those without the outcome (e.g., alive) have a lower predicted risk than those with the
outcome (e.g., dead). for the survival models used within this study, which are time-to-event based,
discrimination is evaluated using harrell‚Äôs c-index, which is a generalization of the area under the
receiver operating characteristic curve for binary outcomes (e.g., logistic regression) [31,37]. harrell‚Äôs
c-index can be interpreted as the probability that, for a randomly chosen pair of patients, the patient
who actually experiences the event of interest earlier in time has a lower predicted value. the c-index
and its standard error were calculated using the rcorr.cens function in the rms library in r.
we also examined the d statistic, which can be interpreted as the separation between two survival
curves (i.e., a difference in log hr) for two equal size prognostic groups derived from cox regression
[32]. it is closely related to the standard deviation of the prognostic index (pi = Œ≤1x1+ Œ≤2x2+‚ãØ+ Œ≤kxk),
which is a weighted sum of the variables (xi) in the model, where the weights are the regression coefficients (Œ≤i). d is calculated by ordering the values from the prognostic index, transforming them using
expected standard normal order statistics, dividing the result by Œ∫ ¬º ffiffiffiffiffiffiffi
8=œÄ p ‚âÉ1:596 and fitting this in a
single term cox regression. d and its standard error are given by the coefficient and standard error in
the single term cox regression model.
table i. ‚Äòtrue‚Äô values based on the entire thin validation cohort.
number of
individuals
number of
events (%)
performance measure
c-index d statistic r2
d œÅ2
oxs brier
score
calibration
slope
qrisk2 [20,51] women 797,373 29,507 (3.64) 0.792 1.650 0.394 0.668 0.052 0.948
men 785,733 42,408 (5.40) 0.775 1.530 0.359 0.607 0.075 1.000
cox framingham [22] women 797,373 29,507 (3.64) 0.756 1.435 0.330 0.553 0.055 0.919
men 785,733 42,408 (5.40) 0.759 1.452 0.335 0.554 0.084 1.001
qdscore [21,23] women 1,211,038 32,200 (2.66) 0.810 1.872 0.456 0.731 0.041 0.875
men 1,185,354 40,786 (3.44) 0.800 1.760 0.425 0.687 0.053 0.869
216
the calibration slope was calculated by estimating the regression coefficient in a cox regression
model with the prognostic index (the linear predictor) as the only covariate. if the slope is <1, discrimination is poorer in the validation data set (regression coefficients are on average smaller than the development data set), and conversely, it is better in the validation data set if the slope is >1(regression
coefficients are on average larger than the development data set) [9,33]. we also examined the calibration of the models over the entire probability range at a single time point (at 10 years) using the val.surv
function in the rms library in r, which implements the hare function from the polspline package for
flexible adaptive hazard regression [38,39]. in summary, for each random sample, hazard regression
using linear splines are used to relate the predicted probabilities from the models at 10 years to the
observed event times (and censoring indicators) to estimate the actual event probability at 10 years as
a function of the estimate event probability at 10 years. to investigate the influence of sample size on
calibration, for each event size, plots of observed outcomes against predicted probabilities were drawn
and overlaid for each of the 10 000 random samples.
we examined two r2
-type measures [40,41] (explained variation [32] and explained randomness
[35]) and the brier score [42]. royston and sauerbrei‚Äôs r2
d is the proportion of the that is explained
by the prognostic model [32,34] and is given by
r2
d ¬º d2=Œ∫2
œÉ2 √æ d2=Œ∫2
where d is the value of the d statistic [32], œÉ2= œÄ2
/6‚âÉ1.645 and Œ∫ ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
8=œÄ‚âÉ1:596 p . the measure of
explained randomness, œÅ2
k of o‚Äôquigley et al. [35] is defined as
œÅ2
oxs ¬º 1  exp 2
k lŒ≤  l0
   
where k is the number of outcome events, and lb and l0 are the log partial likelihoods for the prognostic
model and the null model respectively. standard errors of œÅ2
k were calculated using the nonparametric
bootstrap (200 bootstrap replications).
the brier score for survival data is a measure of the average discrepancy between the true disease
status (0 or 1) and the predicted probability of developing the disease [36,43], defined as a function
of time t>0:
bs t√∞ √æ ¬º 1
n ‚àë
n
i¬º1
s t ^√∞ √æ jxi
2
i t√∞ √æ i ‚â§ t; Œ¥i ¬º 1
ƒù t√∞ √æi
√æ
1  s t ^√∞ √æ jxi
 2
i t√∞ √æ i > t
ƒù√∞ √æt
" #
where ≈ù(¬∑ |xi) is the predicted probability of an event for individual i; ƒù is the kaplan‚Äìmeier estimate of
the censoring distribution, which is based on the observations (ti, 1Œ¥i),Œ¥i is the censoring indicator and
i denotes the indicator function. [36,43,44]. the brier score is implemented in the function sbrier from
the package ipred in r.
3.3. evaluation
the objective of our study was to evaluate the impact of sample size (more precisely the number of
events) on the accuracy, precision and variability of model performance. we examined the sample size
requirements using the guidance by burton et al. [45]. we calculated the following quantities for each of
the performance measures over the b simulations (defined in the preceding section):
‚Ä¢ percentage bias, which is the relative magnitude of the raw bias to the true value, defined as
^Œ∏
-
 Œ∏

=Œ∏

.
‚Ä¢ standardized bias, which is the relative magnitude of the raw bias to the standard error, defined as
^Œ∏
-
 Œ∏

=se ^Œ∏
 	 
. a standardized bias of 25 percent implies that the estimate lies one quarter of
a standard error below the true value.
‚Ä¢ root mean square error, which incorporates both measures of bias and variability of the estimate,
defined as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1
b ‚àë
b
i¬º1
^Œ∏i  Œ∏
	 
2
s .
217
‚Ä¢ estimated coverage rate of the 95% confidence interval for the c-index, d statistic, r2
d and œÅ2
oxs, which
indicate the proportion of times that a confidence interval contains the true value (Œ∏). an acceptable
coverage should not fall outside of approximately two standard errors of the nominal coverage probability √∞ √æp ; se p√∞ √æ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
p√∞ √æ 1  p =b p [46].
‚Ä¢ average width of the confidence interval, defined as 1=b ‚àë
b
i¬º1
2z1Œ±=2se ^Œ∏i
  	 
 .
the true values (Œ∏) of the performance measures were obtained using the entire thin data set for each
model (table i). ^Œ∏
-
¬º ‚àë
b
i¬º1
^Œ∏i=b, where b is the number of simulations performed and ^Œ∏i is the performance
measure of interest for each of the i = 1,‚Ä¶,b= 10 000 simulations. the empirical standard error, se ^Œ∏
	 
,
is the square root of the variance of over all b-simulated ^Œ∏ values. if, for the d statistic andr2
d, the modelbased standard error is valid, then its mean over the 10 000 simulations should be close to the empirical
standard error se ^Œ∏
	 
.
4. result
figure 1 presents the empirical values, with boxplots overlaid, for the c-index, d statistic, r2
d, œÅ2
oxs brier
score and calibration slope for qrisk2 (women), describing pure sampling variation. as expected, considerable variation in the sample values for each of the six performance measures are observed when the
number of events is small. thus, inaccurate estimation of the true performance is more likely in studies
with low numbers of events.
the mean percent bias, standardized bias and rmse of the performance measures are displayed
graphically in figure 2. for all of the models, the mean percent bias of both the c-index and brier score
are within 0.1% when the number of events reaches 50. at 50 events, the average bias of the d statistic,
r2
d and calibration slope is within 2% of the true value. the mean standardized bias for all of the models
and performance measures drops below 10% once the number of events increases to 75‚Äì100.
because of the skewness in bias at small values of number of events, the median percent bias and standardized bias of the performance measures are also presented (supporting information). for all of the
performance measures, the median bias drops below 1% as the number of events reaches 100. similarly,
figure 1. empirical performance of qrisk2 (women), measured using the c-index, d statistic, r2
d, œÅ2
oxs, brier
score and calibration slope.
218
the median standardized bias drops below 10% for all of the performance measures and models when the
number of events approaches 100.
as expected, the rmse decreases as the number of events increases for all six performance measures
(figure 2). the same pattern is observed for all six prognostic models.
coverage of the confidence intervals for the c-index, d statistic and r2
d are displayed in figure 3.
acceptable coverage of the c-index at the nominal level of 95 percent is achieved as the number of
events approaches and exceeds 200. however, the d statistic confidence interval exhibits over-coverage
regardless of sample size. there is under-coverage of r2
d at less than 25 events and over-coverage as the
number of events increases (for four of the six prognostic models examined). the mean widths of the
95% confidence intervals for all of the models are displayed in figure 3. a steep decrease is observed
in the mean width for all models as the number of events approaches 50‚Äì100. within this range, the
decrease in mean width becomes smaller with more events. a similar pattern is observed in the width
variability, as shown in figure 4 for qrisk2 (women).
figure 2. mean percent, standardized bias and rmse of the c-index, d statistic, r2
d, œÅ2
oxs, brier score and calibration slope.
219
the effect of sample size on the performance of the hazard regression assessment of calibration of
qrisk2 (women) is described in figure 5. for each panel (i.e., each event size), 10 000 calibration lines
have been plotted and a diagonal (dashed) line going through the origin with slope 1 has been
superimposed, which depicts perfect calibration. furthermore, we have overlaid a calibration line using
the entire thin data set to judge convergence of increasing event size. for data sets with 10 or fewer
numbers of events, the ability to assess calibration was poor. for predicted probabilities greater than
0.2, there was modest to substantial variation between the fitted calibration curves, which decreased
as the number of events increased. the calibration line (blue line) using the entire thin data set shows
overestimation towards the upper tail of the distribution, whilst some overestimation is captured, from
event sizes in excess of 100, the true magnitude of overestimation in using qrisk2 (women) in the
thin data set is not fully captured even when the number of events reach 1000. calibration plots for
two of the five prediction models (qrisk2 men and cox framingham women) show similar patterns,
figure 3. coverage rates and 95% confidence interval widths for the c-index, d statistic, r2
d, œÅ2
oxs and calibration
slope. [bootstrap standard errors for œÅ2
oxs based on 1000 simulations and 200 bootstrap replications].
220
whilst for the remaining three models accurate assessment of calibration is achieved when the number of
events reach 100 (data not shown).
figure 6 displays the proportion of simulations in which the performance estimates are within 0.5, 2.5,
5 and 10% of the true performance measure as the number of events increases. fewer events are required
to obtain precise estimates of the c-index than of the other performance measures. for example, at 100
events, over 80% of simulations yield estimates of the c-index within 5% of the true value and over 60%
of simulations yield values within 2.5% of the true value. considerably more events are required for the
d statistic, r2
d, brier score and calibration slope.
4.1. additional analyses
as observed in figure 3, coverage of the d statistic is larger than the nominal 95% level regardless of
the number of events. similarly, r2
d coverage tends to be larger than the nominal 95% level as the
number of events increases. therefore, we carried out further analyses to investigate the model-based
standard error and the nonparametric bootstrap standard error of the d statistic and r2
d [47]. the
results are shown in table ii.
figure 4. width of the 95% confidence interval of the c-index, d statistic r2
d , œÅ2
oxs and calibration slope
(qrisk2 women). [bootstrap standard errors for œÅ2
oxs based on 1000 simulations and 200 bootstrap
replications].
221
the results from the additional simulations indicate that the model-based standard error is
overestimated. there is good agreement between the empirical and bootstrap standard errors, with coverage using the bootstrap standard errors close to the nominal 95 percent (table iii).
5. discussion
external validation studies are a vital step in introducing a prognostic model, as they evaluate the performance and transportability of the model using data that were not involved in its development
[2,48]. the performance of a prognostic model is typically worse when evaluated on samples independent of the sample used to develop the model [49]. therefore, the more external validation studies that
demonstrate satisfactory performance, the more likely the model will be useful in untested populations,
and ultimately, the more likely it will be used in clinical practice. however, despite their clear importance, multiple (independent) external validation studies are rare. many prognostic models are only subjected to a single external validation study and are abandoned if that study gives poor results. other
investigators then proceed in developing yet another new model, discarding previous efforts, and the
cycle begins again [2]. however, systematic reviews examining methodological conduct and reporting
have shown that many external validation studies are fraught with deficiencies, including inadequate
sample size [7,49]. the results from our study indicate that small external validation studies are unreliable, inaccurate and possibly biased. we should avoid basing the decision to discard or recommend a
prognostic model on an external validation study with a small sample size.
an alternative approach that could be used to determine an appropriate sample size for an external
validation study is to focus on the ability to detect a clinically relevant deterioration in model performance [12]. whilst this approach may seem appealing, it requires the investigator to pre-specify a
performance measure to base this decision on and to justify the amount of deterioration that will indicate
a lack of validation. neither of these conditions are necessarily straightforward, particularly when the
figure 5. calibration plots for qrisk2 (women). the red dashed line denoted perfect prediction. the blue line
is the model calibration using the entire data set.
222
case-mix is different or the underlying population in the validation data set is different to that from
which the model was originally developed [50]. we take the view that a single external validation is generally insufficient to warrant widespread recommendation of a prognostic model. the case-mix in a
development sample does not necessarily reflect the case-mix of the intended population for which
the model is being developed, as studies developing a prognostic model are rarely prospective and
typically use existing data collected for an entirely different purpose. a prognostic model should be
evaluated on multiple validation samples with different case-mixes from the sample used to develop
the model, thereby allowing a more thorough investigation into the performance of the model, possibly
using meta-analysis methods.
a strength of our study is the use of large data sets, multiple prognostic models and evaluating seven
performance measures (c-index, d statistic, r2
d, œÅ2
oxs, brier score, calibration slope and calibration plots).
figure 6. proportion of estimates within 0.5, 2.5, 5, 1 and 0% of the true value for qrisk2 (women).
223
we also showed that the analytical standard error for the d statistic (and r2
d) are too large, but could be
rectified by calculating bootstrap standard errors.
fundamental issues in the design of external validation studies have received little attention. existing
studies examining the sample size requirements of multivariable prognostic models have focused on
models developed using logistic regression [12,13]. adopting a hypothesis testing framework,
vergouwe and colleagues suggested that a minimum of 100 events and 100 non-events are required for
external validation of prediction models developed using logistic regression [12]. peek and colleagues
examined the influence of sample size when comparing multiple prediction models, including examining
the accuracy of performance measures, and concluded that a substantial sample size is required [13]. our
study took the approach that the sample size of an external validation study should be guided by the
premise of producing accurate and precise estimates of model performance that reasonably reflect the true
underlying population estimate. despite the differences taken in approach, our recommendations coincide.
our study focused on prognostic models predicting time-to-event outcomes, whilst we don‚Äôt expect any
discernable differences, further studies are required to evaluate models predicting binary events. we suggest that externally validating a prognostic model requires a minimum of 100 events, preferably 200 or
more events.

<|EndOfText|>

artificial intelligence versus clinicians: systematic review of
design, reporting standards, and claims of deep learning studies
abstract
objective
to systematically examine the design, reporting
standards, risk of bias, and claims of studies
comparing the performance of diagnostic deep
learning algorithms for medical imaging with that of
expert clinicians.
design
systematic review.
data sources
medline, embase, cochrane central register of
controlled trials, and the world health organization
trial registry from 2010 to june 2019.
eligibility criteria for selecting studies
randomised trial registrations and non-randomised
studies comparing the performance of a deep
learning algorithm in medical imaging with a
contemporary group of one or more expert clinicians.
medical imaging has seen a growing interest in deep
learning research. the main distinguishing feature
of convolutional neural networks (cnns) in deep
learning is that when cnns are fed with raw data,
they develop their own representations needed
for pattern recognition. the algorithm learns for
itself the features of an image that are important
for classification rather than being told by humans
which features to use. the selected studies aimed
to use medical imaging for predicting absolute risk
of existing disease or classification into diagnostic
groups (eg, disease or non-disease). for example,
raw chest radiographs tagged with a label such as
pneumothorax or no pneumothorax and the cnn
learning which pixel patterns suggest pneumothorax.
review methods
adherence to reporting standards was assessed
by using consort (consolidated standards of
reporting trials) for randomised studies and tripod
(transparent reporting of a multivariable prediction
model for individual prognosis or diagnosis) for nonrandomised studies. risk of bias was assessed by
using the cochrane risk of bias tool for randomised
studies and probast (prediction model risk of bias
assessment tool) for non-randomised studies.
results
only 10 records were found for deep learning
randomised clinical trials, two of which have been
published (with low risk of bias, except for lack of
blinding, and high adherence to reporting standards)
and eight are ongoing. of 81 non-randomised clinical
trials identified, only nine were prospective and
just six were tested in a real world clinical setting.
the median number of experts in the comparator
group was only four (interquartile range 2-9).
full access to all datasets and code was severely
limited (unavailable in 95% and 93% of studies,
respectively). the overall risk of bias was high in 58
of 81 studies and adherence to reporting standards
was suboptimal (<50% adherence for 12 of 29 tripod
items). 61 of 81 studies stated in their abstract that
performance of artificial intelligence was at least
comparable to (or better than) that of clinicians. only
31 of 81 studies (38%) stated that further prospective
studies or trials were required.
conclusions
few prospective deep learning studies and
randomised trials exist in medical imaging. most nonrandomised trials are not prospective, are at high risk
of bias, and deviate from existing reporting standards.
data and code availability are lacking in most studies,
and human comparator groups are often small.
future studies should diminish risk of bias, enhance
real world clinical relevance, improve reporting and
transparency, and appropriately temper conclusions.
study registration
prospero crd42019123605.
introduction
the digitisation of society means we are amassing
data at an unprecedented rate. healthcare is no
exception, with ibm estimating approximately one
million gigabytes accruing over an average person‚Äôs
lifetime and the overall volume of global healthcare
data doubling every few years.1
 to make sense of these
big data, clinicians are increasingly collaborating with
computer scientists and other allied disciplines to
for numbered affiliations see
end of the article.
what is already known on this topic
the volume of published research on deep learning, a branch of artificial
intelligence (ai), is rapidly growing
media headlines that claim superior performance to doctors have fuelled hype
among the public and press for accelerated implementation
what this study adds
few prospective deep learning studies and randomised trials exist in medical
imaging
most non-randomised trials are not prospective, are at high risk of bias, and
deviate from existing reporting standards
data and code availability are lacking in most studies, and human comparator
groups are often small
future studies should diminish risk of bias, enhance real world clinical relevance,
improve reporting and transparency, and appropriately temper conclusions
make use of artificial intelligence (ai) techniques that
can help detect signal from noise.2
 a recent forecast
has placed the value of the healthcare ai market as
growing from $2bn (¬£1.5bn; ‚Ç¨1.8bn) in 2018 to $36bn
by 2025, with a 50% compound annual growth rate.3
deep learning is a subset of ai which is formally
defined as ‚Äúcomputational models that are composed
of multiple processing layers to learn representations of
data with multiple levels of abstraction.‚Äù4
 in practice,
the main distinguishing feature between convolutional
neural networks (cnns) in deep learning and traditional
machine learning is that when cnns are fed with raw
data, they develop their own representations needed
for pattern recognition; they do not require domain
expertise to structure the data and design feature
extractors.5
 in plain language, the algorithm learns
for itself the features of an image that are important
for classification rather than being told by humans
which features to use. a typical example would be
feeding in raw chest radiographs tagged with a label
such as either pneumothorax or no pneumothorax
and the cnn learning which pixel patterns suggest
pneumothorax. fields such as medical imaging have
seen a growing interest in deep learning research,
with more and more studies being published.6
 some
media headlines that claim superior performance to
doctors have fuelled hype among the public and press
for accelerated implementation. examples include:
‚Äúgoogle says its ai can spot lung cancer a year before
doctors‚Äù and ‚Äúai is better at diagnosing skin cancer
than your doctor, study finds.‚Äù7 8
the methods and risk of bias of studies behind such
headlines have not been examined in detail. the danger
is that public and commercial appetite for healthcare ai
outpaces the development of a rigorous evidence base
to support this comparatively young field. ideally, the
path to implementation would involve two key steps.
firstly, well conducted and well reported development
and validation studies that describe an algorithm and
its properties in detail, including predictive accuracy
in the target setting. secondly, well conducted and
transparently reported randomised clinical trials that
evaluate usefulness in the real world. both steps are
important to ensure clinical practice is determined
based on the best evidence standards.9-12
our systematic review seeks to give a contemporary
overview of the current standards of deep learning
research for clinical applications. specifically, we
sought to describe the study characteristics, and
evaluate the methods and quality of reporting and
transparency of deep learning studies that compare
diagnostic algorithm performance with human
clinicians. we aim to suggest how we can move forward
in a way that encourages innovation while avoiding
hype, diminishing research waste, and protecting
patients.
methods
the protocol for this study was registered in the
online prospero database (crd42019123605)
before search execution. the supplementary appendix
gives details of any deviations from the protocol.
this manuscript has been prepared according to the
prisma (preferred reporting items for systematic
reviews and meta-analyses) guidelines and a checklist
is available in the supplementary appendix.13
study identification and inclusion criteria
we performed a comprehensive search by using free
text terms for various forms of the keywords ‚Äúdeep
learning‚Äù and ‚Äúclinician‚Äù to identify eligible studies.
appendix 1 presents the exact search strategy. several
electronic databases were searched from 2010 to june
2019: medline, embase, cochrane central register of
controlled trials (central), and the world health
organization international clinical trials registry
platform (who-ictrp) search portal. additional
articles were retrieved by manually scrutinising the
reference lists of relevant publications.
we selected publications for review if they satisfied
several inclusion criteria: a peer reviewed scientific
report of original research; english language; assessed
a deep learning algorithm applied to a clinical problem
in medical imaging; compared algorithm performance
with a contemporary human group not involved in
establishing the ground truth (the true target disease
status verified by best clinical practice); and at least
one human in the group was considered an expert.
we included studies when the aim was to use medical
imaging for predicting absolute risk of existing disease
or classification into diagnostic groups (eg, disease
or non-disease). exclusion criteria included informal
publication types (such as commentaries, letters to the
editor, editorials, meeting abstracts). deep learning
for the purpose of medical imaging was defined as
computational models that are composed of multiple
processing layers to learn representations of data with
multiple levels of abstraction (in practice through a
cnn; see box 1).4
 a clinical problem was defined as a
situation in which a patient would usually see a medical
professional to improve or manage their health (this
did not include segmentation tasks, eg, delineating the
borders of a tumour to calculate tumour volume). an
expert was defined as an appropriately board certified
specialist, attending physician, or equivalent. a real
world clinical environment was defined as a situation
in which the algorithm was embedded into an active
clinical pathway. for example, instead of an algorithm
being fed thousands of chest radiographs from a
database, in a real world implementation it would exist
within the reporting software used by radiologists and
be acting or supporting the radiologists in real time.
study selection and extraction of data
after removal of clearly irrelevant records, four people
(mn, yc, cal, dina radenkovic) independently
screened abstracts for potentially eligible studies so
that each record was reviewed by at least two people.
full text reports were then assessed for eligibility with
disagreements resolved by consensus. at least two
people (mn, yc, cal) extracted data from study reports
independently and in duplicate for each eligible study,
with disagreements resolved by consensus or a third
reviewer.
adherence to reporting standards and risk of bias
we assessed reporting quality of non-randomised
studies against a modified version of the tripod
(transparent reporting of a multivariable prediction model for individual prognosis or diagnosis)
statement.14 this statement aims to improve the transparent reporting of prediction modelling studies of
all types and in all medical settings.15 the tripod
statement consists of a 22 item checklist (37 total points
when all subitems are included), but we considered
some items to be less relevant to deep learning studies
(eg, points that related to predictor variables). deep
learning algorithms can consider multiple predictors;
however, in the cases we assessed, the only predictors
(almost exclusively) were the individual pixels of
the image. the algorithm did not typically receive
information on characteristics such as patient age, sex,
and medical history. therefore, we used a modified list
of 29 total points (see appendix 2). the aim was to
assess whether studies broadly conformed to reporting
recommendations included in tripod, and not the
detailed granularity required for a full assessment of
adherence.16
we assessed risk of bias for non-randomised
studies by applying probast (prediction model
risk of bias assessment tool).17 probast contains
20 signalling questions from four domains (participants, predictors, outcomes, and analysis) to allow
assessment of the risk of bias in predictive modelling
studies.18 we did not assess applicability (because
no specific therapeutic question existed for this
systematic review) or predictor variables (these are
less relevant in deep learning studies on medical
imaging; see appendix 2).
we assessed the broad level reporting of randomised
studies against the consort (consolidated standards
of reporting trials) statement. risk of bias was evaluated
by applying the cochrane risk of bias tool.11 19
data synthesis
we intentionally planned not to conduct formal
quantitative syntheses because of the probable heterogeneity of specialties and outcomes.
patient and public involvement
patients were not involved in any aspect of the study
design, conduct or in the development of the research
question or outcome measures.
results
study selection
our electronic search, which was last updated on
17 june 2019, retrieved 8302 records (7334 study
records and 968 trial registrations; see fig 1). of the
7334 study records, we assessed 140 full text articles;
59 were excluded, which left 81 non-randomised
studies for analysis. of the 968 trial registrations, we
assessed 96 in full; 86 were excluded, which left 10
trial registrations that related to deep learning.
box 1: deep learning in imaging with examples
deep learning is a subset of artificial intelligence that is formally defined as ‚Äúcomputational models that are composed
of multiple processing layers to learn representations of data with multiple levels of abstraction.‚Äù4
 a deep learning
algorithm consists of a structure referred to as a deep neural network of which a convolutional neural network (cnn)
is one particular type frequently used in imaging. cnns are structurally inspired by the hierarchical arrangement of
neurons within the brain. they can take many nuanced forms but the basic structure consists of an input layer, multiple
hidden layers, and a final output layer. each hidden layer responds to a different aspect of the raw input. in the case of
imaging, this could be an edge, colour, or specific pattern.
the key difference between deep learning and other types of machine learning is that cnns develop their own
representations needed for pattern recognition rather than requiring human input to structure the data and design
feature extractors. in plain language, the algorithm learns for itself the features of an image that are important for
classification. therefore, the algorithm has the freedom to discover classification features that might not have
been apparent to humans (particularly when datasets are large) and thereby improve the performance of image
classification.
cnns use raw image data that have been labelled by humans in a process known as supervised learning. each image
is fed into the input layer of the algorithm as raw pixels and then processed sequentially through the layers of the cnn.
the final output is a classification likelihood of the image belonging to a prespecified group.
some examples from this review include the following:
‚Ä¢ feeding in raw chest radiographs tagged with a label (pneumothorax or no pneumothorax) and the cnn learning
which pixel patterns suggest pneumothorax. when fed with new untagged images, the cnn outputs a likelihood of
the new image containing a pneumothorax or not.
‚Ä¢ feeding in raw retinal images tagged with the stage of age related macular degeneration and the cnn learning which
pixel patterns suggest a particular stage. when fed with new untagged images, the cnn outputs a likelihood of the
new image containing a specific stage of age related macular degeneration.
‚Ä¢ feeding in optical coherence tomography scans tagged with a management decision (urgent referral, semi urgent
referral, routine referral, observation). when fed with new untagged images, the cnn outputs a likelihood of the most
appropriate management decision.
randomised clinical trials
table 1 summarises the 10 trial registrations. eight
related to gastroenterology, one to ophthalmology,
and one to radiology. eight were from china, one was
from the united states, and one from taiwan. two
trials have completed and published their results
(both in 2019), three are recruiting, and five are not
yet recruiting.
the first completed trial enrolled 350 paediatric
patients who attended ophthalmology clinics in
china. these patients underwent cataract assessment
with or without an ai platform (using deep learning)
to diagnose and provide a treatment recommendation
(surgery or follow-up).20 the authors found that
accuracy (defined as proportion of true results) of
cataract diagnosis and treatment recommendation
with ai were 87% (sensitivity 90%, specificity
86%) and 71% (sensitivity 87%, specificity 44%),
respectively. these results were significantly lower
than accuracy of diagnosis (99%, sensitivity 98%,
specificity 99.6%) and treatment recommendation
(97%, sensitivity 95%, specificity 100%) by senior
consultants (p<0.001 for both); and also lower than
the results for the same ai when tested in a nonrandomised clinical trial setting (98% and 93%,
respectively). the mean time for receiving a diagnosis
with the ai platform was faster than diagnosis by
consultants (2.8 v 8.5 minutes, p<0.001). the authors
suggested that this might explain why patients were
more satisfied with ai (mean satisfaction score 3.47
v 3.38, p=0.007). risk of bias was low in all domains
except for blinding of participants and personnel. the
reporting showed high adherence (31 of 37 items,
84%) to the consort checklist (which was included
with the manuscript).
the second completed trial enrolled 1058 patients
who underwent a colonoscopy with or without the
assistance of a real time automatic polyp detection
system, which provided simultaneous visual and
sound alerts when it found a polyp.21 the authors
reported that the detection system resulted in a
significant increase in the adenoma detection rate
(29% v 20%, p<0.001), and an increase in the number
of hyperplastic polyps identified (114 v 52, p<0.001).
risk of bias was low in all domains except for blinding
of participants, personnel, and outcome assessors.
one of the other trial registrations belongs to the
same author group. these authors are performing a
additional records identified through trial registry
full text articles excluded
not contemporary comparison, not
 only human or human involved with
 ground truth
not a clinical problem
not english language
not an article
no experts
not deep learning
34
12
4
3
3
3
records screened aÔòÉer duplicates removed
records identified through publication databases
records excluded
full text articles assessed for eligibility
records included in qualitative synthesis
81 studies 10 trial registrations
968
236
quantitative synthesis (meta-analysis) not performed
7334
8302
8066
59
full text trial registrations excluded
not randomised
not deep learning
76
10
86
91
0
fig 1 | prisma (preferred reporting items for systematic reviews and meta-analyses) flowchart of study records
double blind randomised clinical trial with sham ai
to overcome the blinding issue in the previous study.
the reporting showed high adherence (30 of 37 items,
81%) to the consort checklist (though the consort
checklist itself was not included or referenced by the
manuscript).
non-randomised studies
general characteristics
table 2 and table 3 summarise the basic characteristics
of the 81 non-randomised studies. nine of 81 (11%)
non-randomised studies were prospective, but only
six of these nine were tested in a real world clinical
environment. the us and asia accounted for 82%
of studies, with the top four countries as follows: us
(24/81, 30%), china (14/81, 17%), south korea (12/81,
15%), and japan (9/81, 11%). the top five specialties
were radiology (36/81, 44%), ophthalmology (17/81,
21%), dermatology (9/81, 11%), gastroenterology
(5/81, 6%), and histopathology (5/81, 6%). eighteen
(22%) studies compared how long a task took in ai and
human arms in addition to accuracy or performance
metrics. funding was predominantly academic (47/81,
58%) as opposed to commercial (9/81, 11%) or mixed
(1/81, 1%). twelve studies stated they had no funding
and another 12 did not report on funding. a detailed
table with further information on the 81 studies is
included as an online supplementary file.
in 77 of 81 studies, a specific comment was included
in the abstract about the comparison between ai and
clinician performance. ai was described as superior
in 23 (30%), comparable or better in 13 (17%),
comparable in 25 (32%), able to help a clinician
perform better in 14 (18%), and not superior in two
(3%). only nine studies added a caveat in the abstract
that further prospective trials were required (this was
missing in all 23 studies that reported ai was superior
to clinician performance). even in the discussion
section of the paper, a call for prospective studies (or
trials in the case of existing prospective work) was
only made in 31 of 81 (38%) studies. seven of 81 (9%)
studies claimed in the discussion that the algorithm
could now be used in clinical practice despite only
two of the seven having been tested prospectively in
a real world setting. concerning reproducibility, data
were public and available in only four studies (5%).
code (for preprocessing of data and modelling) was
available in only six studies (7%). both raw labelled
data and code were available in only one study.22
methods and risk of bias
most studies developed and validated a model
(63/81, 78%) compared with development only by
using validation through resampling (9/81, 11%) or
validation only (9/81, 11%). when validation occurred
in a separate dataset, this dataset was from a different
geographical region in 19 of 35 (54%) studies, from
a different time period in 11 of 35 (31%), and a
combination of both in five of 35 (14%). in studies that
did not use a separate dataset for validation, the most
common method of internal validation was split sample
(29/37) followed by cross validation (15/37), and then
bootstrapping (6/37); some studies used more than
one method (box 2). sample size calculations were
reported in 14 of 81 (17%) studies. dataset sizes were
as follows (when reported): training, median 2678
(interquartile range 704-21362); validation, 600 (200-
1359); and test, 337 (144-891). the median event rate
for development, validation, and test sets was 42%,
44%, and 44%, respectively, when a binary outcome
was assessed (n=62) as opposed to a multiclass
classification (n=19). forty one of 81 studies used data
augmentation (eg, flipping and inverting images) to
increase the dataset size.
the human comparator group was generally small
and included a median of five clinicians (interquartile
range 3-13, range 1-157), of which a median of four
were experts (interquartile range 2-9, range 1-91).
the number of participating non-experts varied from
0 to 94 (median 1, interquartile range 0-3). experts
were used exclusively in 36 of 81 studies, but in the
45 studies that included non-experts, 41 had separate
performance data available which were exclusive to
the expert group. in most studies, every human (expert
or non-expert) rated the test dataset independently
(blinded to all other clinical information except the
image in 33/81 studies). the volume and granularity
of the separate data for experts varied considerably
among studies, with some reporting individual
performance metrics for each human (usually in
supplementary appendices).
the overall risk of bias assessed using probast led
to 58 of 81 (72%) studies being classified as high risk
(fig 2); the analysis domain was most commonly rated
to be at high risk of bias (as opposed to participant or
outcome ascertainment domains). major deficiencies
in the analysis domain related to probast items 4.1
(were there a reasonable number of participants?),
4.3 (were all enrolled participants included in the
analysis?), 4.7 (were relevant model performance
measures evaluated appropriately?), and 4.8 (were
model overfitting and optimism in model performance
accounted for?).
adherence to reporting standards
adherence to reporting standards was poor (<50%
adherence) for 12 of 29 tripod items (see fig 3).
overall, publications adhered to between 24% and
90% of the tripod items: median 62% (interquartile
range 45-69%). eight tripod items were reported in
90% or more of the 81 studies, and five items in less
than 30% (fig 3). a flowchart for the flow of patients
or data through the study was only present in 25 of
81 (31%) studies. we also looked for reporting of the
hardware that was used for developing or validating
the algorithm, although this was not specifically
requested in the tripod statement. only 29 of 81
(36%) studies reported this information and in most
cases (n=18) it related only to the graphics processing
unit rather than providing full details (eg, random
access memory, central processing unit speed,
configuration settings).
table 1 | randomised trial registrations of deep learning algorithms
trial registration title status
record last
updated country specialty
planned
sample
size intervention control blinding
primary
outcome
anticipated
completion
chictr-ddd17012221
a colorectal polyps auto-detection
system based on deep learning to
increase polyp detection rate: a
prospective clinical study
completed,
published
16 july
2018 china gastroenterology 1000
ai assisted
colonoscopy
standard
colonoscopy none
polyp detection
rate and adenoma
detection rate
28 february
2018
nct03240848
comparison of artificial intelligent
clinic and normal clinic
completed,
published
30 july
2018 china ophthalmology 350 ai assisted clinic normal clinic
double (investigator and outcomes
assessor)
accuracy for
congenital
cataracts
25 may
2018
nct03706534
breast ultrasound image reviewed
with assistance of deep learning
algorithms recruiting
17 october
2018 us radiology 300
computer aided
detection system
manual
ultrasound
imaging review
double (participant
and investigator)
concordance
rate
31 july
2019
nct03840590
adenoma detection rate
using ai system in china
not yet
recruiting
15 february
2019 china gastroenterology 800
csk ai system
assisted
colonoscopy
standard
colonoscopy none
adenoma
detection rate
1 march
2020
nct03842059
computer-aided detection
for colonoscopy
not yet
recruiting
15 february
2019 taiwan gastroenterology 1000
computer aided
detection
standard
colonoscopy
double (participant,
care provider)
adenoma
detection rate
31 december
2021
chictr1800017675
the impact of a computer aided
diagnosis system based on deep
learning on increasing polyp
detection rate during colonoscopy, a
prospective double blind study
not yet
recruiting
21 february
2019 china gastroenterology 1010
ai assisted
colonoscopy
standard
colonoscopy double
polyp detection
rate and adenoma
detection rate
31 january
2019
chictr1900021984
a multicenter randomised controlled
study for evaluating the effectiveness
of artificial intelligence in improving
colonoscopy quality recruiting
19 march
2019 china gastroenterology 1320
endoangel
assisted
colonoscopy colonoscopy
double (participants and evaluators) polyp detection rate 31 december 2020
nct03908645
development and validation of a
deep learning algorithm for bowel
preparation quality scoring
not yet
recruiting 9 april 2019 china gastroenterology 100
ai assisted
scoring group
conventional
human scoring
group
single (outcome
assessor)
adequate bowel
preparation
15 april
2020
nct03883035
quality measurement of esophagogastroduodenoscopy using deep
learning models recruiting 17 april 2019 china gastroenterology 559
dcnn model
assisted egd
conventional
egd
double (participant,
care provider)
detection of upper
gastrointestinal
lesions
20 may
2020
chictr1900023282
prospective clinical study for artificial
intelligence platform for lymph node
pathology detection of gastric cancer
not yet
recruiting 20 may 2019 china gastroenterology 60
pathological
diagnosis of
artificial
intelligence
traditional
pathological
diagnosis not stated
clinical
prognosis
31 august
2021
ai=artificial intelligence; csk=commonsense knowledge; dcnn=deep convolutional neural network; egd=esophagogastroduodenoscopy.
table 2 | characteristics of non-randomised studies lead author year country study type specialty disease outcome caveat in discussion* suggestion in discussion‚Ä† abramoff 2018 us prospective real world ophthalmology diabetic retinopathy more than mild diabetic retinopathy no yes arbabshirani 2018 us prospective real world radiology intracranial haemorrhage haemorrhage yes no arji 2018 japan retrospective radiology oral cancer cervical lymph node metastases no no becker 2017 switzerland retrospective radiology breast cancer bi-rads category 5 yes no becker 2018 switzerland retrospective radiology breast cancer bi-rads category 5 yes no bien 2018 us retrospective radiology knee injuries abnormality on mri no no brinker 2019 germany retrospective dermatology skin cancer melanoma no no brinker 2019 germany retrospective dermatology skin cancer melanoma yes no brown 2018 us retrospective ophthalmology retinopathy of prematurity plus disease no no burlina 2018 us retrospective ophthalmology macular degeneration armd stage no no burlina 2017 us retrospective ophthalmology macular degeneration intermediate or advanced stage armd no no burlina 2017 us retrospective ophthalmology macular degeneration armd stage no no bychov 2018 finland retrospective histopathology colorectal cancer low or high risk for 5 year survival no no byra 2018 us retrospective radiology breast cancer bi-rads category 4 or more no no cha 2018 us retrospective radiology bladder cancer t0 status post chemotherapy no no cha 2019 south korea retrospective radiology lung cancer nodule operability yes no chee 2019 south korea retrospective radiology osteonecrosis of the femoral head stage of osteonecrosis yes no chen 2018 taiwan prospective gastroenterology colorectal cancer neoplastic polyp no no choi 2018 south korea retrospective radiology liver fibrosis fibrosis stage no no choi 2019 south korea retrospective radiology breast cancer malignancy no no chung 2018 south korea retrospective orthopaedics humerus fractures proximal humerus fracture yes no ciompi 2017 netherlands/italy retrospective radiology lung cancer nodule type no no ciritsis 2019 switzerland retrospective radiology breast cancer bi-rads stage no no de fauw 2018 uk retrospective ophthalmology retinopathy diagnosis and referral decision yes no ehtesham bejnordii 2017 netherlands retrospective histopathology breast cancer metastases yes no esteva 2017 us retrospective dermatology skin cancer lesion type yes no fujioka 2019 japan retrospective radiology breast cancer bi-rads malignancy no no fujisawa 2018 japan retrospective dermatology skin cancer malignancy classification yes no gan 2019 china retrospective orthopaedics wrist fractures fracture yes no gulshan 2019 india prospective real world ophthalmology retinopathy moderate or worse diabetic retinopathy or referable macula oedema yes no haenssle 2018 germany retrospective dermatology skin cancer malignancy classification and management decision yes no hamm 2019 us retrospective radiology liver cancer li-rads category no no han 2018 south korea retrospective dermatology skin cancer cancer type no no han 2018 south korea retrospective dermatology onchomycosis onchomycosis diagnosis no no hannun 2019 us retrospective cardiology arrhythmia arrhythmia classification no no he 2019 china retrospective radiology bone cancer recurrence of giant cell tumour no no hwang 2019 taiwan retrospective ophthalmology macular degeneration classification and type of armd no yes hwang 2018 south korea retrospective radiology tuberculosis tb presence yes no hwang 2019 south korea retrospective radiology pulmonary pathology abnormal chest radiograph yes no kim 2018 south korea retrospective radiology sinusitis maxillary sinusitis label no no kise 2019 japan retrospective radiology sjogren‚Äôs syndrome sjogren‚Äôs syndrome presence no no kooi 2017 netherlands retrospective radiology breast cancer classification of mammogram no no krause 2018 us retrospective ophthalmology diabetic retinopathy diabetic retinopathy stage no no kuo 2019 taiwan retrospective nephrology chronic kidney disease egfr<60 ml/min/1.73m2 no yes lee 2018 us prospective radiology intracranial haemorrhage haemorrhage yes no li 2018 china prospective oncology nasopharyngeal cancer malignancy no yes li 2018 china retrospective ophthalmology glaucoma glaucoma no no li 2018 china retrospective radiology thyroid cancer malignancy yes no armd=age related macular degeneration; bi-rads=breast imaging reporting and data system; egfr=estimated glomerular filtration rate; li-rads=liver imaging reporting and data system; mri=magnetic resonance imaging; tb=tuberculosis. *caveat mentioned in discussion about need for further prospective work or trials. ‚Ä†suggestion in discussion that algorithm can now be used clinically.
table 3 | characteristics of non-randomised studies
lead author year country study type specialty disease outcome
caveat in
discussion*
suggestion in
discussion‚Ä†
long 2017 china prospective real world ophthalmology congenital cataracts detection of congenital cataracts no no
lu 2018 china retrospective ophthalmology macular pathologies classification of macular pathology no no
marchetti 2017 us retrospective dermatology skin cancer malignancy (melanoma) yes no
matsuba 2018 japan retrospective ophthalmology macular degeneration wet amd no no
mori 2018 japan prospective real world gastroenterology polyps neoplastic polyp yes yes
nagpal 2019 us retrospective histopathology prostate cancer gleason score no no
nakagawa 2019 japan retrospective gastroenterology oesophageal cancer cancer invasion depth stage sm2/3 no no
nam 2018 south korea retrospective radiology pulmonary nodules classification and localisation of nodule yes no
nirschl 2018 us retrospective histopathology heart failure heart failure (pathologically) no yes
olczak 2017 sweden retrospective orthopaedics fractures fracture no yes
park 2019 us retrospective radiology cerebral aneurysm aneurysm presence yes no
poedjiastoeti 2018 thailand retrospective oncology jaw tumours malignancy no no
rajpurkar 2018 us retrospective radiology pulmonary pathology classification of chest radiograph pathology yes no
raumviboonsuk 2019 thailand prospective real world ophthalmology diabetic retinopathy moderate or worse diabetic retinopathy yes no
rodriguez-ruiz 2018 netherlands retrospective radiology breast cancer classification of mammogram yes no
sayres 2019 us retrospective ophthalmology diabetic retinopathy moderate or worse non-proliferative diabetic retinopathy no no
shichijo 2017 japan retrospective gastroenterology gastritis helicobacter pylori gastritis no no
singh 2018 us retrospective radiology pulmonary pathology chest radiograph abnormality no no
steiner 2018 us retrospective histopathology breast cancer metastases yes no
ting 2017 singapore retrospective ophthalmology
retinopathy, glaucoma,
macular degeneration
referable pathology for retinopathy,
glaucoma, macular degeneration yes no
urakawa 2019 japan retrospective orthopaedics hip fractures intertrochanteric hip fracture no no
van grinsven 2016 netherlands retrospective ophthalmology fundal haemorrhage fundal haemorrhage no no
walsh 2018 uk/italy retrospective radiology fibrotic lung disease fibrotic lung disease no no
wang 2019 china retrospective radiology thyroid nodule nodule presence yes no
wang 2018 china retrospective radiology lung cancer invasive or preinvasive adenocarcinoma nodule no no
wu 2019 us retrospective radiology bladder cancer t0 response to chemotherapy no no
xue 2017 china retrospective orthopaedics hip osteoarthritis radiograph presence of hip osteoarthritis no no
ye 2019 china retrospective radiology intracranial haemorrhage presence of intracranial haemorrhage yes no
yu 2018 south korea retrospective dermatology skin cancer malignancy (melanoma) no no
zhang 2019 china retrospective radiology pulmonary nodules presence of a malignant nodule yes no
zhao 2018 china retrospective radiology lung cancer classification of nodule invasiveness no no
zhu 2019 china retrospective gastroenterology gastric cancer tumour invasion depth (deeper than sm1) no no
zucker 2019 us retrospective radiology cystic fibrosis brasfield score yes no
amd=age related macular degeneration.
*caveat mentioned in discussion about need for further prospective work or trials.
‚Ä†suggestion in discussion that algorithm can now be used clinically.
discussion
we have conducted an appraisal of the methods,
adherence to reporting standards, risk of bias, and
claims of deep learning studies that compare diagnostic
ai performance with human clinicians. the rapidly
advancing nature and commercial drive of this field
has created pressure to introduce ai algorithms into
clinical practice as quickly as possible. the potential
consequences for patients of this implementation
without a rigorous evidence base make our findings
timely and should guide efforts to improve the design,
reporting, transparency, and nuanced conclusions of
deep learning studies.23 24
principal findings
five key findings were established from our review.
firstly, we found few relevant randomised clinical
trials (ongoing or completed) of deep learning in
medical imaging. while time is required to move from
development to validation to prospective feasibility
testing before conducting a trial, this means that
claims about performance against clinicians should
be tempered accordingly. however, deep learning
only became mainstream in 2014, giving a lead time
of approximately five years for testing within clinical
environments, and prospective studies could take a
minimum of one to two years to conduct. therefore,
it is reasonable to assume that many similar trials
will be forthcoming over the next decade. we found
only one randomised trial registered in the us despite
at least 16 deep learning algorithms for medical
imaging approved for marketing by the food and
drug administration (fda). these algorithms cover a
range of fields from radiology to ophthalmology and
cardiology.2 25
secondly, of the non-randomised studies, only
nine were prospective and just six were tested in a
real world clinical environment. comparisons of ai
performance against human clinicians are therefore
difficult to evaluate given the artificial in silico context
in which clinicians are being evaluated. in much the
same way that surrogate endpoints do not always
reflect clinical benefit,26 a higher area under the curve
might not lead to clinical benefit and could even
have unintended adverse effects. such effects could
include an unacceptably high false positive rate,
which is not apparent from an in silico evaluation.
yet it is typically retrospective studies that are
usually cited in fda approval notices for marketing
of algorithms. currently, the fda do not mandate
peer reviewed publication of these studies; instead
internal review alone is performed.27 28 however, the
fda has recognised and acknowledged that their
traditional paradigm of medical device regulation
was not designed for adaptive ai and machine
learning technologies. non-inferior ai (rather than
superior) performance that allows for a lower burden
on clinician workflow (that is, being quicker with
similar accuracy) might warrant further investigation.
however, less than a quarter of studies reported time
taken for task completion in both the ai and human
groups. ensuring fair comparison between ai and
clinicians is arguably done best in a randomised
clinical trial (or at the very least prospective) setting.
however, it should be noted that prospective testing
is not necessary to actually develop the model in the
first place. even in a randomised clinical trial setting,
ensuring that functional robustness tests are present
is crucial. for example, does the algorithm produce
the correct decision for normal anatomical variants
and is the decision independent of the camera or
imaging software used?
thirdly, limited availability of datasets and code
makes it difficult to assess the reproducibility of deep
learning research. descriptions of the hardware used,
when present, were also brief and this vagueness
might affect external validity and implementation.
reproducible research has become a pressing issue
across many scientific disciplines and efforts to
encourage data and code sharing are crucial.29-31 even
when commercial concerns exist about intellectual
property, strong arguments exist for ensuring that
algorithms are non-proprietary and available for
scrutiny.32 commercial companies could collaborate
with non-profit third parties for independent
prospective validation.
fourthly, the number of humans in the comparator
group was typically small with a median of only
four experts. there can be wide intra and inter case
variation even between expert clinicians. therefore,
an appropriately large human sample for comparison
is essential for ensuring reliability. inclusion of nonexperts can dilute the average human performance
and potentially make the ai algorithm look better
than it otherwise might. if the algorithm is designed
specifically to aid performance of more junior clinicians
or non-specialists rather than experts, then this should
be made clear.
box 2: specific terms
‚Ä¢ internal validation: evaluation of model performance with data used in development process
‚Ä¢ external validation: evaluation of model performance with separate data not used in development process
‚Ä¢ cross validation: internal validation approach in which data are randomly split into n equally sized groups; the
model is developed in n‚àí1 of n groups, and performance evaluated in the remaining group with the whole process
repeated n times; model performance is taken as average over n iterations
‚Ä¢ bootstrapping: internal validation approach similar to cross validation but relying on random sampling with
replacement; each sample is the same size as model development dataset
‚Ä¢ split sample: internal validation approach in which the available development dataset is divided into two datasets:
one to develop the model and the other to validate the model; division can be random or non-random.
fifthly, descriptive phrases that suggested at least
comparable (or better) diagnostic performance of an
algorithm to a clinician were found in most abstracts,
despite studies having overt limitations in design,
reporting, transparency, and risk of bias. caveats
about the need for further prospective testing were
rarely mentioned in the abstract (and not at all in
the 23 studies that claimed superior performance
to a clinician). accepting that abstracts are usually
word limited, even in the discussion sections of the
main text, nearly two thirds of studies failed to make
an explicit recommendation for further prospective
studies or trials. one retrospective study gave a
website address in the abstract for patients to upload
their eye scans and use the algorithm themselves.33
overpromising language leaves studies vulnerable
to being misinterpreted by the media and the public.
although it is clearly beyond the power of authors
to control how the media and public interpret their
findings, judicious and responsible use of language in
studies and press releases that factor in the strength
and quality of the evidence can help.34 this issue is
especially concerning given the findings from new
research that suggests patients are more likely to
consider a treatment beneficial when news stories are
reported with spin, and that false news spreads much
faster online than true news.35 36
policy implications
the impetus for guiding best practice has gathered
pace in the last year with the publication of a
report that proposes a framework for developing
transparent, replicable, ethical, and effective research
in healthcare ai (ai-tree).37 this endeavour is led by
a multidisciplinary team of clinicians, methodologists,
statisticians, data scientists, and healthcare policy
makers. the guiding questions of this framework
will probably feed into the creation of more specific
reporting standards such as a tripod extension for
machine learning studies.38 key to the success of these
efforts will be high visibility to researchers and possibly
some degree of enforcement by journals in a similar
vein to preregistering randomised trials and reporting
them according to the consort statement.11 39
enthusiasm exists to speed up the process by which
medical devices that feature ai are approved for
marketing.40 41 better design and more transparent
reporting should be seen eventually as a facilitator of
the innovation, validation, and translation process,
and could help avoid hype.
study limitations
our findings must be considered in light of several
limitations. firstly, although comprehensive, our
search might have missed some studies that could
have been included. secondly, the guidelines that we
used to assess non-randomised studies (tripod and
probast) were designed for conventional prediction
modelling studies, and so the adherence levels we
found should be interpreted in this context. thirdly,
we focused specifically on deep learning for diagnostic
medical imaging. therefore, it might not be appropriate
risk of bias
percentage
participants outcomes analysis overall
high unclear low
0
40
60
100
80
20
fig 2 | probast (prediction model risk of bias assessment tool) risk of bias assessment
for non-randomised studies adherence (%)
title
abstract
introduction - context
introduction - objectives
methods - study design
methods - study dates
methods - study setting
methods - eligibility criteria
methods - outcome predicted
methods - blinding
methods - sample size
methods - missing data
methods - model building
methods - validation predictions
methods - model performance
methods - model updating
methods - data differences
results - flow of data
results - characteristics
results - validation
results - numbers
results - model performance
results - model updating
discussion - limitations
discussion - development v validation
discussion - interpretation
discussion - clinical use
supplementary data
funding
0
40
60
100
80
20
fig 3 | completeness of reporting of individual tripod (transparent reporting of a multivariable prediction model for individual prognosis or
diagnosis) items for non-randomised studies
to generalise our findings to other types of ai, such as
conventional machine learning (eg, an artificial neural
network based mortality prediction model that uses
electronic health record data). similar issues could
exist in many other types of ai paper, however we
cannot definitively make this claim from our findings
because we only assessed medical imaging studies.
moreover, nomenclature in the field is sometimes used
in non-standardised ways, and thus some potentially
eligible studies might have been presented with
terminology that did not lead to them being captured
with our search strategy. fourthly, risk of bias entails
some subjective judgment and people with different
experiences of ai performance could have varying
perceptions.
conclusions
deep learning ai is an innovative and fast moving
field with the potential to improve clinical outcomes.
financial investment is pouring in, global media
coverage is widespread, and in some cases algorithms
are already at marketing and public adoption stage.
however, at present, many arguably exaggerated
claims exist about equivalence with or superiority
over clinicians, which presents a risk for patient
safety and population health at the societal level,
with ai algorithms applied in some cases to millions
of patients. overpromising language could mean that
some studies might inadvertently mislead the media
and the public, and potentially lead to the provision
of inappropriate care that does not align with patients‚Äô
best interests. the development of a higher quality and
more transparently reported evidence base moving
forward will help to avoid hype, diminish research
waste, and protect patients.

<|EndOfText|>

quantifying the impact of different
approaches for handling continuous
predictors on the performance of a
prognostic model
continuous predictors are routinely encountered when developing a prognostic model. investigators, who are
often non-statisticians, must decide how to handle continuous predictors in their models. categorising continuous measurements into two or more categories has been widely discredited, yet is still frequently done because of
its simplicity, investigator ignorance of the potential impact and of suitable alternatives, or to facilitate model
uptake. we examine three broad approaches for handling continuous predictors on the performance of a prognostic model, including various methods of categorising predictors, modelling a linear relationship between the
predictor and outcome and modelling a nonlinear relationship using fractional polynomials or restricted cubic
splines. we compare the performance (measured by the c-index, calibration and net benefit) of prognostic
models built using each approach, evaluating them using separate data from that used to build them. we show
that categorising continuous predictors produces models with poor predictive performance and poor clinical
usefulness. categorising continuous predictors is unnecessary, biologically implausible and inefficient and
should not be used in prognostic model development.
keywords: prognostic modelling; continuous predictors; dichotomisation
1. introduction
categorising continuous measurements in regression models has long been regarded as problematic
(e.g., biologically implausible particularly when dichotomising), highly inefficient and unnecessary
[1‚Äì5]. in the context of clinical prediction, investigators developing new prognostic models frequently
categorise continuous predictors into two or more categories [6]. categorisation is often carried out with
no apparent awareness of its consequences or in a misguided attempt to facilitate model interpretation,
use and uptake. however, categorisation causes a loss of information, and therefore reduces the statistical power to identify a relationship between a continuous measurement and patient outcome [7]. as
studies developing new prognostic models are already generally quite small, it seems unwise to discard
any information [6,8]. despite the numerous cautions and recommendations not to categorise continuous
measurements, there have been relatively few quantitative assessments of the impact of the choice of
approach for handling continuous measurements on the performance of a prognostic model [1,9].
4124
research article
determining the functional form of a variable is an important, yet often overlooked step in modelling
[10]. it is often insufficient to assume linearity, and categorising continuous variables should be avoided.
alternative approaches that allow more flexibility in the functional form of the association between
predictors and outcome should be considered, particularly if they improve model fit and model predictions
[11‚Äì13]. two commonly used approaches are fractional polynomials and restricted cubic splines [14,15].
however, few studies have examined how the choice of approach for handling continuous variables affects
the performance of a prognostic model. two single case studies demonstrated that dichotomising continuous predictors resulted in a loss in information and a decrease in the predictive ability of a prediction
model [1,16]. however, neither study was exhaustive, and both studies only examined the impact on
model performance using the same data that the models were derived from. in a more recent study, nieboer
and colleagues compared the effect of using log transformations, fractional polynomials and restricted cubic splines against retaining continuous predictors as linear on the performance of logistic-based prediction
models, but they did not also examine models that categorised one or more continuous predictors [9].
when developing a new prognostic model, investigators can broadly choose to (i) dichotomise, or
more generally categorise, a continuous predictor using one or more cut-points; (ii) leave the predictor
continuous but assume a linear relationship with the outcome; or (iii) leave the predictor continuous but
allow a nonlinear association with the outcome, such as by using fractional polynomials or restricted
cubic splines. how continuous measurements are included will affect the generalisability and transportability of the model [17]. a key test of a prognostic model is to evaluate its performance on an entirely
separate data set [18]. it is therefore important that the associations are appropriately modelled, to
improve the likelihood that the model will predict sufficiently well using data with different case-mix.
the aim of this article is to quantitatively illustrate the impact of the choice of approach for handling
continuous predictors on the apparent performance (based on the development data set) and validation
performance (in a separate data set) of a prognostic model.
2. methods
2.1. study data: the health improvement network
the health improvement network (thin) is a large database of anonymised electronic primary care
records collected at general practice surgeries around the united kingdom (england, scotland, wales
and northern ireland). the thin database contains medical records on approximately 4% of the united
kingdom population. clinical information from over 2 million individuals (from 364 general practices)
registered between june 1994 and june 2008 form the data set. the data have previously been used in
the external validation of a number of risk prediction models, including those considered in this study
[19‚Äì26]. there are some missing data for the predictors of systolic blood pressure, body mass index
and cholesterol. for simplicity and convenience, we have used an imputed data set used in published
external validation studies (details on the imputation strategy are reported elsewhere [20,27]).
2.2. prognostic models
we used cox regression to develop prognostic models that predict the 10-year risk of cardiovascular disease and 10-year risk of hip fracture. we split the thin database geographically by pulling out two
cohorts: general practices from england and general practices from scotland. data from the england
cohort were used to develop the models, whilst data from scotland were used to validate the model. using
data for 1,803,778 patients (men and women) from england (with 80,880 outcome events) to develop cardiovascular prognostic models. model performance was evaluated on 110,934 patients from scotland
(with 4688 outcome events). similarly, 980,465 women (with 7721 outcome events) in the thin database from england were used to develop hip fracture models. the model performance was evaluated
on data from 61,563 women from scotland (with 565 outcome events). all data come from the same underlying computer system used to collect the patient level data, and thus splitting this large database geographically is a weak form of external validation (sometimes referred to as narrow validation), although
given the health systems in the two countries are ultimately the same the case-mix is not dissimilar. in this
instance, the validation is closer to an assessment of reproducibility than transportability [18].
for convenience and simplicity, the models were developed using a subset of variables that were
chosen from the total list of predictors contained in the qrisk2 [28] and qfracture [29] risk prediction
models. the cardiovascular disease models used age (continuous), sex (binary), family history of cardiovascular disease (binary), serum cholesterol (continuous), systolic blood pressure (continuous), body
4125
mass index (continuous) and treated hypertension (binary). the hip fracture models used age (continuous), body mass index (continuous), townsend score (categorical), diagnosis of asthma (binary) and
prescription of tricyclic antidepressants (binary). table i shows the distribution of the model variables
in the thin data set for both outcomes.
2.3. resampling strategy
a resampling study was performed to examine the impact of different strategies for handling continuous
predictors in the development of prognostic models on the performance of the model when evaluated in
a separate data set.
two hundred samples were randomly drawn (with replacement) from the thin data set so that the
number of events in each sample was fixed at 25, 50, 100 and 2000. individuals in each sample were chosen by sampling a constant fraction of those who experienced the event and those who did not, according
to the overall proportion of events in the thin data set, for the corresponding prognostic model outcome
(cardiovascular disease or hip fracture). models were developed for each sample as described in sections
2.2 and 2.4. model performance (section 2.5) was evaluated on the same data used to derive the model, to
give the apparent performance, and on separate data, for validation performance (section 2.1).
2.4. approaches for handling continuous predictors
we considered three broad approaches for handling continuous predictors:
1. categorise each continuous predictor into equally sized groups, using the median value of the predictor to form two groups, the tertile values to form three groups, the quartile values to form four groups
or the quintile values to form five groups. we further categorised the age predictor by grouping individuals into 5-year or 10-year age intervals. we also categorised the continuous predictors based on
‚Äòoptimal‚Äô cut-points, using a cut-point that minimised the p-value from the logrank test for over 80%
of the observations (the lower and upper 10% of observations removed) [30].
2. model each continuous predictor by assuming that it has a linear relationship with the outcome.
3. model each continuous predictor by assuming that it has a non-linear relationship with the outcome,
using fractional polynomials [14] and restricted cubic splines [15]. fractional polynomials are a set of
flexible power transformations that describe the relationship between a continuous predictor and the
outcome. fractional polynomials of degree one (fp1) and two (fp2) are defined as
fp1√∞ √æ¬º x Œ≤1xp
;
fp2√∞ √æ¬º x Œ≤1xp1 √æ Œ≤2xp2 ; p1‚â†p2
Œ≤1xp1 √æ Œ≤2xp2 lnx; p1 ¬º p2 ¬º p

where the powers p, p1, p2 ‚àà s = {2, 1,0.5, 0, 0.5, 1, 2, 3} and x0= ln x. we used the multivariable
fractional polynomial (mfp) procedure in r to model potentially nonlinear effects while performing
table i. characteristics of the individuals in the thin data set, used as predictors in the developed prognostic models; sd: standard deviation.
variable cardiovascular disease hip fracture
development
(england)
(n = 1,803,778)
validation
(scotland)
(n = 110,934)
development
(england)
(n = 980,465)
validation
(scotland)
(n = 61,563)
mean age in years (sd) 48.6 (14.1) 48.9 (14.0) 50.8 (15.3) 50.1 (14.9)
men 922,913 (51.2%) 62,321 (56.2%) ‚Äî ‚Äî
family history of cardiovascular disease 74,668 (4.1%) 4,268 (3.8%) ‚Äî ‚Äî
mean serum cholesterol (sd) 5.5 (1.2) 5.5 (1.2) ‚Äî ‚Äî
mean systolic blood pressure (sd) 131.8 (20.3) 131.6 (20.2) ‚Äî ‚Äî
mean body mass index (sd) 26.3 (4.4) 26.3 (4.5) 26.1 (4.9) 26.5 (5.1)
treated for hypertension 96,634 (5.4%) 6,223 (5.6%) ‚Äî ‚Äî
diagnosis of asthma ‚Äî ‚Äî 84,279 (8.6%) 5,207 (8.5%)
history of falls ‚Äî ‚Äî 26,124 (2.7%) 1,016 (1.7%)
prescription of tricyclic antidepressants ‚Äî ‚Äî 51,849 (5.3%) 3,528 (5.7%)
4126
variable selection [31]. mfp formally tests for deviations from linearity using fractional polynomials. it
incorporates a closed test procedure that preserves the ‚Äòfamilywise‚Äô nominal significance level. the
default fp2 with significance level of 0.05 was chosen to build the prognostic models. the restricted cubic splines method places knots along the predictor value range and between two adjacent knots, where
the association between the predictor and the outcome is modelled using a cubic polynomial. beyond the
outer two knots, the relationship between the predictor and outcome is modelled as a linear association.
three to five knots are often sufficient to model the complex associations that are usually based on the
percentiles of the predictor values. we used the rcs function in the rms package in r [32]. prior to
conducting the simulations, preliminary analyses suggested that four degrees of freedom should be used
to model each continuous predictor with fractional polynomials (i.e., fp2 models) and that three knots
should be used in the restricted cubic splines approach. the models with the highest degrees of freedom
(df) were those categorising the continuous predictors into fifths (cvd: df = 19, hip fracture: df = 11),
model using fractional polynomials also had a maximum potential degrees of freedom also of 19 and
11 for the cvd and hip fracture models respectively. the fewest degrees of freedom were used by
the models that assumed a linear relationship with the outcome, those dichotomising (cvd: df = 7;
hip fracture: df = 5), see supplementary table 1.
figure 1 shows how the relationship between the continuous predictors (age, systolic blood pressure, body mass index and total serum cholesterol) and cardiovascular disease when estimated,
assuming linearity, nonlinearity (fractional polynomials and restricted cubic splines) and using
categorisation (supplementary figure 1 shows the corresponding relationship for continuous age
and body mass index with hip fracture). age is one of the main risk factors for many diseases,
including cardiovascular disease [33] and hip fractures [34]; we therefore examined models whereby
all continuous predictors apart from age were assumed linear, whilst age was included using the
approaches described above.
all of the continuous predictors in the development and validation data sets were centred and scaled
before modelling. scaling and centring reduce the chance of numerical underflow or overflow, which
can cause inaccuracies and other problems in model estimation [13]. the values used for transformation
were obtained from the development data set and were also applied to the validation data set.
2.5. performance measures
model performance was assessed in terms of discrimination, calibration, overall model performance and
clinical utility. discrimination is the ability of a prognostic model to differentiate between people with
different outcomes, such that those without the outcome have a lower predicted risk than those with
the outcome. the survival models in this study were based on the time-to-event. discrimination was thus
evaluated using harrell‚Äôs c-index, which is a generalisation of the area under the receiver operating characteristic curve for binary outcomes (e.g. logistic regression) [35,36].
we graphically examined the calibration of the models at a single time point, 10 years, using the val.
surv function in the rms library in r. for each random sample, hazard regression with linear splines was
used to relate the predicted probabilities from the models at 10 years to the observed event times (and
censoring indicators). the actual event probability at 10 years was estimated as a function of the estimate
event probability at 10 years. we investigated the influence of the approach used to handle continuous
predictors on calibration by overlaying plots of observed outcomes against predicted probabilities for
each of the 200 random samples.
we evaluated the clinical usefulness, or net benefit, of the models using decision curve analysis [37].
net benefit is the difference between the number of true-positive results and the number of false-positive
results, weighted by a factor that gives the cost of a false-positive relative to a false-negative result [38],
and is defined as:
net benefit ¬º true positives
n  false positives
n
pt
1  pt
 :
the numbers of true and false positives were estimated using the kaplan‚Äìmeier estimates of the
percentage surviving at 10 years among those with calculated risks greater than the threshold probability.
n is the total number of people. pt is the threshold on the probability scale that defines high risk and is
used to weight false positives to false negative results. to calculate the net benefit for survival time data
4127
subject to censoring [39], we defined x = 1 if an individual had a predicted probability from the
model ‚â• pt (the threshold probability), and x = 0 otherwise. s(t) is the kaplan‚Äìmeier survival probability
at time t (t = 10 years) and n is the number of individuals in the data set. the number of true positives is
given by [1(s(t) | x = 1)] √ó p(x = 1) √ó n and the number of false positives by (s(t) | x = 1) √ó p(x = 1) √ó n. a
decision curve was produced by plotting across a range of pt values.
3. results
tables ii and iii show the mean (standard deviation) of the c-index over the 200 samples (each containing 25, 50, 100 and 2000 events) for the models produced by each approach that predict the hip fracture
and cardiovascular outcomes, respectively. for the hip fracture models, there was a large difference of
0.1 between the mean c-index produced by the approaches that did not categorise the continuous predictors and the approach that dichotomised the continuous predictors at the median. the same c-index difference was observed in the apparent performance (i.e. the development performance) and the
geographical validation performance. a similar pattern was observed for other performance measures
including the d-statistic [40], r2 [41] and brier score (see supplementary tables 2‚Äì7). the differences
in all the measures (c-index, d-statistic, r2 and brier score [42]) all favoured the approaches that kept
the variables continuous. the approaches that assumed a linear relationship between the predictor and
outcome and the approaches that used either fractional polynomials or restricted cubic splines had similar results for all of the performance measures. the variability (as reflected in the sd) in the four model
performance metrics was small relative to the mean value for all of the methods of handling continuous
predictors.
similar patterns in model performance were observed in the cardiovascular models. for example, a
difference of 0.055 in the c-index between the approaches that assumed a linear or nonlinear relationship
between the predictor and outcome and the approach that dichotomised at the median predictor value
was observed in both the apparent performance and geographical validation.
figure 2 shows the geographical validation calibration plots for the cardiovascular models. the
models using fractional polynomials or restricted cubic splines were better calibrated than the models
produced by other approaches, including the models that assumed a linear relationship between the
predictor and outcome. a similar pattern was observed in the hip fracture models, although there was
more similarity between the linear and nonlinear approaches (see supplemental figure 7).
figure 3 shows the distribution of the predicted 10-year cardiovascular risk for a subset of the
approaches. the approaches that kept the measurements continuous (linear, fractional polynomial and
restricted cubic splines) showed similar predicted risk spreads. the approaches that categorised the
continuous predictors showed a noticeably wide predicted risk spread in those who did not experience
the outcome, which was widest when the predictor was dichotomised, and showed a noticeable narrow
spread for those who did experience the outcome. as the number of categories increases the spread of
predicted risk in those who did not have an event decreases, whilst the spread of predicted risk in those
who did have an event increases.
models developed with smaller sample sizes (25, 50 and 100 events) showed higher performance
measure values when they were evaluated using the development data set than the models developed
figure 2. calibration plots of cardiovascular disease risk in the validation cohort (2000 events).
4131
with 2000 events. this pattern was observed for both the hip fracture and cardiovascular disease models,
regardless of the how continuous predictors were included in the models. in contrast, lower values were
observed in the geographical validation of models developed with smaller sample sizes than the models
developed with 2000 events, suggesting overfitting. similarly, models developed with fewer events
showed worse calibration on the geographical validation data than the models developed with 2000
events. the models developed by categorising continuous predictors showed the greatest variability in
performance.
table iv shows the clinical consequences of the different strategies for handling the continuous predictors in the cardiovascular models, using dichotomising at the median predictor value as the reference
method. similar findings were observed in the hip fracture models. over a range of probability thresholds (e.g. 0.09 to 0.2), an additional net 5 to 10 cardiovascular disease cases per 1000 were found during
geographical validation if models that implemented fractional polynomials or restricted cubic splines
were used, rather than models that dichotomised all of the continuous predictors at the median without
conducting any unnecessary treatment. for models that categorised continuous predictors, more additional cases per 1000 found as the number of categories increases. the models that used fractional
figure 3. boxplot of the predicted cardiovascular disease risks in the validation cohort (2000 events).
table iv. range of additional net cases per 1000 found when using each approach for handling continuous
predictors, compared with categorising all of the continuous predictors at the median. models developed using
2000 outcome events. range of thresholds 0.09 to 0.2 for cardiovascular disease and 0.01 to 0.05 for hip
fracture.
model cardiovascular disease hip fracture
development validation development validation
linear 6 to 11 5 to 10 4 to 6 5 to 7
fractional polynomials 7 to 11 6 to 10 4 to 6 5 to 7
restricted cubic splines 7 to 11 6 to 10 4 to 6 5 to 7
5-year age categories 6 to 10 6 to 9 4 to 6 4 to 7
10-year age categories 6 to 10 5 to 9 2 to 5 3 to 6
thirds 2 to 6 1 to 5 1 to 3 1 to 3
fourths 4 to 8 3 to 7 1 to 4 2 to 5
fifths 5 to 9 4 to 8 2 to 5 3 to 6
4132
4124‚Äì4135
polynomials or restricted cubic splines, or that assumed a linear relationship between the predictor and
outcome all showed a higher net benefit, over a range of thresholds (0.09 to 0.2), than the categorising
approaches (see supplementary figure 9).
4. discussion
we examined the impact of the choice of approach for handling continuous predictors when developing
a prognostic model and evaluating it on a separate data set. we developed models using data sets of
varying size to illustrate the influence of sample size. the predictive ability of the models was evaluated
on a large geographical validation data set, using performance measures that have been recommended
for evaluation and reporting [43,44].
we have demonstrated that categorising continuous predictors, particularly dichotomising at the median value of the predictor, produces models that have substantially weaker predictive performance than
models produced with alternative approaches that retain the predictor on a continuous scale [1]. it is not
surprising that categorising continuous predictors leads to poor models, as it forces an unrealistic, biologically implausible and ultimately incorrect (step) relationship onto the predictor and discard information. it may appear to be sensible to use two or more quantiles (or similar apparently meaningful cutpoints, such as a particular age) to categorise an outcome into three or more groups, but this approach
does not reflect the actual predictor‚Äìoutcome relationship. individuals whose predictor values are similar
but are either side of a cut-point are assigned different levels of risk, which has clear implications on how
the model will be used in clinical practice. the fewer the cut-points, the larger the difference in risk
between two individuals with similar predictor values immediately either side of a cut-point. we
observed only small differences between the methods that retained the variables as continuous (assuming linearity or nonlinearity). larger differences would be expected if the relationship between a continuous variable and the outcome were markedly nonlinear.
continuous predictors are often categorised as the approach is intuitive, simple to implement and
researchers may expect it to improve model use and uptake. however, categorising comes at a substantial cost to the predictive performance. we believe that this cost is detrimental and counterproductive, as the resulting models have weak predictive accuracy and are unpopular for clinical use.
if ease of use is required, we recommend leaving predictors as continuous during modelling and
instead simplifying the final model, using a points system to allow finer calculation of an individual‚Äôs risk [45‚Äì47].
we focused on exploring the differences in the predictive performance of models produced by each
approach for handling continuous predictors. focusing on a fixed and small number of variables, we
circumvented issues around variable selection procedures for which we anticipate the differences in performance to be more marked. using restricted cubic splines on very small data sets may produce wiggly
functions, and fractional polynomials can result in poorly estimated tails. both problems lead to unstable
models that have poor predictive ability when evaluated on separate data in a geographical validation.
we observed very little difference in model performance when using either fractional polynomials or restricted cubic splines. similarly, when a data set is small, the centile values for dichotomising or
categorising continuous predictor values may vary considerably from sample to sample, again leading
to unstable models and poor predictive performance. we did, however, also examine the influence of
smaller sample sizes on model performance and observed greater variability in model performance in
small samples with few outcome events than in larger samples. furthermore, the apparent performance
on smaller data sets was higher compared to using larger data set (which decreased as sample size
increased), but at the expense of poorer performance in the geographical validation, where recent guidance suggests that a minimum of 100 events are required [48].
systematic reviews have highlighted numerous methodological flaws, including small sample size,
missing data and inappropriate statistical analyses in studies that describe the development or validation of prognostic models [6,8,49‚Äì52]. contributing to why many studies have methodological shortcomings is because it is generally easy to create a (poorly performing) prognostic model. a model
that has been developed using poor methods is likely to produce overoptimistic and misleading
performance. as noted by andrew vickers, ‚Äòwe have a data set, and a statistical package, and
add the former to the latter, hit a few buttons and voila, we have another paper‚Äô [53]. it is therefore
both unsurprising and fortunate that most prognostic models never get used. however, some of these
poorly performing or suboptimal prognostic models will undoubtedly be used, which may have undesirable clinical consequences. we have clearly demonstrated that categorising continuous predictor
4133
4124‚Äì4135
outcomes is an inadequate approach. investigators who wish to develop a new prognostic model for
use on patients that is fit for purpose should follow the extensive methodological guidance on model
building that discourages categorisation and use an approach that keeps the outcome continuous
[11‚Äì13].

