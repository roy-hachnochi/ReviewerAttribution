bstract
objective
to review and appraise the validity and usefulness of
published and preprint reports of prediction models
for diagnosing coronavirus disease 2019 (covid-19)
in patients with suspected infection, for prognosis of
patients with covid-19, and for detecting people in
the general population at increased risk of becoming
infected with covid-19 or being admitted to hospital
with the disease.
design
living systematic review and critical appraisal by the
covid-precise (precise risk estimation to optimise
covid-19 care for infected or suspected patients in
diverse settings) group.
data sources
pubmed and embase through ovid, arxiv, medrxiv,
and biorxiv up to 5 may 2020.
study selection
studies that developed or validated a multivariable
covid-19 related prediction model.
data extraction
at least two authors independently extracted data
using the charms (critical appraisal and data
extraction for systematic reviews of prediction
modelling studies) checklist; risk of bias was
assessed using probast (prediction model risk of
bias assessment tool).
results
14 217 titles were screened, and 107 studies
describing 145 prediction models were included. the
review identified four models for identifying people at
risk in the general population; 91 diagnostic models for
detecting covid-19 (60 were based on medical imaging,
nine to diagnose disease severity); and 50 prognostic
models for predicting mortality risk, progression
to severe disease, intensive care unit admission,
ventilation, intubation, or length of hospital stay. the
most frequently reported predictors of diagnosis and
prognosis of covid-19 are age, body temperature,
lymphocyte count, and lung imaging features. flulike symptoms and neutrophil count are frequently
predictive in diagnostic models, while comorbidities,
sex, c reactive protein, and creatinine are frequent
prognostic factors. c index estimates ranged from 0.73
to 0.81 in prediction models for the general population,
from 0.65 to more than 0.99 in diagnostic models,
and from 0.68 to 0.99 in prognostic models. all
models were rated at high risk of bias, mostly because
of non-representative selection of control patients,
exclusion of patients who had not experienced the
event of interest by the end of the study, high risk of
model overfitting, and vague reporting. most reports
did not include any description of the study population
or intended use of the models, and calibration of the
model predictions was rarely assessed.
conclusion
prediction models for covid-19 are quickly entering
the academic literature to support medical decision
making at a time when they are urgently needed. this
review indicates that proposed models are poorly
reported, at high risk of bias, and their reported
for numbered affiliations see
end of the article
what is already known on this topic
the sharp recent increase in coronavirus disease 2019 (covid-19) incidence has
put a strain on healthcare systems worldwide; an urgent need exists for efficient
early detection of covid-19 in the general population, for diagnosis of covid-19 in
patients with suspected disease, and for prognosis of covid-19 in patients with
confirmed disease
viral nucleic acid testing and chest computed tomography imaging are standard
methods for diagnosing covid-19, but are time consuming
earlier reports suggest that elderly patients, patients with comorbidities (chronic
obstructive pulmonary disease, cardiovascular disease, hypertension), and
patients presenting with dyspnoea are vulnerable to more severe morbidity and
mortality after infection
what this study adds
four models identified patients at risk in the general population (using proxy
outcomes for covid-19)
ninety one diagnostic models were identified for detecting covid-19 (60
were based on medical images; nine were for severity classification); and 50
prognostic models for predicting, among others, mortality risk, progression to
severe disease
proposed models are poorly reported and at high risk of bias, raising concern
that their predictions could be unreliable when applied in daily practice
performance is probably optimistic. hence, we do not
recommend any of these reported prediction models
for use in current practice. immediate sharing of well
documented individual participant data from covid-19
studies and collaboration are urgently needed to
develop more rigorous prediction models, and
validate promising ones. the predictors identified in
included models should be considered as candidate
predictors for new models. methodological guidance
should be followed because unreliable predictions
could cause more harm than benefit in guiding
clinical decisions. finally, studies should adhere to
the tripod (transparent reporting of a multivariable
prediction model for individual prognosis or
diagnosis) reporting guideline.
systematic review registration
protocol https://osf.io/ehc47/, registration https://
osf.io/wy245.
readers’ note
this article is a living systematic review that will
be updated to reflect emerging evidence. updates
may occur for up to two years from the date of
original publication. this version is update 2 of
the original article published on 7 april 2020 (bmj
2020;369:m1328), and previous updates can be
found as data supplements (https://www.bmj.com/
content/369/bmj.m1328/related#datasupp).
introduction
the novel coronavirus disease 2019 (covid-19)
presents an important and urgent threat to global
health. since the outbreak in early december 2019 in
the hubei province of the people’s republic of china,
the number of patients confirmed to have the disease
has exceeded 8 963 350 in 188 countries, and the
number of people infected is probably much higher.
more than 468 330 people have died from covid-19
(up to 22 june 2020).1
 despite public health responses
aimed at containing the disease and delaying the
spread, several countries have been confronted with a
critical care crisis, and more countries could follow.2-4
outbreaks lead to important increases in the demand
for hospital beds and shortage of medical equipment,
while medical staff themselves could also get infected.
to mitigate the burden on the healthcare system,
while also providing the best possible care for patients,
efficient diagnosis and information on the prognosis of
the disease is needed. prediction models that combine
several variables or features to estimate the risk of people
being infected or experiencing a poor outcome from the
infection could assist medical staff in triaging patients
when allocating limited healthcare resources. models
ranging from rule based scoring systems to advanced
machine learning models (deep learning) have been
proposed and published in response to a call to share
relevant covid-19 research findings rapidly and openly
to inform the public health response and help save
lives.5
 many of these prediction models are published in
open access repositories, ahead of peer review.
we aimed to systematically review and critically
appraise all currently available prediction models for
covid-19, in particular models to predict the risk of
developing covid-19 or being admitted to hospital with
covid-19, models to predict the presence of covid-19
in patients with suspected infection, and models to
predict the prognosis or course of infection in patients
with covid-19. we included model development and
external validation studies. this living systematic
review, with periodic updates, is being conducted
by the covid-precise (precise risk estimation to
optimise covid-19 care for infected or suspected
patients in diverse settings) group in collaboration
with the cochrane prognosis methods group.
methods
we searched pubmed and embase through ovid,
biorxiv, medrxiv, and arxiv for research on covid-19
published after 3 january 2020. we used the publicly
available publication list of the covid-19 living
systematic review.6
 this list contains studies on
covid-19 published on pubmed and embase through
ovid, biorxiv, and medrxiv, and is continuously
updated. we validated whether the list is fit for
purpose (online supplementary material) and further
supplemented it with studies on covid-19 retrieved
from arxiv. the online supplementary material
presents the search strings. additionally, we contacted
authors for studies that were not publicly available
at the time of the search,7 8 and included studies that
were publicly available but not on the living systematic
review6
 list at the time of our search.9-12
we searched databases repeatedly up to 5 may 2020
(supplementary table 1). all studies were considered,
regardless of language or publication status (preprint
or peer reviewed articles; updates of preprints are
only included and reassessed after publication in a
peer reviewed journal). we included studies if they
developed or validated a multivariable model or
scoring system, based on individual participant level
data, to predict any covid-19 related outcome. these
models included three types of prediction models:
diagnostic models for predicting the presence or
severity of covid-19 in patients with suspected
infection; prognostic models for predicting the course
of infection in patients with covid-19; and prediction
models to identify people at increased risk of covid-19
in the general population. no restrictions were made
on the setting (eg, inpatients, outpatients, or general
population), prediction horizon (how far ahead the
model predicts), included predictors, or outcomes.
epidemiological studies that aimed to model disease
transmission or fatality rates, diagnostic test accuracy,
and predictor finding studies were excluded. starting
with the second update, retrieved records were initially
screened by a text analysis tool developed by artificial
intelligence to prioritise sensitivity (supplementary
material). titles, abstracts, and full texts were screened
for eligibility in duplicate by independent reviewers
(pairs from lw, bvc, mvs) using eppi-reviewer,13 and
discrepancies were resolved through discussion.
data extraction of included articles was done by
two independent reviewers (from lw, bvc, gsc, tpad,
mch, gh, kgmm, rdr, es, ljms, ews, kies, cw,
al, jm, tt, jaad, kl, jbr, lh, cs, ms, mch, ns, nk,
smjvk, jcs, pd, clan, rw, gpm, it, jyv, dld, jw, fsvr,
ph, vmtdj, and mvs). reviewers used a standardised
data extraction form based on the charms (critical
appraisal and data extraction for systematic reviews
of prediction modelling studies) checklist14 and
probast (prediction model risk of bias assessment
tool) for assessing the reported prediction models.15 we
sought to extract each model’s predictive performance
by using whatever measures were presented. these
measures included any summaries of discrimination
(the extent to which predicted risks discriminate
between participants with and without the outcome),
and calibration (the extent to which predicted risks
correspond to observed risks) as recommended in
the tripod (transparent reporting of a multivariable
prediction model for individual prognosis or diagnosis)
statement.16 discrimination is often quantified by
the c index (c index=1 if the model discriminates
perfectly; c index=0.5 if discrimination is no better
than chance). calibration is often quantified by the
calibration intercept (which is zero when the risks are
not systematically overestimated or underestimated)
and calibration slope (which is one if the predicted
risks are not too extreme or too moderate).17 we
focused on performance statistics as estimated from
the strongest available form of validation (in order
of strength: external (evaluation in an independent
database), internal (bootstrap validation, cross
validation, random training test splits, temporal
splits), apparent (evaluation by using exactly the
same data used for development)). any discrepancies
in data extraction were discussed between reviewers,
and remaining conflicts were resolved by lw and mvs.
the online supplementary material provides details
on data extraction. we considered aspects of prisma
(preferred reporting items for systematic reviews and
meta-analyses)18 and tripod16 in reporting our article.
patient and public involvement
it was not possible to involve patients or the public in
the design, conduct, or reporting of our research. the
study protocol and preliminary results are publicly
available on https://osf.io/ehc47/ and medrxiv.
results
we retrieved 14209 titles through our systematic
search (of which 9306 were included in the present
update; supplementary table 1, fig 1). two additional
unpublished studies were made available on request
(after a call on social media). we included a further
six studies that were publicly available but were not
detected by our search. of 14217 titles, 275 studies were
retained for abstract and full text screening (of which
76 in the present update). one hundred seven studies
describing 145 prediction models met the inclusion
criteria (of which 56 papers and 79 models added in
the present update, supplementary table 1).7-12 19-119
these studies were selected for data extraction and
critical appraisal (table 1, table 2, table 3, and table 4).
primary datasets
forty five studies used data on patients with covid-19
from china (supplementary table 2), six from
italy,32 39 72 74 76 79 three from brazil,69 81 109 three from
france,71 77 110 three from the united states,96 108 112 two
from south korea,6380 one from belgium,82 one from
the netherlands,95 one from the united kingdom,75
one from israel,67 one from mexico,70 and one from
singapore.40 twenty two studies used international data
(supplementary table 2) and two studies used simulated
data.35 41 three studies used proxy data to estimate
covid-19 related risks (eg, medicare claims data from
2015 to 2016).8 90 113 twelve studies were not clear on
the origin of covid-19 data (supplementary table 2).
based on 59 studies that reported study dates,
data were collected between 8 december 2019 and
21 april 2020. four studies reported median followup time (4.5, 8.4, 15, and 18 days),20 37 83 108 while
another study reported a follow-up of at least five
days.42 some centres provided data to multiple studies
and several studies used open github120 or kaggle121
data repositories (version or date of access often
unspecified), and so it was unclear how much these
datasets overlapped across our identified studies
(supplementary table 2). one study25 developed
prediction models for use in paediatric patients. the
median age in studies on adults varied from 34 to 68
years, and the proportion of men varied from 35% to
75%, although this information was often not reported
at all (supplementary table 2).
among the studies that developed prognostic models
to predict mortality risk in people with confirmed or
suspected infection, the percentage of deaths varied
between 1% and 59% (table 3). this wide variation is
partly because of substantial sampling bias caused by
studies excluding participants who still had the disease
at the end of the study period (that is, they had neither
recovered nor died).7 21-23 44 96 98 100 additionally, length
of follow-up could have varied between studies (but was
rarely reported), and there might be local and temporal
variation in how people were diagnosed as having
covid-19 or were admitted to the hospital (and therefore
recruited for the studies). among the diagnostic model
studies, only nine reported on the prevalence of
covid-19 and used a cross sectional or cohort design;
the prevalence varied between 17% and 79% (table 2).
because 58 diagnostic studies used either case-control
sampling or an unclear method of data collection, the
prevalence in these diagnostic studies might not have
been representative of their target population.
table 1, table 2, and table 3 give an overview of the
145 prediction models reported in the 107 identified
studies. supplementary table 2 provides modelling
details and box 1 discusses the availability of models
in a format for use in clinical practice.
models to predict risks of covid-19 in the general
population
we identified four models that predicted risk of
covid-19 in the general population. three models
from one study used hospital admission for non-
tuberculosis pneumonia, influenza, acute bronchitis,
or upper respiratory tract infections as proxy outcomes
in a dataset without any patients with covid-19.8
among the predictors were age, sex, previous hospital
admissions, comorbidity data, and social determinants
of health. the study reported c indices of 0.73,
0.81, and 0.81. a fourth model used deep learning
on thermal videos from the faces of people wearing
facemasks to determine abnormal breathing (not covid
related) with a reported sensitivity of 80%.90
diagnostic models to detect covid-19 in patients
with suspected infection
we identified 22 multivariable models to diagnose
covid-19. most models targeted patients with
suspected covid-19. reported c index values ranged
between 0.65 and 0.99. a few models also evaluated
calibration and reported good results.69 78 117 the most
frequently used diagnostic predictors (at least 10
times) were flu-like signs and symptoms (eg, shiver,
fatigue), imaging features (eg, pneumonia signs on
computed tomography scan), age, body temperature,
lymphocyte count, and neutrophil count (table 2).
nine studies aimed to diagnose severe disease in
patients with covid-19: eight in adults with covid-19
with reported c indices between value of 0.80 and 0.99,
and one in paediatric patients with reported perfect
performance.25 predictors of severe covid-19 used more
than once were comorbidities, liver enzymes, c reactive
protein, imaging features, and neutrophil count.
sixty prediction models were proposed to support
the diagnosis of covid-19 or covid-19 pneumonia (and
some also to monitor progression) based on images.
most studies used computed tomography images
or chest radiographs. others used spectrograms of
cough sounds53 and lung ultrasound.73 the predictive
performance varied widely, with estimated c index
values ranging from 0.81 to more than 0.99.
prognostic models for patients with diagnosis of
covid-19
we identified 50 prognostic models (table 3) for patients
with a diagnosis of covid-19. the intended use of these
models (that is, when to use them, and for whom) was
often not clearly described. prediction horizons varied
between one and 30 days, but were often unspecified.
of these models, 23 estimated mortality risk and eight
aimed to predict progression to a severe or critical state
(table 3). the remaining studies used other outcomes
(single or as part of a composite) including recovery,
length of hospital stay, intensive care unit admission,
intubation, (duration of) mechanical ventilation,
and acute respiratory distress syndrome. one study
used data from 2015 to 2019 to predict mortality and
prolonged assisted mechanical ventilation (as a noncovid-19 proxy outcome).113
additional records identified through other sources
articles excluded
not a prediction model development or validation study
epidemiological model to estimate disease transmission or case fatality rate
commentary, editorial or letter
methods paper
duplicate article
77
27
18
33
13
records screened
records identified through database searching
records excluded
articles assessed for eligibility
studies included in review (with 145 models)
168
107
13 942
14 217
14 209 8
diagnostic models
(including 9 severity models
and 60 imaging studies)
prognostic models
(including 23 for mortality,
8 for progression to
severe or critical state)
models to identify subjects
at risk in general population
275
4 91 50
fig 1 | prisma (preferred reporting items for systematic reviews and meta-analyses) flowchart of study inclusions and
exclusions
the most frequently used prognostic factors (for
any outcome, included at least 10 times) included
comorbidities, age, sex, lymphocyte count, c reactive
protein, body temperature, creatinine, and imaging
features (table 3).
studies that predicted mortality reported c indices
between 0.68 and 0.98. some studies also evaluated
calibration.7 67 116 when applied to new patients, the
model by xie et al yielded probabilities of mortality
that were too high for low risk patients and too low
for high risk patients (calibration slope >1), despite
excellent discrimination.7
 the mortality model by
zhang et al also showed miscalibrated (overfitted and
underestimated) risks at external validation,116 while
the model by barda et al showed underfitting.67
the studies that developed models to predict
progression to a severe or critical state reported c
indices between 0.73 and 0.99. three of these studies
also reported good calibration, but this was evaluated
internally (eg, bootstrapped)88 or in an unclear way.83 119
reported c indices for other outcomes varied
between 0.72 and 0.96. singh et al and zhang et al
also evaluated calibration externally (in new patients).
singh showed that the epic deterioration index
overestimated the risk or a poor outcome, while the
poor outcome model by zhang et al underestimated
the risk of a poort outcome.108 116
risk of bias
all studies were at high risk of bias according to
assessment with probast (table 1, table 2, and table
3), which suggests that their predictive performance
when used in practice is probably lower than that
reported. therefore, we have cause for concern that
the predictions of the proposed models are unreliable
when used in other people. box 2 gives details on
common causes for risk of bias for each type of model.
fifty three of the 107 studies had a high risk of bias
for the participants domain (table 4), which indicates
that the participants enrolled in the studies might not
be representative of the models’ targeted populations.
unclear reporting on the inclusion of participants
prohibited a risk of bias assessment in 26 studies.
fifteen of the 107 studies had a high risk of bias for
the predictor domain, which indicates that predictors
were not available at the models’ intended time of
use, not clearly defined, or influenced by the outcome
measurement. one diagnostic imaging study used a
simple scoring rule and was scored at low predictor
risk of bias. the diagnostic model studies that used
medical images as predictors in artificial intelligence
were all scored as unclear on the predictor domain.
the publications often lacked clear information on the
preprocessing steps (eg, cropping of images). moreover,
complex machine learning algorithms transform
images into predictors in a complex way, which makes
it challenging to fully apply the probast predictors
section for such imaging studies. most studies used
outcomes that are easy to assess (eg, death, presence
of covid-19 by laboratory confirmation). nonetheless,
there was cause for concern about bias induced by the
outcome measurement in 19 studies, for example due
to the use of subjective or proxy outcomes (eg, non
covid-19 severe respiratory infections).
all but one of these studies50 were at high risk
of bias for the analysis domain (table 4). many
table 1 | overview of prediction models for use in the general population
study; setting; and outcome predictors in final model
sample size: total
no of participants for
model development
set (no with outcome)
predictive performance on validation
overall risk
of bias using
probast
type of
validation*
sample size: total
no of participants for
model validation (no
with outcome)
performance* (c index,
sensitivity (%), specificity
(%), ppv/npv (%),
calibration slope, other
(95% ci, if reported))
general population
original review
decaprio et al8
; data from us
general population; hospital
admission for covid-19
pneumonia (proxy events)†
age, sex, number of previous
hospital admissions,
11 diagnostic features,
interactions between age and
diagnostic features
1.5 million (unknown) training test split 369 865 (unknown) c index 0.73 high
decaprio et al8
; data from us
general population; hospital
admission for covid-19
pneumonia (proxy events)†
age and ≥500 features
related to diagnosis history
1.5 million (unknown) training test split 369 865 (unknown) c index 0.81 high
decaprio et al8
; data from us
general population; hospital
admission for covid-19
pneumonia (proxy events)†
≥500 undisclosed features,
including age, diagnostic
history, social determinants
of health, charlson
comorbidity index
1.5 million (unknown) training test split 369 865 (unknown) c index 0.81 high
update 2
jiang et al90; data from
china, respiratory patients
versus healthy volunteers;
detection of respiratory
diseases such as covid-19 infrared/thermal video of face unknown training test split not applicable sensitivity 80, ppv 90 high
npv=negative predictive value; ppv=positive predictive value; probast=prediction model risk of bias assessment tool.
*performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is
reported. apparent performance is the performance observed in the development data.
†proxy events used: pneumonia (except from tuberculosis), influenza, acute bronchitis, or other specified upper respiratory tract infections (no patients with covid-19 pneumonia in data).
setting; and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported)) diagnosis original review feng et al10; data from china, patients presenting at fever clinic; suspected covid-19 pneumonia age, temperature, heart rate, diastolic blood pressure, systolic blood pressure, basophil count, platelet count, mean corpuscular haemoglobin content, eosinophil count, monocyte count, fever, shiver, shortness of breath, headache, fatigue, sore throat, fever classification, interleukin 6 132 (26) temporal validation 32 (unclear) c index 0.94 high lopez-rincon et al35; data from international genome sequencing data repository, target population unclear; covid-19 diagnosis specific sequences of base pairs 553 (66) 10-fold cross validation not applicable c index 0.98, sensitivity100, specificity 99 high meng et al12; data from china, asymptomatic patients with suspected covid-19; covid-19 diagnosis age, activated partial thromboplastin time, red blood cell distribution width sd, uric acid, triglyceride, serum potassium, albumin/globulin, 3-hydroxybutyrate, serum calcium 620 (302) external validation 145 (80) c index 0.87‡ high song et al31; data from china, inpatients with suspected covid-19; covid-19 diagnosis fever, history of close contact, signs of pneumonia on ct, neutrophil to lymphocyte ratio, highest body temperature, sex, age, meaningful respiratory syndromes 304 (73) training test split 95 (18) c index 0.97 (0.93 to 1.00) high update 1 martin et al41; simulated patients with suspected covid-19; covid-19 diagnosis unknown not applicable external validation only (simulation) not applicable sensitivity 97, specificity 96 high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis age, sex, temperature, heart rate, systolic blood pressure, diastolic blood pressure, sore throat 292 (49) leave-one-out cross validation not applicable c index 0.65 (0.57 to 0.73) high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis sex, temperature, heart rate, respiration rate, diastolic blood pressure, sore throat, sputum production, shortness of breath, gastrointestinal symptoms, lymphocytes, neutrophils, eosinophils, creatinine 292 (49) leave-one-out cross validation not applicable c index 0.88 (0.83 to 0.93) high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis sex, temperature, heart rate, respiration rate, diastolic blood pressure, sputum production, gastrointestinal symptoms, chest radiograph or ct scan suggestive of pneumonia, neutrophils, eosinophils, creatinine 292 (49) leave-one-out cross validation not applicable c index 0.88 (0.83 to 0.93) high sun et al40; data from singapore, patients with suspected infection presenting at infectious disease clinic; covid-19 diagnosis sex, covid-19 case contact, travel to wuhan, travel to china, temperature, heart rate, respiration rate, diastolic blood pressure, sore throat, sputum production, gastrointestinal symptoms, chest radiograph or ct scan suggestive of pneumonia, neutrophils, eosinophils, creatinine, sodium 292 (49) leave-one-out cross validation not applicable c index 0.91 (0.86 to 0.96) high wang et al43; data from china, patients with suspected covid-19; covid-19 pneumonia epidemiological history, wedge shaped or fan shaped lesion parallel to or near the pleura, bilateral lower lobes, ground glass opacities, crazy paving pattern, white blood cell count 178 (69) external validation 116 (68) c index 0.85, calibration slope 0.56 high
wu et al45; data from china, inpatients
with suspected covid-19; covid-19
diagnosis
lactate dehydrogenase, calcium, creatinine, total protein,
total bilirubin, basophil, platelet distribution width, kalium,
magnesium, creatinine kinase isoenzyme, glucose
108 (12) training test split 107 (61) c index 0.99, sensitivity 100,
specificity 94
high
update 2
batista et al69; data from brazil,
inpatients with suspected covid-19
admitted to the emergency care
department; covid-19 diagnosis
age, sex, haemoglobin, platelets, red blood cells, mean
corpuscular haemoglobin concentration, mean corpuscular
haemoglobin, red cell distribution width , mean corpuscular
volume, leukocytes, lymphocytes, monocytes, basophils,
eosinophils and c reactive protein
234 (102) training test split 31 (unknown) c index 0.85, sensitivity 68,
specificity 85
high
table 2 | overview of prediction models for diagnosis of covid-19
(continued) and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported)) brinati et al74; data from italy, inpatients with suspected covid-19; covid-19 diagnosis age, aspartate aminotransferase, lymphocytes, lactodehydrogenase, pcr, wbc count, eosinophils, alanine transaminase, neutrophils, gamma-glutamyltransferase, monocytes, basophils, alkaline phosphatase, platelets 279 (102) training test split 56 (20) c index 0.84, sensitivity 92, specificity 65 high brinati et al74; data from italy, inpatients with suspected covid-19; covid-19 diagnosis age, aspartate aminotransferase, lymphocytes, lactodehydrogenase, pcr, wbc count, eosinophils, alanine transaminase, neutrophils, gamma-glutamyltransferase, monocytes, basophils, alkaline phosphatase, platelets 279 (102) training test split 56 (20) sensitivity 95, specificity 75, ppv 86 high
chen et al78; data from china, inpatients
with suspected covid-19; covid-19
diagnosis
total number of mixed ggo in peripheral area, tree-in-bud,
offending vessel augmentation in lesions, respiration, heart
ratio, temperature, wbc count, cough, fatigue, lymphocyte count
98 (51) training test split 38 (19) c index 0.94 (0.87 to 1.00),
sensitivity 74, specificity 79
high
diaz-quijano et al81; data from brazil,
inpatients with suspected covid-19;
covid-19 diagnosis
age, days after reporting first confirmed case in federal unit,
fever, cough, sore throat, diarrhoea, coryza, chills, pulmonary
manifestation, other signs, hiv, kidney diseaes, trip outside
brazil up to 14 days before onset
1243 (541) external validation
(new centres, brazil)
4192 (785) c index 0.73 (0.71 to 0.75),
sensitivity 46, specificity 80
high
kurstjens et al95; data from the
netherlands, inpatients with suspected
covid-19; covid-19 diagnosis
age, sex, crp, ld, ferritin, absolute neutrophil count, absolute
lymphocyte count, chest radiograph
375 (276) external (unclear) 592 (393) c index 0.91 (0,89 to 0,94) high
mei et al101; data from china; inpatients
with suspected covid-19; covid-19
diagnosis
age, sex, ct imaging, exposure history, symptoms (present or
absent of fever, cough and/or sputum), wbc counts, neutrophil
count, percentage neutrophils, lymphocyte counts, percentage
lymphocytes
534 (242) training test split 279 (134) c index 0.92 (0.89 to 0.95),
sensitivity 84 (77 to 90),
specificity 83 (76 to 89),
ppv 81.9 (76 to 87),
npv 85 (79 to 90)
high
menni et al102; data from uk and usa,
suspected covid-19; covid-19 diagnosis
age, sex, loss of smell and taste, severe or significant persistent
cough, severe fatigue, skipped meals
12 510 (5162) external validation
(new centres, usa)
2763 (726) c index 0.76 (0.74 to 0.78),
sensitivity 66 (62 to 69),
specificity 83 (82 to 85),
ppv 58 (55 to 62),
npv 87 (86 to 89)
high
soares et al109; data from brazil;
patients with suspected infection
presenting at triage centre; covid-19
diagnosis
age, red blood cells, mean corpuscular volume, mean
corpuscular haemoglobin concentration, mean corpuscular
haemoglobin, red blood cell distribution width, leukocytes,
basophils, monocytes, lymphocytes, platelets, mean platelet
volume, creatinine, potassium, sodium, crp
599 (81) repeated 10-fold
cross validation
not applicable c index 0.87 (0.86 to 0.88),
sensitivity 70 (67 to 73),
specificity 86 (85 to 87),
npv 95 (94 to 95),
ppv 45 (43 to 47)
high
tordjman et al110; data from france;
suspected patients; covid-19 diagnosis
eosinophils, lymphocytes, neutrophils, basophils 100 (50) external validation
(new centres,
france)
300 (208) c index 0.89 (0.85 to 0.93),
sensitivity 80, specificity 85,
ppv 92
high
zhao et al117; data from china;
inpatients with suspected covid-19;
covid-19 diagnosis
fever, chest ct, crp, pct, wbc 547 (unknown) training test split 275 (unknown) c index 0.97 (0.96 to 0.97) high
diagnostic severity classification
original review
yu et al25; data from china, paediatric
inpatients with confirmed covid-19;
severe disease (yes/no) defined based
on clinical symptoms
direct bilirubin, alanine transaminase 105 (8) apparent
performance only
not applicable f1 score 1.00 high
update 1
zhou et al46; data from china, inpatients
with confirmed covid-19; severe
pneumonia
age, sex, onset-admission time, high blood pressure, diabetes,
chd, copd, white blood cell counts, lymphocyte, neutrophils,
alanine transaminase, aspartate aminotransferase, serum
albumin, serum creatinine, blood urea nitrogen, crp
250 (79) training test split 127 (38) c index 0.88 (0.94 to 0.92),
sensitivity 89, specificity 74
high
table 2 | continued
and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported)) update 2 benchoufi et al71; data from france, inpatients with suspected or confirmed covid-19; lung injury severity (pathologic vs normal) lung ultrasound scores for 8 quadrants in a global score 90 (unknown) internal validation by resampling (bootstrap) not applicable c index 0.93, sensitivity 95, specificity 83 high
chassagnon et al77; data from france,
inpatients with confirmed covid-19;
severe covid-19
unclear 50 (unknown) external validation
(new centres,
france)
130 (unknown) c index 0.80, sensitivity 69,
specificity 79
high
li et al97; data from china, target
population unclear; severe covid-19
portion of infection, average infection hounsfield unit, a
measure of radio density
196 (32) apparent
performance only
not applicable c index 0.97 (0.94 to 0.98),
sensitivity 94 (87 to 98),
specificity 88 (85 to 91)
high
lyu et al99; data from china, target
population unclear; severe/critical
covid-19 pneumonia
unclear 51 (39) apparent
performance only
not applicable c index 0.99 (0.88 to 1.00),
sensitivity 90, specificity 100
high
lyu et al99; data from china , target
population unclear; critical covid-19
pneumonia
unclear 39 (24) apparent
performance only
not applicable c index 0.92 (0.73 to 0.99),
sensitivity 92, specificity 87
high
wang et al114; data from china,
inpatients with confirmed covid-19;
severe covid-19
neutrophil-to-lymphocyte ratio, red cell volume distribution
width
45 (10) apparent
performance only
not applicable c index 0.94 (0.90 to 0.97),
sensitivity 90, specificity 85,
ppv 52, npv 96
high
zhu et al118; data from china, inpatients
with confirmed covid-19; severe
covid-19
peripheral blood cytokine il-6, crp, hypertension 127 (16) apparent
performance only
not applicable c index 0.90 (0.83 to 0.97),
sensitivity 100 (79 to 100),
specificity 66 (56 to 75)
high
diagnostic imaging
original review
barstugan et al32; data from italy,
patients with suspected covid-19;
covid-19 diagnosis
not applicable 53 (not applicable) cross validation not applicable sensitivity 93, specificity 100 high
chen et al27; data from china, people
with suspected covid-19 pneumonia;
covid-19 pneumonia
not applicable 106 (51) training test split 27 (11) sensitivity 100, specificity 82 high
gozes et al26; data from china and
us,§ patients with suspected covid-19;
covid-19 diagnosis
not applicable 50 (unknown) external validation
with chinese cases
and us controls
unclear c index 0.996 (0.989 to 1.000) high
jin et al11; data from china, us, and
switzerland,¶ patients with suspected
covid-19; covid-19 diagnosis
not applicable 416 (196) training test split 1255 (183) c index 0.98, sensitivity 94,
specificity 95
high
jin et al33; data from china, patients
with suspected covid-19; covid-19
pneumonia
not applicable 1136 (723) training test split 282 (154) c index: 0.99, sensitivity 97,
specificity 92
high
li et al34; data from china, patients with
suspected covid-19; covid-19 diagnosis
not applicable 2969 (400) training test split 353 (68) c index 0.96 (0.94 to 0.99),
sensitivity 90 (83 to 94),
specificity 96 (93 to 98)
high
shan et al29; data from china, people
with confirmed covid-19; segmentation
and quantification of infection regions in
lung from chest ct scans
not applicable 249 (not applicable) training test split 300 (not applicable) dice similarity coefficient
91.6%**
high
shi et al36; data from china, target
population unclear; covid-19
pneumonia
five categories of location features from imaging: volume,
number, histogram, surface, radiomics
2685 (1658) fivefold cross
validation
not applicable c index 0.94 high
table 2 | continued
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported))
wang et al30; data from china, target
population unclear; covid-19 diagnosis
not applicable 259 (79) internal, other
images from same
people
not applicable c index 0.81 (0.71 to 0.84),
sensitivity 83, specificity 67
high
xu et al28; data from china, target
population unclear; covid-19 diagnosis
not applicable 509 (110) training test split 90 (30) sensitivity 87, ppv 81 high
song et al24; data from china, target
population unclear; diagnosis of
covid-19 v healthy controls
not applicable 123 (61) training test split 51 (27) c index 0.99 high
song et al24; data from china, target
population unclear; diagnosis of
covid-19 v bacterial pneumonia
not applicable 131 (61) training test split 57 (27) c index 0.96 high
zheng et al38; data from china, target
population unclear; covid-19 diagnosis
not applicable unknown temporal validation unknown c index 0.96 high
update 1
abbas et al47; data from repositories
(origin unspecified), target population
unclear; covid-19 diagnosis
not applicable 137 (unknown) training test split 59 (unknown) c index 0.94, sensitivity 98,
specificity 92
high
apostolopoulos et al48; data from
repositories (us, italy); patients with
suspected covid-19; covid-19 diagnosis
not applicable 1427 (224) tenfold cross
validation
not applicable sensitivity 99, specificity 97 high
bukhari et al49; data from canada and
us; patients with suspected covid-19;
covid-19 diagnosis
not applicable 223 (unknown) training test split 61 (17) sensitivity 98, ppv 91 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; percentage lung
opacity
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.98 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; percentage high
lung opacity
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.98 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; severity score
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.97 high
chaganti et al50; data from canada, us,
and european countries; patients with
suspected covid-19; lung opacity score
not applicable 631 (not applicable) training test split 100 (not applicable) correlation§§ 0.97 high
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal”
not applicable unknown fivefold cross
validation
not applicable c index 0.99 high
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal” and viral
pneumonia
not applicable unknown fivefold cross
validation
not applicable c index 0.98 high
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal”
not applicable unknown fivefold cross
validation
not applicable c index 0.998 high
table 2 | continued
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported))
chowdhury et al39; data from
repositories (italy and other unspecified
countries), target population unclear;
covid-19 v “normal” and viral
pneumonia
not applicable unknown fivefold cross
validation
not applicable c index 0.99 high
fu et al51; data from china, target
population unclear; covid-19 diagnosis
not applicable 610 (100) external validation 309 (50) c index 0.99, sensitivity 97,
specificity 99
high
gozes et al52; data from china, people
with suspected covid-19; covid-19
diagnosis
not applicable 50 (unknown) external validation 199 (109) c index 0.95 (0.91 to 0.99) high
imran et al53; data from unspecified
source, target population unclear;
covid-19 diagnosis
not applicable 357 (48) twofold cross
validation
not applicable sensitivity 90, specificity 81 high
li et al54; data from china, inpatients
with confirmed covid-19; severe and
critical covid-19
severity score based on ct scans not applicable external validation
of existing score
78 (not applicable) c index 0.92 (0.84 to 0.99) high
li et al55; data from unknown origin,
patients with suspected covid-19;
covid-19
not applicable 360 (120) training test split 135 (45) c index 0.97 high
hassanien et al56; data from repositories
(origin unspecified), people with
suspected covid-19; covid-19 diagnosis
not applicable unknown training test split unknown sensitivity 95, specificity 100 high
tang et al57; data from china, patients
with confirmed covid-19; covid-19
severe v non-severe
not applicable 176 (55) threefold cross
validation
not applicable c index 0.91, sensitivity 93,
specificity 75
high
wang et al42; data from china, inpatients
with suspected covid-19; covid-19
not applicable 709 (560) external validation
in other centres
508 (223) c index (average) 0.87 high
zhang et al58; data from repositories
(origin unspecified), people with
suspected covid-19; covid-19
not applicable 1078 (70) twofold cross
validation
not applicable c index 0.95, sensitivity 96,
specificity 71
high
zhou et al59; data from china, patients
with suspected covid-19; covid-19
diagnosis
not applicable 191 (35) external validation
in other centres
107 (57) c index 0.92, sensitivity 83,
specificity 86
high
update 2
angelov et al64; data from unknown
origin; covid-19 diagnosis
not applicable unknown apparent
performance only
not applicable c index 0.89, sensitivity 89,
ppv 90
high
arpan et al65; data from repositories
(multiple countries); covid-19 diagnosis
not applicable 3516 (80) training test split 424 (19) c index >0.99, sensitivity 100,
ppv 94
high
bai et al66; data from china and us,
target population unclear; covid-19
diagnosis
not applicable 830 (377) training test split 119 (42) c index 0.95, sensitivity 95 (83
to 100) , specificity 96 (90 to
99)
high
bassi et al68; data from italy, target
population unclear, covid-19 diagnosis
not applicable unknown training test split unknown sensitivity 98, ppv 98 high
borghesi et al72; data from italy,
target population unclear, ; severity of
covid-19 pneumonia
sum score for lung abnormalities based on chest radiograph not applicable external validation
only
100 (unknown) agreement, kappa 0.82 (0.79
to 0.86)
high
born et al73; data from repositories
(origin unspecified), target population
unclear, covid-19 diagnosis
not applicable 64 (37) fivefold cross
validation
not applicable c index 0.94 (0.82 to 1.00),
sensitivity 96, specificity 79
high
table 2 | continued
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other (95% ci, if reported))
castiglioni et al76; data from italy,
inpatients suspected of covid-19;
covid-19 diagnosis
not applicable 500 (250) temporal validation 110 (36) c index 0.81 (0.73 to 0.87),
sensitivity 80 (72 to 86),
specificity 81 (73 to 87), ppv 89
(82 to 94), npv 66 (57 to 75)
high
guiot et al82; data from belgium,
inpatients suspected of covid-19;
covid-19 diagnosis
30 radiomics features 727 (unknown) training test split 165 (unknown) c index 0.94 (0.88 to 1.00) ,
sensitivity 79, specificity 91, ppv
54, npv 97
high
hu et al86; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 629 (313) training test split 201 (104) c index 92 (84 to 100),
sensitivity 86, specificity 85
high
islam et al87; data from unknown origin,
inpatients suspected of covid-19;
covid-19 diagnosis
not applicable 16130 (98) unknown origin 210 (10) sensitivity 80 high
kana et al91; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 5092 (161) external validation,
different repository
(unknown origin)
600 (200) sensitivity 100, specificity 100 high
karim et al92; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable unknown unknown unknown severe inconsistencies in
reported performance: data not
extracted
high
khan et al93; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 1300 (284) training test split 221 (30) sensitivity 100, ppv 97 high
kumar et al94; data from usa, china and
italy, target population unclear; covid-19
diagnosis; covid-19 diagnosis
not applicable unknown apparent
performance only
not applicable c index 0.997, sensitivity 100,
specificity 100
high
kumar et al94; data from usa, china and
italy, target population unclear; covid-19
diagnosis; covid-19 diagnosis
not applicable unknown apparent
performance only
not applicable c index 0.998, sensitivity 100,
specificity 100
high
moutounet-cartan103; data from
repositories, target population unclear;
covid-19 pneumonia
not applicable 325 (125) training test split 98 (unknown) sensitivity 88 high
ozturk et al104; data from repositories,
target population unclear; covid-19
pneumonia
not applicable 1127 (127) fivefold cross
validation
not applicable sensitivity 85, specificity 92
ppv 90
high
rahimzadeh et al105; data from
repositories, target population unclear;
covid-19 pneumonia
not applicable 633 (149) fivefold cross
validation
not applicable sensitivity 81, specificity 100,
ppv 35
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 320 (160) training test split 80 (40) sensitivity 100, specificity 98,
ppv 96¶¶
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 320 (160) training test split 80 (40) sensitivity 100, specificity 98,
ppv 96¶¶
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 320 (160) training test split 80 (40) sensitivity 100, specificity 98,
ppv 96¶¶
high
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 480 (160) training test split 120 (40) sensitivity 98, specificity 99.
ppv 96
high
table 2 | continued
research
studies had small sample sizes (table 1, table 2,
table 3), which led to an increased risk of overfitting,
particularly if complex modelling strategies were
used. three studies did not report the predictive
performance of the developed model, and four
studies reported only the apparent performance
(the performance with exactly the same data used to
develop the model, without adjustment for optimism
owing to potential overfitting). only 13 studies
assessed calibration,7 12 22 43 50 67 69 78 83 108 116 117 119
but the method to check calibration was probably
suboptimal in two studies.12 119
twenty five models were developed and externally
validated in the same study (in an independent dataset,
excluding random training test splits and temporal
splits).7 12 26 42 43 51 52 59 67 77 81 83 84 91 95 100 102 110 112 113
116 119 however, in 11 of these models, the datasets
used for the external validation were likely not
representative of the target population,7 12 26 42 59
91 100 102 116 and in one study, data from before the
covid-19 crisis were used.113 consequently, predictive
performance could differ if the models are applied in
the targeted population. in one study, commonly used
performance statistics for prognosis (discrimination,
calibration) were not reported.42 gozes,52 fu,51
chassagnon,77 hu,84 kurstjens,95 and vaid112 had
satisfactory predictive performance on an external
validation set, but it is unclear how the data for the
external validation were collected (eg, whether the
patients were consecutive), and whether they are
representative. wang,43 barda,67 guo,83 tordjman,110
and gong119 obtained satisfactory discrimination on
probably unbiased validation datasets, but each of
these had fewer than the recommended number of
events for external validation (100).137 138 diaz-quijano
externally validated a diagnostic model in a large
registry with reasonable discrimination, but many
patients had to be excluded because no polymerase
chain reaction (pcr) testing was performed.81
one study presented a small external validation
(27 participants) that reported satisfactory predictive
performance of a model originally developed for
avian influenza h7n9 pneumonia. however, patients
who had not recovered at the end of the study period
were excluded, which again led to a selection bias.23
another study was a small scale external validation
study (78 participants) of an existing severity score for
lung computed tomography images with satisfactory
reported discrimination.54 three studies validated
existing early warning or severity scores to predict
in-hospital mortality or deterioration.85 96 108 they
had satisfactory discrimination but less than the
recommended number of events for validation137 138 or
unclear sample sizes, excluded patients who remained
in hospital at the end of the study period, or had an
unclear study design.
discussion
in this systematic review of prediction models related
to the covid-19 pandemic, we identified and critically
appraised 107 studies that described 145 models.
study; setting; and outcome predictors in final model
sample size: total
no of participants for
model development
set (no with outcome)
predictive performance on validation
overall risk
of bias using
probast
type of
validation*
sample size: total
no of participants
for model validation
(no with outcome)
performance* (c index,
sensitivity (%), specificity (%),
ppv/npv (%), calibration slope,
other (95% ci, if reported))
rehman et al106; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable 640 (160) training test split 160 (40) sensitivity 82, specificity 93,
ppv 96
high
singh et al107; data from unknown
origin, target population unclear;
covid-19 diagnosis
not applicable unknown twentyfold cross
validation
not applicable sensitivity 91, specificity 89 high
ucar et al111; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable unknown training test split unknown sensitivity 100, specificity 100,
ppv 99
high
wu et al115; data from unknown origin,
target population unclear; covid-19
diagnosis
not applicable 300 (150) training test split 400 (200) sensitivity 95 (91 to 98),
specificity 93 (89 to 97)
high
table 2 | continued
chd=coronary heart disease; copd=chronic obstructive pulmonary disease; covid-19=coronavirus disease 2019; crp=c reactive protein; ct=computed tomography; ggo=ground glass opacities; npv=negative predictive value; ppv=positive predictive
value; probast=prediction model risk of bias assessment tool; pcr=polymerase chain reaction; wbc=white blood cells.
*performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is reported. apparent performance is the performance observed in the
development data.
‡calibration plot presented, but unclear which data were used.
§the development set contains scans from chinese patients, the testing set contains scans from chinese cases and controls, and us controls.
¶data contain mixed cases and controls. chinese data and controls from us and switzerland.
**describes similarity between segmentation of the ct scan by a medical doctor and automated segmentation.
§§pearson correlation between the predicted and ground truth scores for patients with lung abnormalities.
¶¶performance similar for models with different non-cases (healthy, bacterial pneumonia, and viral pneumonia).
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported)) prognosis original review bai et al9; data from china, inpatients at admission with mild confirmed covid-19; deterioration into severe/critical disease (period unspecified) combination of demographics, signs and symptoms, laboratory results and features derived from ct images 133 (54) unclear not applicable c index 0.95 (0.94 to 0.97) high
caramelo et al19; data from china, target population
unclear; mortality (period unspecified)††
age, sex, presence of any comorbidity (hypertension,
diabetes, cardiovascular disease, chronic respiratory
disease, cancer)††
unknown not reported not applicable not reported high
lu et al20; data from china, inpatients at admission
with suspected or confirmed covid-19; mortality
(within 12 days)
age, crp 577 (44) not reported not applicable not reported high
qi et al21; data from china, inpatients with confirmed
covid-19 at admission; hospital stay >10 days
6 features derived from ct images‡‡ (logistic
regression model)
26 (20) fivefold cross
validation
not applicable c index 0.92 high
qi et al21; data from china, inpatients with confirmed
covid-19 at admission; hospital stay >10 days
6 features derived from ct images‡‡ (random forest) 26 (20) 5 fold cross
validation
not applicable c index 0.96 high
shi et al37; data from china, inpatients with
confirmed covid-19 at admission; death or severe
covid-19 (period unspecified)
age (dichotomised), sex, hypertension 478 (49) validation in less
severe cases
66 (15) not reported high
xie et al7; data from china, inpatients with confirmed
covid-19 at admission; mortality (in hospital)
age, ldh, lymphocyte count, spo2 299 (155) external validation
(other chinese
centre)
130 (69) c index 0.98 (0.96 to 1.00),
calibration slope 2.5 (1.7 to
3.7)
high
yan et al22; data from china, inpatients suspected of
covid-19; mortality (period unspecified)
ldh, lymphocyte count, high sensitivity crp 375 (174) temporal
validation, selecting
only severe cases
29 (17) sensitivity 92, ppv 95 high
yuan et al23; data from china, inpatients with
confirmed covid-19 at admission; mortality (period
unspecified)
clinical scorings of ct images (zone, left/right,
location, attenuation, distribution of affected
parenchyma)
not applicable external validation
of existing model
27 (10) c index 0.90 (0.87 to 0.93) high
update 1
huang et al60; data from china, inpatients with
confirmed covid-19 at admission; severe symptoms
three days after admission
underlying diseases, fast respiratory rate >24/min,
elevated crp level (>10 mg/dl), elevated ldh level
(>250 u/l)
125 (32) apparent
performance only
not applicable c index 0.99 (0.97 to 1.00),
sensitivity 91, specificity 96
high
pourhomayoun et al61; data from 76 countries,
inpatients with confirmed covid-19; in-hospital
mortality (period unspecified)
unknown unknown 10-fold cross
validation
not applicable c index 0.96, sensitivity 90,
specificity 97
high
sarkar et al44; data from several continents
(australia, asia, europe, north america), inpatients
with covid-19 symptoms; death v recovery (period
unspecified)
age, days from symptom onset to hospitalisation,
from wuhan, sex, visit to wuhan
80 (37) apparent
performance only
not applicable c index 0.97 high
wang et al42; data from china, inpatients with
confirmed covid-19; length of hospital stay
age and ct features 301 (not
applicable)
not reported not applicable not reported high
zeng et al62; data from china, inpatients with
confirmed covid-19; severe disease progression
(period unspecified)
ct features 338 (76) cross validation
(number of folds
unclear)
not applicable c index 0.88 high
zeng et al62; data from china, inpatients with
confirmed covid-19; severe disease progression
(period unspecified)
ct features and laboratory markers 338 (76) cross validation
(number of folds
unclear)
not applicable c index 0.88 high
update 2
al-najjar et al63; data from south korea, target
population unclear; recovery from covid-19 (period
unspecified)
birth year (age), sex, country, group, infection reason,
confirmed date
466 (40) training test split 193 (14) sensitivity 43, specificity 98 high
table 3 | overview of prediction models for prognosis of covid-19
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported))
al-najjar et al63; data from south korea, target
population unclear; mortality (period unspecified)
age, sex, country, region, infection reason, confirmed
date
463 (25) training test split 191 (7) sensitivity 86, specificity 100 high
barda et al67; data from israel, patients with
confiremed covid-19; mortality (period unspecified)
age, sex, pack years, copd, number of wheezing/
dyspnea diagnoses, albumin, red cell distribution
width, c reactive peptide, urea, lymphocyte, chloride,
creatinine, high density lipoprotein, duration of
hospital admissions, count of hospital admissions,
count of ambulance rides, count of sulfonamide
dispenses, count of anticholinergic dispenses, count
of glucocorticoid dispenses, chronic respiratory
disease, cardiovascular disease, diabetes, malignancy,
hypertension
735,000 (8251) other (specify in
column cl) january
29 to april 8
inclusion, follow-up
until april 22 2020
(covid cases for
external validation)
3176 (87) c index 0.94 (0.92 to 0.96) ,
sensitivity 90 (83 to 96), ppv
17 (14 to 21)
high
bello-chavolla et al70; data from mexico, confirmed
covid-19 patients presenting at gp; 30-day mortality
age, pregnancy, diabetes, obesity, pneumonia, ckd,
copd, immunosuppression
12424 (1137) training test split 3105 (297) c index 0.80, somer’s d 0.60 high
carr et al75; data from united kingdom, inpatients
with confirmed covid-19; progression to severe
covid-19 (period unspecified)
age, national early warning score (news) 2, crp,
neutrophil, egfr, albumin
452 (159) temporal validation 256 (59) c index 0.73, sensitivity 46,
specificity 87
high
chassagnon et al77; data from france, inpatients with
confirmed covid-19; composite, 4-day intubation or
mortality
unclear 383 (84) external validation
(new centres,
france)
95 (26) sensitivity 88, specificity 74 high
colombi et al79; data from italy, inpatients with
confirmed covid-19; icu admission or in-hospital
(period unspecified)
age, cardiovascular comorbidities, median platelet
count, crp, visual assessment of well aerated lung %
236 (108) apparent
performance only
not applicable c index 0.86 (0.81 to 0.90),
sensitivity 72 (63 to 80),
specificity 81 (73 to 88) ppv 70
(61 to 78), npv 78 (72 to 83)
high
colombi et al79; data from italy, inpatients with
confirmed covid-19; icu admission or in-hospital
mortality (period unspecified)
age, cardiovascular comorbidities, median platelet
count, ldh, crp, software assessment of well aerated
lung absolute volume, adipose tissue
236 (108) apparent
performance only
not applicable c index 0.86 (0.81 to 0.90),
sensitivity 75 (66 to 83),
specificity 81 (73 to 88), ppv
70 (61 to 78), npv 78 (72 to
83)
high
das et al80; data from south korea, inpatients with
confirmed covid-19; icu admission or in-hospital
mortality (period unspecified)
age, sex, province, date of diagnosis, place of
exposure to covid-19
3022 (61) training test split 604 (12) c index 0.97 high
gong et al119; data from china, target population
unclear; 15-day progression to severe covid-19
age, direct bilirubin, red cell distribution width, blood
urea nitrogen, crp, lactate dehydrogenase, albumin
189 (28) external validation
(new centres,
china)
165 (40) c index 0.85 (0.79 to 0.92),
sensitivity 78, specificity 78
high
guo et al83; data from china, inpatients with
confirmed covid-19; 14-day progression to severe
covid-19
age, chronic illness, neutrophil to lymphocyte ratio,
crp, d-dimer
818 (24) external validation
(new centres,
china)
320 (38) c index 0.78 (0.70 to 0.87) high
hu et al84; data from china inpatients with confirmed
covid-19;in-hospital mortality (period unspecified)
age, high-sensitivity crp, lymphocyte count, d-dimer 183 (68) external validation
(new centres,
china)
64 (31) c index 0.88, sensitivity 84,
specificity 79
high
hu et al85; data from china, inpatients with
confirmed covid-19; in-hospital mortality (period
unspecified)
modified early warning score (mews): heart rate,
systolic blood pressure, respiratory rate, body
temperature, consciousness
not applicable external validation
only
105 (19) c index 0.68 (0.58 to 0.77),
sensitivity 68, specificity 65,
ppv 30, npv 90
high
hu et al85; data from china, inpatients with
confirmed covid-19; in-hospital mortality (period
unspecified)
rapid emergency medicine score (rems): mean
arterial pressure, pulse rate, respiratory rate, oxygen
saturation, gcs, age
not applicable external validation
only
105 (19) c index 0.84 (0.76 to 0.91),
sensitivity 89, specificity 70,
ppv 40, npv 97
high
 ji et al88; data from china, inpatients with confirmed
covid-19; 10-day progression to severe covid-19
comorbidity, age, lymphocyte count, lactate
dehydrogenase
208 (40) internal validation
by resampling
(bootstrap)
not applicable c index 0.91 (0.86 to 0.94),
sensitivity 95 (83 to 99),
specificity 78 (71 to 84)
high
table 3 | continued
(continued)
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported)) jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 50% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 80% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 70% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 70% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 70% high jiang et al89; data from china, inpatients with confirmed covid-19; acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count 53 (5) 10-fold cross validation not applicable classification accuracy 80% high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) age, serum blood urea nitrogen, emergency severity index, red cell distribution width, absolute neutrophil count, serum bicarbonate, glucose unknown leave-one-out cross validation not applicable c index 0.83 high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) sofa score not applicable external validation only unclear c index 0.73 high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) curb-65 score not applicable external validation only unclear c index 0.74 high levy et al96; data from usa, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) sofa+ score not applicable external validation only unclear c index 0.83 high liu et al98; data from china, inpatients with confirmed covid-19; in-hospital mortality (period unspecified) age, underlying disease status, helper t cells, helper t cells and suppressor t cells ratio 340 (30) apparent performance only not applicable mcfadden pseudo r-squared 0.35 high
mcrae et al100; data from china, inpatients with
confirmed covid-19; in-hospital mortality (period
unspecified)
age, sex, cardiac troponin i, crp, procalcitonin,
myoglobin
160 (43) new centres in
china, case series
12 (unknown) c index 0.94 (0.89 to 0.99) high
singh et al108; data from usa, inpatients with
confirmed covid-19; icu-level care, mechanical
ventilation or in-hospital mortality (period
unspecified)
epic deterioration index unknown external validation
only
174 (61) c index 0.76 (0.68 to 0.84),
sensitivity 39 ppv 80
high
table 3 | continued
final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other (95% ci, if reported))
vaid et al112; data from usa, inpatients with
confirmed covid-19; intubation, discharge to hospice
care or mortalit (period unspecified)
sex, race, ethnicity, age, hypertension, atrial
fibrillation, coronary artery disease, heart failure,
stroke, chronic kidney disease, diabetes, asthma,
copd, cancer, heart rate, pulse, oximetry, respiration
rate, temperature, systolic blood pressure, diastolic
blood pressure, body weight, sodium, potassium,
creatinine, lactate, white blood cells, lymphocyte
percentage, haemoglobin, red blood cell distribution
width, platelets, alanine, aminotransferase,
aspartate, aminotransferase, albumin, total bilirubin,
prothrombin time, partial thromboplastin time, pco2,
ph, crp, ferritin, d-dimer, creatinine phosphokinase,
lactate dehydrogenase, procalcitonin, troponin i
1225 (37) external validation,
new centres (usa)
1830 (unknown) c index 0.84, sensitivity 86,
specificity 82
high
vazquez guillamet et al113; data from usa, target
population unclear; in-hospital mortality (period
unspecified)
age, immunosuppression, copd, congestive heart
failure, bmi, sex, time to mechanical ventilation
(days), length of hospital stay prior to hospital
admission, pao2/fio2, glasgow coma scale, maximum
heart rate, maximum respiratory rate, minimum mean
arterial blood pressure, maximum temperature,
minimum albumin, minimum ph
2122 (429) external validation,
new centres (usa)
1175 (154) c index 0.81, ppv 55, npv 89 high
vazquez guillamet et al113; data from usa, target
population unclear; mechanical ventilation >96 hours
age, immunosuppression, copd, congestive heart
failure, bmi, sex, time to mechanical ventilation
(days), length of hospital stay before hospital
admission, pao2/fio2, glasgow coma scale, maximum
heart rate, maximum respiratory rate, minimum mean
arterial blood pressure, maximum temperature,
minimum albumin, minimum ph
2167 (158) training test split 1063 (96) c index 0.81 high
vazquez guillamet et al113; data from usa, target
population unclear; mechanical ventilation >96 hours
age, immunosuppression, copd, congestive heart
failure, bmi, sex, time to mechanical ventilation
(days), length of hospital stay prior to hospital
admission, pao2/fio2, glasgow coma scale, maximum
heart rate, maximum respiratory rate, minimum mean
arterial blood pressure, maximum temperature,
minimum albumin, minimum ph
1169 (141) training test split 619 (90) c index 0.78 high
zhang et al116; data from china and united kingdom,
inpatients with confirmed covid-19; in hospital
mortality (period unspecified)
age, sex, neutrophil count, lymphocyte count, platelet
count, crp, creatinine
653 (20) external validation
(new centres,
different country)
226 (77) c index 0.75, sensitivity 23,
specificity 95, ppv 69, npv 71
high
zhang et al116; data from china, inpatients with
confirmed covid-19; ards, intubation or ecmo, icu
admission, in hospital mortality (period unspecified)
age, sex, chronic lung disease, diabetes mellitus,
malignancy, cough, dyspnoea, immunocompromised,
hypertension, heart disease, chronic renal disease,
fever, fatigue, diarrhoea
768 (72) repeated five-fold
cross validation
not applicable c index 0.80, sensitivity 9,
specificity 99 ppv 53, npv 91
high
zhang et al116; data from china and united kingdom,
inpatients with confirmed covid-19; ards, intubation
or ecmo, icu admission, in hospital mortality (period
unspecified)
age, sex, neutrophil count, lymphocyte count, platelet
count, crp, creatinine
653 (58) external validation
(new centres,
different country)
226 (97) c index 0.72, sensitivity 40,
specificity 85, ppv 67, npv 65
high
ards=acute respiratory distress syndrome; bmi=body mass index; copd=chronic obstructive pulmonary disease; covid-19=coronavirus disease 2019; crp=c reactive protein; ct=computed tomography; ecmo=extracorporal membrane oxygenation;
icu=intensive care unit; ldh=lactate dehydrogenase; npv=negative predictive value; pao2/fio2=the ratio of arterial oxygen partial pressure to fractional inspired oxygen; pco2=partial pressure of carbon dioxide; ppv=positive predictive value;
probast=prediction model risk of bias assessment tool; sofa=sequential organ failure assessment score; spo2=oxygen saturation; na+=sodium; k+=potassium.
table 3 | continued
*performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is reported. apparent performance is the performance observed in the
development data.
††outcome and predictor data were simulated.
‡‡wavelet-hlh_gldm_smalldependencelowgraylevelemphasis, wavelet-lhh_glcm_correlation, wavelet-lhl_glszm_graylevelvariance, wavelet-llh_glszm_sizezonenonuniformitynormalized, wavelet-llh_glszm_smallareaemphasis, wavelet-llh_
glcm_correlation.
***each model uses a different predictive algorithm.
these prediction models can be divided into three
categories: models for the general population to
predict the risk of having covid-19 or being admitted to
hospital for covid-19; models to support the diagnosis
of covid-19 in patients with suspected infection; and
models to support the prognostication of patients with
covid-19. all models reported moderate to excellent
predictive performance, but all were appraised to
have high risk of bias owing to a combination of
poor reporting and poor methodological conduct
for participant selection, predictor description, and
statistical methods used. models were developed
on data from different countries, but the majority
used data from china or public international data
repositories. with few exceptions, the available sample
sizes and number of events for the outcomes of interest
were limited. this is a well known problem when
building prediction models and increases the risk of
overfitting the model.139 a high risk of bias implies
that the performance of these models in new samples
will probably be worse than that reported by the
researchers. therefore, the estimated c indices, often
close to 1 and indicating near perfect discrimination,
are probably optimistic. the majority of studies
developed new models, only 27 carried out an external
validation, and calibration was rarely assessed.
we reviewed 57 studies that used advanced
machine learning methodology on medical images to
diagnose covid-19, covid-19 related pneumonia, or to
assist in segmentation of lung images. the predictive
performance measures showed a high to almost perfect
ability to identify covid-19, although these models and
their evaluations also had a high risk of bias, notably
because of poor reporting and an artificial mix of
patients with and without covid-19. therefore, we do
not recommend any of the 145 identified prediction
models to be used in practice.
challenges and opportunities
the main aim of prediction models is to support
medical decision making. therefore, it is vital to
identify a target population in which predictions
serve a clinical need, and a representative dataset
(preferably comprising consecutive patients) on which
the prediction model can be developed and validated.
this target population must also be carefully described
so that the performance of the developed or validated
model can be appraised in context, and users know
which people the model applies to when making
predictions. unfortunately, the studies included in our
systematic review often lacked an adequate description
of the study population, which leaves users of these
models in doubt about the models’ applicability.
although we recognise that all studies were done
under severe time constraints, we recommend that
any studies currently in preprint and all future studies
should adhere to the tripod reporting guideline16 to
improve the description of their study population and
their modelling choices. tripod translations (eg, in
chinese and japanese) are also available at https://
www.tripod-statement.org.
authors
risk of bias
participants predictors outcome analysis
hospital admission in general population
original review
decaprio et al8 high low high high
update 2
jiang et al90 high unclear high high
diagnosis
original review
feng et al10 low unclear high high
lopez-rincon et al35 unclear low low high
meng et al12 high low high high
song et al31 high unclear low high
update 1
martin et al41 high high high high
sun et al40 low low unclear high
wang et al43 low unclear unclear high
wu et al45 high unclear low high
update 2
batista et al69 unclear unclear low high
brinati et al74 unclear unclear low high
chen et al78 high high low high
diaz-quijano et al81 high high low high
kurstjens et al95 unclear low high high
mei et al101 high unclear unclear high
menni et al102 high unclear unclear high
soares et al109 unclear unclear low high
tordjman et al110 low unclear unclear high
zhao et al117 high high unclear high
diagnosis of severity
original review
yu et al25 unclear unclear unclear high
update 1
zhou et al46 unclear low high high
update 2
benchoufi et al71 high low low high
chassagnon et al77 low low low high
li et al97 unclear unclear unclear high
lyu et al99 low unclear unclear high
wang et al114 unclear high low high
zhu et al118 low low high high
diagnostic imaging
original review
barstugan et al32 unclear unclear unclear high
chen et al27 high unclear low high*
gozes et al26 unclear unclear high high
jin et al11 high unclear unclear high†
jin et al33 high unclear high high*
li et al34 low unclear low high
shan et al29 unclear unclear high high†
shi et al36 high unclear low high
wang et al30 high unclear low high
xu et al28 high unclear high high
song et al24 unclear unclear low high
zheng et al38 unclear unclear high high
update 1
abbas et al47 high unclear unclear high
apostolopoulos et al48 high unclear high high
bukhari et al49 unclear unclear unclear high
chaganti et al50 high unclear low unclear
chowdhury et al39 high unclear unclear high
fu et al51 high unclear unclear high
gozes et al52 high unclear unclear high
imran et al53 high unclear unclear high*
li et al54 low low unclear high
li et al55 high unclear high high*
hassanien et al56 unclear unclear unclear high*
tang et al57 unclear unclear high high
table 4 | risk of bias assessment (using probast) based on four domains across 107
studies that created prediction models for coronavirus disease 2019
a better description of the study population could
also help us understand the observed variability
in the reported outcomes across studies, such as
covid-19 related mortality and covid-19 prevalence.
the variability in prevalence could in part be reflective
of different diagnostic standards across studies. note
that the majority of diagnostic models use viral nucleic
acid test results as the gold standard, which may have
unacceptable false negative rates.
covid-19 prediction problems will often not present
as a simple binary classification task. complexities
in the data should be handled appropriately. for
example, a prediction horizon should be specified for
prognostic outcomes (eg, 30 day mortality). if study
participants have neither recovered nor died within
that time period, their data should not be excluded
from analysis, which most reviewed studies have done.
instead, an appropriate time to event analysis should
be considered to allow for administrative censoring.17
censoring for other reasons, for instance because of
quick recovery and loss to follow-up of patients who
are no longer at risk of death from covid-19, could
necessitate analysis in a competing risk framework.140
a prediction model applied in a new healthcare
setting or country often produces predictions that
are miscalibrated141 and might need to be updated
before it can safely be applied in that new setting.17
this requires data from patients with covid-19 to be
available from that system. instead of developing and
updating predictions in their local setting, individual
participant data from multiple countries and healthcare
systems might allow better understanding of the
generalisability and implementation of prediction
models across different settings and populations. this
approach could greatly improve the applicability and
robustness of prediction models in routine care.142-146
the evidence base for the development and validation
of prediction models related to covid-19 will quickly
increase over the coming months. together with the
increasing evidence from predictor finding studies147-153
and open peer review initiatives for covid-19
related publications,154 data registries120 121 155-157
are being set up. to maximise the new opportunities
and to facilitate individual participant data metaanalyses, the world health organization has released a
new data platform to encourage sharing of anonymised
covid-19 clinical data.158 to leverage the full potential
of these evolutions, international and interdisciplinary
collaboration in terms of data acquisition, model
building and validation is crucial.
study limitations
with new publications on covid-19 related prediction
models rapidly entering the medical literature, this
systematic review cannot be viewed as an up-to-date list of
all currently available covid-19 related prediction models.
also, 87 of the studies we reviewed were only available
as preprints. these studies might improve after peer
review, when they enter the official medical literature; we
will reassess these peer reviewed publications in future
updates. we also found other prediction models that are
authors
risk of bias
participants predictors outcome analysis
wang et al42 low unclear unclear high
zhang et al58 high unclear high high
zhou et al59 high unclear high high*
update 2
angelov et al64 high unclear high high
arpan et al65 unclear unclear unclear high
bai et al66 high unclear high high
bassi et al68 high unclear high high
borghesi et al72 high unclear unclear high
born et al73 high unclear unclear high
castiglioni et al76 unclear unclear low high
guiot et al82 high unclear low high
hu et al86 high unclear high high
islam et al87 high unclear high high
kana et al91 high unclear high high*
karim et al92 high unclear high high
khan et al93 high unclear high high*
kumar et al94 high unclear unclear high*
moutounet-cartan103 unclear unclear unclear high
ozturk et al104 high unclear unclear high
rahimzadeh et al105 high unclear unclear high
rehman et al106 high unclear unclear high
singh et al107 high unclear unclear high
ucar et al107 high unclear unclear high
wu et al115 high unclear unclear high
prognosis
original review
bai et al9 low unclear unclear high
caramelo et al19 high high high high
lu et al20 low low low high
qi et al21 unclear low low high
shi et al37 high high high high
xie et al7 low low low high
yan et al22 low high low high
yuan et al23 low high low high
update 1
huang et al60 unclear unclear unclear high
pourhomayoun et al61 low low unclear high
sarkar et al44 high high high high
wang et al42 low low low high
zeng et al62 low low low high
update 2
al-najjar et al63 unclear unclear unclear high
barda et al67 low low high high
bello-chavolla et al70 unclear unclear low high
carr et al75 low low low high
chassagnon et al77 low low low high
colombi et al79 high unclear unclear high
das et al80 low low low high
gong et al119 low low high high
guo et al83 low high unclear high
hu et al84 high low low high
hu et al85 low unclear low high
ji et al88 low low low high
jiang et al89 unclear unclear unclear high
levy et al96 low low low high
liu et al98 low low low high
mcrae et al100 high high high high
singh et al108 low unclear high high
vaid et al112 unclear high high high
vazquez guillamet et al113 high low unclear high
zhang et al116 low unclear unclear/low‡ high
probast=prediction model risk of bias assessment tool.
*risk of bias high owing to calibration not being evaluated. if this criterion is not taken into account, analysis risk
of bias would have been unclear.
†risk of bias high owing to calibration not being evaluated. if this criterion is not taken into account, analysis risk
of bias would have been low.
‡zhang et al evaluated two outcomes: death (low risk of bias) and a composite poor outcome (unclear risk of
bias).
table 4 | continued
currently being used in clinical practice without scientific
publications,159 and web risk calculators launched for
use while the scientific manuscript is still under review
(and unavailable on request). these unpublished models
naturally fall outside the scope of this review of the
literature.160 as we have argued extensively elsewhere,161
transparent reporting that enables validation by
independent researchers is key for predictive analytics,
and clinical guidelines should only recommend publicly
available and verifiable algorithms.
implications for practice
all 145 reviewed prediction models were found to have
a high risk of bias, and evidence from independent
external validation of the newly developed models is
currently lacking. however, the urgency of diagnostic
and prognostic models to assist in quick and efficient
triage of patients in the covid-19 pandemic might
encourage clinicians and policymakers to prematurely
implement prediction models without sufficient
documentation and validation. earlier studies have
shown that models were of limited use in the context of
a pandemic,162 and they could even cause more harm
than good.163 therefore, we cannot recommend any
model for use in practice at this point.
the current oversupply of insufficiently validated
models is not useful for clinical practice. future studies
should focus on validating, comparing, improving,
and updating promising available prediction models,
rather than developing new ones.17 for example,
diaz-quijano developed and externally validated a
diagnostic model using brazilian surveillance data
with reasonable discrimination, but many patients had
to be excluded because no pcr testing was performed,
hence this model needs further validation.17 two other
models to diagnose covid-19 also showed promising
discrimination at external validation in small
unselected cohorts.43 110 an externally validated model
that used computed tomography based total severity
scores showed good discrimination between patients
with mild, common, and severe-critical disease.54
two models to predict progression to severe covid-19
within two weeks showed promising discrimination
when validated externally on unselected cohorts.83 119
another model discriminated well between survivors
and non-survivors among confirmed cases, but the
prediction horizon was not specified, and the study had
many missing values for key parameters.67 because
reporting in each of these studies was insufficiently
detailed and the validation was in datasets with fewer
than 100 events in the smallest outcome category,
validation in larger, international datasets is needed.
such external validations should assess not only
discrimination, but also calibration and clinical utility
(net benefit).141 146 163 owing to differences between
healthcare systems (eg, chinese and european) in
when patients are admitted to and discharged from
hospital, as well as the testing criteria for patients
with suspected covid-19, we anticipate most existing
models will be miscalibrated, but this can usually be
solved by updating and adjustment to the local setting.
when creating a new prediction model, we
recommend building on previous literature and expert
opinion to select predictors, rather than selecting
predictors in a purely data driven way.17 this is especially
important for datasets with limited sample size.164
based on the predictors included in multiple models
identified by our review, we encourage researchers to
consider incorporating several candidate predictors.
common predictors include age, body temperature,
lymphocyte count, and lung imaging features. flulike signs and symptoms and neutrophil count are
frequently predictive in diagnostic models, while
comorbidities, sex, c reactive protein, and creatinine
are frequently reported prognostic factors. by pointing
to the most important methodological challenges and
issues in design and reporting of the currently available
models, we hope to have provided a useful starting
point for further studies aiming to develop new models,
or to validate and update existing ones.
this living systematic review has been conducted in
collaboration with the cochrane prognosis methods
group. we will update this review and appraisal
continuously to provide up-to-date information for
healthcare decision makers and professionals as more
international research emerges over time.
box 1: availability of models in format for use in clinical practice
several studies presented their models in a format for use in clinical practice.
however, because all models were at high risk of bias, we do not recommend their
routine use before they are properly externally validated.
models to predict risk of developing coronavirus disease 2019 (covid-19) or of hospital
admission for covid-19 in general population
the “covid-19 vulnerability index” to detect hospital admission for covid-19
pneumonia from other respiratory infections (eg, pneumonia, influenza) is available
as an online tool.8122
diagnostic models
several sum scores,31 95 110 117 and model equations81 102 are available to support the
diagnosis. graphical diagnostic aids include nomograms43 78 117 and a decision tree.74
the “covid-19 diagnosis aid” app is available on ios and android devices to diagnose
covid-19 in asymptomatic patients and those with suspected disease.12 additionally,
online tools are available.10 45 74 95 123-125 classification in terms of disease severity can
be done using a published equation.114 a decision tree to detect severe disease for
paediatric patients with confirmed covid-19 is also available in an article.25
diagnostic models based on images
five artificial intelligence models to assist with diagnosis based on medical images
are available through web applications.24 27 30 73 91 126-130 one model is deployed in 16
hospitals, but the authors do not provide any usable tools in their study.33 two papers
includes a severity scoring system to classify patients based on images.5472
prognostic models
to assist in the prognosis of mortality, a nomogram,7
 a decision tree,22 a score
system,70 online tools,80 84 96 98 131-134 and a computed tomography based scoring rule
are available in the articles.23 other online tools predict in-hospital death and the
need for prolonged mechanical ventilation,113 135 or in-hospital death and a composite
of poor outcomes.116 136 additionally nomograms,88 119 sumscores83 88 and a model
equation60 are available to predict progression to severe covid-19.
several studies made their code available on github.8 11 34 35 38 47 55 65-68 70 73 86 92 98 101
104 105 109 seventy four studies did not include any usable equation, format, code, or
reference for use or validation of their prediction model.
conclusion
several diagnostic and prognostic models for covid-19
are currently available and they all report moderate
to excellent discrimination. however, these models
are all at high risk of bias, mainly because of nonrepresentative selection of control patients, exclusion
of patients who had not experienced the event of
interest by the end of the study, and model overfitting.
therefore, their performance estimates are probably
optimistic and misleading. the covid-precise group
does not recommend any of the current prediction
models to be used in practice. future studies aimed
at developing and validating diagnostic or prognostic
models for covid-19 should explicitly address the
concerns raised. sharing data and expertise for the
validation and updating of covid-19 related prediction
models is urgently needed.

<|EndOfText|>

artificial intelligence versus clinicians: systematic review of
design, reporting standards, and claims of deep learning studies
abstract
objective
to systematically examine the design, reporting
standards, risk of bias, and claims of studies
comparing the performance of diagnostic deep
learning algorithms for medical imaging with that of
expert clinicians.
design
systematic review.
data sources
medline, embase, cochrane central register of
controlled trials, and the world health organization
trial registry from 2010 to june 2019.
eligibility criteria for selecting studies
randomised trial registrations and non-randomised
studies comparing the performance of a deep
learning algorithm in medical imaging with a
contemporary group of one or more expert clinicians.
medical imaging has seen a growing interest in deep
learning research. the main distinguishing feature
of convolutional neural networks (cnns) in deep
learning is that when cnns are fed with raw data,
they develop their own representations needed
for pattern recognition. the algorithm learns for
itself the features of an image that are important
for classification rather than being told by humans
which features to use. the selected studies aimed
to use medical imaging for predicting absolute risk
of existing disease or classification into diagnostic
groups (eg, disease or non-disease). for example,
raw chest radiographs tagged with a label such as
pneumothorax or no pneumothorax and the cnn
learning which pixel patterns suggest pneumothorax.
review methods
adherence to reporting standards was assessed
by using consort (consolidated standards of
reporting trials) for randomised studies and tripod
(transparent reporting of a multivariable prediction
model for individual prognosis or diagnosis) for nonrandomised studies. risk of bias was assessed by
using the cochrane risk of bias tool for randomised
studies and probast (prediction model risk of bias
assessment tool) for non-randomised studies.
results
only 10 records were found for deep learning
randomised clinical trials, two of which have been
published (with low risk of bias, except for lack of
blinding, and high adherence to reporting standards)
and eight are ongoing. of 81 non-randomised clinical
trials identified, only nine were prospective and
just six were tested in a real world clinical setting.
the median number of experts in the comparator
group was only four (interquartile range 2-9).
full access to all datasets and code was severely
limited (unavailable in 95% and 93% of studies,
respectively). the overall risk of bias was high in 58
of 81 studies and adherence to reporting standards
was suboptimal (<50% adherence for 12 of 29 tripod
items). 61 of 81 studies stated in their abstract that
performance of artificial intelligence was at least
comparable to (or better than) that of clinicians. only
31 of 81 studies (38%) stated that further prospective
studies or trials were required.
conclusions
few prospective deep learning studies and
randomised trials exist in medical imaging. most nonrandomised trials are not prospective, are at high risk
of bias, and deviate from existing reporting standards.
data and code availability are lacking in most studies,
and human comparator groups are often small.
future studies should diminish risk of bias, enhance
real world clinical relevance, improve reporting and
transparency, and appropriately temper conclusions.
study registration
prospero crd42019123605.
introduction
the digitisation of society means we are amassing
data at an unprecedented rate. healthcare is no
exception, with ibm estimating approximately one
million gigabytes accruing over an average person’s
lifetime and the overall volume of global healthcare
data doubling every few years.1
 to make sense of these
big data, clinicians are increasingly collaborating with
computer scientists and other allied disciplines to
for numbered affiliations see
end of the article.
what is already known on this topic
the volume of published research on deep learning, a branch of artificial
intelligence (ai), is rapidly growing
media headlines that claim superior performance to doctors have fuelled hype
among the public and press for accelerated implementation
what this study adds
few prospective deep learning studies and randomised trials exist in medical
imaging
most non-randomised trials are not prospective, are at high risk of bias, and
deviate from existing reporting standards
data and code availability are lacking in most studies, and human comparator
groups are often small
future studies should diminish risk of bias, enhance real world clinical relevance,
improve reporting and transparency, and appropriately temper conclusions
make use of artificial intelligence (ai) techniques that
can help detect signal from noise.2
 a recent forecast
has placed the value of the healthcare ai market as
growing from $2bn (£1.5bn; €1.8bn) in 2018 to $36bn
by 2025, with a 50% compound annual growth rate.3
deep learning is a subset of ai which is formally
defined as “computational models that are composed
of multiple processing layers to learn representations of
data with multiple levels of abstraction.”4
 in practice,
the main distinguishing feature between convolutional
neural networks (cnns) in deep learning and traditional
machine learning is that when cnns are fed with raw
data, they develop their own representations needed
for pattern recognition; they do not require domain
expertise to structure the data and design feature
extractors.5
 in plain language, the algorithm learns
for itself the features of an image that are important
for classification rather than being told by humans
which features to use. a typical example would be
feeding in raw chest radiographs tagged with a label
such as either pneumothorax or no pneumothorax
and the cnn learning which pixel patterns suggest
pneumothorax. fields such as medical imaging have
seen a growing interest in deep learning research,
with more and more studies being published.6
 some
media headlines that claim superior performance to
doctors have fuelled hype among the public and press
for accelerated implementation. examples include:
“google says its ai can spot lung cancer a year before
doctors” and “ai is better at diagnosing skin cancer
than your doctor, study finds.”7 8
the methods and risk of bias of studies behind such
headlines have not been examined in detail. the danger
is that public and commercial appetite for healthcare ai
outpaces the development of a rigorous evidence base
to support this comparatively young field. ideally, the
path to implementation would involve two key steps.
firstly, well conducted and well reported development
and validation studies that describe an algorithm and
its properties in detail, including predictive accuracy
in the target setting. secondly, well conducted and
transparently reported randomised clinical trials that
evaluate usefulness in the real world. both steps are
important to ensure clinical practice is determined
based on the best evidence standards.9-12
our systematic review seeks to give a contemporary
overview of the current standards of deep learning
research for clinical applications. specifically, we
sought to describe the study characteristics, and
evaluate the methods and quality of reporting and
transparency of deep learning studies that compare
diagnostic algorithm performance with human
clinicians. we aim to suggest how we can move forward
in a way that encourages innovation while avoiding
hype, diminishing research waste, and protecting
patients.
methods
the protocol for this study was registered in the
online prospero database (crd42019123605)
before search execution. the supplementary appendix
gives details of any deviations from the protocol.
this manuscript has been prepared according to the
prisma (preferred reporting items for systematic
reviews and meta-analyses) guidelines and a checklist
is available in the supplementary appendix.13
study identification and inclusion criteria
we performed a comprehensive search by using free
text terms for various forms of the keywords “deep
learning” and “clinician” to identify eligible studies.
appendix 1 presents the exact search strategy. several
electronic databases were searched from 2010 to june
2019: medline, embase, cochrane central register of
controlled trials (central), and the world health
organization international clinical trials registry
platform (who-ictrp) search portal. additional
articles were retrieved by manually scrutinising the
reference lists of relevant publications.
we selected publications for review if they satisfied
several inclusion criteria: a peer reviewed scientific
report of original research; english language; assessed
a deep learning algorithm applied to a clinical problem
in medical imaging; compared algorithm performance
with a contemporary human group not involved in
establishing the ground truth (the true target disease
status verified by best clinical practice); and at least
one human in the group was considered an expert.
we included studies when the aim was to use medical
imaging for predicting absolute risk of existing disease
or classification into diagnostic groups (eg, disease
or non-disease). exclusion criteria included informal
publication types (such as commentaries, letters to the
editor, editorials, meeting abstracts). deep learning
for the purpose of medical imaging was defined as
computational models that are composed of multiple
processing layers to learn representations of data with
multiple levels of abstraction (in practice through a
cnn; see box 1).4
 a clinical problem was defined as a
situation in which a patient would usually see a medical
professional to improve or manage their health (this
did not include segmentation tasks, eg, delineating the
borders of a tumour to calculate tumour volume). an
expert was defined as an appropriately board certified
specialist, attending physician, or equivalent. a real
world clinical environment was defined as a situation
in which the algorithm was embedded into an active
clinical pathway. for example, instead of an algorithm
being fed thousands of chest radiographs from a
database, in a real world implementation it would exist
within the reporting software used by radiologists and
be acting or supporting the radiologists in real time.
study selection and extraction of data
after removal of clearly irrelevant records, four people
(mn, yc, cal, dina radenkovic) independently
screened abstracts for potentially eligible studies so
that each record was reviewed by at least two people.
full text reports were then assessed for eligibility with
disagreements resolved by consensus. at least two
people (mn, yc, cal) extracted data from study reports
independently and in duplicate for each eligible study,
with disagreements resolved by consensus or a third
reviewer.
adherence to reporting standards and risk of bias
we assessed reporting quality of non-randomised
studies against a modified version of the tripod
(transparent reporting of a multivariable prediction model for individual prognosis or diagnosis)
statement.14 this statement aims to improve the transparent reporting of prediction modelling studies of
all types and in all medical settings.15 the tripod
statement consists of a 22 item checklist (37 total points
when all subitems are included), but we considered
some items to be less relevant to deep learning studies
(eg, points that related to predictor variables). deep
learning algorithms can consider multiple predictors;
however, in the cases we assessed, the only predictors
(almost exclusively) were the individual pixels of
the image. the algorithm did not typically receive
information on characteristics such as patient age, sex,
and medical history. therefore, we used a modified list
of 29 total points (see appendix 2). the aim was to
assess whether studies broadly conformed to reporting
recommendations included in tripod, and not the
detailed granularity required for a full assessment of
adherence.16
we assessed risk of bias for non-randomised
studies by applying probast (prediction model
risk of bias assessment tool).17 probast contains
20 signalling questions from four domains (participants, predictors, outcomes, and analysis) to allow
assessment of the risk of bias in predictive modelling
studies.18 we did not assess applicability (because
no specific therapeutic question existed for this
systematic review) or predictor variables (these are
less relevant in deep learning studies on medical
imaging; see appendix 2).
we assessed the broad level reporting of randomised
studies against the consort (consolidated standards
of reporting trials) statement. risk of bias was evaluated
by applying the cochrane risk of bias tool.11 19
data synthesis
we intentionally planned not to conduct formal
quantitative syntheses because of the probable heterogeneity of specialties and outcomes.
patient and public involvement
patients were not involved in any aspect of the study
design, conduct or in the development of the research
question or outcome measures.
results
study selection
our electronic search, which was last updated on
17 june 2019, retrieved 8302 records (7334 study
records and 968 trial registrations; see fig 1). of the
7334 study records, we assessed 140 full text articles;
59 were excluded, which left 81 non-randomised
studies for analysis. of the 968 trial registrations, we
assessed 96 in full; 86 were excluded, which left 10
trial registrations that related to deep learning.
box 1: deep learning in imaging with examples
deep learning is a subset of artificial intelligence that is formally defined as “computational models that are composed
of multiple processing layers to learn representations of data with multiple levels of abstraction.”4
 a deep learning
algorithm consists of a structure referred to as a deep neural network of which a convolutional neural network (cnn)
is one particular type frequently used in imaging. cnns are structurally inspired by the hierarchical arrangement of
neurons within the brain. they can take many nuanced forms but the basic structure consists of an input layer, multiple
hidden layers, and a final output layer. each hidden layer responds to a different aspect of the raw input. in the case of
imaging, this could be an edge, colour, or specific pattern.
the key difference between deep learning and other types of machine learning is that cnns develop their own
representations needed for pattern recognition rather than requiring human input to structure the data and design
feature extractors. in plain language, the algorithm learns for itself the features of an image that are important for
classification. therefore, the algorithm has the freedom to discover classification features that might not have
been apparent to humans (particularly when datasets are large) and thereby improve the performance of image
classification.
cnns use raw image data that have been labelled by humans in a process known as supervised learning. each image
is fed into the input layer of the algorithm as raw pixels and then processed sequentially through the layers of the cnn.
the final output is a classification likelihood of the image belonging to a prespecified group.
some examples from this review include the following:
• feeding in raw chest radiographs tagged with a label (pneumothorax or no pneumothorax) and the cnn learning
which pixel patterns suggest pneumothorax. when fed with new untagged images, the cnn outputs a likelihood of
the new image containing a pneumothorax or not.
• feeding in raw retinal images tagged with the stage of age related macular degeneration and the cnn learning which
pixel patterns suggest a particular stage. when fed with new untagged images, the cnn outputs a likelihood of the
new image containing a specific stage of age related macular degeneration.
• feeding in optical coherence tomography scans tagged with a management decision (urgent referral, semi urgent
referral, routine referral, observation). when fed with new untagged images, the cnn outputs a likelihood of the most
appropriate management decision.
randomised clinical trials
table 1 summarises the 10 trial registrations. eight
related to gastroenterology, one to ophthalmology,
and one to radiology. eight were from china, one was
from the united states, and one from taiwan. two
trials have completed and published their results
(both in 2019), three are recruiting, and five are not
yet recruiting.
the first completed trial enrolled 350 paediatric
patients who attended ophthalmology clinics in
china. these patients underwent cataract assessment
with or without an ai platform (using deep learning)
to diagnose and provide a treatment recommendation
(surgery or follow-up).20 the authors found that
accuracy (defined as proportion of true results) of
cataract diagnosis and treatment recommendation
with ai were 87% (sensitivity 90%, specificity
86%) and 71% (sensitivity 87%, specificity 44%),
respectively. these results were significantly lower
than accuracy of diagnosis (99%, sensitivity 98%,
specificity 99.6%) and treatment recommendation
(97%, sensitivity 95%, specificity 100%) by senior
consultants (p<0.001 for both); and also lower than
the results for the same ai when tested in a nonrandomised clinical trial setting (98% and 93%,
respectively). the mean time for receiving a diagnosis
with the ai platform was faster than diagnosis by
consultants (2.8 v 8.5 minutes, p<0.001). the authors
suggested that this might explain why patients were
more satisfied with ai (mean satisfaction score 3.47
v 3.38, p=0.007). risk of bias was low in all domains
except for blinding of participants and personnel. the
reporting showed high adherence (31 of 37 items,
84%) to the consort checklist (which was included
with the manuscript).
the second completed trial enrolled 1058 patients
who underwent a colonoscopy with or without the
assistance of a real time automatic polyp detection
system, which provided simultaneous visual and
sound alerts when it found a polyp.21 the authors
reported that the detection system resulted in a
significant increase in the adenoma detection rate
(29% v 20%, p<0.001), and an increase in the number
of hyperplastic polyps identified (114 v 52, p<0.001).
risk of bias was low in all domains except for blinding
of participants, personnel, and outcome assessors.
one of the other trial registrations belongs to the
same author group. these authors are performing a
additional records identified through trial registry
full text articles excluded
not contemporary comparison, not
 only human or human involved with
 ground truth
not a clinical problem
not english language
not an article
no experts
not deep learning
34
12
4
3
3
3
records screened aer duplicates removed
records identified through publication databases
records excluded
full text articles assessed for eligibility
records included in qualitative synthesis
81 studies 10 trial registrations
968
236
quantitative synthesis (meta-analysis) not performed
7334
8302
8066
59
full text trial registrations excluded
not randomised
not deep learning
76
10
86
91
0
fig 1 | prisma (preferred reporting items for systematic reviews and meta-analyses) flowchart of study records
double blind randomised clinical trial with sham ai
to overcome the blinding issue in the previous study.
the reporting showed high adherence (30 of 37 items,
81%) to the consort checklist (though the consort
checklist itself was not included or referenced by the
manuscript).
non-randomised studies
general characteristics
table 2 and table 3 summarise the basic characteristics
of the 81 non-randomised studies. nine of 81 (11%)
non-randomised studies were prospective, but only
six of these nine were tested in a real world clinical
environment. the us and asia accounted for 82%
of studies, with the top four countries as follows: us
(24/81, 30%), china (14/81, 17%), south korea (12/81,
15%), and japan (9/81, 11%). the top five specialties
were radiology (36/81, 44%), ophthalmology (17/81,
21%), dermatology (9/81, 11%), gastroenterology
(5/81, 6%), and histopathology (5/81, 6%). eighteen
(22%) studies compared how long a task took in ai and
human arms in addition to accuracy or performance
metrics. funding was predominantly academic (47/81,
58%) as opposed to commercial (9/81, 11%) or mixed
(1/81, 1%). twelve studies stated they had no funding
and another 12 did not report on funding. a detailed
table with further information on the 81 studies is
included as an online supplementary file.
in 77 of 81 studies, a specific comment was included
in the abstract about the comparison between ai and
clinician performance. ai was described as superior
in 23 (30%), comparable or better in 13 (17%),
comparable in 25 (32%), able to help a clinician
perform better in 14 (18%), and not superior in two
(3%). only nine studies added a caveat in the abstract
that further prospective trials were required (this was
missing in all 23 studies that reported ai was superior
to clinician performance). even in the discussion
section of the paper, a call for prospective studies (or
trials in the case of existing prospective work) was
only made in 31 of 81 (38%) studies. seven of 81 (9%)
studies claimed in the discussion that the algorithm
could now be used in clinical practice despite only
two of the seven having been tested prospectively in
a real world setting. concerning reproducibility, data
were public and available in only four studies (5%).
code (for preprocessing of data and modelling) was
available in only six studies (7%). both raw labelled
data and code were available in only one study.22
methods and risk of bias
most studies developed and validated a model
(63/81, 78%) compared with development only by
using validation through resampling (9/81, 11%) or
validation only (9/81, 11%). when validation occurred
in a separate dataset, this dataset was from a different
geographical region in 19 of 35 (54%) studies, from
a different time period in 11 of 35 (31%), and a
combination of both in five of 35 (14%). in studies that
did not use a separate dataset for validation, the most
common method of internal validation was split sample
(29/37) followed by cross validation (15/37), and then
bootstrapping (6/37); some studies used more than
one method (box 2). sample size calculations were
reported in 14 of 81 (17%) studies. dataset sizes were
as follows (when reported): training, median 2678
(interquartile range 704-21362); validation, 600 (200-
1359); and test, 337 (144-891). the median event rate
for development, validation, and test sets was 42%,
44%, and 44%, respectively, when a binary outcome
was assessed (n=62) as opposed to a multiclass
classification (n=19). forty one of 81 studies used data
augmentation (eg, flipping and inverting images) to
increase the dataset size.
the human comparator group was generally small
and included a median of five clinicians (interquartile
range 3-13, range 1-157), of which a median of four
were experts (interquartile range 2-9, range 1-91).
the number of participating non-experts varied from
0 to 94 (median 1, interquartile range 0-3). experts
were used exclusively in 36 of 81 studies, but in the
45 studies that included non-experts, 41 had separate
performance data available which were exclusive to
the expert group. in most studies, every human (expert
or non-expert) rated the test dataset independently
(blinded to all other clinical information except the
image in 33/81 studies). the volume and granularity
of the separate data for experts varied considerably
among studies, with some reporting individual
performance metrics for each human (usually in
supplementary appendices).
the overall risk of bias assessed using probast led
to 58 of 81 (72%) studies being classified as high risk
(fig 2); the analysis domain was most commonly rated
to be at high risk of bias (as opposed to participant or
outcome ascertainment domains). major deficiencies
in the analysis domain related to probast items 4.1
(were there a reasonable number of participants?),
4.3 (were all enrolled participants included in the
analysis?), 4.7 (were relevant model performance
measures evaluated appropriately?), and 4.8 (were
model overfitting and optimism in model performance
accounted for?).
adherence to reporting standards
adherence to reporting standards was poor (<50%
adherence) for 12 of 29 tripod items (see fig 3).
overall, publications adhered to between 24% and
90% of the tripod items: median 62% (interquartile
range 45-69%). eight tripod items were reported in
90% or more of the 81 studies, and five items in less
than 30% (fig 3). a flowchart for the flow of patients
or data through the study was only present in 25 of
81 (31%) studies. we also looked for reporting of the
hardware that was used for developing or validating
the algorithm, although this was not specifically
requested in the tripod statement. only 29 of 81
(36%) studies reported this information and in most
cases (n=18) it related only to the graphics processing
unit rather than providing full details (eg, random
access memory, central processing unit speed,
configuration settings).
table 1 | randomised trial registrations of deep learning algorithms
trial registration title status
record last
updated country specialty
planned
sample
size intervention control blinding
primary
outcome
anticipated
completion
chictr-ddd17012221
a colorectal polyps auto-detection
system based on deep learning to
increase polyp detection rate: a
prospective clinical study
completed,
published
16 july
2018 china gastroenterology 1000
ai assisted
colonoscopy
standard
colonoscopy none
polyp detection
rate and adenoma
detection rate
28 february
2018
nct03240848
comparison of artificial intelligent
clinic and normal clinic
completed,
published
30 july
2018 china ophthalmology 350 ai assisted clinic normal clinic
double (investigator and outcomes
assessor)
accuracy for
congenital
cataracts
25 may
2018
nct03706534
breast ultrasound image reviewed
with assistance of deep learning
algorithms recruiting
17 october
2018 us radiology 300
computer aided
detection system
manual
ultrasound
imaging review
double (participant
and investigator)
concordance
rate
31 july
2019
nct03840590
adenoma detection rate
using ai system in china
not yet
recruiting
15 february
2019 china gastroenterology 800
csk ai system
assisted
colonoscopy
standard
colonoscopy none
adenoma
detection rate
1 march
2020
nct03842059
computer-aided detection
for colonoscopy
not yet
recruiting
15 february
2019 taiwan gastroenterology 1000
computer aided
detection
standard
colonoscopy
double (participant,
care provider)
adenoma
detection rate
31 december
2021
chictr1800017675
the impact of a computer aided
diagnosis system based on deep
learning on increasing polyp
detection rate during colonoscopy, a
prospective double blind study
not yet
recruiting
21 february
2019 china gastroenterology 1010
ai assisted
colonoscopy
standard
colonoscopy double
polyp detection
rate and adenoma
detection rate
31 january
2019
chictr1900021984
a multicenter randomised controlled
study for evaluating the effectiveness
of artificial intelligence in improving
colonoscopy quality recruiting
19 march
2019 china gastroenterology 1320
endoangel
assisted
colonoscopy colonoscopy
double (participants and evaluators) polyp detection rate 31 december 2020
nct03908645
development and validation of a
deep learning algorithm for bowel
preparation quality scoring
not yet
recruiting 9 april 2019 china gastroenterology 100
ai assisted
scoring group
conventional
human scoring
group
single (outcome
assessor)
adequate bowel
preparation
15 april
2020
nct03883035
quality measurement of esophagogastroduodenoscopy using deep
learning models recruiting 17 april 2019 china gastroenterology 559
dcnn model
assisted egd
conventional
egd
double (participant,
care provider)
detection of upper
gastrointestinal
lesions
20 may
2020
chictr1900023282
prospective clinical study for artificial
intelligence platform for lymph node
pathology detection of gastric cancer
not yet
recruiting 20 may 2019 china gastroenterology 60
pathological
diagnosis of
artificial
intelligence
traditional
pathological
diagnosis not stated
clinical
prognosis
31 august
2021
ai=artificial intelligence; csk=commonsense knowledge; dcnn=deep convolutional neural network; egd=esophagogastroduodenoscopy.
table 2 | characteristics of non-randomised studies lead author year country study type specialty disease outcome caveat in discussion* suggestion in discussion† abramoff 2018 us prospective real world ophthalmology diabetic retinopathy more than mild diabetic retinopathy no yes arbabshirani 2018 us prospective real world radiology intracranial haemorrhage haemorrhage yes no arji 2018 japan retrospective radiology oral cancer cervical lymph node metastases no no becker 2017 switzerland retrospective radiology breast cancer bi-rads category 5 yes no becker 2018 switzerland retrospective radiology breast cancer bi-rads category 5 yes no bien 2018 us retrospective radiology knee injuries abnormality on mri no no brinker 2019 germany retrospective dermatology skin cancer melanoma no no brinker 2019 germany retrospective dermatology skin cancer melanoma yes no brown 2018 us retrospective ophthalmology retinopathy of prematurity plus disease no no burlina 2018 us retrospective ophthalmology macular degeneration armd stage no no burlina 2017 us retrospective ophthalmology macular degeneration intermediate or advanced stage armd no no burlina 2017 us retrospective ophthalmology macular degeneration armd stage no no bychov 2018 finland retrospective histopathology colorectal cancer low or high risk for 5 year survival no no byra 2018 us retrospective radiology breast cancer bi-rads category 4 or more no no cha 2018 us retrospective radiology bladder cancer t0 status post chemotherapy no no cha 2019 south korea retrospective radiology lung cancer nodule operability yes no chee 2019 south korea retrospective radiology osteonecrosis of the femoral head stage of osteonecrosis yes no chen 2018 taiwan prospective gastroenterology colorectal cancer neoplastic polyp no no choi 2018 south korea retrospective radiology liver fibrosis fibrosis stage no no choi 2019 south korea retrospective radiology breast cancer malignancy no no chung 2018 south korea retrospective orthopaedics humerus fractures proximal humerus fracture yes no ciompi 2017 netherlands/italy retrospective radiology lung cancer nodule type no no ciritsis 2019 switzerland retrospective radiology breast cancer bi-rads stage no no de fauw 2018 uk retrospective ophthalmology retinopathy diagnosis and referral decision yes no ehtesham bejnordii 2017 netherlands retrospective histopathology breast cancer metastases yes no esteva 2017 us retrospective dermatology skin cancer lesion type yes no fujioka 2019 japan retrospective radiology breast cancer bi-rads malignancy no no fujisawa 2018 japan retrospective dermatology skin cancer malignancy classification yes no gan 2019 china retrospective orthopaedics wrist fractures fracture yes no gulshan 2019 india prospective real world ophthalmology retinopathy moderate or worse diabetic retinopathy or referable macula oedema yes no haenssle 2018 germany retrospective dermatology skin cancer malignancy classification and management decision yes no hamm 2019 us retrospective radiology liver cancer li-rads category no no han 2018 south korea retrospective dermatology skin cancer cancer type no no han 2018 south korea retrospective dermatology onchomycosis onchomycosis diagnosis no no hannun 2019 us retrospective cardiology arrhythmia arrhythmia classification no no he 2019 china retrospective radiology bone cancer recurrence of giant cell tumour no no hwang 2019 taiwan retrospective ophthalmology macular degeneration classification and type of armd no yes hwang 2018 south korea retrospective radiology tuberculosis tb presence yes no hwang 2019 south korea retrospective radiology pulmonary pathology abnormal chest radiograph yes no kim 2018 south korea retrospective radiology sinusitis maxillary sinusitis label no no kise 2019 japan retrospective radiology sjogren’s syndrome sjogren’s syndrome presence no no kooi 2017 netherlands retrospective radiology breast cancer classification of mammogram no no krause 2018 us retrospective ophthalmology diabetic retinopathy diabetic retinopathy stage no no kuo 2019 taiwan retrospective nephrology chronic kidney disease egfr<60 ml/min/1.73m2 no yes lee 2018 us prospective radiology intracranial haemorrhage haemorrhage yes no li 2018 china prospective oncology nasopharyngeal cancer malignancy no yes li 2018 china retrospective ophthalmology glaucoma glaucoma no no li 2018 china retrospective radiology thyroid cancer malignancy yes no armd=age related macular degeneration; bi-rads=breast imaging reporting and data system; egfr=estimated glomerular filtration rate; li-rads=liver imaging reporting and data system; mri=magnetic resonance imaging; tb=tuberculosis. *caveat mentioned in discussion about need for further prospective work or trials. †suggestion in discussion that algorithm can now be used clinically.
table 3 | characteristics of non-randomised studies
lead author year country study type specialty disease outcome
caveat in
discussion*
suggestion in
discussion†
long 2017 china prospective real world ophthalmology congenital cataracts detection of congenital cataracts no no
lu 2018 china retrospective ophthalmology macular pathologies classification of macular pathology no no
marchetti 2017 us retrospective dermatology skin cancer malignancy (melanoma) yes no
matsuba 2018 japan retrospective ophthalmology macular degeneration wet amd no no
mori 2018 japan prospective real world gastroenterology polyps neoplastic polyp yes yes
nagpal 2019 us retrospective histopathology prostate cancer gleason score no no
nakagawa 2019 japan retrospective gastroenterology oesophageal cancer cancer invasion depth stage sm2/3 no no
nam 2018 south korea retrospective radiology pulmonary nodules classification and localisation of nodule yes no
nirschl 2018 us retrospective histopathology heart failure heart failure (pathologically) no yes
olczak 2017 sweden retrospective orthopaedics fractures fracture no yes
park 2019 us retrospective radiology cerebral aneurysm aneurysm presence yes no
poedjiastoeti 2018 thailand retrospective oncology jaw tumours malignancy no no
rajpurkar 2018 us retrospective radiology pulmonary pathology classification of chest radiograph pathology yes no
raumviboonsuk 2019 thailand prospective real world ophthalmology diabetic retinopathy moderate or worse diabetic retinopathy yes no
rodriguez-ruiz 2018 netherlands retrospective radiology breast cancer classification of mammogram yes no
sayres 2019 us retrospective ophthalmology diabetic retinopathy moderate or worse non-proliferative diabetic retinopathy no no
shichijo 2017 japan retrospective gastroenterology gastritis helicobacter pylori gastritis no no
singh 2018 us retrospective radiology pulmonary pathology chest radiograph abnormality no no
steiner 2018 us retrospective histopathology breast cancer metastases yes no
ting 2017 singapore retrospective ophthalmology
retinopathy, glaucoma,
macular degeneration
referable pathology for retinopathy,
glaucoma, macular degeneration yes no
urakawa 2019 japan retrospective orthopaedics hip fractures intertrochanteric hip fracture no no
van grinsven 2016 netherlands retrospective ophthalmology fundal haemorrhage fundal haemorrhage no no
walsh 2018 uk/italy retrospective radiology fibrotic lung disease fibrotic lung disease no no
wang 2019 china retrospective radiology thyroid nodule nodule presence yes no
wang 2018 china retrospective radiology lung cancer invasive or preinvasive adenocarcinoma nodule no no
wu 2019 us retrospective radiology bladder cancer t0 response to chemotherapy no no
xue 2017 china retrospective orthopaedics hip osteoarthritis radiograph presence of hip osteoarthritis no no
ye 2019 china retrospective radiology intracranial haemorrhage presence of intracranial haemorrhage yes no
yu 2018 south korea retrospective dermatology skin cancer malignancy (melanoma) no no
zhang 2019 china retrospective radiology pulmonary nodules presence of a malignant nodule yes no
zhao 2018 china retrospective radiology lung cancer classification of nodule invasiveness no no
zhu 2019 china retrospective gastroenterology gastric cancer tumour invasion depth (deeper than sm1) no no
zucker 2019 us retrospective radiology cystic fibrosis brasfield score yes no
amd=age related macular degeneration.
*caveat mentioned in discussion about need for further prospective work or trials.
†suggestion in discussion that algorithm can now be used clinically.
discussion
we have conducted an appraisal of the methods,
adherence to reporting standards, risk of bias, and
claims of deep learning studies that compare diagnostic
ai performance with human clinicians. the rapidly
advancing nature and commercial drive of this field
has created pressure to introduce ai algorithms into
clinical practice as quickly as possible. the potential
consequences for patients of this implementation
without a rigorous evidence base make our findings
timely and should guide efforts to improve the design,
reporting, transparency, and nuanced conclusions of
deep learning studies.23 24
principal findings
five key findings were established from our review.
firstly, we found few relevant randomised clinical
trials (ongoing or completed) of deep learning in
medical imaging. while time is required to move from
development to validation to prospective feasibility
testing before conducting a trial, this means that
claims about performance against clinicians should
be tempered accordingly. however, deep learning
only became mainstream in 2014, giving a lead time
of approximately five years for testing within clinical
environments, and prospective studies could take a
minimum of one to two years to conduct. therefore,
it is reasonable to assume that many similar trials
will be forthcoming over the next decade. we found
only one randomised trial registered in the us despite
at least 16 deep learning algorithms for medical
imaging approved for marketing by the food and
drug administration (fda). these algorithms cover a
range of fields from radiology to ophthalmology and
cardiology.2 25
secondly, of the non-randomised studies, only
nine were prospective and just six were tested in a
real world clinical environment. comparisons of ai
performance against human clinicians are therefore
difficult to evaluate given the artificial in silico context
in which clinicians are being evaluated. in much the
same way that surrogate endpoints do not always
reflect clinical benefit,26 a higher area under the curve
might not lead to clinical benefit and could even
have unintended adverse effects. such effects could
include an unacceptably high false positive rate,
which is not apparent from an in silico evaluation.
yet it is typically retrospective studies that are
usually cited in fda approval notices for marketing
of algorithms. currently, the fda do not mandate
peer reviewed publication of these studies; instead
internal review alone is performed.27 28 however, the
fda has recognised and acknowledged that their
traditional paradigm of medical device regulation
was not designed for adaptive ai and machine
learning technologies. non-inferior ai (rather than
superior) performance that allows for a lower burden
on clinician workflow (that is, being quicker with
similar accuracy) might warrant further investigation.
however, less than a quarter of studies reported time
taken for task completion in both the ai and human
groups. ensuring fair comparison between ai and
clinicians is arguably done best in a randomised
clinical trial (or at the very least prospective) setting.
however, it should be noted that prospective testing
is not necessary to actually develop the model in the
first place. even in a randomised clinical trial setting,
ensuring that functional robustness tests are present
is crucial. for example, does the algorithm produce
the correct decision for normal anatomical variants
and is the decision independent of the camera or
imaging software used?
thirdly, limited availability of datasets and code
makes it difficult to assess the reproducibility of deep
learning research. descriptions of the hardware used,
when present, were also brief and this vagueness
might affect external validity and implementation.
reproducible research has become a pressing issue
across many scientific disciplines and efforts to
encourage data and code sharing are crucial.29-31 even
when commercial concerns exist about intellectual
property, strong arguments exist for ensuring that
algorithms are non-proprietary and available for
scrutiny.32 commercial companies could collaborate
with non-profit third parties for independent
prospective validation.
fourthly, the number of humans in the comparator
group was typically small with a median of only
four experts. there can be wide intra and inter case
variation even between expert clinicians. therefore,
an appropriately large human sample for comparison
is essential for ensuring reliability. inclusion of nonexperts can dilute the average human performance
and potentially make the ai algorithm look better
than it otherwise might. if the algorithm is designed
specifically to aid performance of more junior clinicians
or non-specialists rather than experts, then this should
be made clear.
box 2: specific terms
• internal validation: evaluation of model performance with data used in development process
• external validation: evaluation of model performance with separate data not used in development process
• cross validation: internal validation approach in which data are randomly split into n equally sized groups; the
model is developed in n−1 of n groups, and performance evaluated in the remaining group with the whole process
repeated n times; model performance is taken as average over n iterations
• bootstrapping: internal validation approach similar to cross validation but relying on random sampling with
replacement; each sample is the same size as model development dataset
• split sample: internal validation approach in which the available development dataset is divided into two datasets:
one to develop the model and the other to validate the model; division can be random or non-random.
fifthly, descriptive phrases that suggested at least
comparable (or better) diagnostic performance of an
algorithm to a clinician were found in most abstracts,
despite studies having overt limitations in design,
reporting, transparency, and risk of bias. caveats
about the need for further prospective testing were
rarely mentioned in the abstract (and not at all in
the 23 studies that claimed superior performance
to a clinician). accepting that abstracts are usually
word limited, even in the discussion sections of the
main text, nearly two thirds of studies failed to make
an explicit recommendation for further prospective
studies or trials. one retrospective study gave a
website address in the abstract for patients to upload
their eye scans and use the algorithm themselves.33
overpromising language leaves studies vulnerable
to being misinterpreted by the media and the public.
although it is clearly beyond the power of authors
to control how the media and public interpret their
findings, judicious and responsible use of language in
studies and press releases that factor in the strength
and quality of the evidence can help.34 this issue is
especially concerning given the findings from new
research that suggests patients are more likely to
consider a treatment beneficial when news stories are
reported with spin, and that false news spreads much
faster online than true news.35 36
policy implications
the impetus for guiding best practice has gathered
pace in the last year with the publication of a
report that proposes a framework for developing
transparent, replicable, ethical, and effective research
in healthcare ai (ai-tree).37 this endeavour is led by
a multidisciplinary team of clinicians, methodologists,
statisticians, data scientists, and healthcare policy
makers. the guiding questions of this framework
will probably feed into the creation of more specific
reporting standards such as a tripod extension for
machine learning studies.38 key to the success of these
efforts will be high visibility to researchers and possibly
some degree of enforcement by journals in a similar
vein to preregistering randomised trials and reporting
them according to the consort statement.11 39
enthusiasm exists to speed up the process by which
medical devices that feature ai are approved for
marketing.40 41 better design and more transparent
reporting should be seen eventually as a facilitator of
the innovation, validation, and translation process,
and could help avoid hype.
study limitations
our findings must be considered in light of several
limitations. firstly, although comprehensive, our
search might have missed some studies that could
have been included. secondly, the guidelines that we
used to assess non-randomised studies (tripod and
probast) were designed for conventional prediction
modelling studies, and so the adherence levels we
found should be interpreted in this context. thirdly,
we focused specifically on deep learning for diagnostic
medical imaging. therefore, it might not be appropriate
risk of bias
percentage
participants outcomes analysis overall
high unclear low
0
40
60
100
80
20
fig 2 | probast (prediction model risk of bias assessment tool) risk of bias assessment
for non-randomised studies adherence (%)
title
abstract
introduction - context
introduction - objectives
methods - study design
methods - study dates
methods - study setting
methods - eligibility criteria
methods - outcome predicted
methods - blinding
methods - sample size
methods - missing data
methods - model building
methods - validation predictions
methods - model performance
methods - model updating
methods - data differences
results - flow of data
results - characteristics
results - validation
results - numbers
results - model performance
results - model updating
discussion - limitations
discussion - development v validation
discussion - interpretation
discussion - clinical use
supplementary data
funding
0
40
60
100
80
20
fig 3 | completeness of reporting of individual tripod (transparent reporting of a multivariable prediction model for individual prognosis or
diagnosis) items for non-randomised studies
to generalise our findings to other types of ai, such as
conventional machine learning (eg, an artificial neural
network based mortality prediction model that uses
electronic health record data). similar issues could
exist in many other types of ai paper, however we
cannot definitively make this claim from our findings
because we only assessed medical imaging studies.
moreover, nomenclature in the field is sometimes used
in non-standardised ways, and thus some potentially
eligible studies might have been presented with
terminology that did not lead to them being captured
with our search strategy. fourthly, risk of bias entails
some subjective judgment and people with different
experiences of ai performance could have varying
perceptions.
conclusions
deep learning ai is an innovative and fast moving
field with the potential to improve clinical outcomes.
financial investment is pouring in, global media
coverage is widespread, and in some cases algorithms
are already at marketing and public adoption stage.
however, at present, many arguably exaggerated
claims exist about equivalence with or superiority
over clinicians, which presents a risk for patient
safety and population health at the societal level,
with ai algorithms applied in some cases to millions
of patients. overpromising language could mean that
some studies might inadvertently mislead the media
and the public, and potentially lead to the provision
of inappropriate care that does not align with patients’
best interests. the development of a higher quality and
more transparently reported evidence base moving
forward will help to avoid hype, diminish research
waste, and protect patients.

<|EndOfText|>

statistical issues in the development of covid‐19 prediction
models
to the editor,
clinical prediction models to aid diagnosis, assess disease severity, or
prognosis have enormous potential to aid clinical decision making
during the coronavirus disease 2019 (covid‐19) pandemic. a living
systematic review has, so far, identified 145 covid‐19 prediction
models published (or preprinted) between 3 january and 5 may
2020. despite the considerable interest in developing covid‐19
prediction models, the review concluded that all models to date, with
no exception, are at high risk of bias with concerns related to data
quality, flaws in the statistical analysis, and poor reporting, and none
are recommended for use.1 disappointingly, the recent study by yang
et al2 describing the development of a prediction model to identify
covid‐19 patients with severe disease is no different. the study has
failed to report important information needed to judge the study
findings, but numerous methodological problems are apparent.2
our first point relates to the sample size. the sample size requirements in a prediction model study are largely influenced by the
number of individuals experiencing the event to be predicted (in
yang's study, those with mild covid‐19 disease, as this is the smaller
of the two outcome categories). using published sample size formulae for developing prediction models,3 based on information reported in the yang study (40 predictors, outcome prevalence of
0.489), the minimum sample size in the most optimistic scenario
would be 538 individuals (264 events). to precisely estimate the
intercept alone requires 384 individuals (188 events). the study by
yang included 133 individuals, where 65 had the outcome of mild
disease, substantially lower than required.
developing a prediction model with a small sample size and a
large number of predictors will result in a model that is overfit, including unimportant or spurious predictors, and overestimating the
regression coefficients. this means that the model will appear to fit
the data (used in its development) too well—leading to a model that
has poor predictive accuracy in new data. an important step in all
model development studies is to carry out an internal validation of
the model building process (using either bootstrapping or cross‐
validation), whereby the overestimation in regression coefficients can
be determined and shrunk as well as estimating the optimism in
model performance.4 this important step is absent in the study of
yang, who reported an area under the curve of 0.8842 in the same
data used to develop their model—this will almost certainly be substantially overestimated.
another concern is the actual model. the final model contains seven
predictors and the authors have fully reported this permitting individualized prediction. however, an obvious and major concern is the
regression coefficient reported for procalcitonin, with a value of
48.8309 and accompanying odds ratio with a confidence interval of
“>999.999 (>999.999, >999.999)” (sic). this is clearly nonsensical, and to
put it bluntly, makes the model unusable. the reason for the large regression value (standard error and confidence interval) is due to an issue
called separation.
5 this occurred because there was little or no overlap in
the procalcitonin values between individuals with mild and severe disease. the statistical software used by the authors, sas, will report odds
ratios as greater than 999 when this occurs. instead of retaining this in
the model as is, one preferred approach would be to use firth's correction, available in both sas and r5. the authors used the model to
develop an early warning score—this score has not been presented by
the authors—and we caution against such an approach with a preference for alternative formats that permit estimation of absolute risk.6
other concerns include the handling of missing data. while the
authors mention discarding observed values with more than 20%
missing—it is unclear whether individuals were omitted, or whether
entire predictors were omitted. regardless, one can only assume a
complete‐case analysis was conducted in preference for more suitable approaches using multiple imputations.7 finally, we note the
use of univariate screening, whereby predictors are omitted based on
the lack of statistical association. this approach is largely discredited,
as predictors can be spuriously retained or omitted.8
we urge the authors and other investigators developing
(covid‐19) prediction models to read the transparent reporting of a
multivariable prediction model for individual prognosis or diagnosis
(tripod) statement (www.tripod-statement.org) for key information
to report when describing their study so that readers have the
minimal information required to judge the quality of the study.9 the
accompanying tripod explanation and elaboration paper describes
the rationale of the importance of transparent reporting, examples of
good reporting, but also discusses methodological considerations.10
until improved methodological standards are adopted, we should not
expect prediction models to benefit patients, and should consider the
possibility that they might do more harm than good.
