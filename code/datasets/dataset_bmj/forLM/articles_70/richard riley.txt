a guide to systematic review and meta-analysis of prognostic factor studies

prognostic factors are associated with the risk of future health outcomes in individuals with a particular health condition or some clinical start point (eg, a particular diagnosis). research to identify genuine prognostic factors is important because these factors can help improve risk stratification, treatment, and lifestyle decisions, and the design of randomised trials. although thousands of prognostic factor studies are published each year, often they are of variable quality and the findings are inconsistent. systematic reviews and meta-analyses are therefore needed that summarise the evidence about the prognostic value of particular factors. in this article, the key steps involved in this review process are described.

systematic reviews and meta-analyses are common in the medical literature, routinely appearing in specialist and general medical journals, and forming the cornerstone of cochrane. the majority of systematic reviews focus on summarising the benefit of one or more therapeutic interventions for a particular condition. however, they are also important for summarising other evidence, such as the accuracy of screening and diagnostic tests,1 the causal association of risk factors for disease onset, and the prognostic ability of bespoke factors and biomarkers. prognostic evidence arises from prognosis studies, which aim to examine and predict future outcomes (such as death, disease progression, side effects or medical complications like pre-eclampsia) in people with a particular health condition or start point (such as those developing a certain disease, undergoing surgery, or women who are pregnant).

the progress (prognosis research strategy) framework defines four types of prognosis research objectives: (a) to summarise overall prognosis (eg, overall risk or rate) of health outcomes for groups with a particular health condition2; (b) to identify prognostic factors associated with changes in health outcomes3; (c) to develop, validate, and examine the impact of prognostic models for individualised prediction of such outcomes4; and (d) to identify predictors of an individual’s response to treatment.5 each objective requires specific methods and tools for conducting a systematic review and meta-analysis. two recent articles provided a guide to undertaking reviews and meta-analysis of prognostic (prediction) models.67 in this article, we focus on prognostic factors.

a prognostic factor is any variable that is associated with the risk of a subsequent health outcome among people with a particular health condition. different values or categories of a prognostic factor are associated with a better or worse prognosis of future health outcomes. for example, in many cancers, tumour grade at the time of histological examination is a prognostic factor because it is associated with time to disease recurrence or death. each grade represents a group of patients with a different prognosis, and the risk or rate (hazard) of the outcome increases with higher grades. many routinely collected patient characteristics are prognostic, such as sex, age, body mass index, smoking status, blood pressure, comorbidities, and symptoms. many researched prognostic factors are biomarkers, which include a diverse range of blood, urine, imaging, electrophysiological, and physiological variables.

prognostic factors have many potential uses, including aiding treatment and lifestyle decisions, improving individual risk prediction, providing novel targets for new treatment, and enhancing the design and analysis of randomised trials.3 this motivates so-called “prognostic factor research” to identify genuine prognostic factors (sometimes also called “predictor finding studies”8).9 although thousands of such studies are published each year, often they are of variable quality and have inconsistent findings. systematic reviews and meta-analyses are therefore urgently needed to summarise the evidence about the prognostic value of particular factors.101112 in this article, we provide a step-by-step guide on conducting such reviews. our aim is to help readers, healthcare providers, and researchers understand the key principles, methods, and challenges of reviews of prognostic factor studies.

summary points
primary studies to identify prognostic factors are abundant, but often findings are inconsistent and quality is variable. systematic reviews and meta-analyses are urgently needed to identify, evaluate, and summarise prognostic factor studies and their findings.
a clear review question should be defined using the picots system (population, index prognostic factor, comparator prognostic factors, outcome, timing, setting), and a transparent search undertaken for eligible articles. broad search strings may be required, leading to a large number of articles to screen.
a data extraction phase is needed to obtain the relevant information from each study. a modification of charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) can be used for prognostic factors (charms-pf).
the quips tool (quality in prognostic factor studies) can be used to examine each study’s risk of bias. unfortunately, many primary studies may have a high risk of bias because of poor design standards, conduct, and analysis. applicability of a study should also be checked.
if appropriate, meta-analysis can be used to combine prognostic effect estimates (such as hazard ratios, risk ratios, or odds ratios) across studies to produce an overall summary of a factor’s prognostic effect. between-study heterogeneity should be expected and accounted for.
ideally separate meta-analyses should be performed for unadjusted and adjusted prognostic effect estimates; adjusted estimates are important to examine a factor’s independent prognostic value over and above (that is, after adjustment for) other prognostic factors.
separate meta-analyses may also be required for each method of measurement (for factors and outcomes), each approach to handling continuous factors, and each type of estimate (such as hazard ratios or odds ratios).
publication bias is a major threat to the validity of meta-analyses of prognostic factor studies based on published evidence, and may cause small-study effects (asymmetry on a funnel plot).
remark (reporting recommendations for tumour marker prognostic studies) and prisma (preferred reporting items for systematic reviews and meta-analyses) can be used to guide the reporting of the systematic review and meta-analysis of prognostic factor studies; the degree of confidence in the summary results from the review may be examined by use of adapted forms of grade (grades of recommendation, assessment, development, and evaluation) for interventions and diagnostic test accuracy studies.
availability of individual participant data from primary prognostic factor studies may alleviate many of the challenges.
step 1: defining the review question
the first step is to define the review question. a review of prognostic factor studies falls within the second objective of the progress framework2 because it aims to summarise the prognostic value of a particular factor (or each of multiple factors) for relevant health outcomes and time points in people with a specific health condition (eg, disease). some reviews are broad; for example, riley and colleagues aimed to identify any prognostic factor for overall and disease free survival in children with neuroblastoma or ewing’s sarcoma.13 other reviews have a narrower focus; for example, hemingway and colleagues aimed to summarise the evidence on whether c reactive protein (crp) is a prognostic factor for fatal and non-fatal events in patients with stable coronary disease.14 this crp review is used as an example throughout this article.

charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) provides guidance for formulating a review question (table 1 in the article by moons and colleagues15). although charms was developed15 and refined6 for reviews of prediction model studies, it can also be used to define and frame the question for reviews of prognostic factor studies. charms15 and subsequent improvements6 propose a modification of the traditional pico system (population, index intervention, comparison, and outcome) used in systematic reviews of therapeutic intervention studies. the modification is called picots, because it also considers timing and setting (box 1). in the context of prognostic factor reviews, the “p” of population and the “o” of outcome remain largely the same as in the original pico system, but the “i” refers to index prognostic factors and the “c” refers to other prognostic factors that can be considered as comparators in some way. for example, the aim may be to compare the prognostic ability of a certain index factor with one or more other (that is, comparator) prognostic factors; or to investigate the adjusted prognostic value of a particular index factor over and above (adjusted for) other (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, which is not generally recommended, then no comparator factor is being considered. the “t” denotes timing and refers to two concepts of time. firstly, at what time point the prognostic factors under review are to be measured or assessed (that is, the time point at which prognosis information is required); and secondly, over what time period the outcomes are predicted by these factors. the “s” of setting refers to the setting or context in which the index prognostic factors are to be used because the prognostic ability of a factor may change across healthcare settings.

table 1 charms-pf checklist of key items to be extracted from primary studies of prognostic factors, based on additions and modifications of the original charms checklist for primary studies of prediction models15
view popupview inline
box 1
six items (picots) defining the question for systematic reviews of prognostic factor studies, based on charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies)615 and applied to a review of the adjusted prognostic value of c reactive protein (crp)14
population: define the target population for which prognostic factors under review are to be used. for example, crp review: patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of acute coronary syndrome at least two weeks before prognostic factor (crp) measurement.
index prognostic factor: define the factors for which prognostic value is under review. for example, crp review: crp was the single biomarker reviewed for its prognostic value.
comparator prognostic factors: comparator prognostic factors can be considered in a review in various ways. for example, the aim could be to compare the prognostic ability of a certain index factor with two or more other (that is, comparator) prognostic factors; or to review the adjusted prognostic value of a particular index factor—that is, over and above (adjusted for, independent of) other existing (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, then no comparator factor is being considered. for example, crp review: the focus was on the adjusted prognostic value of crp—that is, its prognostic effect after adjusting for existing (comparator) prognostic factors. in particular, adjustment for the following conventional prognostic factors was of interest: age, sex, smoking status, obesity, diabetes, and one or more lipid variables (from total cholesterol, low density lipoprotein cholesterol, high density lipoprotein cholesterol, triglycerides) and inflammatory markers (fibrinogen, interleukin 6, white cell count).
outcome: define the outcomes for which the prognostic ability of the factor(s) under review are of interest. for example, crp review: outcome events were defined as coronary (coronary death, sudden cardiac death, acute non-fatal myocardial infarction, primary percutaneous coronary intervention, unplanned emergency admissions with unstable angina), cardiovascular (when coronary events were reported in combination with heart failure, stroke, or peripheral arterial disease), and all cause mortality.
timing: define firstly at what time points the prognostic factors (index and comparators) are to be used (that is, the time point of prognostication), and secondly over what time period the outcomes are predicted by these factors. for example, crp review: the crp measurement had to be done at least two weeks after diagnosis and all follow-up information on the outcomes (all time periods) was extracted from the studies.
setting: define the intended setting and role of the prognostic factors under review. for example, crp review: crp measurement was studied in primary and secondary care to provide prognostic information about patients diagnosed with coronary heart disease; this information may be useful for healthcare professionals treating and managing such patients.
return to text
an important component of reviews of prognostic factors is whether unadjusted or adjusted estimates of the index prognostic factors will be summarised, or both. we recommend that reviewers primarily focus on adjusted prognostic factor estimates because they reveal whether a certain index factor contributes independently to the prediction of the outcome over and above (that is, after adjustment for) other prognostic factors. in particular, for each clinical scenario there are often so-called “established” or “conventional” prognostic factors that are always measured. therefore, for prognostic factors under review, it is important to understand whether they contribute additional (sometimes called “independent”) prognostic information to the routinely measured ones. this means that reviewers need adjusted (and not unadjusted or crude) prognostic effect estimates to be estimated and reported in primary prognostic factor studies. such adjusted prognostic estimates are typically derived from a multivariable regression model containing the established prognostic factors plus each index prognostic factor of interest.

for example, consider a logistic regression of a binary outcome including three adjustment factors (a1, a2, and a3) and one new index prognostic factor (x1), which is expressed as:

ln(p/(1−p)) = α+β1a1+β2a2+β3a3+β4x1
here, “p” is the probability of the outcome. after estimation of all the unknown parameters (that is, α, β1, β2, β3, β4), of key interest is the estimated β4. this parameter provides the adjusted prognostic effect of the index prognostic factor and reveals its independent contribution to the prediction of the outcome over and above the prognostic effects of the other (established comparator) factors a1, a2, and a3 combined.

the need to focus on adjusted prognostic effects is no different from (systematic reviews of) aetiological studies, in which the focus is on estimating the association of a certain causal risk factor after adjustment for other risk factors. in such causal research, these factors are usually referred to as “confounders” rather than as “other prognostic factors,” which is the term typically used for prognosis research. the crude (unadjusted) prognostic effect of some index factors may completely disappear after adjustment and is therefore rather uninformative, especially because prognostication in healthcare is rarely based on a single prognostic factor but rather on the information from multiple prognostic factors.4

this article focuses on systematic reviews to summarise prognostic factor effect estimates. some primary studies may also evaluate an index factor’s added value in terms of improvement in risk classification and clinical use (eg, measures such as net reclassification improvement and net benefit), and change in prediction model performance (eg, by calculating the change in the concordance index, also known as the c statistic or area under the receiver operating characteristics (roc) curve).17181920 however, this is beyond the scope of this article, and we refer the reader to other relevant sources.62122

application to crp review
crp is widely studied for its prognostic value in patients with coronary disease. however, there is uncertainty whether crp is useful because us and european clinical practice guidelines recommend measurement but clinical practice varies widely. this uncertainty motivated the systematic review by hemingway and colleagues,14 with the corresponding picots system presented in box 1. no studies were excluded on the basis of methodological standards, sample size, duration of follow-up, publication year, or language of publication.

step 2: searching for and selection of eligible studies
the next step is to identify primary studies that are eligible for review; studies that address the review question defined in step 1 following the picots framework. unfortunately, it is more difficult to identify prognostic factor studies than randomised trials of interventions. prognosis studies do not tend to be indexed (“tagged”) because a taxonomy of prognosis research is not widely recognised. moreover, compared with studies of interventions, there is much more variation in the design of prognostic factor studies (eg, data from cohort studies, randomised trials, routine care registries, and case-control studies can all be used), patient inclusion criteria, prognostic factor and outcome measurement, follow-up time, methods of statistical analysis, and adjustment of (and number of) other prognostic factors (covariates). between-study heterogeneity is therefore the rule rather than the exception in prognostic factor research. it is essential that systematic reviews of prognostic factor studies define the study inclusion and exclusion criteria based on the picots structure (step 1) because this determines the study search and selection strategy.

typically, broad search and selection filters are required that combine terms related to prognosis research (such as prognostic, predict, predictor, factor, independent) with domain or disease specific terms (such as the name of prognostic factors and the targeted disease or patient population).23 a broad search comes at the (often considerable) expense of retrieving many irrelevant records. geersing and colleagues24 validated various existing search strategies for prognosis studies and suggested a generic filter for identifying studies of prognostic factors,232526 which extended the work of ingui, haynes, and wong.232526 when tested in a single review of prognostic factors, this generic filter had a number needed to read of 569 to identify one relevant article, emphasising the difficulty in targeting prognostic factor articles. the number needed to read could be considerably reduced when specific factors or populations are added to the filter. even then, care is needed to be inclusive because multiple terms are often used for the same meaning; for example, biomarker mycn is also referred to as n-myc and nmyc, among other terms.13

once the search is complete, each potentially relevant study must be screened for its applicability to the review question. because of the heterogeneity in prognostic factor studies, during this study selection phase more deviations from the defined picots (in step 1) are possible (far greater than what is typically encountered during the selection of randomised intervention studies). the applicability of this primary study selection should firstly be based on title and abstract screening, followed by full text screening, both ideally done by two researchers independently. any discrepancies should be resolved through discussion, potentially with a third reviewer. to check if any relevant articles have been missed, it is helpful to share the list of identified articles with researchers in the field to examine the reference lists of these articles and to perform a citation search.

application to crp review
hemingway and colleagues included any prospective observational study that reported risk of subsequent events among patients with stable coronary disease in relation to measured crp values.14 eligible studies had to include patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of previous acute coronary syndrome at least 2 weeks before crp measurement. hemingway and colleagues searched medline between 1966 and 25 november 2009 and embase between 1980 and 17 december 2009, using a search string containing terms for coronary disease, prognostic studies, and crp. the search identified 1566 articles, of which 83 fulfilled the inclusion criteria. if specific terms for crp had not been included in the search string, then the total number of identified articles would have far exceeded 1566.

step 3: data extraction
the next step is to extract key information from each selected study. data extraction provides the necessary data from each study, which enables reviewers to examine their (eventual) applicability to the review question and their risk of bias (see step 4). this step also provides the information required for subsequent qualitative and quantitative (meta-analysis) synthesis of the evidence across studies. the charms checklist gives explicit guidance (table 2 in the article by moons and colleagues15) about which key items across 11 domains should be extracted from primary studies of prediction models, and for what reason (that is, to provide general information about the primary study, to guide risk of bias assessment, or to assess applicability of the primary study to the review question). based on our experience of conducting systematic reviews of prognostic factor studies, we modified the original charms checklist for prediction model studies to make it suitable for data extraction in reviews of prognostic factors (here referred to as charms-pf; table 1). this basically means that three domains typically addressing multivariable prediction modelling aspects were combined to one overall analysis domain, while other domain names and key items were slightly reworded or extended. reasons for extraction of each key item are similar to charms for prediction models. because we developed the original charms checklist, a wider consensus of the charms-pf content was not considered necessary.

table 2 quips tool (quality in prognostic factor studies), which can be used to classify risk of bias of prognostic factor studies
view popupview inline
reviewers should extract fundamental information from the primary prognostic factor studies, such as the dates, setting, study design, definitions of start points, outcomes, follow-up length, and prognostic factors; reviewers will often find large heterogeneity in this information across studies. the extracted information can be summarised in tables of study characteristics. in addition, more specific information is needed to properly assess applicability and risk of bias (see step 4), such as methods used to measure prognostic factors and outcomes, handling missing data, attrition (loss to follow-up), and whether estimated associations of the prognostic factors under review were adjusted for other prognostic factors. this information also enhances the potential for meta-analysis and the presentation and interpretation of subsequent summary results (see steps 5-8).

to enable meta-analysis of prognostic factor studies, the key elements to extract are estimates, and corresponding standard errors or confidence intervals, of the prognostic effect for each factor of interest; for example, the estimated risk ratio or odds ratio (for binary outcomes), hazard ratio (for time-to-event outcomes), or mean difference (for continuous outcomes). as most prognostic factor studies consider time-to-event outcomes (including censored observations and different follow-up lengths for patients), hazard ratios are often the most suitable effect measure. a concern is that hazard ratios may not be constant over time, and therefore any evaluations of non-proportional hazards (that is, non-constant hazard ratios for the prognostic factors of interest) should also be extracted; however, such information is rarely reported in sufficient detail.

unfortunately, many prognostic factor studies do not adequately report estimated prognostic effect measures or their precision. for this reason, methods are available to restore the missing information upon data extraction. in particular, parmar and colleagues28 and tierney and colleagues29 describe how to obtain unadjusted hazard ratio estimates (and their variances) when they are not reported directly. for example, under assumptions, the number of outcomes (events) and an available p value (eg, from a log rank test or cox regression) can be used to indirectly estimate the unadjusted hazard ratio between two groups defined by a particular factor (eg, “high” versus “normal” levels). perneger and colleagues30 report how to derive unadjusted hazard ratios from survival proportions, and pérez and colleagues suggest using a simulation approach.31 even with such indirect estimation methods, not all results can be obtained. for example, in a systematic review of 575 studies investigating prognostic factors in neuroblastoma,32 the methods of parmar and colleagues were used to obtain 204 hazard ratio estimates and their confidence intervals; but this represented only 35.5% of the potential evidence.

although indirect estimation methods help retrieve unadjusted prognostic factor effect estimates, they often have limited value for obtaining adjusted effect estimates. furthermore, even when multiple studies provide the adjusted prognostic effect of a particular factor, the set of adjustment factors will usually differ across studies, which complicates the interpretation of subsequent meta-analysis results. we recommend that reviewers predefine the core set of prognostic factors for the outcome of interest (eg, age, sex, smoking status, disease stage) that represents the desired “minimal” set of adjustment factors. an agreed process among health professionals and researchers in the field could be required to define this set. for example, a list of established prognostic factors could be identified that are routinely used within current prognostication of the clinical population of interest.

it may also be necessary to standardise the extracted estimates to ensure they all relate to the same scale and direction in each study. in particular, the direction of the prognostic effect will need standardising if one study compares the hazard rate in a factor’s “high” versus “normal” group, whereas another study compares the hazard rate in the factor’s “normal” versus “high” group. when the outcome is defined differently across studies, approaches to convert effect measures on different outcome scales could be useful.33 also, to deal with different cutpoint levels for values of a particular factor,34 the prognostic effects of “high” versus “normal” could be converted to prognostic effects relating to a 1 unit increase in the factor. this requires assumptions about the underlying distribution of the factor. such an approach was used by hemingway and colleagues.14 of concern, however, is that the actual distribution of a prognostic factor may be unknown (or even vary across studies). finally, it is also possible to derive standardised effect estimates by standardising the corresponding regression coefficients.35

application to crp review
hemingway and colleagues extracted background information such as year of study start, number of included patients, mean age, baseline coronary morbidity (eg, proportion with stable angina), average levels of biomarker at baseline, method of crp measurement, follow-up duration, and number and type of events. basic information was often missing. for example, nearly a fifth of studies did not report the method of measurement, and only a quarter gave the number of patients included in the analyses and reasons for dropout. prognostic effect estimates for crp were extracted in terms of the reported risk ratio, odds ratio, or hazard ratio (labelled generally as “risk ratio” in this article), and 95% confidence intervals. these effect estimates were then converted to a standardised scale comparing the highest third with the lowest third of the (log transformed) crp distribution. if available, separate prognostic effect estimates were extracted for different degrees of adjustment for other prognostic factors.

step 4: evaluating applicability and risk of bias of primary studies
once eligible studies are identified and data are extracted, an important next step is to assess the applicability and risk of bias (quality) of each study in the review. as for steps 2 and 3, ideally this is done by two reviewers, independently, with any discrepancies resolved. applicability refers to the extent to which a selected study (in step 2) matches the review question in terms of the population, timing, prognostic factors, and outcomes (endpoints) of interest. just because a study is eligible for inclusion does not mean it is free from applicability concerns. some aspects of a study may be applicable (eg, correct condition at start point, with prognostic factors of interest evaluated) but not others (eg, incorrect population or setting, inappropriate outcome definition, different follow-up time, lack of adjustment for conventional prognostic factors). applicability is typically first assessed during title and abstract screening, and then during this step, so that it is based on full text screening and determined by picots (step 1) and inclusion and exclusion criteria of studies (step 2).

risk of bias refers to the extent to which flaws in the study design or analysis methods could lead to bias in estimates of the prognostic factor effects. unfortunately, based on growing empirical evidence from systematic reviews examining methodology quality, many primary studies will be at high risk of bias.832363738394041424344 for prognostic factor studies, hayden and colleagues developed the quips checklist (quality in prognostic factor studies) for examining risk of bias across six domains27: study participation, study attrition, prognostic factor measurement, outcome measurement, adjustment for other prognostic factors, and statistical analysis and reporting. table 2 shows the signalling items within these domains to help guide reviewers in making low, unclear, or high risk of bias classifications. additional guidance may be found in general tools examining the quality of observational studies,4546 and the remark guideline (reporting recommendations for tumour marker prognostic studies) for reporting of primary prognostic factor studies.4748

we recommend that users first operationalise criteria to assess the signalling items and domains for the specific review question. for example, with the study participation and attrition domains, this includes defining a priori the most important characteristics that could indicate a systematic bias in study recruitment (study participation domain) and loss to follow-up (study attrition domain). defining these characteristics ahead of time will facilitate assessment and consensus related to the importance of potential differences that could influence the observed association between the index prognostic factors and outcomes of interest. definitions of sufficiently valid and reliable measurement of the index prognostic factors and outcomes should also be specified at the protocol stage. similarly, the core set of other (adjustment) prognostic factors that are deemed necessary for the primary studies to have adjusted for, should be predefined to facilitate judgment related to risk of bias in domain 5.

overall assessment of the six risk of bias domains is undertaken by considering the risk of bias information from the signalling items for each domain, rated as low, moderate, and high risk of bias. occasionally, item information needed to assess the bias domains is not available in the study report. when this occurs, other publications that may have used the same dataset (which often occurs in prognostic studies based on large existing cohorts) should be consulted and study authors should be contacted for additional information. an informed judgment about the potential risk of bias for each bias domain should be made independently by two reviewers, and discussed to reach consensus. each of the six domains needs to be rated and reported separately because this will inform readers, flag improvements needed for subsequent primary studies, and facilitate future meta-epidemiological research. we recommend defining studies with an overall “low risk of bias” as those studies where all, or the most important domains (as determined a priori), are rated as having low (or low to moderate) risk of bias.

application to crp review
hemingway and colleagues assessed the quality of included studies by the quality of their reporting on 17 items derived from the remark guideline.48 the median number of study quality items reported was seven of a possible 17, and standards did not change between 1997 and 2009. only two studies referred to a study protocol, with none referring to a statistical analysis plan. hemingway and colleagues noted that this “makes it difficult to know what the specific research objectives were at the start of cohort recruitment, at the time of crp measurement, or at the onset of the statistical analysis.”14 only two studies reported the time elapsed between first lifetime presentation with coronary disease and assessment of crp and this raised applicability concerns.

step 5: meta-analysis
meta-analysis of prognostic factor studies aims to summarise the (adjusted) prognostic effect of each factor of interest. in addition to missing estimates, challenges for the meta-analyst include (a) having different types of prognostic effect measures (eg, odds ratios and hazard ratios), which are not necessarily comparable30; (b) estimates without standard errors, which is a problem because meta-analysis methods typically weight each study by (a function of) their standard error; (c) estimates relating to various time points of the outcome occurrence or measurement; (d) different methods of measurement for prognostic factors and outcomes; (e) various sets of adjustment factors; and (f) different approaches to handling continuous prognostic factors (eg, categorisation, linear, non-linear trends), including the choice of cutpoint value when dichotomising continuous values into “low” and “normal” groups. many of these issues lead to substantial heterogeneity and if a meta-analysis is performed, summary results cannot be directly interpreted.

generally, meta-analysis results will be most interpretable, and therefore useful, when a separate meta-analysis is undertaken for groups of “similar” prognostic effect measures. in particular, we suggest considering a meta-analysis for:

hazard ratios, odds ratios, and risk ratios separately
unadjusted and adjusted associations separately
prognostic factor effects at distinct cutpoints (or groups of similar cutpoints) separately
prognostic factor effects corresponding to a linear trend (association) separately
prognostic factor effects corresponding to non-linear trends separately
each method of measurement (for factors and outcomes) separately.
ideally a meta-analysis of adjusted results should ensure that all included estimates are adjusted for the same set of other prognostic factors. this situation is unlikely and so a compromise could be to ensure that all adjusted estimates in the same meta-analysis have adjusted for at least a (predefined) minimum set of adjustment factors (that is, a core set of established prognostic factors).

even when adhering to this guidance, unexplained heterogeneity is likely to remain because of other reasons (eg, differences in length of follow-up or in treatments received during follow-up). therefore, if a meta-analysis is performed, a random effects approach is essential to allow for unexplained heterogeneity across studies (box 2), as previously described in the bmj.53 this approach provides a summary estimate of the average prognostic effect of the index factor and the variability in effect across studies. also potentially useful are meta-analysis methods to estimate the trend (eg, linear effect) of a prognostic factor that has been grouped into three or more categories within studies (with each category compared with the reference category). these methods generally model the estimated prognostic effect sizes in each category as a function of “exposure” level (eg, midpoint or median prognostic factor value in the category) and account for within-study correlation and between-study heterogeneity.5455565758 to apply these methods, some additional knowledge of the factor’s underlying distribution is usually needed to help define the “exposure” level because the chosen value can have an impact on the results.56

box 2
explanation of a random effects meta-analysis of prognostic factor effect estimates
the true prognostic effect of a factor is likely to vary from study to study; therefore assuming a common (fixed) prognostic effect is not sensible. if yi and var(yi) denote the prognostic effect estimate (eg, ln(hazard ratio), ln(odds ratio), ln(risk ratio), or mean difference) and its variance in study i, then a general random effects meta-analysis model can be specified as:

yi ~n(μ,var(yi)+τ2).

most researchers use either restricted maximum likelihood or the approach of dersimonian and laird to estimate this model,49 but other options are available, including a bayesian approach.50 of key interest is the estimate of μ, which reveals the summary (average) prognostic effect of the index prognostic factor of interest. the standard deviation of this prognostic factor effect across studies is denoted by τ, and non-zero values suggest there is between-study heterogeneity. confidence intervals for µ should ideally account for uncertainty in estimated variances (in particular τ),51 and we have found the approach of hartung-knapp to be robust for this purpose in most settings.1652 when synthesising prognostic effects on the log scale, the summary results and confidence intervals require back transformation (using the exponential function) to the original scale.

return to text
advanced multivariate meta-analysis methods are also available to handle multiple cutpoints,59 multiple methods of measurement,59 or different adjustment factors in prognostic factor studies.60 an introduction to multivariate meta-analysis has been published in the bmj.61

application to crp review
hemingway and colleagues14 applied a random effects meta-analysis to combine 53 adjusted prognostic effect estimates for crp from studies that adjusted for at least one of six conventional risk factors (age, sex, smoking status, diabetes, obesity, and lipids). the summary meta-analysis result was a risk ratio of 1.97 (95% confidence interval 1.78 to 2.17), which gives the average prognostic effect of crp (for those in the top v bottom third of crp distribution), and suggests larger crp values are associated with higher risk. although there was substantial between-study heterogeneity, nearly all estimates were in the same direction (that is, risk ratio >1). when restricting meta-analysis to just the 13 studies that adjusted for at least all six conventional prognostic factors, the summary risk ratio decreased to 1.65 (95% confidence interval 1.39 to 1.96), and the between-study heterogeneity reduced. using the study specific estimates given by hemingway and colleagues, we updated this meta-analysis (fig 1), obtaining the same summary result but a wider confidence interval (1.34 to 2.04) through the hartung-knapp approach.16

fig 1
fig 1
forest plot showing the study specific estimates and meta-analysis summary result of the adjusted prognostic effect (risk ratio) of c reactive protein taken from the review of hemingway and colleagues14; all studies were adjusted for a core set of existing prognostic factors (age, sex, smoking status, diabetes, obesity, and lipids), plus up to 14 other prognostic factors. meta-analysis results shown are based on a random effects meta-analysis model with dersimonian and laird estimation of the between-study variances. the summary result is identical to hemingway and colleagues,14 but the confidence interval is wider because we used the hartung-knapp approach to account for uncertainty in variance estimates.16although “risk ratio” is used, the estimates actually correspond to a mixture of risk ratios, odds ratios, and hazard ratios

download figure open in new tab download powerpoint
step 6: quantifying and examining heterogeneity
for all meta-analyses, when there is large heterogeneity across included studies, it may be better not to synthesise the study results, but rather display the variability in estimates on a forest plot without showing an overall pooled estimate. when a meta-analysis is performed in the face of heterogeneity, it is important to quantify and report the magnitude of heterogeneity itself; for example, through the estimate of (the between-study variance),62 or an approximate 95% prediction interval indicating the potential true prognostic effect of a factor in a new population.5363

subgroup analyses and meta-regression can be used to examine or explore the causes of heterogeneity. a subgroup analysis performs a separate meta-analysis for categories defined by a particular characteristic, such as those with a low risk of bias, those with a follow-up of less than one year or of at least one year, or those set in countries in europe. a better approach is meta-regression, which extends the meta-analysis equation shown in box 2 by including study level covariates,64 and allows a formal comparison of meta-analysis results across groups defined by covariates (eg, low risk of bias studies v studies at higher risk of bias). unfortunately, subgroup analyses and meta-regression are often problematic. there will often be few studies per subgroup and low power to detect genuine causes of heterogeneity. furthermore, study level confounding will be rife so that it is difficult to disentangle the associations for one covariate from another. for example, studies with a low risk of bias may also have a different length of follow-up or a particular cutpoint level compared with studies at higher risk of bias.

application to crp review
hemingway and colleagues reported that meta-regression identified four study level covariates that explained some between-study heterogeneity in the prognostic effect of crp: definition of comparison group, number of adjustment factors, the (log) number of events, and the proportion of patients with stable coronary disease (reflecting study size).14 studies originally reporting unequal crp groups had stronger effects than those reporting crp on a continuous scale. for each additional adjustment factor, the summary risk ratio decreased by 3%. the summary risk ratio was smaller among studies with more than the median number of outcome events, and smaller among studies confined to stable coronary disease. there was no evidence that the crp effect differed according to the number of quality items reported by a study, or by the type of prognostic effect measure provided (that is, risk ratio, odds ratio, or hazard ratio).

step 7: examining small-study effects
the term “small-study effects” refers to when there is a systematic difference in prognostic effect estimates for small studies and large studies.65 a particular concern is when small studies (especially those that are exploratory because these often evaluate many potential prognostic factors with relatively few outcome events) show larger prognostic effects than larger studies. this difference may be due to chance or heterogeneity, but a major threat here is publication bias and selective reporting, which are endemic in prognosis research.363738 such reporting biases lead to smaller studies, with (statistically) significant or larger prognostic factor effect estimates being more likely to be published or reported in sufficient detail, and thus included in a meta-analysis, than smaller studies with non-significant or smaller prognostic effect estimates. this bias is a potential concern for unadjusted and adjusted prognostic effects. a primary study usually estimates an unadjusted prognostic effect for each of multiple prognostic factors, but study authors may only report effects that are statistically significant. in addition, adjusted results are often only reported for prognostic factors that retain statistical significance in univariable and multivariable analysis. a consequence is that meta-analysis results will be biased, with larger summary prognostic effects than in reality, and potentially some factors being deemed to have clinical value when actually they do not.

the evidence for small-study effects is usually considered on a funnel plot, which shows the study estimates (x axis) against their precision (y axis). a funnel plot is usually recommended if there are 10 or more studies.65 the plot should ideally show a symmetric, funnel like shape, with results from larger studies at the centre of the funnel and smaller studies spanning out in both directions equally. asymmetry will arise if there are small-study effects, with a greater proportion of smaller studies in one particular direction. statistical tests for asymmetry in risk, odds and hazard ratios can be used, such as peter’s and debray’s test.6667 contour enhanced funnel plots also show the statistical significance of individual studies, and “missing” studies are perhaps more likely to fall within regions of non-significance if publication bias was the cause of small-study effects. an example is shown in figure 2.

fig 2
fig 2
evidence of funnel plot asymmetry (small-study effects) in the c reactive protein meta-analysis shown in figure 1. the smaller studies (with higher standard errors) have risk ratio (rr) estimates mainly to the right of the larger studies, and therefore give the largest prognostic effect estimates. a concern is that this is due to publication bias, with “missing” studies potentially falling to the left side of the larger studies and in the lighter shaded regions denoting non-significant rr estimates

download figure open in new tab download powerpoint
as mentioned, small-study effects may also arise due to heterogeneity. therefore, it is difficult to disentangle publication bias from heterogeneity in a single review. for example, if smaller studies used an analysis with fewer adjustment factors, then this may cause larger prognostic factor effects in such studies, rather than it being caused by publication bias. a multivariate meta-analysis could reduce the impact of small-study effects by “borrowing strength” from related information.61

a related concern is that smaller prognostic factor studies are generally at higher risk of bias than larger studies. smaller studies tend to be more exploratory in nature and typically based on a convenient sample, often examining many (sometimes hundreds of) potential prognostic factors, with relatively few outcome events. this design leads to spurious (due to chance) and potentially biased (due to poor estimation properties68) prognostic effect estimates, which are more prone to selective reporting. in contrast, larger studies are often confirmatory studies focusing on one or a few prognostic factors, and are more likely to adopt a protocol driven and prospective approach, with clearer reporting regardless of their findings.3 therefore, larger studies are less likely to identify spurious prognostic factor effect estimates. it is helpful to examine small-study effects (potential publication bias) when restricting analysis to the subset of studies at low risk of bias. if this approach resolves previous issues of small-study effects in the full meta-analysis, then it gives even more credence to focus conclusions and recommendations on the meta-analysis results based only on the higher quality studies.

application to crp review
figure 2 shows a funnel plot of the study estimates from the crp meta-analysis shown in figure 1. there is clear asymmetry, which shows the strong potential for publication bias. there was an insufficient number of studies considered at low risk of bias to evaluate small-study effects in a subset of higher quality studies.

step 8: reporting and interpretation of results
as with all research studies, clear and complete reporting is essential for reviews of prognostic factor studies. most of the reporting guidelines of prisma (preferred reporting items for systemic reviews and meta-analyses) and moose (meta-analysis of observational studies in epidemiology) will be relevant,6970 and should be complemented by remark,4748 which was aimed at primary prognostic factor studies. more specific guidance for reporting systematic reviews of prognostic factor studies is under development.

interpretation and translation of summary meta-analysis results is an important final step. the guidance in the previous steps is the essential input for this step. discussion is necessary on whether and how the prognostic factors identified may be useful in practice (that is, translation of results to clinical practice), and what further research is necessary. ideally impact studies (eg, randomised trials that compare groups which do and do not use a prognostic factor to inform clinical practice) are needed before strong recommendations for clinical practice are made; however, these studies are rare and outside the scope of the review framework outlined in this article.

to interpret the certainty (confidence) of the summary results of a review of intervention effectiveness, grade (grades of recommendation, assessment, development, and evaluation) was developed. this approach assesses the overall quality of and certainty in evidence for the summary estimates of the intervention effects by addressing five domains: risk of bias, inconsistency, imprecision, indirectness, and publication bias. the grade domains can be assessed using the information obtained by the tools and methods described in the above steps. however, it is not known whether these domains, developed for reviews of interventions, are equally applicable to assessing the certainty of summary results of systematic reviews of prognostic factor studies. compared with reviews of intervention studies, allowing for heterogeneity (the inconsistency domain) might be more acceptable in reviews of prognostic factor studies because of the inevitable heterogeneity caused by study differences in methods of measurement, adjustment factors, and statistical analysis methods, among others. furthermore, the threat of selective reporting or publication bias in reviews of prognostic factor studies may be more severe than in reviews of intervention studies because of the problems of exploratory studies, poor reporting, and biased analysis methods.

there is limited empirical evidence for using the existing domains to grade the certainty of summary estimates of prognostic factor studies, although a first attempt has been made71; in addition, an assessment has been performed on grading the certainty of evidence of summary estimates of overall prognosis studies.72 reviewers need to be especially cautious when comparing the adjusted prognostic value of multiple index factors, for example, to conclude whether the summary adjusted hazard ratio for prognostic factor a is larger than that for factor b. usually different sets of studies will be available for each index factor, and so the comparison will be indirect and potentially biased. moreover, the studies evaluating factor a may often have used different sets of adjustment factors (other prognostic factors) than those evaluating factor b. it will be rare to find studies on different index factors that used exactly the same set of adjustment factors. we therefore recommend reviewers restrict comparisons (of the adjusted prognostic value) of two or more index factors to those studies that at least used a similar, minimally required set of adjustment factors.73 even then, due to different scales and distributions of each factor (eg, continuous or binary), a simple comparison of the prognostic effect sizes (eg, hazard ratio for factor a v hazard ratio for factor b) may not be straightforward.

application to crp review
the meta-analysis results suggest crp is a prognostic factor for the risk of death and non-fatal cardiovascular events, even when only including the largest studies that adjusted for all six conventional prognostic factors. in their discussion, hemingway and colleagues downgraded the meta-analysis findings because of a strong concern about the quality and reliability of the underlying evidence.14 the absence of prespecified protocols, poor and potentially biased reporting, and strong potential for publication bias prevented the authors from making firm conclusions about whether crp has prognostic value after adjustment for established prognostic factors. they state that the concerns “explicitly challenge the statement for healthcare professionals made by the centers for disease control that measuring crp is both ‘useful’ and ‘independent’ as a marker of prognosis.”74

summary
in this article, we described the key steps and methods for conducting a systematic review and meta-analysis of prognostic factor studies. current reviews are often limited by the quality and heterogeneity of primary studies.7576 we expect the prevalence of such reviews to grow rapidly, especially as cochrane has recently embarked on prognosis reviews (see also the cochrane prognosis methods group website www.methods.cochrane.org/prognosis).77 our guidance will help researchers to write grant applications for reviews of prognostic factor studies, and to develop protocols and conduct such reviews. protocols of prognostic factor reviews should be published ideally at the same time as the review is registered, for example within prospero, the international prospective register of systematic reviews (www.crd.york.ac.uk/prospero/), or the cochrane database.77 our guidance will also allow readers and healthcare providers to better judge reports of prognostic factor reviews.

finally, we note that some of the limitations described (eg, use of different cutpoint values across studies) could be alleviated if the individual participant data were obtained from primary prognostic factor studies78 rather than being extracted from study publications; although, this may not solve all problems (eg, quality of original study, availability of different adjustment factors).79 further discussion on individual participant data meta-analysis of prognostic factor studies is given elsewhere.80

<|EndOfText|>

one-stage individual participant data meta-analysis models
for continuous and binary outcomes: comparison of
treatment coding options and estimation methods

a one-stage individual participant data (ipd) meta-analysis synthesizes ipd
from multiple studies using a general or generalized linear mixed model. this
produces summary results (eg, about treatment effect) in a single step, whilst
accounting for clustering of participants within studies (via a stratified study
intercept, or random study intercepts) and between-study heterogeneity (via
random treatment effects). we use simulation to evaluate the performance of
restricted maximum likelihood (reml) and maximum likelihood (ml) estimation of one-stage ipd meta-analysis models for synthesizing randomized trials
with continuous or binary outcomes. three key findings are identified. first,
for ml or reml estimation of stratified intercept or random intercepts models, a t-distribution based approach generally improves coverage of confidence
intervals for the summary treatment effect, compared with a z-based approach.
second, when using ml estimation of a one-stage model with a stratified intercept, the treatment variable should be coded using “study-specific centering” (ie,
1/0 minus the study-specific proportion of participants in the treatment group),
as this reduces the bias in the between-study variance estimate (compared with
1/0 and other coding options). third, reml estimation reduces downward bias
in between-study variance estimates compared with ml estimation, and does
not depend on the treatment variable coding; for binary outcomes, this requires
reml estimation of the pseudo-likelihood, although this may not be stable in
some situations (eg, when data are sparse). two applied examples are used to
illustrate the findings.
keywords
estimation methods, individual participant data, ipd, maximum likelihood., meta-analysis,
treatment coding
1 introduction
an individual participant data (ipd) meta-analysis synthesizes the raw individual-level data from multiple related studies
to produce summary results, for example, about the effect of a treatment.1 a common approach to ipd meta-analysis is a
two-stage framework, where the first step analyses the ipd from each study separately to produce aggregate data (such as
a treatment effect estimate and its se), which are then synthesized in the second step using a traditional meta-analysis,
such as a random effects model to account for between-study heterogeneity in the (treatment) effect of interest. an alternative approach to ipd meta-analysis is a one-stage framework, in which all studies are analyzed simultaneously using a
hierarchical model, such as a generalized linear mixed model or a frailty survival model, to produce summary results in
a single step. the one-stage approach has been increasingly used in the past decade.2
with a one-stage ipd meta-analysis model, it is essential to account for clustering of participants within studies to
avoid misleading conclusions.3 in particular, in a generalized linear mixed model framework two options to account for
this clustering are (i) by using a stratified intercept term in the analysis, which involves estimating a separate intercept
for each study; or (ii) by assuming random study intercepts, whereby study intercepts are assumed to be drawn from a
distribution (typically a normal distribution). we recently showed through simulation that, when applying a one-stage
ipd meta-analysis of randomized controlled trials (rcts) with a 1:1 treatment:control allocation ratio and a continuous
outcome, the meta-analyst can choose either a stratified intercept or random intercepts model when restricted maximum
likelihood (reml) is used for estimation.4 that is, the statistical properties of the estimate of summary treatment effect,
the 95% confidence interval for the summary treatment effect, and the estimate of between-study variance of treatment
effects are all very similar regardless of whether a stratified intercept or random intercepts model is used. however, when
using maximum likelihood (ml) estimation, there was less downward bias in the estimate of between-study variance
of the treatment effect when using random intercepts rather than a stratified intercept, due to fewer parameters being
estimated.5,6 consequently, for ml estimation, the coverage of 95% confidence intervals for the summary treatment effect
was better (ie, closer to 95%) when random intercepts rather than a stratified intercept was used.
a recommendation to use random study intercepts, rather than a stratified study intercept, for one-stage ipd
meta-analysis models that require ml estimation (eg, for binary outcomes) may be disconcerting to some readers. in
particular, the use of random intercepts is often considered inappropriate on philosophical grounds, because it allows
across-trial information to inform the control group results, which may compromise randomization within each trial and
bias the summary treatment effects. there is the potential for bias in situations when the allocation ratio is associated
with the overall mean outcome (risk). in such situations the introduced bias will often be small,7 but may be substantial in extreme situations. for example, white et al8 show that when using extreme hypothetical binary outcome data
in a network meta-analysis setting, there can be large potential bias in the summary treatment effect when using a random intercept; the summary treatment effect was an odds ratio of 1.35 when the truth was 1. another issue is that it is
usually recommended to allow the random effects on the intercept and treatment effect to be correlated; however, this
might then allow the baseline risk to contribute toward the summary treatment effect estimates. as an extreme example,
the model could incorporate randomized trials alongside observational studies that only provide information about the
control (untreated group); the latter will then contribute (via the correlation) toward the summary treatment effect.
in this article, we aim to build on previous work,4,5,9-11 and to improve ml estimation of the stratified intercept model
so that it is at least comparable to that of the random intercepts model. specifically, we focus on an ipd meta-analysis of
randomized trials, and evaluate whether the coding of the treatment variable is important toward ml estimation properties. our previous simulations focused on situations where the treatment:control allocation ratio was 1:1 in each study
in the meta-analysis, and found a +0.5/−0.5 coding for the treatment variable (instead of 1/0) substantially improved ml
estimation performance.5 subsequently, we realized that a +0.5/−0.5 coding is the same as using the 1/0 coding minus
the proportion in the treatment group (ie, using 1/0 - 0.5) when the treatment:control allocation ratio is 1:1 this raises the
question about whether ipd meta-analysts should always use a +0.5/−0.5 coding of treatment in their one-stage models,
or whether the choice should be context specific, especially when the actual allocation ratio is not 1:1. therefore, in this
article our primary aims to assess ml estimation performance of the stratified intercept model when using four treatment
coding options:
• the traditional 1/0 coding
• the +0.5/−0.5 coding recommended by jackson et al5
• a coding of 1/0 minus the average proportion of participants in the treatment group in all trials (ie, an “overall
centering” approach)
• a coding of 1/0 minus the proportion of participants in the treatment group in that trial (ie, a “study-specific centering”
approach)
we evaluate which coding approach gives the smallest bias in the estimates of the summary treatment effect and
between-study variance of treatment effects. two additional objectives are also considered: (a) whether the coverage of
95% confidence intervals for the summary treatment effect are improved by using a t-distribution rather the standard
z-based (wald) approach, and (b) whether reml estimation of the pseudo likelihood leads to better performance than
ml estimation of the exact likelihood for one-stage ipd meta-analysis models of binary outcomes.
the structure of this article is as follows. in section 2, we introduce one-stage ipd meta-analysis models with a stratified intercept, for both continuous and binary outcomes. in section 3 we evaluate the four treatment coding options in
an extensive simulation study, for both continuous and binary outcomes. section 4 provides real examples and section 5
concludes with discussion.
2 one-stage model specifications and treatment coding
options
in this section we introduce one-stage ipd meta-analysis models for continuous and binary outcomes with either a
stratified intercept or random intercepts model, and then define the various treatment coding options.
2.1 continuous outcomes
consider that ipd have been obtained from i = 1 to k rcts, each of which has a parallel-group design investigating
whether a treatment is effective (vs a control or existing treatment) at improving a continuous outcome. the treatment
effect then relates to the mean difference (at some follow-up time) in the continuous outcome value between the treatment
and control groups. suppose that there are ni participants in trial i, and that yfij represents the end-of-trial final (f)
continuous outcome value for participant j in trial i. let the treatment group variable be denoted by xij, with coding
options (such 1/0 for treatment/control groups) discussed further in section 2.3.
in this situation, a one-stage ipd meta-analysis with a stratified study intercept (ie, a separate intercept per study to
account for within-study clustering of individuals) and assuming between-study heterogeneity of the treatment effect,
can be written as follows:
y𝐹 𝑖𝑗 = 𝛼i + (𝜃 + ui)x𝑖𝑗 + e𝑖𝑗
ui ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
i ). (1)
here the outcome value (yfij) is assumed normally distributed in each study conditional on the included covariates
(here just xij, but additional covariates could also be included such as the baseline value of the continuous outcome12,13).
there are k distinct intercept terms (𝛼i) and the main parameter of interest is 𝜃, which denotes the summary (average)
treatment effect from the included studies. the true treatment effect in each study is assumed drawn from a normal
distribution with mean 𝜃 and between-trial variance 𝜏2, and 𝜎2
i denotes the study-specific residual variance which is
assumed normally distributed (this could also be stratified by treatment group, but we do not consider this here). the
choice of coding of xij (eg, 1/0 or +0.5/−0.5 for treatment/control groups) does not alter the interpretation of 𝜃, our key
parameter of interest; however, it does change interpretation of the 𝛼i and may have implications on estimation of 𝜏2, as
discussed by jackson et al.5 we evaluate this later in our simulations.
alternatively, a random intercepts model could be specified. for example, allowing for between-study correlation
between the random effects of the intercept and the treatment effect, the model can be written as follows:
y𝐹 𝑖𝑗 = (𝛼 + u1i)+(𝜃 + u2i)x𝑖𝑗 + e𝑖𝑗
(
u1i
u2i
)
∼ n
(𝜏2
𝛼 𝜏12
𝜏12 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
i ) (2)
the parameter terms are as defined for model (1), except now the study-specific intercepts are also assumed drawn
from a normal distribution, with mean of 𝛼 and between trial variance of 𝜏2
𝛼 , and the two random effects (u1i and u2i) are
allowed to be correlated through the covariance term 𝜏12. allowing for correlation imposes a between-study relationship
of control group mean response and treatment effect, which might be viewed as controversial (see discussion). to avoid
this, 𝜏12 might be set to zero, but then the coding of treatment is potentially crucial (see later).11
in a frequentist framework, models (1) and (2) are typically fitted using reml estimation. following estimation, a 95%
confidence interval for 𝜃 is conventionally derived using a (wald) z-based method (𝜃
̂ ± (1.96 × s.e.(𝜃
̂))), but other options
include the satterthwaite and kenward-roger approaches, which replace 1.96 with the critical value of a t-distribution
with a particular denominator degrees of freedom.4,14 in this article, we also consider using 𝜃
̂ ± (tk−1,0.975 × s.e.(𝜃
̂)), where
k is the number of studies in the ipd meta-analysis. for brevity, we refer to this as the t-based confidence interval
approach.
2.2 binary outcomes
now let us consider a binary outcome (eg, dead or alive 1 month after surgery), such that yij is 1 for individuals with
an event and 0 for those without an event. we use a logit-link function, such that our one-stage models have a logistic
regression modeling framework (as suggested by simmonds and higgins15) and the treatment effect is measured by a log
odds ratio. again let the treatment group variable be denoted by xij, with coding options (such 1/0 for treatment/control
groups) discussed further in section 2.3.
in this situation, the stratified intercept model can be written as,
y𝑖𝑗 ∼ bernoulli(𝜋𝑖𝑗)
logit(𝜋𝑖𝑗) = 𝛼i + (𝜃 + ui)x𝑖𝑗
ui ∼ n(0, 𝜏2
), (3)
where 𝜋ij is the event probability for individual j in study i. there are k distinct intercept terms, 𝛼i, and the model parameter 𝜃, denotes the summary (average) treatment effect (log odds ratio). the true treatment effects are again assumed
drawn from a normal distribution with mean 𝜃, and between-trial variance 𝜏2. as in models (1) and (2), adjustment for
baseline covariates is also possible.
alternatively specifying a random intercepts model, and allowing for between-study correlation of control group risk
and treatment effect, we have5,11,16:
y𝑖𝑗 ∼ bernoulli(𝜋𝑖𝑗)
logit(𝜋𝑖𝑗)=(𝛼 + u1i)+(𝜃 + u2i)x𝑖𝑗
(
u1i
u2i
)
∼ n
(𝜏2
𝛼 𝜏12
𝜏12 𝜏2
)
. (4)
the parameter terms are as defined for model (3), except now the study-specific intercepts are also assumed drawn
from a normal distribution, with mean of 𝛼 and between trial variance of 𝜏2
𝛼 , and the two random effects (u1i and u2i)
are allowed to be correlated through the covariance term 𝜏12. as discussed for model (3), setting 𝜏12 to zero assumes no
between-study correlation, but then treatment coding is more important (see below).
models (3) and (4) are typically fitted using ml estimation, via a numerical integration approach such as (adaptive)
gaussian quadrature. unfortunately, there is no natural extension from ml to reml estimation for the exact likelihood
defined by a glmm of a binary, ordinal or count outcome, as the model residuals cannot be estimated separately from
the main parameters. thus ml estimation is generally the default frequentist estimation choice for ipd meta-analyses
of noncontinuous outcomes, for which downward bias in between-study variance estimates and low coverage of confidence intervals is a strong concern, especially with 10 or fewer studies in the ipd meta-analysis. however, wolfinger
and o'connell suggest using a pseudo-likelihood approximation of the exact likelihood,17 where the outcome response
variable is transformed to an approximately linear scale. this allows reml to be used for glmms of noncontinuous
outcomes, but at the expense of an approximate likelihood. this may be an acceptable trade-off in some situations, to
improve between-study variance estimates and confidence interval coverage. we will investigate this in section 3.
2.3 coding of treatment
when a treatment variable is entered into a regression model as a covariate, it is typical practice for researchers to code
the variable as 1/0 for treatment/control. however, a coding of +0.5/−0.5 has also been used by others, such as morris
et al9, tudur-smith et al,18 and turner et al.11 for random intercepts models (2) and (4), which allow for between-study
correlation in control group risk and treatment effect, the choice of treatment coding should be unimportant, because one
can show mathematically a one-to-one correspondence of the model parameters with one coding and the model parameters with another coding, so that the maximized likelihood is the same.5,11 however, if the between-study correlation is
set to zero, then turner et al suggest a +0.5/−0.5 coding is crucial; in particular, for model (4) this ensures the variance of
the log-odds in control group patients is modeled as equal to that in intervention group,11 as otherwise with a 1/0 treatment/control coding the variation for the intervention group is modeled as greater than or equal to the variance for the
control group.
for stratified intercept models (1) and (3), jackson et al5 suggest a coding of +0.5/−0.5 improves ml estimation.
however, they mainly evaluated situations where the treatment:control allocations were 1:1 in each trial. therefore, in
the following section we address unequal treatment:control allocations, and examine whether the following alternative
treatment coding options improve ml estimation even further:
• overall centering: a coding of 1/0 minus the average (unweighted across trials) of the proportion of participants in the
treatment group in each trial. for example, if there are 10 studies within an ipd meta-analysis, with five of those
studies having 70% of participants in the treatment group and the other five having 50% in the treatment group, then
the unweighted average proportion treated per trial is 60%. in this situation, the treatment coding is 1/0 - 0.6 in all
trials, and thus individuals in the treatment group are coded as +0.4, and those in the control group are coded as −0.6.
• study-specific centering: a coding of 1/0 minus the study-specific proportion of participants in the treatment group.
for example, for individuals in a trial where 40% of participants are in the treatment group, then an individual in the
treatment group will be coded as 1−0.4 = +0.6, and an individual in the control group would be coded as 0−0.4 = −0.4.
however, if another trial had 30% in the treatment group, then individuals in the treatment and control groups would
be coded as +0.7 and −0.3, respectively.
3 simulation study for continuous and binary outcomes
we now use a simulation study to compare the performance of ipd meta-analysis models with stratified intercept or random intercepts, first for continuous outcomes and then for binary outcomes. we have three research
questions:
q1: does an “overall centering” or “study-specific centering” coding of the treatment variable improve ml estimation
of the between-study variance of the treatment effect, over and above the +0.5/−0.5 coding proposed by jackson et al,
for the stratified intercept models (1) and (3) in situations where one or more trials have an unequal treatment:control
allocation ratio?
q2: does a t-based approach to confidence interval derivation improve coverage compared with a standard z-based
(wald) approach, for both stratified intercept and random intercepts models?
q3: does reml estimation of the pseudo-likelihood perform better than ml estimation of the exact likelihood for
binary outcome models (3) and (4)?
3.1 continuous outcome simulation study
in our first simulation, we extend the simulation study of legha et al for one-stage ipd meta-analysis models of continuous
outcomes to the situation when there are varying treatment:control allocation ratios.
3.1.1 methods
full details of the simulation methods are provided in supplementary material s1. briefly, we simulated ipd according
to model (2), with a 1/0 treatment/control coding and assuming no correlation of the pair of random effects (ie, 𝜏12 = 0)
for simplicity to avoid a relationship between control group response and treatment effect. a range of different simulation scenarios were considered (see supplementary material s1), each involving varying treatment:control allocation
ratios (randomly drawn from a u[0.1,0.9] distribution), and varying the number of studies, number of participants, and
magnitude of between-study variance of treatment effects.
one thousand ipd meta-analysis datasets were generated for each scenario. to each we fitted the random intercepts
model (2) used to generate the data; that is, model (2) using a 1/0 treatment coding option, whilst forcing 𝜏12 to be 0 (its correct value) to avoid estimation issues that often arise when estimating between-study correlations.19 then we also fitted
the stratified intercept model (1) for each of the four treatment coding options. both ml and reml estimation were examined, alongside z-based and sattherwaite confidence intervals. although reml is the preferred estimation method for
one-stage ipd meta-analyses of continuous outcomes, we also considered ml estimation to inform subsequent extension
to one-stage ipd meta-analyses of binary outcomes, for which ml estimation is usually the default (see section 3.2).
performance was summarized by the bias in the summary treatment effect estimate (𝜃
̂), the coverage of 95% confidence
intervals for the summary treatment effect, and the bias in the between-study variance (𝜏̂2) of the treatment effects. for the
latter, we calculated percentage difference between the estimated and true between-study variance of the treatment effect
(ie, 100 × (𝜏̂2 − 𝜏2)∕𝜏2)), and report the mean and median of these percentages across the 1000 simulations. distributions
of 𝜏̂2 were extremely skewed across each set of 1000 results, and so presentation of median values helps indicate estimation
problems in addition to the more formally correct mean bias.
3.1.2 results when using ml estimation
the summary treatment effect estimates were approximately unbiased for all scenarios, modeling approaches, and treatment coding options. however, in most scenarios there was considerable downward bias in the estimated between trial
variance (𝜏̂2) of the treatment effects (see figure 1, and also see supplementary material tables s2(a), (b), and (c)). the
bias was largest when using the stratified intercept model (1) with a 1/0 treatment/control coding for the treatment variable, and the bias was least when using the “study-specific centering” coding. for example, under the stratified intercept
model and setting b1-a1 (which involves five trials where the number of participants per trial was u(30, 1000), the median
downward bias of 𝜏̂2 was 100% with 1/0 treatment/control coding, which improved to 65.4%, 61.4%, and 49.5%, with
+0.5/−0.5 coding, an “overall centering” coding, and a “study-specific centering” coding, respectively.
coverage of 95% confidence intervals for the summary treatment effect was also closest to 95% when using the
“study-specific centering” approach. for example, in scenario b2 the stratified intercept model had a z-based confidence
interval coverage of 79.70%, 84.20%, and 87.10% for the 1/0, +0.5/−0.5 and “study-specific centering” codings, respectively
(table s2(b)).
crucially, “study-specific centering” of the treatment variable not only improves ml estimation of stratified intercept
model (1), but also makes it comparable (in terms of bias and coverage) to ml estimation of the data generating model (3)
(ie, the random intercepts model with 1/0 coding). however, despite having the best performance, both approaches still
have downward bias in 𝜏̂2 and gave z-based confidence interval coverage <95% for most scenarios. this appears worst in
situations where the allocation ratio was most unbalanced (eg, see results in table s2(a) where treatment prevalence was
90% in all studies).
3.1.3 results when using reml estimation
reml estimation also gives approximately unbiased summary treatment effect estimates for all scenarios, modeling
approaches, and treatment coding options. it also reduces the downward bias in the ml estimate of 𝜏̂2 for all modelling
options (although the downward bias was not removed entirely). furthermore, unlike for ml estimation, the choice of
treatment coding becomes irrelevant when fitting the stratified intercept model (1) using reml estimation. the median
(or mean) bias in 𝜏̂2 was generally very similar regardless of the coding used (see supplementary material table s2[d]), as
were the summary treatment effect estimates and their confidence intervals. therefore, when using reml estimation of
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
-100 -50 0 -100 -50 0 -100 -50 0
base case a1 a2
b1 a1, b1 a2, b1
b2 a1, b2 a2, b2
b3 c1 c2
d1 d2
median % bias in τ2
figure 1 the median percentage bias of the between-trial variance of treatment effects (𝜏̂2) for ml estimation of the stratified
intercept model (1) and random intercepts model (2)*, for the continuous outcome simulation scenarios** described in table s1, allowing for
unequal treatment:control allocation ratios in each trial in the ipd meta-analysis from 10% to 90%. circular points denote the estimated
percentage bias, and a horizontal line is drawn from each estimate to the ideal value of 0. ipd, individual participant data; ml, maximum
likelihood. *the random intercepts model refers to the data generating model, which was model (2) but with treatment coded as 1/0 and the
between-study correlation assumed zero. **scenarios are labeled “base case,” “a1,” “a2,” and so on. for explanation of each scenario setting,
see table s1. briefly, the base case was k =10, ni =100, 𝜃 = −9.66, 𝜏2 =7.79. then the other scenarios made changes of: a1: k =5; a2:
k =20; b1: ni ∼ u(30,1000); b2: k =10, ni ∼ u(30,100) for trials 1 to 5, ni ∼ u(900,1000) for trials 6 to 10; a1,b1: k =5 and ni ∼ u(30,1000);
a2,b1: k =20 and ni ∼ u(30,1000). a1,b2: ni ∼ u(30,100) for trials 1 and 2, ni ∼ u(900,1000) for trials 3 to 5. a2,b2: k =20, ni ∼ u(30,100) for
trials 1 to 10, ni ∼ u(900,1000) for trials 11 to 20; b3: ni ∼ u(30,100); c1: halving variance of intercept; c2: doubling variance of intercept; d1:
𝜏2 =3.9; d2: 𝜏2 =15.6
the stratified intercept model (1), “study-specific centering” of the treatment variable does not improve performance over
traditional 1/0 coding, and the coding is irrelevant. results from the stratified intercept model were also comparable to
reml estimation of the random intercepts model (2) with 1/0 coding. coverage of z-based 95% confidence intervals for
the summary treatment effect were generally too low, but improved close to 95% when using the satterthwaite approach
(as also shown by legha et al4).
3.2 binary outcome simulation study
in our second simulation study we extend the simulations of jackson et al for ipd meta-analysis of binary outcomes,5 to
evaluate the ml estimation performance of random intercepts model (4) with a 1/0 coding and the stratified intercept
table 1 simulation study scenarios of jackson et al5 that were extended in this article, by allowing for unequal treatment:control
allocation ratios in each trial in the ipd meta-analysis
data generation
scenario, as labeled
by jackson et al k 𝝉2
number of
participants in the
treatment group (n)
number of participants
in the control groupa
baseline log-odds
of the event in the
control group (loc)
1 10 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
3 10 0.168 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
4 3 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
5 5 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
6 20 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
7 10 0.024 n ∼ u(10, 100) n loc n(logit[0.2], 0.32)
note: all scenarios were performed with 𝜃 =0 and 𝜃 =log (2). for further details see section 6 in jackson et al.
abbreviation: ipd, individual participant data.
ashows the number used in the control group of the jackson et al simulation. however, in our simulations we changed the number in the control group of a
trial to ensure the treatment group prevalence was one of the following (chosen at random): 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, or 90%.
model (3) for each of the four treatment coding options. a variety of simulation settings are considered, allowing for
unequal treatment:control allocation ratios in each trial within the ipd meta-analysis. we examine the estimate of the
between-study variance (𝜏̂2) of the treatment effect, the estimate of the summary treatment effect (𝜃
̂), and the coverage
of the 95% confidence interval for the summary treatment effect.
3.2.1 methods
we chose the simulation scenarios from jackson et al5 that most closely correspond to those used for our continuous
outcome simulations; that is, jackson et al scenario settings 1, 3, 4, 5, 6, and 7, where the mean risk in the control group
was 20%. these are summarized in table 1, and cover a range of settings that vary in terms of the number of trials,
number of participants per trial, and heterogeneity of parameters. we now consider these scenarios when we vary the
treatment:control allocation ratio in the simulated trials. the omitted jackson et al scenarios mainly corresponded to
either zero or extremely large between-study heterogeneity, or a different logit data generating mechanism for the control
group.
for each scenario, 1000 simulated ipd meta-analysis datasets were created from model (4), with a 1/0 treatment/control coding, but assuming no correlation of the pair of random effects (ie, 𝜏12 = 0) for simplicity to avoid a
relationship between baseline risk and treatment effect. within each scenario we allowed for potential unequal treatment:control allocation ratios in each trial, by randomly drawing from a u(0.1,0.9) distribution. each scenario was
undertaken assuming the summary odds ratio of 1 for the treatment effect (ie, 𝜃 = ln (1) =0), and also assuming a summary odds ratio of 2 (ie, 𝜃 = ln (2)). of note, the chosen magnitude of heterogeneity in setting 1 corresponded to a mean
i2 of about 25% in the meta-analysis datasets simulated.
to each simulated ipd meta-analysis dataset, ml estimation was used to fit the random intercepts model used to
generate the data (ie, model (4) with a 1/0 coding option and forcing 𝜏12 = 0 to avoid estimation issues that often arise
when estimating between-study correlation19), and the stratified intercept model (3) for each of the four treatment coding options. we used the lme4 r package (version 1.1-17),20 with ml estimation undertaken using adaptive gaussian
quadrature, with seven quadrature points.
across all 1000 results obtained for each scenario, the median percentage bias of the between-study variance was calculated, as well as the mean bias of the summary treatment effect, and the coverage of 95% confidence intervals for the
summary treatment effect. the latter was derived as the proportion (across the 1000 results) of 95% confidence intervals that contained the true summary effect. confidence intervals were calculated using the standard z-based method
(𝜃
̂ ± (1.96 × s.e.(𝜃
̂))) and also by the t-based approach that replaces 1.96 with the critical value of a t-distribution with
k−1 degrees of freedom (ie, use 𝜃
̂ ± (tk−1,0.975 × s.e.(𝜃
̂))).21 given the 1000 simulations in each scenario, if coverage is
truly 95%, then we would expect to observe an estimated coverage between 93.4% and 96.2%.
finally, we repeated our simulations using reml estimation of the pseudo likelihood. given that all treatment coding options gave similar performance for reml estimation of continuous outcomes (section 3.1.3), we only considered
the 1/0 treatment variable coding when fitting the stratified intercept and random intercepts models. reml estimation
was undertaken using mlwin, via the runmlwin package within stata.22 we obtained parameter estimates using reml
estimation of the first-order marginal quasi-likelihood linearization of the likelihood (“mql1” option). this linearization approach is noted in the runmlwin help file as being the most stable and fastest to converge, and so was deemed
sensible for our large simulation study. in practice, more accurate (though potentially less stable) estimation options
such as second-order penalized quasi-likelihood linearization (“pql2” option) could be used, with the estimates from the
first-order approach used as initial values (see applied examples in section 4 for further discussion on this).
3.2.2 results when using ml estimation of the exact likelihood and standard z-based
confidence intervals
simulation results for the mean bias of the summary treatment effect estimate (𝜃
̂) across all scenarios for a true treatment
effect of 𝜃 =0 and of 𝜃 =1 are shown in supplementary tables s3 and s4, respectively. the bias of 𝜃
̂was generally negligible
in all cases.
figures 2 and 3 show the median percentage bias in the between-study variance of the treatment effect (𝜏̂2) for the
𝜃 =0 and 𝜃 = ln (2) scenarios, respectively (and also supplementary tables s5 and s6 show the mean percentage bias).
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
-100 -50 0 -100 -50 0
1 3
4 5
6 7
median % bias for τ2
figure 2 the median percentage bias of the between-trial variance of treatment effects (𝜏̂2)for ml estimation (exact likelihood) and
reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for simulation scenarios**
where 𝜃 =0 and allowing for random treatment prevalences (10%-90%) for each study within the ipd meta-analysis. circular points denote
the estimated percentage bias, and a horizontal line is drawn from each estimate to the ideal value of 0. ipd, individual participant data; ml,
maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model, which was
model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. **see table 1 for full details of the scenario
corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all settings also allow the treatment
prevalence for a particular study within a meta-analysis to vary, whereby this is selected from u(0.1,0.9) and then rounded to the nearest 0.1
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
-100 -50 0 -100 -50 0
1 3
4 5
6 7
median % bias for τ2
figure 3 the median percentage bias of the between-trial variance of treatment effects (𝜏̂2)for ml estimation (exact likelihood) and
reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for simulation scenarios**
where 𝜃 = ln (2) and allowing for random treatment prevalences (10%-90%) for each study within the ipd meta-analysis. circular points
denote the estimated percentage bias, and a horizontal line is drawn from each estimate to the ideal value of 0. ipd, individual participant
data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model,
which was model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. **see table 1 for full details of the
scenario corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all settings also allow the
treatment prevalence for a particular study within a meta-analysis to vary, whereby this is selected from u(0.1,0.9) and then rounded to the
nearest 0.1 decimal place
as observed for continuous outcomes, the results show that using the stratified intercept model (3) with a 1/0 treatment/control coding gives the most downwardly biased estimates of the between trial variance of treatment effect. often
the median downward bias was 100%, and mean downward bias typically between 40% and 80%. this downward bias was
generally reduced when using either a +0.5/−0.5 treatment coding or the “overall centering” treatment coding, and by a
similar amount. however, the downward bias was smallest when using a “study-specific centering” coding. for example,
under setting 6 (involving 20 trials), the median downward bias of between study variance estimates from stratified intercept model (3) was 100% with a 1/0 treatment/control coding; this was slightly reduced to 80.2% and 79.5% with +0.5/−0.5
coding or “overall centering” coding, but considerably reduced to 28.7% when using the “study-specific centering” coding.
the mean downward bias shows a similar pattern; this was 80.8%, 44.5%, 43.3%, and 7.85% when using the 1/0, +0.5/−0.5,
“overall centering,” and “study-specific centering,” respectively (table s5).
for stratified intercept model (3), the reduction in downward median and mean bias of 𝜏̂2 when using a “study-specific
centering” coding of treatment also improves upon the z-based coverage of 95% confidence intervals for the summary
treatment effect, as shown in figures 4 and 5 (and also supplementary tables s5 and s6) for the 𝜃 =0 and 𝜃 = ln (2)
scenarios, respectively. for example, in setting three of figure 4, the coverage of z-based confidence intervals from the
1/0 coding is 84%, but this improves to 91% when using the “study-specific centering” coding. furthermore, the z-based
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
0.85 0.90 0.95 0.85 0.90 0.95
1 3
4 5
6 7
z-based t-based
ci coverage for θ
figure 4 coverage (proportion) of z-based and t-based 95% confidence intervals for the summary treatment effect for ml estimation
(exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for
simulation scenarios** where 𝜃 =0 and allowing for random treatment prevalences (10%-90%) for each study within the ipd meta-analysis.
circular points denote the estimated coverage, and a horizontal line is drawn from each estimate to the ideal value of 0.95. ipd, individual
participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data
generating model, which was model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. ** see table 1 for full
details of the scenario corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all settings
also allow the treatment prevalence for a particular study within a meta-analysis to vary, whereby this is selected from u(0.1,0.9) and then
rounded to the nearest 0.1
coverage is always best (ie, closest to 95.0%) when using the “study-specific centering” coding, and also worst when using
the 1/0 coding; the coverage of the +0.5/−0.5 and “overall centering” coding are similar and fall in between the z-based
coverage when using the 1/0 coding and “study-specific centering” coding.
crucially, “study-specific centering” of the treatment variable not only improves ml estimation of stratified intercept
model (3), but also makes it comparable to ml estimation of the random intercepts model (4) with 1/0 coding (ie, bias and
coverage are very similar). however, despite having the best performance, both approaches still have (often considerable)
downward bias in 𝜏̂2, and subsequently z-based coverage is less than 95% for most scenarios. the coverage appears closest
to 95% in the scenario 7 setting (see figure 5); this can be explained by the studies being smaller in this setting, and so
the within-study variances dominate the total variability (ie, i2 is small), and so any downward bias in the between-study
variance is less impactful.
3.2.3 results when using ml estimation and t-based confidence intervals
for all treatment coding options, and for both stratified intercept and random intercepts models, coverage of 95% confidence intervals for the summary treatment effect was generally improved (ie, moved closer to 95%) by using the
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
0.85 0.90 0.95 0.85 0.90 0.95
1 3
4 5
6 7
z-based t-based
ci coverage for θ
figure 5 coverage (proportion) of z-based and t-based 95% confidence intervals for the summary treatment effect for ml estimation
(exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for
simulation scenarios** where 𝜃 = ln (2) and allowing for random treatment prevalences (10%-90%) for each study within the ipd
meta-analysis. circular points denote the estimated coverage, and a horizontal line is drawn from each estimate to the ideal value of 0.95.
ipd, individual participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to
the data generating model, which was model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. **see table 1
for full details of the scenario corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all
settings also allow the treatment prevalence for a particular study within a meta-analysis to vary, such that it is selected from u(0.1, 0.9) and
then rounded to the nearest 0.1
t-based approach to deriving confidence intervals based on the t-distribution with k−1 degrees of freedom (see tables
s5 and s6). for example, in setting 1 when using stratified intercept model (3) with “study-specific centering” of the
treatment variable, the coverage was 90.3% and 94.6% for z-based and t-based confidence intervals, respectively. only
in setting 4, where the number of studies was only 3, is the t-based approach a concern as the coverage is close to
100% and so too high; although arguably this is still preferable to the under-coverage from the corresponding z-based
approach.
3.2.4 results when using reml estimation of the pseudo likelihood
when using reml estimation of the pseudo-likelihood with a 1/0 treatment coding, results are shown in table s7 for all
scenarios. stratified intercept model (3) and random intercepts model (4) have similar performance in all scenarios, and
there is negligible or very small bias in the summary treatment effect estimates.
compared with ml estimation of the exact likelihood, reml estimation of the pseudo likelihood improved the
between-study variance estimate (𝜏̂2), for both stratified intercept and random intercepts models. for example, in setting 1 and 𝜃 = ln (2), when using stratified model (3) the median (mean) bias in 𝜏̂2 was −20.93% (10.72%) using reml
box 1 a summary of the key findings based on our simulation study results and applied examples
• for ml estimation of a one-stage ipd meta-analysis model with a stratified intercept, a “study-specific centering” coding of the treatment variable reduces downward bias of between-study variances and improves coverage
of 95% confidence intervals for the summary (treatment) effect, as compared with other treatment coding options
such as 1/0 for treatment/control. supplementary material s8 also shows this mathematically for a simple case
where all studies in the ipd meta-analysis are of the same size.
• reml is better than ml estimation for continuous outcomes. for binary outcomes, the simulations do not
suggest an important difference in terms of bias and coverage of confidence intervals for the summary treatment
effect when using reml estimation of the pseudo likelihood compared with ml estimation of the exact likelihood. however, in most scenarios reml reduces the downward bias in the between-study variance estimates,
which may be important when the focus is on predictive inferences (eg, the predicted treatment effect in a new
study23). thus both ml (exact likelihood) and reml (pseudo likelihood) estimation may be important to consider
for binary outcomes.
• for either ml or reml estimation, coverage of 95% confidence intervals for the summary treatment is
generally too low when using a z-based approach. improvements are generally made for reml estimation of continuous outcomes by using satterthwaite or kenward-roger approaches, and for ml or pseudo reml estimation
of binary outcomes by using ̂θ±(tk−1,0.975 × s.e.(̂θ)) where k is the number of studies in the meta-analysis.
• for continuous outcomes, reml estimation is recommended over ml estimation for either stratified intercept or random intercepts models (see work of legha et al4), as it improves estimates of between-study variances
(though some downward bias may remain), whilst having negligible bias in the summary treatment effect
estimate and does not depend on the treatment coding chosen.
• for binary outcomes, when fitting a stratified intercept model both ml estimation of the exact likelihood
(with “study-specific centering” treatment coding) and reml estimation of the pseudo likelihood (with 1/0 treatment coding) give negligible bias in the summary treatment effect estimate, and their coverage of 95% confidence
intervals is close to 95% when using the t-based approach (unless the number of studies is less than 5). in addition,
reml estimation of the pseudo likelihood often has less downward bias of between-study variance estimates
than ml estimation.
• for binary outcomes, reml estimation of the pseudo likelihood may be unstable in sparse data situations,
such as when most studies in the ipd meta-analysis are small (in terms of participants or events).
• the decision to use random study intercepts, rather than a stratified study intercept, depends on whether
the researcher is willing to borrow information about control group risk across studies and/or assume a
between-study relationship of control risk and treatment effect.
estimation (with a 1/0 treatment/control coding) compared with −63.08% (−19.40%) when using ml estimation (with a
“study-specific centering coding” for treatment).
the coverage of confidence intervals from reml estimation were closest to 95% when using the t-based approach;
for example, in setting (1) with 𝜃 = ln (2), the coverage from z-based and t-based confidence intervals was 91.6% and
93.3%, respectively. indeed, t-based coverage was consistently good in all settings, generally between 93% and 97%, and
comparable to that when using ml estimation with “study-specific centering” and the t-based approach.
3.3 summary of our key findings
a summary of key findings from the simulation study is shown in box 1.
4 illustration of key findings in applied examples
we now illustrate the key findings in applied examples, which focus on binary outcomes. example 2 has data similar to
that used in the simulation studies, whilst example 1 considers more sparse data.
table 2 summary of the ipd
from seven trials examining the effect
of hormone replacement therapy on
the incidence of heart disease, as
reported by simmonds and higgins15
number of women number of cardiovascular disease events
study control treatment control treatment
1 174 701 0 5
2 14 15 1 0
3 16 15 0 1
4 20 20 1 1
5 26 29 0 1
6 84 84 3 1
7 66 68 0 3
abbreviation: ipd, individual participant data.
table 3 results from ml estimation of models (3) and (4) when fitted to the ipd summarized in table 2
model
treatment
coding
summary
treatment
effect, 𝜽̂ 95% ci: z-based 95% ci: t-based
between-study
(co)variances
stratified intercept
model (3)
1/0 0.56 −0.53, 1.64 −0.80, 1.91 𝜏̂2 = 0
“study-specific
centering”
0.65 −0.69, 1.99 −1.02, 2.32 𝜏̂2 = 0.57
random intercepts model (4) 1/0 0.55 −1.10, 2.21 −1.51, 2.62 𝜏̂2 = 0.74
𝜏12 = −0.81
𝜏2
𝛼 = 1.16
abbreviations: ipd, individual participant data; ml, maximum likelihood.
4.1 example 1: hormone replacement therapy and incidence of heart disease
simmonds et al combined ipd from seven trials examining the effect of hormone replacement therapy compared with
control on the incidence of heart disease.15 the binary outcome data are sparse (table 2), such that the number of events is
few in all studies due to the outcome being rare, and some groups have zero events. in this situation, a traditional two-stage
ipd meta-analysis (ie, estimating the treatment effect and its variance in each study separately, and then pooling the
results in an inverse-variance weighted meta-analysis) is problematic. the treatment effect cannot be estimated in every
study unless a continuity correction is applied in those studies with a zero event; further, the assumption in the second
stage that study-specific treatment effect estimates are normally distributed with known variances may be inappropriate.
a one-stage approach avoids these issues by analyzing the ipd in a single step, for example, using either stratified intercept
model (3) or random intercepts model (4), and the results are shown in table 3.
4.1.1 stratified intercept model results
one of the studies in the ipd meta-analysis (study 1) had a treatment:control allocation ratio of about 4:1, whereas
other studies have close to a 1:1 allocation ratio. our simulation results showed that in this situation ml estimation of
model (3) is improved by using a “study-specific centering” of the treatment variable, as this reduces downward bias in
between-study variance estimates compared with a traditional 1/0 coding. the ml estimates in table 2 reflect this, as
𝜏̂2 = 0 when using a 1/0 coding and 𝜏̂2 = 0.57 when using “study-specific centering” coding. this led to a noticeably different summary treatment effect of 𝜃
̂ = 0.65 (odds ratio of 1.91) when using “study-specific centering” compared with
𝜃
̂ = 0.56 (odds ratio of 1.74) when using 1/0 coding. confidence intervals were also much wider.
another key finding of the simulation study was that z-based (wald) confidence intervals are generally too narrow, and
t-based confidence intervals are more appropriate. in our example, t-based confidence intervals were also considerably
wider. for example, when using the “study-specific centering” coding for ml estimation of model (3), the confidence
interval for the summary odds ratio was 0.36 to 10.15 when using t-based, compared with 0.59 to 6.77 when using the
z-based approach.
although our simulations suggest reml estimation of the pseudo likelihood for model (3) performs well, the scenarios did not cover sparse data akin to that in table 2. indeed, when applying reml estimation to this example, parameter
estimates were unstable; there were large differences in parameter estimates from first and second-order linearization of
the likelihood, and even when changing the coding of treatment (which should not occur for reml; see section 3.1.3).
therefore, the ml estimates in table 3 based on the exact likelihood are more reliable for this example. of note, these ml
estimates were very different to those obtained from a traditional two-stage ipd meta-analysis with continuity corrections
of +0.5 added to deal with zero cells. the latter gave a summary odds ratio of 1.31 and 𝜏̂2 = 0 from reml estimation,
which are much lower than the ml estimates from the more exact one-stage model using “study-specific centering.”
4.1.2 comparison of results for stratified intercept and random intercepts models
unlike in the simulation study, the ml estimation results for random intercepts model (4) with 1/0 coding were not
comparable to those from stratified intercept model (3) with “study-specific centering” coding (table 3). the reason is that
the simulation did not allow borrowing of information between baseline (control group) risk and treatment effect when
generating the ipd. however, in the applied example model (4) estimated a strong negative correlation of −0.87 between
the pair of random effects, which had a strong influence on the results. furthermore, in our simulation study we knew
that a normal distribution on the baseline risk was appropriate (as we simulated the ipd from this assumption). however,
in this real example we did not know if such an assumption is correct, and it may even compromise randomization in
each trial, especially given the sparse events in the included trials. the approach of stratified intercept model (4) avoids
making any assumptions about the between-study distribution of baseline risk, or the between-study relationship between
baseline risk and treatment effect.
4.2 example 2: diet and lifestyle interventions and health outcomes during pregnancy
in our second example, we used ipd from 36 randomized trials (12 477 women) evaluating the effect of diet and lifestyle
interventions compared with control (usual care) on health outcomes during pregnancy.24 we focused on a subset of 10
trials that recorded the binary outcome of large for gestational age (yes/no). eight studies had approximately 1:1 treatment:control allocation, and the other two had a 2:1 allocation ratio. the outcome risk in the control group varied, but
was about 15% on average, similar to that used in our simulation studies. although the number of participants was reasonably large in most studies, often there are fewer than 10 events in each group (table 4), which again raised doubt as
to the suitability of a traditional two-stage approach.
ml and reml estimates for one-stage model (3) are shown in table 5. the findings again mirror those of the simulation study. when using ml estimation for model (3), the estimated between-study variance was much larger when
using “study-specific centering” (𝜏̂2 = 0.42) rather than 1/0 coding (𝜏̂2 = 0.29) of the treatment variable, and this led
to wider confidence intervals for the summary treatment effect. reml estimation of the pseudo likelihood was quite
stable, with more similar estimates for first- and second-order linearizations of the likelihood. the reml estimates of
between-study variances were larger than the ml estimates (table 5), and this widened confidence intervals for the summary treatment effect. results for model (4) were again somewhat different to model (3), for the same reasons described
in the previous example. for all models, the widest confidence intervals arose when using the t-based rather than z-based
approach.
5 discussion
our simulation study and applied examples identify key findings for estimation of one-stage ipd meta-analysis models,
which are summarized in box 1. there are three major implications. first, for ml or reml estimation of stratified intercept or random intercepts models, z-based (wald) confidence intervals for the summary treatment effect are generally
too narrow; performance is generally improved by using the satterthwaite (or kenward-roger) approaches for continuous outcomes,4,14 or a t-based approach with k−1 degrees of freedom for binary outcomes. second, when using ml
table 4 summary of the ipd
from 10 trials examining the effect of
diet and lifestyle interventions on
large for gestational age
number of women number of babies large for gestational age
study control treatment control treatment
1 120 109 23 22
2 37 33 5 7
3 68 72 7 2
4 33 34 1 2
5 143 136 7 2
6 63 134 5 11
7 1095 1104 154 132
8 47 46 27 5
9 65 130 16 22
10 50 51 11 14
abbreviation: ipd, individual participant data.
table 5 results from maximum likelihood (ml) and restricted maximum likelihood (reml) estimation of models
(3) and (4) when fitted to the individual participant data summarized in table 4
model
estimation
method
treatment
coding
summary
treatment
effect, 𝜽̂ 95% ci:z-based 95% ci:t-based
between-study
(co)variances
stratified
intercept
model (3)
ml exact 1/0 −0.43 −0.89, 0.04 −0.96, 0.11 𝜏̂2 = 0.29
ml exact “study-specific
centering”
−0.40 −0.92, 0.12 −1.00, 0.20 𝜏̂2 = 0.42
reml pseudo 1/0 −0.48 −1.06, 0.09 −1.14, 0.18 𝜏̂2 = 0.54
random
intercepts
model (4)
ml exact 1/0 −0.38 −0.91, 0.16 −1.00, 0.24 𝜏̂2 = 0.43
𝜏12 = −0.29
𝜏2
𝛼 = 0.81
reml pseudo 1/0 −0.38 −0.94, 0.18 −1.03, 0.27 𝜏̂2 = 0.54
𝜏12 = −0.36
𝜏2
𝛼 = 0.92
note: ml estimates-based numerical quadrature of the exact likelihood (with seven quadrature points), and reml estimates based on a
second-order penalized quasi-likelihood linearization with the estimates from the first-order marginal quasi-likelihood linearization used as
initial values.
estimation of a one-stage model with a stratified intercept, a “study-specific centering” coding of the treatment variable
should be chosen, as this reduces the bias in the between-study variance estimate (compared with 1/0 and other coding
options). third, reml estimation reduces downward bias in between-study variance estimates compared with ml estimation, and does not depend on the treatment coding chosen, thus should be used where possible; for ipd meta-analyses
of binary outcomes, this requires reml estimation of the pseudo-likelihood, although it may be unstable when data are
sparse.
5.1 reml vs ml estimation
our simulations of continuous outcomes show that reml is better than ml estimation to improve variance estimates,
which will not be a surprise to most readers. in particular, reml reduces the bias in 𝜏̂2 by adjusting for the total number
of parameters being estimated.6,25,26 however, reml is not an option when using numerical integration of the exact likelihood for a one-stage ipd meta-analysis of a binary outcome, and therefore ml estimation is the most common method
used. in most software packages the default estimation method to fit generalized linear mixed models is ml estimation
via a numerical integration method such as quadrature. therefore, our findings about the importance of “study-specific
centering” coding are most relevant for one-stage ipd meta-analyses of binary outcomes, or other generalized linear
mixed models or frailty models that apply ml estimation. indeed, improving ml estimation by centering covariates has
a reml essence to it, as both approaches aim to disentangle (ie, make uncorrelated) the estimation of main parameters
of interest from other nuisance parameters.
given that considerable downward bias in between-study variance estimates often occurs using ml estimation (even
after “study-specific” treatment coding; see figures 1 to 3), reml estimation of the pseudo likelihood is appealing for
binary outcomes.17 our simulations do not suggest an important difference between reml and ml in terms of bias of the
summary treatment effect estimate and coverage of t-based confidence intervals for the summary treatment effect. however, in most scenarios reml estimation did reduce the median downward bias in the between-study variance estimates,
and therefore we suggest it is the default. however, caution is advised if the data are sparse, such that most studies are small
and have few or even zero events. our simulations did not cover scenarios with sparse data, but previous work suggests
that reml estimation of pseudo likelihood is not accurate in such situations and ml estimation of the exact likelihood
is preferred.27 indeed, reml estimates may be unstable in such situations. instability is evident when first-order and
second-order linearizations of the likelihood lead to very different parameter estimates, or when reparametizations that
should not affect reml (such as centering of covariates) do still change parameter estimates importantly. in our applied
example using the trials in table 2, the data were sparse, and these stability problems were evident when using reml
estimation, and so the ml estimates with “study-specific centering” were deemed more reliable. a bayesian approach
could also be considered in such situations, which would retain the exact likelihood during parameter estimation and
could be combined with empirically based prior distributions for the between-study variance.28,29
our findings warrant further evaluation in a wider variety of settings than those considered in our simulation, but
concur with related work,30 including piepho et al31 who evaluate frequentist network meta-analysis of binary outcomes.
they too show that reml estimation of the pseudo-likelihood, and also the use of the h-likelihood, reduce bias in
between-study variance estimates and give satisfactory coverage rates, especially when the kenward-roger approach is
used to derive confidence intervals. they also consider improving ml estimation by various reparameterizations of the
exact likelihood that aim to mimic the reml approach for linear mixed models. these reparameterizations also reduce
the downward bias in ml estimates, but coverage of summary treatment effects is often too low. our “study-specific centering of covariates” approach showed suitable coverage when using the t-based approach to confidence intervals, and is
potentially simpler to implement in existing software. however, formal comparison of our proposal with those of piepho
et al is needed. thomas et al also compare the performance of one-stage ipd meta-analyses for binary outcomes,30 but do
not find a “meaningful difference” between results from reml of the pseudo likelihood and ml of the exact likelihood.
however, the authors only considered 1:1 treatment:control allocations and implemented a +0.5/−0.5 treatment variable
coding, and thus essentially adapted “study-specific centering” in their setting, which ensures ml estimation performs
well. still, their bias in between-study variances were generally lower using reml, akin to our findings. coverage of
confidence intervals was generally much lower than 95%, but were based on a z-based approach.
comparisons to the traditional two-stage ipd meta-analysis approach are also needed, for which reml is often recommended in the second stage.32 although it assumes normality of the between-study variance of treatment effects,
reml is quite robust to deviations from this assumption.33 we suspect that situations where reml estimation of the
pseudo-likelihood performs well for a one-stage analysis of binary outcomes, a two-stage approach using reml will also
perform well. thomas et al30 recommend one-stage rather than two-stage analyses when data in the ipd meta-analysis
are sparse. we agree with langan et al32 that, especially in ipd meta-analyses of few studies, any heterogeneity variance
estimate “should not be used as a reliable gauge for the extent of heterogeneity in a meta-analysis”.
5.2 implications of our findings
our findings have important consequences, as they allow researchers to apply one-stage ipd meta-analysis models with a
stratified intercept, rather than random intercepts, when using either reml or ml estimation. this is important, as the use
of random intercepts makes distributional assumptions and potentially compromises within-trial randomization, but this
is avoided using a stratified intercept. previously, we recommended random intercepts for one-stage ipd meta-analysis
models fitted using ml estimation,4 as this approach had better estimates of between-study variances due to reducing
the number of parameters. however, our simulations show that the “study-specific centering” makes the stratified intercept model comparable to the random intercepts model (when no borrowing of information across studies is allowed in
control group risk). such comparable performance is despite the data generating mechanism actually being based on the
random intercepts model, and so the simulation set-up might be considered more favorable toward the random intercepts
model.
a potential limitation of stratified intercept models is that they may fail to converge when the number of events are
rare, and in particular when some studies have a zero event in the control group (although this was not an issue for our
first applied example). in that situation, assuming random study intercepts may help, because study-specific intercepts
are not estimated directly, and studies rather contribute toward the estimation of the between-study distribution of intercepts (with the caveat of sharing information about control group risk across trials and thus potentially compromising
randomization within trials). if between-study correlation is included in such random intercepts models (ie, model (4)
is used), then the coding of treatment should not matter. however, if between-study correlation is assumed zero, then
a +0.5/−0.5 coding is recommended. an alternative method is the hypergeometric-normal approach of stijnen et al34
(referred to as model (7) in jackson et al5), which conditions out the study-specific intercepts (thus avoiding their estimation); in the scenarios of the jackson et al simulation study, this approach performs well and comparable to using a
stratified intercept with “study-specific centering”.
5.3 extensions
although we focused on randomized trials, our findings also apply more generally. in particular, any one-stage ipd
meta-analysis model fitted using ml estimation should include covariates (treatments, prognostic factors, adjustment
factors, and so on) centered by their study-specific means; for example, when synthesizing ipd from observational
studies to evaluate risk or prognostic factors for binary or survival outcomes,35 the included factors and any adjustment covariates should be coded with “study-specific centering”. indeed, exposure prevalence (eg, the proportion
of individuals classed as biomarker positive) is likely to be more varied across included covariates in observational
studies (than treatment prevalence in randomized trials), and thus “study-specific centering” will be even more
important.
we focused on parallel-group trials, for which our “study-specific centering” approach centers by the proportion in
the treatment group; equivalently, we could center around the proportion in the control group. we considered binary
variables, but “study-specific centering” should also be used for continuous variables where they are centered by the
mean value. indeed, it generalizes to any covariate: we simply center at the covariate's mean value in each study. for
example, for an ordinal covariate with possible values of 0, 1, 2, and 3, the “study-specific centering” coding is the
original value minus the mean value for all individuals in the same study. in our simulations, we assumed a uniform distribution or fixed treatment prevalences across studies. similarly we generated control groups risks assuming
a normal distribution, and did not allow any correlation between baseline risk and treatment effect. other approaches
could be considered for data generation in further work. we also only consider one treatment and one control group
per study.
our simulations did not allow the between-study correlation to be estimated when fitting the random intercepts models (2) or (4), as we forced the correlation to be zero as it was in the data generating model. further research might also
consider how the random intercepts models perform when the correlation is freely estimated, although related simulations have shown that between-study correlations are difficult to estimate reliably, and often estimated values are +1 or
−1.19
6 conclusions
we recommend one-stage ipd meta-analysis models for continuous or binary outcomes use a stratified intercept. when
using ml estimation to fit such models, researchers should use a “study-specific centering” coding of included variables.
this will improve estimation of between-study variances and give more appropriate coverage of 95% confidence intervals
for the summary (treatment) effects of interest. for continuous outcomes, reml estimation is recommended and then
the coding should not be important. for binary outcomes, reml estimation of the pseudo likelihood will often improve
upon ml estimation of the exact likelihood, although it may be unstable when data are sparse. for either ml or reml
estimation, confidence intervals should be derived using an approach based on the t-distribution.

<|EndOfText|>

the value of preseason screening for injury prediction: the development and internal validation of a multivariable prognostic model to predict indirect muscle injury risk in elite football (soccer) players

abstract
background
in elite football (soccer), periodic health examination (phe) could provide prognostic factors to predict injury risk.

objective
to develop and internally validate a prognostic model to predict individualised indirect (non-contact) muscle injury (imi) risk during a season in elite footballers, only using phe-derived candidate prognostic factors.

methods
routinely collected preseason phe and injury data were used from 152 players over 5 seasons (1st july 2013 to 19th may 2018). ten candidate prognostic factors (12 parameters) were included in model development. multiple imputation was used to handle missing values. the outcome was any time-loss, index indirect muscle injury (i-imi) affecting the lower extremity. a full logistic regression model was fitted, and a parsimonious model developed using backward-selection to remove factors that exceeded a threshold that was equivalent to akaike’s information criterion (alpha 0.157). predictive performance was assessed through calibration, discrimination and decision-curve analysis, averaged across all imputed datasets. the model was internally validated using bootstrapping and adjusted for overfitting.

results
during 317 participant-seasons, 138 i-imis were recorded. the parsimonious model included only age and frequency of previous imis; apparent calibration was perfect, but discrimination was modest (c-index = 0.641, 95% confidence interval (ci) = 0.580 to 0.703), with clinical utility evident between risk thresholds of 37–71%. after validation and overfitting adjustment, performance deteriorated (c-index = 0.589 (95% ci = 0.528 to 0.651); calibration-in-the-large = − 0.009 (95% ci = − 0.239 to 0.239); calibration slope = 0.718 (95% ci = 0.275 to 1.161)).

conclusion
the selected phe data were insufficient prognostic factors from which to develop a useful model for predicting imi risk in elite footballers. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.

trial registration
nct03782389

key points
factors measured through preseason screening generally have weak prognostic strength for future indirect muscle injuries, and further research is needed to identify novel, robust prognostic factors.

because of sample size restrictions and until the evidence base improves, it is likely that any further attempts at creating a prognostic model at individual club level would also suffer from poor performance.

the value of using preseason screening data to make injury predictions or to select bespoke injury prevention strategies remains to be demonstrated, so screening should only be considered as useful for detection of salient pathology or for rehabilitation/performance monitoring purposes at this time.

background
in elite football (soccer), indirect (non-contact) muscle injuries (imis) predominantly affect the lower extremities and account for 30.3 to 47.9% of all injuries that result in time lost to training or competition [1,2,3,4,5]. reduced player availability negatively impacts upon medical [6] and financial resources [7, 8] and has implications for team performance [9]. therefore, injury prevention strategies are important to professional teams [9].

periodic health examination (phe), or screening, is a key component of injury prevention practice in elite sport [10]. specifically, in elite football, phe is used by 94% of teams and consists of medical, musculoskeletal, functional and performance tests that are typically evaluated during preseason and in-season periods [11]. phe has a rehabilitation and performance monitoring function [12] and is also used to detect musculoskeletal or medical conditions that may be dangerous or performance limiting [13]. another perceived role of phe is to recognise and manage factors that may increase, or predict, an athlete’s future injury risk [10], although this function is currently unsubstantiated [13].

phe-derived variables associated with particular injury outcomes (such as imis) are called prognostic factors [14], which can be used to identify risk differences between players within a team [12]. single prognostic factors are unlikely to satisfactorily predict an individual’s injury risk if used independently [15]. however, several factors could be combined in a multivariable prognostic prediction model to offer more accurate personalised risk estimates for the occurrence of a future event or injury [15, 16]. such models could be used to identify high-risk individuals who may require an intervention that is designed to reduce risk [17], thus assisting decisions in clinical practice [18]. despite the potential benefits of using prognostic models for injury risk prediction, we are unaware of any that have been developed using phe data in elite football [19].

therefore, the aim of this study was to develop and internally validate a prognostic model to predict individualised imi risk during a season in elite footballers, using a set of candidate prognostic factors derived from preseason phe data.

methods
the methods have been described in a published protocol [20] so will only be briefly outlined. this study has been registered on clinicaltrials.gov (identifier: nct03782389) and is reported according to the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) statement [21, 22].

data sources
this study was a retrospective cohort design. eligible participants were identified from a population of male elite footballers, aged 16–40 years old at manchester united football club. a dataset was created using routinely collected injury and preseason phe data over 5 seasons (1st july 2013 to 19th may 2018). for each season, which started on 1st july, participants completed a mandatory phe during week 1 and were followed up to the final first team game of the season. if eligible participants were injured at the time of phe, a risk assessment was completed by medical staff. only tests that were appropriate and safe for the participant’s condition were completed; examiners were not blinded to injury status.

participants and eligibility criteria
during any season, participants were eligible if they (1) were not a goalkeeper and (2) participated in phe for the relevant season. participants were excluded if they were not contracted to the club for the forthcoming season at the time of phe.

ethics and data use
informed consent was not required as data were captured from the mandatory phe completed through the participants’ employment. the data usage was approved by the club and university of manchester research ethics service.

outcome
the outcome was any time-loss, index imi (i-imi) of the lower extremity. that is, any i-imi sustained by a participant during matches or training, which affected lower abdominal, hip, thigh, calf or foot muscle groups and prohibited future football participation [23]. i-imis were graded by a club doctor or physiotherapist according to the validated munich consensus statement for the classification of muscle injuries in sport [24, 25], during routine assessments undertaken within 24 h of injury. these healthcare professionals were not blinded to phe data.

sample size
we allowed a maximum of one candidate prognostic factor parameter per 10 i-imis, which at the time of protocol development, was the main recommendation to minimise overfitting (additional file 1) [20, 26]. the whole dataset was used for model development and internal validation, which agrees with methodological recommendations [27].

candidate prognostic factors
the available dataset contained 60 candidate factors [20]. because of the sample size considerations, before any analysis, the set of candidate factors was reduced. initially, an audit was conducted to quantify missing values and to determine the measurement reliability of the eligible candidate factors [20]. any candidate factors which had greater than 15% missing data or where reliability was classed as fair to poor (intraclass correlation coefficient < 0.70) were excluded [20] (additional file 2). of the remaining 45 eligible factors, previous evidence of prognostic value [19] and clinical reasoning were used to select candidate prognostic factors suitable for inclusion [20]. this process left a final set of 10 candidate factors, represented by 12 model parameters (table 1). the 35 factors that were not included in model development are also listed in additional file 2, and will be utilised in a related, forthcoming exploratory study which aims to examine their association with indirect muscle injuries in elite football players.

table 1 set of candidate prognostic factors (with corresponding number of parameters) for model development
full size table
statistical analysis
data handling—outcome measures
each participant-season was treated as independent. participants who sustained an i-imi were no longer considered at risk for that season and were included for further analysis at the start of the next season if still eligible. any upper limb imi, trunk imi or non-imi injuries were ignored, and participants were still considered at risk.

eligible participants who were loaned to another club throughout that season, but had not sustained an i-imi prior to the loan, were still considered at risk. i-imis that occurred whilst on loan were included for analysis, as above. permanently transferred participants (who had not sustained an i-imi prior to leaving) were recorded as not having an i-imi during the relevant season and exited the cohort at the season end.

data handling—missing data
missing values were assumed to be missing at random [20]. the continuous parameters generally demonstrated non-normal distributions, so were transformed using normal scores [35] to approximate normality before imputation, and back-transformed following imputation [36]. multivariate normal multiple imputation was performed, using a model that included all candidates and i-imi outcomes. fifty imputed datasets were created in stata 15.1 (statacorp llc, texas, usa) and analysed using the mim module.

prognostic model development
continuous parameters were retained on their original scales, and their effects assumed linear [22]. a full multivariable logistic regression model was constructed, which contained all 12 parameters. parameter estimates were combined across imputed datasets using rubin’s rules [37]. to develop a parsimonious model that would be easier to utilise in practice, backward variable selection was performed using estimates pooled across the imputed datasets at each stage of the selection procedure to successively remove non-significant factors with p values > 0.157. this threshold was selected to approximate equivalence with akaike’s information criterion [38, 39]. multiple parameters representing the same candidate factor were tested together so that the whole factor was either retained or removed. candidate interactions were not examined, and no terms were forced into the model. all analyses were conducted in stata 15.1.

assessment of model performance
the full and parsimonious models were used to predict i-imi risk over a season, for every participant-season in all imputed datasets. for all performance measures, each model’s apparent performance was assessed in each imputed dataset and then averaged across all imputed datasets using rubin’s rules [37]. discrimination determines a model’s ability to differentiate between participants who have experienced an outcome compared to those who have not [40], quantified using the concordance index (c-index). this is equivalent to the area under the receiver operating characteristic (roc) curve for logistic regression, where 1 demonstrates perfect discrimination, whilst 0.5 indicates that discrimination is no better than chance [41].

calibration determines the agreement between the model’s predicted outcome risks and those observed [42], evaluated using an apparent calibration plot in each imputed dataset. all predicted risks were divided into ten groups defined by tenths of predicted risk. the mean predicted risks for the groups were plotted against the observed group outcome proportions with corresponding 95% confidence intervals (cis). a loess smoothing algorithm showed calibration across the range of predicted values [43]. for grouped and smoothed data points, perfect predictions lie on the 45° line (i.e. a slope of 1).

the systematic (mean) error in model predictions was quantified using calibration-in-the-large (citl), which has an ideal value of 0 [40, 42], and the expected/observed (e/o) statistic, which is the ratio of the mean predicted risk against the mean observed risk (ideal value of 1) [40, 42]. the degree of over or underfitting was determined using the calibration slope, where a value of 1 equals perfect calibration on average across the entire range of predicted risks [22]. nagelkerke’s pseudo-r2 was also calculated, which quantifies the overall model fit, with a range of 0 (no variation explained) to 1 (all variation explained) [44].

assessment of clinical utility
decision-curve analysis was used to assess the parsimonious model’s apparent clinical usefulness in terms of net benefit (nb) if used to allocate possible preventative interventions. this assumed that the model’s predicted risks were classed as positive (i.e. may require a preventative intervention) if greater than a chosen risk threshold, and negative otherwise. nb is then the difference between the proportion of true positives and false positives, where both were weighted by the odds of the chosen risk threshold and also divided by the sample size [45]. positive nb values suggest the model is beneficial compared to treating none, which has no benefit to the team but with no negative cost and efficiency implications. the maximum possible nb value is the proportion with the outcome in the dataset.

the model’s nb was also compared to the nb of delivering an intervention to all individuals. this is considered a treat-all strategy, offering maximum benefit to the team, but with maximum negative cost and efficiency implications [17]. a model has potential clinical value if it demonstrates higher nb than the default strategies over the range of risk thresholds which could be considered as high risk in practice [46].

internal validation and adjustment for overfitting
to examine overfitting, the parsimonious model was internally validated using 200 bootstrap samples, drawn from the original dataset with replacement. in each sample, the complete model-building procedure (including multiple imputation, backward variable selection and performance assessment) was conducted as described earlier. the difference in apparent performance (of a bootstrap model in its bootstrap sample) and test performance (of the bootstrap model in the original dataset) was averaged across all samples. this generated optimism estimates for the calibration slope, citl and c-index statistics. these were subtracted from the original apparent calibration slope, citl and c-index statistics to obtain final optimism-adjusted performance estimates. the nagelkerke r2 was adjusted using a relative reduction equivalent to the relative reduction in the calibration slope.

to produce a final model adjusted for overfitting, the regression coefficients produced in the parsimonious model were multiplied by the optimism-adjusted calibration slope (also termed a uniform shrinkage factor), to adjust (or shrink) for overfitting [47]. finally, the citl (also termed model intercept) was then re-estimated to give the final model, suitable for evaluation in other populations or datasets.

complete case and sensitivity analyses
to determine the effect of multiple imputation and player transfer assumptions on model stability, the model development process was repeated: (1) as a complete case analysis and (2) as sensitivity analyses which excluded all participant-seasons where participants had not experienced an i-imi up to the point of loan or transfer, which were performed as both multiple imputation and complete case analyses.

results
participants
during the five seasons, 134 participants were included, contributing 317 participant-seasons and 138 imis in the primary analyses (fig. 1). three players were classified as injured when they took part in phe (which affected three participant-seasons). this meant they were unavailable for full training or to play matches at that time. however, these players had commenced football specific, field-based rehabilitation around this time, so also had similar exposure to training activities as the uninjured players. as such, these players were included in the cohort because it was reasonable to assume that they could also be considered at risk of an i-imi event even during their rehabilitation activities.

fig. 1
figure1
participant flow chart. key: n = participants; i-imi = index indirect muscle injury

full size image
table 2 describes the frequency of included participant-seasons, and the frequency and proportion of recorded i-imi outcomes across all five seasons. for the sensitivity analyses (excluding loans and transfers), 260 independent participant-seasons with 129 imis were included; 36 participants were transferred on loan, whilst 14 participants were permanently transferred during a season, which excluded 57 participant-seasons in total (fig. 1). table 2 also describes the frequency of excluded participant-seasons where players were transferred either permanently or on loan, across the 5 seasons.

table 2 frequency of included participant-seasons, i-imi outcomes and participant-seasons affected by transfers, per season (primary analysis)
full size table
table 3 shows anthropometric and all prognostic factor characteristics for participants included in the primary analyses. these were similar to those included in the sensitivity analyses (additional file 3).

table 3 characteristics of included participants in the primary analysis
full size table
missing data and multiple imputation
all i-imi, age and previous muscle injury data were complete (table 3). for all other candidates, missing data ranged from 6.31 (for hip internal and external rotation difference) to 13.25% for countermovement jump (cmj) power (table 3). the distribution of imputed values approximated observed values (additional file 4), confirming their plausibility.

model development
table 4 shows the parameter estimates for the full model and parsimonious model after variable selection (averaged across imputations).

table 4 results of the full and parsimonious multivariable logistic regression models, with prediction formulae
full size table
for both models, only age and frequency of previous imis had a statistically significant (but modest) association with increased i-imi risk (p < 0.157). no clear evidence for an association was observed for any other candidate factor.

model performance assessment and clinical utility
table 4 shows the apparent performance measures for the full and parsimonious models, all of which were similar. figure 2 shows the apparent calibration of the parsimonious model in the dataset used to develop the model (i.e. before adjustment for overfitting). these were identical across all imputed datasets because the retained prognostic factors contained no missing values. the parsimonious model had perfect apparent overall citl and calibration slope by definition, but calibration was more variable around the 45° line between the expected risk ranges of 28 to 54%. discrimination was similarly modest for the full (c-index = 0.670, 95% ci = 0.609 to 0.731) and parsimonious models (c-index = 0.641, 95% ci = 0.580–0.703). the apparent overall model fit was low for both models, indicated by nagelkerke r2 values of 0.120 for the full model and 0.089 for the parsimonious model.

fig. 2
figure2
apparent calibration of the parsimonious model (before adjustment for overfitting). key: e:o = expected to observed ratio; ci = confidence interval; i-imi = index indirect muscle injury

full size image
figure 3 displays the decision-curve analysis. the nb of the parsimonious model was comparable to the treat-all strategy at risk thresholds up to 31%, marginally greater between 32 and 36% and exceeded the nb of either default strategies between 37 and 71%.

fig. 3
figure3
decision curve analysis for the parsimonious model (before adjustment for overfitting)

full size image
internal validation and adjustment for overfitting
table 4 shows the optimism-adjusted performance statistics for the parsimonious model, with full internal validation results shown in additional file 9. after adjustment for optimism, the overall model fit and the model’s discrimination performance deteriorated (nagelkerke r2 = 0.064; c-index = 0.589 (95% ci = 0.528 to 0.651). furthermore, bootstrapping suggested the model would be severely overfitted in new data (calibration slope = 0.718 (95% ci = 0.275 to 1.161)), so a shrinkage factor of 0.718 was applied to the parsimonious parameter estimates, and the model intercept re-estimated to produce our final model (table 4).

complete case and sensitivity analyses
the full and parsimonious models were robust to complete case analyses and excluding loans and transfers, with comparable apparent performance estimates. for the full models, the c-index range was 0.675 to 0.705, and nagelkerke r2 range was 0.135 to 0.178, whilst for the parsimonious models, the c-index range was 0.632 to 0.691, and nagelkerke r2 range was 0.102 to 0.154 (additional files 5, 6, 7, 8 and 9). the same prognostic factors were selected in all parsimonious models. the degree of estimated overfitting observed in the complete case and sensitivity analyses was comparable to that observed in the main analysis (calibration slope range = 0.678 to 0.715) (additional files 5, 6, 7, 8 and 9).

discussion
we have developed and internally validated a multivariable prognostic model to predict individualised i-imi risk during a season in elite footballers, using routinely, prospectively collected preseason phe and injury data that was available at manchester united football club. this is the only study that we know of that has developed a prognostic model for this purpose, so the results cannot be compared to previous work.

we included both a full model which did not include variable selection and a parsimonious model, which included a subset of variables that were statistically significant. the full model was included because overfitting is likely to increase when variable inclusion decisions are based upon p values. in addition, the use of p value thresholds for variable selection is somewhat arbitrary. however, the overfitting that could have arisen in the parsimonious model after using p values in this way was accounted for during the bootstrapping process, which replicated the variable selection strategy based on p values in each bootstrap sample.

the performance of the full and parsimonious models was similar, which means that utilising all candidate factors offered very little advantage over using two for making predictions. indeed, variable selection eliminated 8 candidate prognostic factors that had no clear evidence for an association with i-imis. our findings confirm previous suggestions that phe tests designed to measure modifiable physical and performance characteristics typically offer poor predictive value [10]. this may be because unless particularly strong associations are observed between a phe test and injury outcome, the overlap in scores between individuals who sustain a future injury and those who do not results in poor discrimination [10]. additionally, after measurement at a single timepoint (i.e. preseason), it is likely that the prognostic value of these modifiable factors may vary over time [48] due to training exposure, environmental adaptations and the occurrence of injuries [49].

the variable selection process resulted in a model which included only age and the frequency of previous imis within the last 3 years, which are simple to measure and routinely available in practice. our findings were similar to the modest association previously observed between age and hamstring imis in elite players [19]. however, whilst a positive previous hamstring imi history has a confirmed association with future hamstring imis [19], we found that for lower extremity i-imis, cumulative imi frequency was preferred to the time proximity of any previous imi as a multivariable prognostic factor. nevertheless, the weak prognostic strength of these factors explains the parsimonious model’s poor discrimination and low potential for clinical utility.

our study is the first to utilise decision-curve analysis to examine the clinical usefulness of a model for identifying players at high risk of imis and who may benefit from preventative interventions such as training load management, strength and conditioning or physiotherapy programmes. our parsimonious model demonstrated no clinical value at risk thresholds of less than 36%, because its nb was comparable to that of providing all players with an intervention. indeed, the only clinically useful thresholds that would indicate a high-risk player would be 37–71%, where the model’s nb was greater than giving all players an intervention. however, because of the high baseline imi risk in our population (approximately 44% of participant-seasons affected), the burden of imis [1,2,3,4,5] and the minimal costs [10] versus the potential benefits of such preventative interventions in an elite club setting, these thresholds are likely to be too high to be acceptable in practice. accordingly, it would be inappropriate to allocate or withhold interventions based upon our model’s predictions.

because of severe overfitting our parsimonious model was optimistic, which means that if used with new players, prediction performance is likely to be worse [39]. although our model was adjusted to account for overfitting and hence improve its calibration performance in new datasets, given the limitations in performance and clinical value, we cannot recommend that it is validated externally or used in clinical practice.

this study has some limitations. we acknowledge that the development of our model does not formally take account of the use of existing injury prevention strategies, including those informed by phe, and their potential effects on the outcome. rather, we predicted i-imis under typical training and match exposure and under routine medical care. in addition, it should be noted that injury risk predictions at an elite level football club may not generalise to other types of football clubs or sporting institutions, where ongoing injury prevention strategies may not be comparable in terms of application and equipment.

we measured candidate factors at one timepoint each season and assumed that participant-seasons were independent. whilst statistically complex, future studies may improve predictive performance and external validity by harnessing longitudinal measurements and incorporating between-season correlations.

we did not perform a competing risks analysis to account for players not being exposed to training and match play due to injuries other than i-imis. that is, our approach predicted the risk of i-imis in the follow up of players, allowing other injury types to occur and therefore possibly limiting the opportunity for i-imis during any rehabilitation period. the competing risk of the occurrence of non-imis was therefore not explicitly modelled and players remained in the risk set after a non-imi had occurred.

we also merged all lower extremity i-imis rather than using specific muscle group outcomes. although less clinically meaningful, this was necessary to maximise statistical power. nevertheless, our limited sample size prohibited examination of complex non-linear associations and only permitted a small number of candidates to be considered. a lack of known prognostic factors [19] meant that selection was mainly guided by data quality control processes and clinical reasoning, so it is possible that important factors were not included.

risk prediction improves when multiple factors with strong prognostic value are used [15]. therefore, future research should aim to identify novel prognostic factors, so that these can be used to develop models with greater potential clinical benefit. this may also allow updating of our model to improve its performance and clinical utility [50].

until the evidence base improves, and because of sample size limitations, it is likely that any further attempts to create a prognostic model at individual club level would suffer similar issues. importantly, this means that for any team, the value of using preseason phe data to make individualised predictions or to select bespoke injury prevention strategies remains to be demonstrated. however, the pooling of individual participant data from several participating clubs may increase sample sizes sufficiently to allow further model development studies [51], where a greater number of candidate factors could be utilised.

conclusion
using phe and injury data available preseason, we have developed and internally validated a prognostic model to predict i-imi risk in players at an elite club, using current methodological best practice. the paucity of known prognostic factors and data requirements for model building severely limited the model’s performance and clinical utility, so it cannot be recommended for external validation or use in practice. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.

<|EndOfText|>

temporal recalibration for improving prognostic model development and risk predictions in settings where survival is improving over time 

abstract
background
prognostic models are typically developed in studies covering long time periods. however, if more recent years have seen improvements in survival, then using the full dataset may lead to out-of-date survival predictions. period analysis addresses this by developing the model in a subset of the data from a recent time window, but results in a reduction of sample size.

methods
we propose a new approach, called temporal recalibration, to combine the advantages of period analysis and full cohort analysis. this approach develops a model in the entire dataset and then recalibrates the baseline survival using a period analysis sample.

the approaches are demonstrated utilizing a prognostic model in colon cancer built using both cox proportional hazards and flexible parametric survival models with data from 1996–2005 from the surveillance, epidemiology, and end results (seer) program database. comparison of model predictions with observed survival estimates were made for new patients subsequently diagnosed in 2006 and followed-up until 2015.

results
period analysis and temporal recalibration provided more up-to-date survival predictions that more closely matched observed survival in subsequent data than the standard full cohort models. in addition, temporal recalibration provided more precise estimates of predictor effects.

conclusion
prognostic models are typically developed using a full cohort analysis that can result in out-of-date long-term survival estimates when survival has improved in recent years. temporal recalibration is a simple method to address this, which can be used when developing and updating prognostic models to ensure survival predictions are more closely calibrated with the observed survival of individuals diagnosed subsequently.

prognostic models, temporal recalibration, period analysis, up-to-date survival predictions, flexible parametric survival models, cox proportional hazards models
topic: patient prognosisdatasets
issue section: original article
key messages
if survival has been improving over time, standard full cohort models can under-estimate survival.

period analysis uses a more recent subset of data to produce survival estimates which are more up-to-date, however it reduces the sample size and number of events used in the analysis.

temporal recalibration combines the sample size advantages associated with full cohort analysis with the up-to-date estimates produced with period analysis.

temporal recalibration can be used at the model development stage or to update existing prognostic models when new data becomes available.

introduction
for individuals diagnosed with a particular disease or health condition, prognostic models can provide outcome predictions and aid treatment decisions.1,2 in this article, we focus on the outcome of time-until-death from colon cancer and survival predictions, however the approach can be generalized. prognostic models contain multiple predictors and are typically developed using a regression format such as logistic, cox or a parametric survival model. it is often of interest to provide survival predictions at different time points, such as 1, 5 and 10 years after diagnosis. for 10-year predictions, it is necessary to have a model development dataset that includes individuals who were diagnosed at least 10 years ago, such that the analysis has sufficient follow-up length. however, this can lead to out-of-date (miscalibrated) survival predictions for recently diagnosed individuals if there have been improvements in survival over calendar time: e.g. in recent years treatment may have improved survival compared with 5 or 10 years earlier. improvements in survival for colorectal cancer have been reported in a number of different countries.3–6

with the development of online tools and apps, survival estimates from prognostic models have become more accessible. some models such as predict, a prognostic model for breast cancer,7 and qcancer, a prognostic model for colorectal cancer,8 are freely available online for both clinicians and the public. the survival estimates produced from these, and many other webtools, are from a standard full cohort analysis approach. such models may produce survival predictions that under-estimate the true survival probability of recently diagnosed patients (and conversely over-estimate the actual risk of adverse outcomes).

period analysis has been used in population-based cancer studies to obtain up-to-date estimates of survival9–12 and in this article we explore its use in the development and updating of prognostic models. period analysis defines a recent time window and only the risk-time and events that fall within this window contribute to the estimates of the hazard rates and predictor effects.13 this method is not commonly used for prognostic models, however keogh et al.14 produced survival predictions for cystic fibrosis patients using period analysis. a disadvantage with period analysis is that it results in a reduction of sample size for model development. this could be particularly problematic in small datasets, when there are rare predictor patterns or rare events, and may lead to a low number of events per predictor parameter, which increases the potential for model overfitting.15

in this article we introduce a new approach, called temporal recalibration, that combines the use of full cohort analysis, period analysis and recalibration methods. specifically it aims to maximize the use of data toward model development, with the full dataset used to model predictor effects and the baseline survival recalibrated in a recent time window to produce more up-to-date survival predictions for new individuals. we illustrate and compare these methods using an example of colon cancer from the surveillance, epidemiology, and end results (seer) program database.16

methods
cox proportional hazards models and post-estimation of the baseline
cox proportional hazards (ph) models are frequently used to develop prognostic models.17 the model is of the form:  
h(t;xi)=h0(t)eβxi 
with h(t;xi) the hazard function, h0(t) the baseline hazard function and βxi the prognostic index.18
the cumulative hazard function h(t;xi) must be approximated to calculate survival predictions as it is not directly modelled. this can be achieved post-estimation using a non-parametric approach, or by a smoother using fractional polynomials or splines.7,19 in this article, restricted cubic splines are used to create a smooth approximation of the log cumulative baseline hazard post-estimation. the same knot locations as the flexible parametric survival models (fpms) (see supplementary file 1, available as supplementary data at ije online) were used to ensure a fair comparison. the baseline survival curve was approximated by sˆ0(t)=e−h0ˆ(t)⁠, and survival predictions for individuals with different values of the prognostic index by sˆ(t;xi)=sˆ0(t)eβˆxi⁠.

it is possible to extend these models to include time-dependent predictor effects (i.e. non-proportional hazards). period analysis20 (see the period analysis section) can be performed using delayed entry techniques.

flexible parametric survival models
although cox models are widely used for prognostic modelling, fpms have several advantages. fpms directly model the log baseline cumulative hazard function which allows for smooth survival curves to be produced during model development, without the need for post-estimation smoothing.21 it remains straightforward to include time-dependent predictor effects22 and incorporate delayed entry. fpms use restricted cubic splines to directly model the baseline ln[h0(t;xi)] (see supplementary file 1, available as supplementary data at ije online). a prognostic model can be written in the following form where ζ(ln(t)|γ,k0) is the restricted cubic spline function and βxi is the prognostic index.23 
ln[h(t;xi)]=ζ(ln(t)|γ,k0)+βxi
period analysis
period analysis, in the context of population-based cancer data, was developed by brenner and gefeller.20 only individuals who contribute follow-up time during the period window are included in the analysis to estimate predictor effects and baseline survival (see table 1). this reduces the sample size since people who experienced the event before the window (e.g. participant b, see figure 1) are excluded. only the events that occur within the window are considered in the analysis and therefore the choice of window width is a balance between ensuring up-to-date survival estimates and having sufficient events (and events per predictor parameter). the width of the window could be determined by meeting the criteria defined by riley et al.15 further details and a sensitivity analysis of using different window widths are included in supplementary file 4, available as supplementary data at ije online.

figure 1
contribution of follow-up time from four hypothetical participants (diagnosed 1 january) to a 2-year period window of 2004–05.
open in new tabdownload slide
contribution of follow-up time from four hypothetical participants (diagnosed 1 january) to a 2-year period window of 2004–05.

table 1.
summary of the data used for the estimation of the baseline and predictor effects for each method

method	baseline	predictor effects
full cohort 	full 	full 
temporal recalibration 	recent 	full 
period analysis 	recent 	recent 
open in new tab
delayed entry techniques are used to left truncate the follow-up time of people diagnosed before the window so that the short-term hazard rates are only estimated from those diagnosed within or shortly before the period window (e.g. participant d, see figure 1).

this method has been shown to produce more up-to-date survival estimates than full cohort analysis in population-based cancer settings for many types of cancer in different countries9–12 and is used routinely within international cancer survival comparisons.3,24

temporal recalibration
a key disadvantage with period analysis is the reduction in sample size and number of events for model estimation. to address this, we propose temporal recalibration, which combines the sample size advantages associated with the full cohort analysis with the up-to-date predictions from period analysis.

the process of fitting a temporal recalibration model is as follows. (i) fit a survival model using the full cohort dataset to estimate the predictor effects using all individuals. (ii) recalibrate the model by re-estimating the baseline using the subset of individuals from a period analysis sample, while holding the predictor effect estimates from step (i) fixed.

recalibrating the baseline in a recent period analysis sample allows for improvements in survival to be captured and leads to more up-to-date predictions. under proportional hazards the model can be written in the following form for fpms:  
ln[hnew(t;xi)]=ζnew(ln(t)|γ,k0)+offset(pii)
where ζnew(ln(t)|γ,k0) is the updated spline function for the log cumulative baseline hazard function estimated in the recent period data, k0 are the knot locations from the full cohort model and offset(pii) is the prognostic index estimated from the full cohort model as an offset term. fixing the predictor effects with constraints when fitting in the period analysis sample would offer an equivalent approach.
for a cox ph model it can be written as:  
hnew(t;xi)=h0new(t)eoffset(pii) 
where hnew(t;xi) and h0new(t) are the hazard and baseline hazard functions respectively, estimated on the recent time window, and offset(pii) is the prognostic index estimated from the full cohort model as an offset term.
as with period analysis, the choice of the window width is a bias-variance trade-off (see supplementary file 4, available as supplementary data at ije online). the width of the window could possibly be reduced compared with a standard period analysis approach as it is only necessary to have a sufficient number of events to estimate the baseline (and not the predictor effects). in temporal recalibration we explicitly assume the predictor effects are the same as they were in the full cohort model (see table 1).

assessing the performance of predictions
marginal survival (i.e. average across all individuals) can be calculated both within-sample (i.e. in the same dataset used to develop the model) and out-of-sample (i.e. in new individuals) by calculating every individual’s predicted survival over time, and then averaging the survival curves:25 
sˆ¯¯¯(t)=1n∑ni=1sˆ(t;xi)
out-of-sample marginal survival predictions can be compared with the observed survival (kaplan–meier estimates) to determine the calibration of a model’s survival predictions for a new group of individuals.

studying the marginal survival only assesses how well the model performs on average (sometimes referred to as calibration-in-the-large1,26), whereas calibration plots can be used to determine the model’s performance in different risk groups at particular time points. in this article the risk groups were defined by dividing the prognostic index from the full cohort models into 10 equally sized risk groups.

the e/o statistic quantifies calibration-in-the-large by comparing predicted or expected (e) outcome risk to the observed (o) risk through eo(t)=1−s exp (t)1−sobs(t)⁠. e is calculated from the marginal survival prediction from the model [sexp(t)] and o is from the observed kaplan–meier curve [sobs(t)]. a value of 1 indicates agreement1,27

harrell’s c-index can be used to assess the concordance of survival predictions from proportional hazards models. a value of 1 indicates perfect concordance.28

we now compare full cohort, temporal recalibration and period analysis approaches using an illustrative example of colon cancer.

example
data
we used the public-access seer database from the usa.16 the seer program covers ∼34% of the us population and collects population-based data on all reported cases of cancer within the cancer registries included in the seer program.29 the analysis was restricted to adults who were aged 18–99 years at the time of their diagnosis of colon cancer (icd10 codes c18.0–c18.9). if there were any duplicates of the patient id, only the first record was retained. patients with an unknown survival time (recorded to the nearest month) or incomplete dates for their diagnosis or death were also excluded. data from 1996–2015 were available for this analysis. as the aim was to identify which model gave better long-term survival predictions in new data, the data were split at 2005 for illustration purposes. data from 1996–2005 were used to develop the models and a 2 year period window from 1 january 2004 to 31 december 2005 was used to fit the temporal recalibration and period analysis models. the data from 2006–15 were then used to validate the models. baseline characteristics for the development dataset can be found in table 2.


table 2.
baseline characteristics of the 48 861 participants in the development dataset once participants with missing predictor values were removed. mean (sd) is presented for continuous variables and n (%) for categorical variables

variable	mean (sd) or n (%)
age 	70.1 (13.0) 
sex 	 
 male 	23 674 (48.5%) 
 female 	25 187 (51.5%) 
race 	 
 white 	42 296 (86.6%) 
 black 	6565 (13.4%) 
stage at diagnosis 	 
 stage 1 	18 469 (37.8%) 
 stage 2 	21 529 (44.1%) 
 stage 3 	8863 (18.1%) 
grade of tumour at diagnosis 	 
 grade 1 	5496 (11.2%) 
 grade 2 	32 992 (67.5%) 
 grade 3 	9871 (20.2%) 
 grade 4 	502 (1.0%) 
open in new tab
models
cause-specific cox and fpms were fitted, meaning that deaths due to causes other than colon cancer were censored. age at diagnosis, stage at diagnosis (localized, regional, distant), grade of the tumour (i–iv), sex and race (restricted to white and black patients only) were included as predictors. age was modelled using restricted cubic splines with three degrees of freedom, and stage, grade, sex and race were modelled categorically. all predictors were forced to be included (i.e. there was no variable selection). for the fpms, five degrees of freedom were used to model the log baseline cumulative hazard and, to simplify the process of recalibration, the baseline splines were not orthogonalized. example code used to fit these models is provided in supplementary file 2, available as supplementary data at ije online. in this illustrative example, any participants with missing predictor values were excluded in order to more easily compare the approaches, though in practice multiple imputation is usually preferable.

age at diagnosis was winsorized30 to provide more stability in the extremes by adding an additional constraint forcing the splines to be constant for the top and bottom 2% of the age distribution.31 in further analyses, the ph assumption was relaxed using time-dependent predictor effects for age and stage. to compare the model predictions from these three approaches, the marginal predicted survival for the 5601 patients diagnosed in 2006 was calculated using each model and compared with the observed kaplan–meier estimates. this was further assessed through calibration plots at 10 years after diagnosis.

all analyses were performed using stata version 15.0.32 fpms were fitted using the user-written package stpm233 and harrell’s c-index was calculated for these models using the user-written package stcstat2.34

results
in terms of predictor effect estimates, the log hazard ratios and standard errors were very similar regardless of whether cox models or fpms were used (table 3). the log hazard ratios were fairly similar for full cohort and period analysis, however the standard errors from the period analysis approaches were around twice as large due to the reduction in sample size. overfitting was minimial due to the large number of events relative to the number of predictor parameters, highlighted by a uniform shrinkage factor35 for the full cohort model of 0.999.


table 3.
comparison of the sample size, number of events, log hazard ratios (hr) and standard errors (s.e.) of the log hazard ratios for the categorical predictors in each model

flexible parametric survival model
cox proportional hazards model
full cohort	period analysis	full cohort	period analysis
sample size 	48 861 	33 197 	48 861 	33 197 
number of events 	12 040 	2900 	12 040 	2900 
predictor effects: log hr (s.e. of log hr) 	female 	−0.05 (0.018) 	−0.10 (0.038) 	−0.05 (0.018) 	−0.10 (0.038) 
black 	0.24 (0.025) 	0.27 (0.051) 	0.24 (0.025) 	0.27 (0.051) 
stage 2 	1.15 (0.031) 	1.18 (0.060) 	1.15 (0.031) 	1.18 (0.060) 
stage 3 	2.98 (0.031) 	2.92 (0.062) 	2.96 (0.031) 	2.90 (0.062) 
grade 2 	0.22 (0.039) 	0.09 (0.073) 	0.22 (0.039) 	0.10 (0.073) 
grade 3 	0.68 (0.041) 	0.55 (0.078) 	0.67 (0.041) 	0.54 (0.078) 
grade 4 	0.81 (0.088) 	0.78 (0.146) 	0.79 (0.088) 	0.75 (0.146) 
open in new tab
similar marginal survival predictions were produced regardless of whether cox or fpms were used when predicting for patients diagnosed in 2006 (figure 2). the marginal survival predictions for temporal recalibration and period analysis were very similar and consistently provided more well-calibrated estimates than the standard full cohort model. the survival probability is under-estimated for all risk groups in the full cohort analysis models, and in 9 of the 10 groups the predictions are the furthest from the reference line. however, using temporal recalibration, all the predicted survival estimates increase and agree more closely with the kaplan–meier estimates. although the marginal survival predictions from the temporal recalibration and period analysis models are very similar, small differences in predicted survival can be seen for the highest risk groups. including time-dependent effects for age and stage in the fpm improves the calibration in the third highest risk group, however there is very little difference in the marginal survival estimates, see supplementary file 3, available as supplementary data at ije online.

figure 2
external validation of the models to assess the calibration of survival predictions for new patients (diagnosed in 2006 with follow-up data until 2015). top: comparison of marginal observed (kaplan–meier) and predicted survival from each model. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. bottom: 10-year calibration plots comparing the observed and predicted cancer-specific survival probabilities from each model.
open in new tabdownload slide
external validation of the models to assess the calibration of survival predictions for new patients (diagnosed in 2006 with follow-up data until 2015). top: comparison of marginal observed (kaplan–meier) and predicted survival from each model. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. bottom: 10-year calibration plots comparing the observed and predicted cancer-specific survival probabilities from each model.


a comparison of the model performance in terms of calibration and concordance of survival predictions is displayed in table 4. calibration improves by 0.02 by performing temporal recalibration which is large at the population level and improves the net benefit of the model.36 in other scenarios, the difference may be greater if there have been more substantial changes in baseline survival over calendar time. as the predictor effects for the temporal recalibration models are constrained to be the same as those from the full cohort model, harrell’s c-index will always be the same for these models. in this example, the predictor effects for the period analysis models were also very similar and therefore harrell’s c-index is the same to three decimal places.



table 4.
comparison of model performance in the validation dataset. the difference in observed and predicted marginal survival at 10 years after diagnosis [sobs(10) – sexp(10)], the ratio of expected to observed risk at 10 years after diagnosis (e/o) and harrell’s c-index

model	sobs(10) – sexp(10)a	eo(10)	harrell’s c-index
full cohort: fpm 	0.056 	1.169 	0.788 
full cohort: cox 	0.051 	1.155 	0.788 
temporal recalibration: fpm 	0.031 	1.094 	0.788 
temporal recalibration: cox 	0.031 	1.095 	0.788 
period analysis: fpm 	0.032 	1.098 	0.788 
period analysis: cox 	0.033 	1.101 	0.788 
a
sobs(10), kaplan–meier estimate at 10 years after diagnosis; sexp(10), 10 year marginal survival prediction from the model.

open in new tab
updating prognostic models
temporal recalibration can also be used to produce up-to-date survival estimates when new data become available by simply re-estimating the baseline without the need for repeating the model-building process or re-estimating the predictor effects. this is akin to previous work by riley et al.,37 schuetz et al.38 and steyerberg26 that show how recalibrating the baseline hazard in new (local) settings can be important. to illustrate this, prognostic models were fitted using fpms with data from 1986–95, and data from 1996–2005 was used to update these models. as stage was only available from 1995 onwards, only age, sex, race and grade were included as predictors, and for simplicity phs was assumed. table 5 defines the models m1-m6 that were compared in this analysis.


table 5.
comparison of the data used to estimate the predictor effects and baseline of each flexible parametric survival model

model	description	data for predictor effects	data for baseline
m1 	original full cohort model 	1986–95 	1986–95 
m2 	full cohort model with all available data 	1986–2005 	1986–2005 
m3 	full cohort model with most recent data 	1996–2005 	1996–2005 
m4 	temporal recalibration of m1 	1986–95 	period window 2004–05 
m5 	temporal recalibration of m3 	1996–2005 	period window 2004–05 
m6 	period analysis 	period window 2004–05 	period window 2004–05 
open in new tab
to illustrate the difference in survival predictions for these models, 10-year survival was estimated for patients diagnosed in 2006 and compared with the kaplan–meier estimates for these patients, see figure 3.

figure 3
comparison of marginal observed (kaplan–meier curve) and predicted survival from the original and updated models. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly.
open in new tabdownload slide
comparison of marginal observed (kaplan–meier curve) and predicted survival from the original and updated models. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly.


using the original model (m1) resulted in a difference between the observed and predicted survival of 0.12, which was reduced to 0.08 by using a longer timespan (m2) and 0.05 by using a more recent cohort (m3). temporal recalibration models (m4 and m5) and the period analysis model (m6) produced the closest estimates which differed by <0.02. performing temporal recalibration improves the calibration of the full cohort models by at least 0.03 and a larger improvement of >0.10 is observed when recalibrating the original full cohort model. despite different models being temporally recalibrated, the predictions overlaid exactly. this demonstrates that temporal recalibration is appropriate in this example since the predictor effects do not greatly change over time and therefore it is only necessary to re-estimate the baseline.

discussion
often there are large underlying improvements in survival over the follow-up available in a model development dataset, which presents a challenge for subsequently making predictions for newly diagnosed patients. we have shown that survival predictions from prognostic models developed using a standard full cohort approach underestimate survival of recently diagnosed patients. however, more up-to-date, and thus accurate, survival predictions can be produced by developing prognostic models using temporal recalibration, where the baseline hazard is recalibrated in a subset of most recent data. this idea is similar to the approach of period analysis, but has the additional benefit of more precisely estimating predictor effects as it uses all the data to estimate the prognostic index.

unlike period analysis, it is possible to directly apply temporal recalibration to a range of existing prognostic models (i.e. cox ph models, fpms with time-dependent effects) to update the survival predictions without the need of repeating the model-building process or re-estimating predictor effects. no additional data are required, only a period analysis sample of the most recent data is needed to re-estimate the baseline and produce more up-to-date predictions which better reflect the survival of those currently being diagnosed. we have also shown the importance of regularly updating prognostic models when new data become available and how this can easily be achieved using temporal recalibration.

we have used seer public use data for colon cancer patients, with a range of predictors in order to illustrate the approach. for cancer sites and settings with smaller improvements over calendar time, the predicted survival estimates from a standard and temporally recalibrated approach would differ less. however, the approach would still be valid in this case. in this example we only showed complete case analysis, however, temporal recalibration could be performed on imputed datasets and the survival predictions from the models could be combined using rubin’s rules.39,40 example code for fitting these models is included in supplementary file 2, available as supplementary data at ije online.

temporal recalibration assumes that the predictor effects are the same in the recent data as in the full cohort and therefore do not change as a function of diagnosis date. this is in contrast to period analysis which updates both the baseline and the prognostic index. therefore, the parameter estimates from the full cohort and period analysis models can be informally compared to verify that this assumption is plausible. further, careful consideration should be given to the consistency of predictor’s values over time, but this is an issue generally and not specific to the approach we outline here.

temporal recalibration is a similar concept to model updating,41 in which the calibration of predictions from a previously developed prognostic model are externally validated using new data obtained from a more recent time point. in that setting, if the model consistently under or over predicts survival, it is recalibrated; typically predictor effects are kept fixed (i.e. as originally estimated), but the baseline is updated. the difference with temporal recalibration is that the period analysis sample (used for the recalibration) is not a separate dataset and has already been included in the full cohort model to estimate predictor effects.

an alternative to temporal recalibration and period analysis would be to model the year of diagnosis directly and then predict survival using the most recent year included in the model. this approach would make developing and updating existing prognostic models more challenging as it would require the year of diagnosis to be modelled appropriately, which may include time-dependent effects and non-linear terms. this method would also rely more heavily on extrapolation of effects when producing long-term survival predictions for the most recent calendar year. however, with temporal recalibration the long-term hazards are estimated directly from those included in the period window. with both temporal recalibration and modelling the year of diagnosis it may be necessary to consider interactions between predictor effects and year of diagnosis.42

many existing prognostic models use the standard full cohort approach. we have illustrated that using temporal recalibration could update these survival predictions and be a more accurate reflection of the prognosis of patients who are currently being diagnosed.

<|EndOfText|>

a study protocol for the development and
internal validation of a multivariable
prognostic model to determine lower
extremity muscle injury risk in elite football
(soccer) players, with further exploration of
prognostic factors
abstract
background: indirect muscle injuries (imis) are a considerable burden to elite football (soccer) teams, and
prevention of these injuries offers many benefits. preseason medical, musculoskeletal and performance screening
(termed periodic health examination (phe)) can be used to help determine players at risk of injuries such as imis,
where identification of phe-derived prognostic factors (pf) may inform imi prevention strategies. furthermore, using
several pfs in combination within a multivariable prognostic model may allow individualised imi risk estimation and
specific targeting of prevention strategies, based upon an individual’s pf profile. no such models have been
developed in elite football and the current imi prognostic factor evidence is limited. this study aims to (1) develop
and internally validate a prognostic model for individualised imi risk prediction within a season in elite footballers,
using the extent of the prognostic evidence and clinical reasoning; and (2) explore potential phe-derived pfs
associated with imi outcomes in elite footballers, using available phe data from a professional team.
methods: this is a protocol for a retrospective cohort study. phe and injury data were routinely collected over 5
seasons (1 july 2013 to 19 may 2018), from a population of elite male players aged 16–40 years old. of 60
candidate pfs, 15 were excluded. twelve variables (derived from 10 pfs) will be included in model development
that were identified from a systematic review, missing data assessment, measurement reliability evaluation and
clinical reasoning. a full multivariable logistic regression model will be fitted, to ensure adjustment before backward
elimination. the performance and internal validation of the model will be assessed. the remaining 35 candidate pfs
are eligible for further exploration, using univariable logistic regression to obtain unadjusted risk estimates.
exploratory pfs will also be incorporated into multivariable logistic regression models to determine risk estimates
whilst adjusting for age, height and body weight.

background
indirect muscle injuries (imis) are the most common injury type in elite football (soccer), predominantly affecting lower extremity muscle groups [1, 2]. such injuries
occur in the absence of direct impact-related trauma
(during sprinting for example) [3, 4] and are subclassified into functional disorders without macroscopic structural tissue muscle damage, or structural injuries with
clear evidence of muscle disruption [3, 4].
imis are problematic for elite teams in terms of both incidence and severity [5], accounting for 30.3% to 47.9% of
all injuries that result in time lost to both training and
competition [1, 6–9], with the mean and median absence
duration reported as 14.4 [1] and 15 days respectively [8].
player availability is crucial to team prosperity, with vast
commercial and financial rewards on offer to successful
teams and players [10, 11]. conversely, player absences
through injury negatively affect team performance [12,
13], increase demand on medical services and carry a significant financial burden. as an illustration, for each first
team player missing through injury, the daily cost to a participating team in the uefa champions league is approximately €17,000 to €20,000 [14, 15].
periodic health examination (phe) is used by 94% of
elite teams and typically consists of medical examination, musculoskeletal assessment, functional movement
evaluation and performance tests, conducted during preseason and in-season periods [16]. phe is considered
important because its intended purposes are to: (1) allow
regular health monitoring for underlying but asymptomatic pathology [17]; (2) establish baseline measures for
setting rehabilitation or training targets [18]; and (3)
identify individuals who are susceptible to common or
severe injury types (such as imis) [19]. for the latter
function, phe cannot detect causes of injury, but can
highlight factors that may be associated with an injury
outcome (prognostic factors) and therefore help explain
differences in injury risk across individuals within the
team [18]. several prognostic factors could also be used
in combination within a multivariable prognostic model
to predict an individual’s absolute injury risk [20, 21].
importantly, both prognostic models and prognostic factors (pfs) can be used to inform management approaches designed to modify an individual’s absolute risk
[21]. despite the potential benefits of prognostic models
for shaping injury prevention strategies aimed at
clinically important injuries such as imis, none have
been developed in elite football [22]. in addition, there
are significant methodological limitations in the evidence
base relating to phe-derived pfs [22].
therefore, this study will consist of two primary objectives: (1) to develop and internally validate a prognostic
model for individualised imi risk prediction during a
season in elite footballers, using a small number of phederived candidate pfs selected from a previous systematic review [22] and clinical reasoning; and (2) to explore
potential pfs associated with imi outcomes during a
season in this elite cohort, using available phe data from
a professional team.
methods
study design
this study will be of retrospective cohort design, using a
population of elite male football players aged 16–40 years
old who were employed on a full-time basis at an english
premier league club. the first objective will be conducted
in accordance with existing guidelines for model development and internal validation [23, 24] and reported in accordance with the transparent reporting of a
multivariable prediction model for individual prognosis
or diagnosis (tripod) statement [25, 26]. the second
objective will be conducted in accordance with existing
guidelines [27] and reported in accordance with the
reporting recommendations for marker prognostic
studies [28, 29].
data sources
this study will use routinely collected data that was obtained over five seasons (from 1 july 2013 to 19 may
2018). data collected from the musculoskeletal and performance test components of the club’s phe will be used
to identify candidate pfs. injury outcome data will also be
used to establish the available number of imi outcomes.
preseason phe data collection
each new season commenced from july 1st. available
players completed a mandatory phe on one of 3 days during the first week of the season. typically, the musculoskeletal and performance components of the phe
included the following: (1) anthropometric measurements;
(2) medical history review (i.e. previous injury history); (3)
musculoskeletal examination tests; (4) functional
hughes et al. diagnostic and prognostic research (2019) 3:19 page 2 of 13
movement and balance tests; and (5) strength and power
tests. detailed descriptions of all tests are provided in
additional file 1.
the phe test order was self-selected by each player. a
standardised warm up was not implemented, although
players could undertake their own warm up procedures
if they wished. each component of the phe test battery
was standardised according to a written protocol and
conducted by physiotherapists, sports scientists or club
medical doctors. to avoid inter-tester variability, the
same examiners performed the same test every season
and throughout the 5-year data collection period, no
examiner attrition occurred.
if a participant was injured at the time of phe, a risk
assessment was completed by medical staff. in such instances, participants only completed tests that were
deemed appropriate and safe for the participant’s condition; examiners were therefore not blinded to the injury
status of participants.
participant follow-up and injury data collection
participants were followed up to the last day of each
competitive domestic season (defined as the date of the
last first team game of the season) irrespective of
whether they had completed the phe procedure or not.
participants completed their routine training and match
programmes throughout. for every player in the squad,
any injuries that occurred during the season were
assessed and electronically documented within 24 h by a
club medical doctor or physiotherapist in accordance
with the consensus statement on injury definitions and
data collection procedures in studies of football injuries [30]. musculoskeletal assessments were dependent
on the clinical presentation, although typically consisted
of observation, effusion, range of movement, muscle
length and resisted muscle tests, palpation and special
diagnostic manual tests. radiological imaging was used
to assist diagnosis as required. ultrasound scans were
performed by the club medical doctor using a toshiba
aplio 500 or 1900 machine (toshiba corporation,
tokyo, japan). magnetic resonance imaging (mri) was
performed as appropriate, using a canon vantage titan
3 t scanner (canon medical systems, otowara, japan)
according to sequences determined by the club medical
doctor. images were evaluated by a club medical doctor
and an independent musculoskeletal radiologist.
the medical professionals were not blinded to phe
data at the time of diagnosis. these data were not routinely used to inform diagnoses, but instead used to
identify functional rehabilitation targets and for
benchmarking purposes. following injury, players
completed a rehabilitation programme as directed by
club medical staff to enable them to return to training
and match participation.
participants and eligibility criteria
eligible participants were identified from a review of the
phe database entries during the dates stated above. during any season, participants were eligible for inclusion
into the analysis if they: (1) had an outfield position (i.e.
not a goalkeeper); and (2) participated in phe testing for
the relevant season. participants were excluded from the
analysis for any season if they were a triallist player or
not contracted to the club at the time of phe.
ethics and data use
because all data were captured from the mandatory phe
procedure completed through the participants’ employment, informed consent was not required. the anonymity and rights of all participants were protected. the
football club granted permission to use these data, and
the use of the data for this study was approved by the
research ethics service at the university of manchester.
this study has been registered on clinicaltrials.gov,
with registered number as nct03782389.
data extraction
all phe records from eligible participants were extracted and placed into a separate database. using the
club’s electronic medical records system, a further database was generated of all recorded injuries for each season and a manual review of each eligible participant’s
medical record was undertaken to ensure accuracy. each
injury was categorised according to the following: (1)
contact or non-contact mechanism of injury; (2) injured
side; (3) affected body area; (4) injury type, i.e. imi/ligament/tendon/cartilage/contusion or laceration/bone/
concussion/other musculoskeletal injury; and (5) muscle
group and diagnostic classification if recorded as an imi.
this process allowed an in-house audit of injury incidence and absolute risk evaluation for each injury type
for the squad overall and for those who underwent phe.
all imis were then extracted and merged with the phe
database of included participants, for each season in
which they remained eligible.
outcome measures
for this study, the primary outcome measure will be the
occurrence of an initial (index) lower extremity imi sustained by a participant during a season. only time-loss
injuries will be included; that is, any index lower extremity imi that occurred during match play or training that
resulted in the player being unable to take full part in future match play or training [30]. an imi was confirmed
during the injury assessment procedure outlined above
and graded by the club medical doctor or physiotherapist according to the munich consensus statement for
the classification of muscle injuries in sport [4]. this
diagnostic classification system was the primary method
hughes et al. diagnostic and prognostic research (2019) 3:19 page 3 of 13
of muscle injury classification used by the club and has
been validated previously [31].
each participant-season will be treated as independent.
if an index lower extremity imi occurred, the participant’s outcome for the season will be determined and
that participant will no longer be considered at risk beyond the time of imi occurrence. in these circumstances, participants will be included for further analysis
at the start of the consecutive season, providing they remain eligible. if participants sustained any upper limb
imi, trunk imi or non-imi injury type, these will be ignored and the participant will still be considered at risk
of a lower extremity index imi.
eligible participants who were loaned out or transferred to another club throughout that season, but had
not sustained an index imi prior to the loan or transfer,
will still be considered in the risk set. participants who
sustained an index imi whilst on loan will be included
for analysis, as outlined above. any participants who
were permanently transferred during a season (but had
not sustained an index imi prior to the transfer) will be
recorded as not having an imi event during the relevant
season, and they will exit the cohort at this point. a sensitivity analysis may be conducted to evaluate the effect
of player loans or transfers on the results.
sample size
to maximise statistical power, we have elected to use all
data from the 5-season period. this approach agrees
with methodological recommendations that data splitting should be avoided, and all available data should be
used for model validation [32]. the extracted injury data
were audited in parallel with the development of this
protocol to determine the number of available index imi
events in the dataset. this was essential to allow calculation of the maximum number of candidate pfs that
could be included in model development in order to
limit the effects of statistical overfitting [33].
the number of candidate pfs for inclusion in model
development will be restricted to a minimum of 10
events per variable (epv), which is recommended to reduce overfitting and optimism during the development
of a logistic regression model [34]. note that ‘variable’
here means a parameter included (or considered for
inclusion) in the model that corresponds to one of
the pfs.
following the audit, the number of independent
participant-seasons that will be included for analysis is
317, with 138 index imi events recorded during the 5-
season period. therefore, we have chosen to restrict the
number of parameters (variables) for inclusion in model
development to 12, which corresponds to having >10
epv and thus above the minimum recommendation
of 10. we also checked if this met the criteria to
minimise overfitting recently proposed by riley et al.
[33]. assuming the model will have a modest nagelkerke r-squared of 25%, then with an outcome proportion of 0.435, our 12 candidate pf variables
correspond to targeting an approximate shrinkage factor of 0.85, and thus a relatively small amount of
overfitting (15%) [33]. we deemed this a suitable
compromise between increasing the number of pf parameters and minimising the overfitting.
candidate prognostic factors
the extracted phe data were audited as per current
methodological recommendations [23], to establish data
quality and quantify missing values. this process was
also conducted in parallel with the development of this
protocol, to assist selection of candidate pfs to be included in either model development or exploration a
priori and to inform strategies for handling missing data
in the final analysis.
a complete list of all 60 candidate pfs extracted from
the phe dataset is presented in table 1, with quantitative analysis of missing values for each pf.
missing data
as presented in table 1, all medical history and age
factors were complete (23 factors). of the 37
remaining candidate pfs, the proportion of missingness ranged from 5.68% (for height and weight) to
76.34% (for body fat). eleven of these had > 15%
missing observations (which included body fat, toe
touch in standing, sacroiliac kinematic function, all y
balance test and upper body peak power variables).
for these factors, the large degree of missingness was
because of procedural changes in the phe process,
which meant that these tests were not conducted
across all seasons.
for candidate pfs with < 15% missing observations, all
tests were conducted consistently across all 5 seasons.
for these factors, the sample characteristics of cases with
complete pf data were compared to incomplete cases
which had at least one missing observation (table 2).
for complete cases, the mean values of all characteristics were less than incomplete cases, with the largest
differences observed in age (20.83 and 23.55 years, respectively) and weight (74.15 and 77.86 kg, respectively). therefore, a complete case only analysis was
not appropriate and we will rather assume that the
mechanism of missingness can be considered as missing at random (mar), where the distribution of missing values is related to values of observed variables
[26], to allow imputation and so inclusion of individuals with missing data.
hughes et al. diagnostic and prognostic research (2019) 3:19 page 4 of 13
table 1 list of candidate prognostic factors, methods and units of measurement, frequency of complete and incomplete
observations and proportion of missing observations
type of
prognostic factor
candidate prognostic factor measurement method measurement unit data
type
complete
obs
missing
obs
percent
missing (%)
anthropometric age birthdate years cont. 317 0 0
height standing height cm cont. 299 18 5.68
weight digital scales kg cont. 299 18 5.68
bmi height/weight kg/m2 cont. 294 23 7.26
body fat skin callipers % cont. 75 242 76.34
medical history frequency of previous imis within 3 years
prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous imi within 3 years
prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous foot or ankle
injuries within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous foot or ankle injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous hip or groin injuries
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous hip or groin injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous knee injuries within
3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous knee injury within 3
years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous shoulder injuries
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous shoulder injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous lumbar spine
injuries within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous lumbar spine injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous iliopsoas imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous iliopsoas imi within
3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous adductor imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous adductor imi within
3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous hamstring imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous hamstring imi within
3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous quadriceps imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous quadriceps imi
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous calf imis within 3
years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous calf imi within 3
years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
musculoskeletal prom r hip joint internal rotation digital inclinometer degrees cont. 297 20 6.31
prom l hip joint internal rotation digital inclinometer degrees cont. 297 20 6.31
prom r hip joint external rotation digital inclinometer degrees cont. 297 20 6.31
prom l hip joint external rotation digital inclinometer degrees cont. 297 20 6.31
r hip flexor muscle length digital inclinometer—thomas
test
degrees cont. 294 23 7.26
hughes et al. diagnostic and prognostic research (2019) 3:19 page 5 of 13
model development and internal validation
we have chosen to conduct the model development before the pf exploration because of the restrictions on the
number of pfs permitted to limit potential overfitting of
the model.
because only 12 pf variables will be used in model building, we have defined these candidate pfs a priori (table 3).
three candidate pfs have known importance based on the
results of our previous systematic review so were selected
for inclusion [22]. all other pfs listed in table 1 were eligible unless there were > 15% missing observations or if reliability (where applicable) was classed as fair to poor (icc
< 0.70) [35]. in these cases, the relevant candidate pfs were
excluded (table 4). this was to ensure that only the highest
quality data will be used in the analysis, with pfs that would
generally be available and routinely measured.
co-linearity amongst factors within a logistic regression model can cause inaccuracies in standard error and
table 1 list of candidate prognostic factors, methods and units of measurement, frequency of complete and incomplete
observations and proportion of missing observations (continued)
type of
prognostic factor
candidate prognostic factor measurement method measurement unit data
type
complete
obs
missing
obs
percent
missing (%)
l hip flexor muscle length digital inclinometer—thomas
test
degrees cont. 294 23 7.26
r hamstring muscle length /neural
mobility
digital inclinometer—slr degrees cont. 297 20 6.31
l hamstring muscle length /neural
mobility
digital inclinometer—slr degrees cont. 297 20 6.31
r quadriceps muscle length goniometer—ely’s test degrees cont. 297 20 6.31
l quadriceps muscle length goniometer—ely’s test degrees cont. 297 20 6.31
r calf muscle length digital inclinometer—wbl degrees cont. 297 20 6.31
l calf muscle length digital inclinometer—wbl degrees cont. 297 20 6.31
toe touch in standing fingertip-floor distance cm cont. 250 67 21.14
sacroiliac joint kinematic function gillet’s test subjective kinematic
function
cat. 250 67 21.14
functional
movement/
balance
y balance test—r anterior translation y balance test cm cont. 179 138 43.53
y balance test—l anterior translation y balance test cm cont. 179 138 43.53
y balance test—r posteromedial
translation
y balance test cm cont. 179 138 43.53
y balance test—l posteromedial
translation
y balance test cm cont. 179 138 43.53
y balance test—r posterolateral
translation
y balance test cm cont. 179 138 43.53
y balance test—l posterolateral
translation
y balance test cm cont. 179 138 43.53
r relative tibial angles sls measured with imu degrees cont. * * *
l relative tibial angles sls measured with imu degrees cont. * * *
strength/power r upper body peak power horizontal press w/kg−0.67 cont. 178 139 43.85
l upper body peak power horizontal press w/kg−0.67 cont. 178 139 43.85
r maximal loaded leg extension power double leg press w/kg−0.67 cont. 276 41 12.93
l maximal loaded leg extension power double leg press w/kg−0.67 cont. 276 41 12.93
r maximal loaded leg extension velocity double leg press m s−1 cont. 276 41 12.93
l maximal loaded leg extension velocity double leg press m s−1 cont. 276 41 12.93
r maximal loaded leg extension force double leg press n/kg−0.67 cont. 276 41 12.93
l maximal loaded leg extension force double leg press n/kg−0.67 cont. 276 41 12.93
cmj height cmj cm cont. 275 42 13.25
cmj force per kilogram of body mass cmj n/kg cont. 275 42 13.25
cmj peak power cmj w cont. 275 42 13.25
freq. frequency, obs observations, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom
passive range of movement, slr straight leg raise, sls single leg squat, imu inertial measurement units, bmi body mass index, kg/m2 kilograms/body
height (metres) squared, w watts (note: w/kg−0.67 has a scaling factor to normalise power to body mass), n newtons (note: n/kg−0.67 has a scaling factor to
normalise force to body mass), cm centimetres, kg kilograms, cont. continuous, dis./cont. discrete treated as continuous, cat categorical, r right, l left, m s-1
metres/second, “–” not applicable/not available, “*” missing data analysis not completed—test evaluated through a reliability/agreement study published as a
related part of this project and excluded from further analysis based on the results
hughes et al. diagnostic and prognostic research (2019) 3:19 page 6 of 13
confidence interval estimates [45], so a scatterplot
matrix was used to informally assess between-factor correlations for eligible pfs. if pfs were highly correlated,
one of the pfs was dropped or new composite pfs were
generated and replaced the original factors (highlighted
in tables 3, 4 and 5). typically, this occurred where
measurements examined both right and left limbs separately; composite factor variables were therefore created
for both between-limb measurement differences and the
mean of the measurements for both limbs.
of the remaining eligible pfs, 9 further candidate factor variables were selected for inclusion, through use of
clinical reasoning to identify those with a biologically
plausible association with imi development. the final
set of 12 pf variables is shown in table 3.
prognostic factor exploration
candidate pfs that were that were not selected for use in
model development (but not excluded) will be eligible for
further exploratory analysis (table 5). this will allow identification of other potentially useful associations which
may assist future analyses or updating of the model created under the first objective of this investigation.
statistical analysis
model development and internal validation
multivariable logistic regression will be used for the analysis as this is an appropriate method where outcomes
are binary [26] and independent variables (pfs) are continuous, categorical or a combination [45]. initially, we
will fit a full multivariable model containing all 12 candidate pf variables to ensure a fully adjusted model prior
to the potential elimination of unimportant candidate
factors [23]. backward elimination will then be used to
successively remove non-significant factors with p values
of greater than 0.157. this threshold was set to approximate equivalence with akaike’s information criterion
[48]. using backward elimination in this way may deliver
a more parsimonious model which is therefore easier to
implement in clinical practice than a full model. where
possible, we will retain continuous candidate pfs in their
continuous form to avoid statistical power loss [49].
because the missing data mechanism is considered as
missing at random (mar), multiple imputation (mi) will
be implemented, using 50 imputations. we have chosen
to utilise mi because it avoids excluding participants
from the analysis, is an effective method of handling
missing prognostic factor information and can be used
to account for uncertainty in missing data [50].
the apparent performance of the developed model will
be summarised in the development datasets (averaged
over imputation datasets), via calibration and discrimination. model calibration determines performance in terms
of the agreement between predicted outcome risks and
those actually observed [51]. graphical plots are useful to
assess calibration [23], so will be produced and utilised in
the analysis. we will calculate calibration-in-the-large
(citl, ideal value of 0), which quantifies the systematic
error in model predictions (overall agreement). a related
measure is e/o (ideal value of 1), which gives the ratio of
the mean of the predicted (expected (e)) risks against the
mean of the observed risks (o) [51, 52]. a calibration
slope will also be calculated, where a value of 1 equals perfect calibration [26]. models demonstrate perfect calibration within development data, but in new data, the slope
may be < 1 due to overfitting in the model development
dataset (see below for how this will be handled) [52].
discrimination performance is a measure of a model’s
ability to separate participants who have experienced an
outcome compared to those who have not, quantified
using the c (concordance) statistic (equivalent to the
area under the roc curve) [23]. this index measure will
be calculated for the development model, where 1 demonstrates perfect discrimination, whilst 0.5 indicates that
discrimination is no better than by chance alone.
to quantify the degree of optimism due to overfitting,
our model will be internally validated using bootstrap
re-sampling. this will be conducted as previously outlined [26, 53]. the prognostic factor variable selection
procedure and model construction will be repeated for
200 bootstrap samples. for each sample, the difference
in bootstrap apparent performance (of the bootstrap
model in the bootstrap data) and test performance (of
the bootstrap model in the original dataset) will be averaged across the 200 samples, to obtain a single estimate
table 2 characteristics of cases with complete candidate prognostic factor data, and cases with at least one missing observation for
any candidate prognostic factor in the phe dataset with < 15% missing values
complete cases (n=264 person-seasons) incomplete cases (n=53 person-seasons)
sample characteristic number of obs mean (sd) number of obs mean (sd)
age (years) 264 20.83 (4.42) 53 23.55 (4.48)
height (cm) 264 180.58 (6.34) 35 181.12 (6.68)
weight (kg) 264 74.15 (7.55) 35 77.86 (7.98)
bmi (kg/m2 ) 264 22.72 (1.76) 30 23.75 (2.24)
pf prognostic factor, obs observations, sd standard deviation, cm centimetres, kg kilograms, kg/m2 kilograms divided by body height (metres) squared
hughes et al. diagnostic and prognostic research (2019) 3:19 page 7 of 13
of optimism for each performance statistic. then, to calculate optimism-adjusted estimates of performance for
our new model, the estimates of optimism will be subtracted from the original apparent estimates of
performance.
the optimism-adjusted calibration slope will provide a
uniform shrinkage factor, which will be applied to all
prognostic factor effects in the developed model to adjust (shrink) for overfitting. the intercept of the model
will then be re-estimated accordingly. this will then
form our final model.
prognostic factor exploration
all remaining candidate factors that are eligible for exploration (table 5) will undergo univariable logistic regression
analyses to determine unadjusted associations with imis.
candidate pfs will also be incorporated into multivariable
logistic regression models to determine odds ratios after
adjustment for age, height and body weight. note that because age was included as a candidate in the original
model and will also be used for adjustment purposes in
the exploratory multivariable models, the total number of
candidate pfs eligible for exploration is 36. exploration of
non-linear associations between candidate factors and
index imi outcomes will also be evaluated using a fractional polynomial approach [49].
discussion
although previous studies in elite football have investigated the association between factors obtained during
phe and imis using multivariable models, none have
developed, validated or evaluated the performance a
prognostic model for injury prediction purposes [22].
whilst it is possible to develop a prognostic model from
phe data [18], our investigation will offer valuable insights into the practical aspects of this process and the
clinical usefulness of a model when applied to an individual football club. our findings may also outline how
these principles may be used in future at other clubs or
sports, or on larger datasets which could be derived
from several collaborating clubs.
table 3 restricted set of candidate prognostic factors for model development and validation
selection
method
candidate prognostic
factor
composite
pf
measurement unit number of
parameters in
model
measurement
method
data type reliability (if
applicable)
systematic
review
age no years 1 date of birth continuous -
frequency of previous
imis within 3 years prior
to phe
no freq. 1 medical records discrete
(treated as
continuous)
-
most recent previous
imi within 3 years prior to
phe
no < 6 months, 6–12
months, > 12
months
3 medical records categorical -
clinical
reasoning/
data quality
cmj peak power no watts 1 cmj using force
plates
continuous test-retest icc
= 0.92–0.98 [36]
prom hip joint internal
rotation difference*
yes degrees 1 digital
inclinometer
continuous intra-rater icc =
0.90 [37]
prom hip joint external
rotation difference*
yes degrees 1 digital
inclinometer
continuous intra-rater icc =
0.90 [37]
hip flexor muscle length
difference*
yes degrees 1 digital
inclinometer -
thomas test
continuous inter-rater icc =
0.89 [38]
hamstring muscle length
/neural mobility
difference*
yes degrees 1 digital
inclinometer - slr
continuous intra-rater icc =
0.95–0.98 [39]
interrater icc =
0.80–0.97 [40]
calf muscle length
difference*
yes degrees 1 digital
inclinometer - wbl
continuous inter-rater icc =
0.80–0.95 [41,
42]
intra-rater = icc
0.88 [42]
bmi yes kg/m2 1 composite height
(cm) and weight
(kg)
continuous –
pf prognostic factor, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom passive range of
movement, icc intraclass correlation coefficient, slr straight leg raise, bmi body mass index, kg kilos, freq. frequency, kg/m2 kilograms/body height (metres)
squared. "*" denotes between limb differences
hughes et al. diagnostic and prognostic research (2019) 3:19 page 8 of 13
despite the availability of high-quality phe and injury
data, the relatively small number of outcomes in this
dataset is problematic and will permit only a limited selection of candidate prognostic factors for use in model
development. utilising more than one prognostic factor variable for every 10 injury outcomes may cause
significant issues with model overfitting, where spurious observed relationships occur because of regression value distortion [34]. this leads to an
overestimation of predictive performance (optimism)
which is especially evident in small datasets [54]. to
limit the effects of overfitting, only 10 pfs (resulting
in 12 variables) will be permitted and use of data reduction methods have been required to select appropriate candidate factors for inclusion.
pfs for clinical injury outcomes are either intrinsic
(person specific) or extrinsic (environment specific) [55]
and can be modifiable or non-modifiable [56]. only the
non-modifiable factors of increasing age and history of
previous muscle injury have been shown to have modest
table 4 candidate prognostic factors excluded from both model development and prognostic factor exploration
type of
prognostic
factor
candidate prognostic
factor
composite
pf
measurement unit measurement method data type reason for
elimination
anthropometric body fat no percentage skin callipers continuous missing data > 15%
musculoskeletal quadriceps muscle length
difference*
yes degrees goniometer - ely's test continuous intra-rater icc =
0.69 [43]
inter-rater icc =
0.66 [43]
mean quadriceps muscle
length**
yes degrees goniometer - ely's test continuous intra-rater icc =
0.69 [43]
inter-rater icc =
0.66 [43]
toe touch in standing no centimetres fingertip to floor distance continuous missing data > 15%
sacroiliac joint kinematic
function
no subjective score gillet’s test categorical missing data > 15%
functional
movement/
balance
y balance test—anterior
translation difference*
yes centimetres y balance test continuous missing data > 15%
y balance test—mean
anterior translation**
yes centimetres y balance test continuous missing data > 15%
y balance
test—posteromedial
translation difference*
yes centimetres y balance test continuous missing data > 15%
y balance test—mean
posteromedial translation**
yes centimetres y balance test continuous missing data > 15%
y balance
test—posterolateral
translation difference*
yes centimetres y balance test continuous missing data > 15%
y balance test—mean
posterolateral translation**
yes centimetres y balance test continuous missing data > 15%
r relative tibial angles no degrees sls measured with dorsavi
viperform imu
continuous within-session iccs
= 0.27–0.75
between-session
iccs = 0.55–0.77
[44]
l relative tibial angles no degrees sls measured with dorsavi
viperform imu
continuous within-session iccs
= 0.27–0.75
between-session
iccs = 0.55–0.77
[44]
strength/power upper body peak power
difference*
yes normalised watts
per kilo (w/kg−0.67)
double horizontal press using a
keiser chest press air 350
machine
continuous missing data > 15%
mean upper body peak
power**
yes normalised watts
per kilo (w/kg−0.67)
double horizontal press using a
keiser chest press air 350
machine
continuous missing data > 15%
icc intraclass correlation coefficient, sls single leg squat, w watts, (note that w/kg−0.67 has a scaling factor to normalise power to body mass), kg kilos, imu inertial
measurement units, r right, l left. “*” denotes between limb differences and “**” denotes the mean of the measurements for both limbs
hughes et al. diagnostic and prognostic research (2019) 3:19 page 9 of 13
table 5 candidate prognostic factors—exploration
type of
prognostic
factor
candidate prognostic factor composite pf measurement unit measurement method data type reliability (if
applicable/available)
anthropometric height no centimetres standing height measure continuous –
weight no kilograms digital scales continuous –
medical history frequency of previous foot or ankle
injuries within 3 years prior to phe
no freq. medical records continuous –
most recent previous foot or ankle injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous hip or groin
injuries within 3 years prior to phe
no freq. medical records continuous –
most recent previous hip or groin injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous knee injuries
within 3 years prior to phe
no freq. medical records continuous –
most recent previous knee injury within 3
years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous shoulder injuries
within 3 years prior to phe
no freq. medical records continuous –
most recent previous shoulder injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous lumbar spine
injuries within 3 years prior to phe
no freq. medical records continuous –
most recent previous lumbar spine injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous iliopsoas imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous iliopsoas imi within
3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous adductor imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous adductor imi within
3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous hamstring imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous hamstring imi
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous quadriceps imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous quadriceps imi
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous calf imis within 3
years prior to phe
no freq. medical records continuous –
most recent previous calf imi within 3
years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
musculoskeletal mean prom hip joint internal rotation** yes degrees digital inclinometer continuous intra-rater icc =
0.90 [37]
mean prom hip joint external rotation** yes degrees digital inclinometer continuous intra-rater icc =
0.90 [37]
mean hip flexor muscle length** yes degrees digital inclinometer - thomas
test
continuous inter-rater icc =
0.89 [38]
mean hamstring muscle length/neural
mobility**
yes degrees digital inclinometer - slr continuous intra-rater icc =
0.95–0.98 [40]
inter-rater icc =
0.80–0.97 [40]
mean calf muscle length** yes degrees digital inclinometer - wbl continuous inter-rater icc =
0.80–0.95 [41, 42]
intra-rater icc =
0.88 [42]
strength/power maximal loaded leg extension power
difference*
yes normalised watts per
kilo (w/kg−0.67)
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.886 [46]
hughes et al. diagnostic and prognostic research (2019) 3:19 page 10 of 13
prognostic value for hamstring muscle injuries in elite
footballers [22], so will be included in model development. however, their non-modifiable nature means that
they have limited use in terms of informing injury prevention strategies. to enhance the clinical applicability
of the model, other potentially relevant and modifiable
factors have been selected for inclusion.
the methodological shortcomings in the literature
mean that only three candidate prognostic factors could
be selected for model development from our previous
systematic review [22]. subsequently, candidate pf selection for our model has been largely based upon the
evaluation of collinearity, measurement reliability and
clinical reasoning, which means that it is possible that
some important factors have not been considered. it is
also possible that some potentially useful factors have
been excluded on the basis of having >15% of missing
values. as such, only modest performance of this initial
model is expected.
it is acknowledged that the proposed prognostic model
will assume that participants are independent for each
season and utilise the binary outcome of at least one
imi in a season, rather than evaluating time to individual imi events. this means that we will not account for
within-person correlations from season to season. although this is not fully representative of the real world,
because this is a novel area and we are restricted to a
relatively small dataset, we have elected to perform the
analyses in a more simplistic manner in the first instance. further, more complex analyses may be conducted in the future.
to assess the generalisability of a prognostic model, it
should be externally validated using data from another
location [21, 24], such as a dataset from another comparable elite level football team. because there is likely
to be considerable between-team heterogeneity in phe
processes [16], candidate prognostic factors within our
model may not translate externally at this time. there
are no immediate plans to externally validate this model.
however, depending on the outcome of the model development and exploratory objectives, it may be possible to
conduct a future prospective temporal validation study
within the same football club, or external validation
study in different population. if feasible, such investigations will require a separate associated protocol.
the current evidence relating to pfs for injury in
football is frequently flawed due to issues with the reliability of data measurement, adjustment, dichotomisation and potential diagnostic misclassification, so
there is a need for further studies that address these
issues [22]. further hypothesis-free exploratory studies
that investigate many factors (including those that are
not necessarily biologically plausible) may assist with
identification of new factors that may help inform
management decisions and monitoring purposes [20].
furthermore, these types of studies are helpful because
new pfs may be used to update a developed model to improve performance [57]. we have therefore outlined an
exploratory objective to investigate the association between imis and other factors from the current dataset,
using a validated diagnostic outcome classification system
and recommended statistical approaches, ensuring that
where possible, analysis of continuous data remains on
the continuous scale to explore linear and non-linear
associations.
we anticipate that this investigation will provide a
comprehensive evaluation of what is currently possible
in terms of using phe data to predict imis at an elite
football club, by adhering to transparent reporting procedures and current best practice for model
table 5 candidate prognostic factors—exploration (continued)
type of
prognostic
factor
candidate prognostic factor composite pf measurement unit measurement method data type reliability (if
applicable/available)
mean of maximal loaded leg extension
power**
yes normalised watts per
kilo (w/kg−0.67)
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.886 [46]
loaded maximal leg extension velocity
difference*
yes peak velocity (m s−1
) double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.792 [46]
mean of maximal loaded leg extension
velocity**
yes peak velocity (m s−1
) double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.792 [46]
loaded maximal leg extension force
difference*
yes normalised peak force
(n/kg−0.67 )
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.914 [46]
mean of maximal loaded leg extension
force**
yes normalised peak
force (n/kg−0.67)
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.914 [46]
cmj force per kilogram of body mass no force per kg (n/kg) cmj using force plates continuous –
cmj height no centimetres cmj using force plates continuous test-retest icc =
0.80–0.88 [47]
pf prognostic factor, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom passive range of
movement, icc intraclass correlation coefficient, slr straight leg raise, kg kilos, w watts, (note that w/kg-0.67 has a scaling factor to normalise power to body
mass), n newtons, (note that n/kg-0.67 has a scaling factor to normalise force to body mass), m s−1 metres/second, "-" not applicable/not available. “*” denotes
between limb differences and “**” denotes the mean of the measurements for both limbs
hughes et al. diagnostic and prognostic research (2019) 3:19 page 11 of 13
development, validation and exploration of potential
pfs. we hope this study will also identify further research priorities for this novel and potentially valuable
area of sports/football medicine research.

<|EndOfText|>

individual participant data meta-analysis of continuous
outcomes: a comparison of approaches for specifying
and estimating one-stage models

one-stage individual participant data meta-analysis models should account for
within-trial clustering, but it is currently debated how to do this. for continuous outcomes modeled using a linear regression framework, two competing approaches are a stratified intercept or a random intercept. the stratified
approach involves estimating a separate intercept term for each trial, whereas
the random intercept approach assumes that trial intercepts are drawn from
a normal distribution. here, through an extensive simulation study for continuous outcomes, we evaluate the impact of using the stratified and random
intercept approaches on statistical properties of the summary treatment effect
estimate. further aims are to compare (i) competing estimation options for the
one-stage models, including maximum likelihood and restricted maximum likelihood, and (ii) competing options for deriving confidence intervals (ci) for
the summary treatment effect, including the standard normal-based 95% ci,
and more conservative approaches of kenward-roger and satterthwaite, which
inflate cis to account for uncertainty in variance estimates. the findings reveal
that, for an individual participant data meta-analysis of randomized trials with a
1:1 treatment:control allocation ratio and heterogeneity in the treatment effect,
(i) bias and coverage of the summary treatment effect estimate are very similar when using stratified or random intercept models with restricted maximum
likelihood, and thus either approach could be taken in practice, (ii) cis are generally best derived using either a kenward-roger or satterthwaite correction,
although occasionally overly conservative, and (iii) if maximum likelihood is
required, a random intercept performs better than a stratified intercept model.
an illustrative example is provided.
keywords
continuous outcomes, estimation, individual participant data, ipd, meta-analysis

1 introduction
individual participant data (ipd) meta-analysis involves obtaining and then synthesizing raw individual-level data from
multiple related studies, to produce summary results that inform clinical decision making.1 the ipd approach is increasingly popular and has many potential advantages over a traditional meta-analysis of published aggregate data, such as
increased power to detect treatment-covariate interactions and avoiding reliance on published results.2
statistical methods to perform an ipd meta-analysis involve either a one-stage or two-stage approach.3 generally, these
approaches give very similar meta-analysis results, especially when they use the same modeling assumptions and/or
estimation methods.4,5 however, the one-stage approach has become increasingly popular over the past decade.6 it conveniently allows all studies to be analyzed simultaneously and avoids the assumption of normally distributed study effect
estimates with known variances that is usually made in the second stage of the two-stage approach. it also allows greater
flexibility of parameter specification over the two-stage approach.6
when conducting a one-stage ipd meta-analysis, it is important to account for clustering of participants within studies,
to correctly condition an individual's response to the study they are in. ignoring clustering and analyzing ipd as if coming
from a single study can result in misleading conclusions. for example, abo-zaid et al7 showed that family history of
thrombophilia was statistically significant as a diagnostic marker of deep vein thrombosis when clustering was accounted
for (odds ratio = 1.30; 95% confidence interval (ci): 1.00, 1.70; p value = 0.05) but not when clustering was ignored (odds
ratio = 1.06; 95% ci: 0.83, 1.37; p value = 0.64). while it is well established that clustering should be accounted for, it is
debatable exactly how this should be done. in particular, there are two competing approaches to account for clustering in
a one-stage model: a stratified intercept or a random intercept. the stratified approach involves a separate intercept term
being estimated for each study; thus, if there are 10 studies, 10 intercept terms would be estimated (one for each study). in
the random intercept approach, the intercepts are assumed to be drawn from some distribution, typically normal with an
underlying mean value and variance. the advantage of the stratified intercept approach is that it makes no assumptions
about the distribution of intercepts across studies. in contrast, the advantage of the random intercept approach is that it
requires fewer parameters to be estimated.
in this article, we evaluate through an extensive simulation study the impact of using either the stratified or random
intercept approach on the statistical properties of the summary treatment effect estimate (for example, in terms of bias,
precision, mean square error (mse), and coverage). this is considered in the context of randomized trials with a continuous outcome and a 1:1 treatment:control allocation ratio, assuming either common or random treatment effects across
trials. two further aims are to (i) compare competing estimation options for the one-stage models, including maximum
likelihood (ml) and restricted maximum likelihood (reml) and (ii) compare competing options for deriving confidence intervals for the summary treatment effect, including the standard normal-based 95% ci, and (for reml, but not
ml estimation) the kenward-roger (kr)8 and satterthwaite9 corrections that inflate confidence intervals to account for
uncertainty in variance estimates.
this paper is structured as follows. in section 2, we introduce the two competing one-stage ipd meta-analysis models of
interest that account for clustering, as well as the competing estimation and ci derivation options. in section 3, we outline
how the simulation study was conducted and present the results, and in section 4, we provide a real example to illustrate
the methods considered. finally, in section 5, we conclude with a discussion of the key findings and limitations and offer
a recommendation for those conducting one-stage ipd meta-analysis of randomized trials with 1:1 treatment:control
allocation ratio and with a continuous outcome.
2 introducing different model specification and
estimation options
consider that ipd have been obtained from i = 1 to k related randomized trials, each investigating a treatment effect
based on a continuous outcome y (say, blood pressure); that is, the mean difference in outcome value between a treatment
and a control group. suppose that there are ni participants in trial i. let yfi j be the end-of-trial (f used to denote final)
continuous outcome value, for participant j in trial i, and ybi j (b to denote baseline) be the pre-treatment outcome value.
let treati j take the value 1 or 0 for participants in the treatment or control group, respectively.
given such ipd, there are several ways in which researchers can use a one-stage meta-analysis to model the summary
treatment effect across trials. we focus initially on presenting one-stage analysis of covariance (ancova) mixed models,
which either use a stratified intercept or a random intercept to account for clustering of participants within trials. we also
assume a random treatment effect since heterogeneity is usually expected.
2.1 model (1): stratified intercept
with the following approach, a stratified intercept is used to account for within-trial clustering.
y𝐹 𝑖𝑗 = 𝛽i + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + ui) treat𝑖𝑗 + e𝑖𝑗 (1)
ui ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
here, 𝛽i denotes the intercept term for trial i (expected final outcome value for participants in the control group in trial i
who have the mean baseline outcome value), and the distinct intercept for each trial is used to account for within trial
clustering. the term 𝜆i denotes a trial-specific adjustment term for the baseline outcome value (here, centered at the
mean for each trial (y𝐵𝑖) to aid interpretation of the trial-specific intercepts). for example, when there are k = 10 trials,
there would be 10 𝛽i terms and 10 𝜆i terms. of main interest is an estimate of the model parameter 𝜃, as this denotes
the summary (average) treatment effect. the random effect, ui, indicates that the true treatment effects in each trial are
assumed to arise from a distribution of true effects with mean 𝜃 and between-trial variance 𝜏2. this assumption could be
constrained if considered appropriate, with a common (fixed) treatment effect (ie, constrain 𝜏2 = 0). lastly, 𝜎2
i denotes a
distinct residual variance per trial.
the flexibility of the one-stage ipd approach allows us to make further modifications by considering, for example, a
common baseline adjustment term (ie, 𝜆i= 𝜆) across trials, or common residual variances (ie, 𝜎2
i = 𝜎2) if necessary5,10,11;
however, this should be justified (eg, based on computational reasons or estimation problems), and sensitivity analysis to
the choice of assumptions is often sensible.
2.2 model (2): random intercept
when there are a large number of trials to be synthesized, a stratified intercept approach to clustering can be computationally intensive (as equation (1) requires estimation of 3 k + 2 parameters).4 an alternative approach for dealing with
clustering, which is preferred by some researchers,12 is to use a random intercept term.
y𝐹 𝑖𝑗 = (𝛽 + u1i) + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗 (2)
u1i ∼ n
(
0, 𝜏2
𝛽
)
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
parameters are as in equation (1), except that within-trial clustering has now been accounted for by a random (instead
of stratified) intercept term, with 𝜏2
𝛽 denoting the between trial variance in the intercept about the mean intercept (𝛽).
equation (2) assumes independence of the two random effects (ie, a covariance of zero), but their correlation could be
accounted for assuming a bivariate random effect distribution; indeed, this might be of special interest when evaluating
the relationship across trials of mean baseline in the control group and true treatment effect.13
compared to equation (1), the number of parameters to be estimated has been reduced, with only 𝛽 and 𝜏𝛽 for the
intercept, instead of k separate terms. therefore, fewer estimation problems might be anticipated than in equation (1). on
the downside, equation (2) makes a strong and potentially unnecessary assumption that control group means are drawn
from a normal distribution with a common mean and variance. furthermore, the estimation of an additional random
effect term might increase computational intensity.
2.3 options for estimation and ci derivation
the parameters in models (1) and (2) are typically estimated using either a ml or reml approach. ml is known to
produce downwardly biased estimates of between trial variance when there are few trials,14-16 whereas reml addresses
the downward bias and is thus generally preferred.17,18
in addition to competing options for model parameter estimation, there are also competing options to subsequently derive (1 − 𝛼)100% cis for the true summary treatment effect (𝜃). standard cis are based on large-sample
inference and assume 𝜃
̂ is approximately normally distributed:
𝜃
̂± z1−𝛼
2
√
var(𝜃
̂), (3)
where 𝜃
̂is the estimate of 𝜃, var(𝜃
̂) is its variance, and z1−𝛼
2
is the upper 1− 𝛼
2 quantile of the standard normal distribution.
this standard approach may produce cis that are too narrow, as var(𝜃
̂) does not account for the uncertainty in the estimate
of the between trial variation of 𝜃
̂.
4,18
to address this, more conservative options are available based on small-sample inference, which define the uncertainty
around 𝜃
̂ using approximations based on a t-distribution, such as the kr8 and satterthwaite9 corrections, which are also
known as denominator-degrees-of-freedom adjustments.
the kr corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
varkr(𝜃
̂), (4)
where 𝜃
̂ is as before, but now a bias-adjusted (inflated) variance (varkr(𝜃
̂)) is used, and t𝜐;1−𝛼
2
(the upper 1−𝛼
2 quantile of
the t-distribution with an adjusted degrees of freedom, 𝜐) instead of z1−𝛼
2
.
for a single parameter of interest (as in our case), the satterthwaite corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
var(𝜃
̂), (5)
where t𝜐;1−𝛼
2
is as in the kr correction, but the original (unadjusted) variance of 𝜃
̂is used. note that, while the denominator
degrees of freedom calculated from the kr and satterthwaite corrections are the same for single hypothesis tests, the kr
correction uses a bias-adjusted variance; therefore, cis derived using equations (4) and (5) will potentially differ, with the
one using the kr correction (equation (4)) leading to slightly wider intervals.19
although schaalje et al20 recommend kr over satterthwaite in special cases when the sampling distribution of the
test statistic is known, there remains debate over the best method, and a lack of literature in this area in regard to ipd
meta-analysis for estimation of a parameter of interest.
3 simulation study
we now perform a simulation study to examine the statistical performance of the summary treatment effect estimate (𝜃
̂)
from a one-stage ipd meta-analysis across a range of scenarios. our aim is to assess the different model specifications,
parameter estimation methods and ci derivation options described in section 2. that is, we compare the following: stratified or random intercept specifications; ml or reml estimation options; and, for reml estimation, 95% cis based on
asymptotic formula (equation (3)) or with either kr or satterthwaite corrections (equations (4) and (5), respectively)).
3.1 methods
provided is a step-by-step guide to our simulation study. for simplicity, and to considerably speed up the many simulations, we removed the baseline adjustment term in models (1) and (2), such that it does not exist in any of the data
generating mechanisms or models fitted in our simulations. in other words, we generate data without baseline imbalances
and thus analyze the data according to a final score ipd meta-analysis model, which is appropriate in this situation.
21 for
similar reasons of simplicity and computational complexity, we assumed a common residual variance across trials (both
in data generation and models fitted). extension to different residual variances is considered in our discussion (section 5).
to inform the true parameter values for the simulation, we used a previous ipd meta-analysis of treatment for lower
blood pressure outcomes.22
all analyses were conducted using stata v.14.2 (stata corporation, tx, usa).23
3.1.1 scenario 1 (base case)
the simulation process is now explained, in the context of an initial base case scenario with ipd from 10 trials and a
relatively simple data generating mechanism. extensions to other more complex scenarios are described afterwards.
step 1: data generating mechanism for one ipd meta-analysis of 10 trials
consider that an ipd meta-analysis of i = 1 to k related trials is of interest, with the goal to summarize a treatment
effect on a continuous outcome. to generate such data for the base case of this simulation study, we started by setting the
number of trials, k, to 10. we set a fixed number of participants, n = 100 in each trial, and assumed a fixed randomization
of 1:1 in each trial; that is, on average, 50% of participants within any given trial are allocated to a treatment group, and
the remaining 50% to a control group. this gave us a triali (trial 1/0 indicator) and treatij (treatment group 1/0 indicator)
value for each of 100 participants in each of 10 trials.
next, based on the previous meta-analysis,22 we set the true parameter values for this simulation to be as follows:
𝜃 = −9.66 (summary treatment effect; negative value favors treatment group), 𝜏2 = 7.79 (between trial variation in the
treatment effect), 𝛽 = 159.73 (mean blood pressure response in control group), 𝜏2
𝛽 = 233.99 (between trial variation in the
intercept), and 𝜎2 = 333.74 (residual variance).
we then used these parameter values to generate further terms, beginning with using 𝜎2 to generate an error term ei j,
for the jth participant from the ith trial
e𝑖𝑗 ∼ n(0, 𝜎2
). (6)
then, we generated the trial level values for the random parts of the intercept and treatment effect terms, u1i and u2i,
respectively,
u1i ∼ n
(
0, 𝜏2
𝛽
)
(7)
u2i ∼ n(0, 𝜏2
).
finally, with all the parameters defined (𝛽, u1i, 𝜃, u2i, treati j, and ei j), we generated the end-of-trial continuous outcome
value yfi j, under the random intercept model (2) (with no baseline adjustment term and assuming a common residual
variance)
y𝐹 𝑖𝑗 = (𝛽 + u1i) + (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗. (8)
this gave one complete ipd meta-analysis dataset of 1000 total participants, containing 100 participants in each of
10 trials, consisting of the following data for each individual: a trial indicator (triali), a treatment group indicator (treati j),
and an end-of-trial continuous outcome value (yfi j).
step 2: model fit and replication
using the generated data, we fitted a stratified intercept model (1) and a random intercept model (2) (without the
baseline adjustment term and assuming a common residual variance) separately to this simulated ipd, under all the combinations of estimation and ci derivation methods outlined in section 2. figure 1 provides a flow diagram summarizing
the possible combinations. each time a model was fitted (under a particular combination of estimation and ci derivation
methods), we stored the following: the summary treatment effect estimate, 𝜃
̂; its corresponding 95% ci; a binary indicator variable for coverage of 𝜃
̂ (ie, the value 1 if the 95% ci of 𝜃
̂ contained the true 𝜃, and 0 otherwise); estimates of any
variance parameters; model run time (from start of model fit to end of post estimation); and model convergence (1/0 for
convergence within 100 iterations/nonconvergence, respectively).
standard
ci
stratified
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
intercept
option
estimation
method
ci method standard
ci
random
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
figure 1 flow diagram of possible combinations of intercept option, estimation, and ci methods. ci, confidence interval;
kr, kenward-roger correction; ml, maximum likelihood estimation; reml, restricted maximum likelihood
for each model (stratified or random intercept) fitted to the data, this enabled us to obtain two estimates of 𝜃
(one each for the models fitted using ml and reml estimation, respectively) and four 95% cis for 𝜃
̂ (one for ml estimation with a standard ci derivation, and then one each for reml estimation with the standard, kr-corrected, and
satterthwaite-corrected ci derivations).
step 3: simulation replications
steps 1 and 2 were repeated until 1000 ipd meta-analysis datasets had been generated using the true parameter values
and procedure as outlined thus far, followed by application of the various intercept option, estimation, and ci methods to
each of the 1000 replicated datasets (note: 1000 simulations were chosen to give a monte carlo error of 0.7% on a coverage
of 95%).
step 4: summarizing performance
using the results obtained after step 3, the statistical properties of 𝜃
̂ under the different model specification and estimation options were assessed by summarizing the 1000 results obtained using the following metrics: mean percentage
(%) bias, empirical standard error (se), mse, coverage (separately for each ci method), convergence, and mean run time
(separately for each ci method). additionally, we considered the median percentage bias in the heterogeneity (𝜏2) of the
true treatment effects also. definitions of these performance measures are provided in web appendix a.
3.1.2 extended set of 38 scenarios changing number of trials, participants, between-trial
distributions, and data generating mechanisms
the base case scenario defined in section 3.1.1 was extended to further settings, leading to an extensive range of
38 scenarios in total (see table 1), which we now summarize.
we varied the number of trials (scenarios a1 and a2), so that k = 5, 10, and 20 were considered, which cover the typical
sizes of ipd meta-analyses in our experience. we also considered trials with differing sample sizes within an ipd, so that
ni (number of participants within trial i) was drawn from a uniform distribution, ni∼u(a, b). fixing a = 30, b = 1000
(scenario b1) allowed for mixed sample sizes, and having 5 trials with a = 30, b = 100 and 5 with a = 900, b = 1000 within
an ipd (scenario b2) tested the effect of a mix of small and large sample sizes only. lastly, fixing a = 30, b = 100 tested
the effect of having only small trials (scenario b3).
we also tested the combined effect of varying the number of trials and number of participants per trial simultaneously
(scenarios b1-a1, b1-a2, b2-a1, b2-a2), and we tested the effects of adjusting the magnitude of the intercept or treatment
effect heterogeneity (scenarios c1, c2, d1, d2).
scenarios 15 to 38 replicate the first 14 scenarios where possible, for modifications to the base case data generating
mechanism. first, to test the robustness of the normality of the intercept assumption in the random intercept model, we
altered the final step of the data generating mechanism in equation (8), so that the final outcome was calculated by
y𝐹 𝑖𝑗 = 𝛽i + (𝜃 + u2i)treat𝑖𝑗 + e𝑖𝑗 (9)
𝛽i ∼ (beta (15, 3)) × 220
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
).
therefore, the intercept term 𝛽i was now derived from a beta distribution with shape parameters of 15 and 3, which
represent a negatively skewed distribution that was then scaled by 220 to give sensible values for systolic blood pressure
(the outcome upon which the hypothetical data is based). an example density plot of this beta distribution for modeling
the intercept term is shown in web figure a.1.
secondly, we also considered a data generating mechanism with a common (fixed) treatment effect (ie, 𝜏2 = 0). here,
the fitted stratified and random intercept models were also modified to have a common treatment effect.
3.2 results
simulation results are shown in tables 2 and 3, covering most of the scenarios under the normal and beta distribution
intercept data generating mechanisms, across all options for specifying and estimating the intercept. these tables show the
mean percentage bias of the summary treatment effect estimate (𝜃
̂) (table 2) and the median percentage bias in its heterogeneity (𝜏̂2) (table 3). figure 2 graphically depicts the percentage coverage of the summary treatment effect estimate (𝜃
̂).
.
table 1 summary of the different simulation scenarios*
scenario data generation details modification from base case scenario
base case (i) number of trials, k = 10 -
(ii) number of participants in trial i, ni = 100 (fixed across all trials)
(iii) fixed treatment exposure of 50%
(iv) 𝜃 = −9.66 (summary treatment effect; negative value favors treatment group)
(v) 𝜏2 = 7.79 (between trial variation in 𝜃)
(vi) 𝛽 = 159.73 (mean response in control group)
(vii) 𝜏𝜷 2 = 233.99 (between trial variation in 𝛽)
(viii) 𝜎2 = 333.74 (residual variance)
a1 same as base case, except changed (i) k = 5
a2 same as base case, except changed (i) k = 20
b1 same as base case, except changed (ii) n ∼i u(30, 1000)
b2 same as base case, except changed (ii) n ∼i u(30, 100) for trials 1 to 5,
n ∼i u(900, 1000) for trials 6 to 10
b1-a1 same as base case, except changed (i) and (ii) k = 5 and n ∼i u(30, 1000)
b1-a2 same as base case, except changed (i) and (ii) k = 20 and n ∼i u(30, 1000)
b2-a1 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 and 2, n ∼i u(900, 1000) for trials 3 to 5
b2-a2 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 to 10, n ∼i u(900, 1000) for trials 11 to 20
b3 same as base case, except changed (ii) n ∼i u(30, 100)
c1 same as base case, except changed (vii) halving 𝜏𝛽 2 to 117
c2 same as base case, except changed (vii) doubling 𝜏𝛽 2 to 468
d1 same as base case, except changed (v) halving 𝜏2 to 3.9
d2 same as base case, except changed (v) doubling 𝜏2 to 15.6
*each scenario was repeated under the following data generating mechanisms: (1) random treatment effect with a normally distributed intercept, (2) random treatment effect with a
220*beta(15, 3) distribution for the intercept (except scenarios c1 and c2), and (3) common treatment effect with a normally distributed intercept (except scenarios d1 and d2).
abbreviations: k = number of trials, ni = number of participants in trial i, 𝜃 = summary treatment effect, 𝜏2 = between trial variation in summary treatment effect, 𝛽 = mean response in
control group, 𝜏𝛽 2 = between trial variation in mean response in control group, 𝜎2 = residual variance, u (a, b) = uniform distribution over the interval (a, b).
table 2 mean percentage bias of the summary treatment effect estimate (𝜃
̂) under different scenarios, for the
random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified (1) and random (2) intercept models, under each of the different
estimation options considered
mean percentage bias of 𝜽̂
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −0.01 0.00 −0.01 −0.01 0.34 0.31 0.33 0.29
a1 −0.90 −0.90 −0.90 −0.90 −0.02 0.13 −0.06 0.10
a2 0.15 0.18 0.16 0.18 −0.48 −0.41 −0.47 −0.40
b1 0.67 0.58 0.68 0.58 −0.57 −0.63 −0.58 −0.63
b2 −0.47 −0.59 −0.47 −0.56 0.29 0.27 0.33 0.28
b1-a1 0.54 0.53 0.53 0.53 −0.14 −0.27 −0.11 −0.24
b1-a2 −0.10 −0.11 −0.10 −0.11 0.08 −0.01 0.10 0.01
b2-a1 −0.41 −0.43 −0.37 −0.41 0.46 0.52 0.05 −0.17
b2-a2 −0.45 −0.41 −0.44 −0.42 −0.45 −0.37 −0.37 −0.34
b3 0.19 0.24 0.21 0.25 1.36 1.34 1.20 1.20
c1 −0.03 −0.02 −0.03 −0.02 n/a n/a n/a n/a
c2 −0.01 0.07 −0.01 0.07 n/a n/a n/a n/a
d1 −0.10 −0.13 −0.10 −0.13 0.26 0.34 0.24 0.32
d2 0.13 0.12 0.13 0.12 0.46 0.49 0.45 0.46
* see table 1 for full data generation details relating to each scenario. true value for 𝜃 is −9.66.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
we focus on the results when assuming a random treatment effect. further results assuming a common treatment effect
data generating mechanism and for additional performance measures (percentage convergence of models, numerical
percentage coverage of the summary treatment effect estimate, average run time of simulations, and empirical se and
mse of the summary treatment effect estimate) are shown in the supplementary material (web appendices b and c,
respectively). in the following, we summarize the key findings.
3.2.1 convergence of models
under a random treatment effect data generating mechanism, the proportion of models that converged was consistently
high, with a minimum convergence of 94.3% across all situations (web table c.i).
note that all other performance measures to follow are estimated conditional on model convergence.
3.2.2 bias of summary treatment effect estimate
generally, there were negligible differences in mean percentage bias of 𝜃
̂ between ml and reml estimation options for
either model (stratified or random intercept), under any given scenario and data generating mechanism (table 2 and
web table b.i). nor were there any important differences in the mean percentage bias of 𝜃
̂ between the stratified model
and random intercept model. furthermore, mean bias was close to zero in all situations and only reached a maximum
absolute percentage of 1.36%.
3.2.3 bias of estimated between-trial variance of treatment effects
for either model (stratified or random intercept), under any given scenario and data generating mechanism, using ml
always produced more downwardly biased estimates than reml (table 3), as expected.14-18 for example, for the base
table 3 median percentage bias of the between-trial variance of treatment effects (𝜏̂2), under different scenarios
for the random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified and random intercept models, under each of the estimation options considered
median percentage bias of ̂𝝉𝟐
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −100.00 −16.86 −41.50 −15.85 −100.00 −14.36 −56.17 −32.86
a1 −100.00 −36.88 −80.33 −33.10 −100.00 −73.62 −100.00 −80.15
a2 −100.00 −8.59 −20.86 −7.74 −100.00 −13.96 −39.78 −25.06
b1 −49.64 −10.74 −22.94 −10.09 −100.00 −14.23 −28.91 −16.39
b2 −56.93 −18.03 −35.11 −17.84 −72.90 −17.92 −35.95 −19.81
b1-a1 −77.28 −19.57 −42.62 −18.45 −100.00 −27.27 −54.23 −30.91
b1-a2 −40.64 −5.83 −13.08 −6.31 −88.22 −9.50 −19.59 −13.53
b2-a1 −72.73 −28.35 −56.23 −28.10 −100.00 −34.30 −64.33 −32.61
b2-a2 −36.66 −4.98 −14.36 −5.39 −50.99 −4.84 −12.86 −5.35
b3 −100.00 −28.68 −61.86 −24.74 −100.00 −17.42 −81.05 −48.75
c1 −100.00 −16.72 −39.54 −14.38 n/a n/a n/a n/a
c2 −100.00 −16.86 −40.56 −15.94 n/a n/a n/a n/a
d1 −100.00 −19.20 −66.97 −24.63 −100.00 −33.67 −99.98 −62.07
d2 −100.00 −11.65 −30.04 −11.79 −100.00 −10.50 −37.84 −22.19
* see table 1 for full data generation details relating to each scenario. true value for 𝜏2 is 7.79, except scenarios d1 and d2 where 𝜏2
is equal to 3.9 and 15.6, respectively.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
case scenario with the random intercept model, under the normal intercept data generating mechanism, the median
percentage bias using reml estimation was −15.9% compared to −41.5% using ml estimation. the bias was worse when
using a stratified intercept model (due to the extra number of parameters to estimate), as ml estimation often produced
a downward median bias of 100%.
when using reml estimation, there were generally only small differences between random and stratified intercept
models in terms of bias of the between-trial variance of treatment effects; however, while better than ml, downward bias
was not removed entirely with reml. furthermore, the overall size of the bias was typically greater in the beta distribution
intercept case than in the normal distribution intercept case, regardless of which model was used.
3.2.4 empirical se and mse of summary treatment effect estimate
there were negligible differences in empirical se or mse of 𝜃
̂ between the two models (stratified or random intercept),
under any given scenario and data generating mechanism (web tables c.viii to c.x).
3.2.5 coverage of summary treatment effect estimate
there were marked differences observed in the coverage of 𝜃
̂ across the different estimation approaches (ml or reml)
and ci derivations (standard, kr, or satterthwaite), as now explained.
(i) under a normal distribution intercept generating mechanism
we consider first the normal distribution intercept generating mechanism (figure 2a and web table c.ii). across both
models and all scenarios, ml with standard ci (ml + standard) derivation always exhibited under-coverage compared to
the other options (reml+standard, reml+kr, reml+satterthwaite). for example, for scenario b2 using the stratified
intercept model, the percentage coverage using ml + standard was 81.3% compared to 88.8%, 95.8%, and 94.8% using
figure 2 percentage coverage of the summary treatment effect estimate (𝜃
̂) under different scenarios for the random treatment effect
with normal (figure 2a) and beta distributions (figure 2b) for the intercept data generating mechanisms, for stratified (left) and random
(right) intercept models, under each of the estimation and ci derivation options considered. options: ml, maximum likelihood estimation
with standard confidence interval (ci) derivation; reml, restricted maximum likelihood estimation with standard ci derivation;
reml+kr, reml estimation with kenward-roger ci derivation; reml+satt, reml estimation with satterthwaite ci derivation [colour
figure can be viewed at wileyonlinelibrary.com]
reml+standard, reml+kr and reml+satterthwaite, respectively. the random intercept model always performed
better with respect to coverage under ml than the stratified intercept model under ml, likely due to the reduction in the
number of parameters that needed estimation. for example, when considering only small trials (scenario b3), percentage
coverage improved from 91.3% to 94.5% (close to the nominal 95% level), when comparing the stratified to a random
intercept model with ml estimation.
using reml substantially improved on the coverage obtained from ml and removed any important differences
between the stratified and random intercept models. however, for either model (stratified or random intercept),
reml+standard still had important under-coverage in some scenarios. for example, in scenario b2-a1, a percentage
coverage of 85.7% and 85.8% was observed, under a stratified and random intercept model, respectively.
the reml+kr approach generally improved on the coverage compared to reml+standard, again with no important
differences observed between the stratified and random intercept models. percentage coverage ranged from 95.1 to 98.8%
using reml+kr, while the percentage coverage ranged from 85.7% to 94.9% using reml+standard. the improvement
gained by using reml+kr was especially important for scenarios that involved at least 10 trials and a large variation
in sample sizes (b1, b2, b1-a2, b2-a2). for example, for scenario b2 (five small and five large sample sized trials, with
average sample size 66 and 949 in the small and large trials, respectively), percentage coverage from the stratified intercept
model was 88.8%, using reml+standard, but 95.8% using reml+kr.
using reml+satterthwaite gave very similar results to reml+kr. occasionally, there was some over-coverage using
reml+kr or reml+satterthwaite, particularly when using a low number of trials (k = 5). for example, coverage was
close to 99% (regardless of which model was used), in a setting of k = 5 trials with an equal number of participants
per trial (scenario a1; ni = 100), and in a setting of k = 5 trials with some small-sized and some large-sized trials (scenario
b2-a1; 2 small trials where ni∼u(30, 100), and 3 large trials where ni∼u(900, 1000)).
(ii) under a beta distribution intercept generating mechanism
for the beta distribution intercept generating mechanism (figure 2b and web table c.iii), using reml+standard again
gave better coverage than using ml, and using reml+kr or reml+satterthwaite generally further improved upon this
coverage (ie, moved it closer to 95%), especially with scenarios concerning at least 10 trials that had a large variation in
sample sizes.
as before, under ml estimation, the random intercept model showed better estimates of between-trial variance and
improved coverage (closer to 95%) than the stratified intercept model. however, differences between the two models were
generally small for estimation under reml (with or without a 95% ci correction).
3.2.6 common treatment effect data generating mechanism
results based on a common (fixed) treatment effect data generating mechanism are shown in web appendix b. all fitted
models assumed a common treatment effect and converged every time (ie, 100% convergence), and there was negligible
difference in mean percentage bias of 𝜃
̂ between ml and reml estimation options for either model (stratified or
random intercept), or between either model (web table b.1). the percentage coverage results were stable across all comparisons, ranging from 93.8 to 96.0%, with negligible differences between the various models and estimation options
(web figure b.1).
3.2.7 key findings
a summary of the key findings from this simulation study for settings with between-trial heterogeneity in the treatment
effect is given in figure 3.
figure 3 key simulation findings and recommendation for estimating a summary treatment effect based on a one-stage individual
participant data (ipd) meta-analysis of randomized trials with a 1:1 treatment:control allocation ratio and a continuous outcome, with
between-study heterogeneity in the treatment effect. ci, confidence interval; ml, maximum likelihood estimation; mse, mean square error;
reml, restricted maximum likelihood; se, standard error
4 illustration of methods and key findings in a real example
the international weight management in pregnancy (i-wip) collaborative group dataset includes ipd from 36 trials
(12,447 women), collected for a health technology assessment report in 2017.24 the authors investigated the association
between diet and lifestyle interventions to prevent weight gain in pregnancy and several other primary outcomes. here, we
present ipd meta-analysis results using the i-wip dataset, for illustration purposes only, to demonstrate the key findings
from our simulation study. we include only trials that collected follow-up outcome values for weight in pregnancy and
apply a one-stage ipd model for this continuous outcome, with model assumptions in line with our simulation study
analysis. however, while we did not generate any baseline imbalances in our simulation data, baseline weight imbalance
was present in some trials from the i-wip data. to remove this imbalance, we apply a baseline adjustment in our models,
as is recommended.21
table 4 shows ipd meta-analysis results for a random sample of 5 and 10 trials that investigated exercise interventions
and for 20 trials (using all 15 exercise trials, plus 5 additional trials that investigated mixed interventions).
these results are in agreement with the key findings observed in section 3.2 and summarized in figure 3. firstly,
the magnitude of summary treatment effect estimate was similar throughout, irrespective of model used or estimation
method. secondly, with ml estimation, the stratified intercept model gave narrower 95% cis and smaller estimates of the
between trial variance than the random intercept model, especially with k = 20 trials. thirdly, using reml overcame this
discrepancy, with now very similar results between the random and stratified intercept models; in addition, the 95% cis
were wider when using reml, due to the larger estimates of the between trial variance. fourthly, applying a kr or
satterthwaite correction in addition to reml further widened the 95% cis. finally, although 95% cis were slightly wider
when using a kr correction instead of satterthwaite, results were generally similar from these two corrections, especially
with k = 20 trials.
5 discussion
5.1 key findings
in summary, we have conducted an extensive simulation study to examine the estimation of a summary treatment effect
using a one-stage ipd meta-analysis model for a continuous outcome. specifically, we examined different options for
specifying the trial-specific intercepts and compared different options for parameter estimation and ci derivation. fourteen different scenarios were tested (varying the number of trials, number of participants per trial, and heterogeneity of
parameters), for each of three different data generating mechanisms (encompassing a common and random treatment
effect with a normally distributed intercept, as well as a beta distributed intercept and random treatment effect). all scenarios assumed a 1:1 treatment:control allocation ratio, and a data generating mechanism that was based on a random
intercept model; hence, our conclusions are restricted to this context.
our key findings, for settings with heterogeneity in treatment effect, were illustrated using a real example, and these
are summarized in figure 3. firstly, the results suggest that, as long as the same estimation method is used, there are no
important differences between the stratified and random intercept models in terms of bias, empirical se or mse for the
summary treatment effect estimate. indeed, the mean bias in 𝜃
̂ was close to zero throughout, which is perhaps expected
given the statistical theory underpinning linear mixed models. furthermore, when using reml (with or without a ci
correction), there were generally no important differences in coverage performance between the stratified and random
intercept models. interestingly, the random intercept model (which assumes normality of the intercept) performed well
even when the trial intercepts were drawn from a highly asymmetric beta distribution. kahan and morris25 also found
that misspecifying the random intercept distribution of random effects models did not impact treatment effect results.
secondly, the kr and satterthwaite corrections generally performed similarly in terms of improving the coverage and
were especially effective for scenarios involving at least 10 trials with a mix of small and large sample sizes, but also
considerably increased mean run time in these instances (see web tables c.v to c.vii). the satterthwaite correction
always had a similar or quicker average run time than kr (sometimes by more than eight times). one could surmise from
the similarity in coverage performance that the main impact of both corrections is in the use of a t-distribution to derive
cis and that the kr adjusted variance of the summary estimate has relatively less impact.
thirdly, when using ml estimation, the random intercept model always showed better or comparable coverage to the
stratified intercept model (closer to 95%). this is likely due to the random intercept model having a reduced number of
.
table 4 results from baseline weight adjusted individual participant data meta-analysis of i-wip data: summary treatment effect estimate (
̂
𝜃) with 95% confidence interval and
between-trial variance of treatment effects estimate (̂𝜏2). from meta-analysis with different numbers of trials (k = 5, 10, or 20), and assuming a random treatment effect and a common
residual variance throughout
̂
𝜽 (95% ci); ̂𝝉𝟐
method for modeling stratified random
intercept intercept intercept
estimation ml reml reml+kr reml+satt ml reml reml+kr reml+satt
number of trials
5 −1.172 −1.172 −1.172 −1.172 −1.170 −1.171 −1.171 −1.171
(−1.811,−0.534); (−1.815,−0.530); (−3.114, 0.770); (−2.712, 0.367); (−1.811,−0.529); (−1.813,−0.528); (−3.072, 0.731); (−2.681, 0.340);
8.58e−17 3.94e−15 3.94e−15 3.94e−15 2.95e−14 4.51e−12 4.51e−12 4.51e−12
10 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972
(−1.479,−0.465); (−1.482,−0.462); (−1.740,−0.204); (−1.653,−0.291); (−1.481,−0.462); (−1.482,−0.462); (−1.731,−0.212); (−1.646,−0.298);
1.97e−16 2.58e−12 2.58e−12 2.58e−12 9.52e−11 5.94e−16 5.94e−16 5.94e−16
20 −0.821 −0.820 −0.820 −0.820 −0.830 −0.830 −0.830 −0.830
(−1.102,−0.540); (−1.243,−0.396); (−1.298,−0.342); (−1.286,−0.354); (−1.217,−0.442); (−1.235,−0.426); (−1.290,−0.370); (−1.276,−0.384);
1.11e−14 0.317 0.317 0.317 0.210 0.258 0.258 0.258
ci = confidence interval
options: ml, maximum likelihood estimation with standard ci derivation; reml, restricted maximum likelihood estimation with standard ci derivation; reml+kr, reml estimation with kenward-roger
ci derivation; reml+satt, reml estimation with satterthwaite ci derivation.
parameters, and thus improved ml estimation of the between-trial variance. a similar finding was also recently shown
by jackson et al26 for one-stage meta-analysis models for a binary outcome. nevertheless, even the random intercept
model produced downwardly biased estimates of the between-trial variance using ml and low coverage. using reml
is therefore important, to improve on this coverage. indeed, coverage is more consistently near 95% when using reml
with either a kr or satterthwaite correction. however, on some occasions (particularly, when there are a low number of
trials), the kr and satterthwaite corrections lead to over-coverage. this is similar to the hartung-knapp sidik-jonkman
correction to 95% cis following a two-stage analysis,27,28 which generally gives a more suitable coverage than a standard
95% ci, although on occasion is overly conservative.29
if there is genuinely no heterogeneity in treatment effect across trials, however, our findings suggest that there are
generally no differences in mean bias, empirical se, mse, or coverage for the treatment effect between the stratified
and random intercept models, for any estimation method, ci derivation approach, and under any simulation scenario.
however, in our experience, situations of completely homogeneous treatment effects are unlikely.
unreported simulations
following recent work by morris et al5 and jackson et al,26 which considered an alternative coding for the binary treatment group variable (+0.5/−0.5 for treatment/control groups, respectively), we also tested this treatment group coding
for our reml estimation simulation results but found only small differences in performance results compared to the 1/0
coding. hence, we did not present the results here. we also tested using stratified (instead of common) residual variances
for both the data generating mechanisms and models fitted. again, no difference in performance of the summary treatment effect estimate was observed, suggesting that the ipd model may be robust to the (mis) specification of the residual
variances. morris et al5 also found that assuming common or distinct residual variances, in a common treatment effect
ipd meta-analysis setting, has very little impact on the precision of the summary effect when the number of patients per
trial is over 25. in general, from a point of principle, we recommend a separate residual variance for each trial, but in situations where this has convergence problems, a common residual variance would seem apt. further research of this issue
would be welcome.
5.2 recommendation
for researchers conducting a one-stage ipd meta-analysis of randomized trials with a 1:1 treatment:control allocation
ratio and a continuous outcome and aiming to estimate a summary treatment effect that is heterogeneous across trials,
we recommend that either a stratified or random intercept model is used, and estimated using reml, ideally followed
by a 95% ci derived using either the kr or satterthwaite approaches. in our simulations, this approach gave close to zero
mean bias in the summary treatment effect estimate and coverage generally close to 95%, except in a few situations where
there was over-coverage (particularly, when there were a low number of trials).
using reml with a kr correction for linear mixed models based on continuous outcomes has already been proposed
by some researchers,30,31 while literature advocating the merits of the satterthwaite correction is less common. however,
in our simulations, we found that the satterthwaite correction generally obtains similar results to the kr approach, hence
making for an excellent alternative.
5.3 limitations and further research
throughout this simulation study, we focused solely on synthesizing trials containing a 1:1 treatment:control allocation ratio; hence, an important limitation is that our conclusions may not hold under settings involving other treatment
allocation ratios.
in addition, we have focused solely on ipd of continuous outcomes, hence another important limitation is that our
conclusions are not necessarily generalizable to other popular outcome types in the meta-analytical field, such as binary
and time-to-event outcomes. binary outcomes, for example, are more complex to deal with than continuous outcomes,
as a logistic mixed effects model is nonlinear, and hence, the corresponding maximum likelihood function has no closed
form. jackson et al26 recently investigated the use of ml estimation and found that a stratified intercept model leads
to substantial downward bias in between-trial variance estimates and under-coverage of cis for the summary result,
which increases as the number of trials (and thus parameters) increases. interestingly, the issue was resolved when using
a + 0.5/−0.5 coding for the treatment variable, rather than a 1/0 coding, or when placing random effects on the trial
intercept.26 mcneish32 investigated logistic mixed models by either retaining the nonlinearity of the model and making
an approximation for the likelihood function or linearly approximating the model to give the likelihood function a closed
form (pseudo-likelihood approach). the latter option was shown to be favorable (under the specific conditions of the
study), by use of a residual penalized quasi-likelihood with a kr correction.
while our simulation study did consider an extensive range of scenarios—we varied the number of trials, number of
participants per trial, and heterogeneity of parameters—we recognize that our conclusions were based on a final score
model that did not adjust for baseline outcome value. often, the ancova model should be used, as in our applied
example, because there will be baseline imbalances in practice. however, as baseline values did not vary across individuals in our simulation study, using a final score analysis model rather than ancova was appropriate. when using
one-stage ancova ipd models, an additional issue is using stratified adjustment terms or placing a random effect on
the adjustment term. based on our study findings, we expect that, with reml, either approach should be suitable.
we also assumed independence of the two random effects (ie, a covariance of zero) when assuming random intercept
and random treatment effects, both in the data generating mechanism and when fitting the corresponding model. their
correlation could be taken into account if deemed sensible13; however, we did not consider this alternative assumption
in our simulation study, largely due to the added complexity and difficulty in estimating the correlation parameter in
practice with few trials. importantly, it is perhaps likely that the effect of treatment could be correlated with the control
group outcome, and therefore, the most appropriate assumption needs further consideration.
another limitation is that we did not consider prediction intervals. these allow us to make predictive inferences of
the potential treatment effect in a single setting of application.33 some researchers argue that prediction intervals offer
a more appropriate summary of trial findings than cis of the average effect.34 however, partlett and riley18 showed in a
two-stage ipd meta-analysis setting that there was considerable under-coverage of prediction intervals in some situations.
for example, under-coverage was observed in settings involving a low heterogeneity or with varied trial sample sizes and
was not improved upon by increasing the number of trials or using ci corrections such as kr. hence, we did not consider
it useful to consider prediction intervals in our study.
finally, we could have considered a bayesian approach to our simulation study, which is an alternative to frequentist
methods, and a natural way to account for all parameter uncertainty, to make predictions and to derive (joint) probabilistic
statements regarding parameters of interest. however, we deemed this extension to be beyond the scope of this paper.
yet, if a bayesian approach is to be used in practice, bayesians still need to choose between random or stratified intercept
one-stage ipd models, which is something that our work can help with.
5.4 conclusions
in an ipd meta-analysis of trials with a 1:1 treatment:control allocation ratio and a continuous outcome, aiming to estimate a summary treatment effect that is heterogeneous across trials, our findings suggest that researchers use either a
stratified or random intercept model with reml estimation and ideally derive 95% cis using either the kr or satterthwaite approach. further work is needed to improve upon coverage in a few situations where the kr and satterthwaite
intervals are overly conservative. such situations include when there are a low number of trials; these are also situations
where corrections to cis in a two-stage ipd meta-analysis are overly conservative.18


<|EndOfText|>

minimum sample size for developing a multivariable
prediction model: part ii - binary
and time-to-event outcomes

when designing a study to develop a new prediction model with binary or
time-to-event outcomes, researchers should ensure their sample size is adequate
in terms of the number of participants (n) and outcome events (e) relative to
the number of predictor parameters (p) considered for inclusion. we propose
that the minimum values of n and e (and subsequently the minimum number
of events per predictor parameter, epp) should be calculated to meet the following three criteria: (i) small optimism in predictor effect estimates as defined by
a global shrinkage factor of ≥0.9, (ii) small absolute difference of ≤ 0.05 in the
model's apparent and adjusted nagelkerke's r2, and (iii) precise estimation of
the overall risk in the population. criteria (i) and (ii) aim to reduce overfitting
conditional on a chosen p, and require prespecification of the model's anticipated cox-snell r2, which we show can be obtained from previous studies. the
values of n and e that meet all three criteria provides the minimum sample
size required for model development. upon application of our approach, a new
diagnostic model for chagas disease requires an epp of at least 4.8 and a new
prognostic model for recurrent venous thromboembolism requires an epp of at
least 23. this reinforces why rules of thumb (eg, 10 epp) should be avoided.
researchers might additionally ensure the sample size gives precise estimates of
key predictor effects; this is especially important when key categorical predictors have few events in some categories, as this may substantially increase the
numbers required.
keywords
binary and time-to-event outcomes, logistic and cox regression, multivariable prediction model,
pseudo r-squared, sample size, shrinkage
1 introduction
statistical models for risk prediction are needed to inform clinical diagnosis and prognosis in healthcare.1-3 for example,
they may be used to predict an individual's risk of having an undiagnosed disease or condition (“diagnostic prediction
model”), or to predict an individual's risk of experiencing a specific event in the future (“prognostic prediction model”).

they are typically developed using a multivariable regression framework, such as logistic or cox (proportional hazards)
regression, which provides an equation to estimate an individual's risk based on their values of multiple predictors (such
as age and smoking, or biomarkers and genetic information). well-known examples are the wells score for predicting
the presence of a pulmonary embolism4,5
; the framingham risk score and qrisk2,6,7 which estimate the 10-year risk
of developing cardiovascular disease (cvd); and the nottingham prognostic index, which predicts the 5-year survival
probability of a woman with newly diagnosed breast cancer.8,9
researchers planning or designing a study to develop a new multivariable prediction model must consider sample size
requirements for their development data set. our related paper considered this issue for prediction models of a continuous
outcome using linear regression.10 here, we focus on binary and time-to-event outcomes, such as the risk of already having
a pulmonary embolism, or the risk of developing cvd in the next 10 years. in this situation, the effective sample size is
often considered to be the number of outcome events (eg, the number with existing pulmonary embolism, or the number
diagnosed with cvd during follow-up). in particular, a well-used “rule of thumb” for sample size is to ensure at least
10 events per candidate predictor (variable),11-13 where “candidate” indicates a predictor in the development data set that
is considered, before any variable selection, for inclusion in the final model. note that, if a predictor is categorical with
three of more categories, or continuous and modelled as a nonlinear trend, then including the predictor will require two
or more parameters being included in the model. therefore, we refer to events per predictor parameter (epp) here, rather
than events per variable.
the 10 epp rule has generated much debate. some authors claim that the epp can sometimes be lowered below 10.14
in contrast, harrell generally recommends at least 15 epp,15 and others identify situations where at least 20 epp or up
to 50 epp are required.16-19 however, a concern is that any blanket rule of thumb is too simplistic, and that the number
of participants required will depend on many intricate aspects, including the magnitude of predictor effects, the overall
outcome risk, the distribution of predictors, and the number of events for each category of categorical predictors.16 for
example, courvoisier et al20 concluded that “there is no single rule based on epp that would guarantee an accurate
estimation of logistic regression parameters.” a new sample size approach is needed to address this.
in this article, we propose the sample size (n) and number of events (e) in the model development data set must, at
the very least, meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global
shrinkage factor of ≥0.9, (ii) small absolute difference of ≤ 0.05 in the model's apparent and adjusted nagelkerke's r2, and
(iii) precise estimation of the overall risk or rate in the population (or similarly, precise estimation of the model intercept
when predictors are mean centred). the values of n and e (and subsequently epp) that meet all three criteria provide the
minimum values required for model development. criteria (i) and (ii) aim to reduce the potential for a developed model to
be overfitted to the development data set at hand. overfitting leads to model predictions that are more extreme than they
ought to be when applied to new individuals, and most notably occurs when the number of candidate predictors is large
relative to the number of outcome events. a consequence is that a developed model's apparent predictive performance
(as observed in the development data set itself) will be optimistic, and its performance in new data will usually be lower.
therefore, it is good practise to reduce the potential for overfitting when developing a prediction model,15 which criteria
(i) and (ii) aim to achieve. in addition, criterion (iii) aims to ensure that the overall risk (eg, by a key time point for
prediction) is estimated precisely, as fundamentally, before tailoring predictions to individuals, a model must be able to
reliably predict the overall or mean risk in the target population.
the article is structured as follows. section 2 introduces our proposed criterion (i), for which key concepts of a global
shrinkage factor and the cox-snell r2 are introduced.21 the latter needs to prespecified to utilise our sample size formula,
and so in section 3, we suggest how realistic values of the cox-snell r2 can be obtained in advance of any data collection,
eg, by using published information from an existing model in the same field, including values of the c statistic or alternative r2 measures. extension to criteria (ii) and (iii) is then made in section 4. section 5 then provides two examples,
which demonstrate our sample size approach for diagnostic and prognostic models. section 6 raises a potential additional
criteria to consider: ensuring precise estimates of key predictor effects, to help ensure precise predictions across the entire
spectrum of predicted risk. section 7 concludes with discussion.
2 sample size required to minimise overfitting of predictor
effects
to adjust for overfitting during model development (and thereby improve the model's predictive performance in
new individuals), statistical methods for penalisation of predictor effect estimates are available, where regression
coefficients are shrunk toward zero from their usual estimated value (eg, from standard maximum likelihood
estimation).22-26 van houwelingen notes that “ … shrinkage works on the average but may fail in the particular unique
problem on which the statistician is working.”22 therefore, it is important to minimise the potential for overfitting during
model development, and this criterion forms the basis of our first sample size calculation. our approach is motivated
by the concept of a global shrinkage factor (a measure of overfitting), and so we begin by introducing this, before then
deriving a sample size formula.
2.1 concept of a global shrinkage for logistic and cox regression
the concept of shrinkage (penalisation) was outlined in our accompanying paper,10 and is explained in detail
elsewhere.1,15,27 here, we focus on using a global shrinkage factor (s), sometimes referred to as a uniform shrinkage
factor. consider a logistic regression model has been fitted using standard maximum likelihood estimation (ie, traditional
and unpenalised estimation). subsequently, s can be estimated (eg, using bootstrapping,28 or via a closed-form solution;
see section 2.2) and applied to the estimated predictor effects, so that the revised model is
𝑙𝑛 ( pi
1 − pi
)
= 𝛼∗ + s
(
𝛽̂
1x1i + 𝛽̂
2x2i + 𝛽̂
3x3i +···)
. (1)
here, pi is the outcome probability for the ith individual, the 𝛽̂ terms denote the original predictor effect estimates
(ln odds ratios) from maximum likelihood, and 𝛼* is the intercept that has been re-estimated (after shrinkage of predictor
effects) to ensure perfect calibration-in-the-large, such that, the overall predicted risk still agrees with the overall observed
risk in the development data set (for details on how to do this, we refer to the works of harrell15 and steyerberg1
). similarly, after fitting a proportional hazards (cox) regression model using standard maximum likelihood, the model can be
revised using
hi(t) = h0(t)
∗ exp (
s
(
𝛽̂
1x1i + 𝛽̂
2x2i + 𝛽̂
3x3i +···)) , (2)
where hi(t) is the hazard rate of the outcome over time (t) for the ith individual and ho(t)
* is the baseline hazard function
re-estimated (after shrinkage of predictor effects) to ensure the predicted and observed outcome rates agree for the development data set as whole. compared to the original (nonpenalised) models, the revised models (1) and (2) will shrink
predicted probabilities away from zero and one, toward the overall mean outcome probability in the development data set.
example of a global shrinkage factor
van diepen et al developed a prognostic model for 1-year mortality risk in patients with diabetes starting dialysis.29
they use a logistic regression framework, with backwards selection to choose predictors in a dataset of 394 patients with
84 deaths by 1 year, and the estimated model is shown in table 1. to examine overfitting, the authors use bootstrapping to
estimate a global shrinkage factor of 0.903, indicating that the original model was slightly overfitted to the data. therefore,
a revised prediction model was produced by multiplying the original 𝛽̂ coefficients (ln odds ratios) from the original
logistic regression model by a global shrinkage factor of s = 0.903.
table 1 example of global shrinkage applied to a prognostic model for 1-year mortality risk in patients with diabetes starting
dialysis29
developed (unpenalised) model final (penalised) model adjusted for overfitting
intercept 𝜶 𝜶 ̂ *
1.962 1.427
predictor 𝜷
̂ s𝜷
̂ = 0.903𝜷
̂
age (years) 0.047 0.042
smoking 0.631 0.570
macrovascular complications 1.195 1.078
duration of diabetes mellitus (years) 0.026 0.023
karnofsky scale −0.043 −0.039
haemoglobin level (g/dl) −0.186 −0.168
albumin level (g/l) −0.060 −0.054

2.2 expressing sample size in terms of a global shrinkage factor
bootstrapping is an excellent way to calculate the shrinkage factor postestimation, but (as it is a resampling method) is
not useful for us in advance of data collection. an alternative approach to calculating a global shrinkage factor is to use
the closed form “heuristic” shrinkage factor of van houwelingen and le cessie,23 defined by
svh = 1 − p
lr , (3)
where p is the total number of predictor parameters for the full set of candidate predictors (ie, all those considered for
inclusion in the model) and lr is the likelihood ratio (chi-squared) statistic for the fitted model defined as
lr = −2 (ln lnull − ln lmodel) , (4)
where ln lnull is the log-likelihood of a model with no predictors (eg, intercept-only logistic regression model), and ln lmodel
is the log-likelihood of the final model. in our related paper on linear regression, we used the copas shrinkage estimate that
is similar to equation (3), but with p replaced by p + 2. in our experience, svh performs better for generalised linear models
than the copas estimate, with svh further from 1 and closer to the corresponding estimate obtained from bootstrapping.
copas also notes that, unlike for linear regression, a formal justification for replacing p by p + 2 in equation (2) has not
been proved for logistic regression.30
hence, we use equation (3) as our shrinkage estimate (ie, our measure of overfitting) for logistic and cox regression
models, which now motivates our sample size approach to meet criterion (i). first, let us re-express the right-hand side
of equation (3) in terms of sample size (n), number of candidate predictor parameters (p), and the cox-snell generalised
r2.
21 the latter is also known as the maximum likelihood r2, the likelihood ratio r2, or magee's r2,
31 and it provides a
generalisation (eg, to logistic and cox regression models) of the well-known proportion of variance explained for linear
regression models. let us use r2
cs_app to denote the apparent (“app”) estimate of a prediction model's cox-snell (“cs”) r2
performance as obtained from the model development data set. it can be shown (eg, see the works of magee31 or hendry
and nielsen32) that the lr statistic can be expressed in terms of the sample size (n) and r2
cs_app as follows:
lr = −n ln (
1 − r2
cs_app)
. (5)
this leads to the cox-snell generalised definition of the apparent r2 expressed in terms of the lr value for any regression
model, including logistic and cox regression
r2
cs_app = 1 − exp (−lr
n
)
. (6)
applying equation (5) within equation (3), the van houwelingen and le cessie shrinkage factor becomes
svh = 1 + p
n ln (
1 − r2
cs_app) . (7)
2.3 criterion (i): calculating sample size to ensure a shrinkage factor ≥ 0.9
equation (7) provides a closed-form solution for the expected shrinkage conditional on n, p, and r2
cs_app. therefore, if
we could specify a realistic value for r2
cs_app in advance of our study starting, we could identify values of n and p that
correspond to a desired shrinkage factor (eg, 0.9), thus informing the required sample size. however, a major problem is
that r2
cs_app is a postestimation measure of model fit, whereas for a sample size calculation, this needs to be specified in
advance of collecting the data when designing a new study. furthermore, due to overfitting in the model development
data set, the observed r2
cs_app is generally an upwardly biased (optimistic) estimate of the cox-snell r2 as it is estimated
in the same data used to develop the model. thus, in new data, the actual cox-snell r2 peformance is likely to be lower.
therefore, we need to re-express svh in terms of r2
cs_adj, an adjusted (approximately unbiased) estimate of the model's
expected r2
cs performance in new individuals from the same population. in other words, r2
cs_adj is a modification of r2
cs_app
to adjust for optimism (caused by overfitting) in the model development data set. for generalised linear models such as
logistic regression, mittlboeck and heinzl suggest that r2
cs_adj can be obtained by33
r2
cs_adj = svhr2
cs_app (8)
as the expected value of this r2
cs_adj corresponds to the underlying population value.33 by rearranging equation (8), we
can express r2
cs_app in terms of r2
cs_adj
r2
cs_app =
r2
cs_adj
svh
. (9)
applying equation (9) within equation (7), we can now express svh in terms of r2
cs_adj, rather than r2
cs_app
svh = 1 + p
n ln (
1 − r2
cs_adj
svh ). (10)
finally, a simple rearrangement of equation (10) leads to a closed-form solution for the required sample size to develop
a prediction model conditional on p, svh and r2
cs_adj
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ). (11)
for example, for developing a new logistic regression model based on up to 20 candidate predictor parameters with an
anticipated r2
cs_adj of at least 0.1, then to target an expected shrinkage of 0.9, we need a sample size of
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ) = 20
(0.9 − 1) ln (
1 − 0.1
0.9
) = 1698,
and thus 1698 individuals.
2.4 translating the calculated sample size to the number of events and epp
it may be surprising that the overall outcome proportion (or overall outcome rate) is not directly included in the right-hand
side of the sample size equation (11), especially because the total number of events, e, (which depends on the outcome
proportion or rate) is often considered the effective sample size for binary and time-to-event outcomes.15 however, the
outcome proportion (rate) is indirectly accounted for in the sample size calculation via the chosen r2
cs_adj, as the maximum value of r2
cs_adj for the intended population of the model depends on the overall outcome proportion (rate) for
that population. as the outcome proportion decreases, the maximum value of r2
cs decreases. this is explained further in
section 3.4. therefore, after n is derived from the sample size equation (11), e can be obtained by combining the calculated
n with the outcome proportion (rate) for the intended population. similarly, epp can be obtained.
for example for binary outcomes, e = n𝜙 and epp = n𝜙/p, where 𝜙 is the overall outcome proportion in the target
population (ie, the overall prevalence for diagnostic models, or the overall cumulative incidence by a key time point
for prognostic models). in our aforementioned hypothetical example, where 1698 subjects were needed based on an
r2
cs_adj of 0.1 and svh of 0.9, then if the intended setting has 𝜙 of 0.1 (ie, overall outcome risk is 10%), the required
e = 1698 × 0.1 = 169.8. with 20 predictor parameters, the required epp = (1698 × 0.1)/20 = 8.5. however, if the intended
setting has 𝜙 of 0.3, then e = 509.4 and epp = 25.5. the big change in epp is because, although the chosen value of r2
cs_adj
is fixed at 0.1, the maximum value of r2
cs is much higher for the setting with the higher outcome proportion.
we can explain this further using nagelkerke's “proportion of total variance explained”,34 which is calculated as
r2
cs_adj∕ max(r2
cs). if two models have the same r2
cs_adj (say at 0.1, as in the aforementioned examples), then nagelkerke's
measure of predictive performance will be lower for the model whose setting has a higher outcome proportion, as the
max(r2
cs) is larger in that setting. models with lower performance have larger overfitting concerns,22 and therefore require
larger epp to minimise overfitting than models with high performance. hence, explaining why epp was larger when 𝜙
was 0.3 compared with 0.1 in the aforementioned example. this highlights that a blanket rule of thumb (such as at least
10 epp) is unlikely to be sensible to meet criterion (i), as the actual epp depends on the setting/population of interest
(which dictates the overall outcome proportion or rate) and expected model performance.
3 how to prespecify r𝟐
cs_adj based on previous information
our sample size proposal in equation (11) requires researchers to provide a value for the model's r2
cs_adj, that is, to prespecify the anticipated cox-snell r2 value if the model was applied to new individuals. how should this be done? we
recommend using r2
cs_adj values from previous prediction model studies for the same (or similar) population, considering
the same (or similar) outcomes and time points of interest. for example, the researcher could consult systematic reviews
of existing models and their performance, which are also increasingly available,35 or registries that record the prediction
models available in a particular field.36
often, a new prediction model is developed specifically to update or improve upon the performance of an existing model,
by using additional predictors. then, the existing model's r2
cs_adj could be used as a lower bound for the new model's
anticipated r2
cs_adj. in this situation, if the apparent cox-snell estimate, r2
cs_app, is available in an article describing the
development of the existing model, then its r2
cs_adj can be derived using equation (8) as long as the study's n and p can
also be obtained. in addition, as in van diepen et al's example (table 1), a global shrinkage factor may be reported directly
for an existing model development study, and if so, r2
cs_adj can be derived from a simple rearrangement of equation (10),
again as long as the study's n and p are also available.
note that, if r2
cs_app is available from an external validation study of an existing model, there is no need for adjustment
(ie, r2
cs_app = r2
cs_adj), as the validation dataset provides a direct estimate of the model's performance in new individuals
(free from overfitting concerns as there is no model development therein).
other options to obtain r2
cs_adj from the existing literature are now described. for guidance on choosing an r2
cs_adj value
in the absence of any prior information, please see our discussion.
3.1 using the lr statistic to derive the cox-snell r𝟐
adj
if the r2
cs_app or r2
cs_adj is not available in the publication of an existing model, the lr value may be reported, which would
allow r2
cs_app to be derived using equation (6), then svh for the model derived using equation (7) (assuming the model's
n and p are also provided), and finally r2
cs_adj using equation (8).
sometimes the log-likelihood of the final model (lnlmodel) is reported, but not the lr value itself. in this situation, the
researcher should calculate ln lnull based on other information in the article, and then calculate lr using equation (4),
thus allowing r2
cs_app and r2
cs_adj to be derived using equations (6) and (8), respectively. for example, in a logistic
regression model, the loglnull value can be calculated using
ln lnull = e ln (e
n
)
+ (n − e)ln (
1 − e
n
)
, (12)
where e is the total number of outcome events. of course, this assumes e and n are actually available in the article.
similarly, for an exponential survival model (equivalent to a poisson model with ln (survival time) as an offset), the
ln lnull can be calculated using
ln lnull = e ln(𝜆) + 𝜆t = e ln (e
t
)
+ e (13)
as long as 𝜆 (the constant hazard rate), e (the total number of events), and t (the total time at risk, eg, total person-years)
are available in the article. note that, for survival models, packages such as sas and stata usually add a constant to the
reported log-likelihood to ensure it remains the same value regardless of the time scale used. for example, stata adds the
sum of the ln (survival times) for the noncensored individuals to the reported ln lmodel and ln lnull, and so this constant
must be either consistently used or consistently removed in each of ln lmodel and ln lnull when deriving the lr value.
3.2 using other pseudo-r2 statistics to derive r𝟐
cs_adj
sometimes other pseudo-r2 statistics are reported for logistic and survival models, rather than the cox-snell version
specified in equation (6). in particular, because r2
cs_app has a maximum value less than 1, nagelkerke's r2 is sometimes
reported,34 which divides r2
cs_app by the maximum value defined by 1 − exp (2 ln lnull
n
)
, as follows:
r2
nagelkerke_app =
r2
cs_app
max (
r2
cs_app) =
r2
cs_app
1 − exp (2 ln lnull
n
). (14)
recall that ln lnull is derivable from other information, eg, using equations (12) or (13) for logistic and exponential
(poisson) models, respectively. when nagelkerke's r2, ln lnull, and n are available, the r2
cs_app can be calculated by
rearranging equation (14) to give
r2
cs_app = r2
nagelkerke_app (
1 − exp (2 ln lnull
n
)) , (15)
and then r2
cs_adj calculated via equation (8).
another measure sometimes reported is mcfadden's r2 37
r2
mcfadden_app = 1 − ln lmodel
ln lnull
. (16)
as ln lnull is often obtainable (see previous equation), when r2
mcfadden_app is reported, we can rearrange equation (16) to
obtain ln lmodel, and subsequently derive the lr statistic using equation (4), the cox-snell r2
cs_app from equation (6), svh
from equation (7) (assuming the model's n and p are also provided), and finally r2
cs_adj via equation (8).
for proportional hazards survival models, o'quigley et al suggested to modify r2
cs_app by replacing n with the number
of events (e)
38
r2
óquigley_app = 1 − exp (−lr
e
)
. (17)
therefore, if r2
óquigley_app and e were reported, the lr value could be found using
lr = −e ln (
1 − r2
óquigley_app)
, (18)
and subsequently, r2
cs_app can be obtained using equation (6), svh using equation (7), and finally r2
cs_adj using
equation (8).
another measure increasingly being reported for survival models is royston's measure of explained variation,39 which
is given by
r2
royston_app =
r2
óquigley_app
r2
óquigley_app +
(𝜋2
6
) (1 − r2
óquigley_app). (19)
when r2
royston_app is reported it can be used to obtain r2
óquigley_app by rearranging equation (19) as
r2
óquigley_app =
−𝜋2
6
r2
royston_app
(
1 − 𝜋2
6
)
r2
royston_app − 1
. (20)
this subsequently allows lr, r2
cs_app, svh and then r2
cs_adj to be derived as explained previously. a similar measure
to r2
royston is royston and sauerbrei's r2
d,
40 which can be derived from their proposed d statistic (the ln(hazard ratio)
comparing two groups defined by the median value of the model's risk score in the population of application)
r2
d_app =
𝜋
8
d2
𝜋2
6 + 𝜋
8
d2
. (21)
in examples shown by royston,39 r2
royston_app and r2
d_app are reasonably similar, and thus, we tentatively suggest r2
d_app
as
a proxy for r2
royston_app when only r2
d_app (or d) is reported; though, we recognise that further research is needed on the
link between r2
d_app
and r2
royston.
table 2 predicted values of the d statistic
and r2
d from equation (23) for selected values of
the c statistic (values taken from table 1 in the
work of jinks et al41)
c dr𝟐
d cdr𝟐
d
0.50 0 0 0.72 1.319 0.294
0.52 0.11 0.003 0.74 1.462 0.338
0.54 0.221 0.011 0.76 1.61 0.382
0.56 0.332 0.026 0.78 1.765 0.427
0.58 0.445 0.045 0.80 1.927 0.470
0.60 0.560 0.070 0.82 2.096 0.512
0.62 0.678 0.099 0.84 2.273 0.552
0.64 0.798 0.132 0.86 2.459 0.591
0.66 0.922 0.169 0.88 2.652 0.627
0.68 1.05 0.208 0.90 2.857 0.661
0.70 1.182 0.25 0.92 3.070 0.692
3.3 using values of the c statistic to derive r𝟐
cs_adj
jinks et al also proposed the following equation, based on empirical evidence, for predicting royston's d (and thus
subsequently r2
d_app) when only the c statistic is reported for a survival model41
d = 5.50(c − 0.5) + 10.26(c − 0.5)
3
. (22)
table 2 provides values of d (and corresponding values of r2
d_app from equation (21)) predicted from equation (22) for
selected values of the c statistic, as taken from the work of jinks et al.41 thus, if only the c statistic is reported, we can
use equation (22) to predict royston's d statistic and calculate r2
d_app (using equation (21)) as a proxy to r2
royston_app, and
then r2
óquigley_app, lr, r2
cs_app and finally r2
cs_adj computed sequentially using the equations given previously.
further evaluation of the performance of jinks' formula is required, eg, using simulation and across settings with different cumulative outcome incidences. indeed, based on figure 5 in the work of jinks et al,41 the potential error in the
predictions of d appears to increase as c increases, and is about +/− 0.25 when c is 0.8. nevertheless, equation (22)
serves as a good starting point and works well in our applied example (see section 5.2.1). further research is also needed
to ascertain how to predict r2
cs from other measures, such as somer's d statistic.
3.4 the anticipated value of r𝟐
cs_adj may be small
it is important to emphasise that the cox-snell, r2
cs, values for logistic and survival models are usually much lower than
for linear regression models, with values often less than 0.3. a key reason is that (unlike for linear regression) the r2
cs_app
has a maximum value less than 1, defined by
max(
r2
cs_app)
= 1 − exp(2 ln lnull
n
)
. (23)
this is because ln lnull is itself bounded for binary and time-to-event outcomes (see equations (12) and (13)). for example,
for a logistic regression model with an outcome proportion of 50%, using equation (12) and an arbitrary sample size of
100, we have
ln lnull = e ln (e
n
)
+ (n − e)ln (
1 − e
n
)
= 50 ln ( 50
100
)
+ (100 − 50)ln (
1 − 50
100
)
= −69.315,
and therefore, using equation (23),
max(
r2
cs_app)
= 1 − exp(2 ln lnull
n
)
= 1 − exp(−69.315
100
)
= 0.75.
however, for an outcome proportion of 5%, the max(r2
cs_app) is 0.33, and for an outcome proportion of 1%, the max(r2
cs_app)
is 0.11. therefore, especially in situations where the outcome proportion is low, researchers should anticipate a model
with a (seemingly) low r2
cs_app value, and subsequently a low r2
cs_adj value.
low values of r2
cs_app or r2
cs_adj do not necessarily indicate poor model performance. consider the following three
examples. first, poppe et al used a cox regression to develop a model (“predict-cvd”) to predict the risk of future
cvd events within two years in patients with atherosclerotic cvd,42 and directly report an r2
cs_app of 0.04. however,
the corresponding c statistic is 0.72, which shows discriminatory magnitude typical of many prognostic models used in
practice. second, hippisley-cox and coupland use the qresearch database to produce three models (qdiabetes) that
estimates the risk of future diabetes in a general population.43 in their validation of their “model a,” there were 27 311
incident cases of diabetes recorded in 1 322 435 women (3.77 cases per 1000 person-years) during follow-up, and the
reported r2
royston_app was 0.505. using the approach described previously to convert r2
royston to lr, this leads to a r2
cs_app of
0.02; however, the corresponding d statistic of 2.07 and c statistic of 0.89 are large. third, in a risk prediction model for
venous thromboembolism (vte) in women during the first 6 weeks after delivery,44 r2
cs_app was 0.001 due to the extremely
low event risk (7.2 per 10 000 deliveries), but the model still had important discriminatory ability as the corresponding c
statistic was 0.70.
4 additional sample size criteria
criterion (i) focuses on shrinkage of predictor effects, which is a multiplicative measure of overfitting (ie, on the relative
scale). harrell suggests to also evaluate overfitting on the absolute scale and to check key model parameters are estimated
precsiely.15 we now address this with two further criteria.
4.1 criterion (ii): ensuring a small absolute difference in the apparent
and adjusted r𝟐
nagelkerke
our second criterion for minimum sample size is to ensure a small absolute difference (𝛿) between the model's apparent
and adjusted proportion of variance explained. we suggest using nagelkerke's r2 for this purpose as, unlike the cox-snell
r2 value, it can range between 0 and 1, and so a small difference (say ≤ 0.05) can be ubiquitously defined. based on
equation (14), the difference in the apparent and adjusted nagelkerke's r2 can be defined as
r2
nagelkerke_app − r2
nagelkerke_adj =
r2
cs_app
max (
r2
cs_app) −
r2
cs_adj
max (
r2
cs_app)
=
r2
cs_adj
svh
− r2
cs_adj
max (
r2
cs_app)
=
r2
cs_adj (1 − s𝑉 𝐻 )
svh max (
r2
cs_app), (24)
where max(r2
cs_app) = 1 − exp (2 ln lnull
n
)
, as shown in equation (23).
therefore, to meet sample size criterion (ii) and ensure the difference is less than a small value (say, 𝛿), we require
r2
csadj
(1 − svh)
svh max (
r2
csapp) ≤ 𝛿. (25)
we generally recommend 𝛿 is ≤ 0.05, such that the optimism is nagelkerke's percentage of variation explained is ≤ 5%.
rearranging equation (25), we find that
(1 − svh)
svh
≤
𝛿 max (
r2
csapp)
r2
csadj
,
and therefore,
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp). (26)
equation (26) allows the researcher to calculate the required svh to satisfy criterion (ii), conditional on prespecifying the
model's anticipated r2
cs_adj (as they did for criterion (i)) and also the value of max(r2
csapp
) as outlined for equation (23).
then, sample size equation (11) can be used to derive the sample size needed to satisfy criterion (ii). this is only necessary
when the calculated value of svh from equation (26) is larger than that chosen for criterion (i), as then the sample size
required to meet criterion (ii) will be larger than that for criterion (i).
for example, consider the development of a logistic regression model with anticipated r2
cs_adj of at least 0.1, and in a
setting with the outcome proportion of 5%, such that the max(r2
cs_app) is 0.33. then, to ensure 𝛿 is ≤ 0.05, we require
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp) = 0.1
0.1 + (0.05 × 0.33) = 0.858.
therefore, svh must be at least 0.86 to meet criterion (ii). as this is lower than the recommended value of at least 0.90 to
meet criterion (i), no further work is required. however, had the anticipated r2
cs_adj been 0.2, then
svh ≥ 0.2
0.2 + (0.05 × 0.33) = 0.924.
as this is higher than 0.90, we would need to reapply sample size equation (11) using 0.924, rather than 0.90, to obtain a
sample size that meets both criteria (i) and (ii).
4.2 criterion (iii): ensure precise estimate of overall risk (model intercept)
for logistic and time-to-event models, it is fundamental that the available sample size can precisely estimate the overall
risk in the population by key time-points of interest. one way to examine this is to calculate the margin of error in outcome
proportion estimates (𝜙̂) for a null model (ie, no predictors included). for example, for a binary outcome, an approximate
95% confidence interval for the overall outcome proportion is
𝜙̂ ± 1.96√
𝜙̂(1 − 𝜙̂)
n .
therefore, the absolute margin of error (𝛿) is 1.96√𝜙̂(1−𝜙̂)
n , which leads to
n =
(1.96
𝛿
)2
𝜙̂(1 − 𝜙̂) . (27)
this is largest when the outcome proportion is 0.5. we require 96 individuals to ensure a margin of error ≤ 0.1 when the
true value is 0.5.15 however, we recommend a more stringent margin of error ≤ 0.05, which, when the outcome proportion
is 0.5, requires
n =
(1.96
0.05
)2
0.5(1 − 0.5) = 384.2,
and thus, 385 participants (and hence, about 193 events) are required. if the outcome proportion is 0.1, then we require
139 subjects to ensure a margin of error ≤ 0.05, whilst an outcome proportion of 0.2 requires 246 subjects.
these sample sizes aim to ensure precise estimation of the overall risk in the population of interest. strictly speaking,
we are more interested in precise estimation of the mean risk in an actual model including multiple predictors. if we
centre predictors at their mean value, then the model's intercept is the logit risk for an individual with mean predictor
values. the corresponding risk for this individual will often be very similar (though not identical) to the mean risk in
the overall population. furthermore, the variance of the estimated risk for this individual will be approximately 𝜙̂(1−𝜙̂)
n .*
*as obtained by inversing the information matrix x'v−1x and replacing individual variances defined by pi(1-pi) with a constant variance defined by
𝜙̂(1 − 𝜙̂).
thus, it follows that equation (27) is also a good approximation to the sample size required to precisely estimate the mean
risk in a model containing predictors centred at their mean.
for time-to-event data, we could consider the precision of the estimated cumulative incidence (outcome risk) at a key
time point of interest. a simple (and therefore practical) approach is to assume an exponential survival model, for which
the estimated cumulative incidence function is f(t) = 1 − exp(−𝜆̂ t), where 𝜆̂ is the estimated rate (number of events per
person-year). an approximate 95% confidence interval for the estimated f(t) is 1−exp (
−
(
𝜆̂ ± 1.96√𝜆̂
t
)
t
)
, where t is
the total person-years of follow-up. therefore, to ensure a small absolute margin of error, such that the lower and upper
bounds of the confidence interval are ≤ 𝛿 (eg, 0.05) of the true value, we must ensure both the following are satisfied:
− exp (
−
(
𝜆̂ + 1.96√
𝜆̂
t
)
t
)
+ exp(−𝜆̂ t) ≤ 𝛿
− exp(−𝜆̂ t) + exp (
−
(
𝜆̂ − 1.96√
𝜆̂
t
)
t
)
≤ 𝛿.
(28)
for example, for a constant event rate of 0.10 (10 events per 100 person-years), then by 10 years, the outcome risk is f(10)
= 1 − exp (−0.1 × 10) = 0.632. then, 2366 person-years of follow-up (and thus 0.1 × 2366 ≈ 237 events) are needed to
provide a confidence interval, which has a maximum absolute error of 0.05 from the true value. that is,
1 − exp (
−
(
𝜆̂ ± 1.96√
𝜆̂
t
)
t
)
= 1 − exp (
−
(
0.10 ± 1.96√ 0.10
2366)
10)
= 0.582 to 0.676.
thus, equation (28) is satisfied, as both the lower and upper bounds are ≤ 0.05 of the true value of 0.632. more generally, to
avoid assuming simple survival distributions like the exponential, harrell suggests using the dvoretzky-kiefer-wolfowitz
inequality to estimate the probability of a chosen margin of error anywhere in the estimated cumulative incidence
function.15,45
5 worked examples
to summarise our sample size approach for researchers, we provide a step-by-step guide in figure 1. the sample size (and
corresponding number of events and epp) that meets criteria (i) to (iii) provides the minimum sample size required for
model development. we now present two worked examples to illustrate our approach.
5.1 a diagnostic prediction model for chronic chagas disease
our first example considers the minimum sample size required for developing a diagnostic model for predicting a binary
outcome (disease: yes or no). brasil et al developed a logistic regression model containing 14 predictor parameters for
predicting the risk of having chronic chagas disease in patients with suspected chagas disease.46 upon external validation
in a cohort of 138 participants containing 24 with chagas disease, the model had an estimated c statistic of 0.91 and an
r2
nagelkerke_app of 0.48. consider that a researcher wants to update this model and improve the predictive performance. our
sample size approach can be applied as follows.
5.1.1 steps 1 and 2: identifying values for p, r𝟐
cs_adj, and max(r𝟐
cs_app)
assume that the researcher has identified (eg, based on recent studies) 10 additional predictor parameters that they wish
to add to the original model. thus, in total, the number of predictor parameters, p, is 24. the next step is to identify a
sensible value for the anticipated cox-snell r2
adj. to achieve this, we can convert the r2
nagelkerke_app value for brasil's existing
model into a r2
cs_app value. assume the disease prevalence is 17.4%, as in the brasil validation study, and use equation (12)
to calculate the log-likelihood for the null model in brasil's validation study
ln lnull = e ln (e
n
)
+ (n − e)ln (
1 − e
n
)
= 24 ln ( 24
138
)
+ (138 − 24)ln (
1 − 24
138
)
= −63.761.
figure 1 summary of the steps involved in calculating the minimum sample size required for developing a multivariable prediction
model for binary or time-to-event outcomes
hence, the max(r2
csapp
) = 1 − exp (2 ln lnull
n
)
= 1 − exp (2×−63.761
138 )
= 0.60. now, we can use equation (15) to obtain
r2
cs_app = r2
nagelkerke_app (
max (
r2
csapp)) = 0.48 × 0.60 = 0.288.
this apparent cox-snell value of 0.288 can be directly used as an estimate of the model's r2
cs_adj, as it was obtained in a
different data set to that used for model development. therefore no adjustment is needed, because r2
cs_app= r2
cs_adj here.
5.1.2 step 3: criterion (i) - ensuring a global shrinkage factor of 0.9
let us assume 0.288 is a lower bound for the r2
cs_adj of our new model. we now use equation (11) to estimate the sample
size required to ensure an expected shrinkage factor (svh = 0.90) conditional on a number of predictor parameters (p= 24)
n = p
(svh − 1)ln (
1 − r2
csadj
svh ) = 24
(0.90 − 1)ln (
1 − 0.288
0.90 ) = 622.31.
thus, 623 participants are required to meet criterion (i).
5.1.3 step 4: criterion (ii) - ensuring a small absolute difference in the apparent
and adjusted r𝟐
nagelkerke
to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of 0.05 or less
in the apparent and adjusted r2
nagelkerke. using equation (26), we obtain
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp) = 0.288
0.288 + (0.05 × 0.60) = 0.906.
this is more stringent than the 0.90 assumed for criterion (i). therefore, we need to reapply equation (11) to estimate the
sample size required conditional on svh = 0.906 (rather than 0.90)
n = p
(svh − 1)ln (
1 − r2
csadj
svh ) = 24
(0.906 − 1) ln (
1 − 0.288
0.906 ) = 667.41.
therefore, 668 subjects are required to meet criterion (ii), exceeding the 623 subjects required for criterion (i).
5.1.4 step 5: criterion (iii) - ensure precise estimate of overall risk (model intercept)
assuming the prevalence of chagas disease is 17.4% (as observed from the brasil validation study), then to ensure we
estimate this with a margin of error ≤ 0.05, we require (using equation (27))
n =
(1.96
0.05
)2
0.174 (1 − 0.174) = 220.85
and thus 221 subjects. this is far fewer than the sample size required to meet criteria (i) and (ii).
5.1.5 step 6: minimum sample size that ensures all criteria are met
the largest sample size required was 668 subjects to meet criterion (ii), and so this provides the minimum sample size
required for developing our new model. it corresponds to 668 × 0.174 = 116.2 events, and an epp of 116.2/24 = 4.84,
which is considerably lower than the “epp of at least 10” rule of thumb.
5.2 a prognostic model to predict a recurrence of vte
our second example considers the sample size required to develop a prognostic model with a time-to-event outcome.
ensor et al developed a prognostic time-to-event model for the risk of a recurrent vte following cessation of therapy for
a first vte.47 the sample size was 1200 participants, with a median follow-up of 22 months, a total of 2483 person-years
of follow-up, and 161 (13.42% of) individuals had a vte recurrence by end of follow-up.47 the model included predictors
of age, gender, site of first clot, d-dimer level, and the lag time from cessation of therapy until measurement of d-dimer
(often around 30 days). these predictors corresponded to six parameters in the model, which was developed using the
flexible parametric survival modelling framework of royston and parmar48 and royston and lambert.49 although ensor's
model performed well on average, the model's predicted risks did not calibrate well with the observed risks in some
populations.47 therefore, new research is needed to update and extend this model, eg, by including additional predictors.
we now identify suitable sample sizes to inform such research.
5.2.1 steps 1 and 2: identifying values for p, r𝟐
cs_adj and max(r𝟐
cs_app)
assume that there are 25 potential predictor parameters for inclusion in the new model, and thus, p = 25. we next need
to identify suitable values for 𝑅2
cs_adj and max(𝑅2
cs_app).
calculating max(r𝟐
cs_app)
for the ensor model, r2
cs_app was not reported but we should expect it to be quite small because the maximum value
of r2
cs_app is low. for example, assuming (for simplicity) an exponential survival model was fitted to the ensor data, then
using equation (13), we have
ln lnull = e ln (e
t
)
+ e = 161 ln(161∕2483) + 161 = −279.47,
and therefore, using equation (23),
max (
r2
cs_app)
= 1 − exp (2 ln lnull
n
)
= 1 − exp (−2 × 279.47
1200
)
= 0.37.
thus, max(r2
cs_app) is considerably less than 1.
obtaining a sensible value for r𝟐
cs_adj from the study authors
as r2
cs_app was not reported for the ensor model, we need to obtain it. we contacted the original authors who told us
their model's r2
cs_app was 0.056 in the development data set. thus, let us use this value to derive r2
adj from equation (8).
based on ensor's sample size of 1200, and six predictor parameters, we obtain
r2
cs_adj = svhr2
cs_app =
⎛
⎜
⎜
⎜
⎝
1 + p
n ln (
1 − r2
cs_app)
⎞
⎟
⎟
⎟
⎠
r2
cs_app =
(
1 +
6
1200 ln (1 − 0.056)
)
0.056 = 0.051.
hence, when developing a new model in this field, we could assume 0.051 is a lower bound for the expected r2
cs_adj of the
new model. this corresponds to nagelkerke's proportion variation explained of r2
cs_adj∕max(r2
cs_app) ≈ 0.051/0.37 = 0.14
(or 14%).
calculating a sensible value for r𝟐
cs_adj from other reported information
for illustration, we also consider how r2
cs_app could have been estimated indirectly from other available information.
the model's reported c statistic was 0.69, and so we can use equation (22) to predict the corresponding d statistic
d = 5.50 (c − 0.5) + 10.26(c − 0.5)
3 = 5.50 (0.69 − 0.5) + 10.26(0.69 − 0.5)
3 = 1.115.
the corresponding r2
d_app can be derived from equation (21)
r2
d_app =
𝜋
8
d2
𝜋2
6 + 𝜋
8
d2
=
𝜋
8
1.1152
𝜋2
6 + 𝜋
8
1.1152
= 0.229.
taking r2
d_app as a proxy for r2
royston_app, we can then use equation (20) to obtain
r2
óquigley_app =
−𝜋2
6
r2
royston_app
(
1 − 𝜋2
6
)
r2
royston_app − 1
= −𝜋2
6
0.229
(
1 − 𝜋2
6
)
0.229 − 1
= 0.328.
next, we can use r2
óquigley_app and the number of reported events (e = 161) to derive the lr statistic from equation (18)
lr = −e ln (
1 − r2
ó
quigleyapp)
= −161 ln (1 − 0.328) = 64.05.
using equation (6), this corresponds to
r2
cs_app = 1 − exp (−lr
n
)
= 1 − exp (−64.05
1200
)
= 0.052.
thus, based on using the reported c statistic, an indirect estimate of the r2
cs_app is 0.052 for the ensor model. this is
reassuringly close to the estimate of 0.056 provided directly by the study authors.
5.2.2 step 3: criterion (i) - ensuring a global shrinkage factor of 0.9
equation (11) can now be applied to derive the required sample size to meet criterion (i). using an r2
cs_adj of 0.051, for a
model with 25 predictor parameters and a targeted expected shrinkage of 0.9, the sample size required is
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ) = 25
(0.9 − 1) ln (
1 − 0.051
0.9
) = 4285.5
and thus 4286 participants.
5.2.3 step 4: criterion (ii) - ensuring a small absolute difference in the apparent
and adjusted r𝟐
nagelkerke
to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of 0.05 or less
in the apparent and adjusted r2
nagelkerke. recall, assuming an exponential model for simplicity, we calculated that the
max(r2
csapp
) = 0.37. then, using equation (26), we obtain
svh ≥
r2
csadj
r2
csadj
+ 𝛿 max (
r2
csapp) = 0.051
0.051 + (0.05 × 0.37) = 0.73.
this is less stringent than the 0.90 assumed for criterion (i), and so no further sample size calculation is required to meet
criterion (ii).
5.2.4 step 5: criterion (iii) - ensure precise estimate of overall risk
assuming a simple exponential model, we can check the width of the confidence interval for the overall risk at a particular
time point based on the sample size identified, using the approach outlined in section 4.2. ensor et al47 reported an overall
vte recurrence rate of 161/2483 = 0.065, with an average follow-up of 2.07 years. therefore, assuming 𝜆 is 0.065 in our
new study, and that a predicted risk at 2 years is of key interest, an exponential survival model would give the cumulative
incidence of f(2)=1− exp (−0.065 × 2)=0.122. based on the calculated sample size of 4286 participants from criterion (i),
and thus an estimated 4286×2.07 = 8872 person-years of follow-up, the 95% confidence interval would be
1 − exp (
−
(
𝜆̂ ± 1.96√
𝜆̂
t
)
t
)
= 1 − exp (
−
(
0.065 ± 1.96√0.065
8872 )
2
)
= 0.113 to 0.131.
this is reassuringly narrow, and satisfies equation (28) as both the lower and upper bounds are well within an error of
0.05 of the true value of 0.122.
5.2.5 step 6: minimum sample size that ensures all criteria are met
the largest sample size required was 4286 participants to meet criterion (i), which therefore provides the minimum sample
size required for developing our new model. this assumes the new cohort will have a similar follow-up, censoring rate,
and event rate to that reported by ensor et al, where the mean follow-up per person was 2.07 years, 13.42% of individuals
had a vte recurrence by end of follow-up, and the event rate was 0.065.47
then, the required 4286 participants corresponds to about 4286 × 2.07 = 8872 person-years of follow-up, and
8872 × 0.065 ≈ 577 outcome events, and thus an epp of 577/25 ≈ 23. this is over twice the “epp of at least 10” rule of
thumb. figure 2 shows that an epp of 10 only ensures a shrinkage factor of 0.79, which would reflect relatively large
overfitting.
5.2.6 what if the sample size is not achievable?
if a researcher was restricted in their total sample size, for example, by the time and cost of a new cohort study, then a
sample size of 4286 may not be practical. in this situation, we do not recommend reducing sample size by decreasing
sc below 0.9 (as this would reflect larger overfitting) or by assuming a larger r2
cs_adj value (as this is anticonservative
for criterion (i)). rather, to ensure an svh of 0.9 (ie, an expected shrinkage of 10%), the researcher should lower p by
reducing the number of candidate predictors. for example, predictors could be prioritised based on previous evidence (eg,
systematic reviews). after data collection, unsupervised learning techniques such as principal component analysis may
be useful, which are blinded to the outcome data. figure 3 shows how changing p changes the required sample size to
meet criterion (i). for example, if a researcher was restricted to a sample size of about 2000 participants, then they would
need to reduce p to 12 to ensure an expected shrinkage of 0.90. this is because, for an svh of 0.9 and r2
cs_adj of 0.051, the
sample size required is
n = p
(svh − 1) ln (
1 − r2
cs_adj
svh ) = 12
(0.9 − 1) ln (
1 − 0.051
0.9
) = 2057
figure 2 events per predictor parameter required to achieve various expected shrinkage (svh) values for a new prediction model of
venous thromboembolism recurrence risk with an assumed r2
cs_adj of 0.051 [colour figure can be viewed at wileyonlinelibrary.com]
figure 3 sample size required (based on equation (11)) for a particular number of predictor parameters (p) to achieve a particular value
of expected shrinkage (svh), for a new prediction model of venous thromboembolism recurrence risk with an assumed r2
cs_adj of 0.051
[colour figure can be viewed at wileyonlinelibrary.com]
and so now close to 2000. figure 3 also shows how larger values of svh require larger sample sizes; in particular, the
increase in sample size required is substantial when moving from svh of 0.90 to 0.95. values of svh < 0.9 lead to lower
sample sizes, but come at the cost of larger expected overfitting, and so are not recommended. therefore, targeting a value
of svh of 0.9 would seem a pragmatic choice.
6 potential additional criterion: precise estimates of
predictor effects
ideally, predictions should also be precise across the entire spectrum of predicted values, not just at the mean. this is challenging to achieve, but is helped by ensuring the sample size will give precise estimates of the effects of key predictors;50
hence, this may form a further criterion for researchers to check (ie, in addition to criteria (i) to (iii)). briefly, for a particular predictor of a binary or time-to-event outcome, the sample size required to precisely estimate its association with
the outcome (ie, an odds ratio or hazard ratio) depends on the assumed magnitude of this effect, the variability of the
predictor's values across subjects, the predictor's correlation with other predictors in the model, and the overall outcome
proportion in the study.51-53 ideally, we want to ensure a sample size that gives a precise confidence interval around the
predictor's effect estimate.54 however, this is taxing, as closed-form solutions for the variance of adjusted log odds ratio
or hazard ratios, from logistic and cox regression, respectively, are nontrivial. one solution is to use simulation-based
evaluations.54,55 however, perhaps a more practical option is to utilise readily available power-based sample size calculations that calculate the sample size required to detect (based on statistical significance) a predictor's effect for a chosen
type i error level (eg, 0.05) and power.51-53,56 as such sample size calculations are likely to be less stringent than those
based on confidence interval width (especially for predictors with large effect sizes), we might use a high power, say of
95%, in the calculation.
checking sample size for predictor effects will be laborious with many predictors, and so it may be practical to focus
on the subset of key predictors with smallest variance of their values, as these predictors will have the least precision.
in particular, when there are important categorical predictors but with few subjects and/or outcome events in some categories, substantially larger sample sizes may be needed to avoid separation issues (ie, no event or nonevents in some
categories).57 in addition, any predictors whose effect is small (and thus harder to detect), but still important, may warrant
special attention.
for example, returning to the vte prediction model from section 5.2, a key predictor in the original model by ensor et al
was age,47 with an adjusted log hazard ratio of −0.0105. although this is close to zero, as age is on a continuous scale, the
impact of age on outcome risk is potentially large; for example, it corresponds to an adjusted hazard ratio of 0.66 comparing
two individuals aged 40 years apart. based on the results presented by ensor et al,47 the standard deviation of age was 15.21
and the overall outcome occurrence by end of follow-up was 13.5%. based on these values, and assuming other included
predictors explain 20% of the variation in age, then the sample size approach of hsieh and lavori52 suggests 4718 subjects
are required to have 95% power to detect a prognostic effect for age. this is larger than the 4286 subjects required to meet
criterion (i), and so, to be extra stringent beyond criteria (i) to (iii), the researcher might raise the recommended sample
size to 4718 subjects, if possible.
7 discussion
sample size calculations for prediction models of binary and time-to-event outcomes are typically based on blanket rules
of thumb, such as at least 10 epp, which generates much debate and criticism.14,16,57 in this article, building on our related
work for linear regression,10 we have proposed an alternative approach that identifies the sample size, events and epp
required to meet three key criteria, which minimise overfitting whilst ensuring precise estimates of overall outcome risk.
criterion (i) aims to ensure the optimism of predictor effect estimates is small, as defined by a global shrinkage factor of
≥ 0.9. this idea extends the work of harrell who suggests that, after a model is developed, if the shrinkage estimate “falls
below 0.9, for example, we may be concerned with the lack of calibration the model may experience on new data.”15 our
premise is the same, except we focused on calculating the expected shrinkage before data collection, to inform sample
size calculations for a new study. criterion (ii) extends this idea to ensure the optimism is small on the r2
nagelkerke scale,
such that there is a difference of ≤ 5% in the apparent and adjusted percentage of variation explained by the model. lastly,
criterion (iii) ensures the sample size will precisely estimate the overall outcome risk, which is fundamental.
by utilising the model's anticipated cox-snell r2, the sample size calculations are essentially tailored to the model and
setting at hand, because the cox-snell r2 reflects many factors including the outcome proportion (ie, outcome prevalence
or cumulative incidence) and the overall fit (performance) of the model. it therefore better reflects the trait of a particular
model and setting at hand rather than a blanket epp rule.16 in our examples, the sample sizes required often differed
considerably from an epp of 10, reinforcing the idea that this rule is too simplistic.57 indeed, the required epp was much
higher (23) in our second example than our first (4.8), illustrating the problem with a blanket epp rule trying to cover all
situations.14,16-18
section 3 also showed how to obtain a realistic value for cox-snell r2 based on previous models to make our proposal
more achievable in practice. if no previous prediction model exists for the outcome and setting of interest, then information might be used from studies in a related setting or using a different but similar outcome definition or time points to
those intended for the new model. information can also be borrowed from predictor finding studies (eg, studies aiming
to estimate the prognostic effect of a particular predictor adjusted for other predictors58). typically, these studies apply
multivariable modelling, and although mainly focused on predictor effect estimates, they often report the c statistic and
pseudo-r2 values.
further research is needed to help researchers when there are no existing studies or information to identify a sensible
value of the expected cox-snell r2. medical diagnosis and prediction of health-related outcomes are, generally speaking,
low signal-to-noise ratio situations. it is not uncommon in these situations to see r2
nagelkerke values in the 0.1 to 0.2 range.
therefore, in the absence of any other information, we suggest that sample sizes be derived assuming the value of r2
cs_adj
corresponds to an r2
nagelkerke of 0.15 (ie,
r2
csadj
max(r2
csadj) = 0.15). an exception is when predictors include “direct” (mechanistic)
measurements, such as including the baseline version of the binary or ordinal outcome (eg, including smoking status at
baseline when predicting smoking status at 1 year), or direct measures of the processes involved (eg, including physiologic
function of patients in intensive care when predicting risk of death within 48 hours). then, in this special situation,
an r2
nagelkerke = 0.5 may be a more appropriate default choice.
the rule of having an epp of at least 10 stems from limited simulation studies examining the bias and precision of
predictor effects in the prediction model.11-13 jinks et al41 alternatively developed sample size formulae for a time-to-event
prediction model based on the d statistic.40 they suggest to predefine the d statistic that would be expected, and then,
based on a desired significance or confidence interval width, their formulae provide the number of events required to
achieve this. however, their method does not account for the number of candidate predictors and does not consider the
potential for overfitting when developing a model. our sample size calculations address this, and are meant to be used
before any data collection. in situations where a development data set is already available, containing a specific number
of participants and predictors, our criteria could be used to identify whether a reduction in the number of predictors
is needed before starting model development. indeed, harrell already illustrated this concept by using the shrinkage
estimate from the full model (including all predictors) to gauge whether the number of predictors should be reduced via
data reduction techniques.15 ideally, this should be done blind to the estimated predictor effects (ie, just calculate the
shrinkage factor for the full model, but do not observe the predictor effect estimates and associated p-values), as otherwise
decisions about predictor inclusion are influenced by a “quick look” at the effect estimates from the full model results.
similarly, when planning to use a predictor selection method (such as backwards selection) during model development,
researchers should define p as the total number of parameters due to all predictors considered (screened), and not just
the subset that are included in the final model.59 as harrell notes,15 the value of p should be honest.
section 6 also highlighted the potential additional requirement to ensure precise estimates of key predictor effects.
in particular, special attention may be given to those predictors with strong predictive value (and thus most influential
to the predicted outcome risk), especially if the variance in their values is small, or when events or nonevents in some
categories of the predictor are rare, as this leads to larger sample sizes. for example, van smeden et al highlighted that
“separation” between events and nonevents is an important consideration toward the required sample size, which occurs
when a single predictor (or a linear combination of multiple predictors) perfectly separates all events from all nonevents,
and thus causes estimation difficulties.57 this may lead to substantially larger epp to resolve the issue (eg, so that all
categories of a predictor have both events and nonevents). for such reasons, we labelled our criteria (i) to (iii) proposal
as the “minimum” sample size required.
further research should identify how our sample size criteria relates to that of the work of van smeden et al, who focused
on sample size in regards to the mean squared error in predictions from the model.60 specifically, they use simulation
to evaluate the characteristics that influence the mean squared prediction error of a logistic model, and identify that the
outcome proportion and number of predictors are important,60 in addition to total sample size. this leads to a sample size
equation to minimise root mean-squared prediction error in a new model development study. harrell also suggested using
simulation to inform sample size, and illustrates this for a logistic regression model with a single predictor.15 for example,
one could simulate a very large dataset from an assumed prediction model, and quantify the mean square (prediction)
error and mean absolute (prediction) error of a model developed from this data set. then, repeat this process each time
removing an individual at random, until a sample size is identified below which the mean squared (prediction) error is
unacceptable.
in summary, we have proposed criteria for identifying the minimum sample size required when developing a prediction
model for binary or time-to-event outcomes. we hope this, and our related paper,10 encourages researchers to move away
from rules of thumb, and to rather focus on attaining sample sizes that minimise overfitting and ensure precise estimates
of overall risk within the model and setting of interest. we are currently writing software modules to implement the
approach.