a guide to systematic review and meta-analysis of prognostic factor studies

prognostic factors are associated with the risk of future health outcomes in individuals with a particular health condition or some clinical start point (eg, a particular diagnosis). research to identify genuine prognostic factors is important because these factors can help improve risk stratification, treatment, and lifestyle decisions, and the design of randomised trials. although thousands of prognostic factor studies are published each year, often they are of variable quality and the findings are inconsistent. systematic reviews and meta-analyses are therefore needed that summarise the evidence about the prognostic value of particular factors. in this article, the key steps involved in this review process are described.

systematic reviews and meta-analyses are common in the medical literature, routinely appearing in specialist and general medical journals, and forming the cornerstone of cochrane. the majority of systematic reviews focus on summarising the benefit of one or more therapeutic interventions for a particular condition. however, they are also important for summarising other evidence, such as the accuracy of screening and diagnostic tests,1 the causal association of risk factors for disease onset, and the prognostic ability of bespoke factors and biomarkers. prognostic evidence arises from prognosis studies, which aim to examine and predict future outcomes (such as death, disease progression, side effects or medical complications like pre-eclampsia) in people with a particular health condition or start point (such as those developing a certain disease, undergoing surgery, or women who are pregnant).

the progress (prognosis research strategy) framework defines four types of prognosis research objectives: (a) to summarise overall prognosis (eg, overall risk or rate) of health outcomes for groups with a particular health condition2; (b) to identify prognostic factors associated with changes in health outcomes3; (c) to develop, validate, and examine the impact of prognostic models for individualised prediction of such outcomes4; and (d) to identify predictors of an individual’s response to treatment.5 each objective requires specific methods and tools for conducting a systematic review and meta-analysis. two recent articles provided a guide to undertaking reviews and meta-analysis of prognostic (prediction) models.67 in this article, we focus on prognostic factors.

a prognostic factor is any variable that is associated with the risk of a subsequent health outcome among people with a particular health condition. different values or categories of a prognostic factor are associated with a better or worse prognosis of future health outcomes. for example, in many cancers, tumour grade at the time of histological examination is a prognostic factor because it is associated with time to disease recurrence or death. each grade represents a group of patients with a different prognosis, and the risk or rate (hazard) of the outcome increases with higher grades. many routinely collected patient characteristics are prognostic, such as sex, age, body mass index, smoking status, blood pressure, comorbidities, and symptoms. many researched prognostic factors are biomarkers, which include a diverse range of blood, urine, imaging, electrophysiological, and physiological variables.

prognostic factors have many potential uses, including aiding treatment and lifestyle decisions, improving individual risk prediction, providing novel targets for new treatment, and enhancing the design and analysis of randomised trials.3 this motivates so-called “prognostic factor research” to identify genuine prognostic factors (sometimes also called “predictor finding studies”8).9 although thousands of such studies are published each year, often they are of variable quality and have inconsistent findings. systematic reviews and meta-analyses are therefore urgently needed to summarise the evidence about the prognostic value of particular factors.101112 in this article, we provide a step-by-step guide on conducting such reviews. our aim is to help readers, healthcare providers, and researchers understand the key principles, methods, and challenges of reviews of prognostic factor studies.

summary points
primary studies to identify prognostic factors are abundant, but often findings are inconsistent and quality is variable. systematic reviews and meta-analyses are urgently needed to identify, evaluate, and summarise prognostic factor studies and their findings.
a clear review question should be defined using the picots system (population, index prognostic factor, comparator prognostic factors, outcome, timing, setting), and a transparent search undertaken for eligible articles. broad search strings may be required, leading to a large number of articles to screen.
a data extraction phase is needed to obtain the relevant information from each study. a modification of charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) can be used for prognostic factors (charms-pf).
the quips tool (quality in prognostic factor studies) can be used to examine each study’s risk of bias. unfortunately, many primary studies may have a high risk of bias because of poor design standards, conduct, and analysis. applicability of a study should also be checked.
if appropriate, meta-analysis can be used to combine prognostic effect estimates (such as hazard ratios, risk ratios, or odds ratios) across studies to produce an overall summary of a factor’s prognostic effect. between-study heterogeneity should be expected and accounted for.
ideally separate meta-analyses should be performed for unadjusted and adjusted prognostic effect estimates; adjusted estimates are important to examine a factor’s independent prognostic value over and above (that is, after adjustment for) other prognostic factors.
separate meta-analyses may also be required for each method of measurement (for factors and outcomes), each approach to handling continuous factors, and each type of estimate (such as hazard ratios or odds ratios).
publication bias is a major threat to the validity of meta-analyses of prognostic factor studies based on published evidence, and may cause small-study effects (asymmetry on a funnel plot).
remark (reporting recommendations for tumour marker prognostic studies) and prisma (preferred reporting items for systematic reviews and meta-analyses) can be used to guide the reporting of the systematic review and meta-analysis of prognostic factor studies; the degree of confidence in the summary results from the review may be examined by use of adapted forms of grade (grades of recommendation, assessment, development, and evaluation) for interventions and diagnostic test accuracy studies.
availability of individual participant data from primary prognostic factor studies may alleviate many of the challenges.
step 1: defining the review question
the first step is to define the review question. a review of prognostic factor studies falls within the second objective of the progress framework2 because it aims to summarise the prognostic value of a particular factor (or each of multiple factors) for relevant health outcomes and time points in people with a specific health condition (eg, disease). some reviews are broad; for example, riley and colleagues aimed to identify any prognostic factor for overall and disease free survival in children with neuroblastoma or ewing’s sarcoma.13 other reviews have a narrower focus; for example, hemingway and colleagues aimed to summarise the evidence on whether c reactive protein (crp) is a prognostic factor for fatal and non-fatal events in patients with stable coronary disease.14 this crp review is used as an example throughout this article.

charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) provides guidance for formulating a review question (table 1 in the article by moons and colleagues15). although charms was developed15 and refined6 for reviews of prediction model studies, it can also be used to define and frame the question for reviews of prognostic factor studies. charms15 and subsequent improvements6 propose a modification of the traditional pico system (population, index intervention, comparison, and outcome) used in systematic reviews of therapeutic intervention studies. the modification is called picots, because it also considers timing and setting (box 1). in the context of prognostic factor reviews, the “p” of population and the “o” of outcome remain largely the same as in the original pico system, but the “i” refers to index prognostic factors and the “c” refers to other prognostic factors that can be considered as comparators in some way. for example, the aim may be to compare the prognostic ability of a certain index factor with one or more other (that is, comparator) prognostic factors; or to investigate the adjusted prognostic value of a particular index factor over and above (adjusted for) other (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, which is not generally recommended, then no comparator factor is being considered. the “t” denotes timing and refers to two concepts of time. firstly, at what time point the prognostic factors under review are to be measured or assessed (that is, the time point at which prognosis information is required); and secondly, over what time period the outcomes are predicted by these factors. the “s” of setting refers to the setting or context in which the index prognostic factors are to be used because the prognostic ability of a factor may change across healthcare settings.

table 1 charms-pf checklist of key items to be extracted from primary studies of prognostic factors, based on additions and modifications of the original charms checklist for primary studies of prediction models15
view popupview inline
box 1
six items (picots) defining the question for systematic reviews of prognostic factor studies, based on charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies)615 and applied to a review of the adjusted prognostic value of c reactive protein (crp)14
population: define the target population for which prognostic factors under review are to be used. for example, crp review: patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of acute coronary syndrome at least two weeks before prognostic factor (crp) measurement.
index prognostic factor: define the factors for which prognostic value is under review. for example, crp review: crp was the single biomarker reviewed for its prognostic value.
comparator prognostic factors: comparator prognostic factors can be considered in a review in various ways. for example, the aim could be to compare the prognostic ability of a certain index factor with two or more other (that is, comparator) prognostic factors; or to review the adjusted prognostic value of a particular index factor—that is, over and above (adjusted for, independent of) other existing (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, then no comparator factor is being considered. for example, crp review: the focus was on the adjusted prognostic value of crp—that is, its prognostic effect after adjusting for existing (comparator) prognostic factors. in particular, adjustment for the following conventional prognostic factors was of interest: age, sex, smoking status, obesity, diabetes, and one or more lipid variables (from total cholesterol, low density lipoprotein cholesterol, high density lipoprotein cholesterol, triglycerides) and inflammatory markers (fibrinogen, interleukin 6, white cell count).
outcome: define the outcomes for which the prognostic ability of the factor(s) under review are of interest. for example, crp review: outcome events were defined as coronary (coronary death, sudden cardiac death, acute non-fatal myocardial infarction, primary percutaneous coronary intervention, unplanned emergency admissions with unstable angina), cardiovascular (when coronary events were reported in combination with heart failure, stroke, or peripheral arterial disease), and all cause mortality.
timing: define firstly at what time points the prognostic factors (index and comparators) are to be used (that is, the time point of prognostication), and secondly over what time period the outcomes are predicted by these factors. for example, crp review: the crp measurement had to be done at least two weeks after diagnosis and all follow-up information on the outcomes (all time periods) was extracted from the studies.
setting: define the intended setting and role of the prognostic factors under review. for example, crp review: crp measurement was studied in primary and secondary care to provide prognostic information about patients diagnosed with coronary heart disease; this information may be useful for healthcare professionals treating and managing such patients.
return to text
an important component of reviews of prognostic factors is whether unadjusted or adjusted estimates of the index prognostic factors will be summarised, or both. we recommend that reviewers primarily focus on adjusted prognostic factor estimates because they reveal whether a certain index factor contributes independently to the prediction of the outcome over and above (that is, after adjustment for) other prognostic factors. in particular, for each clinical scenario there are often so-called “established” or “conventional” prognostic factors that are always measured. therefore, for prognostic factors under review, it is important to understand whether they contribute additional (sometimes called “independent”) prognostic information to the routinely measured ones. this means that reviewers need adjusted (and not unadjusted or crude) prognostic effect estimates to be estimated and reported in primary prognostic factor studies. such adjusted prognostic estimates are typically derived from a multivariable regression model containing the established prognostic factors plus each index prognostic factor of interest.

for example, consider a logistic regression of a binary outcome including three adjustment factors (a1, a2, and a3) and one new index prognostic factor (x1), which is expressed as:

ln(p/(1−p)) = α+β1a1+β2a2+β3a3+β4x1
here, “p” is the probability of the outcome. after estimation of all the unknown parameters (that is, α, β1, β2, β3, β4), of key interest is the estimated β4. this parameter provides the adjusted prognostic effect of the index prognostic factor and reveals its independent contribution to the prediction of the outcome over and above the prognostic effects of the other (established comparator) factors a1, a2, and a3 combined.

the need to focus on adjusted prognostic effects is no different from (systematic reviews of) aetiological studies, in which the focus is on estimating the association of a certain causal risk factor after adjustment for other risk factors. in such causal research, these factors are usually referred to as “confounders” rather than as “other prognostic factors,” which is the term typically used for prognosis research. the crude (unadjusted) prognostic effect of some index factors may completely disappear after adjustment and is therefore rather uninformative, especially because prognostication in healthcare is rarely based on a single prognostic factor but rather on the information from multiple prognostic factors.4

this article focuses on systematic reviews to summarise prognostic factor effect estimates. some primary studies may also evaluate an index factor’s added value in terms of improvement in risk classification and clinical use (eg, measures such as net reclassification improvement and net benefit), and change in prediction model performance (eg, by calculating the change in the concordance index, also known as the c statistic or area under the receiver operating characteristics (roc) curve).17181920 however, this is beyond the scope of this article, and we refer the reader to other relevant sources.62122

application to crp review
crp is widely studied for its prognostic value in patients with coronary disease. however, there is uncertainty whether crp is useful because us and european clinical practice guidelines recommend measurement but clinical practice varies widely. this uncertainty motivated the systematic review by hemingway and colleagues,14 with the corresponding picots system presented in box 1. no studies were excluded on the basis of methodological standards, sample size, duration of follow-up, publication year, or language of publication.

step 2: searching for and selection of eligible studies
the next step is to identify primary studies that are eligible for review; studies that address the review question defined in step 1 following the picots framework. unfortunately, it is more difficult to identify prognostic factor studies than randomised trials of interventions. prognosis studies do not tend to be indexed (“tagged”) because a taxonomy of prognosis research is not widely recognised. moreover, compared with studies of interventions, there is much more variation in the design of prognostic factor studies (eg, data from cohort studies, randomised trials, routine care registries, and case-control studies can all be used), patient inclusion criteria, prognostic factor and outcome measurement, follow-up time, methods of statistical analysis, and adjustment of (and number of) other prognostic factors (covariates). between-study heterogeneity is therefore the rule rather than the exception in prognostic factor research. it is essential that systematic reviews of prognostic factor studies define the study inclusion and exclusion criteria based on the picots structure (step 1) because this determines the study search and selection strategy.

typically, broad search and selection filters are required that combine terms related to prognosis research (such as prognostic, predict, predictor, factor, independent) with domain or disease specific terms (such as the name of prognostic factors and the targeted disease or patient population).23 a broad search comes at the (often considerable) expense of retrieving many irrelevant records. geersing and colleagues24 validated various existing search strategies for prognosis studies and suggested a generic filter for identifying studies of prognostic factors,232526 which extended the work of ingui, haynes, and wong.232526 when tested in a single review of prognostic factors, this generic filter had a number needed to read of 569 to identify one relevant article, emphasising the difficulty in targeting prognostic factor articles. the number needed to read could be considerably reduced when specific factors or populations are added to the filter. even then, care is needed to be inclusive because multiple terms are often used for the same meaning; for example, biomarker mycn is also referred to as n-myc and nmyc, among other terms.13

once the search is complete, each potentially relevant study must be screened for its applicability to the review question. because of the heterogeneity in prognostic factor studies, during this study selection phase more deviations from the defined picots (in step 1) are possible (far greater than what is typically encountered during the selection of randomised intervention studies). the applicability of this primary study selection should firstly be based on title and abstract screening, followed by full text screening, both ideally done by two researchers independently. any discrepancies should be resolved through discussion, potentially with a third reviewer. to check if any relevant articles have been missed, it is helpful to share the list of identified articles with researchers in the field to examine the reference lists of these articles and to perform a citation search.

application to crp review
hemingway and colleagues included any prospective observational study that reported risk of subsequent events among patients with stable coronary disease in relation to measured crp values.14 eligible studies had to include patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of previous acute coronary syndrome at least 2 weeks before crp measurement. hemingway and colleagues searched medline between 1966 and 25 november 2009 and embase between 1980 and 17 december 2009, using a search string containing terms for coronary disease, prognostic studies, and crp. the search identified 1566 articles, of which 83 fulfilled the inclusion criteria. if specific terms for crp had not been included in the search string, then the total number of identified articles would have far exceeded 1566.

step 3: data extraction
the next step is to extract key information from each selected study. data extraction provides the necessary data from each study, which enables reviewers to examine their (eventual) applicability to the review question and their risk of bias (see step 4). this step also provides the information required for subsequent qualitative and quantitative (meta-analysis) synthesis of the evidence across studies. the charms checklist gives explicit guidance (table 2 in the article by moons and colleagues15) about which key items across 11 domains should be extracted from primary studies of prediction models, and for what reason (that is, to provide general information about the primary study, to guide risk of bias assessment, or to assess applicability of the primary study to the review question). based on our experience of conducting systematic reviews of prognostic factor studies, we modified the original charms checklist for prediction model studies to make it suitable for data extraction in reviews of prognostic factors (here referred to as charms-pf; table 1). this basically means that three domains typically addressing multivariable prediction modelling aspects were combined to one overall analysis domain, while other domain names and key items were slightly reworded or extended. reasons for extraction of each key item are similar to charms for prediction models. because we developed the original charms checklist, a wider consensus of the charms-pf content was not considered necessary.

table 2 quips tool (quality in prognostic factor studies), which can be used to classify risk of bias of prognostic factor studies
view popupview inline
reviewers should extract fundamental information from the primary prognostic factor studies, such as the dates, setting, study design, definitions of start points, outcomes, follow-up length, and prognostic factors; reviewers will often find large heterogeneity in this information across studies. the extracted information can be summarised in tables of study characteristics. in addition, more specific information is needed to properly assess applicability and risk of bias (see step 4), such as methods used to measure prognostic factors and outcomes, handling missing data, attrition (loss to follow-up), and whether estimated associations of the prognostic factors under review were adjusted for other prognostic factors. this information also enhances the potential for meta-analysis and the presentation and interpretation of subsequent summary results (see steps 5-8).

to enable meta-analysis of prognostic factor studies, the key elements to extract are estimates, and corresponding standard errors or confidence intervals, of the prognostic effect for each factor of interest; for example, the estimated risk ratio or odds ratio (for binary outcomes), hazard ratio (for time-to-event outcomes), or mean difference (for continuous outcomes). as most prognostic factor studies consider time-to-event outcomes (including censored observations and different follow-up lengths for patients), hazard ratios are often the most suitable effect measure. a concern is that hazard ratios may not be constant over time, and therefore any evaluations of non-proportional hazards (that is, non-constant hazard ratios for the prognostic factors of interest) should also be extracted; however, such information is rarely reported in sufficient detail.

unfortunately, many prognostic factor studies do not adequately report estimated prognostic effect measures or their precision. for this reason, methods are available to restore the missing information upon data extraction. in particular, parmar and colleagues28 and tierney and colleagues29 describe how to obtain unadjusted hazard ratio estimates (and their variances) when they are not reported directly. for example, under assumptions, the number of outcomes (events) and an available p value (eg, from a log rank test or cox regression) can be used to indirectly estimate the unadjusted hazard ratio between two groups defined by a particular factor (eg, “high” versus “normal” levels). perneger and colleagues30 report how to derive unadjusted hazard ratios from survival proportions, and pérez and colleagues suggest using a simulation approach.31 even with such indirect estimation methods, not all results can be obtained. for example, in a systematic review of 575 studies investigating prognostic factors in neuroblastoma,32 the methods of parmar and colleagues were used to obtain 204 hazard ratio estimates and their confidence intervals; but this represented only 35.5% of the potential evidence.

although indirect estimation methods help retrieve unadjusted prognostic factor effect estimates, they often have limited value for obtaining adjusted effect estimates. furthermore, even when multiple studies provide the adjusted prognostic effect of a particular factor, the set of adjustment factors will usually differ across studies, which complicates the interpretation of subsequent meta-analysis results. we recommend that reviewers predefine the core set of prognostic factors for the outcome of interest (eg, age, sex, smoking status, disease stage) that represents the desired “minimal” set of adjustment factors. an agreed process among health professionals and researchers in the field could be required to define this set. for example, a list of established prognostic factors could be identified that are routinely used within current prognostication of the clinical population of interest.

it may also be necessary to standardise the extracted estimates to ensure they all relate to the same scale and direction in each study. in particular, the direction of the prognostic effect will need standardising if one study compares the hazard rate in a factor’s “high” versus “normal” group, whereas another study compares the hazard rate in the factor’s “normal” versus “high” group. when the outcome is defined differently across studies, approaches to convert effect measures on different outcome scales could be useful.33 also, to deal with different cutpoint levels for values of a particular factor,34 the prognostic effects of “high” versus “normal” could be converted to prognostic effects relating to a 1 unit increase in the factor. this requires assumptions about the underlying distribution of the factor. such an approach was used by hemingway and colleagues.14 of concern, however, is that the actual distribution of a prognostic factor may be unknown (or even vary across studies). finally, it is also possible to derive standardised effect estimates by standardising the corresponding regression coefficients.35

application to crp review
hemingway and colleagues extracted background information such as year of study start, number of included patients, mean age, baseline coronary morbidity (eg, proportion with stable angina), average levels of biomarker at baseline, method of crp measurement, follow-up duration, and number and type of events. basic information was often missing. for example, nearly a fifth of studies did not report the method of measurement, and only a quarter gave the number of patients included in the analyses and reasons for dropout. prognostic effect estimates for crp were extracted in terms of the reported risk ratio, odds ratio, or hazard ratio (labelled generally as “risk ratio” in this article), and 95% confidence intervals. these effect estimates were then converted to a standardised scale comparing the highest third with the lowest third of the (log transformed) crp distribution. if available, separate prognostic effect estimates were extracted for different degrees of adjustment for other prognostic factors.

step 4: evaluating applicability and risk of bias of primary studies
once eligible studies are identified and data are extracted, an important next step is to assess the applicability and risk of bias (quality) of each study in the review. as for steps 2 and 3, ideally this is done by two reviewers, independently, with any discrepancies resolved. applicability refers to the extent to which a selected study (in step 2) matches the review question in terms of the population, timing, prognostic factors, and outcomes (endpoints) of interest. just because a study is eligible for inclusion does not mean it is free from applicability concerns. some aspects of a study may be applicable (eg, correct condition at start point, with prognostic factors of interest evaluated) but not others (eg, incorrect population or setting, inappropriate outcome definition, different follow-up time, lack of adjustment for conventional prognostic factors). applicability is typically first assessed during title and abstract screening, and then during this step, so that it is based on full text screening and determined by picots (step 1) and inclusion and exclusion criteria of studies (step 2).

risk of bias refers to the extent to which flaws in the study design or analysis methods could lead to bias in estimates of the prognostic factor effects. unfortunately, based on growing empirical evidence from systematic reviews examining methodology quality, many primary studies will be at high risk of bias.832363738394041424344 for prognostic factor studies, hayden and colleagues developed the quips checklist (quality in prognostic factor studies) for examining risk of bias across six domains27: study participation, study attrition, prognostic factor measurement, outcome measurement, adjustment for other prognostic factors, and statistical analysis and reporting. table 2 shows the signalling items within these domains to help guide reviewers in making low, unclear, or high risk of bias classifications. additional guidance may be found in general tools examining the quality of observational studies,4546 and the remark guideline (reporting recommendations for tumour marker prognostic studies) for reporting of primary prognostic factor studies.4748

we recommend that users first operationalise criteria to assess the signalling items and domains for the specific review question. for example, with the study participation and attrition domains, this includes defining a priori the most important characteristics that could indicate a systematic bias in study recruitment (study participation domain) and loss to follow-up (study attrition domain). defining these characteristics ahead of time will facilitate assessment and consensus related to the importance of potential differences that could influence the observed association between the index prognostic factors and outcomes of interest. definitions of sufficiently valid and reliable measurement of the index prognostic factors and outcomes should also be specified at the protocol stage. similarly, the core set of other (adjustment) prognostic factors that are deemed necessary for the primary studies to have adjusted for, should be predefined to facilitate judgment related to risk of bias in domain 5.

overall assessment of the six risk of bias domains is undertaken by considering the risk of bias information from the signalling items for each domain, rated as low, moderate, and high risk of bias. occasionally, item information needed to assess the bias domains is not available in the study report. when this occurs, other publications that may have used the same dataset (which often occurs in prognostic studies based on large existing cohorts) should be consulted and study authors should be contacted for additional information. an informed judgment about the potential risk of bias for each bias domain should be made independently by two reviewers, and discussed to reach consensus. each of the six domains needs to be rated and reported separately because this will inform readers, flag improvements needed for subsequent primary studies, and facilitate future meta-epidemiological research. we recommend defining studies with an overall “low risk of bias” as those studies where all, or the most important domains (as determined a priori), are rated as having low (or low to moderate) risk of bias.

application to crp review
hemingway and colleagues assessed the quality of included studies by the quality of their reporting on 17 items derived from the remark guideline.48 the median number of study quality items reported was seven of a possible 17, and standards did not change between 1997 and 2009. only two studies referred to a study protocol, with none referring to a statistical analysis plan. hemingway and colleagues noted that this “makes it difficult to know what the specific research objectives were at the start of cohort recruitment, at the time of crp measurement, or at the onset of the statistical analysis.”14 only two studies reported the time elapsed between first lifetime presentation with coronary disease and assessment of crp and this raised applicability concerns.

step 5: meta-analysis
meta-analysis of prognostic factor studies aims to summarise the (adjusted) prognostic effect of each factor of interest. in addition to missing estimates, challenges for the meta-analyst include (a) having different types of prognostic effect measures (eg, odds ratios and hazard ratios), which are not necessarily comparable30; (b) estimates without standard errors, which is a problem because meta-analysis methods typically weight each study by (a function of) their standard error; (c) estimates relating to various time points of the outcome occurrence or measurement; (d) different methods of measurement for prognostic factors and outcomes; (e) various sets of adjustment factors; and (f) different approaches to handling continuous prognostic factors (eg, categorisation, linear, non-linear trends), including the choice of cutpoint value when dichotomising continuous values into “low” and “normal” groups. many of these issues lead to substantial heterogeneity and if a meta-analysis is performed, summary results cannot be directly interpreted.

generally, meta-analysis results will be most interpretable, and therefore useful, when a separate meta-analysis is undertaken for groups of “similar” prognostic effect measures. in particular, we suggest considering a meta-analysis for:

hazard ratios, odds ratios, and risk ratios separately
unadjusted and adjusted associations separately
prognostic factor effects at distinct cutpoints (or groups of similar cutpoints) separately
prognostic factor effects corresponding to a linear trend (association) separately
prognostic factor effects corresponding to non-linear trends separately
each method of measurement (for factors and outcomes) separately.
ideally a meta-analysis of adjusted results should ensure that all included estimates are adjusted for the same set of other prognostic factors. this situation is unlikely and so a compromise could be to ensure that all adjusted estimates in the same meta-analysis have adjusted for at least a (predefined) minimum set of adjustment factors (that is, a core set of established prognostic factors).

even when adhering to this guidance, unexplained heterogeneity is likely to remain because of other reasons (eg, differences in length of follow-up or in treatments received during follow-up). therefore, if a meta-analysis is performed, a random effects approach is essential to allow for unexplained heterogeneity across studies (box 2), as previously described in the bmj.53 this approach provides a summary estimate of the average prognostic effect of the index factor and the variability in effect across studies. also potentially useful are meta-analysis methods to estimate the trend (eg, linear effect) of a prognostic factor that has been grouped into three or more categories within studies (with each category compared with the reference category). these methods generally model the estimated prognostic effect sizes in each category as a function of “exposure” level (eg, midpoint or median prognostic factor value in the category) and account for within-study correlation and between-study heterogeneity.5455565758 to apply these methods, some additional knowledge of the factor’s underlying distribution is usually needed to help define the “exposure” level because the chosen value can have an impact on the results.56

box 2
explanation of a random effects meta-analysis of prognostic factor effect estimates
the true prognostic effect of a factor is likely to vary from study to study; therefore assuming a common (fixed) prognostic effect is not sensible. if yi and var(yi) denote the prognostic effect estimate (eg, ln(hazard ratio), ln(odds ratio), ln(risk ratio), or mean difference) and its variance in study i, then a general random effects meta-analysis model can be specified as:

yi ~n(μ,var(yi)+τ2).

most researchers use either restricted maximum likelihood or the approach of dersimonian and laird to estimate this model,49 but other options are available, including a bayesian approach.50 of key interest is the estimate of μ, which reveals the summary (average) prognostic effect of the index prognostic factor of interest. the standard deviation of this prognostic factor effect across studies is denoted by τ, and non-zero values suggest there is between-study heterogeneity. confidence intervals for µ should ideally account for uncertainty in estimated variances (in particular τ),51 and we have found the approach of hartung-knapp to be robust for this purpose in most settings.1652 when synthesising prognostic effects on the log scale, the summary results and confidence intervals require back transformation (using the exponential function) to the original scale.

return to text
advanced multivariate meta-analysis methods are also available to handle multiple cutpoints,59 multiple methods of measurement,59 or different adjustment factors in prognostic factor studies.60 an introduction to multivariate meta-analysis has been published in the bmj.61

application to crp review
hemingway and colleagues14 applied a random effects meta-analysis to combine 53 adjusted prognostic effect estimates for crp from studies that adjusted for at least one of six conventional risk factors (age, sex, smoking status, diabetes, obesity, and lipids). the summary meta-analysis result was a risk ratio of 1.97 (95% confidence interval 1.78 to 2.17), which gives the average prognostic effect of crp (for those in the top v bottom third of crp distribution), and suggests larger crp values are associated with higher risk. although there was substantial between-study heterogeneity, nearly all estimates were in the same direction (that is, risk ratio >1). when restricting meta-analysis to just the 13 studies that adjusted for at least all six conventional prognostic factors, the summary risk ratio decreased to 1.65 (95% confidence interval 1.39 to 1.96), and the between-study heterogeneity reduced. using the study specific estimates given by hemingway and colleagues, we updated this meta-analysis (fig 1), obtaining the same summary result but a wider confidence interval (1.34 to 2.04) through the hartung-knapp approach.16

fig 1
fig 1
forest plot showing the study specific estimates and meta-analysis summary result of the adjusted prognostic effect (risk ratio) of c reactive protein taken from the review of hemingway and colleagues14; all studies were adjusted for a core set of existing prognostic factors (age, sex, smoking status, diabetes, obesity, and lipids), plus up to 14 other prognostic factors. meta-analysis results shown are based on a random effects meta-analysis model with dersimonian and laird estimation of the between-study variances. the summary result is identical to hemingway and colleagues,14 but the confidence interval is wider because we used the hartung-knapp approach to account for uncertainty in variance estimates.16although “risk ratio” is used, the estimates actually correspond to a mixture of risk ratios, odds ratios, and hazard ratios

download figure open in new tab download powerpoint
step 6: quantifying and examining heterogeneity
for all meta-analyses, when there is large heterogeneity across included studies, it may be better not to synthesise the study results, but rather display the variability in estimates on a forest plot without showing an overall pooled estimate. when a meta-analysis is performed in the face of heterogeneity, it is important to quantify and report the magnitude of heterogeneity itself; for example, through the estimate of (the between-study variance),62 or an approximate 95% prediction interval indicating the potential true prognostic effect of a factor in a new population.5363

subgroup analyses and meta-regression can be used to examine or explore the causes of heterogeneity. a subgroup analysis performs a separate meta-analysis for categories defined by a particular characteristic, such as those with a low risk of bias, those with a follow-up of less than one year or of at least one year, or those set in countries in europe. a better approach is meta-regression, which extends the meta-analysis equation shown in box 2 by including study level covariates,64 and allows a formal comparison of meta-analysis results across groups defined by covariates (eg, low risk of bias studies v studies at higher risk of bias). unfortunately, subgroup analyses and meta-regression are often problematic. there will often be few studies per subgroup and low power to detect genuine causes of heterogeneity. furthermore, study level confounding will be rife so that it is difficult to disentangle the associations for one covariate from another. for example, studies with a low risk of bias may also have a different length of follow-up or a particular cutpoint level compared with studies at higher risk of bias.

application to crp review
hemingway and colleagues reported that meta-regression identified four study level covariates that explained some between-study heterogeneity in the prognostic effect of crp: definition of comparison group, number of adjustment factors, the (log) number of events, and the proportion of patients with stable coronary disease (reflecting study size).14 studies originally reporting unequal crp groups had stronger effects than those reporting crp on a continuous scale. for each additional adjustment factor, the summary risk ratio decreased by 3%. the summary risk ratio was smaller among studies with more than the median number of outcome events, and smaller among studies confined to stable coronary disease. there was no evidence that the crp effect differed according to the number of quality items reported by a study, or by the type of prognostic effect measure provided (that is, risk ratio, odds ratio, or hazard ratio).

step 7: examining small-study effects
the term “small-study effects” refers to when there is a systematic difference in prognostic effect estimates for small studies and large studies.65 a particular concern is when small studies (especially those that are exploratory because these often evaluate many potential prognostic factors with relatively few outcome events) show larger prognostic effects than larger studies. this difference may be due to chance or heterogeneity, but a major threat here is publication bias and selective reporting, which are endemic in prognosis research.363738 such reporting biases lead to smaller studies, with (statistically) significant or larger prognostic factor effect estimates being more likely to be published or reported in sufficient detail, and thus included in a meta-analysis, than smaller studies with non-significant or smaller prognostic effect estimates. this bias is a potential concern for unadjusted and adjusted prognostic effects. a primary study usually estimates an unadjusted prognostic effect for each of multiple prognostic factors, but study authors may only report effects that are statistically significant. in addition, adjusted results are often only reported for prognostic factors that retain statistical significance in univariable and multivariable analysis. a consequence is that meta-analysis results will be biased, with larger summary prognostic effects than in reality, and potentially some factors being deemed to have clinical value when actually they do not.

the evidence for small-study effects is usually considered on a funnel plot, which shows the study estimates (x axis) against their precision (y axis). a funnel plot is usually recommended if there are 10 or more studies.65 the plot should ideally show a symmetric, funnel like shape, with results from larger studies at the centre of the funnel and smaller studies spanning out in both directions equally. asymmetry will arise if there are small-study effects, with a greater proportion of smaller studies in one particular direction. statistical tests for asymmetry in risk, odds and hazard ratios can be used, such as peter’s and debray’s test.6667 contour enhanced funnel plots also show the statistical significance of individual studies, and “missing” studies are perhaps more likely to fall within regions of non-significance if publication bias was the cause of small-study effects. an example is shown in figure 2.

fig 2
fig 2
evidence of funnel plot asymmetry (small-study effects) in the c reactive protein meta-analysis shown in figure 1. the smaller studies (with higher standard errors) have risk ratio (rr) estimates mainly to the right of the larger studies, and therefore give the largest prognostic effect estimates. a concern is that this is due to publication bias, with “missing” studies potentially falling to the left side of the larger studies and in the lighter shaded regions denoting non-significant rr estimates

download figure open in new tab download powerpoint
as mentioned, small-study effects may also arise due to heterogeneity. therefore, it is difficult to disentangle publication bias from heterogeneity in a single review. for example, if smaller studies used an analysis with fewer adjustment factors, then this may cause larger prognostic factor effects in such studies, rather than it being caused by publication bias. a multivariate meta-analysis could reduce the impact of small-study effects by “borrowing strength” from related information.61

a related concern is that smaller prognostic factor studies are generally at higher risk of bias than larger studies. smaller studies tend to be more exploratory in nature and typically based on a convenient sample, often examining many (sometimes hundreds of) potential prognostic factors, with relatively few outcome events. this design leads to spurious (due to chance) and potentially biased (due to poor estimation properties68) prognostic effect estimates, which are more prone to selective reporting. in contrast, larger studies are often confirmatory studies focusing on one or a few prognostic factors, and are more likely to adopt a protocol driven and prospective approach, with clearer reporting regardless of their findings.3 therefore, larger studies are less likely to identify spurious prognostic factor effect estimates. it is helpful to examine small-study effects (potential publication bias) when restricting analysis to the subset of studies at low risk of bias. if this approach resolves previous issues of small-study effects in the full meta-analysis, then it gives even more credence to focus conclusions and recommendations on the meta-analysis results based only on the higher quality studies.

application to crp review
figure 2 shows a funnel plot of the study estimates from the crp meta-analysis shown in figure 1. there is clear asymmetry, which shows the strong potential for publication bias. there was an insufficient number of studies considered at low risk of bias to evaluate small-study effects in a subset of higher quality studies.

step 8: reporting and interpretation of results
as with all research studies, clear and complete reporting is essential for reviews of prognostic factor studies. most of the reporting guidelines of prisma (preferred reporting items for systemic reviews and meta-analyses) and moose (meta-analysis of observational studies in epidemiology) will be relevant,6970 and should be complemented by remark,4748 which was aimed at primary prognostic factor studies. more specific guidance for reporting systematic reviews of prognostic factor studies is under development.

interpretation and translation of summary meta-analysis results is an important final step. the guidance in the previous steps is the essential input for this step. discussion is necessary on whether and how the prognostic factors identified may be useful in practice (that is, translation of results to clinical practice), and what further research is necessary. ideally impact studies (eg, randomised trials that compare groups which do and do not use a prognostic factor to inform clinical practice) are needed before strong recommendations for clinical practice are made; however, these studies are rare and outside the scope of the review framework outlined in this article.

to interpret the certainty (confidence) of the summary results of a review of intervention effectiveness, grade (grades of recommendation, assessment, development, and evaluation) was developed. this approach assesses the overall quality of and certainty in evidence for the summary estimates of the intervention effects by addressing five domains: risk of bias, inconsistency, imprecision, indirectness, and publication bias. the grade domains can be assessed using the information obtained by the tools and methods described in the above steps. however, it is not known whether these domains, developed for reviews of interventions, are equally applicable to assessing the certainty of summary results of systematic reviews of prognostic factor studies. compared with reviews of intervention studies, allowing for heterogeneity (the inconsistency domain) might be more acceptable in reviews of prognostic factor studies because of the inevitable heterogeneity caused by study differences in methods of measurement, adjustment factors, and statistical analysis methods, among others. furthermore, the threat of selective reporting or publication bias in reviews of prognostic factor studies may be more severe than in reviews of intervention studies because of the problems of exploratory studies, poor reporting, and biased analysis methods.

there is limited empirical evidence for using the existing domains to grade the certainty of summary estimates of prognostic factor studies, although a first attempt has been made71; in addition, an assessment has been performed on grading the certainty of evidence of summary estimates of overall prognosis studies.72 reviewers need to be especially cautious when comparing the adjusted prognostic value of multiple index factors, for example, to conclude whether the summary adjusted hazard ratio for prognostic factor a is larger than that for factor b. usually different sets of studies will be available for each index factor, and so the comparison will be indirect and potentially biased. moreover, the studies evaluating factor a may often have used different sets of adjustment factors (other prognostic factors) than those evaluating factor b. it will be rare to find studies on different index factors that used exactly the same set of adjustment factors. we therefore recommend reviewers restrict comparisons (of the adjusted prognostic value) of two or more index factors to those studies that at least used a similar, minimally required set of adjustment factors.73 even then, due to different scales and distributions of each factor (eg, continuous or binary), a simple comparison of the prognostic effect sizes (eg, hazard ratio for factor a v hazard ratio for factor b) may not be straightforward.

application to crp review
the meta-analysis results suggest crp is a prognostic factor for the risk of death and non-fatal cardiovascular events, even when only including the largest studies that adjusted for all six conventional prognostic factors. in their discussion, hemingway and colleagues downgraded the meta-analysis findings because of a strong concern about the quality and reliability of the underlying evidence.14 the absence of prespecified protocols, poor and potentially biased reporting, and strong potential for publication bias prevented the authors from making firm conclusions about whether crp has prognostic value after adjustment for established prognostic factors. they state that the concerns “explicitly challenge the statement for healthcare professionals made by the centers for disease control that measuring crp is both ‘useful’ and ‘independent’ as a marker of prognosis.”74

summary
in this article, we described the key steps and methods for conducting a systematic review and meta-analysis of prognostic factor studies. current reviews are often limited by the quality and heterogeneity of primary studies.7576 we expect the prevalence of such reviews to grow rapidly, especially as cochrane has recently embarked on prognosis reviews (see also the cochrane prognosis methods group website www.methods.cochrane.org/prognosis).77 our guidance will help researchers to write grant applications for reviews of prognostic factor studies, and to develop protocols and conduct such reviews. protocols of prognostic factor reviews should be published ideally at the same time as the review is registered, for example within prospero, the international prospective register of systematic reviews (www.crd.york.ac.uk/prospero/), or the cochrane database.77 our guidance will also allow readers and healthcare providers to better judge reports of prognostic factor reviews.

finally, we note that some of the limitations described (eg, use of different cutpoint values across studies) could be alleviated if the individual participant data were obtained from primary prognostic factor studies78 rather than being extracted from study publications; although, this may not solve all problems (eg, quality of original study, availability of different adjustment factors).79 further discussion on individual participant data meta-analysis of prognostic factor studies is given elsewhere.80

<|EndOfText|>

individual participant data meta-analysis of continuous
outcomes: a comparison of approaches for specifying
and estimating one-stage models

one-stage individual participant data meta-analysis models should account for
within-trial clustering, but it is currently debated how to do this. for continuous outcomes modeled using a linear regression framework, two competing approaches are a stratified intercept or a random intercept. the stratified
approach involves estimating a separate intercept term for each trial, whereas
the random intercept approach assumes that trial intercepts are drawn from
a normal distribution. here, through an extensive simulation study for continuous outcomes, we evaluate the impact of using the stratified and random
intercept approaches on statistical properties of the summary treatment effect
estimate. further aims are to compare (i) competing estimation options for the
one-stage models, including maximum likelihood and restricted maximum likelihood, and (ii) competing options for deriving confidence intervals (ci) for
the summary treatment effect, including the standard normal-based 95% ci,
and more conservative approaches of kenward-roger and satterthwaite, which
inflate cis to account for uncertainty in variance estimates. the findings reveal
that, for an individual participant data meta-analysis of randomized trials with a
1:1 treatment:control allocation ratio and heterogeneity in the treatment effect,
(i) bias and coverage of the summary treatment effect estimate are very similar when using stratified or random intercept models with restricted maximum
likelihood, and thus either approach could be taken in practice, (ii) cis are generally best derived using either a kenward-roger or satterthwaite correction,
although occasionally overly conservative, and (iii) if maximum likelihood is
required, a random intercept performs better than a stratified intercept model.
an illustrative example is provided.
keywords
continuous outcomes, estimation, individual participant data, ipd, meta-analysis

1 introduction
individual participant data (ipd) meta-analysis involves obtaining and then synthesizing raw individual-level data from
multiple related studies, to produce summary results that inform clinical decision making.1 the ipd approach is increasingly popular and has many potential advantages over a traditional meta-analysis of published aggregate data, such as
increased power to detect treatment-covariate interactions and avoiding reliance on published results.2
statistical methods to perform an ipd meta-analysis involve either a one-stage or two-stage approach.3 generally, these
approaches give very similar meta-analysis results, especially when they use the same modeling assumptions and/or
estimation methods.4,5 however, the one-stage approach has become increasingly popular over the past decade.6 it conveniently allows all studies to be analyzed simultaneously and avoids the assumption of normally distributed study effect
estimates with known variances that is usually made in the second stage of the two-stage approach. it also allows greater
flexibility of parameter specification over the two-stage approach.6
when conducting a one-stage ipd meta-analysis, it is important to account for clustering of participants within studies,
to correctly condition an individual's response to the study they are in. ignoring clustering and analyzing ipd as if coming
from a single study can result in misleading conclusions. for example, abo-zaid et al7 showed that family history of
thrombophilia was statistically significant as a diagnostic marker of deep vein thrombosis when clustering was accounted
for (odds ratio = 1.30; 95% confidence interval (ci): 1.00, 1.70; p value = 0.05) but not when clustering was ignored (odds
ratio = 1.06; 95% ci: 0.83, 1.37; p value = 0.64). while it is well established that clustering should be accounted for, it is
debatable exactly how this should be done. in particular, there are two competing approaches to account for clustering in
a one-stage model: a stratified intercept or a random intercept. the stratified approach involves a separate intercept term
being estimated for each study; thus, if there are 10 studies, 10 intercept terms would be estimated (one for each study). in
the random intercept approach, the intercepts are assumed to be drawn from some distribution, typically normal with an
underlying mean value and variance. the advantage of the stratified intercept approach is that it makes no assumptions
about the distribution of intercepts across studies. in contrast, the advantage of the random intercept approach is that it
requires fewer parameters to be estimated.
in this article, we evaluate through an extensive simulation study the impact of using either the stratified or random
intercept approach on the statistical properties of the summary treatment effect estimate (for example, in terms of bias,
precision, mean square error (mse), and coverage). this is considered in the context of randomized trials with a continuous outcome and a 1:1 treatment:control allocation ratio, assuming either common or random treatment effects across
trials. two further aims are to (i) compare competing estimation options for the one-stage models, including maximum
likelihood (ml) and restricted maximum likelihood (reml) and (ii) compare competing options for deriving confidence intervals for the summary treatment effect, including the standard normal-based 95% ci, and (for reml, but not
ml estimation) the kenward-roger (kr)8 and satterthwaite9 corrections that inflate confidence intervals to account for
uncertainty in variance estimates.
this paper is structured as follows. in section 2, we introduce the two competing one-stage ipd meta-analysis models of
interest that account for clustering, as well as the competing estimation and ci derivation options. in section 3, we outline
how the simulation study was conducted and present the results, and in section 4, we provide a real example to illustrate
the methods considered. finally, in section 5, we conclude with a discussion of the key findings and limitations and offer
a recommendation for those conducting one-stage ipd meta-analysis of randomized trials with 1:1 treatment:control
allocation ratio and with a continuous outcome.
2 introducing different model specification and
estimation options
consider that ipd have been obtained from i = 1 to k related randomized trials, each investigating a treatment effect
based on a continuous outcome y (say, blood pressure); that is, the mean difference in outcome value between a treatment
and a control group. suppose that there are ni participants in trial i. let yfi j be the end-of-trial (f used to denote final)
continuous outcome value, for participant j in trial i, and ybi j (b to denote baseline) be the pre-treatment outcome value.
let treati j take the value 1 or 0 for participants in the treatment or control group, respectively.
given such ipd, there are several ways in which researchers can use a one-stage meta-analysis to model the summary
treatment effect across trials. we focus initially on presenting one-stage analysis of covariance (ancova) mixed models,
which either use a stratified intercept or a random intercept to account for clustering of participants within trials. we also
assume a random treatment effect since heterogeneity is usually expected.
2.1 model (1): stratified intercept
with the following approach, a stratified intercept is used to account for within-trial clustering.
y𝐹 𝑖𝑗 = 𝛽i + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + ui) treat𝑖𝑗 + e𝑖𝑗 (1)
ui ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
here, 𝛽i denotes the intercept term for trial i (expected final outcome value for participants in the control group in trial i
who have the mean baseline outcome value), and the distinct intercept for each trial is used to account for within trial
clustering. the term 𝜆i denotes a trial-specific adjustment term for the baseline outcome value (here, centered at the
mean for each trial (y𝐵𝑖) to aid interpretation of the trial-specific intercepts). for example, when there are k = 10 trials,
there would be 10 𝛽i terms and 10 𝜆i terms. of main interest is an estimate of the model parameter 𝜃, as this denotes
the summary (average) treatment effect. the random effect, ui, indicates that the true treatment effects in each trial are
assumed to arise from a distribution of true effects with mean 𝜃 and between-trial variance 𝜏2. this assumption could be
constrained if considered appropriate, with a common (fixed) treatment effect (ie, constrain 𝜏2 = 0). lastly, 𝜎2
i denotes a
distinct residual variance per trial.
the flexibility of the one-stage ipd approach allows us to make further modifications by considering, for example, a
common baseline adjustment term (ie, 𝜆i= 𝜆) across trials, or common residual variances (ie, 𝜎2
i = 𝜎2) if necessary5,10,11;
however, this should be justified (eg, based on computational reasons or estimation problems), and sensitivity analysis to
the choice of assumptions is often sensible.
2.2 model (2): random intercept
when there are a large number of trials to be synthesized, a stratified intercept approach to clustering can be computationally intensive (as equation (1) requires estimation of 3 k + 2 parameters).4 an alternative approach for dealing with
clustering, which is preferred by some researchers,12 is to use a random intercept term.
y𝐹 𝑖𝑗 = (𝛽 + u1i) + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗 (2)
u1i ∼ n
(
0, 𝜏2
𝛽
)
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
parameters are as in equation (1), except that within-trial clustering has now been accounted for by a random (instead
of stratified) intercept term, with 𝜏2
𝛽 denoting the between trial variance in the intercept about the mean intercept (𝛽).
equation (2) assumes independence of the two random effects (ie, a covariance of zero), but their correlation could be
accounted for assuming a bivariate random effect distribution; indeed, this might be of special interest when evaluating
the relationship across trials of mean baseline in the control group and true treatment effect.13
compared to equation (1), the number of parameters to be estimated has been reduced, with only 𝛽 and 𝜏𝛽 for the
intercept, instead of k separate terms. therefore, fewer estimation problems might be anticipated than in equation (1). on
the downside, equation (2) makes a strong and potentially unnecessary assumption that control group means are drawn
from a normal distribution with a common mean and variance. furthermore, the estimation of an additional random
effect term might increase computational intensity.
2.3 options for estimation and ci derivation
the parameters in models (1) and (2) are typically estimated using either a ml or reml approach. ml is known to
produce downwardly biased estimates of between trial variance when there are few trials,14-16 whereas reml addresses
the downward bias and is thus generally preferred.17,18
in addition to competing options for model parameter estimation, there are also competing options to subsequently derive (1 − 𝛼)100% cis for the true summary treatment effect (𝜃). standard cis are based on large-sample
inference and assume 𝜃
̂ is approximately normally distributed:
𝜃
̂± z1−𝛼
2
√
var(𝜃
̂), (3)
where 𝜃
̂is the estimate of 𝜃, var(𝜃
̂) is its variance, and z1−𝛼
2
is the upper 1− 𝛼
2 quantile of the standard normal distribution.
this standard approach may produce cis that are too narrow, as var(𝜃
̂) does not account for the uncertainty in the estimate
of the between trial variation of 𝜃
̂.
4,18
to address this, more conservative options are available based on small-sample inference, which define the uncertainty
around 𝜃
̂ using approximations based on a t-distribution, such as the kr8 and satterthwaite9 corrections, which are also
known as denominator-degrees-of-freedom adjustments.
the kr corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
varkr(𝜃
̂), (4)
where 𝜃
̂ is as before, but now a bias-adjusted (inflated) variance (varkr(𝜃
̂)) is used, and t𝜐;1−𝛼
2
(the upper 1−𝛼
2 quantile of
the t-distribution with an adjusted degrees of freedom, 𝜐) instead of z1−𝛼
2
.
for a single parameter of interest (as in our case), the satterthwaite corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
var(𝜃
̂), (5)
where t𝜐;1−𝛼
2
is as in the kr correction, but the original (unadjusted) variance of 𝜃
̂is used. note that, while the denominator
degrees of freedom calculated from the kr and satterthwaite corrections are the same for single hypothesis tests, the kr
correction uses a bias-adjusted variance; therefore, cis derived using equations (4) and (5) will potentially differ, with the
one using the kr correction (equation (4)) leading to slightly wider intervals.19
although schaalje et al20 recommend kr over satterthwaite in special cases when the sampling distribution of the
test statistic is known, there remains debate over the best method, and a lack of literature in this area in regard to ipd
meta-analysis for estimation of a parameter of interest.
3 simulation study
we now perform a simulation study to examine the statistical performance of the summary treatment effect estimate (𝜃
̂)
from a one-stage ipd meta-analysis across a range of scenarios. our aim is to assess the different model specifications,
parameter estimation methods and ci derivation options described in section 2. that is, we compare the following: stratified or random intercept specifications; ml or reml estimation options; and, for reml estimation, 95% cis based on
asymptotic formula (equation (3)) or with either kr or satterthwaite corrections (equations (4) and (5), respectively)).
3.1 methods
provided is a step-by-step guide to our simulation study. for simplicity, and to considerably speed up the many simulations, we removed the baseline adjustment term in models (1) and (2), such that it does not exist in any of the data
generating mechanisms or models fitted in our simulations. in other words, we generate data without baseline imbalances
and thus analyze the data according to a final score ipd meta-analysis model, which is appropriate in this situation.
21 for
similar reasons of simplicity and computational complexity, we assumed a common residual variance across trials (both
in data generation and models fitted). extension to different residual variances is considered in our discussion (section 5).
to inform the true parameter values for the simulation, we used a previous ipd meta-analysis of treatment for lower
blood pressure outcomes.22
all analyses were conducted using stata v.14.2 (stata corporation, tx, usa).23
3.1.1 scenario 1 (base case)
the simulation process is now explained, in the context of an initial base case scenario with ipd from 10 trials and a
relatively simple data generating mechanism. extensions to other more complex scenarios are described afterwards.
step 1: data generating mechanism for one ipd meta-analysis of 10 trials
consider that an ipd meta-analysis of i = 1 to k related trials is of interest, with the goal to summarize a treatment
effect on a continuous outcome. to generate such data for the base case of this simulation study, we started by setting the
number of trials, k, to 10. we set a fixed number of participants, n = 100 in each trial, and assumed a fixed randomization
of 1:1 in each trial; that is, on average, 50% of participants within any given trial are allocated to a treatment group, and
the remaining 50% to a control group. this gave us a triali (trial 1/0 indicator) and treatij (treatment group 1/0 indicator)
value for each of 100 participants in each of 10 trials.
next, based on the previous meta-analysis,22 we set the true parameter values for this simulation to be as follows:
𝜃 = −9.66 (summary treatment effect; negative value favors treatment group), 𝜏2 = 7.79 (between trial variation in the
treatment effect), 𝛽 = 159.73 (mean blood pressure response in control group), 𝜏2
𝛽 = 233.99 (between trial variation in the
intercept), and 𝜎2 = 333.74 (residual variance).
we then used these parameter values to generate further terms, beginning with using 𝜎2 to generate an error term ei j,
for the jth participant from the ith trial
e𝑖𝑗 ∼ n(0, 𝜎2
). (6)
then, we generated the trial level values for the random parts of the intercept and treatment effect terms, u1i and u2i,
respectively,
u1i ∼ n
(
0, 𝜏2
𝛽
)
(7)
u2i ∼ n(0, 𝜏2
).
finally, with all the parameters defined (𝛽, u1i, 𝜃, u2i, treati j, and ei j), we generated the end-of-trial continuous outcome
value yfi j, under the random intercept model (2) (with no baseline adjustment term and assuming a common residual
variance)
y𝐹 𝑖𝑗 = (𝛽 + u1i) + (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗. (8)
this gave one complete ipd meta-analysis dataset of 1000 total participants, containing 100 participants in each of
10 trials, consisting of the following data for each individual: a trial indicator (triali), a treatment group indicator (treati j),
and an end-of-trial continuous outcome value (yfi j).
step 2: model fit and replication
using the generated data, we fitted a stratified intercept model (1) and a random intercept model (2) (without the
baseline adjustment term and assuming a common residual variance) separately to this simulated ipd, under all the combinations of estimation and ci derivation methods outlined in section 2. figure 1 provides a flow diagram summarizing
the possible combinations. each time a model was fitted (under a particular combination of estimation and ci derivation
methods), we stored the following: the summary treatment effect estimate, 𝜃
̂; its corresponding 95% ci; a binary indicator variable for coverage of 𝜃
̂ (ie, the value 1 if the 95% ci of 𝜃
̂ contained the true 𝜃, and 0 otherwise); estimates of any
variance parameters; model run time (from start of model fit to end of post estimation); and model convergence (1/0 for
convergence within 100 iterations/nonconvergence, respectively).
standard
ci
stratified
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
intercept
option
estimation
method
ci method standard
ci
random
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
figure 1 flow diagram of possible combinations of intercept option, estimation, and ci methods. ci, confidence interval;
kr, kenward-roger correction; ml, maximum likelihood estimation; reml, restricted maximum likelihood
for each model (stratified or random intercept) fitted to the data, this enabled us to obtain two estimates of 𝜃
(one each for the models fitted using ml and reml estimation, respectively) and four 95% cis for 𝜃
̂ (one for ml estimation with a standard ci derivation, and then one each for reml estimation with the standard, kr-corrected, and
satterthwaite-corrected ci derivations).
step 3: simulation replications
steps 1 and 2 were repeated until 1000 ipd meta-analysis datasets had been generated using the true parameter values
and procedure as outlined thus far, followed by application of the various intercept option, estimation, and ci methods to
each of the 1000 replicated datasets (note: 1000 simulations were chosen to give a monte carlo error of 0.7% on a coverage
of 95%).
step 4: summarizing performance
using the results obtained after step 3, the statistical properties of 𝜃
̂ under the different model specification and estimation options were assessed by summarizing the 1000 results obtained using the following metrics: mean percentage
(%) bias, empirical standard error (se), mse, coverage (separately for each ci method), convergence, and mean run time
(separately for each ci method). additionally, we considered the median percentage bias in the heterogeneity (𝜏2) of the
true treatment effects also. definitions of these performance measures are provided in web appendix a.
3.1.2 extended set of 38 scenarios changing number of trials, participants, between-trial
distributions, and data generating mechanisms
the base case scenario defined in section 3.1.1 was extended to further settings, leading to an extensive range of
38 scenarios in total (see table 1), which we now summarize.
we varied the number of trials (scenarios a1 and a2), so that k = 5, 10, and 20 were considered, which cover the typical
sizes of ipd meta-analyses in our experience. we also considered trials with differing sample sizes within an ipd, so that
ni (number of participants within trial i) was drawn from a uniform distribution, ni∼u(a, b). fixing a = 30, b = 1000
(scenario b1) allowed for mixed sample sizes, and having 5 trials with a = 30, b = 100 and 5 with a = 900, b = 1000 within
an ipd (scenario b2) tested the effect of a mix of small and large sample sizes only. lastly, fixing a = 30, b = 100 tested
the effect of having only small trials (scenario b3).
we also tested the combined effect of varying the number of trials and number of participants per trial simultaneously
(scenarios b1-a1, b1-a2, b2-a1, b2-a2), and we tested the effects of adjusting the magnitude of the intercept or treatment
effect heterogeneity (scenarios c1, c2, d1, d2).
scenarios 15 to 38 replicate the first 14 scenarios where possible, for modifications to the base case data generating
mechanism. first, to test the robustness of the normality of the intercept assumption in the random intercept model, we
altered the final step of the data generating mechanism in equation (8), so that the final outcome was calculated by
y𝐹 𝑖𝑗 = 𝛽i + (𝜃 + u2i)treat𝑖𝑗 + e𝑖𝑗 (9)
𝛽i ∼ (beta (15, 3)) × 220
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
).
therefore, the intercept term 𝛽i was now derived from a beta distribution with shape parameters of 15 and 3, which
represent a negatively skewed distribution that was then scaled by 220 to give sensible values for systolic blood pressure
(the outcome upon which the hypothetical data is based). an example density plot of this beta distribution for modeling
the intercept term is shown in web figure a.1.
secondly, we also considered a data generating mechanism with a common (fixed) treatment effect (ie, 𝜏2 = 0). here,
the fitted stratified and random intercept models were also modified to have a common treatment effect.
3.2 results
simulation results are shown in tables 2 and 3, covering most of the scenarios under the normal and beta distribution
intercept data generating mechanisms, across all options for specifying and estimating the intercept. these tables show the
mean percentage bias of the summary treatment effect estimate (𝜃
̂) (table 2) and the median percentage bias in its heterogeneity (𝜏̂2) (table 3). figure 2 graphically depicts the percentage coverage of the summary treatment effect estimate (𝜃
̂).
.
table 1 summary of the different simulation scenarios*
scenario data generation details modification from base case scenario
base case (i) number of trials, k = 10 -
(ii) number of participants in trial i, ni = 100 (fixed across all trials)
(iii) fixed treatment exposure of 50%
(iv) 𝜃 = −9.66 (summary treatment effect; negative value favors treatment group)
(v) 𝜏2 = 7.79 (between trial variation in 𝜃)
(vi) 𝛽 = 159.73 (mean response in control group)
(vii) 𝜏𝜷 2 = 233.99 (between trial variation in 𝛽)
(viii) 𝜎2 = 333.74 (residual variance)
a1 same as base case, except changed (i) k = 5
a2 same as base case, except changed (i) k = 20
b1 same as base case, except changed (ii) n ∼i u(30, 1000)
b2 same as base case, except changed (ii) n ∼i u(30, 100) for trials 1 to 5,
n ∼i u(900, 1000) for trials 6 to 10
b1-a1 same as base case, except changed (i) and (ii) k = 5 and n ∼i u(30, 1000)
b1-a2 same as base case, except changed (i) and (ii) k = 20 and n ∼i u(30, 1000)
b2-a1 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 and 2, n ∼i u(900, 1000) for trials 3 to 5
b2-a2 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 to 10, n ∼i u(900, 1000) for trials 11 to 20
b3 same as base case, except changed (ii) n ∼i u(30, 100)
c1 same as base case, except changed (vii) halving 𝜏𝛽 2 to 117
c2 same as base case, except changed (vii) doubling 𝜏𝛽 2 to 468
d1 same as base case, except changed (v) halving 𝜏2 to 3.9
d2 same as base case, except changed (v) doubling 𝜏2 to 15.6
*each scenario was repeated under the following data generating mechanisms: (1) random treatment effect with a normally distributed intercept, (2) random treatment effect with a
220*beta(15, 3) distribution for the intercept (except scenarios c1 and c2), and (3) common treatment effect with a normally distributed intercept (except scenarios d1 and d2).
abbreviations: k = number of trials, ni = number of participants in trial i, 𝜃 = summary treatment effect, 𝜏2 = between trial variation in summary treatment effect, 𝛽 = mean response in
control group, 𝜏𝛽 2 = between trial variation in mean response in control group, 𝜎2 = residual variance, u (a, b) = uniform distribution over the interval (a, b).
table 2 mean percentage bias of the summary treatment effect estimate (𝜃
̂) under different scenarios, for the
random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified (1) and random (2) intercept models, under each of the different
estimation options considered
mean percentage bias of 𝜽̂
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −0.01 0.00 −0.01 −0.01 0.34 0.31 0.33 0.29
a1 −0.90 −0.90 −0.90 −0.90 −0.02 0.13 −0.06 0.10
a2 0.15 0.18 0.16 0.18 −0.48 −0.41 −0.47 −0.40
b1 0.67 0.58 0.68 0.58 −0.57 −0.63 −0.58 −0.63
b2 −0.47 −0.59 −0.47 −0.56 0.29 0.27 0.33 0.28
b1-a1 0.54 0.53 0.53 0.53 −0.14 −0.27 −0.11 −0.24
b1-a2 −0.10 −0.11 −0.10 −0.11 0.08 −0.01 0.10 0.01
b2-a1 −0.41 −0.43 −0.37 −0.41 0.46 0.52 0.05 −0.17
b2-a2 −0.45 −0.41 −0.44 −0.42 −0.45 −0.37 −0.37 −0.34
b3 0.19 0.24 0.21 0.25 1.36 1.34 1.20 1.20
c1 −0.03 −0.02 −0.03 −0.02 n/a n/a n/a n/a
c2 −0.01 0.07 −0.01 0.07 n/a n/a n/a n/a
d1 −0.10 −0.13 −0.10 −0.13 0.26 0.34 0.24 0.32
d2 0.13 0.12 0.13 0.12 0.46 0.49 0.45 0.46
* see table 1 for full data generation details relating to each scenario. true value for 𝜃 is −9.66.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
we focus on the results when assuming a random treatment effect. further results assuming a common treatment effect
data generating mechanism and for additional performance measures (percentage convergence of models, numerical
percentage coverage of the summary treatment effect estimate, average run time of simulations, and empirical se and
mse of the summary treatment effect estimate) are shown in the supplementary material (web appendices b and c,
respectively). in the following, we summarize the key findings.
3.2.1 convergence of models
under a random treatment effect data generating mechanism, the proportion of models that converged was consistently
high, with a minimum convergence of 94.3% across all situations (web table c.i).
note that all other performance measures to follow are estimated conditional on model convergence.
3.2.2 bias of summary treatment effect estimate
generally, there were negligible differences in mean percentage bias of 𝜃
̂ between ml and reml estimation options for
either model (stratified or random intercept), under any given scenario and data generating mechanism (table 2 and
web table b.i). nor were there any important differences in the mean percentage bias of 𝜃
̂ between the stratified model
and random intercept model. furthermore, mean bias was close to zero in all situations and only reached a maximum
absolute percentage of 1.36%.
3.2.3 bias of estimated between-trial variance of treatment effects
for either model (stratified or random intercept), under any given scenario and data generating mechanism, using ml
always produced more downwardly biased estimates than reml (table 3), as expected.14-18 for example, for the base
table 3 median percentage bias of the between-trial variance of treatment effects (𝜏̂2), under different scenarios
for the random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified and random intercept models, under each of the estimation options considered
median percentage bias of ̂𝝉𝟐
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −100.00 −16.86 −41.50 −15.85 −100.00 −14.36 −56.17 −32.86
a1 −100.00 −36.88 −80.33 −33.10 −100.00 −73.62 −100.00 −80.15
a2 −100.00 −8.59 −20.86 −7.74 −100.00 −13.96 −39.78 −25.06
b1 −49.64 −10.74 −22.94 −10.09 −100.00 −14.23 −28.91 −16.39
b2 −56.93 −18.03 −35.11 −17.84 −72.90 −17.92 −35.95 −19.81
b1-a1 −77.28 −19.57 −42.62 −18.45 −100.00 −27.27 −54.23 −30.91
b1-a2 −40.64 −5.83 −13.08 −6.31 −88.22 −9.50 −19.59 −13.53
b2-a1 −72.73 −28.35 −56.23 −28.10 −100.00 −34.30 −64.33 −32.61
b2-a2 −36.66 −4.98 −14.36 −5.39 −50.99 −4.84 −12.86 −5.35
b3 −100.00 −28.68 −61.86 −24.74 −100.00 −17.42 −81.05 −48.75
c1 −100.00 −16.72 −39.54 −14.38 n/a n/a n/a n/a
c2 −100.00 −16.86 −40.56 −15.94 n/a n/a n/a n/a
d1 −100.00 −19.20 −66.97 −24.63 −100.00 −33.67 −99.98 −62.07
d2 −100.00 −11.65 −30.04 −11.79 −100.00 −10.50 −37.84 −22.19
* see table 1 for full data generation details relating to each scenario. true value for 𝜏2 is 7.79, except scenarios d1 and d2 where 𝜏2
is equal to 3.9 and 15.6, respectively.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
case scenario with the random intercept model, under the normal intercept data generating mechanism, the median
percentage bias using reml estimation was −15.9% compared to −41.5% using ml estimation. the bias was worse when
using a stratified intercept model (due to the extra number of parameters to estimate), as ml estimation often produced
a downward median bias of 100%.
when using reml estimation, there were generally only small differences between random and stratified intercept
models in terms of bias of the between-trial variance of treatment effects; however, while better than ml, downward bias
was not removed entirely with reml. furthermore, the overall size of the bias was typically greater in the beta distribution
intercept case than in the normal distribution intercept case, regardless of which model was used.
3.2.4 empirical se and mse of summary treatment effect estimate
there were negligible differences in empirical se or mse of 𝜃
̂ between the two models (stratified or random intercept),
under any given scenario and data generating mechanism (web tables c.viii to c.x).
3.2.5 coverage of summary treatment effect estimate
there were marked differences observed in the coverage of 𝜃
̂ across the different estimation approaches (ml or reml)
and ci derivations (standard, kr, or satterthwaite), as now explained.
(i) under a normal distribution intercept generating mechanism
we consider first the normal distribution intercept generating mechanism (figure 2a and web table c.ii). across both
models and all scenarios, ml with standard ci (ml + standard) derivation always exhibited under-coverage compared to
the other options (reml+standard, reml+kr, reml+satterthwaite). for example, for scenario b2 using the stratified
intercept model, the percentage coverage using ml + standard was 81.3% compared to 88.8%, 95.8%, and 94.8% using
figure 2 percentage coverage of the summary treatment effect estimate (𝜃
̂) under different scenarios for the random treatment effect
with normal (figure 2a) and beta distributions (figure 2b) for the intercept data generating mechanisms, for stratified (left) and random
(right) intercept models, under each of the estimation and ci derivation options considered. options: ml, maximum likelihood estimation
with standard confidence interval (ci) derivation; reml, restricted maximum likelihood estimation with standard ci derivation;
reml+kr, reml estimation with kenward-roger ci derivation; reml+satt, reml estimation with satterthwaite ci derivation [colour
figure can be viewed at wileyonlinelibrary.com]
reml+standard, reml+kr and reml+satterthwaite, respectively. the random intercept model always performed
better with respect to coverage under ml than the stratified intercept model under ml, likely due to the reduction in the
number of parameters that needed estimation. for example, when considering only small trials (scenario b3), percentage
coverage improved from 91.3% to 94.5% (close to the nominal 95% level), when comparing the stratified to a random
intercept model with ml estimation.
using reml substantially improved on the coverage obtained from ml and removed any important differences
between the stratified and random intercept models. however, for either model (stratified or random intercept),
reml+standard still had important under-coverage in some scenarios. for example, in scenario b2-a1, a percentage
coverage of 85.7% and 85.8% was observed, under a stratified and random intercept model, respectively.
the reml+kr approach generally improved on the coverage compared to reml+standard, again with no important
differences observed between the stratified and random intercept models. percentage coverage ranged from 95.1 to 98.8%
using reml+kr, while the percentage coverage ranged from 85.7% to 94.9% using reml+standard. the improvement
gained by using reml+kr was especially important for scenarios that involved at least 10 trials and a large variation
in sample sizes (b1, b2, b1-a2, b2-a2). for example, for scenario b2 (five small and five large sample sized trials, with
average sample size 66 and 949 in the small and large trials, respectively), percentage coverage from the stratified intercept
model was 88.8%, using reml+standard, but 95.8% using reml+kr.
using reml+satterthwaite gave very similar results to reml+kr. occasionally, there was some over-coverage using
reml+kr or reml+satterthwaite, particularly when using a low number of trials (k = 5). for example, coverage was
close to 99% (regardless of which model was used), in a setting of k = 5 trials with an equal number of participants
per trial (scenario a1; ni = 100), and in a setting of k = 5 trials with some small-sized and some large-sized trials (scenario
b2-a1; 2 small trials where ni∼u(30, 100), and 3 large trials where ni∼u(900, 1000)).
(ii) under a beta distribution intercept generating mechanism
for the beta distribution intercept generating mechanism (figure 2b and web table c.iii), using reml+standard again
gave better coverage than using ml, and using reml+kr or reml+satterthwaite generally further improved upon this
coverage (ie, moved it closer to 95%), especially with scenarios concerning at least 10 trials that had a large variation in
sample sizes.
as before, under ml estimation, the random intercept model showed better estimates of between-trial variance and
improved coverage (closer to 95%) than the stratified intercept model. however, differences between the two models were
generally small for estimation under reml (with or without a 95% ci correction).
3.2.6 common treatment effect data generating mechanism
results based on a common (fixed) treatment effect data generating mechanism are shown in web appendix b. all fitted
models assumed a common treatment effect and converged every time (ie, 100% convergence), and there was negligible
difference in mean percentage bias of 𝜃
̂ between ml and reml estimation options for either model (stratified or
random intercept), or between either model (web table b.1). the percentage coverage results were stable across all comparisons, ranging from 93.8 to 96.0%, with negligible differences between the various models and estimation options
(web figure b.1).
3.2.7 key findings
a summary of the key findings from this simulation study for settings with between-trial heterogeneity in the treatment
effect is given in figure 3.
figure 3 key simulation findings and recommendation for estimating a summary treatment effect based on a one-stage individual
participant data (ipd) meta-analysis of randomized trials with a 1:1 treatment:control allocation ratio and a continuous outcome, with
between-study heterogeneity in the treatment effect. ci, confidence interval; ml, maximum likelihood estimation; mse, mean square error;
reml, restricted maximum likelihood; se, standard error
4 illustration of methods and key findings in a real example
the international weight management in pregnancy (i-wip) collaborative group dataset includes ipd from 36 trials
(12,447 women), collected for a health technology assessment report in 2017.24 the authors investigated the association
between diet and lifestyle interventions to prevent weight gain in pregnancy and several other primary outcomes. here, we
present ipd meta-analysis results using the i-wip dataset, for illustration purposes only, to demonstrate the key findings
from our simulation study. we include only trials that collected follow-up outcome values for weight in pregnancy and
apply a one-stage ipd model for this continuous outcome, with model assumptions in line with our simulation study
analysis. however, while we did not generate any baseline imbalances in our simulation data, baseline weight imbalance
was present in some trials from the i-wip data. to remove this imbalance, we apply a baseline adjustment in our models,
as is recommended.21
table 4 shows ipd meta-analysis results for a random sample of 5 and 10 trials that investigated exercise interventions
and for 20 trials (using all 15 exercise trials, plus 5 additional trials that investigated mixed interventions).
these results are in agreement with the key findings observed in section 3.2 and summarized in figure 3. firstly,
the magnitude of summary treatment effect estimate was similar throughout, irrespective of model used or estimation
method. secondly, with ml estimation, the stratified intercept model gave narrower 95% cis and smaller estimates of the
between trial variance than the random intercept model, especially with k = 20 trials. thirdly, using reml overcame this
discrepancy, with now very similar results between the random and stratified intercept models; in addition, the 95% cis
were wider when using reml, due to the larger estimates of the between trial variance. fourthly, applying a kr or
satterthwaite correction in addition to reml further widened the 95% cis. finally, although 95% cis were slightly wider
when using a kr correction instead of satterthwaite, results were generally similar from these two corrections, especially
with k = 20 trials.
5 discussion
5.1 key findings
in summary, we have conducted an extensive simulation study to examine the estimation of a summary treatment effect
using a one-stage ipd meta-analysis model for a continuous outcome. specifically, we examined different options for
specifying the trial-specific intercepts and compared different options for parameter estimation and ci derivation. fourteen different scenarios were tested (varying the number of trials, number of participants per trial, and heterogeneity of
parameters), for each of three different data generating mechanisms (encompassing a common and random treatment
effect with a normally distributed intercept, as well as a beta distributed intercept and random treatment effect). all scenarios assumed a 1:1 treatment:control allocation ratio, and a data generating mechanism that was based on a random
intercept model; hence, our conclusions are restricted to this context.
our key findings, for settings with heterogeneity in treatment effect, were illustrated using a real example, and these
are summarized in figure 3. firstly, the results suggest that, as long as the same estimation method is used, there are no
important differences between the stratified and random intercept models in terms of bias, empirical se or mse for the
summary treatment effect estimate. indeed, the mean bias in 𝜃
̂ was close to zero throughout, which is perhaps expected
given the statistical theory underpinning linear mixed models. furthermore, when using reml (with or without a ci
correction), there were generally no important differences in coverage performance between the stratified and random
intercept models. interestingly, the random intercept model (which assumes normality of the intercept) performed well
even when the trial intercepts were drawn from a highly asymmetric beta distribution. kahan and morris25 also found
that misspecifying the random intercept distribution of random effects models did not impact treatment effect results.
secondly, the kr and satterthwaite corrections generally performed similarly in terms of improving the coverage and
were especially effective for scenarios involving at least 10 trials with a mix of small and large sample sizes, but also
considerably increased mean run time in these instances (see web tables c.v to c.vii). the satterthwaite correction
always had a similar or quicker average run time than kr (sometimes by more than eight times). one could surmise from
the similarity in coverage performance that the main impact of both corrections is in the use of a t-distribution to derive
cis and that the kr adjusted variance of the summary estimate has relatively less impact.
thirdly, when using ml estimation, the random intercept model always showed better or comparable coverage to the
stratified intercept model (closer to 95%). this is likely due to the random intercept model having a reduced number of
.
table 4 results from baseline weight adjusted individual participant data meta-analysis of i-wip data: summary treatment effect estimate (
̂
𝜃) with 95% confidence interval and
between-trial variance of treatment effects estimate (̂𝜏2). from meta-analysis with different numbers of trials (k = 5, 10, or 20), and assuming a random treatment effect and a common
residual variance throughout
̂
𝜽 (95% ci); ̂𝝉𝟐
method for modeling stratified random
intercept intercept intercept
estimation ml reml reml+kr reml+satt ml reml reml+kr reml+satt
number of trials
5 −1.172 −1.172 −1.172 −1.172 −1.170 −1.171 −1.171 −1.171
(−1.811,−0.534); (−1.815,−0.530); (−3.114, 0.770); (−2.712, 0.367); (−1.811,−0.529); (−1.813,−0.528); (−3.072, 0.731); (−2.681, 0.340);
8.58e−17 3.94e−15 3.94e−15 3.94e−15 2.95e−14 4.51e−12 4.51e−12 4.51e−12
10 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972
(−1.479,−0.465); (−1.482,−0.462); (−1.740,−0.204); (−1.653,−0.291); (−1.481,−0.462); (−1.482,−0.462); (−1.731,−0.212); (−1.646,−0.298);
1.97e−16 2.58e−12 2.58e−12 2.58e−12 9.52e−11 5.94e−16 5.94e−16 5.94e−16
20 −0.821 −0.820 −0.820 −0.820 −0.830 −0.830 −0.830 −0.830
(−1.102,−0.540); (−1.243,−0.396); (−1.298,−0.342); (−1.286,−0.354); (−1.217,−0.442); (−1.235,−0.426); (−1.290,−0.370); (−1.276,−0.384);
1.11e−14 0.317 0.317 0.317 0.210 0.258 0.258 0.258
ci = confidence interval
options: ml, maximum likelihood estimation with standard ci derivation; reml, restricted maximum likelihood estimation with standard ci derivation; reml+kr, reml estimation with kenward-roger
ci derivation; reml+satt, reml estimation with satterthwaite ci derivation.
parameters, and thus improved ml estimation of the between-trial variance. a similar finding was also recently shown
by jackson et al26 for one-stage meta-analysis models for a binary outcome. nevertheless, even the random intercept
model produced downwardly biased estimates of the between-trial variance using ml and low coverage. using reml
is therefore important, to improve on this coverage. indeed, coverage is more consistently near 95% when using reml
with either a kr or satterthwaite correction. however, on some occasions (particularly, when there are a low number of
trials), the kr and satterthwaite corrections lead to over-coverage. this is similar to the hartung-knapp sidik-jonkman
correction to 95% cis following a two-stage analysis,27,28 which generally gives a more suitable coverage than a standard
95% ci, although on occasion is overly conservative.29
if there is genuinely no heterogeneity in treatment effect across trials, however, our findings suggest that there are
generally no differences in mean bias, empirical se, mse, or coverage for the treatment effect between the stratified
and random intercept models, for any estimation method, ci derivation approach, and under any simulation scenario.
however, in our experience, situations of completely homogeneous treatment effects are unlikely.
unreported simulations
following recent work by morris et al5 and jackson et al,26 which considered an alternative coding for the binary treatment group variable (+0.5/−0.5 for treatment/control groups, respectively), we also tested this treatment group coding
for our reml estimation simulation results but found only small differences in performance results compared to the 1/0
coding. hence, we did not present the results here. we also tested using stratified (instead of common) residual variances
for both the data generating mechanisms and models fitted. again, no difference in performance of the summary treatment effect estimate was observed, suggesting that the ipd model may be robust to the (mis) specification of the residual
variances. morris et al5 also found that assuming common or distinct residual variances, in a common treatment effect
ipd meta-analysis setting, has very little impact on the precision of the summary effect when the number of patients per
trial is over 25. in general, from a point of principle, we recommend a separate residual variance for each trial, but in situations where this has convergence problems, a common residual variance would seem apt. further research of this issue
would be welcome.
5.2 recommendation
for researchers conducting a one-stage ipd meta-analysis of randomized trials with a 1:1 treatment:control allocation
ratio and a continuous outcome and aiming to estimate a summary treatment effect that is heterogeneous across trials,
we recommend that either a stratified or random intercept model is used, and estimated using reml, ideally followed
by a 95% ci derived using either the kr or satterthwaite approaches. in our simulations, this approach gave close to zero
mean bias in the summary treatment effect estimate and coverage generally close to 95%, except in a few situations where
there was over-coverage (particularly, when there were a low number of trials).
using reml with a kr correction for linear mixed models based on continuous outcomes has already been proposed
by some researchers,30,31 while literature advocating the merits of the satterthwaite correction is less common. however,
in our simulations, we found that the satterthwaite correction generally obtains similar results to the kr approach, hence
making for an excellent alternative.
5.3 limitations and further research
throughout this simulation study, we focused solely on synthesizing trials containing a 1:1 treatment:control allocation ratio; hence, an important limitation is that our conclusions may not hold under settings involving other treatment
allocation ratios.
in addition, we have focused solely on ipd of continuous outcomes, hence another important limitation is that our
conclusions are not necessarily generalizable to other popular outcome types in the meta-analytical field, such as binary
and time-to-event outcomes. binary outcomes, for example, are more complex to deal with than continuous outcomes,
as a logistic mixed effects model is nonlinear, and hence, the corresponding maximum likelihood function has no closed
form. jackson et al26 recently investigated the use of ml estimation and found that a stratified intercept model leads
to substantial downward bias in between-trial variance estimates and under-coverage of cis for the summary result,
which increases as the number of trials (and thus parameters) increases. interestingly, the issue was resolved when using
a + 0.5/−0.5 coding for the treatment variable, rather than a 1/0 coding, or when placing random effects on the trial
intercept.26 mcneish32 investigated logistic mixed models by either retaining the nonlinearity of the model and making
an approximation for the likelihood function or linearly approximating the model to give the likelihood function a closed
form (pseudo-likelihood approach). the latter option was shown to be favorable (under the specific conditions of the
study), by use of a residual penalized quasi-likelihood with a kr correction.
while our simulation study did consider an extensive range of scenarios—we varied the number of trials, number of
participants per trial, and heterogeneity of parameters—we recognize that our conclusions were based on a final score
model that did not adjust for baseline outcome value. often, the ancova model should be used, as in our applied
example, because there will be baseline imbalances in practice. however, as baseline values did not vary across individuals in our simulation study, using a final score analysis model rather than ancova was appropriate. when using
one-stage ancova ipd models, an additional issue is using stratified adjustment terms or placing a random effect on
the adjustment term. based on our study findings, we expect that, with reml, either approach should be suitable.
we also assumed independence of the two random effects (ie, a covariance of zero) when assuming random intercept
and random treatment effects, both in the data generating mechanism and when fitting the corresponding model. their
correlation could be taken into account if deemed sensible13; however, we did not consider this alternative assumption
in our simulation study, largely due to the added complexity and difficulty in estimating the correlation parameter in
practice with few trials. importantly, it is perhaps likely that the effect of treatment could be correlated with the control
group outcome, and therefore, the most appropriate assumption needs further consideration.
another limitation is that we did not consider prediction intervals. these allow us to make predictive inferences of
the potential treatment effect in a single setting of application.33 some researchers argue that prediction intervals offer
a more appropriate summary of trial findings than cis of the average effect.34 however, partlett and riley18 showed in a
two-stage ipd meta-analysis setting that there was considerable under-coverage of prediction intervals in some situations.
for example, under-coverage was observed in settings involving a low heterogeneity or with varied trial sample sizes and
was not improved upon by increasing the number of trials or using ci corrections such as kr. hence, we did not consider
it useful to consider prediction intervals in our study.
finally, we could have considered a bayesian approach to our simulation study, which is an alternative to frequentist
methods, and a natural way to account for all parameter uncertainty, to make predictions and to derive (joint) probabilistic
statements regarding parameters of interest. however, we deemed this extension to be beyond the scope of this paper.
yet, if a bayesian approach is to be used in practice, bayesians still need to choose between random or stratified intercept
one-stage ipd models, which is something that our work can help with.
5.4 conclusions
in an ipd meta-analysis of trials with a 1:1 treatment:control allocation ratio and a continuous outcome, aiming to estimate a summary treatment effect that is heterogeneous across trials, our findings suggest that researchers use either a
stratified or random intercept model with reml estimation and ideally derive 95% cis using either the kr or satterthwaite approach. further work is needed to improve upon coverage in a few situations where the kr and satterthwaite
intervals are overly conservative. such situations include when there are a low number of trials; these are also situations
where corrections to cis in a two-stage ipd meta-analysis are overly conservative.18


<|EndOfText|>

the value of preseason screening for injury prediction: the development and internal validation of a multivariable prognostic model to predict indirect muscle injury risk in elite football (soccer) players

abstract
background
in elite football (soccer), periodic health examination (phe) could provide prognostic factors to predict injury risk.

objective
to develop and internally validate a prognostic model to predict individualised indirect (non-contact) muscle injury (imi) risk during a season in elite footballers, only using phe-derived candidate prognostic factors.

methods
routinely collected preseason phe and injury data were used from 152 players over 5 seasons (1st july 2013 to 19th may 2018). ten candidate prognostic factors (12 parameters) were included in model development. multiple imputation was used to handle missing values. the outcome was any time-loss, index indirect muscle injury (i-imi) affecting the lower extremity. a full logistic regression model was fitted, and a parsimonious model developed using backward-selection to remove factors that exceeded a threshold that was equivalent to akaike’s information criterion (alpha 0.157). predictive performance was assessed through calibration, discrimination and decision-curve analysis, averaged across all imputed datasets. the model was internally validated using bootstrapping and adjusted for overfitting.

results
during 317 participant-seasons, 138 i-imis were recorded. the parsimonious model included only age and frequency of previous imis; apparent calibration was perfect, but discrimination was modest (c-index = 0.641, 95% confidence interval (ci) = 0.580 to 0.703), with clinical utility evident between risk thresholds of 37–71%. after validation and overfitting adjustment, performance deteriorated (c-index = 0.589 (95% ci = 0.528 to 0.651); calibration-in-the-large = − 0.009 (95% ci = − 0.239 to 0.239); calibration slope = 0.718 (95% ci = 0.275 to 1.161)).

conclusion
the selected phe data were insufficient prognostic factors from which to develop a useful model for predicting imi risk in elite footballers. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.

trial registration
nct03782389

key points
factors measured through preseason screening generally have weak prognostic strength for future indirect muscle injuries, and further research is needed to identify novel, robust prognostic factors.

because of sample size restrictions and until the evidence base improves, it is likely that any further attempts at creating a prognostic model at individual club level would also suffer from poor performance.

the value of using preseason screening data to make injury predictions or to select bespoke injury prevention strategies remains to be demonstrated, so screening should only be considered as useful for detection of salient pathology or for rehabilitation/performance monitoring purposes at this time.

background
in elite football (soccer), indirect (non-contact) muscle injuries (imis) predominantly affect the lower extremities and account for 30.3 to 47.9% of all injuries that result in time lost to training or competition [1,2,3,4,5]. reduced player availability negatively impacts upon medical [6] and financial resources [7, 8] and has implications for team performance [9]. therefore, injury prevention strategies are important to professional teams [9].

periodic health examination (phe), or screening, is a key component of injury prevention practice in elite sport [10]. specifically, in elite football, phe is used by 94% of teams and consists of medical, musculoskeletal, functional and performance tests that are typically evaluated during preseason and in-season periods [11]. phe has a rehabilitation and performance monitoring function [12] and is also used to detect musculoskeletal or medical conditions that may be dangerous or performance limiting [13]. another perceived role of phe is to recognise and manage factors that may increase, or predict, an athlete’s future injury risk [10], although this function is currently unsubstantiated [13].

phe-derived variables associated with particular injury outcomes (such as imis) are called prognostic factors [14], which can be used to identify risk differences between players within a team [12]. single prognostic factors are unlikely to satisfactorily predict an individual’s injury risk if used independently [15]. however, several factors could be combined in a multivariable prognostic prediction model to offer more accurate personalised risk estimates for the occurrence of a future event or injury [15, 16]. such models could be used to identify high-risk individuals who may require an intervention that is designed to reduce risk [17], thus assisting decisions in clinical practice [18]. despite the potential benefits of using prognostic models for injury risk prediction, we are unaware of any that have been developed using phe data in elite football [19].

therefore, the aim of this study was to develop and internally validate a prognostic model to predict individualised imi risk during a season in elite footballers, using a set of candidate prognostic factors derived from preseason phe data.

methods
the methods have been described in a published protocol [20] so will only be briefly outlined. this study has been registered on clinicaltrials.gov (identifier: nct03782389) and is reported according to the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) statement [21, 22].

data sources
this study was a retrospective cohort design. eligible participants were identified from a population of male elite footballers, aged 16–40 years old at manchester united football club. a dataset was created using routinely collected injury and preseason phe data over 5 seasons (1st july 2013 to 19th may 2018). for each season, which started on 1st july, participants completed a mandatory phe during week 1 and were followed up to the final first team game of the season. if eligible participants were injured at the time of phe, a risk assessment was completed by medical staff. only tests that were appropriate and safe for the participant’s condition were completed; examiners were not blinded to injury status.

participants and eligibility criteria
during any season, participants were eligible if they (1) were not a goalkeeper and (2) participated in phe for the relevant season. participants were excluded if they were not contracted to the club for the forthcoming season at the time of phe.

ethics and data use
informed consent was not required as data were captured from the mandatory phe completed through the participants’ employment. the data usage was approved by the club and university of manchester research ethics service.

outcome
the outcome was any time-loss, index imi (i-imi) of the lower extremity. that is, any i-imi sustained by a participant during matches or training, which affected lower abdominal, hip, thigh, calf or foot muscle groups and prohibited future football participation [23]. i-imis were graded by a club doctor or physiotherapist according to the validated munich consensus statement for the classification of muscle injuries in sport [24, 25], during routine assessments undertaken within 24 h of injury. these healthcare professionals were not blinded to phe data.

sample size
we allowed a maximum of one candidate prognostic factor parameter per 10 i-imis, which at the time of protocol development, was the main recommendation to minimise overfitting (additional file 1) [20, 26]. the whole dataset was used for model development and internal validation, which agrees with methodological recommendations [27].

candidate prognostic factors
the available dataset contained 60 candidate factors [20]. because of the sample size considerations, before any analysis, the set of candidate factors was reduced. initially, an audit was conducted to quantify missing values and to determine the measurement reliability of the eligible candidate factors [20]. any candidate factors which had greater than 15% missing data or where reliability was classed as fair to poor (intraclass correlation coefficient < 0.70) were excluded [20] (additional file 2). of the remaining 45 eligible factors, previous evidence of prognostic value [19] and clinical reasoning were used to select candidate prognostic factors suitable for inclusion [20]. this process left a final set of 10 candidate factors, represented by 12 model parameters (table 1). the 35 factors that were not included in model development are also listed in additional file 2, and will be utilised in a related, forthcoming exploratory study which aims to examine their association with indirect muscle injuries in elite football players.

table 1 set of candidate prognostic factors (with corresponding number of parameters) for model development
full size table
statistical analysis
data handling—outcome measures
each participant-season was treated as independent. participants who sustained an i-imi were no longer considered at risk for that season and were included for further analysis at the start of the next season if still eligible. any upper limb imi, trunk imi or non-imi injuries were ignored, and participants were still considered at risk.

eligible participants who were loaned to another club throughout that season, but had not sustained an i-imi prior to the loan, were still considered at risk. i-imis that occurred whilst on loan were included for analysis, as above. permanently transferred participants (who had not sustained an i-imi prior to leaving) were recorded as not having an i-imi during the relevant season and exited the cohort at the season end.

data handling—missing data
missing values were assumed to be missing at random [20]. the continuous parameters generally demonstrated non-normal distributions, so were transformed using normal scores [35] to approximate normality before imputation, and back-transformed following imputation [36]. multivariate normal multiple imputation was performed, using a model that included all candidates and i-imi outcomes. fifty imputed datasets were created in stata 15.1 (statacorp llc, texas, usa) and analysed using the mim module.

prognostic model development
continuous parameters were retained on their original scales, and their effects assumed linear [22]. a full multivariable logistic regression model was constructed, which contained all 12 parameters. parameter estimates were combined across imputed datasets using rubin’s rules [37]. to develop a parsimonious model that would be easier to utilise in practice, backward variable selection was performed using estimates pooled across the imputed datasets at each stage of the selection procedure to successively remove non-significant factors with p values > 0.157. this threshold was selected to approximate equivalence with akaike’s information criterion [38, 39]. multiple parameters representing the same candidate factor were tested together so that the whole factor was either retained or removed. candidate interactions were not examined, and no terms were forced into the model. all analyses were conducted in stata 15.1.

assessment of model performance
the full and parsimonious models were used to predict i-imi risk over a season, for every participant-season in all imputed datasets. for all performance measures, each model’s apparent performance was assessed in each imputed dataset and then averaged across all imputed datasets using rubin’s rules [37]. discrimination determines a model’s ability to differentiate between participants who have experienced an outcome compared to those who have not [40], quantified using the concordance index (c-index). this is equivalent to the area under the receiver operating characteristic (roc) curve for logistic regression, where 1 demonstrates perfect discrimination, whilst 0.5 indicates that discrimination is no better than chance [41].

calibration determines the agreement between the model’s predicted outcome risks and those observed [42], evaluated using an apparent calibration plot in each imputed dataset. all predicted risks were divided into ten groups defined by tenths of predicted risk. the mean predicted risks for the groups were plotted against the observed group outcome proportions with corresponding 95% confidence intervals (cis). a loess smoothing algorithm showed calibration across the range of predicted values [43]. for grouped and smoothed data points, perfect predictions lie on the 45° line (i.e. a slope of 1).

the systematic (mean) error in model predictions was quantified using calibration-in-the-large (citl), which has an ideal value of 0 [40, 42], and the expected/observed (e/o) statistic, which is the ratio of the mean predicted risk against the mean observed risk (ideal value of 1) [40, 42]. the degree of over or underfitting was determined using the calibration slope, where a value of 1 equals perfect calibration on average across the entire range of predicted risks [22]. nagelkerke’s pseudo-r2 was also calculated, which quantifies the overall model fit, with a range of 0 (no variation explained) to 1 (all variation explained) [44].

assessment of clinical utility
decision-curve analysis was used to assess the parsimonious model’s apparent clinical usefulness in terms of net benefit (nb) if used to allocate possible preventative interventions. this assumed that the model’s predicted risks were classed as positive (i.e. may require a preventative intervention) if greater than a chosen risk threshold, and negative otherwise. nb is then the difference between the proportion of true positives and false positives, where both were weighted by the odds of the chosen risk threshold and also divided by the sample size [45]. positive nb values suggest the model is beneficial compared to treating none, which has no benefit to the team but with no negative cost and efficiency implications. the maximum possible nb value is the proportion with the outcome in the dataset.

the model’s nb was also compared to the nb of delivering an intervention to all individuals. this is considered a treat-all strategy, offering maximum benefit to the team, but with maximum negative cost and efficiency implications [17]. a model has potential clinical value if it demonstrates higher nb than the default strategies over the range of risk thresholds which could be considered as high risk in practice [46].

internal validation and adjustment for overfitting
to examine overfitting, the parsimonious model was internally validated using 200 bootstrap samples, drawn from the original dataset with replacement. in each sample, the complete model-building procedure (including multiple imputation, backward variable selection and performance assessment) was conducted as described earlier. the difference in apparent performance (of a bootstrap model in its bootstrap sample) and test performance (of the bootstrap model in the original dataset) was averaged across all samples. this generated optimism estimates for the calibration slope, citl and c-index statistics. these were subtracted from the original apparent calibration slope, citl and c-index statistics to obtain final optimism-adjusted performance estimates. the nagelkerke r2 was adjusted using a relative reduction equivalent to the relative reduction in the calibration slope.

to produce a final model adjusted for overfitting, the regression coefficients produced in the parsimonious model were multiplied by the optimism-adjusted calibration slope (also termed a uniform shrinkage factor), to adjust (or shrink) for overfitting [47]. finally, the citl (also termed model intercept) was then re-estimated to give the final model, suitable for evaluation in other populations or datasets.

complete case and sensitivity analyses
to determine the effect of multiple imputation and player transfer assumptions on model stability, the model development process was repeated: (1) as a complete case analysis and (2) as sensitivity analyses which excluded all participant-seasons where participants had not experienced an i-imi up to the point of loan or transfer, which were performed as both multiple imputation and complete case analyses.

results
participants
during the five seasons, 134 participants were included, contributing 317 participant-seasons and 138 imis in the primary analyses (fig. 1). three players were classified as injured when they took part in phe (which affected three participant-seasons). this meant they were unavailable for full training or to play matches at that time. however, these players had commenced football specific, field-based rehabilitation around this time, so also had similar exposure to training activities as the uninjured players. as such, these players were included in the cohort because it was reasonable to assume that they could also be considered at risk of an i-imi event even during their rehabilitation activities.

fig. 1
figure1
participant flow chart. key: n = participants; i-imi = index indirect muscle injury

full size image
table 2 describes the frequency of included participant-seasons, and the frequency and proportion of recorded i-imi outcomes across all five seasons. for the sensitivity analyses (excluding loans and transfers), 260 independent participant-seasons with 129 imis were included; 36 participants were transferred on loan, whilst 14 participants were permanently transferred during a season, which excluded 57 participant-seasons in total (fig. 1). table 2 also describes the frequency of excluded participant-seasons where players were transferred either permanently or on loan, across the 5 seasons.

table 2 frequency of included participant-seasons, i-imi outcomes and participant-seasons affected by transfers, per season (primary analysis)
full size table
table 3 shows anthropometric and all prognostic factor characteristics for participants included in the primary analyses. these were similar to those included in the sensitivity analyses (additional file 3).

table 3 characteristics of included participants in the primary analysis
full size table
missing data and multiple imputation
all i-imi, age and previous muscle injury data were complete (table 3). for all other candidates, missing data ranged from 6.31 (for hip internal and external rotation difference) to 13.25% for countermovement jump (cmj) power (table 3). the distribution of imputed values approximated observed values (additional file 4), confirming their plausibility.

model development
table 4 shows the parameter estimates for the full model and parsimonious model after variable selection (averaged across imputations).

table 4 results of the full and parsimonious multivariable logistic regression models, with prediction formulae
full size table
for both models, only age and frequency of previous imis had a statistically significant (but modest) association with increased i-imi risk (p < 0.157). no clear evidence for an association was observed for any other candidate factor.

model performance assessment and clinical utility
table 4 shows the apparent performance measures for the full and parsimonious models, all of which were similar. figure 2 shows the apparent calibration of the parsimonious model in the dataset used to develop the model (i.e. before adjustment for overfitting). these were identical across all imputed datasets because the retained prognostic factors contained no missing values. the parsimonious model had perfect apparent overall citl and calibration slope by definition, but calibration was more variable around the 45° line between the expected risk ranges of 28 to 54%. discrimination was similarly modest for the full (c-index = 0.670, 95% ci = 0.609 to 0.731) and parsimonious models (c-index = 0.641, 95% ci = 0.580–0.703). the apparent overall model fit was low for both models, indicated by nagelkerke r2 values of 0.120 for the full model and 0.089 for the parsimonious model.

fig. 2
figure2
apparent calibration of the parsimonious model (before adjustment for overfitting). key: e:o = expected to observed ratio; ci = confidence interval; i-imi = index indirect muscle injury

full size image
figure 3 displays the decision-curve analysis. the nb of the parsimonious model was comparable to the treat-all strategy at risk thresholds up to 31%, marginally greater between 32 and 36% and exceeded the nb of either default strategies between 37 and 71%.

fig. 3
figure3
decision curve analysis for the parsimonious model (before adjustment for overfitting)

full size image
internal validation and adjustment for overfitting
table 4 shows the optimism-adjusted performance statistics for the parsimonious model, with full internal validation results shown in additional file 9. after adjustment for optimism, the overall model fit and the model’s discrimination performance deteriorated (nagelkerke r2 = 0.064; c-index = 0.589 (95% ci = 0.528 to 0.651). furthermore, bootstrapping suggested the model would be severely overfitted in new data (calibration slope = 0.718 (95% ci = 0.275 to 1.161)), so a shrinkage factor of 0.718 was applied to the parsimonious parameter estimates, and the model intercept re-estimated to produce our final model (table 4).

complete case and sensitivity analyses
the full and parsimonious models were robust to complete case analyses and excluding loans and transfers, with comparable apparent performance estimates. for the full models, the c-index range was 0.675 to 0.705, and nagelkerke r2 range was 0.135 to 0.178, whilst for the parsimonious models, the c-index range was 0.632 to 0.691, and nagelkerke r2 range was 0.102 to 0.154 (additional files 5, 6, 7, 8 and 9). the same prognostic factors were selected in all parsimonious models. the degree of estimated overfitting observed in the complete case and sensitivity analyses was comparable to that observed in the main analysis (calibration slope range = 0.678 to 0.715) (additional files 5, 6, 7, 8 and 9).

discussion
we have developed and internally validated a multivariable prognostic model to predict individualised i-imi risk during a season in elite footballers, using routinely, prospectively collected preseason phe and injury data that was available at manchester united football club. this is the only study that we know of that has developed a prognostic model for this purpose, so the results cannot be compared to previous work.

we included both a full model which did not include variable selection and a parsimonious model, which included a subset of variables that were statistically significant. the full model was included because overfitting is likely to increase when variable inclusion decisions are based upon p values. in addition, the use of p value thresholds for variable selection is somewhat arbitrary. however, the overfitting that could have arisen in the parsimonious model after using p values in this way was accounted for during the bootstrapping process, which replicated the variable selection strategy based on p values in each bootstrap sample.

the performance of the full and parsimonious models was similar, which means that utilising all candidate factors offered very little advantage over using two for making predictions. indeed, variable selection eliminated 8 candidate prognostic factors that had no clear evidence for an association with i-imis. our findings confirm previous suggestions that phe tests designed to measure modifiable physical and performance characteristics typically offer poor predictive value [10]. this may be because unless particularly strong associations are observed between a phe test and injury outcome, the overlap in scores between individuals who sustain a future injury and those who do not results in poor discrimination [10]. additionally, after measurement at a single timepoint (i.e. preseason), it is likely that the prognostic value of these modifiable factors may vary over time [48] due to training exposure, environmental adaptations and the occurrence of injuries [49].

the variable selection process resulted in a model which included only age and the frequency of previous imis within the last 3 years, which are simple to measure and routinely available in practice. our findings were similar to the modest association previously observed between age and hamstring imis in elite players [19]. however, whilst a positive previous hamstring imi history has a confirmed association with future hamstring imis [19], we found that for lower extremity i-imis, cumulative imi frequency was preferred to the time proximity of any previous imi as a multivariable prognostic factor. nevertheless, the weak prognostic strength of these factors explains the parsimonious model’s poor discrimination and low potential for clinical utility.

our study is the first to utilise decision-curve analysis to examine the clinical usefulness of a model for identifying players at high risk of imis and who may benefit from preventative interventions such as training load management, strength and conditioning or physiotherapy programmes. our parsimonious model demonstrated no clinical value at risk thresholds of less than 36%, because its nb was comparable to that of providing all players with an intervention. indeed, the only clinically useful thresholds that would indicate a high-risk player would be 37–71%, where the model’s nb was greater than giving all players an intervention. however, because of the high baseline imi risk in our population (approximately 44% of participant-seasons affected), the burden of imis [1,2,3,4,5] and the minimal costs [10] versus the potential benefits of such preventative interventions in an elite club setting, these thresholds are likely to be too high to be acceptable in practice. accordingly, it would be inappropriate to allocate or withhold interventions based upon our model’s predictions.

because of severe overfitting our parsimonious model was optimistic, which means that if used with new players, prediction performance is likely to be worse [39]. although our model was adjusted to account for overfitting and hence improve its calibration performance in new datasets, given the limitations in performance and clinical value, we cannot recommend that it is validated externally or used in clinical practice.

this study has some limitations. we acknowledge that the development of our model does not formally take account of the use of existing injury prevention strategies, including those informed by phe, and their potential effects on the outcome. rather, we predicted i-imis under typical training and match exposure and under routine medical care. in addition, it should be noted that injury risk predictions at an elite level football club may not generalise to other types of football clubs or sporting institutions, where ongoing injury prevention strategies may not be comparable in terms of application and equipment.

we measured candidate factors at one timepoint each season and assumed that participant-seasons were independent. whilst statistically complex, future studies may improve predictive performance and external validity by harnessing longitudinal measurements and incorporating between-season correlations.

we did not perform a competing risks analysis to account for players not being exposed to training and match play due to injuries other than i-imis. that is, our approach predicted the risk of i-imis in the follow up of players, allowing other injury types to occur and therefore possibly limiting the opportunity for i-imis during any rehabilitation period. the competing risk of the occurrence of non-imis was therefore not explicitly modelled and players remained in the risk set after a non-imi had occurred.

we also merged all lower extremity i-imis rather than using specific muscle group outcomes. although less clinically meaningful, this was necessary to maximise statistical power. nevertheless, our limited sample size prohibited examination of complex non-linear associations and only permitted a small number of candidates to be considered. a lack of known prognostic factors [19] meant that selection was mainly guided by data quality control processes and clinical reasoning, so it is possible that important factors were not included.

risk prediction improves when multiple factors with strong prognostic value are used [15]. therefore, future research should aim to identify novel prognostic factors, so that these can be used to develop models with greater potential clinical benefit. this may also allow updating of our model to improve its performance and clinical utility [50].

until the evidence base improves, and because of sample size limitations, it is likely that any further attempts to create a prognostic model at individual club level would suffer similar issues. importantly, this means that for any team, the value of using preseason phe data to make individualised predictions or to select bespoke injury prevention strategies remains to be demonstrated. however, the pooling of individual participant data from several participating clubs may increase sample sizes sufficiently to allow further model development studies [51], where a greater number of candidate factors could be utilised.

conclusion
using phe and injury data available preseason, we have developed and internally validated a prognostic model to predict i-imi risk in players at an elite club, using current methodological best practice. the paucity of known prognostic factors and data requirements for model building severely limited the model’s performance and clinical utility, so it cannot be recommended for external validation or use in practice. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.