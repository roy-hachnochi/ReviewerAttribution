a guide to systematic review and meta-analysis of prognostic factor studies

prognostic factors are associated with the risk of future health outcomes in individuals with a particular health condition or some clinical start point (eg, a particular diagnosis). research to identify genuine prognostic factors is important because these factors can help improve risk stratification, treatment, and lifestyle decisions, and the design of randomised trials. although thousands of prognostic factor studies are published each year, often they are of variable quality and the findings are inconsistent. systematic reviews and meta-analyses are therefore needed that summarise the evidence about the prognostic value of particular factors. in this article, the key steps involved in this review process are described.

systematic reviews and meta-analyses are common in the medical literature, routinely appearing in specialist and general medical journals, and forming the cornerstone of cochrane. the majority of systematic reviews focus on summarising the benefit of one or more therapeutic interventions for a particular condition. however, they are also important for summarising other evidence, such as the accuracy of screening and diagnostic tests,1 the causal association of risk factors for disease onset, and the prognostic ability of bespoke factors and biomarkers. prognostic evidence arises from prognosis studies, which aim to examine and predict future outcomes (such as death, disease progression, side effects or medical complications like pre-eclampsia) in people with a particular health condition or start point (such as those developing a certain disease, undergoing surgery, or women who are pregnant).

the progress (prognosis research strategy) framework defines four types of prognosis research objectives: (a) to summarise overall prognosis (eg, overall risk or rate) of health outcomes for groups with a particular health condition2; (b) to identify prognostic factors associated with changes in health outcomes3; (c) to develop, validate, and examine the impact of prognostic models for individualised prediction of such outcomes4; and (d) to identify predictors of an individual’s response to treatment.5 each objective requires specific methods and tools for conducting a systematic review and meta-analysis. two recent articles provided a guide to undertaking reviews and meta-analysis of prognostic (prediction) models.67 in this article, we focus on prognostic factors.

a prognostic factor is any variable that is associated with the risk of a subsequent health outcome among people with a particular health condition. different values or categories of a prognostic factor are associated with a better or worse prognosis of future health outcomes. for example, in many cancers, tumour grade at the time of histological examination is a prognostic factor because it is associated with time to disease recurrence or death. each grade represents a group of patients with a different prognosis, and the risk or rate (hazard) of the outcome increases with higher grades. many routinely collected patient characteristics are prognostic, such as sex, age, body mass index, smoking status, blood pressure, comorbidities, and symptoms. many researched prognostic factors are biomarkers, which include a diverse range of blood, urine, imaging, electrophysiological, and physiological variables.

prognostic factors have many potential uses, including aiding treatment and lifestyle decisions, improving individual risk prediction, providing novel targets for new treatment, and enhancing the design and analysis of randomised trials.3 this motivates so-called “prognostic factor research” to identify genuine prognostic factors (sometimes also called “predictor finding studies”8).9 although thousands of such studies are published each year, often they are of variable quality and have inconsistent findings. systematic reviews and meta-analyses are therefore urgently needed to summarise the evidence about the prognostic value of particular factors.101112 in this article, we provide a step-by-step guide on conducting such reviews. our aim is to help readers, healthcare providers, and researchers understand the key principles, methods, and challenges of reviews of prognostic factor studies.

summary points
primary studies to identify prognostic factors are abundant, but often findings are inconsistent and quality is variable. systematic reviews and meta-analyses are urgently needed to identify, evaluate, and summarise prognostic factor studies and their findings.
a clear review question should be defined using the picots system (population, index prognostic factor, comparator prognostic factors, outcome, timing, setting), and a transparent search undertaken for eligible articles. broad search strings may be required, leading to a large number of articles to screen.
a data extraction phase is needed to obtain the relevant information from each study. a modification of charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) can be used for prognostic factors (charms-pf).
the quips tool (quality in prognostic factor studies) can be used to examine each study’s risk of bias. unfortunately, many primary studies may have a high risk of bias because of poor design standards, conduct, and analysis. applicability of a study should also be checked.
if appropriate, meta-analysis can be used to combine prognostic effect estimates (such as hazard ratios, risk ratios, or odds ratios) across studies to produce an overall summary of a factor’s prognostic effect. between-study heterogeneity should be expected and accounted for.
ideally separate meta-analyses should be performed for unadjusted and adjusted prognostic effect estimates; adjusted estimates are important to examine a factor’s independent prognostic value over and above (that is, after adjustment for) other prognostic factors.
separate meta-analyses may also be required for each method of measurement (for factors and outcomes), each approach to handling continuous factors, and each type of estimate (such as hazard ratios or odds ratios).
publication bias is a major threat to the validity of meta-analyses of prognostic factor studies based on published evidence, and may cause small-study effects (asymmetry on a funnel plot).
remark (reporting recommendations for tumour marker prognostic studies) and prisma (preferred reporting items for systematic reviews and meta-analyses) can be used to guide the reporting of the systematic review and meta-analysis of prognostic factor studies; the degree of confidence in the summary results from the review may be examined by use of adapted forms of grade (grades of recommendation, assessment, development, and evaluation) for interventions and diagnostic test accuracy studies.
availability of individual participant data from primary prognostic factor studies may alleviate many of the challenges.
step 1: defining the review question
the first step is to define the review question. a review of prognostic factor studies falls within the second objective of the progress framework2 because it aims to summarise the prognostic value of a particular factor (or each of multiple factors) for relevant health outcomes and time points in people with a specific health condition (eg, disease). some reviews are broad; for example, riley and colleagues aimed to identify any prognostic factor for overall and disease free survival in children with neuroblastoma or ewing’s sarcoma.13 other reviews have a narrower focus; for example, hemingway and colleagues aimed to summarise the evidence on whether c reactive protein (crp) is a prognostic factor for fatal and non-fatal events in patients with stable coronary disease.14 this crp review is used as an example throughout this article.

charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) provides guidance for formulating a review question (table 1 in the article by moons and colleagues15). although charms was developed15 and refined6 for reviews of prediction model studies, it can also be used to define and frame the question for reviews of prognostic factor studies. charms15 and subsequent improvements6 propose a modification of the traditional pico system (population, index intervention, comparison, and outcome) used in systematic reviews of therapeutic intervention studies. the modification is called picots, because it also considers timing and setting (box 1). in the context of prognostic factor reviews, the “p” of population and the “o” of outcome remain largely the same as in the original pico system, but the “i” refers to index prognostic factors and the “c” refers to other prognostic factors that can be considered as comparators in some way. for example, the aim may be to compare the prognostic ability of a certain index factor with one or more other (that is, comparator) prognostic factors; or to investigate the adjusted prognostic value of a particular index factor over and above (adjusted for) other (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, which is not generally recommended, then no comparator factor is being considered. the “t” denotes timing and refers to two concepts of time. firstly, at what time point the prognostic factors under review are to be measured or assessed (that is, the time point at which prognosis information is required); and secondly, over what time period the outcomes are predicted by these factors. the “s” of setting refers to the setting or context in which the index prognostic factors are to be used because the prognostic ability of a factor may change across healthcare settings.

table 1 charms-pf checklist of key items to be extracted from primary studies of prognostic factors, based on additions and modifications of the original charms checklist for primary studies of prediction models15
view popupview inline
box 1
six items (picots) defining the question for systematic reviews of prognostic factor studies, based on charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies)615 and applied to a review of the adjusted prognostic value of c reactive protein (crp)14
population: define the target population for which prognostic factors under review are to be used. for example, crp review: patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of acute coronary syndrome at least two weeks before prognostic factor (crp) measurement.
index prognostic factor: define the factors for which prognostic value is under review. for example, crp review: crp was the single biomarker reviewed for its prognostic value.
comparator prognostic factors: comparator prognostic factors can be considered in a review in various ways. for example, the aim could be to compare the prognostic ability of a certain index factor with two or more other (that is, comparator) prognostic factors; or to review the adjusted prognostic value of a particular index factor—that is, over and above (adjusted for, independent of) other existing (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, then no comparator factor is being considered. for example, crp review: the focus was on the adjusted prognostic value of crp—that is, its prognostic effect after adjusting for existing (comparator) prognostic factors. in particular, adjustment for the following conventional prognostic factors was of interest: age, sex, smoking status, obesity, diabetes, and one or more lipid variables (from total cholesterol, low density lipoprotein cholesterol, high density lipoprotein cholesterol, triglycerides) and inflammatory markers (fibrinogen, interleukin 6, white cell count).
outcome: define the outcomes for which the prognostic ability of the factor(s) under review are of interest. for example, crp review: outcome events were defined as coronary (coronary death, sudden cardiac death, acute non-fatal myocardial infarction, primary percutaneous coronary intervention, unplanned emergency admissions with unstable angina), cardiovascular (when coronary events were reported in combination with heart failure, stroke, or peripheral arterial disease), and all cause mortality.
timing: define firstly at what time points the prognostic factors (index and comparators) are to be used (that is, the time point of prognostication), and secondly over what time period the outcomes are predicted by these factors. for example, crp review: the crp measurement had to be done at least two weeks after diagnosis and all follow-up information on the outcomes (all time periods) was extracted from the studies.
setting: define the intended setting and role of the prognostic factors under review. for example, crp review: crp measurement was studied in primary and secondary care to provide prognostic information about patients diagnosed with coronary heart disease; this information may be useful for healthcare professionals treating and managing such patients.
return to text
an important component of reviews of prognostic factors is whether unadjusted or adjusted estimates of the index prognostic factors will be summarised, or both. we recommend that reviewers primarily focus on adjusted prognostic factor estimates because they reveal whether a certain index factor contributes independently to the prediction of the outcome over and above (that is, after adjustment for) other prognostic factors. in particular, for each clinical scenario there are often so-called “established” or “conventional” prognostic factors that are always measured. therefore, for prognostic factors under review, it is important to understand whether they contribute additional (sometimes called “independent”) prognostic information to the routinely measured ones. this means that reviewers need adjusted (and not unadjusted or crude) prognostic effect estimates to be estimated and reported in primary prognostic factor studies. such adjusted prognostic estimates are typically derived from a multivariable regression model containing the established prognostic factors plus each index prognostic factor of interest.

for example, consider a logistic regression of a binary outcome including three adjustment factors (a1, a2, and a3) and one new index prognostic factor (x1), which is expressed as:

ln(p/(1−p)) = α+β1a1+β2a2+β3a3+β4x1
here, “p” is the probability of the outcome. after estimation of all the unknown parameters (that is, α, β1, β2, β3, β4), of key interest is the estimated β4. this parameter provides the adjusted prognostic effect of the index prognostic factor and reveals its independent contribution to the prediction of the outcome over and above the prognostic effects of the other (established comparator) factors a1, a2, and a3 combined.

the need to focus on adjusted prognostic effects is no different from (systematic reviews of) aetiological studies, in which the focus is on estimating the association of a certain causal risk factor after adjustment for other risk factors. in such causal research, these factors are usually referred to as “confounders” rather than as “other prognostic factors,” which is the term typically used for prognosis research. the crude (unadjusted) prognostic effect of some index factors may completely disappear after adjustment and is therefore rather uninformative, especially because prognostication in healthcare is rarely based on a single prognostic factor but rather on the information from multiple prognostic factors.4

this article focuses on systematic reviews to summarise prognostic factor effect estimates. some primary studies may also evaluate an index factor’s added value in terms of improvement in risk classification and clinical use (eg, measures such as net reclassification improvement and net benefit), and change in prediction model performance (eg, by calculating the change in the concordance index, also known as the c statistic or area under the receiver operating characteristics (roc) curve).17181920 however, this is beyond the scope of this article, and we refer the reader to other relevant sources.62122

application to crp review
crp is widely studied for its prognostic value in patients with coronary disease. however, there is uncertainty whether crp is useful because us and european clinical practice guidelines recommend measurement but clinical practice varies widely. this uncertainty motivated the systematic review by hemingway and colleagues,14 with the corresponding picots system presented in box 1. no studies were excluded on the basis of methodological standards, sample size, duration of follow-up, publication year, or language of publication.

step 2: searching for and selection of eligible studies
the next step is to identify primary studies that are eligible for review; studies that address the review question defined in step 1 following the picots framework. unfortunately, it is more difficult to identify prognostic factor studies than randomised trials of interventions. prognosis studies do not tend to be indexed (“tagged”) because a taxonomy of prognosis research is not widely recognised. moreover, compared with studies of interventions, there is much more variation in the design of prognostic factor studies (eg, data from cohort studies, randomised trials, routine care registries, and case-control studies can all be used), patient inclusion criteria, prognostic factor and outcome measurement, follow-up time, methods of statistical analysis, and adjustment of (and number of) other prognostic factors (covariates). between-study heterogeneity is therefore the rule rather than the exception in prognostic factor research. it is essential that systematic reviews of prognostic factor studies define the study inclusion and exclusion criteria based on the picots structure (step 1) because this determines the study search and selection strategy.

typically, broad search and selection filters are required that combine terms related to prognosis research (such as prognostic, predict, predictor, factor, independent) with domain or disease specific terms (such as the name of prognostic factors and the targeted disease or patient population).23 a broad search comes at the (often considerable) expense of retrieving many irrelevant records. geersing and colleagues24 validated various existing search strategies for prognosis studies and suggested a generic filter for identifying studies of prognostic factors,232526 which extended the work of ingui, haynes, and wong.232526 when tested in a single review of prognostic factors, this generic filter had a number needed to read of 569 to identify one relevant article, emphasising the difficulty in targeting prognostic factor articles. the number needed to read could be considerably reduced when specific factors or populations are added to the filter. even then, care is needed to be inclusive because multiple terms are often used for the same meaning; for example, biomarker mycn is also referred to as n-myc and nmyc, among other terms.13

once the search is complete, each potentially relevant study must be screened for its applicability to the review question. because of the heterogeneity in prognostic factor studies, during this study selection phase more deviations from the defined picots (in step 1) are possible (far greater than what is typically encountered during the selection of randomised intervention studies). the applicability of this primary study selection should firstly be based on title and abstract screening, followed by full text screening, both ideally done by two researchers independently. any discrepancies should be resolved through discussion, potentially with a third reviewer. to check if any relevant articles have been missed, it is helpful to share the list of identified articles with researchers in the field to examine the reference lists of these articles and to perform a citation search.

application to crp review
hemingway and colleagues included any prospective observational study that reported risk of subsequent events among patients with stable coronary disease in relation to measured crp values.14 eligible studies had to include patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of previous acute coronary syndrome at least 2 weeks before crp measurement. hemingway and colleagues searched medline between 1966 and 25 november 2009 and embase between 1980 and 17 december 2009, using a search string containing terms for coronary disease, prognostic studies, and crp. the search identified 1566 articles, of which 83 fulfilled the inclusion criteria. if specific terms for crp had not been included in the search string, then the total number of identified articles would have far exceeded 1566.

step 3: data extraction
the next step is to extract key information from each selected study. data extraction provides the necessary data from each study, which enables reviewers to examine their (eventual) applicability to the review question and their risk of bias (see step 4). this step also provides the information required for subsequent qualitative and quantitative (meta-analysis) synthesis of the evidence across studies. the charms checklist gives explicit guidance (table 2 in the article by moons and colleagues15) about which key items across 11 domains should be extracted from primary studies of prediction models, and for what reason (that is, to provide general information about the primary study, to guide risk of bias assessment, or to assess applicability of the primary study to the review question). based on our experience of conducting systematic reviews of prognostic factor studies, we modified the original charms checklist for prediction model studies to make it suitable for data extraction in reviews of prognostic factors (here referred to as charms-pf; table 1). this basically means that three domains typically addressing multivariable prediction modelling aspects were combined to one overall analysis domain, while other domain names and key items were slightly reworded or extended. reasons for extraction of each key item are similar to charms for prediction models. because we developed the original charms checklist, a wider consensus of the charms-pf content was not considered necessary.

table 2 quips tool (quality in prognostic factor studies), which can be used to classify risk of bias of prognostic factor studies
view popupview inline
reviewers should extract fundamental information from the primary prognostic factor studies, such as the dates, setting, study design, definitions of start points, outcomes, follow-up length, and prognostic factors; reviewers will often find large heterogeneity in this information across studies. the extracted information can be summarised in tables of study characteristics. in addition, more specific information is needed to properly assess applicability and risk of bias (see step 4), such as methods used to measure prognostic factors and outcomes, handling missing data, attrition (loss to follow-up), and whether estimated associations of the prognostic factors under review were adjusted for other prognostic factors. this information also enhances the potential for meta-analysis and the presentation and interpretation of subsequent summary results (see steps 5-8).

to enable meta-analysis of prognostic factor studies, the key elements to extract are estimates, and corresponding standard errors or confidence intervals, of the prognostic effect for each factor of interest; for example, the estimated risk ratio or odds ratio (for binary outcomes), hazard ratio (for time-to-event outcomes), or mean difference (for continuous outcomes). as most prognostic factor studies consider time-to-event outcomes (including censored observations and different follow-up lengths for patients), hazard ratios are often the most suitable effect measure. a concern is that hazard ratios may not be constant over time, and therefore any evaluations of non-proportional hazards (that is, non-constant hazard ratios for the prognostic factors of interest) should also be extracted; however, such information is rarely reported in sufficient detail.

unfortunately, many prognostic factor studies do not adequately report estimated prognostic effect measures or their precision. for this reason, methods are available to restore the missing information upon data extraction. in particular, parmar and colleagues28 and tierney and colleagues29 describe how to obtain unadjusted hazard ratio estimates (and their variances) when they are not reported directly. for example, under assumptions, the number of outcomes (events) and an available p value (eg, from a log rank test or cox regression) can be used to indirectly estimate the unadjusted hazard ratio between two groups defined by a particular factor (eg, “high” versus “normal” levels). perneger and colleagues30 report how to derive unadjusted hazard ratios from survival proportions, and pérez and colleagues suggest using a simulation approach.31 even with such indirect estimation methods, not all results can be obtained. for example, in a systematic review of 575 studies investigating prognostic factors in neuroblastoma,32 the methods of parmar and colleagues were used to obtain 204 hazard ratio estimates and their confidence intervals; but this represented only 35.5% of the potential evidence.

although indirect estimation methods help retrieve unadjusted prognostic factor effect estimates, they often have limited value for obtaining adjusted effect estimates. furthermore, even when multiple studies provide the adjusted prognostic effect of a particular factor, the set of adjustment factors will usually differ across studies, which complicates the interpretation of subsequent meta-analysis results. we recommend that reviewers predefine the core set of prognostic factors for the outcome of interest (eg, age, sex, smoking status, disease stage) that represents the desired “minimal” set of adjustment factors. an agreed process among health professionals and researchers in the field could be required to define this set. for example, a list of established prognostic factors could be identified that are routinely used within current prognostication of the clinical population of interest.

it may also be necessary to standardise the extracted estimates to ensure they all relate to the same scale and direction in each study. in particular, the direction of the prognostic effect will need standardising if one study compares the hazard rate in a factor’s “high” versus “normal” group, whereas another study compares the hazard rate in the factor’s “normal” versus “high” group. when the outcome is defined differently across studies, approaches to convert effect measures on different outcome scales could be useful.33 also, to deal with different cutpoint levels for values of a particular factor,34 the prognostic effects of “high” versus “normal” could be converted to prognostic effects relating to a 1 unit increase in the factor. this requires assumptions about the underlying distribution of the factor. such an approach was used by hemingway and colleagues.14 of concern, however, is that the actual distribution of a prognostic factor may be unknown (or even vary across studies). finally, it is also possible to derive standardised effect estimates by standardising the corresponding regression coefficients.35

application to crp review
hemingway and colleagues extracted background information such as year of study start, number of included patients, mean age, baseline coronary morbidity (eg, proportion with stable angina), average levels of biomarker at baseline, method of crp measurement, follow-up duration, and number and type of events. basic information was often missing. for example, nearly a fifth of studies did not report the method of measurement, and only a quarter gave the number of patients included in the analyses and reasons for dropout. prognostic effect estimates for crp were extracted in terms of the reported risk ratio, odds ratio, or hazard ratio (labelled generally as “risk ratio” in this article), and 95% confidence intervals. these effect estimates were then converted to a standardised scale comparing the highest third with the lowest third of the (log transformed) crp distribution. if available, separate prognostic effect estimates were extracted for different degrees of adjustment for other prognostic factors.

step 4: evaluating applicability and risk of bias of primary studies
once eligible studies are identified and data are extracted, an important next step is to assess the applicability and risk of bias (quality) of each study in the review. as for steps 2 and 3, ideally this is done by two reviewers, independently, with any discrepancies resolved. applicability refers to the extent to which a selected study (in step 2) matches the review question in terms of the population, timing, prognostic factors, and outcomes (endpoints) of interest. just because a study is eligible for inclusion does not mean it is free from applicability concerns. some aspects of a study may be applicable (eg, correct condition at start point, with prognostic factors of interest evaluated) but not others (eg, incorrect population or setting, inappropriate outcome definition, different follow-up time, lack of adjustment for conventional prognostic factors). applicability is typically first assessed during title and abstract screening, and then during this step, so that it is based on full text screening and determined by picots (step 1) and inclusion and exclusion criteria of studies (step 2).

risk of bias refers to the extent to which flaws in the study design or analysis methods could lead to bias in estimates of the prognostic factor effects. unfortunately, based on growing empirical evidence from systematic reviews examining methodology quality, many primary studies will be at high risk of bias.832363738394041424344 for prognostic factor studies, hayden and colleagues developed the quips checklist (quality in prognostic factor studies) for examining risk of bias across six domains27: study participation, study attrition, prognostic factor measurement, outcome measurement, adjustment for other prognostic factors, and statistical analysis and reporting. table 2 shows the signalling items within these domains to help guide reviewers in making low, unclear, or high risk of bias classifications. additional guidance may be found in general tools examining the quality of observational studies,4546 and the remark guideline (reporting recommendations for tumour marker prognostic studies) for reporting of primary prognostic factor studies.4748

we recommend that users first operationalise criteria to assess the signalling items and domains for the specific review question. for example, with the study participation and attrition domains, this includes defining a priori the most important characteristics that could indicate a systematic bias in study recruitment (study participation domain) and loss to follow-up (study attrition domain). defining these characteristics ahead of time will facilitate assessment and consensus related to the importance of potential differences that could influence the observed association between the index prognostic factors and outcomes of interest. definitions of sufficiently valid and reliable measurement of the index prognostic factors and outcomes should also be specified at the protocol stage. similarly, the core set of other (adjustment) prognostic factors that are deemed necessary for the primary studies to have adjusted for, should be predefined to facilitate judgment related to risk of bias in domain 5.

overall assessment of the six risk of bias domains is undertaken by considering the risk of bias information from the signalling items for each domain, rated as low, moderate, and high risk of bias. occasionally, item information needed to assess the bias domains is not available in the study report. when this occurs, other publications that may have used the same dataset (which often occurs in prognostic studies based on large existing cohorts) should be consulted and study authors should be contacted for additional information. an informed judgment about the potential risk of bias for each bias domain should be made independently by two reviewers, and discussed to reach consensus. each of the six domains needs to be rated and reported separately because this will inform readers, flag improvements needed for subsequent primary studies, and facilitate future meta-epidemiological research. we recommend defining studies with an overall “low risk of bias” as those studies where all, or the most important domains (as determined a priori), are rated as having low (or low to moderate) risk of bias.

application to crp review
hemingway and colleagues assessed the quality of included studies by the quality of their reporting on 17 items derived from the remark guideline.48 the median number of study quality items reported was seven of a possible 17, and standards did not change between 1997 and 2009. only two studies referred to a study protocol, with none referring to a statistical analysis plan. hemingway and colleagues noted that this “makes it difficult to know what the specific research objectives were at the start of cohort recruitment, at the time of crp measurement, or at the onset of the statistical analysis.”14 only two studies reported the time elapsed between first lifetime presentation with coronary disease and assessment of crp and this raised applicability concerns.

step 5: meta-analysis
meta-analysis of prognostic factor studies aims to summarise the (adjusted) prognostic effect of each factor of interest. in addition to missing estimates, challenges for the meta-analyst include (a) having different types of prognostic effect measures (eg, odds ratios and hazard ratios), which are not necessarily comparable30; (b) estimates without standard errors, which is a problem because meta-analysis methods typically weight each study by (a function of) their standard error; (c) estimates relating to various time points of the outcome occurrence or measurement; (d) different methods of measurement for prognostic factors and outcomes; (e) various sets of adjustment factors; and (f) different approaches to handling continuous prognostic factors (eg, categorisation, linear, non-linear trends), including the choice of cutpoint value when dichotomising continuous values into “low” and “normal” groups. many of these issues lead to substantial heterogeneity and if a meta-analysis is performed, summary results cannot be directly interpreted.

generally, meta-analysis results will be most interpretable, and therefore useful, when a separate meta-analysis is undertaken for groups of “similar” prognostic effect measures. in particular, we suggest considering a meta-analysis for:

hazard ratios, odds ratios, and risk ratios separately
unadjusted and adjusted associations separately
prognostic factor effects at distinct cutpoints (or groups of similar cutpoints) separately
prognostic factor effects corresponding to a linear trend (association) separately
prognostic factor effects corresponding to non-linear trends separately
each method of measurement (for factors and outcomes) separately.
ideally a meta-analysis of adjusted results should ensure that all included estimates are adjusted for the same set of other prognostic factors. this situation is unlikely and so a compromise could be to ensure that all adjusted estimates in the same meta-analysis have adjusted for at least a (predefined) minimum set of adjustment factors (that is, a core set of established prognostic factors).

even when adhering to this guidance, unexplained heterogeneity is likely to remain because of other reasons (eg, differences in length of follow-up or in treatments received during follow-up). therefore, if a meta-analysis is performed, a random effects approach is essential to allow for unexplained heterogeneity across studies (box 2), as previously described in the bmj.53 this approach provides a summary estimate of the average prognostic effect of the index factor and the variability in effect across studies. also potentially useful are meta-analysis methods to estimate the trend (eg, linear effect) of a prognostic factor that has been grouped into three or more categories within studies (with each category compared with the reference category). these methods generally model the estimated prognostic effect sizes in each category as a function of “exposure” level (eg, midpoint or median prognostic factor value in the category) and account for within-study correlation and between-study heterogeneity.5455565758 to apply these methods, some additional knowledge of the factor’s underlying distribution is usually needed to help define the “exposure” level because the chosen value can have an impact on the results.56

box 2
explanation of a random effects meta-analysis of prognostic factor effect estimates
the true prognostic effect of a factor is likely to vary from study to study; therefore assuming a common (fixed) prognostic effect is not sensible. if yi and var(yi) denote the prognostic effect estimate (eg, ln(hazard ratio), ln(odds ratio), ln(risk ratio), or mean difference) and its variance in study i, then a general random effects meta-analysis model can be specified as:

yi ~n(μ,var(yi)+τ2).

most researchers use either restricted maximum likelihood or the approach of dersimonian and laird to estimate this model,49 but other options are available, including a bayesian approach.50 of key interest is the estimate of μ, which reveals the summary (average) prognostic effect of the index prognostic factor of interest. the standard deviation of this prognostic factor effect across studies is denoted by τ, and non-zero values suggest there is between-study heterogeneity. confidence intervals for µ should ideally account for uncertainty in estimated variances (in particular τ),51 and we have found the approach of hartung-knapp to be robust for this purpose in most settings.1652 when synthesising prognostic effects on the log scale, the summary results and confidence intervals require back transformation (using the exponential function) to the original scale.

return to text
advanced multivariate meta-analysis methods are also available to handle multiple cutpoints,59 multiple methods of measurement,59 or different adjustment factors in prognostic factor studies.60 an introduction to multivariate meta-analysis has been published in the bmj.61

application to crp review
hemingway and colleagues14 applied a random effects meta-analysis to combine 53 adjusted prognostic effect estimates for crp from studies that adjusted for at least one of six conventional risk factors (age, sex, smoking status, diabetes, obesity, and lipids). the summary meta-analysis result was a risk ratio of 1.97 (95% confidence interval 1.78 to 2.17), which gives the average prognostic effect of crp (for those in the top v bottom third of crp distribution), and suggests larger crp values are associated with higher risk. although there was substantial between-study heterogeneity, nearly all estimates were in the same direction (that is, risk ratio >1). when restricting meta-analysis to just the 13 studies that adjusted for at least all six conventional prognostic factors, the summary risk ratio decreased to 1.65 (95% confidence interval 1.39 to 1.96), and the between-study heterogeneity reduced. using the study specific estimates given by hemingway and colleagues, we updated this meta-analysis (fig 1), obtaining the same summary result but a wider confidence interval (1.34 to 2.04) through the hartung-knapp approach.16

fig 1
fig 1
forest plot showing the study specific estimates and meta-analysis summary result of the adjusted prognostic effect (risk ratio) of c reactive protein taken from the review of hemingway and colleagues14; all studies were adjusted for a core set of existing prognostic factors (age, sex, smoking status, diabetes, obesity, and lipids), plus up to 14 other prognostic factors. meta-analysis results shown are based on a random effects meta-analysis model with dersimonian and laird estimation of the between-study variances. the summary result is identical to hemingway and colleagues,14 but the confidence interval is wider because we used the hartung-knapp approach to account for uncertainty in variance estimates.16although “risk ratio” is used, the estimates actually correspond to a mixture of risk ratios, odds ratios, and hazard ratios

download figure open in new tab download powerpoint
step 6: quantifying and examining heterogeneity
for all meta-analyses, when there is large heterogeneity across included studies, it may be better not to synthesise the study results, but rather display the variability in estimates on a forest plot without showing an overall pooled estimate. when a meta-analysis is performed in the face of heterogeneity, it is important to quantify and report the magnitude of heterogeneity itself; for example, through the estimate of (the between-study variance),62 or an approximate 95% prediction interval indicating the potential true prognostic effect of a factor in a new population.5363

subgroup analyses and meta-regression can be used to examine or explore the causes of heterogeneity. a subgroup analysis performs a separate meta-analysis for categories defined by a particular characteristic, such as those with a low risk of bias, those with a follow-up of less than one year or of at least one year, or those set in countries in europe. a better approach is meta-regression, which extends the meta-analysis equation shown in box 2 by including study level covariates,64 and allows a formal comparison of meta-analysis results across groups defined by covariates (eg, low risk of bias studies v studies at higher risk of bias). unfortunately, subgroup analyses and meta-regression are often problematic. there will often be few studies per subgroup and low power to detect genuine causes of heterogeneity. furthermore, study level confounding will be rife so that it is difficult to disentangle the associations for one covariate from another. for example, studies with a low risk of bias may also have a different length of follow-up or a particular cutpoint level compared with studies at higher risk of bias.

application to crp review
hemingway and colleagues reported that meta-regression identified four study level covariates that explained some between-study heterogeneity in the prognostic effect of crp: definition of comparison group, number of adjustment factors, the (log) number of events, and the proportion of patients with stable coronary disease (reflecting study size).14 studies originally reporting unequal crp groups had stronger effects than those reporting crp on a continuous scale. for each additional adjustment factor, the summary risk ratio decreased by 3%. the summary risk ratio was smaller among studies with more than the median number of outcome events, and smaller among studies confined to stable coronary disease. there was no evidence that the crp effect differed according to the number of quality items reported by a study, or by the type of prognostic effect measure provided (that is, risk ratio, odds ratio, or hazard ratio).

step 7: examining small-study effects
the term “small-study effects” refers to when there is a systematic difference in prognostic effect estimates for small studies and large studies.65 a particular concern is when small studies (especially those that are exploratory because these often evaluate many potential prognostic factors with relatively few outcome events) show larger prognostic effects than larger studies. this difference may be due to chance or heterogeneity, but a major threat here is publication bias and selective reporting, which are endemic in prognosis research.363738 such reporting biases lead to smaller studies, with (statistically) significant or larger prognostic factor effect estimates being more likely to be published or reported in sufficient detail, and thus included in a meta-analysis, than smaller studies with non-significant or smaller prognostic effect estimates. this bias is a potential concern for unadjusted and adjusted prognostic effects. a primary study usually estimates an unadjusted prognostic effect for each of multiple prognostic factors, but study authors may only report effects that are statistically significant. in addition, adjusted results are often only reported for prognostic factors that retain statistical significance in univariable and multivariable analysis. a consequence is that meta-analysis results will be biased, with larger summary prognostic effects than in reality, and potentially some factors being deemed to have clinical value when actually they do not.

the evidence for small-study effects is usually considered on a funnel plot, which shows the study estimates (x axis) against their precision (y axis). a funnel plot is usually recommended if there are 10 or more studies.65 the plot should ideally show a symmetric, funnel like shape, with results from larger studies at the centre of the funnel and smaller studies spanning out in both directions equally. asymmetry will arise if there are small-study effects, with a greater proportion of smaller studies in one particular direction. statistical tests for asymmetry in risk, odds and hazard ratios can be used, such as peter’s and debray’s test.6667 contour enhanced funnel plots also show the statistical significance of individual studies, and “missing” studies are perhaps more likely to fall within regions of non-significance if publication bias was the cause of small-study effects. an example is shown in figure 2.

fig 2
fig 2
evidence of funnel plot asymmetry (small-study effects) in the c reactive protein meta-analysis shown in figure 1. the smaller studies (with higher standard errors) have risk ratio (rr) estimates mainly to the right of the larger studies, and therefore give the largest prognostic effect estimates. a concern is that this is due to publication bias, with “missing” studies potentially falling to the left side of the larger studies and in the lighter shaded regions denoting non-significant rr estimates

download figure open in new tab download powerpoint
as mentioned, small-study effects may also arise due to heterogeneity. therefore, it is difficult to disentangle publication bias from heterogeneity in a single review. for example, if smaller studies used an analysis with fewer adjustment factors, then this may cause larger prognostic factor effects in such studies, rather than it being caused by publication bias. a multivariate meta-analysis could reduce the impact of small-study effects by “borrowing strength” from related information.61

a related concern is that smaller prognostic factor studies are generally at higher risk of bias than larger studies. smaller studies tend to be more exploratory in nature and typically based on a convenient sample, often examining many (sometimes hundreds of) potential prognostic factors, with relatively few outcome events. this design leads to spurious (due to chance) and potentially biased (due to poor estimation properties68) prognostic effect estimates, which are more prone to selective reporting. in contrast, larger studies are often confirmatory studies focusing on one or a few prognostic factors, and are more likely to adopt a protocol driven and prospective approach, with clearer reporting regardless of their findings.3 therefore, larger studies are less likely to identify spurious prognostic factor effect estimates. it is helpful to examine small-study effects (potential publication bias) when restricting analysis to the subset of studies at low risk of bias. if this approach resolves previous issues of small-study effects in the full meta-analysis, then it gives even more credence to focus conclusions and recommendations on the meta-analysis results based only on the higher quality studies.

application to crp review
figure 2 shows a funnel plot of the study estimates from the crp meta-analysis shown in figure 1. there is clear asymmetry, which shows the strong potential for publication bias. there was an insufficient number of studies considered at low risk of bias to evaluate small-study effects in a subset of higher quality studies.

step 8: reporting and interpretation of results
as with all research studies, clear and complete reporting is essential for reviews of prognostic factor studies. most of the reporting guidelines of prisma (preferred reporting items for systemic reviews and meta-analyses) and moose (meta-analysis of observational studies in epidemiology) will be relevant,6970 and should be complemented by remark,4748 which was aimed at primary prognostic factor studies. more specific guidance for reporting systematic reviews of prognostic factor studies is under development.

interpretation and translation of summary meta-analysis results is an important final step. the guidance in the previous steps is the essential input for this step. discussion is necessary on whether and how the prognostic factors identified may be useful in practice (that is, translation of results to clinical practice), and what further research is necessary. ideally impact studies (eg, randomised trials that compare groups which do and do not use a prognostic factor to inform clinical practice) are needed before strong recommendations for clinical practice are made; however, these studies are rare and outside the scope of the review framework outlined in this article.

to interpret the certainty (confidence) of the summary results of a review of intervention effectiveness, grade (grades of recommendation, assessment, development, and evaluation) was developed. this approach assesses the overall quality of and certainty in evidence for the summary estimates of the intervention effects by addressing five domains: risk of bias, inconsistency, imprecision, indirectness, and publication bias. the grade domains can be assessed using the information obtained by the tools and methods described in the above steps. however, it is not known whether these domains, developed for reviews of interventions, are equally applicable to assessing the certainty of summary results of systematic reviews of prognostic factor studies. compared with reviews of intervention studies, allowing for heterogeneity (the inconsistency domain) might be more acceptable in reviews of prognostic factor studies because of the inevitable heterogeneity caused by study differences in methods of measurement, adjustment factors, and statistical analysis methods, among others. furthermore, the threat of selective reporting or publication bias in reviews of prognostic factor studies may be more severe than in reviews of intervention studies because of the problems of exploratory studies, poor reporting, and biased analysis methods.

there is limited empirical evidence for using the existing domains to grade the certainty of summary estimates of prognostic factor studies, although a first attempt has been made71; in addition, an assessment has been performed on grading the certainty of evidence of summary estimates of overall prognosis studies.72 reviewers need to be especially cautious when comparing the adjusted prognostic value of multiple index factors, for example, to conclude whether the summary adjusted hazard ratio for prognostic factor a is larger than that for factor b. usually different sets of studies will be available for each index factor, and so the comparison will be indirect and potentially biased. moreover, the studies evaluating factor a may often have used different sets of adjustment factors (other prognostic factors) than those evaluating factor b. it will be rare to find studies on different index factors that used exactly the same set of adjustment factors. we therefore recommend reviewers restrict comparisons (of the adjusted prognostic value) of two or more index factors to those studies that at least used a similar, minimally required set of adjustment factors.73 even then, due to different scales and distributions of each factor (eg, continuous or binary), a simple comparison of the prognostic effect sizes (eg, hazard ratio for factor a v hazard ratio for factor b) may not be straightforward.

application to crp review
the meta-analysis results suggest crp is a prognostic factor for the risk of death and non-fatal cardiovascular events, even when only including the largest studies that adjusted for all six conventional prognostic factors. in their discussion, hemingway and colleagues downgraded the meta-analysis findings because of a strong concern about the quality and reliability of the underlying evidence.14 the absence of prespecified protocols, poor and potentially biased reporting, and strong potential for publication bias prevented the authors from making firm conclusions about whether crp has prognostic value after adjustment for established prognostic factors. they state that the concerns “explicitly challenge the statement for healthcare professionals made by the centers for disease control that measuring crp is both ‘useful’ and ‘independent’ as a marker of prognosis.”74

summary
in this article, we described the key steps and methods for conducting a systematic review and meta-analysis of prognostic factor studies. current reviews are often limited by the quality and heterogeneity of primary studies.7576 we expect the prevalence of such reviews to grow rapidly, especially as cochrane has recently embarked on prognosis reviews (see also the cochrane prognosis methods group website www.methods.cochrane.org/prognosis).77 our guidance will help researchers to write grant applications for reviews of prognostic factor studies, and to develop protocols and conduct such reviews. protocols of prognostic factor reviews should be published ideally at the same time as the review is registered, for example within prospero, the international prospective register of systematic reviews (www.crd.york.ac.uk/prospero/), or the cochrane database.77 our guidance will also allow readers and healthcare providers to better judge reports of prognostic factor reviews.

finally, we note that some of the limitations described (eg, use of different cutpoint values across studies) could be alleviated if the individual participant data were obtained from primary prognostic factor studies78 rather than being extracted from study publications; although, this may not solve all problems (eg, quality of original study, availability of different adjustment factors).79 further discussion on individual participant data meta-analysis of prognostic factor studies is given elsewhere.80

<|EndOfText|>

individual participant data meta-analysis to examine
interactions between treatment effect and participant-level
covariates: statistical recommendations for conduct
and planning

precision medicine research often searches for treatment-covariate interactions,
which refers to when a treatment effect (eg, measured as a mean difference,
odds ratio, hazard ratio) changes across values of a participant-level covariate
(eg, age, gender, biomarker). single trials do not usually have sufficient power
to detect genuine treatment-covariate interactions, which motivate the sharing
of individual participant data (ipd) from multiple trials for meta-analysis. here,
we provide statistical recommendations for conducting and planning an ipd
meta-analysis of randomized trials to examine treatment-covariate interactions.
for conduct, two-stage and one-stage statistical models are described, and we
recommend: (i) interactions should be estimated directly, and not by calculating differences in meta-analysis results for subgroups; (ii) interaction estimates
should be based solely on within-study information; (iii) continuous covariates and outcomes should be analyzed on their continuous scale; (iv) nonlinear
relationships should be examined for continuous covariates, using a multivariate meta-analysis of the trend (eg, using restricted cubic spline functions);
and (v) translation of interactions into clinical practice is nontrivial, requiring
individualized treatment effect prediction. for planning, we describe first why
the decision to initiate an ipd meta-analysis project should not be based on
between-study heterogeneity in the overall treatment effect; and second, how to
calculate the power of a potential ipd meta-analysis project in advance of ipd
collection, conditional on characteristics (eg, number of participants, standard
deviation of covariates) of the trials (potentially) promising their ipd. real ipd
meta-analysis projects are used for illustration throughout.
keywords
effect modifier, individual participant data (ipd), meta-analysis, subgroup effect,
treatment-covariate interaction
this is an open access article under the terms of the creative commons attribution license, which permits use, distribution and reproduction in any medium, provided the
original work is properly cited.

1 introduction
precision medicine and stratified healthcare tailors treatment decisions to individuals based on their particular
characteristics.1,2 the goal is to optimize treatment decisions and reduce unnecessary costs for each individual, by selecting treatments most likely to benefit (or least likely to harm) them based on their participant-level covariate values. for
example, trastuzumab is only given to the subgroup of breast cancer patients who are human epidermal growth factor
receptor 2 (her-2) positive, as it is known to lock on to the her-2 protein, block the receptor, and stop the cells from
dividing and growing.3 it is therefore considered unnecessary for those who are her-2 negative. in this situation, there is
a so-called treatment-covariate interaction, such that the treatment effect (measured on a scale such as a mean difference,
risk ratio, odds ratio, or hazard ratio) changes according to an individual's her-2 status. participant-level covariates that
interact with treatment effect are also known as effect modifiers, moderators, predictors of treatment effect and, mainly
in the cancer literature, predictive markers.
though some treatment-covariate interactions, such as her-2, are suspected in advance due to strong biological rationale, others are only identified following secondary investigations of existing data. single randomized trials are typically
powered on the overall treatment effect (ie, the treatment effect averaged across all individuals), and so do not usually
have sufficient power to detect differences in treatment effect between individuals. powering a single trial to detect a
genuine treatment-covariate interaction will typically require at least four times the sample size needed to test the overall treatment effect;4 thus the funding of such trials is expensive and often infeasible. when individual participant data
(ipd) from multiple randomized trials are available, meta-analysis provides the opportunity to increase power to detect
true treatment-covariate interactions.5 for this reason, many ipd meta-analyses of randomized trials are initiated specifically to examine one or more potential treatment-covariate interactions, often with the aim to reduce or explain observed
between-study heterogeneity in a previous meta-analysis of (published) aggregate data. for pharmaceutical companies,
a driver may be to rescue a treatment that previously showed no overall benefit on all patients combined, but which may
still benefit a select group of individuals.
over the last decade we have been involved in a number of ipd meta-analysis projects aiming to examine
treatment-covariate interactions at the participant-level, and learnt important lessons and pitfalls from a statistical point
of view. in this tutorial article, we share our experience to help other researchers in this situation, building on previous
work in both ipd meta-analysis and single study settings.4,6-16 our main goal is to provide statistical recommendations
for the meta-analysis part of the project; in particular, how to estimate treatment-covariate interactions using either a
one-stage or two-stage ipd meta-analysis framework. a further aim is to highlight two statistical topics that are relevant
to consider before embarking on an ipd meta-analysis project to examine treatment-covariate interactions. the outline
of the article is as follows. section 2 describes two motivating examples used for illustration throughout the whole article.
section 3 describes a framework for two-stage and one-stage ipd meta-analysis models for estimating treatment-covariate
interactions, and section 4 highlights five recommendations in this context: (i) calculating differences in meta-analysis
results for subgroups is misleading; (ii) interaction estimates should be based solely on within-study information; (iii)
continuous covariates (and outcomes) should be analyzed on their continuous scale; (iv) nonlinear relationships should
be examined for continuous covariates, using a multivariate meta-analysis of the trend (eg, using restricted cubic spline
functions); and (v) translation of interactions into clinical practice requires individualized treatment effect prediction.
section 5 then describes two statistical issues in guiding decisions to initiate an ipd meta-analysis project to examine
interactions. first, how the decision to initiate such ipd meta-analysis projects should not be based on the amount of
between-study heterogeneity in the overall treatment effect. second, how to calculate, in advance of ipd collection, the
power of a potential ipd meta-analysis conditional on the characteristics of those trials promising their ipd. section 6
concludes with discussion.
2 motivating examples
we now introduce two examples, which will be used to illustrate some of the key issues in later sections.
2.1 an ipd meta-analysis examining the effect of antihypertensive treatment
wang et al17 performed an ipd meta-analysis of trials in participants with hypertension to investigate to what extent lowering of systolic blood pressure (sbp) contributed to the prevention of cardiovascular events. they selected randomized
trials that tested active antihypertensive drugs against control. ipd was sought from trials in the individual data analysis of antihypertensive intervention trials (indana) dataset or at the studies coordinating centre in leuven (belgium).
ten trials (with a parallel group design) were ultimately included, and these provided ipd for a total of 28 581 patients.
key outcomes of interest by end of follow-up were sbp, cardiovascular disease (cvd), and all-cause mortality. an important focus is on whether the effect of antihypertensive treatment on these outcomes is modified by (ie, interacts with)
participant-level covariates such as gender and baseline blood pressure.
2.2 an ipd meta-analysis examining the effect of interventions to reduce gestational
weight gain in pregnancy
the international weight management in pregnancy (i-wip) collaborative group obtained ipd from 36 trials (12 447
women), to investigate whether diet and lifestyle interventions improve outcomes during pregnancy. a primary outcome
was whether interventions reduced gestational weight gain, for which 33 studies and a total of 9320 women were available.
weight was recorded as a continuous outcome (in kg), available at baseline (ie, confirmation of pregnancy) and follow-up
(ie, last available weight recorded before delivery). other outcomes include pre-eclampsia and stillbirth. a key aim was to
examine if the intervention effect on these maternal and fetal outcomes is modified by (ie, interacts with) participant-level
covariates such as the mother's age and body mass index (bmi) at baseline.
3 two- stage and one- stage ipd meta-analysis models
for estimating treatment-covariate interactions
in this section, we introduce the framework of two-stage and one-stage models for conducting an ipd meta-analysis to
examine treatment-covariate interactions based on within-study information.
3.1 the two-stage approach
a two-stage ipd meta-analysis for summarizing treatment-covariate interactions is a straightforward application of
a traditional meta-analysis framework. in the first stage the treatment-covariate interactions are estimated using the
ipd from each trial separately; then in the second stage these interaction estimates are pooled using a traditional (eg,
inverse-variance weighted) meta-analysis model.
consider an ipd meta-analysis of multiple randomized trials, each comparing the effect of a particular treatment
relative to a control using a simple parallel group design. let i denote trial (i = 1 to s), ni denote the number of participants in the ith trial, j denote participant (j = 1 to ni), and xij denote allocation to either the treatment (xij =1) or
control (xij =0) group for the jth participant in the ith trial. let zij be a key participant-level covariate of interest (eg, the
sex of participant j in trial i), observed for all participants in each trial. then, the two-stage approach can be detailed as
follows.
3.1.1 first stage
the analysis to apply in the first stage depends on the outcome type. for example, assume the aim is to evaluate the
treatment effect on a continuous outcome (yij), such as sbp, then the first stage of the ipd meta-analysis should apply a
linear regression in each trial separately, adjusting for the baseline sbp (y0ij) and the covariate (zij), whilst including the
treatment (xij) and the treatment-covariate interaction (xijzij):
y𝑖𝑗 = 𝛼i + 𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗 + e𝑖𝑗
e𝑖𝑗 ∼ n(0, 𝜎2
i ). (1)
the key parameter in model (1) is the treatment-covariate interaction (𝛾wi), which represents the change in the treatment effect (ie, the change in the mean difference in outcome value for treatment compared to control) for a 1-unit
increase in zij after adjusting for the prognostic effects of zij and y0ij. other terms are 𝛼i (the intercept, that is, the expected
outcome value for participants in the control group with zero values of zij and y0ij), 𝛽1i and 𝛽3i (the expected change in
the outcome value for a 1-unit increase in zij and y0ij, respectively) and 𝛽2i (the treatment effect for those with a zij of zero
after adjusting for baseline). the same residual variance (𝜎2
i ) is assumed for the treatment and control groups, but this
can be relaxed if considered appropriate.18
if a binary outcome was rather of interest (ie, yij =0 or 1), then alternatively a binomial regression could be fitted in
each trial separately, such as a logistic regression,
y𝑖𝑗 ∼ bernoulli(p𝑖𝑗)
ln ( p𝑖𝑗
1 − p𝑖𝑗 )
= 𝛼i + 𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗, (2)
where pij is the probability of an outcome event (ie, yij = 1) for the jth participant in the ith trial conditional on their
covariate values. the parameters are defined similar to those for model (1), expect now we model the log-odds of the outcome event (
ie, ln ( p𝑖𝑗
1−p𝑖𝑗 ))
such that the treatment effect is measured by a log odds ratio. hence, the treatment-covariate
interaction (𝛾wi) represents the change in the log odds ratio for a 1-unit increase in zij after adjusting for the prognostic
effects of zij and y0ij.
if a time-to-event outcome was of interest, then a cox proportional hazards regression model could be fitted in each
trial separately, such as
𝜆𝑖𝑗(t) = 𝜆0i(t) exp(𝛽1izij + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗), (3)
where 𝜆ij(t) is the hazard rate of the outcome for the jth participant in the ith trial conditional on their covariate values,
whilst 𝜆0i(t) is the baseline hazard (ie, the hazard rate for a participant in the control group with zero values of zij and y0ij).
now the treatment effect is measured by a log hazard ratio, such that the treatment-covariate interaction (𝛾wi) represents
the change in the log hazard ratio for a 1-unit increase in zij after adjusting for the prognostic effects of zij and y0ij.
extension of models (1) to (3) to adjust for further baseline covariates (ie, in addition to zij and y0ij) is recommended
when there are other known prognostic factors.19 ideally, the researcher should predefine a core set of strong prognostic
factors to be adjusted for in each trial, particularly focusing on those routinely recorded in the trials available for ipd
meta-analysis. for example, in many disease fields, age and stage of disease are key prognostic factors and routinely
recorded at baseline, such that they can be adjusted for in the analysis. other more complex trial designs (eg, cluster trials,
or multicenter or multiarm trials) would also require the models to be modified, as would allowing for nonproportional
hazards in model (3).
estimation of models (1) to (3) in each study, for example using (restricted) maximum likelihood estimation, produces
a treatment-covariate interaction estimate, ̂𝛾𝑊𝑖, and its variance, var(̂𝛾𝑊𝑖). the “w” is used to emphasize that the interaction, 𝛾wi, in each study is based solely on within-study information; that is, it is only based on differences in the treatment
effect across participant-level covariate values observed within each study. this is an important point (as it avoids using
across-study information and potentially introducing aggregation bias), which we return to in the next section. each 𝛾wi
indicates for trial i the change in treatment effect for a one-unit increase in the participant-level covariate zij. for a continuous covariate, this assumes the effect of the interaction is linear (although extension to nonlinear trends is important,
as described later in the article).
3.1.2 second stage
in the second stage of a two-stage ipd meta-analysis, we simply combine the ̂𝛾𝑊𝑖 values across trials in a traditional
meta-analysis model, such as a common-effect (sometimes known as a fixed-effect) model,
̂𝛾𝑊𝑖 ∼ n(𝛾w , var(̂𝛾𝑊𝑖)), (4)
or a random-effects model:
̂𝛾𝑊𝑖 ∼ n(𝛾𝑊𝑖, var(̂𝛾𝑊𝑖))
𝛾𝑊𝑖 ∼ n(𝛾w , 𝜏2
). (5)
after estimation of the chosen meta-analysis model (eg, using restricted maximum likelihood estimation), the estimate
of 𝛾w summarizes the difference in the treatment effect for two individuals who differ in zij by one-unit. based on the
linear, logistic, and cox models used in the first-stage, 𝛾w represents a difference in mean difference for a continuous
outcome, a difference in log odds ratios for a binary outcome (ie, exp(̂𝛾w ) gives a ratio of odds ratios), and a difference in
log hazard ratios for a time-to-event outcome (ie, exp(̂𝛾w ) is a ratio of hazard ratios).
note that random-effects meta-analysis model (5) allows for between-study heterogeneity in the true treatment
-covariate interaction. it may arise due to differences across studies in, for example, the dose of the treatment, the length
of follow-up, the way the covariate has been measured, and the magnitude of any interaction. it may also be due to
case-mix differences in the study populations, for example leading to between-study differences in the distribution of
within-study confounders and even the covariate itself. for example, if a treatment-covariate interaction is nonlinear, and
the covariate distribution is narrow in some studies and wide in others, then this will induce between-study heterogeneity in the treatment-covariate interaction, unless the nonlinear association is modeled directly (see later). the magnitude
and impact of heterogeneity can be summarized by providing estimates of 𝜏2 and using 95% prediction intervals.20,21
to account for uncertainty in the estimate of 𝜏2 when deriving 95% confidence intervals for 𝛾w , we recommend the
hartung-knapp sidik-jonkman (hksj) approach,22,23 or alternatively using a bayesian framework.
if some studies do not agree to share their ipd but do provide (either directly or in a publication) the required
treatment-covariate interaction estimate and its corresponding standard error (se), these can be incorporated in the second stage. that is, the ̂𝛾𝑊𝑖 derived directly from ipd trials are combined with the ̂𝛾𝑊𝑖 extracted (or provided) from non-ipd
trials.
3.2 the one-stage approach
a one-stage ipd meta-analysis can also be used to summarize treatment-covariate interactions. this approach analyzes
the ipd from all trials in a single step using a general or generalized linear (mixed) model framework,24,25 or a survival
(frailty) model.26 this allows a more exact likelihood specification than that used in the second stage of the two-stage
approach, and thus avoids assuming study-specific treatment-covariate interaction estimates are normally distributed
with known variances.24 hence one-stage models may be most advantageous when the studies in the ipd meta-analysis
have small numbers of participants and/or events.27,28
inclusion of a treatment-covariate interaction term in a one-stage model is not as straightforward as it may seem.
depending on the model specification, the apparently simple inclusion of a global treatment-covariate interaction
term may allow across-study information to contribute toward the summary interaction estimate, in combination with
within-study information.8,29,30 this may lead to aggregation bias (also known as ecological bias), which refers to when
the information across studies distorts the interaction estimate compared to when using only within-study information.
let zi represent the study-specific mean of the covariate zij, and let us assume there is potential between-study heterogeneity in the treatment-covariate interaction. then, to disentangle within-study and across-study information in a
one-stage model, there are two key options:
(i) center the covariate zij about its study-specific mean, zi and add an additional term which allows the covariate means
(zi) to explain between-study heterogeneity in the overall treatment effect7,31,32;
and/or.
(ii) stratify by trial other parameters outside the interaction term, including the parameter representing the treatment
effect (ie, the treatment effect at the covariate's reference value, typically zij = 0 or if the covariate is centered z𝑖𝑗 = zi).
implementing either approach should give very similar summary estimates of the treatment-covariate interaction, 𝛾w ,
from a one-stage ipd meta-analysis. approach (i) leads to a one-stage model for continuous, binary, and time-to-event
outcomes of the following format7,8,29:
y𝑖𝑗 = 𝛼i + 𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi) + 𝜀𝑖𝑗
𝛽2i ∼ n(𝜑 + 𝛾azi, 𝜏2
1 ) 𝛾𝑊𝑖 ∼ n(𝛾w , 𝜏2
2 ), (6)
y𝑖𝑗 ∼ bernoulli(p𝑖𝑗)
logit(p𝑖𝑗) = 𝛼i + 𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi)
𝛽2i ∼ n(𝜑 + 𝛾azi, 𝜏2
1 ) 𝛾𝑊𝑖 ∼ n(𝛾w , 𝜏2
2 ), (7)
𝜆𝑖𝑗(t) = 𝜆0i(t) exp(𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi))
𝛽2i ∼ n(𝜑 + 𝛾azi, 𝜏2
1 ) 𝛾𝑊𝑖 ∼ n(𝛾w , 𝜏2
2 ). (8)
parameters in models (6) to (8) are similar to those defined for models (1) to (3) in section 3.1.1. in addition, the study-specific treatment-covariate interactions (𝛾wi) are assumed normally distributed with a mean 𝛾w and
between-study variance 𝜏2
2 , as for random-effects model (5) in the second stage of the two-stage approach. of key interest
is an estimate of 𝛾w , to summarize the expected change in the treatment effect (eg, log hazard ratio in model (8)) for each
one unit increase in zij. as previously, “w” indicates that the interaction will be based solely on within-study information.
models (6) to (8) also include a meta-regression component (𝜑 + 𝛾azi), where 𝜑 denotes the summary treatment effect
for participants with zero values of zij and y0ij in trials with zi equal to zero; and 𝛾a is the change in the summary treatment
effect for each 1-unit increase in zi. the models also allow for residual between-study variance (𝜏2
1 ) in the summary
treatment effect (ie, that not explained by zi). inclusion of the 𝛾azi term, together with the centering of zij within the
interaction term, disentangles 𝛾w and 𝛾a such that they are uncorrelated with each other, and thus ̂𝛾w will be based
solely on within-trial information.8,32 note that if the 𝛾azi term is not included in model (8), then the interaction term
will represent some weighted average of ̂𝛾w and the magnitude of aggregation bias (̂𝛾e = ̂𝛾a − ̂𝛾w ). other parameters (ie,
intercepts, baseline hazards, prognostic effects of zij and y0ij) are stratified by trial (ie, estimated separately for each trial).
approach (ii) leads to one-stage models for continuous, binary and time-to-event outcomes of the following format:
y𝑖𝑗 = 𝛼i + 𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗 + 𝜀𝑖𝑗
𝛾𝑊𝑖 ∼ n(𝛾w , 𝜏2
), (9)
y𝑖𝑗 ∼ bernoulli(p𝑖𝑗)
logit(p𝑖𝑗) = 𝛼i + 𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗
𝛾𝑊𝑖 ∼ n(𝛾w , 𝜏2
), (10)
𝜆𝑖𝑗(t) = 𝜆0i(t) exp(𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗)
𝛾𝑊𝑖 ∼ n(𝛾w , 𝜏2
). (11)
in models (9) to (11) each of the “nuisance” parameters (ie, the 𝛼i, 𝜆0i(t), 𝛽1i, 𝛽2i, and 𝛽3i parameters which are not of
primary interest) are stratified by trial, and random effects are placed only on the within-study interaction. this stratification of all nuisance parameters by trial, in particular 𝛽2i representing the treatment effect at the covariate's reference
value, ensures that 𝛾w only contains within-trial information. it also closely reflects the two-stage approach, where all
nuisance parameters are naturally stratified by trial as they are estimated in each trial separately in the first stage.
as mentioned for the two-stage approach, all these one-stage models (6) to (11) need to be modified for other trial
designs (eg, cluster trials) and when adjusting for further prognostic factors (ie, in addition to zij and y0ij). the latter is a
sensible strategy (see discussion in section 3.1.1), but again requires each prognostic factor's effect to be stratified by study.
this will increase the number of parameters and so, for estimation reasons, centering each included prognostic factor by
figure 1 a two-stage ipd meta-analysis
of treatment-sex interactions, summarizing the
difference in the effect of antihypertensive
treatment for males compared to females. note:
the interactions refer to the difference between
males and females in the treatment effect (ie,
their difference in the mean difference in final
systolic blood pressure for treatment vs control,
after adjusting for baseline), with negative
values indicating that the treatment effect is
better in females than males [colour figure can
be viewed at wileyonlinelibrary.com]
its study-specific mean is a sensible default approach (especially when unrestricted maximum likelihood estimation is
used to fit the one-stage model).
3.3 applied example: is the effect of antihypertensive treatment different for males
and females?
the two-stage and one-stage approaches are now illustrated using the ipd from the 10 randomized trials examining the
effect of antihypertensive treatment on sbp, as introduced in section 2.1. the question is whether the treatment effect is
different for males compared to females; that is, whether there is a treatment-sex interaction.
two-stage approach
in the first stage, restricted maximum likelihood (reml) estimation was used to fit model (1) with final sbp as
the response, and sex (males = 1, females = 0) as the covariate, zij, of interest. figure 1 provides a forest plot of the
treatment-sex interaction estimates, ̂𝛾𝑊𝑖, plotted as circles. circles are recommended by fisher et al to help distinguish a
forest plot of interaction estimates from a standard forest plot of treatment effect estimates,6,9,33 for which squares are typically used. in the second stage, reml estimation was used to fit random-effects meta-analysis model (5) and this gave a
summary interaction estimate of ̂𝛾w =0.77 (95% ci: −0.52 to 2.07). hence there is no evidence of an important difference
in treatment effect for males compared to females; the summary interaction estimate is close to zero and clinically unimportant, the confidence interval overlaps zero, and there is also between-study heterogeneity. using the hksj approach,
the 95% confidence is slightly wider (−0.73 to 2.27).
one-stage approach
for the one-stage approach, we used reml estimation to fit models (6) and (9). this gave the same summary interaction estimate of ̂𝛾w =0.77 (95% ci: −0.52 to 2.06), practically identical to that from the two-stage approach when reml
and a wald-based confidence interval are used. when we rather used the satterthwaite approach to derive confidence
intervals following estimation of the one-stage model (as recommended for continuous outcomes34,35) the obtained 95%
confidence interval for 𝛾w was slightly wider (−0.72 to 2.26), and practically identical to that derived using the hksj
method in the two-stage approach (as they both utilize the t-distribution rather than the standard normal distribution).
thus, one-stage and two-stage ipd meta-analysis results for the treatment-sex interaction are almost identical. this agrees
with burke et al who describe in detail that when the same assumptions and the same estimation approaches are applied,
one-stage and two-stage approaches will closely agree unless most studies are small.28 in conclusion, this example does
not support there being an important treatment-sex interaction.
4 statistical modeling recommendations when conducting
an ipd meta-analysis to examine treatment-covariate
interactions
some published ipd meta-analysis projects that examine treatment-covariate interactions use inappropriate statistical
approaches that ignore or incorrectly modify the models described in section 3.9 we now make five recommendations to
address the key concerns.
4.1 do not make inferences about interactions using the summary treatment effect
derived in each subgroup separately
when the participant-level covariate is categorical, it may be tempting to perform a one-stage or two-stage ipd
meta-analysis of the overall treatment effect in each category (subgroup) separately. for example, an ipd meta-analysis
might be conducted for males and females separately, to obtain the summary treatment effect for males and females.
however, fisher et al and belias et al show that it is dangerous to use the subsequent results to make inferences about
whether an interaction exists.9,36 in particular, a common mistake is to conclude a treatment-covariate interaction exists
if the summary treatment effect estimate is statistically significant in one subgroup but not the other. in this situation
the actual treatment-covariate interaction (difference between subgroups based on within-study information) may not be
statistically significant. altman and bland consider this eloquently.37
it is even flawed to compare the summary treatment effects for each subgroup, for example via a statistical test or
by calculating their difference. although this is apparently simple, it amalgamates within-study and across-study information, and so should be avoided. the larger the differences in mean covariate values across studies, the larger the
potential contribution of the across-study information. one way to understand this is to consider an extreme situation,
where a treatment-sex interaction is of interest, but some trials contain only males. such trials cannot contribute any
within-study information about the interaction between treatment effect and sex at the individual level, as there are no
females. however, the trial would still contribute toward the subgroup result for males, and thus subsequently toward
the difference between meta-analysis results for male and female subgroups. this issue is avoided by meta-analyzing
the treatment-covariate interaction estimates observed within trials, by using the two-stage and one-stage approaches
outlined in section 3.
application to the hypertension example
recall that a two-stage ipd meta-analysis using only within-study information gave a summary treatment-sex interaction estimate of ̂𝛾w =0.77 (95% ci: −0.52 to 2.07). when we do a separate ipd meta-analysis for males and females,
we obtain summary treatment effect estimates of −9.02 (95% ci: −10.47 to −7.57) and −10.59 (95% ci: −12.60 to −8.58),
respectively. the estimated difference in these subgroup results is 1.57, which is an amalgamation of within-trial and
across-trial information, and about twice the size of the summary interaction estimate of 0.77 based solely on within-study
information. the across-trial information arises because the proportion of males varies considerably across trials, from
about 0.23 to 0.70.
4.2 separate within-trial and across-trial information in one-stage models
one-stage ipd meta-analysis models are appealing to statisticians, as they eloquently synthesize all the ipd in a single
statistical model as described in section 3.2. however, compared to the two-stage approach, it is easier to make modeling
errors when estimating 𝛾wa. one-stage models (6) to (11) show how to separate within-study and across-study information, and ensure that 𝛾wa is estimated based solely on within-study information. as such, they are labeled as “deft” by
fisher et al.9 however, many researchers wrongly fit models (6) to (8) without the 𝛾azi term, and with 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi)
replaced by 𝛾waixijzij. for example, a flawed one-stage linear regression model for a continuous outcome is:
y𝑖𝑗 = 𝛼i + 𝛽1iz𝑖𝑗 + 𝛽2ix𝑖𝑗 + 𝛽3iy0𝑖𝑗 + 𝛾𝑊𝐴𝑖x𝑖𝑗z𝑖𝑗 + e𝑖𝑗
𝛾𝑊𝐴𝑖 ∼ n(𝛾𝑊𝐴, 𝜏2
𝛾𝑊𝐴 ) 𝛽2i ∼ n(𝛽2, 𝜏2
𝛽2
) e𝑖𝑗 ∼ n(0, 𝜎2
i ). (12)
here, the interaction 𝛾wa is an amalgamation of within-study information and across-study information; essentially
a weighted average of 𝛾w and 𝛾a.
8 the estimate of 𝛾wa from model (12) will be more precise than the estimate of 𝛾w from
figure 2 does age interact with the effect of
antihypertensive treatment on systolic blood pressure? findings
from an ipd meta-analysis of 10 trials, showing difference in
results based on across-study (solid line) and within-study
(dashed lines) information. across-study relationship (from
meta-regression of trial's treatment effect estimates vs mean age)
denoted by gradient of solid line. participant-level relationship
using within-study information (ie, treatment-sex interaction
within each trial) denoted by gradient of dashed lines and the
summary gradient (̂γw) is −0.036 (95% ci: −0.19 to 0.12). each
block represents one trial, and the block size is proportional to the
size of the trial.
-20 -15 -10 -5
trial treatment effect estimate (mm hg)
40 50 60 70 80
mean age in the trial
model (6), but at the expense of the across-study information distorting the estimate and interpretation compared to 𝛾w .
this motivates fisher et al to call it a “deluded” analysis.9
application to the hypertension example
returning to the hypertension example, let us consider whether there is a treatment-age interaction whilst assuming
that the effect of age is linear. reml estimation of one-stage model (6) gives ̂𝛾w =−0.036 (95% ci: −0.19 to 0.12), and thus
no clear evidence of a treatment-age interaction. when rather fitting model (12), we obtain ̂𝛾𝑊𝐴= −0.079 (95% ci: −0.14
to −0.02), which is almost twice the size of that based solely on within-study information and now strongly suggests a
treatment-age interaction. the difference is because ̂𝛾𝑊𝐴 is strongly influenced by the across-study information, which
is suggesting a different magnitude of effect than suggested by the within-study information in half of the trials. this is
illustrated in figure 2, which also suggests that there may be a nonlinear interaction with age, as the direction of the
interactions appears to be different in trials with younger and older ages. extension to nonlinear trends is considered later.
4.3 do not dichotomize continuous covariates or outcomes
categorization, and in particular dichotomization, of continuous covariates (eg, such as age, blood pressure, and most
biomarkers) using cut-points is best avoided when examining their interaction with treatment effect.38-40 the usual argument for categorization is to aid clinical interpretation and maintain simplicity. however it can rarely, if ever, be justified
that an individual whose value is just below the cut-point is completely different from an individual whose value is just
above it. moreover, categorization reduces the power to detect genuine covariates that interact with treatment effect. ensor
et al show that, in the aforementioned ipd meta-analysis to examine exercise interventions to reduce gestational weight
gain in pregnancy, the loss of information by dichotomizing bmi (rather than keeping bmi as continuous) is equivalent
to throwing away about one-third of the ipd available.41
categorization of continuous outcomes should also be avoided, as it can mislead researchers into thinking there are
differential responses to treatment. in particular, classifying individuals as either responders or nonresponders, based
on an arbitrary cut-point value for a continuous outcome (eg, sbp<120 mmhg at follow-up), will lead to those just
above the threshold being classed differently to those just below it, which is nonsense. such dichotomization also leads
to misclassification when there is measurement error.42
application to the pregnancy example
in the pregnancy example an outcome of interest was caesarean section and, across all participants in 32 trials, the
intervention reduces the odds of caesarean section by about 10% (summary or = 0.91; 95% ci: 0.83 to 0.99). it is of interest
whether the mother's age interacts with this intervention effect. if we arbitrarily dichotomize age at 35 years, so that it
becomes a binary variable (ie, 0 if age ≤35, and 1 if age >35), then a two-stage ipd meta-analysis (model (2) followed by
model (5)) shows no clear evidence of an interaction between age and treatment effect (summary ratio of ors = 1.14,
95% ci: 0.90 to 1.45). however, if age is kept as a continuous variable, then there is stronger evidence of a treatment-age
interaction. the intervention becomes less effective at reducing the odds of caesarean section as a women's age increases.
for every 5-year increase in age the odds ratio increases by about 10% (summary ratio of ors = 1.10, 95% ci: 1.00 to 1.20).
when analyzed on the risk (rather than odds scale), using a binomial regression with a log-link, this translates to about
a 6% increase in the risk ratio for every 5-year increase (95% ci: 0.98 to 1.15).
4.4 allow for potential nonlinear relationships when modeling interactions
with continuous covariates
in the previous example we assumed a linear trend for the interaction of treatment and a continuous covariate. however, sometimes the interaction may be nonlinear, as emphasized by royston and sauerbrei,15 and considered in detail by
kasenda et al.43,44 this implies the change in treatment effect for every 1-unit increase in the covariate may vary across
the distribution of the covariate. therefore, nonlinear interactions should be evaluated when the interaction of a continuous covariate and treatment effect is of interest. we now illustrate this with an example, and explain how it can be
implemented using a multivariate meta-analysis of spline functions.
application to the hypertension example
wang et al consider whether there is an interaction between age and the effect of hypertensive treatment, and identify
a nonlinear relationship, with older patients between 60 and 80 years old seeming to benefit more than younger patients
and those older than 80.17 their original analysis categorized age, but we now update their analysis to rather display the
change in treatment effect as a smooth, nonlinear function of age (figure 3), with the reference group being the treatment
effect for a 55 year old. a j-shaped relationship is visible; in particular, there is strong evidence that younger individuals
have the smallest treatment effect. for example, compared to an individual aged 40 years, an individual aged 55 years has
about a 3 mmhg greater reduction in sbp due to the treatment (compared to control). in very old ages the treatment effect
also appears to reduce slightly, but there is large uncertainty (wide confidence intervals). interestingly, if we just include
an interaction with age (ie, assume a linear interaction term) then there is no evidence of a treatment-age interaction.
the change in treatment effect for each year increase of age is −0.05 (95% ci: −0.19 to 0.12). the linear assumption hides
the more j-shaped interaction revealed by the nonlinear modeling approach.
conducting a two-stage multivariate ipd meta-analysis of restricted cubic splines
figure 3 was obtained by conducting a two-stage ipd meta-analysis, where in the first stage a restricted cubic spline
function was estimated in each study separately and in the second stage a multivariate meta-analysis was fitted. splines
are a flexible way of modeling smooth nonlinear relationships.45 briefly, a restricted cubic spline is obtained by fitting
a series of cubic functions and forcing them to join (and be smoothed) at certain points (called internal knots), whilst
constraining the function to be linear in the tails (ie, before the first internal knot and after the last internal knot). the
magnitude and shape of the curve are defined by multiple parameters depending on the number of knots chosen; as
explained in figure 4. the knot locations are forced to be the same in every study, to ensure the study-specific curves can be
synthesized meaningfully. rather than using a reference group whose covariate value is 0, it helps to center the covariate
at a meaningful value (eg, 55 years was chosen as the reference group in figure 3), which is the same in every study. in
our example, three internal knots (at the same locations in every study) were chosen for the restricted cubic function
representing the association between age and final sbp in the control group. hence, in the first stage three parameters
defining the spline function are estimated in each study (an intercept and two slope terms), plus the interaction of this
spline function with the treatment effect. the latter provides the within-study treatment-covariate interaction defined by
𝛾w1i and 𝛾w2i (see figure 5a), which represent the change (difference) in treatment effect across covariate values, relative
to the chosen reference group of an individual aged 55 years.
figure 3 summary of the nonlinear interaction between
age and effect of hypertension treatment on final sbp value. figure
created by fitting an analysis of covariance model in each study
separately, with the interaction between age and treatment modeled
via a restricted cubic spline function (with knot positions of 39, 60,
and 75), and then the study-specific parameter estimates (relating to
the interaction) pooled in a multivariate random-effects
meta-analysis
figure 4 introduction to modeling nonlinear relationships in a single study using restricted cubic splines; for further details we
recommend the reader refer to harrell45
figure 5 (a): overview of the first stage of a two-stage multivariate ipd meta-analysis to summarize a nonlinear treatment-covariate
interaction using a restricted cubic spline. the steps are described in relation to the hypertension example of figure 3 which examined a
treatment-age interaction.
figure 5 (b): overview of the second stage of a two-stage multivariate ipd meta-analysis to summarize a nonlinear
treatment-covariate interaction using a restricted cubic spline. the steps are described in relation to the hypertension example of figure 3
which examined a treatment-age interaction
in the second stage, the study estimates of the difference in slope parameters of the spline function (ie, ̂𝛾w1i and
̂𝛾w2i) can be meta-analyzed using a multivariate random-effects meta-analysis.46-50 this allows the synthesis of multiple
parameter estimates whilst accounting for their within-study and between-study correlation, and produces a summary
estimate for each parameter, from which the summary spline (nonlinear) function can be derived. it can handle missing
parameter estimates (ie, for missing parts of the spline function in studies with a narrow distribution of covariate values)
as described in figure 5a,b. this summary curve describes the association between values of the covariate and the change
in treatment effect, relative to the reference group (age 55 years in the example). this can then be plotted graphically to aid
interpretation. the study-specific estimated curves from the first stage (or study-specific empirical bayes curves obtained
postestimation from the second stage) might also be presented, as shown by gasparrini et al.46
a summary of the two-stage multivariate ipd meta-analysis of restricted spline functions is given in figure 5a,b.
a similar approach is a multivariate ipd meta-analysis of a polynomial function51; in the first stage the differences (ie,
treatment minus control) of the parameters of a chosen polynomial function are estimated in each study (eg, differences
in a quadratic shape, such as 𝛾w1x𝑖𝑗z𝑖𝑗 + 𝛾w2x𝑖𝑗z2
𝑖𝑗) and the parameter estimates (̂𝛾w1 and ̂𝛾w2) are jointly synthesized to
produce a summary function.51 in particular, fractional polynomial functions provide a flexible set of power transformations to describe a potentially nonlinear association.52-54 in order for the parameters of the function to be combinable
across studies in a multivariate meta-analysis, the same powers of the fractional polynomial function must be specified
in each study (ie, the shape of the nonlinear association is fixed across studies).51 in contrast, as long as the same number and location of knots is used in each study, a restricted cubic spline function is more flexible (ie, the shape of the
nonlinear association can vary across studies) and it can handle different covariate distributions across studies by strategic placement of knots. sauerbrei and royston show that it is possible to combine different fractional polynomial power
transformations across studies, if the predicted values of the function are pooled (rather than the parameters defining the
function),55 with the pooling of predictions done at each value of the covariate separately.
a one-stage ipd meta-analysis model can also be used to examine nonlinear treatment-covariate interactions, but
difficulties can arise. first, the extra parameters required to model nonlinear functions, and potentially multiple random effects, may cause estimation and convergence problems. second, when centering covariates by their study-specific
means (ie, to avoid aggregation bias), by extending models (6) to (8), the interpretation of the spline function becomes
problematic. unless all studies have the same mean covariate value, the change in treatment effect for a 1-unit increase
in a covariate from its mean will have a different interpretation in each study; this will make the summary spline function uninterpretable. therefore, it is preferable to examine nonlinear trends by extending one-stage models (9) to (11),
stratifying by trial parameters outside the interaction term, to remove aggregation bias.
4.5 to personalize decision-making, individualized predictions of treatment effect are
required
for clinical practice, translation of identified treatment-covariate interactions is required for the individual patient, in
order to tailor their treatment decisions. this is nontrivial. a fundamental error is to assume that if a treatment-covariate
interaction exists then the treatment is effective in some individuals but not in others. actually, even when the magnitude
of treatment effect varies across individuals, the direction of effect may consistently suggest the treatment is beneficial
for everyone.
to better guide decision-making, a predicted treatment effect is required for each individual. this requires a robust
prediction model equation, developed and validated using methodology principles for clinical prediction models.2,45,56 in
particular, to reduce overfitting (too extreme predictions) the model development might require penalization and shrinkage approaches,57-59 to reduce the variability of predictions in new datasets, thereby reducing the mean-square error of the
predictions. a step further is to predict an individual's absolute outcome value or risk conditional on their prognostic factors and expected treatment effect. these and related issues are discussed in detail elsewhere.60-63 crucially, aggregation
bias should also be avoided when making predictions, and we now illustrate this with an example.
application to the hypertension example
consider the example within figure 3 that suggests a potential nonlinear interaction between age and the effect of
antihypertensive treatment, with younger patients seemingly having less benefit (in terms of reduction in sbp) than older
individuals. the smooth curve reflects the treatment-age interaction; it reveals the summary estimate of the difference
in treatment effect for an individual with a particular age compared to an individual aged 55 years. however, it does not
tell us the predicted (expected) treatment effect for an individual with a particular age. to obtain this, we performed a
multivariate ipd random-effects meta-analysis, where we estimate and then meta-analyze the two slopes (𝛾w1i and 𝛾w2i)
of the spline function (as defined in figure 5a), and also the reference treatment group (𝛽2i, the study-specific treatment
effect for individuals aged 55 years, our reference group). then, the summary estimates (𝛽
̂2, ̂𝛾w1, ̂𝛾w2) obtained were used
to define the prediction model for an individual's treatment effect conditional on their age:
predicted treatment effect for jth individual = 𝛽
̂2 + ̂𝛾w1(age1j − 55) + ̂𝛾w2(age2j − 3.16)
= −10.66–(0.251 × (age1j − 55))
+ (0.156 × (age2j − 3.16)). (13)
here, −10.66 is the predicted treatment effect for an individual aged 55 years, and age1j and age2j represent, respectively, the individual's values of the first and second spline transformation of their age. for example, for an individual
aged 40, their corresponding age1j is 40 and age2j is 0.00077, and thus their predicted treatment effect is
−10.66–(0.251 × (40–55)) + (0.15 × (0.00077–3.16)) = −7.39.
overfitting is potentially of limited concern for prediction equation (13), as it was derived using over 28 000 participants with only two parameters (excluding the intercept) estimated. the predicted treatment effect across the age range is
summarized in figure 6. across all ages the predicted treatment effect is at least 5 mmhg. hence, on average, all patients
are predicted to benefit by a clinically useful amount, even though the magnitude of effect varies due to the treatment-age
interaction.
a concern is that a multivariate meta-analysis of 𝛽
̂2i, ̂𝛾w1i, and ̂𝛾w2i accounts for the correlation amongst these parameters, both within and between studies. yet again this allows the potential for aggregation bias to influence the summary
interaction terms, ̂𝛾w1 and ̂𝛾w2, as their correlation with 𝛽
̂2 allows the borrowing of across-trial information. though it
is perhaps sensible for 𝛽
̂2 (as it is our reference treatment effect averaged across studies), we should want our summary
interactions terms to be based solely on within-trial information, as previously argued. to address this, we replace the
summary ̂𝛾w1 and ̂𝛾w2 estimates in our prediction equation (13) with −0.178 and 0.133, their respective estimates from a
multivariate meta-analysis ignoring any correlation with 𝛽
̂2i terms then, the modified prediction equation is:
predicted treatment effect for jth individual = 𝛽
̂2 + ̂𝛾w1(age1j − 55) + ̂𝛾w2(age2j − 3.16)
= −10.66–(0.178 × (age1j − 55))
+ (0.133 × (age2j − 3.16)). (14)
this allows us to produce predicted treatment effects conditional on age, summarized across all studies and removing
aggregation bias. using equation (14) for an individual aged 40 years, their predicted treatment effect is,
−10.66–(0.178 × (40–55)) + (0.133 × (0.00077–3.16)) = −8.41 mmhg
figure 6 the predicted effect of antihypertensive treatment
on sbp conditional on an individual's age, based on a multivariate
meta-analysis either with or without aggregation bias in the
summary treatment-age interactions
which is larger than the −7.39 predicted from the previous equation. indeed, the predicted curve across the entire age
range is noticeably shallower after removing aggregation bias (figure 6), because the aggregation bias (arising from incorporating across trial information) makes interaction estimates larger than when based solely on within-trial information.
note that the predictions shown here are averaged across studies. if possible, incorporation of study-level covariates (eg,
country) that explain between-study heterogeneity in parameter estimates might help tailor predictions further.
5 to ipd or not to ipd? statistical recommendations when
planning an ipd meta-analysis project to examine
treatment-covariate interactions
statistical considerations are also important when deciding whether to initiate an ipd meta-analysis project (ie, before
ipd collection), and two key issues are now discussed in the context of treatment-covariate interactions.
5.1 the decision to initiate an ipd meta-analysis project to examine
treatment-covariate interactions should not be based on the amount of between-study
heterogeneity in the overall treatment effect
without ipd, most meta-analyses of randomized trials will summarize the treatment effect based on all trial participants (ie, without consideration of treatment-covariate interactions). if such a meta-analysis does not find evidence of
between-study heterogeneity in the treatment effect, it may be tempting to conclude that the treatment effect is the same
for all individuals. however, this logic is flawed: the absence of heterogeneity in the overall effect (across all participants)
does not necessarily imply an interaction does not exist. first, if there is a genuine treatment-covariate interaction, but
the distribution of the covariate is very similar across studies, then (assuming no other patient-level or study-level effect
modifiers differ across studies), the overall treatment effect will be the same in each study (ie, there will be no heterogeneity). second, even if the distribution of the covariate does change across studies, the overall treatment effect may still
be homogenous; for example this may arise due to chance, or because the covariate has a nonlinear (eg, u-shaped) interaction with treatment (so that the overall treatment effect may still be the same in two studies with very different mean
covariate values), or even due to multiple effect modifiers acting in combination and different directions.
conversely, if there is heterogeneity in the overall treatment effect this does not imply that an interaction exists at the
participant-level. heterogeneity can arise due to changes in study-level characteristics such as dose, follow-up length, and
setting, even when there is no interaction at the participant-level. hence, the decision to investigate treatment-covariate
interactions should not be driven by the presence of between-study heterogeneity in treatment effect, and should be rather
motivated by the supposed (biological) mechanism of treatment response.
application to the hypertension example
as an illustration of why heterogeneity is a poor indicator of whether treatment-covariate interactions exist, consider
a two-stage ipd meta-analysis examining the effect of antihypertensive treatment on rate of cvd, for which the summary
hazard ratio was 0.74 (95% ci: 0.67 to 0.81) with no observed heterogeneity (𝜏̂2 = 0).64 however, there is some suggestion
figure 7 evidence of a potential nonlinear interaction
between baseline sbp and the effect of hypertension treatment on
the rate of cvd, even though there was no between-study
heterogeneity in the overall treatment effect
of a nonlinear interaction between baseline sbp and the treatment effect on cvd, with the treatment effect gradually
reducing as the baseline sbp moves from about 170 mmhg toward 120 mmhg (figure 7), as obtained using a two-stage
multivariate meta-analysis of restricted cubic splines as described in figure 5a,b.
5.2 calculate the power to identify a treatment-covariate interaction prior
to collection of ipd
ipd meta-analysis projects are more likely to be funded if they have sufficient power to answer the research question at
hand. before ipd collection, power calculations for ipd meta-analysis projects can be made conditional on the number
of trials promising their ipd, using known trial characteristics such as the number of participants and standard deviation
(sd) of covariate values. closed-form solutions are difficult to obtain, especially for noncontinuous outcomes unless
approximations are made.65 a pragmatic starting point is to consider whether the total sample size (and overall outcome
proportion or rate for binary and time-to-event outcomes) of the promised ipd has adequate power if naively considered
to come from a single study. then standard statistical software for estimating the power of an interaction in a single
randomized trial could be used.
to mirror the ipd meta-analysis setting more exactly, a simulation-based approach has been proposed.66-69 in particular, ensor et al suggest generating ipd containing the same number of studies (and same number of participants and
events therein) as promising their ipd,41 with covariate values simulated based on assumed true treatment effects and
treatment-covariate interaction effect size. then a two-stage ipd meta-analysis is applied to this simulated dataset, and
the estimated treatment-covariate interaction and its confidence interval are stored. this is repeated m times (ideally thousands). based on a traditional frequentist paradigm, power can then be estimated by calculating the proportion of times
the summary estimate was statistically significant (eg, as defined by the associated 95% confidence interval excluding the
null value). this process might also be repeated assuming different values for the size of the assumed treatment-covariate
interaction, and different values for the assumed magnitude of heterogeneity in the interaction across studies (starting
from zero).
for an ipd meta-analysis of s randomized trials with continuous outcomes, closed-form solutions are obtainable for
the power to evaluate a treatment-covariate interaction.10 simmonds and higgins provide the following analytic solution for the maximum likelihood estimate of a treatment-covariate interaction for a continuous outcome in a single
randomized trial with two parallel groups:10
̂𝛾𝑊𝑖 = 2
∑ni
j=1 z′2
𝑖𝑗 (
∑
j∈ti
y𝑖𝑗z′
𝑖𝑗 − ∑
j∈ci
y𝑖𝑗z′
𝑖𝑗)
.
here, ti denotes the treatment group and ci the control group in study i, and z′
𝑖𝑗 denotes that each zij is centered
about the study-specific mean zij value (ie, z′
𝑖𝑗 = z𝑖𝑗 − zi). simmonds and higgins use this to derive subsequent power
calculations,10 assuming common residual variances across trials. if we extend their work by allowing for different
residual variances in each trial (𝜎2
i ), the variance (var) of the interaction estimate in a particular study i is:
var(̂𝛾𝑊𝑖) = var (
2
∑ni
j=1 z′2
𝑖𝑗 (
∑
j∈ti
y𝑖𝑗z′
𝑖𝑗 − ∑
j∈ci
y𝑖𝑗z′
𝑖𝑗))
= 4
(∑ni
j=1 z′2
𝑖𝑗 )2
(
∑
j∈ti
z′2
𝑖𝑗 var(y𝑖𝑗) + ∑
j∈ci
z′2
𝑖𝑗 var(y𝑖𝑗)
)
= 4𝜎2
i
(∑ni
j=1 z′2
𝑖𝑗 )2
(
∑
j∈ti
z′2
𝑖𝑗 + ∑
j∈ci
z′2
𝑖𝑗 )
. (15)
let us assume an equal number of participants in the treatment and control groups, and that the variance of the
covariate (𝜎2
zi
) is the same in each treatment group (
ie, 𝜎2
tizi
= 𝜎2
cizi
= 𝜎2
zi
)
. then ∑ni
j=1 z′2
𝑖𝑗 = ni𝜎2
zi
, and equation (15)
simplifies to:
var(̂𝛾𝑊𝑖) =
4𝜎2
i
(∑ni
j=1 z′2
𝑖𝑗 )2
(
∑ni
j=1
z′2
𝑖𝑗 )
= 4𝜎2
i
∑ni
j=1 z′2
𝑖𝑗
= 4𝜎2
i
ni𝜎2
zi
. (16)
using the solution for var(̂𝛾𝑊𝑖) from either equation (15) or (16), we can derive a closed-form solution for the variance
of the summary interaction estimate from the second stage of a two-stage ipd meta-analysis. assuming common-effect
model (4), var(̂𝛾w ) is simply the sum of the inverse of the variances from each study,
var(̂𝛾w ) = (
∑
s
i=1
1
var(̂𝛾𝑊𝑖)
)−1
, (17)
and the subsequent power to estimate a treatment-covariate interaction of size 𝛾w using the ipd meta-analysis is
approximately,
power = prob (
̂𝛾w
√var(̂𝛾w )
> 1.96)
+ prob (
̂𝛾w
√var(̂𝛾w )
< −1.96)
. (18)
for example, assuming a common interaction and that trials have equally sized groups, we can use equations (16) and
(17) to give,
power = prob (
̂𝛾w
√var(̂𝛾w )
> 1.96)
+ prob (
̂𝛾w
√var(̂𝛾w )
< −1.96)
= φ
⎛
⎜
⎜
⎝
−1.96 + 𝛾w
√√√√∑
s
i=1
ni𝜎2
zi
4𝜎2
i
⎞
⎟
⎟
⎠
+ φ
⎛
⎜
⎜
⎝
−1.96 − 𝛾w
√√√√∑
s
i=1
ni𝜎2
zi
4𝜎2
i
⎞
⎟
⎟
⎠
, (19)
where φ(x) is the probability of sampling a value < x from the standard normal distribution. allowing for different sized
groups in a trial (ie, using equation (15) rather than (16)), the power calculation becomes,
power = φ
⎛
⎜
⎜
⎜
⎜
⎝
−1.96 + 𝛾w
√√√√√√√
∑
s
i=1
⎛
⎜
⎜
⎜
⎝
(∑ni
j=1 z′2
𝑖𝑗 )2
4𝜎2
i
(∑
j∈ti
z′2
𝑖𝑗 + ∑
j∈ci
z′2
𝑖𝑗 )
⎞
⎟
⎟
⎟
⎠
⎞
⎟
⎟
⎟
⎟
⎠
+ φ
⎛
⎜
⎜
⎜
⎜
⎝
−1.96 − 𝛾w
√√√√√√√
∑
s
i=1
⎛
⎜
⎜
⎜
⎝
(∑ni
j=1 z′2
𝑖𝑗 )2
4𝜎2
i
(∑
j∈ti
z′2
𝑖𝑗 + ∑
j∈ci
z′2
𝑖𝑗 )
⎞
⎟
⎟
⎟
⎠
⎞
⎟
⎟
⎟
⎟
⎠
, (20)
and as before we could replace ∑ni
j=1 z′2
𝑖𝑗 with ni𝜎2
zi
, and similarly ∑
j∈ti
z′2
𝑖𝑗 and ∑
j∈ci
z′2
𝑖𝑗 could be replaced with nti
𝜎2
tizi
and
nci
𝜎2
cizi
, respectively.
equations (19) and (20) require the user to specify a minimally important value for 𝛾w , and also the values of ni, 𝜎2
i ,
and 𝜎2
zi based on published study information (eg, from the baseline characteristics table for a trial) or provided directly by
study authors. the residual variance (𝜎2
i ) might be unavailable, and so approximated by the variance of outcome values.
extension can be made to allow for potential heterogeneity in the treatment-covariate interaction, but the amount of
heterogeneity is difficult to know in advance. thus we suggest to focus on the power in an ideal situation where there is
no between-study heterogeneity in the size of the interaction.
5.2.1 application to the pregnancy example
in the ipd meta-analysis conducted by the i-wip collaboration introduced in section 2.2, a primary objective was to
examine a potential interaction between baseline bmi and intervention effect on gestational weight gain. the prior
hypothesis was that those with high baseline bmi may benefit most from weight management interventions. no formal
power calculation was performed in advance of ipd collection but ultimately ipd from 24 trials. for illustration, here
we reconstruct the power calculation for this project assuming it was known that ipd could be obtained from these 24
trials. the values of ni, 𝜎2
i , and 𝜎2
zi were extracted from the publications of the 24 trials, and are shown by ensor et al41;
occasionally 𝜎2
i and 𝜎2
zi were missing and for these we used a weighted average of values from other studies. these values were input into equation (16) to derive var(̂𝛾𝑊𝑖) for each trial, which then were used within equation (17) to give
var(̂𝛾w ) = 0.0014.
a minimally important interaction size of −0.1 was suggested by clinical experts, such that the reduction in weight is
at least 1 kg larger for a 10-unit increase in bmi. thus inputting 𝛾w = −0.1 and var(̂𝛾w ) = 0.0014 into equation (19), we
obtain:
power = φ (
−1.96 − 0.1√ 1
0.0014)
+ φ (
−1.96 + 0.1√ 1
0.0014)
= φ(−4.917) + φ(0.997)
= 0 + 0.84.
thus the estimated power is 84%. ensor et al estimate a comparable 83% power using a simulation-based power
calculation,41 but our closed-form solution is obtained in a much quicker time-frame. had this power been known
at the time of the i-wip grant application, it would have given further merit to undertaking and funding the ipd
meta-analysis project. although, even if power was deemed low, there are often many other potential benefits of
undertaking an ipd meta-analysis project (eg, obtaining additional follow-up, standardizing inclusion criteria, and
so on).
6 discussion
there are multiple reasons for pursuing the collection and synthesis of ipd, 70,71 but personalized medicine is driving many ipd meta-analyses to search for treatment-covariate interactions. to guide such ipd meta-analysis projects,
this article has outlined key statistical methods for their conduct and planning, whilst drawing attention to issues and
pitfalls. we focused on interactions in randomized trials, but key points also apply to modeling interactions in ipd
meta-analyses of study types, such as those evaluating the accuracy of diagnostic tests or the association of prognostic
factors to subsequent outcomes.72
the actual identification, validation, and successful implementation of treatment-covariate interaction is rare.2 a
sensible starting point—for funders, researchers, health professionals, and patients—is that the relative treatment effect
is similar for all individuals, unless there is strong justification otherwise (eg, from previous findings, and especially
biological rationale).11,73 we highlighted many issues and pitfalls that if ignored may produce misleading conclusions,
and we suspect many will affect the 102 ipd meta-analyses identified by schuit et al as providing a significant interaction
estimate.74
our work focuses on statistical models for estimating an interaction. however if we are willing to make clinical decisions (and even withhold treatment) based on an individual's predicted treatment effect, then there should be a firm
understanding of the causal mechanism and pathway, and the cost-effectiveness of the approach. sun et al suggest criteria
to assess the credibility of a treatment-covariate interaction (which they call a subgroup effect),12 including an item about
the need for evidence to be based on within-study rather than across-study information. a key role is to disentangle different sources of variation,75 and senn recommends we need more n-of-1 trials,76 to repeatedly test multiple treatments
in the same person, including the same treatment multiple times. for further consideration of whether interactions are
likely to be genuine, we refer to two excellent tutorials which discuss intricate issues such as dose-response relationships,
measurement error, adjusting for confounders, dealing with multiple covariates, and multiplicative vs additive scales.72,77
scale of the analysis is an important issue. for example, a treatment-covariate interaction may occur on the odds ratio
scale but not the risk ratio scale, when the covariate is also a prognostic factor, as superbly illustrated by shrier and pang.78
conversely the risk ratio scale may also be problematic in some particular situations as, unlike the odds ratio, its value
may be bounded.45 nonlinear trends may also exist on one scale but not another. therefore consideration of multiple
scales may be important, as illustrated in the example of section 4.3. allowing for nonproportional hazards may also be
important for similar reasons.
more research is needed to help address the issue of multiple testing and exploratory research when investigating
multiple interactions in ipd meta-analysis settings, extending recommendations for a single trial.79 mistry et al propose
a tree-based recursive partitioning algorithm (called “ipd-sides”) to identify subgroup effects in an ipd meta-analysis
when there are many covariates of interest (ie, in an exploratory analysis).80 a limitation of their proposal is that it requires
the use of cut-points to dichotomize continuous covariates, rather than leaving them as continuous, and—as far as we
can tell—amalgamated within-study and across-study information. extension of their work to address these issues would
be welcome.
treatment-covariate interactions correspond to changes across individuals in the treatment effect as measured on the
relative scale (eg, risk ratio, odds ratio, hazard ratio). however, another approach to personalizing or stratifying the use
of treatments is to consider their impact on absolute risks. those people with the highest absolute risk will derive the
largest absolute benefit from a treatment (eg, greatest reduction in probability of the outcome) when the treatment effect
expressed as a risk ratio is the same for all patients. therefore, even in the absence of treatment-covariate interactions,
treatment decisions might be tailored conditional on absolute outcome risk.1,2 lastly, we note that those considering ipd
meta-analysis projects should also consider other practicalities aside from the analysis, such as obtaining, cleaning and
harmonizing the ipd.17,18


<|EndOfText|>

one-stage individual participant data meta-analysis models
for continuous and binary outcomes: comparison of
treatment coding options and estimation methods

a one-stage individual participant data (ipd) meta-analysis synthesizes ipd
from multiple studies using a general or generalized linear mixed model. this
produces summary results (eg, about treatment effect) in a single step, whilst
accounting for clustering of participants within studies (via a stratified study
intercept, or random study intercepts) and between-study heterogeneity (via
random treatment effects). we use simulation to evaluate the performance of
restricted maximum likelihood (reml) and maximum likelihood (ml) estimation of one-stage ipd meta-analysis models for synthesizing randomized trials
with continuous or binary outcomes. three key findings are identified. first,
for ml or reml estimation of stratified intercept or random intercepts models, a t-distribution based approach generally improves coverage of confidence
intervals for the summary treatment effect, compared with a z-based approach.
second, when using ml estimation of a one-stage model with a stratified intercept, the treatment variable should be coded using “study-specific centering” (ie,
1/0 minus the study-specific proportion of participants in the treatment group),
as this reduces the bias in the between-study variance estimate (compared with
1/0 and other coding options). third, reml estimation reduces downward bias
in between-study variance estimates compared with ml estimation, and does
not depend on the treatment variable coding; for binary outcomes, this requires
reml estimation of the pseudo-likelihood, although this may not be stable in
some situations (eg, when data are sparse). two applied examples are used to
illustrate the findings.
keywords
estimation methods, individual participant data, ipd, maximum likelihood., meta-analysis,
treatment coding
1 introduction
an individual participant data (ipd) meta-analysis synthesizes the raw individual-level data from multiple related studies
to produce summary results, for example, about the effect of a treatment.1 a common approach to ipd meta-analysis is a
two-stage framework, where the first step analyses the ipd from each study separately to produce aggregate data (such as
a treatment effect estimate and its se), which are then synthesized in the second step using a traditional meta-analysis,
such as a random effects model to account for between-study heterogeneity in the (treatment) effect of interest. an alternative approach to ipd meta-analysis is a one-stage framework, in which all studies are analyzed simultaneously using a
hierarchical model, such as a generalized linear mixed model or a frailty survival model, to produce summary results in
a single step. the one-stage approach has been increasingly used in the past decade.2
with a one-stage ipd meta-analysis model, it is essential to account for clustering of participants within studies to
avoid misleading conclusions.3 in particular, in a generalized linear mixed model framework two options to account for
this clustering are (i) by using a stratified intercept term in the analysis, which involves estimating a separate intercept
for each study; or (ii) by assuming random study intercepts, whereby study intercepts are assumed to be drawn from a
distribution (typically a normal distribution). we recently showed through simulation that, when applying a one-stage
ipd meta-analysis of randomized controlled trials (rcts) with a 1:1 treatment:control allocation ratio and a continuous
outcome, the meta-analyst can choose either a stratified intercept or random intercepts model when restricted maximum
likelihood (reml) is used for estimation.4 that is, the statistical properties of the estimate of summary treatment effect,
the 95% confidence interval for the summary treatment effect, and the estimate of between-study variance of treatment
effects are all very similar regardless of whether a stratified intercept or random intercepts model is used. however, when
using maximum likelihood (ml) estimation, there was less downward bias in the estimate of between-study variance
of the treatment effect when using random intercepts rather than a stratified intercept, due to fewer parameters being
estimated.5,6 consequently, for ml estimation, the coverage of 95% confidence intervals for the summary treatment effect
was better (ie, closer to 95%) when random intercepts rather than a stratified intercept was used.
a recommendation to use random study intercepts, rather than a stratified study intercept, for one-stage ipd
meta-analysis models that require ml estimation (eg, for binary outcomes) may be disconcerting to some readers. in
particular, the use of random intercepts is often considered inappropriate on philosophical grounds, because it allows
across-trial information to inform the control group results, which may compromise randomization within each trial and
bias the summary treatment effects. there is the potential for bias in situations when the allocation ratio is associated
with the overall mean outcome (risk). in such situations the introduced bias will often be small,7 but may be substantial in extreme situations. for example, white et al8 show that when using extreme hypothetical binary outcome data
in a network meta-analysis setting, there can be large potential bias in the summary treatment effect when using a random intercept; the summary treatment effect was an odds ratio of 1.35 when the truth was 1. another issue is that it is
usually recommended to allow the random effects on the intercept and treatment effect to be correlated; however, this
might then allow the baseline risk to contribute toward the summary treatment effect estimates. as an extreme example,
the model could incorporate randomized trials alongside observational studies that only provide information about the
control (untreated group); the latter will then contribute (via the correlation) toward the summary treatment effect.
in this article, we aim to build on previous work,4,5,9-11 and to improve ml estimation of the stratified intercept model
so that it is at least comparable to that of the random intercepts model. specifically, we focus on an ipd meta-analysis of
randomized trials, and evaluate whether the coding of the treatment variable is important toward ml estimation properties. our previous simulations focused on situations where the treatment:control allocation ratio was 1:1 in each study
in the meta-analysis, and found a +0.5/−0.5 coding for the treatment variable (instead of 1/0) substantially improved ml
estimation performance.5 subsequently, we realized that a +0.5/−0.5 coding is the same as using the 1/0 coding minus
the proportion in the treatment group (ie, using 1/0 - 0.5) when the treatment:control allocation ratio is 1:1 this raises the
question about whether ipd meta-analysts should always use a +0.5/−0.5 coding of treatment in their one-stage models,
or whether the choice should be context specific, especially when the actual allocation ratio is not 1:1. therefore, in this
article our primary aims to assess ml estimation performance of the stratified intercept model when using four treatment
coding options:
• the traditional 1/0 coding
• the +0.5/−0.5 coding recommended by jackson et al5
• a coding of 1/0 minus the average proportion of participants in the treatment group in all trials (ie, an “overall
centering” approach)
• a coding of 1/0 minus the proportion of participants in the treatment group in that trial (ie, a “study-specific centering”
approach)
we evaluate which coding approach gives the smallest bias in the estimates of the summary treatment effect and
between-study variance of treatment effects. two additional objectives are also considered: (a) whether the coverage of
95% confidence intervals for the summary treatment effect are improved by using a t-distribution rather the standard
z-based (wald) approach, and (b) whether reml estimation of the pseudo likelihood leads to better performance than
ml estimation of the exact likelihood for one-stage ipd meta-analysis models of binary outcomes.
the structure of this article is as follows. in section 2, we introduce one-stage ipd meta-analysis models with a stratified intercept, for both continuous and binary outcomes. in section 3 we evaluate the four treatment coding options in
an extensive simulation study, for both continuous and binary outcomes. section 4 provides real examples and section 5
concludes with discussion.
2 one-stage model specifications and treatment coding
options
in this section we introduce one-stage ipd meta-analysis models for continuous and binary outcomes with either a
stratified intercept or random intercepts model, and then define the various treatment coding options.
2.1 continuous outcomes
consider that ipd have been obtained from i = 1 to k rcts, each of which has a parallel-group design investigating
whether a treatment is effective (vs a control or existing treatment) at improving a continuous outcome. the treatment
effect then relates to the mean difference (at some follow-up time) in the continuous outcome value between the treatment
and control groups. suppose that there are ni participants in trial i, and that yfij represents the end-of-trial final (f)
continuous outcome value for participant j in trial i. let the treatment group variable be denoted by xij, with coding
options (such 1/0 for treatment/control groups) discussed further in section 2.3.
in this situation, a one-stage ipd meta-analysis with a stratified study intercept (ie, a separate intercept per study to
account for within-study clustering of individuals) and assuming between-study heterogeneity of the treatment effect,
can be written as follows:
y𝐹 𝑖𝑗 = 𝛼i + (𝜃 + ui)x𝑖𝑗 + e𝑖𝑗
ui ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
i ). (1)
here the outcome value (yfij) is assumed normally distributed in each study conditional on the included covariates
(here just xij, but additional covariates could also be included such as the baseline value of the continuous outcome12,13).
there are k distinct intercept terms (𝛼i) and the main parameter of interest is 𝜃, which denotes the summary (average)
treatment effect from the included studies. the true treatment effect in each study is assumed drawn from a normal
distribution with mean 𝜃 and between-trial variance 𝜏2, and 𝜎2
i denotes the study-specific residual variance which is
assumed normally distributed (this could also be stratified by treatment group, but we do not consider this here). the
choice of coding of xij (eg, 1/0 or +0.5/−0.5 for treatment/control groups) does not alter the interpretation of 𝜃, our key
parameter of interest; however, it does change interpretation of the 𝛼i and may have implications on estimation of 𝜏2, as
discussed by jackson et al.5 we evaluate this later in our simulations.
alternatively, a random intercepts model could be specified. for example, allowing for between-study correlation
between the random effects of the intercept and the treatment effect, the model can be written as follows:
y𝐹 𝑖𝑗 = (𝛼 + u1i)+(𝜃 + u2i)x𝑖𝑗 + e𝑖𝑗
(
u1i
u2i
)
∼ n
(𝜏2
𝛼 𝜏12
𝜏12 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
i ) (2)
the parameter terms are as defined for model (1), except now the study-specific intercepts are also assumed drawn
from a normal distribution, with mean of 𝛼 and between trial variance of 𝜏2
𝛼 , and the two random effects (u1i and u2i) are
allowed to be correlated through the covariance term 𝜏12. allowing for correlation imposes a between-study relationship
of control group mean response and treatment effect, which might be viewed as controversial (see discussion). to avoid
this, 𝜏12 might be set to zero, but then the coding of treatment is potentially crucial (see later).11
in a frequentist framework, models (1) and (2) are typically fitted using reml estimation. following estimation, a 95%
confidence interval for 𝜃 is conventionally derived using a (wald) z-based method (𝜃
̂ ± (1.96 × s.e.(𝜃
̂))), but other options
include the satterthwaite and kenward-roger approaches, which replace 1.96 with the critical value of a t-distribution
with a particular denominator degrees of freedom.4,14 in this article, we also consider using 𝜃
̂ ± (tk−1,0.975 × s.e.(𝜃
̂)), where
k is the number of studies in the ipd meta-analysis. for brevity, we refer to this as the t-based confidence interval
approach.
2.2 binary outcomes
now let us consider a binary outcome (eg, dead or alive 1 month after surgery), such that yij is 1 for individuals with
an event and 0 for those without an event. we use a logit-link function, such that our one-stage models have a logistic
regression modeling framework (as suggested by simmonds and higgins15) and the treatment effect is measured by a log
odds ratio. again let the treatment group variable be denoted by xij, with coding options (such 1/0 for treatment/control
groups) discussed further in section 2.3.
in this situation, the stratified intercept model can be written as,
y𝑖𝑗 ∼ bernoulli(𝜋𝑖𝑗)
logit(𝜋𝑖𝑗) = 𝛼i + (𝜃 + ui)x𝑖𝑗
ui ∼ n(0, 𝜏2
), (3)
where 𝜋ij is the event probability for individual j in study i. there are k distinct intercept terms, 𝛼i, and the model parameter 𝜃, denotes the summary (average) treatment effect (log odds ratio). the true treatment effects are again assumed
drawn from a normal distribution with mean 𝜃, and between-trial variance 𝜏2. as in models (1) and (2), adjustment for
baseline covariates is also possible.
alternatively specifying a random intercepts model, and allowing for between-study correlation of control group risk
and treatment effect, we have5,11,16:
y𝑖𝑗 ∼ bernoulli(𝜋𝑖𝑗)
logit(𝜋𝑖𝑗)=(𝛼 + u1i)+(𝜃 + u2i)x𝑖𝑗
(
u1i
u2i
)
∼ n
(𝜏2
𝛼 𝜏12
𝜏12 𝜏2
)
. (4)
the parameter terms are as defined for model (3), except now the study-specific intercepts are also assumed drawn
from a normal distribution, with mean of 𝛼 and between trial variance of 𝜏2
𝛼 , and the two random effects (u1i and u2i)
are allowed to be correlated through the covariance term 𝜏12. as discussed for model (3), setting 𝜏12 to zero assumes no
between-study correlation, but then treatment coding is more important (see below).
models (3) and (4) are typically fitted using ml estimation, via a numerical integration approach such as (adaptive)
gaussian quadrature. unfortunately, there is no natural extension from ml to reml estimation for the exact likelihood
defined by a glmm of a binary, ordinal or count outcome, as the model residuals cannot be estimated separately from
the main parameters. thus ml estimation is generally the default frequentist estimation choice for ipd meta-analyses
of noncontinuous outcomes, for which downward bias in between-study variance estimates and low coverage of confidence intervals is a strong concern, especially with 10 or fewer studies in the ipd meta-analysis. however, wolfinger
and o'connell suggest using a pseudo-likelihood approximation of the exact likelihood,17 where the outcome response
variable is transformed to an approximately linear scale. this allows reml to be used for glmms of noncontinuous
outcomes, but at the expense of an approximate likelihood. this may be an acceptable trade-off in some situations, to
improve between-study variance estimates and confidence interval coverage. we will investigate this in section 3.
2.3 coding of treatment
when a treatment variable is entered into a regression model as a covariate, it is typical practice for researchers to code
the variable as 1/0 for treatment/control. however, a coding of +0.5/−0.5 has also been used by others, such as morris
et al9, tudur-smith et al,18 and turner et al.11 for random intercepts models (2) and (4), which allow for between-study
correlation in control group risk and treatment effect, the choice of treatment coding should be unimportant, because one
can show mathematically a one-to-one correspondence of the model parameters with one coding and the model parameters with another coding, so that the maximized likelihood is the same.5,11 however, if the between-study correlation is
set to zero, then turner et al suggest a +0.5/−0.5 coding is crucial; in particular, for model (4) this ensures the variance of
the log-odds in control group patients is modeled as equal to that in intervention group,11 as otherwise with a 1/0 treatment/control coding the variation for the intervention group is modeled as greater than or equal to the variance for the
control group.
for stratified intercept models (1) and (3), jackson et al5 suggest a coding of +0.5/−0.5 improves ml estimation.
however, they mainly evaluated situations where the treatment:control allocations were 1:1 in each trial. therefore, in
the following section we address unequal treatment:control allocations, and examine whether the following alternative
treatment coding options improve ml estimation even further:
• overall centering: a coding of 1/0 minus the average (unweighted across trials) of the proportion of participants in the
treatment group in each trial. for example, if there are 10 studies within an ipd meta-analysis, with five of those
studies having 70% of participants in the treatment group and the other five having 50% in the treatment group, then
the unweighted average proportion treated per trial is 60%. in this situation, the treatment coding is 1/0 - 0.6 in all
trials, and thus individuals in the treatment group are coded as +0.4, and those in the control group are coded as −0.6.
• study-specific centering: a coding of 1/0 minus the study-specific proportion of participants in the treatment group.
for example, for individuals in a trial where 40% of participants are in the treatment group, then an individual in the
treatment group will be coded as 1−0.4 = +0.6, and an individual in the control group would be coded as 0−0.4 = −0.4.
however, if another trial had 30% in the treatment group, then individuals in the treatment and control groups would
be coded as +0.7 and −0.3, respectively.
3 simulation study for continuous and binary outcomes
we now use a simulation study to compare the performance of ipd meta-analysis models with stratified intercept or random intercepts, first for continuous outcomes and then for binary outcomes. we have three research
questions:
q1: does an “overall centering” or “study-specific centering” coding of the treatment variable improve ml estimation
of the between-study variance of the treatment effect, over and above the +0.5/−0.5 coding proposed by jackson et al,
for the stratified intercept models (1) and (3) in situations where one or more trials have an unequal treatment:control
allocation ratio?
q2: does a t-based approach to confidence interval derivation improve coverage compared with a standard z-based
(wald) approach, for both stratified intercept and random intercepts models?
q3: does reml estimation of the pseudo-likelihood perform better than ml estimation of the exact likelihood for
binary outcome models (3) and (4)?
3.1 continuous outcome simulation study
in our first simulation, we extend the simulation study of legha et al for one-stage ipd meta-analysis models of continuous
outcomes to the situation when there are varying treatment:control allocation ratios.
3.1.1 methods
full details of the simulation methods are provided in supplementary material s1. briefly, we simulated ipd according
to model (2), with a 1/0 treatment/control coding and assuming no correlation of the pair of random effects (ie, 𝜏12 = 0)
for simplicity to avoid a relationship between control group response and treatment effect. a range of different simulation scenarios were considered (see supplementary material s1), each involving varying treatment:control allocation
ratios (randomly drawn from a u[0.1,0.9] distribution), and varying the number of studies, number of participants, and
magnitude of between-study variance of treatment effects.
one thousand ipd meta-analysis datasets were generated for each scenario. to each we fitted the random intercepts
model (2) used to generate the data; that is, model (2) using a 1/0 treatment coding option, whilst forcing 𝜏12 to be 0 (its correct value) to avoid estimation issues that often arise when estimating between-study correlations.19 then we also fitted
the stratified intercept model (1) for each of the four treatment coding options. both ml and reml estimation were examined, alongside z-based and sattherwaite confidence intervals. although reml is the preferred estimation method for
one-stage ipd meta-analyses of continuous outcomes, we also considered ml estimation to inform subsequent extension
to one-stage ipd meta-analyses of binary outcomes, for which ml estimation is usually the default (see section 3.2).
performance was summarized by the bias in the summary treatment effect estimate (𝜃
̂), the coverage of 95% confidence
intervals for the summary treatment effect, and the bias in the between-study variance (𝜏̂2) of the treatment effects. for the
latter, we calculated percentage difference between the estimated and true between-study variance of the treatment effect
(ie, 100 × (𝜏̂2 − 𝜏2)∕𝜏2)), and report the mean and median of these percentages across the 1000 simulations. distributions
of 𝜏̂2 were extremely skewed across each set of 1000 results, and so presentation of median values helps indicate estimation
problems in addition to the more formally correct mean bias.
3.1.2 results when using ml estimation
the summary treatment effect estimates were approximately unbiased for all scenarios, modeling approaches, and treatment coding options. however, in most scenarios there was considerable downward bias in the estimated between trial
variance (𝜏̂2) of the treatment effects (see figure 1, and also see supplementary material tables s2(a), (b), and (c)). the
bias was largest when using the stratified intercept model (1) with a 1/0 treatment/control coding for the treatment variable, and the bias was least when using the “study-specific centering” coding. for example, under the stratified intercept
model and setting b1-a1 (which involves five trials where the number of participants per trial was u(30, 1000), the median
downward bias of 𝜏̂2 was 100% with 1/0 treatment/control coding, which improved to 65.4%, 61.4%, and 49.5%, with
+0.5/−0.5 coding, an “overall centering” coding, and a “study-specific centering” coding, respectively.
coverage of 95% confidence intervals for the summary treatment effect was also closest to 95% when using the
“study-specific centering” approach. for example, in scenario b2 the stratified intercept model had a z-based confidence
interval coverage of 79.70%, 84.20%, and 87.10% for the 1/0, +0.5/−0.5 and “study-specific centering” codings, respectively
(table s2(b)).
crucially, “study-specific centering” of the treatment variable not only improves ml estimation of stratified intercept
model (1), but also makes it comparable (in terms of bias and coverage) to ml estimation of the data generating model (3)
(ie, the random intercepts model with 1/0 coding). however, despite having the best performance, both approaches still
have downward bias in 𝜏̂2 and gave z-based confidence interval coverage <95% for most scenarios. this appears worst in
situations where the allocation ratio was most unbalanced (eg, see results in table s2(a) where treatment prevalence was
90% in all studies).
3.1.3 results when using reml estimation
reml estimation also gives approximately unbiased summary treatment effect estimates for all scenarios, modeling
approaches, and treatment coding options. it also reduces the downward bias in the ml estimate of 𝜏̂2 for all modelling
options (although the downward bias was not removed entirely). furthermore, unlike for ml estimation, the choice of
treatment coding becomes irrelevant when fitting the stratified intercept model (1) using reml estimation. the median
(or mean) bias in 𝜏̂2 was generally very similar regardless of the coding used (see supplementary material table s2[d]), as
were the summary treatment effect estimates and their confidence intervals. therefore, when using reml estimation of
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
random, 1/0:
stratified, 1/0:
stratified, +0.5/–0.5:
stratified, average centred coding:
stratified, study-specific centred coding:
-100 -50 0 -100 -50 0 -100 -50 0
base case a1 a2
b1 a1, b1 a2, b1
b2 a1, b2 a2, b2
b3 c1 c2
d1 d2
median % bias in τ2
figure 1 the median percentage bias of the between-trial variance of treatment effects (𝜏̂2) for ml estimation of the stratified
intercept model (1) and random intercepts model (2)*, for the continuous outcome simulation scenarios** described in table s1, allowing for
unequal treatment:control allocation ratios in each trial in the ipd meta-analysis from 10% to 90%. circular points denote the estimated
percentage bias, and a horizontal line is drawn from each estimate to the ideal value of 0. ipd, individual participant data; ml, maximum
likelihood. *the random intercepts model refers to the data generating model, which was model (2) but with treatment coded as 1/0 and the
between-study correlation assumed zero. **scenarios are labeled “base case,” “a1,” “a2,” and so on. for explanation of each scenario setting,
see table s1. briefly, the base case was k =10, ni =100, 𝜃 = −9.66, 𝜏2 =7.79. then the other scenarios made changes of: a1: k =5; a2:
k =20; b1: ni ∼ u(30,1000); b2: k =10, ni ∼ u(30,100) for trials 1 to 5, ni ∼ u(900,1000) for trials 6 to 10; a1,b1: k =5 and ni ∼ u(30,1000);
a2,b1: k =20 and ni ∼ u(30,1000). a1,b2: ni ∼ u(30,100) for trials 1 and 2, ni ∼ u(900,1000) for trials 3 to 5. a2,b2: k =20, ni ∼ u(30,100) for
trials 1 to 10, ni ∼ u(900,1000) for trials 11 to 20; b3: ni ∼ u(30,100); c1: halving variance of intercept; c2: doubling variance of intercept; d1:
𝜏2 =3.9; d2: 𝜏2 =15.6
the stratified intercept model (1), “study-specific centering” of the treatment variable does not improve performance over
traditional 1/0 coding, and the coding is irrelevant. results from the stratified intercept model were also comparable to
reml estimation of the random intercepts model (2) with 1/0 coding. coverage of z-based 95% confidence intervals for
the summary treatment effect were generally too low, but improved close to 95% when using the satterthwaite approach
(as also shown by legha et al4).
3.2 binary outcome simulation study
in our second simulation study we extend the simulations of jackson et al for ipd meta-analysis of binary outcomes,5 to
evaluate the ml estimation performance of random intercepts model (4) with a 1/0 coding and the stratified intercept
table 1 simulation study scenarios of jackson et al5 that were extended in this article, by allowing for unequal treatment:control
allocation ratios in each trial in the ipd meta-analysis
data generation
scenario, as labeled
by jackson et al k 𝝉2
number of
participants in the
treatment group (n)
number of participants
in the control groupa
baseline log-odds
of the event in the
control group (loc)
1 10 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
3 10 0.168 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
4 3 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
5 5 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
6 20 0.024 n ∼ u(50, 500) n loc n(logit[0.2], 0.32)
7 10 0.024 n ∼ u(10, 100) n loc n(logit[0.2], 0.32)
note: all scenarios were performed with 𝜃 =0 and 𝜃 =log (2). for further details see section 6 in jackson et al.
abbreviation: ipd, individual participant data.
ashows the number used in the control group of the jackson et al simulation. however, in our simulations we changed the number in the control group of a
trial to ensure the treatment group prevalence was one of the following (chosen at random): 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, or 90%.
model (3) for each of the four treatment coding options. a variety of simulation settings are considered, allowing for
unequal treatment:control allocation ratios in each trial within the ipd meta-analysis. we examine the estimate of the
between-study variance (𝜏̂2) of the treatment effect, the estimate of the summary treatment effect (𝜃
̂), and the coverage
of the 95% confidence interval for the summary treatment effect.
3.2.1 methods
we chose the simulation scenarios from jackson et al5 that most closely correspond to those used for our continuous
outcome simulations; that is, jackson et al scenario settings 1, 3, 4, 5, 6, and 7, where the mean risk in the control group
was 20%. these are summarized in table 1, and cover a range of settings that vary in terms of the number of trials,
number of participants per trial, and heterogeneity of parameters. we now consider these scenarios when we vary the
treatment:control allocation ratio in the simulated trials. the omitted jackson et al scenarios mainly corresponded to
either zero or extremely large between-study heterogeneity, or a different logit data generating mechanism for the control
group.
for each scenario, 1000 simulated ipd meta-analysis datasets were created from model (4), with a 1/0 treatment/control coding, but assuming no correlation of the pair of random effects (ie, 𝜏12 = 0) for simplicity to avoid a
relationship between baseline risk and treatment effect. within each scenario we allowed for potential unequal treatment:control allocation ratios in each trial, by randomly drawing from a u(0.1,0.9) distribution. each scenario was
undertaken assuming the summary odds ratio of 1 for the treatment effect (ie, 𝜃 = ln (1) =0), and also assuming a summary odds ratio of 2 (ie, 𝜃 = ln (2)). of note, the chosen magnitude of heterogeneity in setting 1 corresponded to a mean
i2 of about 25% in the meta-analysis datasets simulated.
to each simulated ipd meta-analysis dataset, ml estimation was used to fit the random intercepts model used to
generate the data (ie, model (4) with a 1/0 coding option and forcing 𝜏12 = 0 to avoid estimation issues that often arise
when estimating between-study correlation19), and the stratified intercept model (3) for each of the four treatment coding options. we used the lme4 r package (version 1.1-17),20 with ml estimation undertaken using adaptive gaussian
quadrature, with seven quadrature points.
across all 1000 results obtained for each scenario, the median percentage bias of the between-study variance was calculated, as well as the mean bias of the summary treatment effect, and the coverage of 95% confidence intervals for the
summary treatment effect. the latter was derived as the proportion (across the 1000 results) of 95% confidence intervals that contained the true summary effect. confidence intervals were calculated using the standard z-based method
(𝜃
̂ ± (1.96 × s.e.(𝜃
̂))) and also by the t-based approach that replaces 1.96 with the critical value of a t-distribution with
k−1 degrees of freedom (ie, use 𝜃
̂ ± (tk−1,0.975 × s.e.(𝜃
̂))).21 given the 1000 simulations in each scenario, if coverage is
truly 95%, then we would expect to observe an estimated coverage between 93.4% and 96.2%.
finally, we repeated our simulations using reml estimation of the pseudo likelihood. given that all treatment coding options gave similar performance for reml estimation of continuous outcomes (section 3.1.3), we only considered
the 1/0 treatment variable coding when fitting the stratified intercept and random intercepts models. reml estimation
was undertaken using mlwin, via the runmlwin package within stata.22 we obtained parameter estimates using reml
estimation of the first-order marginal quasi-likelihood linearization of the likelihood (“mql1” option). this linearization approach is noted in the runmlwin help file as being the most stable and fastest to converge, and so was deemed
sensible for our large simulation study. in practice, more accurate (though potentially less stable) estimation options
such as second-order penalized quasi-likelihood linearization (“pql2” option) could be used, with the estimates from the
first-order approach used as initial values (see applied examples in section 4 for further discussion on this).
3.2.2 results when using ml estimation of the exact likelihood and standard z-based
confidence intervals
simulation results for the mean bias of the summary treatment effect estimate (𝜃
̂) across all scenarios for a true treatment
effect of 𝜃 =0 and of 𝜃 =1 are shown in supplementary tables s3 and s4, respectively. the bias of 𝜃
̂was generally negligible
in all cases.
figures 2 and 3 show the median percentage bias in the between-study variance of the treatment effect (𝜏̂2) for the
𝜃 =0 and 𝜃 = ln (2) scenarios, respectively (and also supplementary tables s5 and s6 show the mean percentage bias).
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
-100 -50 0 -100 -50 0
1 3
4 5
6 7
median % bias for τ2
figure 2 the median percentage bias of the between-trial variance of treatment effects (𝜏̂2)for ml estimation (exact likelihood) and
reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for simulation scenarios**
where 𝜃 =0 and allowing for random treatment prevalences (10%-90%) for each study within the ipd meta-analysis. circular points denote
the estimated percentage bias, and a horizontal line is drawn from each estimate to the ideal value of 0. ipd, individual participant data; ml,
maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model, which was
model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. **see table 1 for full details of the scenario
corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all settings also allow the treatment
prevalence for a particular study within a meta-analysis to vary, whereby this is selected from u(0.1,0.9) and then rounded to the nearest 0.1
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
-100 -50 0 -100 -50 0
1 3
4 5
6 7
median % bias for τ2
figure 3 the median percentage bias of the between-trial variance of treatment effects (𝜏̂2)for ml estimation (exact likelihood) and
reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for simulation scenarios**
where 𝜃 = ln (2) and allowing for random treatment prevalences (10%-90%) for each study within the ipd meta-analysis. circular points
denote the estimated percentage bias, and a horizontal line is drawn from each estimate to the ideal value of 0. ipd, individual participant
data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model,
which was model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. **see table 1 for full details of the
scenario corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all settings also allow the
treatment prevalence for a particular study within a meta-analysis to vary, whereby this is selected from u(0.1,0.9) and then rounded to the
nearest 0.1 decimal place
as observed for continuous outcomes, the results show that using the stratified intercept model (3) with a 1/0 treatment/control coding gives the most downwardly biased estimates of the between trial variance of treatment effect. often
the median downward bias was 100%, and mean downward bias typically between 40% and 80%. this downward bias was
generally reduced when using either a +0.5/−0.5 treatment coding or the “overall centering” treatment coding, and by a
similar amount. however, the downward bias was smallest when using a “study-specific centering” coding. for example,
under setting 6 (involving 20 trials), the median downward bias of between study variance estimates from stratified intercept model (3) was 100% with a 1/0 treatment/control coding; this was slightly reduced to 80.2% and 79.5% with +0.5/−0.5
coding or “overall centering” coding, but considerably reduced to 28.7% when using the “study-specific centering” coding.
the mean downward bias shows a similar pattern; this was 80.8%, 44.5%, 43.3%, and 7.85% when using the 1/0, +0.5/−0.5,
“overall centering,” and “study-specific centering,” respectively (table s5).
for stratified intercept model (3), the reduction in downward median and mean bias of 𝜏̂2 when using a “study-specific
centering” coding of treatment also improves upon the z-based coverage of 95% confidence intervals for the summary
treatment effect, as shown in figures 4 and 5 (and also supplementary tables s5 and s6) for the 𝜃 =0 and 𝜃 = ln (2)
scenarios, respectively. for example, in setting three of figure 4, the coverage of z-based confidence intervals from the
1/0 coding is 84%, but this improves to 91% when using the “study-specific centering” coding. furthermore, the z-based
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
0.85 0.90 0.95 0.85 0.90 0.95
1 3
4 5
6 7
z-based t-based
ci coverage for θ
figure 4 coverage (proportion) of z-based and t-based 95% confidence intervals for the summary treatment effect for ml estimation
(exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for
simulation scenarios** where 𝜃 =0 and allowing for random treatment prevalences (10%-90%) for each study within the ipd meta-analysis.
circular points denote the estimated coverage, and a horizontal line is drawn from each estimate to the ideal value of 0.95. ipd, individual
participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data
generating model, which was model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. ** see table 1 for full
details of the scenario corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all settings
also allow the treatment prevalence for a particular study within a meta-analysis to vary, whereby this is selected from u(0.1,0.9) and then
rounded to the nearest 0.1
coverage is always best (ie, closest to 95.0%) when using the “study-specific centering” coding, and also worst when using
the 1/0 coding; the coverage of the +0.5/−0.5 and “overall centering” coding are similar and fall in between the z-based
coverage when using the 1/0 coding and “study-specific centering” coding.
crucially, “study-specific centering” of the treatment variable not only improves ml estimation of stratified intercept
model (3), but also makes it comparable to ml estimation of the random intercepts model (4) with 1/0 coding (ie, bias and
coverage are very similar). however, despite having the best performance, both approaches still have (often considerable)
downward bias in 𝜏̂2, and subsequently z-based coverage is less than 95% for most scenarios. the coverage appears closest
to 95% in the scenario 7 setting (see figure 5); this can be explained by the studies being smaller in this setting, and so
the within-study variances dominate the total variability (ie, i2 is small), and so any downward bias in the between-study
variance is less impactful.
3.2.3 results when using ml estimation and t-based confidence intervals
for all treatment coding options, and for both stratified intercept and random intercepts models, coverage of 95% confidence intervals for the summary treatment effect was generally improved (ie, moved closer to 95%) by using the
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
intercept estimation treatment coding
stratified ml 1/0
stratified ml +0.5/–0.5
stratified ml avg. centering
stratified ml study-specific centering
random ml 1/0
stratified reml 1/0
random reml 1/0
0.85 0.90 0.95 0.85 0.90 0.95
1 3
4 5
6 7
z-based t-based
ci coverage for θ
figure 5 coverage (proportion) of z-based and t-based 95% confidence intervals for the summary treatment effect for ml estimation
(exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model (3) and random intercepts model (4)*, for
simulation scenarios** where 𝜃 = ln (2) and allowing for random treatment prevalences (10%-90%) for each study within the ipd
meta-analysis. circular points denote the estimated coverage, and a horizontal line is drawn from each estimate to the ideal value of 0.95.
ipd, individual participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to
the data generating model, which was model (4) with treatment coded as 1/0 and the between-study correlation assumed zero. **see table 1
for full details of the scenario corresponding to the number shown. true value for 𝜏2 is 0.024, except in setting 3 where 𝜏2 equals 0.168. all
settings also allow the treatment prevalence for a particular study within a meta-analysis to vary, such that it is selected from u(0.1, 0.9) and
then rounded to the nearest 0.1
t-based approach to deriving confidence intervals based on the t-distribution with k−1 degrees of freedom (see tables
s5 and s6). for example, in setting 1 when using stratified intercept model (3) with “study-specific centering” of the
treatment variable, the coverage was 90.3% and 94.6% for z-based and t-based confidence intervals, respectively. only
in setting 4, where the number of studies was only 3, is the t-based approach a concern as the coverage is close to
100% and so too high; although arguably this is still preferable to the under-coverage from the corresponding z-based
approach.
3.2.4 results when using reml estimation of the pseudo likelihood
when using reml estimation of the pseudo-likelihood with a 1/0 treatment coding, results are shown in table s7 for all
scenarios. stratified intercept model (3) and random intercepts model (4) have similar performance in all scenarios, and
there is negligible or very small bias in the summary treatment effect estimates.
compared with ml estimation of the exact likelihood, reml estimation of the pseudo likelihood improved the
between-study variance estimate (𝜏̂2), for both stratified intercept and random intercepts models. for example, in setting 1 and 𝜃 = ln (2), when using stratified model (3) the median (mean) bias in 𝜏̂2 was −20.93% (10.72%) using reml
box 1 a summary of the key findings based on our simulation study results and applied examples
• for ml estimation of a one-stage ipd meta-analysis model with a stratified intercept, a “study-specific centering” coding of the treatment variable reduces downward bias of between-study variances and improves coverage
of 95% confidence intervals for the summary (treatment) effect, as compared with other treatment coding options
such as 1/0 for treatment/control. supplementary material s8 also shows this mathematically for a simple case
where all studies in the ipd meta-analysis are of the same size.
• reml is better than ml estimation for continuous outcomes. for binary outcomes, the simulations do not
suggest an important difference in terms of bias and coverage of confidence intervals for the summary treatment
effect when using reml estimation of the pseudo likelihood compared with ml estimation of the exact likelihood. however, in most scenarios reml reduces the downward bias in the between-study variance estimates,
which may be important when the focus is on predictive inferences (eg, the predicted treatment effect in a new
study23). thus both ml (exact likelihood) and reml (pseudo likelihood) estimation may be important to consider
for binary outcomes.
• for either ml or reml estimation, coverage of 95% confidence intervals for the summary treatment is
generally too low when using a z-based approach. improvements are generally made for reml estimation of continuous outcomes by using satterthwaite or kenward-roger approaches, and for ml or pseudo reml estimation
of binary outcomes by using ̂θ±(tk−1,0.975 × s.e.(̂θ)) where k is the number of studies in the meta-analysis.
• for continuous outcomes, reml estimation is recommended over ml estimation for either stratified intercept or random intercepts models (see work of legha et al4), as it improves estimates of between-study variances
(though some downward bias may remain), whilst having negligible bias in the summary treatment effect
estimate and does not depend on the treatment coding chosen.
• for binary outcomes, when fitting a stratified intercept model both ml estimation of the exact likelihood
(with “study-specific centering” treatment coding) and reml estimation of the pseudo likelihood (with 1/0 treatment coding) give negligible bias in the summary treatment effect estimate, and their coverage of 95% confidence
intervals is close to 95% when using the t-based approach (unless the number of studies is less than 5). in addition,
reml estimation of the pseudo likelihood often has less downward bias of between-study variance estimates
than ml estimation.
• for binary outcomes, reml estimation of the pseudo likelihood may be unstable in sparse data situations,
such as when most studies in the ipd meta-analysis are small (in terms of participants or events).
• the decision to use random study intercepts, rather than a stratified study intercept, depends on whether
the researcher is willing to borrow information about control group risk across studies and/or assume a
between-study relationship of control risk and treatment effect.
estimation (with a 1/0 treatment/control coding) compared with −63.08% (−19.40%) when using ml estimation (with a
“study-specific centering coding” for treatment).
the coverage of confidence intervals from reml estimation were closest to 95% when using the t-based approach;
for example, in setting (1) with 𝜃 = ln (2), the coverage from z-based and t-based confidence intervals was 91.6% and
93.3%, respectively. indeed, t-based coverage was consistently good in all settings, generally between 93% and 97%, and
comparable to that when using ml estimation with “study-specific centering” and the t-based approach.
3.3 summary of our key findings
a summary of key findings from the simulation study is shown in box 1.
4 illustration of key findings in applied examples
we now illustrate the key findings in applied examples, which focus on binary outcomes. example 2 has data similar to
that used in the simulation studies, whilst example 1 considers more sparse data.
table 2 summary of the ipd
from seven trials examining the effect
of hormone replacement therapy on
the incidence of heart disease, as
reported by simmonds and higgins15
number of women number of cardiovascular disease events
study control treatment control treatment
1 174 701 0 5
2 14 15 1 0
3 16 15 0 1
4 20 20 1 1
5 26 29 0 1
6 84 84 3 1
7 66 68 0 3
abbreviation: ipd, individual participant data.
table 3 results from ml estimation of models (3) and (4) when fitted to the ipd summarized in table 2
model
treatment
coding
summary
treatment
effect, 𝜽̂ 95% ci: z-based 95% ci: t-based
between-study
(co)variances
stratified intercept
model (3)
1/0 0.56 −0.53, 1.64 −0.80, 1.91 𝜏̂2 = 0
“study-specific
centering”
0.65 −0.69, 1.99 −1.02, 2.32 𝜏̂2 = 0.57
random intercepts model (4) 1/0 0.55 −1.10, 2.21 −1.51, 2.62 𝜏̂2 = 0.74
𝜏12 = −0.81
𝜏2
𝛼 = 1.16
abbreviations: ipd, individual participant data; ml, maximum likelihood.
4.1 example 1: hormone replacement therapy and incidence of heart disease
simmonds et al combined ipd from seven trials examining the effect of hormone replacement therapy compared with
control on the incidence of heart disease.15 the binary outcome data are sparse (table 2), such that the number of events is
few in all studies due to the outcome being rare, and some groups have zero events. in this situation, a traditional two-stage
ipd meta-analysis (ie, estimating the treatment effect and its variance in each study separately, and then pooling the
results in an inverse-variance weighted meta-analysis) is problematic. the treatment effect cannot be estimated in every
study unless a continuity correction is applied in those studies with a zero event; further, the assumption in the second
stage that study-specific treatment effect estimates are normally distributed with known variances may be inappropriate.
a one-stage approach avoids these issues by analyzing the ipd in a single step, for example, using either stratified intercept
model (3) or random intercepts model (4), and the results are shown in table 3.
4.1.1 stratified intercept model results
one of the studies in the ipd meta-analysis (study 1) had a treatment:control allocation ratio of about 4:1, whereas
other studies have close to a 1:1 allocation ratio. our simulation results showed that in this situation ml estimation of
model (3) is improved by using a “study-specific centering” of the treatment variable, as this reduces downward bias in
between-study variance estimates compared with a traditional 1/0 coding. the ml estimates in table 2 reflect this, as
𝜏̂2 = 0 when using a 1/0 coding and 𝜏̂2 = 0.57 when using “study-specific centering” coding. this led to a noticeably different summary treatment effect of 𝜃
̂ = 0.65 (odds ratio of 1.91) when using “study-specific centering” compared with
𝜃
̂ = 0.56 (odds ratio of 1.74) when using 1/0 coding. confidence intervals were also much wider.
another key finding of the simulation study was that z-based (wald) confidence intervals are generally too narrow, and
t-based confidence intervals are more appropriate. in our example, t-based confidence intervals were also considerably
wider. for example, when using the “study-specific centering” coding for ml estimation of model (3), the confidence
interval for the summary odds ratio was 0.36 to 10.15 when using t-based, compared with 0.59 to 6.77 when using the
z-based approach.
although our simulations suggest reml estimation of the pseudo likelihood for model (3) performs well, the scenarios did not cover sparse data akin to that in table 2. indeed, when applying reml estimation to this example, parameter
estimates were unstable; there were large differences in parameter estimates from first and second-order linearization of
the likelihood, and even when changing the coding of treatment (which should not occur for reml; see section 3.1.3).
therefore, the ml estimates in table 3 based on the exact likelihood are more reliable for this example. of note, these ml
estimates were very different to those obtained from a traditional two-stage ipd meta-analysis with continuity corrections
of +0.5 added to deal with zero cells. the latter gave a summary odds ratio of 1.31 and 𝜏̂2 = 0 from reml estimation,
which are much lower than the ml estimates from the more exact one-stage model using “study-specific centering.”
4.1.2 comparison of results for stratified intercept and random intercepts models
unlike in the simulation study, the ml estimation results for random intercepts model (4) with 1/0 coding were not
comparable to those from stratified intercept model (3) with “study-specific centering” coding (table 3). the reason is that
the simulation did not allow borrowing of information between baseline (control group) risk and treatment effect when
generating the ipd. however, in the applied example model (4) estimated a strong negative correlation of −0.87 between
the pair of random effects, which had a strong influence on the results. furthermore, in our simulation study we knew
that a normal distribution on the baseline risk was appropriate (as we simulated the ipd from this assumption). however,
in this real example we did not know if such an assumption is correct, and it may even compromise randomization in
each trial, especially given the sparse events in the included trials. the approach of stratified intercept model (4) avoids
making any assumptions about the between-study distribution of baseline risk, or the between-study relationship between
baseline risk and treatment effect.
4.2 example 2: diet and lifestyle interventions and health outcomes during pregnancy
in our second example, we used ipd from 36 randomized trials (12 477 women) evaluating the effect of diet and lifestyle
interventions compared with control (usual care) on health outcomes during pregnancy.24 we focused on a subset of 10
trials that recorded the binary outcome of large for gestational age (yes/no). eight studies had approximately 1:1 treatment:control allocation, and the other two had a 2:1 allocation ratio. the outcome risk in the control group varied, but
was about 15% on average, similar to that used in our simulation studies. although the number of participants was reasonably large in most studies, often there are fewer than 10 events in each group (table 4), which again raised doubt as
to the suitability of a traditional two-stage approach.
ml and reml estimates for one-stage model (3) are shown in table 5. the findings again mirror those of the simulation study. when using ml estimation for model (3), the estimated between-study variance was much larger when
using “study-specific centering” (𝜏̂2 = 0.42) rather than 1/0 coding (𝜏̂2 = 0.29) of the treatment variable, and this led
to wider confidence intervals for the summary treatment effect. reml estimation of the pseudo likelihood was quite
stable, with more similar estimates for first- and second-order linearizations of the likelihood. the reml estimates of
between-study variances were larger than the ml estimates (table 5), and this widened confidence intervals for the summary treatment effect. results for model (4) were again somewhat different to model (3), for the same reasons described
in the previous example. for all models, the widest confidence intervals arose when using the t-based rather than z-based
approach.
5 discussion
our simulation study and applied examples identify key findings for estimation of one-stage ipd meta-analysis models,
which are summarized in box 1. there are three major implications. first, for ml or reml estimation of stratified intercept or random intercepts models, z-based (wald) confidence intervals for the summary treatment effect are generally
too narrow; performance is generally improved by using the satterthwaite (or kenward-roger) approaches for continuous outcomes,4,14 or a t-based approach with k−1 degrees of freedom for binary outcomes. second, when using ml
table 4 summary of the ipd
from 10 trials examining the effect of
diet and lifestyle interventions on
large for gestational age
number of women number of babies large for gestational age
study control treatment control treatment
1 120 109 23 22
2 37 33 5 7
3 68 72 7 2
4 33 34 1 2
5 143 136 7 2
6 63 134 5 11
7 1095 1104 154 132
8 47 46 27 5
9 65 130 16 22
10 50 51 11 14
abbreviation: ipd, individual participant data.
table 5 results from maximum likelihood (ml) and restricted maximum likelihood (reml) estimation of models
(3) and (4) when fitted to the individual participant data summarized in table 4
model
estimation
method
treatment
coding
summary
treatment
effect, 𝜽̂ 95% ci:z-based 95% ci:t-based
between-study
(co)variances
stratified
intercept
model (3)
ml exact 1/0 −0.43 −0.89, 0.04 −0.96, 0.11 𝜏̂2 = 0.29
ml exact “study-specific
centering”
−0.40 −0.92, 0.12 −1.00, 0.20 𝜏̂2 = 0.42
reml pseudo 1/0 −0.48 −1.06, 0.09 −1.14, 0.18 𝜏̂2 = 0.54
random
intercepts
model (4)
ml exact 1/0 −0.38 −0.91, 0.16 −1.00, 0.24 𝜏̂2 = 0.43
𝜏12 = −0.29
𝜏2
𝛼 = 0.81
reml pseudo 1/0 −0.38 −0.94, 0.18 −1.03, 0.27 𝜏̂2 = 0.54
𝜏12 = −0.36
𝜏2
𝛼 = 0.92
note: ml estimates-based numerical quadrature of the exact likelihood (with seven quadrature points), and reml estimates based on a
second-order penalized quasi-likelihood linearization with the estimates from the first-order marginal quasi-likelihood linearization used as
initial values.
estimation of a one-stage model with a stratified intercept, a “study-specific centering” coding of the treatment variable
should be chosen, as this reduces the bias in the between-study variance estimate (compared with 1/0 and other coding
options). third, reml estimation reduces downward bias in between-study variance estimates compared with ml estimation, and does not depend on the treatment coding chosen, thus should be used where possible; for ipd meta-analyses
of binary outcomes, this requires reml estimation of the pseudo-likelihood, although it may be unstable when data are
sparse.
5.1 reml vs ml estimation
our simulations of continuous outcomes show that reml is better than ml estimation to improve variance estimates,
which will not be a surprise to most readers. in particular, reml reduces the bias in 𝜏̂2 by adjusting for the total number
of parameters being estimated.6,25,26 however, reml is not an option when using numerical integration of the exact likelihood for a one-stage ipd meta-analysis of a binary outcome, and therefore ml estimation is the most common method
used. in most software packages the default estimation method to fit generalized linear mixed models is ml estimation
via a numerical integration method such as quadrature. therefore, our findings about the importance of “study-specific
centering” coding are most relevant for one-stage ipd meta-analyses of binary outcomes, or other generalized linear
mixed models or frailty models that apply ml estimation. indeed, improving ml estimation by centering covariates has
a reml essence to it, as both approaches aim to disentangle (ie, make uncorrelated) the estimation of main parameters
of interest from other nuisance parameters.
given that considerable downward bias in between-study variance estimates often occurs using ml estimation (even
after “study-specific” treatment coding; see figures 1 to 3), reml estimation of the pseudo likelihood is appealing for
binary outcomes.17 our simulations do not suggest an important difference between reml and ml in terms of bias of the
summary treatment effect estimate and coverage of t-based confidence intervals for the summary treatment effect. however, in most scenarios reml estimation did reduce the median downward bias in the between-study variance estimates,
and therefore we suggest it is the default. however, caution is advised if the data are sparse, such that most studies are small
and have few or even zero events. our simulations did not cover scenarios with sparse data, but previous work suggests
that reml estimation of pseudo likelihood is not accurate in such situations and ml estimation of the exact likelihood
is preferred.27 indeed, reml estimates may be unstable in such situations. instability is evident when first-order and
second-order linearizations of the likelihood lead to very different parameter estimates, or when reparametizations that
should not affect reml (such as centering of covariates) do still change parameter estimates importantly. in our applied
example using the trials in table 2, the data were sparse, and these stability problems were evident when using reml
estimation, and so the ml estimates with “study-specific centering” were deemed more reliable. a bayesian approach
could also be considered in such situations, which would retain the exact likelihood during parameter estimation and
could be combined with empirically based prior distributions for the between-study variance.28,29
our findings warrant further evaluation in a wider variety of settings than those considered in our simulation, but
concur with related work,30 including piepho et al31 who evaluate frequentist network meta-analysis of binary outcomes.
they too show that reml estimation of the pseudo-likelihood, and also the use of the h-likelihood, reduce bias in
between-study variance estimates and give satisfactory coverage rates, especially when the kenward-roger approach is
used to derive confidence intervals. they also consider improving ml estimation by various reparameterizations of the
exact likelihood that aim to mimic the reml approach for linear mixed models. these reparameterizations also reduce
the downward bias in ml estimates, but coverage of summary treatment effects is often too low. our “study-specific centering of covariates” approach showed suitable coverage when using the t-based approach to confidence intervals, and is
potentially simpler to implement in existing software. however, formal comparison of our proposal with those of piepho
et al is needed. thomas et al also compare the performance of one-stage ipd meta-analyses for binary outcomes,30 but do
not find a “meaningful difference” between results from reml of the pseudo likelihood and ml of the exact likelihood.
however, the authors only considered 1:1 treatment:control allocations and implemented a +0.5/−0.5 treatment variable
coding, and thus essentially adapted “study-specific centering” in their setting, which ensures ml estimation performs
well. still, their bias in between-study variances were generally lower using reml, akin to our findings. coverage of
confidence intervals was generally much lower than 95%, but were based on a z-based approach.
comparisons to the traditional two-stage ipd meta-analysis approach are also needed, for which reml is often recommended in the second stage.32 although it assumes normality of the between-study variance of treatment effects,
reml is quite robust to deviations from this assumption.33 we suspect that situations where reml estimation of the
pseudo-likelihood performs well for a one-stage analysis of binary outcomes, a two-stage approach using reml will also
perform well. thomas et al30 recommend one-stage rather than two-stage analyses when data in the ipd meta-analysis
are sparse. we agree with langan et al32 that, especially in ipd meta-analyses of few studies, any heterogeneity variance
estimate “should not be used as a reliable gauge for the extent of heterogeneity in a meta-analysis”.
5.2 implications of our findings
our findings have important consequences, as they allow researchers to apply one-stage ipd meta-analysis models with a
stratified intercept, rather than random intercepts, when using either reml or ml estimation. this is important, as the use
of random intercepts makes distributional assumptions and potentially compromises within-trial randomization, but this
is avoided using a stratified intercept. previously, we recommended random intercepts for one-stage ipd meta-analysis
models fitted using ml estimation,4 as this approach had better estimates of between-study variances due to reducing
the number of parameters. however, our simulations show that the “study-specific centering” makes the stratified intercept model comparable to the random intercepts model (when no borrowing of information across studies is allowed in
control group risk). such comparable performance is despite the data generating mechanism actually being based on the
random intercepts model, and so the simulation set-up might be considered more favorable toward the random intercepts
model.
a potential limitation of stratified intercept models is that they may fail to converge when the number of events are
rare, and in particular when some studies have a zero event in the control group (although this was not an issue for our
first applied example). in that situation, assuming random study intercepts may help, because study-specific intercepts
are not estimated directly, and studies rather contribute toward the estimation of the between-study distribution of intercepts (with the caveat of sharing information about control group risk across trials and thus potentially compromising
randomization within trials). if between-study correlation is included in such random intercepts models (ie, model (4)
is used), then the coding of treatment should not matter. however, if between-study correlation is assumed zero, then
a +0.5/−0.5 coding is recommended. an alternative method is the hypergeometric-normal approach of stijnen et al34
(referred to as model (7) in jackson et al5), which conditions out the study-specific intercepts (thus avoiding their estimation); in the scenarios of the jackson et al simulation study, this approach performs well and comparable to using a
stratified intercept with “study-specific centering”.
5.3 extensions
although we focused on randomized trials, our findings also apply more generally. in particular, any one-stage ipd
meta-analysis model fitted using ml estimation should include covariates (treatments, prognostic factors, adjustment
factors, and so on) centered by their study-specific means; for example, when synthesizing ipd from observational
studies to evaluate risk or prognostic factors for binary or survival outcomes,35 the included factors and any adjustment covariates should be coded with “study-specific centering”. indeed, exposure prevalence (eg, the proportion
of individuals classed as biomarker positive) is likely to be more varied across included covariates in observational
studies (than treatment prevalence in randomized trials), and thus “study-specific centering” will be even more
important.
we focused on parallel-group trials, for which our “study-specific centering” approach centers by the proportion in
the treatment group; equivalently, we could center around the proportion in the control group. we considered binary
variables, but “study-specific centering” should also be used for continuous variables where they are centered by the
mean value. indeed, it generalizes to any covariate: we simply center at the covariate's mean value in each study. for
example, for an ordinal covariate with possible values of 0, 1, 2, and 3, the “study-specific centering” coding is the
original value minus the mean value for all individuals in the same study. in our simulations, we assumed a uniform distribution or fixed treatment prevalences across studies. similarly we generated control groups risks assuming
a normal distribution, and did not allow any correlation between baseline risk and treatment effect. other approaches
could be considered for data generation in further work. we also only consider one treatment and one control group
per study.
our simulations did not allow the between-study correlation to be estimated when fitting the random intercepts models (2) or (4), as we forced the correlation to be zero as it was in the data generating model. further research might also
consider how the random intercepts models perform when the correlation is freely estimated, although related simulations have shown that between-study correlations are difficult to estimate reliably, and often estimated values are +1 or
−1.19
6 conclusions
we recommend one-stage ipd meta-analysis models for continuous or binary outcomes use a stratified intercept. when
using ml estimation to fit such models, researchers should use a “study-specific centering” coding of included variables.
this will improve estimation of between-study variances and give more appropriate coverage of 95% confidence intervals
for the summary (treatment) effects of interest. for continuous outcomes, reml estimation is recommended and then
the coding should not be important. for binary outcomes, reml estimation of the pseudo likelihood will often improve
upon ml estimation of the exact likelihood, although it may be unstable when data are sparse. for either ml or reml
estimation, confidence intervals should be derived using an approach based on the t-distribution.

<|EndOfText|>

a study protocol for the development and
internal validation of a multivariable
prognostic model to determine lower
extremity muscle injury risk in elite football
(soccer) players, with further exploration of
prognostic factors
abstract
background: indirect muscle injuries (imis) are a considerable burden to elite football (soccer) teams, and
prevention of these injuries offers many benefits. preseason medical, musculoskeletal and performance screening
(termed periodic health examination (phe)) can be used to help determine players at risk of injuries such as imis,
where identification of phe-derived prognostic factors (pf) may inform imi prevention strategies. furthermore, using
several pfs in combination within a multivariable prognostic model may allow individualised imi risk estimation and
specific targeting of prevention strategies, based upon an individual’s pf profile. no such models have been
developed in elite football and the current imi prognostic factor evidence is limited. this study aims to (1) develop
and internally validate a prognostic model for individualised imi risk prediction within a season in elite footballers,
using the extent of the prognostic evidence and clinical reasoning; and (2) explore potential phe-derived pfs
associated with imi outcomes in elite footballers, using available phe data from a professional team.
methods: this is a protocol for a retrospective cohort study. phe and injury data were routinely collected over 5
seasons (1 july 2013 to 19 may 2018), from a population of elite male players aged 16–40 years old. of 60
candidate pfs, 15 were excluded. twelve variables (derived from 10 pfs) will be included in model development
that were identified from a systematic review, missing data assessment, measurement reliability evaluation and
clinical reasoning. a full multivariable logistic regression model will be fitted, to ensure adjustment before backward
elimination. the performance and internal validation of the model will be assessed. the remaining 35 candidate pfs
are eligible for further exploration, using univariable logistic regression to obtain unadjusted risk estimates.
exploratory pfs will also be incorporated into multivariable logistic regression models to determine risk estimates
whilst adjusting for age, height and body weight.

background
indirect muscle injuries (imis) are the most common injury type in elite football (soccer), predominantly affecting lower extremity muscle groups [1, 2]. such injuries
occur in the absence of direct impact-related trauma
(during sprinting for example) [3, 4] and are subclassified into functional disorders without macroscopic structural tissue muscle damage, or structural injuries with
clear evidence of muscle disruption [3, 4].
imis are problematic for elite teams in terms of both incidence and severity [5], accounting for 30.3% to 47.9% of
all injuries that result in time lost to both training and
competition [1, 6–9], with the mean and median absence
duration reported as 14.4 [1] and 15 days respectively [8].
player availability is crucial to team prosperity, with vast
commercial and financial rewards on offer to successful
teams and players [10, 11]. conversely, player absences
through injury negatively affect team performance [12,
13], increase demand on medical services and carry a significant financial burden. as an illustration, for each first
team player missing through injury, the daily cost to a participating team in the uefa champions league is approximately €17,000 to €20,000 [14, 15].
periodic health examination (phe) is used by 94% of
elite teams and typically consists of medical examination, musculoskeletal assessment, functional movement
evaluation and performance tests, conducted during preseason and in-season periods [16]. phe is considered
important because its intended purposes are to: (1) allow
regular health monitoring for underlying but asymptomatic pathology [17]; (2) establish baseline measures for
setting rehabilitation or training targets [18]; and (3)
identify individuals who are susceptible to common or
severe injury types (such as imis) [19]. for the latter
function, phe cannot detect causes of injury, but can
highlight factors that may be associated with an injury
outcome (prognostic factors) and therefore help explain
differences in injury risk across individuals within the
team [18]. several prognostic factors could also be used
in combination within a multivariable prognostic model
to predict an individual’s absolute injury risk [20, 21].
importantly, both prognostic models and prognostic factors (pfs) can be used to inform management approaches designed to modify an individual’s absolute risk
[21]. despite the potential benefits of prognostic models
for shaping injury prevention strategies aimed at
clinically important injuries such as imis, none have
been developed in elite football [22]. in addition, there
are significant methodological limitations in the evidence
base relating to phe-derived pfs [22].
therefore, this study will consist of two primary objectives: (1) to develop and internally validate a prognostic
model for individualised imi risk prediction during a
season in elite footballers, using a small number of phederived candidate pfs selected from a previous systematic review [22] and clinical reasoning; and (2) to explore
potential pfs associated with imi outcomes during a
season in this elite cohort, using available phe data from
a professional team.
methods
study design
this study will be of retrospective cohort design, using a
population of elite male football players aged 16–40 years
old who were employed on a full-time basis at an english
premier league club. the first objective will be conducted
in accordance with existing guidelines for model development and internal validation [23, 24] and reported in accordance with the transparent reporting of a
multivariable prediction model for individual prognosis
or diagnosis (tripod) statement [25, 26]. the second
objective will be conducted in accordance with existing
guidelines [27] and reported in accordance with the
reporting recommendations for marker prognostic
studies [28, 29].
data sources
this study will use routinely collected data that was obtained over five seasons (from 1 july 2013 to 19 may
2018). data collected from the musculoskeletal and performance test components of the club’s phe will be used
to identify candidate pfs. injury outcome data will also be
used to establish the available number of imi outcomes.
preseason phe data collection
each new season commenced from july 1st. available
players completed a mandatory phe on one of 3 days during the first week of the season. typically, the musculoskeletal and performance components of the phe
included the following: (1) anthropometric measurements;
(2) medical history review (i.e. previous injury history); (3)
musculoskeletal examination tests; (4) functional
hughes et al. diagnostic and prognostic research (2019) 3:19 page 2 of 13
movement and balance tests; and (5) strength and power
tests. detailed descriptions of all tests are provided in
additional file 1.
the phe test order was self-selected by each player. a
standardised warm up was not implemented, although
players could undertake their own warm up procedures
if they wished. each component of the phe test battery
was standardised according to a written protocol and
conducted by physiotherapists, sports scientists or club
medical doctors. to avoid inter-tester variability, the
same examiners performed the same test every season
and throughout the 5-year data collection period, no
examiner attrition occurred.
if a participant was injured at the time of phe, a risk
assessment was completed by medical staff. in such instances, participants only completed tests that were
deemed appropriate and safe for the participant’s condition; examiners were therefore not blinded to the injury
status of participants.
participant follow-up and injury data collection
participants were followed up to the last day of each
competitive domestic season (defined as the date of the
last first team game of the season) irrespective of
whether they had completed the phe procedure or not.
participants completed their routine training and match
programmes throughout. for every player in the squad,
any injuries that occurred during the season were
assessed and electronically documented within 24 h by a
club medical doctor or physiotherapist in accordance
with the consensus statement on injury definitions and
data collection procedures in studies of football injuries [30]. musculoskeletal assessments were dependent
on the clinical presentation, although typically consisted
of observation, effusion, range of movement, muscle
length and resisted muscle tests, palpation and special
diagnostic manual tests. radiological imaging was used
to assist diagnosis as required. ultrasound scans were
performed by the club medical doctor using a toshiba
aplio 500 or 1900 machine (toshiba corporation,
tokyo, japan). magnetic resonance imaging (mri) was
performed as appropriate, using a canon vantage titan
3 t scanner (canon medical systems, otowara, japan)
according to sequences determined by the club medical
doctor. images were evaluated by a club medical doctor
and an independent musculoskeletal radiologist.
the medical professionals were not blinded to phe
data at the time of diagnosis. these data were not routinely used to inform diagnoses, but instead used to
identify functional rehabilitation targets and for
benchmarking purposes. following injury, players
completed a rehabilitation programme as directed by
club medical staff to enable them to return to training
and match participation.
participants and eligibility criteria
eligible participants were identified from a review of the
phe database entries during the dates stated above. during any season, participants were eligible for inclusion
into the analysis if they: (1) had an outfield position (i.e.
not a goalkeeper); and (2) participated in phe testing for
the relevant season. participants were excluded from the
analysis for any season if they were a triallist player or
not contracted to the club at the time of phe.
ethics and data use
because all data were captured from the mandatory phe
procedure completed through the participants’ employment, informed consent was not required. the anonymity and rights of all participants were protected. the
football club granted permission to use these data, and
the use of the data for this study was approved by the
research ethics service at the university of manchester.
this study has been registered on clinicaltrials.gov,
with registered number as nct03782389.
data extraction
all phe records from eligible participants were extracted and placed into a separate database. using the
club’s electronic medical records system, a further database was generated of all recorded injuries for each season and a manual review of each eligible participant’s
medical record was undertaken to ensure accuracy. each
injury was categorised according to the following: (1)
contact or non-contact mechanism of injury; (2) injured
side; (3) affected body area; (4) injury type, i.e. imi/ligament/tendon/cartilage/contusion or laceration/bone/
concussion/other musculoskeletal injury; and (5) muscle
group and diagnostic classification if recorded as an imi.
this process allowed an in-house audit of injury incidence and absolute risk evaluation for each injury type
for the squad overall and for those who underwent phe.
all imis were then extracted and merged with the phe
database of included participants, for each season in
which they remained eligible.
outcome measures
for this study, the primary outcome measure will be the
occurrence of an initial (index) lower extremity imi sustained by a participant during a season. only time-loss
injuries will be included; that is, any index lower extremity imi that occurred during match play or training that
resulted in the player being unable to take full part in future match play or training [30]. an imi was confirmed
during the injury assessment procedure outlined above
and graded by the club medical doctor or physiotherapist according to the munich consensus statement for
the classification of muscle injuries in sport [4]. this
diagnostic classification system was the primary method
hughes et al. diagnostic and prognostic research (2019) 3:19 page 3 of 13
of muscle injury classification used by the club and has
been validated previously [31].
each participant-season will be treated as independent.
if an index lower extremity imi occurred, the participant’s outcome for the season will be determined and
that participant will no longer be considered at risk beyond the time of imi occurrence. in these circumstances, participants will be included for further analysis
at the start of the consecutive season, providing they remain eligible. if participants sustained any upper limb
imi, trunk imi or non-imi injury type, these will be ignored and the participant will still be considered at risk
of a lower extremity index imi.
eligible participants who were loaned out or transferred to another club throughout that season, but had
not sustained an index imi prior to the loan or transfer,
will still be considered in the risk set. participants who
sustained an index imi whilst on loan will be included
for analysis, as outlined above. any participants who
were permanently transferred during a season (but had
not sustained an index imi prior to the transfer) will be
recorded as not having an imi event during the relevant
season, and they will exit the cohort at this point. a sensitivity analysis may be conducted to evaluate the effect
of player loans or transfers on the results.
sample size
to maximise statistical power, we have elected to use all
data from the 5-season period. this approach agrees
with methodological recommendations that data splitting should be avoided, and all available data should be
used for model validation [32]. the extracted injury data
were audited in parallel with the development of this
protocol to determine the number of available index imi
events in the dataset. this was essential to allow calculation of the maximum number of candidate pfs that
could be included in model development in order to
limit the effects of statistical overfitting [33].
the number of candidate pfs for inclusion in model
development will be restricted to a minimum of 10
events per variable (epv), which is recommended to reduce overfitting and optimism during the development
of a logistic regression model [34]. note that ‘variable’
here means a parameter included (or considered for
inclusion) in the model that corresponds to one of
the pfs.
following the audit, the number of independent
participant-seasons that will be included for analysis is
317, with 138 index imi events recorded during the 5-
season period. therefore, we have chosen to restrict the
number of parameters (variables) for inclusion in model
development to 12, which corresponds to having >10
epv and thus above the minimum recommendation
of 10. we also checked if this met the criteria to
minimise overfitting recently proposed by riley et al.
[33]. assuming the model will have a modest nagelkerke r-squared of 25%, then with an outcome proportion of 0.435, our 12 candidate pf variables
correspond to targeting an approximate shrinkage factor of 0.85, and thus a relatively small amount of
overfitting (15%) [33]. we deemed this a suitable
compromise between increasing the number of pf parameters and minimising the overfitting.
candidate prognostic factors
the extracted phe data were audited as per current
methodological recommendations [23], to establish data
quality and quantify missing values. this process was
also conducted in parallel with the development of this
protocol, to assist selection of candidate pfs to be included in either model development or exploration a
priori and to inform strategies for handling missing data
in the final analysis.
a complete list of all 60 candidate pfs extracted from
the phe dataset is presented in table 1, with quantitative analysis of missing values for each pf.
missing data
as presented in table 1, all medical history and age
factors were complete (23 factors). of the 37
remaining candidate pfs, the proportion of missingness ranged from 5.68% (for height and weight) to
76.34% (for body fat). eleven of these had > 15%
missing observations (which included body fat, toe
touch in standing, sacroiliac kinematic function, all y
balance test and upper body peak power variables).
for these factors, the large degree of missingness was
because of procedural changes in the phe process,
which meant that these tests were not conducted
across all seasons.
for candidate pfs with < 15% missing observations, all
tests were conducted consistently across all 5 seasons.
for these factors, the sample characteristics of cases with
complete pf data were compared to incomplete cases
which had at least one missing observation (table 2).
for complete cases, the mean values of all characteristics were less than incomplete cases, with the largest
differences observed in age (20.83 and 23.55 years, respectively) and weight (74.15 and 77.86 kg, respectively). therefore, a complete case only analysis was
not appropriate and we will rather assume that the
mechanism of missingness can be considered as missing at random (mar), where the distribution of missing values is related to values of observed variables
[26], to allow imputation and so inclusion of individuals with missing data.
hughes et al. diagnostic and prognostic research (2019) 3:19 page 4 of 13
table 1 list of candidate prognostic factors, methods and units of measurement, frequency of complete and incomplete
observations and proportion of missing observations
type of
prognostic factor
candidate prognostic factor measurement method measurement unit data
type
complete
obs
missing
obs
percent
missing (%)
anthropometric age birthdate years cont. 317 0 0
height standing height cm cont. 299 18 5.68
weight digital scales kg cont. 299 18 5.68
bmi height/weight kg/m2 cont. 294 23 7.26
body fat skin callipers % cont. 75 242 76.34
medical history frequency of previous imis within 3 years
prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous imi within 3 years
prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous foot or ankle
injuries within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous foot or ankle injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous hip or groin injuries
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous hip or groin injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous knee injuries within
3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous knee injury within 3
years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous shoulder injuries
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous shoulder injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous lumbar spine
injuries within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous lumbar spine injury
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous iliopsoas imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous iliopsoas imi within
3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous adductor imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous adductor imi within
3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous hamstring imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous hamstring imi within
3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous quadriceps imis
within 3 years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous quadriceps imi
within 3 years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
frequency of previous calf imis within 3
years prior to phe
medical records freq. dis./
cont.
317 0 0
most recent previous calf imi within 3
years prior to phe
medical records never, < 6 months, 6–12
months, > 12 months
cat. 317 0 0
musculoskeletal prom r hip joint internal rotation digital inclinometer degrees cont. 297 20 6.31
prom l hip joint internal rotation digital inclinometer degrees cont. 297 20 6.31
prom r hip joint external rotation digital inclinometer degrees cont. 297 20 6.31
prom l hip joint external rotation digital inclinometer degrees cont. 297 20 6.31
r hip flexor muscle length digital inclinometer—thomas
test
degrees cont. 294 23 7.26
hughes et al. diagnostic and prognostic research (2019) 3:19 page 5 of 13
model development and internal validation
we have chosen to conduct the model development before the pf exploration because of the restrictions on the
number of pfs permitted to limit potential overfitting of
the model.
because only 12 pf variables will be used in model building, we have defined these candidate pfs a priori (table 3).
three candidate pfs have known importance based on the
results of our previous systematic review so were selected
for inclusion [22]. all other pfs listed in table 1 were eligible unless there were > 15% missing observations or if reliability (where applicable) was classed as fair to poor (icc
< 0.70) [35]. in these cases, the relevant candidate pfs were
excluded (table 4). this was to ensure that only the highest
quality data will be used in the analysis, with pfs that would
generally be available and routinely measured.
co-linearity amongst factors within a logistic regression model can cause inaccuracies in standard error and
table 1 list of candidate prognostic factors, methods and units of measurement, frequency of complete and incomplete
observations and proportion of missing observations (continued)
type of
prognostic factor
candidate prognostic factor measurement method measurement unit data
type
complete
obs
missing
obs
percent
missing (%)
l hip flexor muscle length digital inclinometer—thomas
test
degrees cont. 294 23 7.26
r hamstring muscle length /neural
mobility
digital inclinometer—slr degrees cont. 297 20 6.31
l hamstring muscle length /neural
mobility
digital inclinometer—slr degrees cont. 297 20 6.31
r quadriceps muscle length goniometer—ely’s test degrees cont. 297 20 6.31
l quadriceps muscle length goniometer—ely’s test degrees cont. 297 20 6.31
r calf muscle length digital inclinometer—wbl degrees cont. 297 20 6.31
l calf muscle length digital inclinometer—wbl degrees cont. 297 20 6.31
toe touch in standing fingertip-floor distance cm cont. 250 67 21.14
sacroiliac joint kinematic function gillet’s test subjective kinematic
function
cat. 250 67 21.14
functional
movement/
balance
y balance test—r anterior translation y balance test cm cont. 179 138 43.53
y balance test—l anterior translation y balance test cm cont. 179 138 43.53
y balance test—r posteromedial
translation
y balance test cm cont. 179 138 43.53
y balance test—l posteromedial
translation
y balance test cm cont. 179 138 43.53
y balance test—r posterolateral
translation
y balance test cm cont. 179 138 43.53
y balance test—l posterolateral
translation
y balance test cm cont. 179 138 43.53
r relative tibial angles sls measured with imu degrees cont. * * *
l relative tibial angles sls measured with imu degrees cont. * * *
strength/power r upper body peak power horizontal press w/kg−0.67 cont. 178 139 43.85
l upper body peak power horizontal press w/kg−0.67 cont. 178 139 43.85
r maximal loaded leg extension power double leg press w/kg−0.67 cont. 276 41 12.93
l maximal loaded leg extension power double leg press w/kg−0.67 cont. 276 41 12.93
r maximal loaded leg extension velocity double leg press m s−1 cont. 276 41 12.93
l maximal loaded leg extension velocity double leg press m s−1 cont. 276 41 12.93
r maximal loaded leg extension force double leg press n/kg−0.67 cont. 276 41 12.93
l maximal loaded leg extension force double leg press n/kg−0.67 cont. 276 41 12.93
cmj height cmj cm cont. 275 42 13.25
cmj force per kilogram of body mass cmj n/kg cont. 275 42 13.25
cmj peak power cmj w cont. 275 42 13.25
freq. frequency, obs observations, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom
passive range of movement, slr straight leg raise, sls single leg squat, imu inertial measurement units, bmi body mass index, kg/m2 kilograms/body
height (metres) squared, w watts (note: w/kg−0.67 has a scaling factor to normalise power to body mass), n newtons (note: n/kg−0.67 has a scaling factor to
normalise force to body mass), cm centimetres, kg kilograms, cont. continuous, dis./cont. discrete treated as continuous, cat categorical, r right, l left, m s-1
metres/second, “–” not applicable/not available, “*” missing data analysis not completed—test evaluated through a reliability/agreement study published as a
related part of this project and excluded from further analysis based on the results
hughes et al. diagnostic and prognostic research (2019) 3:19 page 6 of 13
confidence interval estimates [45], so a scatterplot
matrix was used to informally assess between-factor correlations for eligible pfs. if pfs were highly correlated,
one of the pfs was dropped or new composite pfs were
generated and replaced the original factors (highlighted
in tables 3, 4 and 5). typically, this occurred where
measurements examined both right and left limbs separately; composite factor variables were therefore created
for both between-limb measurement differences and the
mean of the measurements for both limbs.
of the remaining eligible pfs, 9 further candidate factor variables were selected for inclusion, through use of
clinical reasoning to identify those with a biologically
plausible association with imi development. the final
set of 12 pf variables is shown in table 3.
prognostic factor exploration
candidate pfs that were that were not selected for use in
model development (but not excluded) will be eligible for
further exploratory analysis (table 5). this will allow identification of other potentially useful associations which
may assist future analyses or updating of the model created under the first objective of this investigation.
statistical analysis
model development and internal validation
multivariable logistic regression will be used for the analysis as this is an appropriate method where outcomes
are binary [26] and independent variables (pfs) are continuous, categorical or a combination [45]. initially, we
will fit a full multivariable model containing all 12 candidate pf variables to ensure a fully adjusted model prior
to the potential elimination of unimportant candidate
factors [23]. backward elimination will then be used to
successively remove non-significant factors with p values
of greater than 0.157. this threshold was set to approximate equivalence with akaike’s information criterion
[48]. using backward elimination in this way may deliver
a more parsimonious model which is therefore easier to
implement in clinical practice than a full model. where
possible, we will retain continuous candidate pfs in their
continuous form to avoid statistical power loss [49].
because the missing data mechanism is considered as
missing at random (mar), multiple imputation (mi) will
be implemented, using 50 imputations. we have chosen
to utilise mi because it avoids excluding participants
from the analysis, is an effective method of handling
missing prognostic factor information and can be used
to account for uncertainty in missing data [50].
the apparent performance of the developed model will
be summarised in the development datasets (averaged
over imputation datasets), via calibration and discrimination. model calibration determines performance in terms
of the agreement between predicted outcome risks and
those actually observed [51]. graphical plots are useful to
assess calibration [23], so will be produced and utilised in
the analysis. we will calculate calibration-in-the-large
(citl, ideal value of 0), which quantifies the systematic
error in model predictions (overall agreement). a related
measure is e/o (ideal value of 1), which gives the ratio of
the mean of the predicted (expected (e)) risks against the
mean of the observed risks (o) [51, 52]. a calibration
slope will also be calculated, where a value of 1 equals perfect calibration [26]. models demonstrate perfect calibration within development data, but in new data, the slope
may be < 1 due to overfitting in the model development
dataset (see below for how this will be handled) [52].
discrimination performance is a measure of a model’s
ability to separate participants who have experienced an
outcome compared to those who have not, quantified
using the c (concordance) statistic (equivalent to the
area under the roc curve) [23]. this index measure will
be calculated for the development model, where 1 demonstrates perfect discrimination, whilst 0.5 indicates that
discrimination is no better than by chance alone.
to quantify the degree of optimism due to overfitting,
our model will be internally validated using bootstrap
re-sampling. this will be conducted as previously outlined [26, 53]. the prognostic factor variable selection
procedure and model construction will be repeated for
200 bootstrap samples. for each sample, the difference
in bootstrap apparent performance (of the bootstrap
model in the bootstrap data) and test performance (of
the bootstrap model in the original dataset) will be averaged across the 200 samples, to obtain a single estimate
table 2 characteristics of cases with complete candidate prognostic factor data, and cases with at least one missing observation for
any candidate prognostic factor in the phe dataset with < 15% missing values
complete cases (n=264 person-seasons) incomplete cases (n=53 person-seasons)
sample characteristic number of obs mean (sd) number of obs mean (sd)
age (years) 264 20.83 (4.42) 53 23.55 (4.48)
height (cm) 264 180.58 (6.34) 35 181.12 (6.68)
weight (kg) 264 74.15 (7.55) 35 77.86 (7.98)
bmi (kg/m2 ) 264 22.72 (1.76) 30 23.75 (2.24)
pf prognostic factor, obs observations, sd standard deviation, cm centimetres, kg kilograms, kg/m2 kilograms divided by body height (metres) squared
hughes et al. diagnostic and prognostic research (2019) 3:19 page 7 of 13
of optimism for each performance statistic. then, to calculate optimism-adjusted estimates of performance for
our new model, the estimates of optimism will be subtracted from the original apparent estimates of
performance.
the optimism-adjusted calibration slope will provide a
uniform shrinkage factor, which will be applied to all
prognostic factor effects in the developed model to adjust (shrink) for overfitting. the intercept of the model
will then be re-estimated accordingly. this will then
form our final model.
prognostic factor exploration
all remaining candidate factors that are eligible for exploration (table 5) will undergo univariable logistic regression
analyses to determine unadjusted associations with imis.
candidate pfs will also be incorporated into multivariable
logistic regression models to determine odds ratios after
adjustment for age, height and body weight. note that because age was included as a candidate in the original
model and will also be used for adjustment purposes in
the exploratory multivariable models, the total number of
candidate pfs eligible for exploration is 36. exploration of
non-linear associations between candidate factors and
index imi outcomes will also be evaluated using a fractional polynomial approach [49].
discussion
although previous studies in elite football have investigated the association between factors obtained during
phe and imis using multivariable models, none have
developed, validated or evaluated the performance a
prognostic model for injury prediction purposes [22].
whilst it is possible to develop a prognostic model from
phe data [18], our investigation will offer valuable insights into the practical aspects of this process and the
clinical usefulness of a model when applied to an individual football club. our findings may also outline how
these principles may be used in future at other clubs or
sports, or on larger datasets which could be derived
from several collaborating clubs.
table 3 restricted set of candidate prognostic factors for model development and validation
selection
method
candidate prognostic
factor
composite
pf
measurement unit number of
parameters in
model
measurement
method
data type reliability (if
applicable)
systematic
review
age no years 1 date of birth continuous -
frequency of previous
imis within 3 years prior
to phe
no freq. 1 medical records discrete
(treated as
continuous)
-
most recent previous
imi within 3 years prior to
phe
no < 6 months, 6–12
months, > 12
months
3 medical records categorical -
clinical
reasoning/
data quality
cmj peak power no watts 1 cmj using force
plates
continuous test-retest icc
= 0.92–0.98 [36]
prom hip joint internal
rotation difference*
yes degrees 1 digital
inclinometer
continuous intra-rater icc =
0.90 [37]
prom hip joint external
rotation difference*
yes degrees 1 digital
inclinometer
continuous intra-rater icc =
0.90 [37]
hip flexor muscle length
difference*
yes degrees 1 digital
inclinometer -
thomas test
continuous inter-rater icc =
0.89 [38]
hamstring muscle length
/neural mobility
difference*
yes degrees 1 digital
inclinometer - slr
continuous intra-rater icc =
0.95–0.98 [39]
interrater icc =
0.80–0.97 [40]
calf muscle length
difference*
yes degrees 1 digital
inclinometer - wbl
continuous inter-rater icc =
0.80–0.95 [41,
42]
intra-rater = icc
0.88 [42]
bmi yes kg/m2 1 composite height
(cm) and weight
(kg)
continuous –
pf prognostic factor, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom passive range of
movement, icc intraclass correlation coefficient, slr straight leg raise, bmi body mass index, kg kilos, freq. frequency, kg/m2 kilograms/body height (metres)
squared. "*" denotes between limb differences
hughes et al. diagnostic and prognostic research (2019) 3:19 page 8 of 13
despite the availability of high-quality phe and injury
data, the relatively small number of outcomes in this
dataset is problematic and will permit only a limited selection of candidate prognostic factors for use in model
development. utilising more than one prognostic factor variable for every 10 injury outcomes may cause
significant issues with model overfitting, where spurious observed relationships occur because of regression value distortion [34]. this leads to an
overestimation of predictive performance (optimism)
which is especially evident in small datasets [54]. to
limit the effects of overfitting, only 10 pfs (resulting
in 12 variables) will be permitted and use of data reduction methods have been required to select appropriate candidate factors for inclusion.
pfs for clinical injury outcomes are either intrinsic
(person specific) or extrinsic (environment specific) [55]
and can be modifiable or non-modifiable [56]. only the
non-modifiable factors of increasing age and history of
previous muscle injury have been shown to have modest
table 4 candidate prognostic factors excluded from both model development and prognostic factor exploration
type of
prognostic
factor
candidate prognostic
factor
composite
pf
measurement unit measurement method data type reason for
elimination
anthropometric body fat no percentage skin callipers continuous missing data > 15%
musculoskeletal quadriceps muscle length
difference*
yes degrees goniometer - ely's test continuous intra-rater icc =
0.69 [43]
inter-rater icc =
0.66 [43]
mean quadriceps muscle
length**
yes degrees goniometer - ely's test continuous intra-rater icc =
0.69 [43]
inter-rater icc =
0.66 [43]
toe touch in standing no centimetres fingertip to floor distance continuous missing data > 15%
sacroiliac joint kinematic
function
no subjective score gillet’s test categorical missing data > 15%
functional
movement/
balance
y balance test—anterior
translation difference*
yes centimetres y balance test continuous missing data > 15%
y balance test—mean
anterior translation**
yes centimetres y balance test continuous missing data > 15%
y balance
test—posteromedial
translation difference*
yes centimetres y balance test continuous missing data > 15%
y balance test—mean
posteromedial translation**
yes centimetres y balance test continuous missing data > 15%
y balance
test—posterolateral
translation difference*
yes centimetres y balance test continuous missing data > 15%
y balance test—mean
posterolateral translation**
yes centimetres y balance test continuous missing data > 15%
r relative tibial angles no degrees sls measured with dorsavi
viperform imu
continuous within-session iccs
= 0.27–0.75
between-session
iccs = 0.55–0.77
[44]
l relative tibial angles no degrees sls measured with dorsavi
viperform imu
continuous within-session iccs
= 0.27–0.75
between-session
iccs = 0.55–0.77
[44]
strength/power upper body peak power
difference*
yes normalised watts
per kilo (w/kg−0.67)
double horizontal press using a
keiser chest press air 350
machine
continuous missing data > 15%
mean upper body peak
power**
yes normalised watts
per kilo (w/kg−0.67)
double horizontal press using a
keiser chest press air 350
machine
continuous missing data > 15%
icc intraclass correlation coefficient, sls single leg squat, w watts, (note that w/kg−0.67 has a scaling factor to normalise power to body mass), kg kilos, imu inertial
measurement units, r right, l left. “*” denotes between limb differences and “**” denotes the mean of the measurements for both limbs
hughes et al. diagnostic and prognostic research (2019) 3:19 page 9 of 13
table 5 candidate prognostic factors—exploration
type of
prognostic
factor
candidate prognostic factor composite pf measurement unit measurement method data type reliability (if
applicable/available)
anthropometric height no centimetres standing height measure continuous –
weight no kilograms digital scales continuous –
medical history frequency of previous foot or ankle
injuries within 3 years prior to phe
no freq. medical records continuous –
most recent previous foot or ankle injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous hip or groin
injuries within 3 years prior to phe
no freq. medical records continuous –
most recent previous hip or groin injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous knee injuries
within 3 years prior to phe
no freq. medical records continuous –
most recent previous knee injury within 3
years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous shoulder injuries
within 3 years prior to phe
no freq. medical records continuous –
most recent previous shoulder injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous lumbar spine
injuries within 3 years prior to phe
no freq. medical records continuous –
most recent previous lumbar spine injury
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous iliopsoas imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous iliopsoas imi within
3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous adductor imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous adductor imi within
3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous hamstring imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous hamstring imi
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous quadriceps imis
within 3 years prior to phe
no freq. medical records continuous –
most recent previous quadriceps imi
within 3 years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
frequency of previous calf imis within 3
years prior to phe
no freq. medical records continuous –
most recent previous calf imi within 3
years prior to phe
no < 6 months, 6–12
months, > 12 months
medical records categorical –
musculoskeletal mean prom hip joint internal rotation** yes degrees digital inclinometer continuous intra-rater icc =
0.90 [37]
mean prom hip joint external rotation** yes degrees digital inclinometer continuous intra-rater icc =
0.90 [37]
mean hip flexor muscle length** yes degrees digital inclinometer - thomas
test
continuous inter-rater icc =
0.89 [38]
mean hamstring muscle length/neural
mobility**
yes degrees digital inclinometer - slr continuous intra-rater icc =
0.95–0.98 [40]
inter-rater icc =
0.80–0.97 [40]
mean calf muscle length** yes degrees digital inclinometer - wbl continuous inter-rater icc =
0.80–0.95 [41, 42]
intra-rater icc =
0.88 [42]
strength/power maximal loaded leg extension power
difference*
yes normalised watts per
kilo (w/kg−0.67)
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.886 [46]
hughes et al. diagnostic and prognostic research (2019) 3:19 page 10 of 13
prognostic value for hamstring muscle injuries in elite
footballers [22], so will be included in model development. however, their non-modifiable nature means that
they have limited use in terms of informing injury prevention strategies. to enhance the clinical applicability
of the model, other potentially relevant and modifiable
factors have been selected for inclusion.
the methodological shortcomings in the literature
mean that only three candidate prognostic factors could
be selected for model development from our previous
systematic review [22]. subsequently, candidate pf selection for our model has been largely based upon the
evaluation of collinearity, measurement reliability and
clinical reasoning, which means that it is possible that
some important factors have not been considered. it is
also possible that some potentially useful factors have
been excluded on the basis of having >15% of missing
values. as such, only modest performance of this initial
model is expected.
it is acknowledged that the proposed prognostic model
will assume that participants are independent for each
season and utilise the binary outcome of at least one
imi in a season, rather than evaluating time to individual imi events. this means that we will not account for
within-person correlations from season to season. although this is not fully representative of the real world,
because this is a novel area and we are restricted to a
relatively small dataset, we have elected to perform the
analyses in a more simplistic manner in the first instance. further, more complex analyses may be conducted in the future.
to assess the generalisability of a prognostic model, it
should be externally validated using data from another
location [21, 24], such as a dataset from another comparable elite level football team. because there is likely
to be considerable between-team heterogeneity in phe
processes [16], candidate prognostic factors within our
model may not translate externally at this time. there
are no immediate plans to externally validate this model.
however, depending on the outcome of the model development and exploratory objectives, it may be possible to
conduct a future prospective temporal validation study
within the same football club, or external validation
study in different population. if feasible, such investigations will require a separate associated protocol.
the current evidence relating to pfs for injury in
football is frequently flawed due to issues with the reliability of data measurement, adjustment, dichotomisation and potential diagnostic misclassification, so
there is a need for further studies that address these
issues [22]. further hypothesis-free exploratory studies
that investigate many factors (including those that are
not necessarily biologically plausible) may assist with
identification of new factors that may help inform
management decisions and monitoring purposes [20].
furthermore, these types of studies are helpful because
new pfs may be used to update a developed model to improve performance [57]. we have therefore outlined an
exploratory objective to investigate the association between imis and other factors from the current dataset,
using a validated diagnostic outcome classification system
and recommended statistical approaches, ensuring that
where possible, analysis of continuous data remains on
the continuous scale to explore linear and non-linear
associations.
we anticipate that this investigation will provide a
comprehensive evaluation of what is currently possible
in terms of using phe data to predict imis at an elite
football club, by adhering to transparent reporting procedures and current best practice for model
table 5 candidate prognostic factors—exploration (continued)
type of
prognostic
factor
candidate prognostic factor composite pf measurement unit measurement method data type reliability (if
applicable/available)
mean of maximal loaded leg extension
power**
yes normalised watts per
kilo (w/kg−0.67)
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.886 [46]
loaded maximal leg extension velocity
difference*
yes peak velocity (m s−1
) double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.792 [46]
mean of maximal loaded leg extension
velocity**
yes peak velocity (m s−1
) double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.792 [46]
loaded maximal leg extension force
difference*
yes normalised peak force
(n/kg−0.67 )
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.914 [46]
mean of maximal loaded leg extension
force**
yes normalised peak
force (n/kg−0.67)
double leg press test using a
keiser air 300 machine
continuous test-retest icc =
0.914 [46]
cmj force per kilogram of body mass no force per kg (n/kg) cmj using force plates continuous –
cmj height no centimetres cmj using force plates continuous test-retest icc =
0.80–0.88 [47]
pf prognostic factor, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom passive range of
movement, icc intraclass correlation coefficient, slr straight leg raise, kg kilos, w watts, (note that w/kg-0.67 has a scaling factor to normalise power to body
mass), n newtons, (note that n/kg-0.67 has a scaling factor to normalise force to body mass), m s−1 metres/second, "-" not applicable/not available. “*” denotes
between limb differences and “**” denotes the mean of the measurements for both limbs
hughes et al. diagnostic and prognostic research (2019) 3:19 page 11 of 13
development, validation and exploration of potential
pfs. we hope this study will also identify further research priorities for this novel and potentially valuable
area of sports/football medicine research.

<|EndOfText|>

calculating the sample size required for developing a clinical prediction model

summary points
patients and healthcare professionals require clinical prediction models to accurately guide healthcare decisions
larger sample sizes lead to the development of more robust models
data should be of sufficient quality and representative of the target population and settings of application
it is better to use all available data for model development (ie, avoid data splitting), with resampling methods (such as bootstrapping) used for internal validation
when developing prediction models for binary or time-to-event outcomes, a well known rule of thumb for the required sample size is to ensure at least 10 events for each predictor parameter
the actual required sample size is, however, context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the model
we propose to use such information to tailor sample size requirements to the specific setting of interest, with the aim of minimising the potential for model overfitting while targeting precise estimates of key parameters
our proposal can be implemented in a four step procedure and is applicable for continuous, binary, or time-to-event outcomes
the pmsampsize package in stata or r allows researchers to implement the procedure
clinical prediction models are needed to inform diagnosis and prognosis in healthcare.123 well known examples include the wells score,45 qrisk,67 and the nottingham prognostic index.89 such models allow health professionals to predict an individual’s outcome value, or to predict an individual’s risk of an outcome being present (diagnostic prediction model) or developed in the future (prognostic prediction model). most prediction models are developed using a regression model, such as linear regression for continuous outcomes (eg, pain score), logistic regression for binary outcomes (eg, presence or absence of pre-eclampsia), or proportional hazards regression models for time-to-event data (eg, recurrence of venous thromboembolism).10 an equation is then produced that can be used to predict an individual’s outcome value or outcome risk conditional on his or her values of multiple predictors, which might include basic characteristics such as age, weight, family history, and comorbidities; biological measurements such as blood pressure and biomarkers; and imaging or other test results. supplementary material s1 shows examples of regression equations.

developing a prediction model requires a development dataset, which contains data from a sample of individuals from the target population, containing their observed predictor values (available at the intended moment of prediction11) and observed outcome. the sample size of the development dataset must be large enough to develop a prediction model equation that is reliable when applied to new individuals in the target population. what constitutes an adequately large sample size for model development is, however, unclear,12 with various blanket “rules of thumb” proposed and debated.1314151617 this has created confusion about how to perform sample size calculations for studies aiming to develop a prediction model.

in this article we provide practical guidance for calculating the sample size required for the development of clinical prediction models, which builds on our recent methodology papers.1314151618 we suggest that current minimum sample size rules of thumb are too simplistic and outline a more scientific approach that tailors sample size requirements to the specific setting of interest. we illustrate our proposal for continuous, binary, and time-to-event outcomes and conclude with some extensions.

moving beyond the 10 events per variable rule of thumb
in a development dataset, the effective sample size for a continuous outcome is determined by the total number of study participants. for binary outcomes, the effective sample size is often considered about equal to the minimum of the number of events (those with the outcome) and non-events (those without the outcome); time-to-event outcomes are often considered roughly equal to the total number of events.10 when developing prediction models for binary or time-to-event outcomes, an established rule of thumb for the required sample size is to ensure at least 10 events for each predictor parameter (ie, each β term in the regression equation) being considered for inclusion in the prediction model equation.192021 this is widely referred to as needing at least 10 events per variable (10 epv). the word “variable” is, however, misleading as some predictors actually require multiple β terms in the model equation—for example, two β terms are needed for a categorical predictor with three categories (eg, tumour grades i, ii, and iii), and two or more β terms are needed to model any non-linear effects of a continuous predictor, such as age or blood pressure. the inclusion of interactions between two or more predictors also increases the number of model parameters. hence, as prediction models usually have more parameters than actual predictors, it is preferable to refer to events per candidate predictor parameter (epp). the word candidate is important, as the amount of model overfitting is dictated by the total number of predictor parameters considered, not just those included in the final model equation.

the rule of at least 10 epp has been widely advocated perhaps as a result of its simplicity, and it is regularly used to justify sample sizes within published articles, grant applications, and protocols for new model development studies, including by ourselves previously. the most prominent work advocating the rule came from simulation studies conducted in the 1990s,192021 although this work actually focused more on the bias and precision of predictor effect estimates than on the accuracy of risk predictions from a developed model. the adequacy of the 10 epp rule has often been debated. although the rule provides a useful starting point, counter suggestions include either lowering the epp to below 10 or increasing it to 15, 20, or even 50.102223242526 these inconsistent recommendations reflect that the required epp is actually context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the model.1314151617 this finding is unsurprising as sample size considerations for other study designs, such as randomised trials of interventions, are all context dependent and tailored to the setting and research question. rules of thumb have also been advocated in the continuous outcome setting, such as two participants per predictor,27 but these share the same concerns as for 10 epp.16

sample size calculation to ensure precise predictions and minimise overfitting
recent work by van smeden et al1314 and riley et al1516 describe how to calculate the required sample size for prediction model development, conditional on the user specifying the overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit (r2). these authors’ approaches can be implemented in a four step procedure. each step leads to a sample size calculation, and ultimately the largest sample size identified is the one required. we describe these four steps, and, to aid general readers, provide the more technical details of each step in the figures.

step 1: what sample size will produce a precise estimate of the overall outcome risk or mean outcome value?
fundamentally, the sample size must allow the prediction model’s intercept to be precisely estimated, to ensure that the developed model can accurately predict the mean outcome value (for continuous outcomes) or overall outcome proportion (for binary or time-to-event outcomes). a simple way to do this is to calculate the sample size needed to precisely estimate (within a small margin of error) the intercept in a model when no predictors are included (the null model).15figure 1 shows the calculation for binary and time-to-event outcomes, and we generally recommend aiming for a margin of error of ≤0.05 in the overall outcome proportion estimate. for example, with a binary outcome that occurs in half of individuals, a sample size of at least 385 people is needed to target a confidence interval of 0.45 to 0.55 for the overall outcome proportion, and thus an error of at most 0.05 around the true value of 0.5. to achieve the same margin of error with outcome proportions of 0.1 and 0.2, at least 139 and 246 participants, respectively, are required.

fig 1
fig 1
calculation of sample size required for precise estimation of the overall outcome probability in the target population

download figure open in new tab download powerpoint
for time-to-event outcomes, a key time point needs to be identified, along with the anticipated outcome event rate. for example, with an anticipated event rate of 10 per 100 person years of the entire follow-up, the sample size must include a total of 2366 person years of follow-up to ensure an expected margin of error of ≤0.05 in the estimate of a 10 year outcome probability of 0.63, such that the expected confidence interval is 0.58 to 0.68.

for continuous outcomes, the anticipated mean and variance of outcome values must be prespecified, alongside the anticipated percentage of variation explained by the prediction model (see supplementary material s2 for details).16

step 2: what sample size will produce predicted values that have a small mean error across all individuals?
in addition to predicting the average outcome value precisely (see step 1), the sample size for model development should also aim for precise predictions across the spectrum of predicted values. for binary outcomes, van smeden et al use simulation across a wide range of scenarios to evaluate how the error of predicted outcome probabilities from a developed model depends on various characteristics of the development dataset sampled from a target population.14 they found that the number of candidate predictor parameters, total sample size, and outcome proportion were the three main drivers of a model’s mean predictive accuracy. this led to a sample size formula (fig 2) to help ensure that new prediction models will, on average, have a small prediction error in the estimated outcome probabilities in the target population (as measured by the mean absolute prediction error, mape). the calculation requires the number of candidate predictor parameters and the anticipated outcome proportion in the target population to be prespecified. for example, with 10 candidate predictor parameters and an outcome proportion of 0.3, a sample size of at least 461 participants and 13.8 epp is required to target a mean absolute error of 0.05 between observed and true outcome probabilities (see fig 2 for calculation). the calculation is available as an interactive tool (https://mvansmeden.shinyapps.io/beyondepv/) and applicable to situations with 30 or fewer candidate predictors. ongoing work aims to extend to larger numbers of candidate predictors and also to time-to-event outcomes.

fig 2
fig 2
sample size required to help ensure a developed prediction model of a binary outcome will have a small mean absolute error in predicted probabilities when applied in other targeted individuals

download figure open in new tab download powerpoint
for continuous outcomes, accurate predictions across the spectrum of predicted values require the standard deviation of the residuals to be precisely estimated.1016 supplementary material s3 shows that to target a less than 10% multiplicative error in the estimated residual standard deviation, the required sample size is simply 234+p, where p is the number of predictor parameters considered.

step 3: what sample size will produce a small required shrinkage of predictor effects?
our third recommended step is to identify the sample size required to minimise the problem of overfitting.28 overfitting is when a developed model’s predictions are more extreme than they ought to be for individuals in a new dataset from the same target population. for example, an overfitted prediction model for a binary outcome will give a predicted outcome probability too close to 1 for individuals with a higher than the average outcome probability and too close to 0 for individuals with a lower than the average outcome probability. overfitting notably occurs when the sample size is too small. in particular, when the number of candidate predictor parameters is large relative to the number of participants in total (for continuous outcomes) or to the number of participants with the outcome event (for binary or time-to-event outcomes). a consequence of overfitting is that a developed model’s apparent predictive performance (as observed in the development dataset itself) will be optimistic (ie, too high), and its actual predictive performance in new data from the same target population will be lower (ie, worse).

shrinkage (also known as penalisation or regularisation) methods deal with the problem of overfitting by reducing the variability in the developed model’s predictions such that extreme predictions (eg, predicted probabilities close to 0 or 1) are pulled back toward the overall average.293031323334 however, there is no guarantee that shrinkage will fully overcome the problem of overfitting when developing a prediction model. this is because the shrinkage or penalty factors (which dictate the magnitude of shrinkage required) are also estimated from the development dataset and, especially when the sample size is small, are often imprecise and so fail to tackle the magnitude of overfitting correctly in a particular application.30 furthermore, a negative correlation tends to occur between the estimated shrinkage required and the apparent performance of a model. if the apparent model performance is excellent simply by chance, the required shrinkage is typically estimated too low.30 thus, ironically, in those situations when overfitting is of most concern (and thus shrinkage is most urgently needed), the prediction model developer has insufficient assurance in selecting the proper amount of shrinkage to cancel the impact of overfitting.

riley et al therefore suggest identifying the sample size and number of candidate predictors that correspond to a small amount of desired shrinkage (≤10%) during model development.1516 the sample size calculation (fig 3) requires the researcher to prespecify the number of candidate predictor parameters and, for binary or time-to-event outcomes, the anticipated outcome proportion or rate, respectively, in the target population. in addition, a (conservative) value for the anticipated model performance is required, as defined by the cox-snell r squared statistic (r2cs).1535 the anticipated value of r2cs is important because it reflects the signal:noise ratio, which has an impact on the estimation of multiple parameters and the potential for overfitting. when the signal:noise ratio is anticipated to be high (eg, r2cs is close to 1 for a prediction model with a continuous outcome), true patterns are easier to detect and so overfitting is less of a concern, such that more predictor parameters can be estimated. however, when the signal:noise ratio is low (ie, r2cs is anticipated to be close to 0), true patterns are harder to identify and there is more potential for overfitting, such that fewer predictor parameters can be estimated reliably.

fig 3
fig 3
how to calculate the sample size needed to target a small magnitude of required shrinkage of predictor effects (to minimise potential model overfitting) for binary or time-to-event outcomes

download figure open in new tab download powerpoint
in the continuous outcome setting, r2cs is simply the coefficient of determination r2, which quantifies the proportion of the variance of outcome values that is explained by the prediction model and thus is between 0 and 1. for example, when developing a prediction model for a continuous outcome with up to 30 predictor parameters and an anticipated r2cs of 0.7, a sample size of 206 participants is required to ensure the expected shrinkage is 10% (see supplementary material s4 for full calculation). this corresponds to about seven participants for each predictor parameter considered.

the r2cs statistic generalises to non-continuous outcomes and allows sample size calculations to minimise the expected shrinkage when developing a prediction model for binary and time-to-event outcomes (fig 3). for example, when developing a new logistic regression model with up to 20 candidate predictor parameters and an anticipated r2cs of at least 0.1, a sample size of 1698 participants is required to ensure the expected shrinkage is 10% (see fig 3 for full calculation). if the target setting has an outcome proportion of 0.3, this corresponds to an epp of 25.5. the required sample size and epp are sensitive to the choice of r2cs, with lower anticipated values of r2cs leading to higher required sample sizes. therefore, a conservative choice of r2cs is recommended (fig 4).

fig 4
fig 4
how to decide on the model’s anticipated r2cs in advance of data collection

download figure open in new tab download powerpoint
as in sample size calculations for randomised trials evaluating intervention effects, external evidence and expert opinion are required to inform the values that need specifying in the sample size calculator. figure 4 provides guidance for specifying r2cs. importantly, unlike for continuous outcomes when r2cs is bounded between 0 and 1, the r2cs is bounded between 0 and max(r2cs) for binary and time-to-event outcomes. the max(r2cs) denotes the maximum possible value of r2cs, which is dictated by the overall outcome proportion or rate in the development dataset and is often much less than 1. supplementary material s5 shows the calculation of max(r2cs). for logistic regression models with outcome proportions of 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, and 0.01, the corresponding max(r2cs) values are 0.75, 0.74, 0.71, 0.63, 0.48, 0.33, and 0.11, respectively. thus the anticipated r2cs might be small, even for a model with potentially good performance.

step 4: what sample size will produce a small optimism in apparent model fit?
the sample size should also ensure a small difference in the developed models apparent and optimism adjusted values of r2nagelkerke (ie, r2cs/max(r2cs)), as this is a fundamental overall measure of model fit.1038 the apparent r2nagelkerke value is simply the model’s observed performance in the same data as used to develop the model, whereas the optimism adjusted r2nagelkerke value is a more realistic (approximately unbiased) estimate of the model’s fit in the target population. the sample size calculations are shown in supplementary material s6 for continuous outcomes and in figure 5 for binary and time-to-event outcomes. as before, they require the user to specify the anticipated r2cs and the max(r2cs), as described in figure 4. for example, when developing a logistic regression model with an anticipated r2cs of 0.2, and in a setting with an outcome proportion of 0.05 (such that the max(r2cs) is 0.33), 1079 participants are required to ensure the expected optimism in the apparent r2nagelkerke is just 0.05 (see figure 5 for calculation).

fig 5
fig 5
how to calculate the sample size needed to target a small optimism in model fit (to minimise potential model overfitting) for binary and time-to-event outcomes

download figure open in new tab download powerpoint
recommendations and software
box 1 summarises our recommended steps for calculating the minimum sample size required for prediction model development. this involves four calculations for binary outcomes (b1 to b4), three for time-to-event outcomes (t1 to t3), and four for continuous outcomes (c1 to c4). to implement the calculations, we have written the pmsampsize package for stata and r. the software calculates the sample size needed to meet all the criteria listed in box 1 (except b2, which is available at https://mvansmeden.shinyapps.io/beyondepv/), conditional on the user inputting values of required parameters such as the number of candidate predictors, the anticipated outcome proportion in the target population, and the anticipated r2cs. the calculations are especially helpful when prospective data collection (eg, new cohort study) are required before model development; however, they are also relevant when existing data are available to guide the number of predictors that can be considered.

box 1 recommendations for calculating the sample size needed when developing a clinical prediction model for continuous, binary, and time-to-event outcomes
to increase the potential for developing a robust prediction model, the sample size should be at least large enough to minimise model overfitting and to target sufficiently precise model predictions
binary outcomes
for binary outcomes, ensure the sample size is enough to:
estimate the overall outcome proportion with sufficient precision (use equation in figure 1) (b1)
target a small mean absolute prediction error (use equation in figure 2, if number of predictor parameters is ≤30) (b2)
target a shrinkage factor of 0.9 (use equation in figure 3) (b3)
target small optimism of 0.05 in the apparent r2nagelkerke (use equation in figure 5) (b4)
time-to-event outcomes
for time-to-event outcomes, ensure the sample size is enough to:
estimate the overall outcome proportion with sufficient precision at one or more key time-points in follow-up (use equation in figure 1) (t1)
target a shrinkage factor of 0.9 (use equation in figure 3) (t2)
target small optimism of 0.05 in the apparent r2nagelkerke (use equation in figure 5) (t3)
continuous outcomes
for continuous outcomes, ensure the sample size is enough to:
estimate the model intercept precisely (see supplementary material 1) (c1)
estimate the model residual variance with sufficient precision (see supplementary material 2) (c2)
target a shrinkage factor of 0.9 (use equation in figure 3) (c3)
target small optimism of 0.05 in the apparent r2nagelkerke (use equation in figure 5) (c4)
these approaches require researchers to specify the anticipated overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit (r2cs). when the choice of values is uncertain, we generally recommend being conservative and so taking those values (eg, smallest r2cs) that give larger sample sizes
when an existing dataset is already available (such that sample size is already defined), the calculations can be used to identify if the sample size is sufficient to estimate the overall outcome risk or the mean outcome value, and how many predictor parameters can be considered before overfitting becomes a concern
applied examples
we now illustrate the recommendations in box 1 by using three examples.

example 1: binary outcome
north et al developed a model predicting pre-eclampsia in pregnant women based on clinical predictors measured at 15 weeks’ gestation,43 including vaginal bleeding, age, previous miscarriage, family history, smoking, and alcohol consumption. the model included 13 predictor parameters and had a c statistic of 0.71. emerging research aims to improve this and other pre-eclampsia prediction models by including additional predictors (eg, biomarkers and ultrasound measurements).

as the outcome is binary, the sample size calculation for a new prediction model needs to examine criteria b1 to b4 in box 1. this requires us to input the overall proportion of women who will develop pre-eclampsia (0.05) and the number of candidate predictor parameters (assumed to be 30 for illustration). for an outcome proportion of 0.05, the max(r2cs) value is 0.33 (see supplementary material s5). if we assume, conservatively, that the new model will explain 15% of the variability, the anticipated r2cs value is 0.15×0.33=0.05. now we can check criteria b1, b3, and b4 by typing in stata:

pmsampsize, type(b) rsquared(0.05) parameters(30) prevalence(0.05)
this indicates that at least 5249 women are required, corresponding to 263 events and an epp of 8.75. this is driven by criterion b3, to ensure the expected shrinkage required is just 10% (to minimise the potential overfitting). to check criterion b2 in box 1, we can apply the formula in figure 2. this suggests that 544 women are needed to target a mean absolute error in predicted probabilities of ≤0.05. this is much lower than the 5249 women needed to meet criterion b3.

if recruiting 5249 women is impractical (eg, because of time, cost, or practical constraints for data collection), the sample size required can be reduced by identifying a smaller number of candidate predictors (eg, based on existing evidence from systematic reviews44). for example, with 20 rather than 30 candidate predictors, the required sample size to meet all four criteria is at least 3500 women and 175 events (still 8.75 epp).

example 2: time-to-event outcome
many prognostic models are available for the risk of a recurrent venous thromboembolism (vte) after cessation of treatment for a first vte.45 for example, the model of ensor et al included predictors of age, sex, site of first clot, d-dimer level, and the lag time from cessation of treatment until measurement of d-dimer (often around 30 days).46 the model’s c statistic was 0.69 and the adjusted r2cs was 0.051 (corresponding to 8% of the total variation). emerging research aims to extend such models by including additional predictors.

the sample size required for a new model must at least meet criteria t1 to t3.15 this requires us to input a key time point for prediction of vte recurrence risk (eg, two years), alongside the number of candidate predictor parameters (n=30), the anticipated mean follow-up (2.07 years), and outcome event rate (0.065, or 65 vte recurrences for every 1000 person years of follow-up), and the conservative value of r2cs (0.051), with all chosen values based on ensor et al.46 now criteria t1 to t3 can be checked, for example by typing in stata:

pmsampsize, type(s) rsquared(0.051) parameters(30) rate(0.065) timepoint(2) meanfup
(2.07)
this indicates that at least 5143 participants are required, corresponding to 692 events and an epp of 23.1. this is considerably more than 10 epp, and is driven by a desired shrinkage factor (criterion t2) of only 10% to minimise overfitting based on just 8% of variation explained by the model. if the number of candidate predictor parameters is lowered to 20, the required sample size is reduced to 3429 (still an epp of 23.1).

example 3: continuous outcome
hudda et al developed a prediction model for fat free mass in children and adolescents aged 4 to 15 years, including 10 predictor parameters based on height, weight, age, sex, and ethnicity.47 the model is needed to provide an estimate of an individual’s current fat mass (=weight minus predicted fat free mass). on external validation, the model had an r2cs of 0.90. let us assume that the model will need updating (eg, in 10 years owing to changes in the population behaviour and environment), and that an additional 10 predictor parameters (and thus a total of 20 parameters) will need to be considered in the model development.

the sample size for a model development dataset must at least meet the four criteria of c1 to c4 in box 1. this requires us to specify the anticipated r2cs (0.90), number of candidate predictor parameters (n=20), and mean (26.7 kg) and standard deviation (8.7 kg) of fat free mass in the target population (taken from hudda et al47). for example, in stata, after installation of pmsampsize (type: ssc install pmsampsize), we can type:

pmsampsize, type(c) rsquared(0.9) parameters(20) intercept(26.7) sd
(8.7)
this returns that at least 254 participants are required, and so 12.7 participants for each predictor parameter. the sample size of 254 is driven by the number needed to precisely estimate the model standard deviation (criterion c3), as only 68 participants are needed to minimise overfitting (criteria c1 and c2).

extensions and further topics
ensuring accurate predictions in key subgroups
alongside the criteria outlined in box 1, a more stringent task is to ensure model predictions are accurate in key subgroups defined by particular values or categories of included predictors.48 one way to tackle this is to ensure predictor effects in the model equation are precisely estimated, at least for key subgroups of interest.1516 for binary and time-to-event outcomes, the precision of a predictor’s effect depends on its magnitude, the variance of the predictor’s values, the predictor’s correlation with other predictors in the model, the sample size, and the outcome proportion or rate in the study.495051 for continuous outcomes, it depends on the sample size, the residual variance, the correlation of the predictor with other included predictors, and the variance of the predictor’s values.4852535455 note that for important categorical predictors large sample sizes might be needed to avoid separation issues (ie, where no events or non-events occur in some categories),13 and potential bias from sparse events.56

sample size considerations when using an existing dataset
our proposed sample size calculations (ie, based on the criteria in box 1) are still useful in situations when an existing dataset is already available, with a specific number of participants and predictors. firstly, the calculations might identify that the dataset is too small (for example, if the overall outcome risk cannot be estimated precisely) and so the collection of further data is required.5758 secondly, the calculations might help identify how many predictors can be considered before overfitting becomes a concern. the shrinkage estimate obtained from fitting the full model (including all predictors) can be used to gauge whether the number of predictors could be reduced through data reduction techniques such as principal components analysis.10 this process should be done blind to the estimated predictor effects in the full model, as otherwise decisions about predictor inclusion will be influenced by a “quick look” at the results (which increases the overfitting).

sample size requirements when using variable selection
further research on sample size requirements with variable selection is required, especially for the use of more modern penalisation methods such as the lasso (least absolute shrinkage and selection operator) or elastic net.3359 such methods allow shrinkage and variable selection to operate simultaneously, and they even allow the consideration of more predictor parameters than number of participants or outcome events (ie, in high dimensional settings). however, there is no guarantee such models solve the problem of overfitting in the dataset at hand. as mentioned, they require penalty and shrinkage factors to be estimated using the development dataset, and such estimates will often be hugely imprecise. also, the subset of included predictors might be highly unstable60616263; that is, if the prediction model development was repeated on a different sample of the same size, a different subset of predictors might be selected and important predictors missed (especially if sample size is small). in healthcare the final set of predictors is a crucial consideration, owing to their cost, time, burden (eg, blood test, invasiveness), and measurement requirements.

larger sample sizes might be needed when using machine learning approaches to develop risk prediction models
an alternative to regression based prediction models are those based on machine learning methods, such as random forests and neural networks (of which “deep learning” methods are a special case).64 when the focus is on individualised outcome risk prediction, it has been shown that extremely large datasets might be needed for machine learning techniques. for binary outcomes, machine learning techniques could need more than 10 times as many events for each predictor to achieve a small amount of overfitting compared with classic modelling techniques such as logistic regression, and might show instability and a high optimism even with more than 200 epp.26 a major cause of this problem is that the number of predictor (“feature”) parameters considered by machine learning approaches will usually far exceed that for regression, even when the same set of predictors is considered, particularly because they routinely examine multiple interaction terms and categorise continuous predictors.

therefore, machine learning methods are not immune to sample size requirements, and actually might need truly “big data” to ensure their developed models have small overfitting, and for their potential advantages (eg, dealing with highly non-linear relations and complex interactions) to reach fruition. the size of most medical research datasets is better suited to using regression (including penalisation and shrinkage approaches),65 especially as regression also leads to a transparent model equation that facilitates implementation, validation, and graphical displays.

sample size for model updating
when an existing prediction model is updated, the existing model equation is revised using a new dataset. the required sample size for this dataset depends on how the model is to be updated and whether additional predictors are to be included. in our worked examples, we assumed that all parameters in the existing model will be re-estimated using the model updating dataset. in that situation, the researcher can still follow the guidance in box 1 for calculating the required sample size, with the total predictor parameters the same as in the original model plus those new parameters required for any additional predictors.

sometimes, however, only a subset of the existing model’s parameters is to be updated.6667 in particular, to deal with calibration-in-the-large, researchers might only want to revise the model intercept (or baseline survival), while constraining the other parameter estimates to be the same as those in the existing model. in this case the required sample size only needs to be large enough to estimate the mean outcome value or outcome risk precisely (ie, to meet criteria c1, b1, or t1 in box 1). even if researchers also want to update the existing predictor effects, they might decide to constrain their updated values to be equal to the original values multiplied by a constant. then, the sample size only needs to be large enough to estimate one predictor parameter (ie, the constant) for the existing predictors, plus any new parameters the researchers decide to add. such model updating techniques therefore reduce the sample size needed (to meet the criteria in box 1) compared with when every predictor parameter is re-estimated without constraint.

conclusion
patients and healthcare professionals require clinical prediction models to accurately guide healthcare decisions.1 larger sample sizes lead to more robust models being developed, and our guidance in box 1 outlines how to calculate the minimum sample size required. clearly, the more data for model development the better; so if larger sample sizes are achievable than our guidance suggests, use it! of course, any data collected should be of sufficient quality and representative of the target population and settings of application.6869

after data collection, careful model building is required using appropriate methods.1310 in particular, we do not recommend data splitting (eg, into model training and testing samples), as this is inefficient and it is better to use all the data for model development, with resampling methods (such as bootstrapping) used for internal validation.7071 sometimes external information might be used to supplement the development dataset further.727374 lastly, sample size requirements when externally validating an existing prediction model require a different approach, as discussed elsewhere.

<|EndOfText|>

individual participant data meta-analysis of continuous
outcomes: a comparison of approaches for specifying
and estimating one-stage models

one-stage individual participant data meta-analysis models should account for
within-trial clustering, but it is currently debated how to do this. for continuous outcomes modeled using a linear regression framework, two competing approaches are a stratified intercept or a random intercept. the stratified
approach involves estimating a separate intercept term for each trial, whereas
the random intercept approach assumes that trial intercepts are drawn from
a normal distribution. here, through an extensive simulation study for continuous outcomes, we evaluate the impact of using the stratified and random
intercept approaches on statistical properties of the summary treatment effect
estimate. further aims are to compare (i) competing estimation options for the
one-stage models, including maximum likelihood and restricted maximum likelihood, and (ii) competing options for deriving confidence intervals (ci) for
the summary treatment effect, including the standard normal-based 95% ci,
and more conservative approaches of kenward-roger and satterthwaite, which
inflate cis to account for uncertainty in variance estimates. the findings reveal
that, for an individual participant data meta-analysis of randomized trials with a
1:1 treatment:control allocation ratio and heterogeneity in the treatment effect,
(i) bias and coverage of the summary treatment effect estimate are very similar when using stratified or random intercept models with restricted maximum
likelihood, and thus either approach could be taken in practice, (ii) cis are generally best derived using either a kenward-roger or satterthwaite correction,
although occasionally overly conservative, and (iii) if maximum likelihood is
required, a random intercept performs better than a stratified intercept model.
an illustrative example is provided.
keywords
continuous outcomes, estimation, individual participant data, ipd, meta-analysis

1 introduction
individual participant data (ipd) meta-analysis involves obtaining and then synthesizing raw individual-level data from
multiple related studies, to produce summary results that inform clinical decision making.1 the ipd approach is increasingly popular and has many potential advantages over a traditional meta-analysis of published aggregate data, such as
increased power to detect treatment-covariate interactions and avoiding reliance on published results.2
statistical methods to perform an ipd meta-analysis involve either a one-stage or two-stage approach.3 generally, these
approaches give very similar meta-analysis results, especially when they use the same modeling assumptions and/or
estimation methods.4,5 however, the one-stage approach has become increasingly popular over the past decade.6 it conveniently allows all studies to be analyzed simultaneously and avoids the assumption of normally distributed study effect
estimates with known variances that is usually made in the second stage of the two-stage approach. it also allows greater
flexibility of parameter specification over the two-stage approach.6
when conducting a one-stage ipd meta-analysis, it is important to account for clustering of participants within studies,
to correctly condition an individual's response to the study they are in. ignoring clustering and analyzing ipd as if coming
from a single study can result in misleading conclusions. for example, abo-zaid et al7 showed that family history of
thrombophilia was statistically significant as a diagnostic marker of deep vein thrombosis when clustering was accounted
for (odds ratio = 1.30; 95% confidence interval (ci): 1.00, 1.70; p value = 0.05) but not when clustering was ignored (odds
ratio = 1.06; 95% ci: 0.83, 1.37; p value = 0.64). while it is well established that clustering should be accounted for, it is
debatable exactly how this should be done. in particular, there are two competing approaches to account for clustering in
a one-stage model: a stratified intercept or a random intercept. the stratified approach involves a separate intercept term
being estimated for each study; thus, if there are 10 studies, 10 intercept terms would be estimated (one for each study). in
the random intercept approach, the intercepts are assumed to be drawn from some distribution, typically normal with an
underlying mean value and variance. the advantage of the stratified intercept approach is that it makes no assumptions
about the distribution of intercepts across studies. in contrast, the advantage of the random intercept approach is that it
requires fewer parameters to be estimated.
in this article, we evaluate through an extensive simulation study the impact of using either the stratified or random
intercept approach on the statistical properties of the summary treatment effect estimate (for example, in terms of bias,
precision, mean square error (mse), and coverage). this is considered in the context of randomized trials with a continuous outcome and a 1:1 treatment:control allocation ratio, assuming either common or random treatment effects across
trials. two further aims are to (i) compare competing estimation options for the one-stage models, including maximum
likelihood (ml) and restricted maximum likelihood (reml) and (ii) compare competing options for deriving confidence intervals for the summary treatment effect, including the standard normal-based 95% ci, and (for reml, but not
ml estimation) the kenward-roger (kr)8 and satterthwaite9 corrections that inflate confidence intervals to account for
uncertainty in variance estimates.
this paper is structured as follows. in section 2, we introduce the two competing one-stage ipd meta-analysis models of
interest that account for clustering, as well as the competing estimation and ci derivation options. in section 3, we outline
how the simulation study was conducted and present the results, and in section 4, we provide a real example to illustrate
the methods considered. finally, in section 5, we conclude with a discussion of the key findings and limitations and offer
a recommendation for those conducting one-stage ipd meta-analysis of randomized trials with 1:1 treatment:control
allocation ratio and with a continuous outcome.
2 introducing different model specification and
estimation options
consider that ipd have been obtained from i = 1 to k related randomized trials, each investigating a treatment effect
based on a continuous outcome y (say, blood pressure); that is, the mean difference in outcome value between a treatment
and a control group. suppose that there are ni participants in trial i. let yfi j be the end-of-trial (f used to denote final)
continuous outcome value, for participant j in trial i, and ybi j (b to denote baseline) be the pre-treatment outcome value.
let treati j take the value 1 or 0 for participants in the treatment or control group, respectively.
given such ipd, there are several ways in which researchers can use a one-stage meta-analysis to model the summary
treatment effect across trials. we focus initially on presenting one-stage analysis of covariance (ancova) mixed models,
which either use a stratified intercept or a random intercept to account for clustering of participants within trials. we also
assume a random treatment effect since heterogeneity is usually expected.
2.1 model (1): stratified intercept
with the following approach, a stratified intercept is used to account for within-trial clustering.
y𝐹 𝑖𝑗 = 𝛽i + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + ui) treat𝑖𝑗 + e𝑖𝑗 (1)
ui ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
here, 𝛽i denotes the intercept term for trial i (expected final outcome value for participants in the control group in trial i
who have the mean baseline outcome value), and the distinct intercept for each trial is used to account for within trial
clustering. the term 𝜆i denotes a trial-specific adjustment term for the baseline outcome value (here, centered at the
mean for each trial (y𝐵𝑖) to aid interpretation of the trial-specific intercepts). for example, when there are k = 10 trials,
there would be 10 𝛽i terms and 10 𝜆i terms. of main interest is an estimate of the model parameter 𝜃, as this denotes
the summary (average) treatment effect. the random effect, ui, indicates that the true treatment effects in each trial are
assumed to arise from a distribution of true effects with mean 𝜃 and between-trial variance 𝜏2. this assumption could be
constrained if considered appropriate, with a common (fixed) treatment effect (ie, constrain 𝜏2 = 0). lastly, 𝜎2
i denotes a
distinct residual variance per trial.
the flexibility of the one-stage ipd approach allows us to make further modifications by considering, for example, a
common baseline adjustment term (ie, 𝜆i= 𝜆) across trials, or common residual variances (ie, 𝜎2
i = 𝜎2) if necessary5,10,11;
however, this should be justified (eg, based on computational reasons or estimation problems), and sensitivity analysis to
the choice of assumptions is often sensible.
2.2 model (2): random intercept
when there are a large number of trials to be synthesized, a stratified intercept approach to clustering can be computationally intensive (as equation (1) requires estimation of 3 k + 2 parameters).4 an alternative approach for dealing with
clustering, which is preferred by some researchers,12 is to use a random intercept term.
y𝐹 𝑖𝑗 = (𝛽 + u1i) + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗 (2)
u1i ∼ n
(
0, 𝜏2
𝛽
)
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
parameters are as in equation (1), except that within-trial clustering has now been accounted for by a random (instead
of stratified) intercept term, with 𝜏2
𝛽 denoting the between trial variance in the intercept about the mean intercept (𝛽).
equation (2) assumes independence of the two random effects (ie, a covariance of zero), but their correlation could be
accounted for assuming a bivariate random effect distribution; indeed, this might be of special interest when evaluating
the relationship across trials of mean baseline in the control group and true treatment effect.13
compared to equation (1), the number of parameters to be estimated has been reduced, with only 𝛽 and 𝜏𝛽 for the
intercept, instead of k separate terms. therefore, fewer estimation problems might be anticipated than in equation (1). on
the downside, equation (2) makes a strong and potentially unnecessary assumption that control group means are drawn
from a normal distribution with a common mean and variance. furthermore, the estimation of an additional random
effect term might increase computational intensity.
2.3 options for estimation and ci derivation
the parameters in models (1) and (2) are typically estimated using either a ml or reml approach. ml is known to
produce downwardly biased estimates of between trial variance when there are few trials,14-16 whereas reml addresses
the downward bias and is thus generally preferred.17,18
in addition to competing options for model parameter estimation, there are also competing options to subsequently derive (1 − 𝛼)100% cis for the true summary treatment effect (𝜃). standard cis are based on large-sample
inference and assume 𝜃
̂ is approximately normally distributed:
𝜃
̂± z1−𝛼
2
√
var(𝜃
̂), (3)
where 𝜃
̂is the estimate of 𝜃, var(𝜃
̂) is its variance, and z1−𝛼
2
is the upper 1− 𝛼
2 quantile of the standard normal distribution.
this standard approach may produce cis that are too narrow, as var(𝜃
̂) does not account for the uncertainty in the estimate
of the between trial variation of 𝜃
̂.
4,18
to address this, more conservative options are available based on small-sample inference, which define the uncertainty
around 𝜃
̂ using approximations based on a t-distribution, such as the kr8 and satterthwaite9 corrections, which are also
known as denominator-degrees-of-freedom adjustments.
the kr corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
varkr(𝜃
̂), (4)
where 𝜃
̂ is as before, but now a bias-adjusted (inflated) variance (varkr(𝜃
̂)) is used, and t𝜐;1−𝛼
2
(the upper 1−𝛼
2 quantile of
the t-distribution with an adjusted degrees of freedom, 𝜐) instead of z1−𝛼
2
.
for a single parameter of interest (as in our case), the satterthwaite corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
var(𝜃
̂), (5)
where t𝜐;1−𝛼
2
is as in the kr correction, but the original (unadjusted) variance of 𝜃
̂is used. note that, while the denominator
degrees of freedom calculated from the kr and satterthwaite corrections are the same for single hypothesis tests, the kr
correction uses a bias-adjusted variance; therefore, cis derived using equations (4) and (5) will potentially differ, with the
one using the kr correction (equation (4)) leading to slightly wider intervals.19
although schaalje et al20 recommend kr over satterthwaite in special cases when the sampling distribution of the
test statistic is known, there remains debate over the best method, and a lack of literature in this area in regard to ipd
meta-analysis for estimation of a parameter of interest.
3 simulation study
we now perform a simulation study to examine the statistical performance of the summary treatment effect estimate (𝜃
̂)
from a one-stage ipd meta-analysis across a range of scenarios. our aim is to assess the different model specifications,
parameter estimation methods and ci derivation options described in section 2. that is, we compare the following: stratified or random intercept specifications; ml or reml estimation options; and, for reml estimation, 95% cis based on
asymptotic formula (equation (3)) or with either kr or satterthwaite corrections (equations (4) and (5), respectively)).
3.1 methods
provided is a step-by-step guide to our simulation study. for simplicity, and to considerably speed up the many simulations, we removed the baseline adjustment term in models (1) and (2), such that it does not exist in any of the data
generating mechanisms or models fitted in our simulations. in other words, we generate data without baseline imbalances
and thus analyze the data according to a final score ipd meta-analysis model, which is appropriate in this situation.
21 for
similar reasons of simplicity and computational complexity, we assumed a common residual variance across trials (both
in data generation and models fitted). extension to different residual variances is considered in our discussion (section 5).
to inform the true parameter values for the simulation, we used a previous ipd meta-analysis of treatment for lower
blood pressure outcomes.22
all analyses were conducted using stata v.14.2 (stata corporation, tx, usa).23
3.1.1 scenario 1 (base case)
the simulation process is now explained, in the context of an initial base case scenario with ipd from 10 trials and a
relatively simple data generating mechanism. extensions to other more complex scenarios are described afterwards.
step 1: data generating mechanism for one ipd meta-analysis of 10 trials
consider that an ipd meta-analysis of i = 1 to k related trials is of interest, with the goal to summarize a treatment
effect on a continuous outcome. to generate such data for the base case of this simulation study, we started by setting the
number of trials, k, to 10. we set a fixed number of participants, n = 100 in each trial, and assumed a fixed randomization
of 1:1 in each trial; that is, on average, 50% of participants within any given trial are allocated to a treatment group, and
the remaining 50% to a control group. this gave us a triali (trial 1/0 indicator) and treatij (treatment group 1/0 indicator)
value for each of 100 participants in each of 10 trials.
next, based on the previous meta-analysis,22 we set the true parameter values for this simulation to be as follows:
𝜃 = −9.66 (summary treatment effect; negative value favors treatment group), 𝜏2 = 7.79 (between trial variation in the
treatment effect), 𝛽 = 159.73 (mean blood pressure response in control group), 𝜏2
𝛽 = 233.99 (between trial variation in the
intercept), and 𝜎2 = 333.74 (residual variance).
we then used these parameter values to generate further terms, beginning with using 𝜎2 to generate an error term ei j,
for the jth participant from the ith trial
e𝑖𝑗 ∼ n(0, 𝜎2
). (6)
then, we generated the trial level values for the random parts of the intercept and treatment effect terms, u1i and u2i,
respectively,
u1i ∼ n
(
0, 𝜏2
𝛽
)
(7)
u2i ∼ n(0, 𝜏2
).
finally, with all the parameters defined (𝛽, u1i, 𝜃, u2i, treati j, and ei j), we generated the end-of-trial continuous outcome
value yfi j, under the random intercept model (2) (with no baseline adjustment term and assuming a common residual
variance)
y𝐹 𝑖𝑗 = (𝛽 + u1i) + (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗. (8)
this gave one complete ipd meta-analysis dataset of 1000 total participants, containing 100 participants in each of
10 trials, consisting of the following data for each individual: a trial indicator (triali), a treatment group indicator (treati j),
and an end-of-trial continuous outcome value (yfi j).
step 2: model fit and replication
using the generated data, we fitted a stratified intercept model (1) and a random intercept model (2) (without the
baseline adjustment term and assuming a common residual variance) separately to this simulated ipd, under all the combinations of estimation and ci derivation methods outlined in section 2. figure 1 provides a flow diagram summarizing
the possible combinations. each time a model was fitted (under a particular combination of estimation and ci derivation
methods), we stored the following: the summary treatment effect estimate, 𝜃
̂; its corresponding 95% ci; a binary indicator variable for coverage of 𝜃
̂ (ie, the value 1 if the 95% ci of 𝜃
̂ contained the true 𝜃, and 0 otherwise); estimates of any
variance parameters; model run time (from start of model fit to end of post estimation); and model convergence (1/0 for
convergence within 100 iterations/nonconvergence, respectively).
standard
ci
stratified
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
intercept
option
estimation
method
ci method standard
ci
random
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
figure 1 flow diagram of possible combinations of intercept option, estimation, and ci methods. ci, confidence interval;
kr, kenward-roger correction; ml, maximum likelihood estimation; reml, restricted maximum likelihood
for each model (stratified or random intercept) fitted to the data, this enabled us to obtain two estimates of 𝜃
(one each for the models fitted using ml and reml estimation, respectively) and four 95% cis for 𝜃
̂ (one for ml estimation with a standard ci derivation, and then one each for reml estimation with the standard, kr-corrected, and
satterthwaite-corrected ci derivations).
step 3: simulation replications
steps 1 and 2 were repeated until 1000 ipd meta-analysis datasets had been generated using the true parameter values
and procedure as outlined thus far, followed by application of the various intercept option, estimation, and ci methods to
each of the 1000 replicated datasets (note: 1000 simulations were chosen to give a monte carlo error of 0.7% on a coverage
of 95%).
step 4: summarizing performance
using the results obtained after step 3, the statistical properties of 𝜃
̂ under the different model specification and estimation options were assessed by summarizing the 1000 results obtained using the following metrics: mean percentage
(%) bias, empirical standard error (se), mse, coverage (separately for each ci method), convergence, and mean run time
(separately for each ci method). additionally, we considered the median percentage bias in the heterogeneity (𝜏2) of the
true treatment effects also. definitions of these performance measures are provided in web appendix a.
3.1.2 extended set of 38 scenarios changing number of trials, participants, between-trial
distributions, and data generating mechanisms
the base case scenario defined in section 3.1.1 was extended to further settings, leading to an extensive range of
38 scenarios in total (see table 1), which we now summarize.
we varied the number of trials (scenarios a1 and a2), so that k = 5, 10, and 20 were considered, which cover the typical
sizes of ipd meta-analyses in our experience. we also considered trials with differing sample sizes within an ipd, so that
ni (number of participants within trial i) was drawn from a uniform distribution, ni∼u(a, b). fixing a = 30, b = 1000
(scenario b1) allowed for mixed sample sizes, and having 5 trials with a = 30, b = 100 and 5 with a = 900, b = 1000 within
an ipd (scenario b2) tested the effect of a mix of small and large sample sizes only. lastly, fixing a = 30, b = 100 tested
the effect of having only small trials (scenario b3).
we also tested the combined effect of varying the number of trials and number of participants per trial simultaneously
(scenarios b1-a1, b1-a2, b2-a1, b2-a2), and we tested the effects of adjusting the magnitude of the intercept or treatment
effect heterogeneity (scenarios c1, c2, d1, d2).
scenarios 15 to 38 replicate the first 14 scenarios where possible, for modifications to the base case data generating
mechanism. first, to test the robustness of the normality of the intercept assumption in the random intercept model, we
altered the final step of the data generating mechanism in equation (8), so that the final outcome was calculated by
y𝐹 𝑖𝑗 = 𝛽i + (𝜃 + u2i)treat𝑖𝑗 + e𝑖𝑗 (9)
𝛽i ∼ (beta (15, 3)) × 220
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
).
therefore, the intercept term 𝛽i was now derived from a beta distribution with shape parameters of 15 and 3, which
represent a negatively skewed distribution that was then scaled by 220 to give sensible values for systolic blood pressure
(the outcome upon which the hypothetical data is based). an example density plot of this beta distribution for modeling
the intercept term is shown in web figure a.1.
secondly, we also considered a data generating mechanism with a common (fixed) treatment effect (ie, 𝜏2 = 0). here,
the fitted stratified and random intercept models were also modified to have a common treatment effect.
3.2 results
simulation results are shown in tables 2 and 3, covering most of the scenarios under the normal and beta distribution
intercept data generating mechanisms, across all options for specifying and estimating the intercept. these tables show the
mean percentage bias of the summary treatment effect estimate (𝜃
̂) (table 2) and the median percentage bias in its heterogeneity (𝜏̂2) (table 3). figure 2 graphically depicts the percentage coverage of the summary treatment effect estimate (𝜃
̂).
.
table 1 summary of the different simulation scenarios*
scenario data generation details modification from base case scenario
base case (i) number of trials, k = 10 -
(ii) number of participants in trial i, ni = 100 (fixed across all trials)
(iii) fixed treatment exposure of 50%
(iv) 𝜃 = −9.66 (summary treatment effect; negative value favors treatment group)
(v) 𝜏2 = 7.79 (between trial variation in 𝜃)
(vi) 𝛽 = 159.73 (mean response in control group)
(vii) 𝜏𝜷 2 = 233.99 (between trial variation in 𝛽)
(viii) 𝜎2 = 333.74 (residual variance)
a1 same as base case, except changed (i) k = 5
a2 same as base case, except changed (i) k = 20
b1 same as base case, except changed (ii) n ∼i u(30, 1000)
b2 same as base case, except changed (ii) n ∼i u(30, 100) for trials 1 to 5,
n ∼i u(900, 1000) for trials 6 to 10
b1-a1 same as base case, except changed (i) and (ii) k = 5 and n ∼i u(30, 1000)
b1-a2 same as base case, except changed (i) and (ii) k = 20 and n ∼i u(30, 1000)
b2-a1 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 and 2, n ∼i u(900, 1000) for trials 3 to 5
b2-a2 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 to 10, n ∼i u(900, 1000) for trials 11 to 20
b3 same as base case, except changed (ii) n ∼i u(30, 100)
c1 same as base case, except changed (vii) halving 𝜏𝛽 2 to 117
c2 same as base case, except changed (vii) doubling 𝜏𝛽 2 to 468
d1 same as base case, except changed (v) halving 𝜏2 to 3.9
d2 same as base case, except changed (v) doubling 𝜏2 to 15.6
*each scenario was repeated under the following data generating mechanisms: (1) random treatment effect with a normally distributed intercept, (2) random treatment effect with a
220*beta(15, 3) distribution for the intercept (except scenarios c1 and c2), and (3) common treatment effect with a normally distributed intercept (except scenarios d1 and d2).
abbreviations: k = number of trials, ni = number of participants in trial i, 𝜃 = summary treatment effect, 𝜏2 = between trial variation in summary treatment effect, 𝛽 = mean response in
control group, 𝜏𝛽 2 = between trial variation in mean response in control group, 𝜎2 = residual variance, u (a, b) = uniform distribution over the interval (a, b).
table 2 mean percentage bias of the summary treatment effect estimate (𝜃
̂) under different scenarios, for the
random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified (1) and random (2) intercept models, under each of the different
estimation options considered
mean percentage bias of 𝜽̂
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −0.01 0.00 −0.01 −0.01 0.34 0.31 0.33 0.29
a1 −0.90 −0.90 −0.90 −0.90 −0.02 0.13 −0.06 0.10
a2 0.15 0.18 0.16 0.18 −0.48 −0.41 −0.47 −0.40
b1 0.67 0.58 0.68 0.58 −0.57 −0.63 −0.58 −0.63
b2 −0.47 −0.59 −0.47 −0.56 0.29 0.27 0.33 0.28
b1-a1 0.54 0.53 0.53 0.53 −0.14 −0.27 −0.11 −0.24
b1-a2 −0.10 −0.11 −0.10 −0.11 0.08 −0.01 0.10 0.01
b2-a1 −0.41 −0.43 −0.37 −0.41 0.46 0.52 0.05 −0.17
b2-a2 −0.45 −0.41 −0.44 −0.42 −0.45 −0.37 −0.37 −0.34
b3 0.19 0.24 0.21 0.25 1.36 1.34 1.20 1.20
c1 −0.03 −0.02 −0.03 −0.02 n/a n/a n/a n/a
c2 −0.01 0.07 −0.01 0.07 n/a n/a n/a n/a
d1 −0.10 −0.13 −0.10 −0.13 0.26 0.34 0.24 0.32
d2 0.13 0.12 0.13 0.12 0.46 0.49 0.45 0.46
* see table 1 for full data generation details relating to each scenario. true value for 𝜃 is −9.66.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
we focus on the results when assuming a random treatment effect. further results assuming a common treatment effect
data generating mechanism and for additional performance measures (percentage convergence of models, numerical
percentage coverage of the summary treatment effect estimate, average run time of simulations, and empirical se and
mse of the summary treatment effect estimate) are shown in the supplementary material (web appendices b and c,
respectively). in the following, we summarize the key findings.
3.2.1 convergence of models
under a random treatment effect data generating mechanism, the proportion of models that converged was consistently
high, with a minimum convergence of 94.3% across all situations (web table c.i).
note that all other performance measures to follow are estimated conditional on model convergence.
3.2.2 bias of summary treatment effect estimate
generally, there were negligible differences in mean percentage bias of 𝜃
̂ between ml and reml estimation options for
either model (stratified or random intercept), under any given scenario and data generating mechanism (table 2 and
web table b.i). nor were there any important differences in the mean percentage bias of 𝜃
̂ between the stratified model
and random intercept model. furthermore, mean bias was close to zero in all situations and only reached a maximum
absolute percentage of 1.36%.
3.2.3 bias of estimated between-trial variance of treatment effects
for either model (stratified or random intercept), under any given scenario and data generating mechanism, using ml
always produced more downwardly biased estimates than reml (table 3), as expected.14-18 for example, for the base
table 3 median percentage bias of the between-trial variance of treatment effects (𝜏̂2), under different scenarios
for the random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified and random intercept models, under each of the estimation options considered
median percentage bias of ̂𝝉𝟐
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −100.00 −16.86 −41.50 −15.85 −100.00 −14.36 −56.17 −32.86
a1 −100.00 −36.88 −80.33 −33.10 −100.00 −73.62 −100.00 −80.15
a2 −100.00 −8.59 −20.86 −7.74 −100.00 −13.96 −39.78 −25.06
b1 −49.64 −10.74 −22.94 −10.09 −100.00 −14.23 −28.91 −16.39
b2 −56.93 −18.03 −35.11 −17.84 −72.90 −17.92 −35.95 −19.81
b1-a1 −77.28 −19.57 −42.62 −18.45 −100.00 −27.27 −54.23 −30.91
b1-a2 −40.64 −5.83 −13.08 −6.31 −88.22 −9.50 −19.59 −13.53
b2-a1 −72.73 −28.35 −56.23 −28.10 −100.00 −34.30 −64.33 −32.61
b2-a2 −36.66 −4.98 −14.36 −5.39 −50.99 −4.84 −12.86 −5.35
b3 −100.00 −28.68 −61.86 −24.74 −100.00 −17.42 −81.05 −48.75
c1 −100.00 −16.72 −39.54 −14.38 n/a n/a n/a n/a
c2 −100.00 −16.86 −40.56 −15.94 n/a n/a n/a n/a
d1 −100.00 −19.20 −66.97 −24.63 −100.00 −33.67 −99.98 −62.07
d2 −100.00 −11.65 −30.04 −11.79 −100.00 −10.50 −37.84 −22.19
* see table 1 for full data generation details relating to each scenario. true value for 𝜏2 is 7.79, except scenarios d1 and d2 where 𝜏2
is equal to 3.9 and 15.6, respectively.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
case scenario with the random intercept model, under the normal intercept data generating mechanism, the median
percentage bias using reml estimation was −15.9% compared to −41.5% using ml estimation. the bias was worse when
using a stratified intercept model (due to the extra number of parameters to estimate), as ml estimation often produced
a downward median bias of 100%.
when using reml estimation, there were generally only small differences between random and stratified intercept
models in terms of bias of the between-trial variance of treatment effects; however, while better than ml, downward bias
was not removed entirely with reml. furthermore, the overall size of the bias was typically greater in the beta distribution
intercept case than in the normal distribution intercept case, regardless of which model was used.
3.2.4 empirical se and mse of summary treatment effect estimate
there were negligible differences in empirical se or mse of 𝜃
̂ between the two models (stratified or random intercept),
under any given scenario and data generating mechanism (web tables c.viii to c.x).
3.2.5 coverage of summary treatment effect estimate
there were marked differences observed in the coverage of 𝜃
̂ across the different estimation approaches (ml or reml)
and ci derivations (standard, kr, or satterthwaite), as now explained.
(i) under a normal distribution intercept generating mechanism
we consider first the normal distribution intercept generating mechanism (figure 2a and web table c.ii). across both
models and all scenarios, ml with standard ci (ml + standard) derivation always exhibited under-coverage compared to
the other options (reml+standard, reml+kr, reml+satterthwaite). for example, for scenario b2 using the stratified
intercept model, the percentage coverage using ml + standard was 81.3% compared to 88.8%, 95.8%, and 94.8% using
figure 2 percentage coverage of the summary treatment effect estimate (𝜃
̂) under different scenarios for the random treatment effect
with normal (figure 2a) and beta distributions (figure 2b) for the intercept data generating mechanisms, for stratified (left) and random
(right) intercept models, under each of the estimation and ci derivation options considered. options: ml, maximum likelihood estimation
with standard confidence interval (ci) derivation; reml, restricted maximum likelihood estimation with standard ci derivation;
reml+kr, reml estimation with kenward-roger ci derivation; reml+satt, reml estimation with satterthwaite ci derivation [colour
figure can be viewed at wileyonlinelibrary.com]
reml+standard, reml+kr and reml+satterthwaite, respectively. the random intercept model always performed
better with respect to coverage under ml than the stratified intercept model under ml, likely due to the reduction in the
number of parameters that needed estimation. for example, when considering only small trials (scenario b3), percentage
coverage improved from 91.3% to 94.5% (close to the nominal 95% level), when comparing the stratified to a random
intercept model with ml estimation.
using reml substantially improved on the coverage obtained from ml and removed any important differences
between the stratified and random intercept models. however, for either model (stratified or random intercept),
reml+standard still had important under-coverage in some scenarios. for example, in scenario b2-a1, a percentage
coverage of 85.7% and 85.8% was observed, under a stratified and random intercept model, respectively.
the reml+kr approach generally improved on the coverage compared to reml+standard, again with no important
differences observed between the stratified and random intercept models. percentage coverage ranged from 95.1 to 98.8%
using reml+kr, while the percentage coverage ranged from 85.7% to 94.9% using reml+standard. the improvement
gained by using reml+kr was especially important for scenarios that involved at least 10 trials and a large variation
in sample sizes (b1, b2, b1-a2, b2-a2). for example, for scenario b2 (five small and five large sample sized trials, with
average sample size 66 and 949 in the small and large trials, respectively), percentage coverage from the stratified intercept
model was 88.8%, using reml+standard, but 95.8% using reml+kr.
using reml+satterthwaite gave very similar results to reml+kr. occasionally, there was some over-coverage using
reml+kr or reml+satterthwaite, particularly when using a low number of trials (k = 5). for example, coverage was
close to 99% (regardless of which model was used), in a setting of k = 5 trials with an equal number of participants
per trial (scenario a1; ni = 100), and in a setting of k = 5 trials with some small-sized and some large-sized trials (scenario
b2-a1; 2 small trials where ni∼u(30, 100), and 3 large trials where ni∼u(900, 1000)).
(ii) under a beta distribution intercept generating mechanism
for the beta distribution intercept generating mechanism (figure 2b and web table c.iii), using reml+standard again
gave better coverage than using ml, and using reml+kr or reml+satterthwaite generally further improved upon this
coverage (ie, moved it closer to 95%), especially with scenarios concerning at least 10 trials that had a large variation in
sample sizes.
as before, under ml estimation, the random intercept model showed better estimates of between-trial variance and
improved coverage (closer to 95%) than the stratified intercept model. however, differences between the two models were
generally small for estimation under reml (with or without a 95% ci correction).
3.2.6 common treatment effect data generating mechanism
results based on a common (fixed) treatment effect data generating mechanism are shown in web appendix b. all fitted
models assumed a common treatment effect and converged every time (ie, 100% convergence), and there was negligible
difference in mean percentage bias of 𝜃
̂ between ml and reml estimation options for either model (stratified or
random intercept), or between either model (web table b.1). the percentage coverage results were stable across all comparisons, ranging from 93.8 to 96.0%, with negligible differences between the various models and estimation options
(web figure b.1).
3.2.7 key findings
a summary of the key findings from this simulation study for settings with between-trial heterogeneity in the treatment
effect is given in figure 3.
figure 3 key simulation findings and recommendation for estimating a summary treatment effect based on a one-stage individual
participant data (ipd) meta-analysis of randomized trials with a 1:1 treatment:control allocation ratio and a continuous outcome, with
between-study heterogeneity in the treatment effect. ci, confidence interval; ml, maximum likelihood estimation; mse, mean square error;
reml, restricted maximum likelihood; se, standard error
4 illustration of methods and key findings in a real example
the international weight management in pregnancy (i-wip) collaborative group dataset includes ipd from 36 trials
(12,447 women), collected for a health technology assessment report in 2017.24 the authors investigated the association
between diet and lifestyle interventions to prevent weight gain in pregnancy and several other primary outcomes. here, we
present ipd meta-analysis results using the i-wip dataset, for illustration purposes only, to demonstrate the key findings
from our simulation study. we include only trials that collected follow-up outcome values for weight in pregnancy and
apply a one-stage ipd model for this continuous outcome, with model assumptions in line with our simulation study
analysis. however, while we did not generate any baseline imbalances in our simulation data, baseline weight imbalance
was present in some trials from the i-wip data. to remove this imbalance, we apply a baseline adjustment in our models,
as is recommended.21
table 4 shows ipd meta-analysis results for a random sample of 5 and 10 trials that investigated exercise interventions
and for 20 trials (using all 15 exercise trials, plus 5 additional trials that investigated mixed interventions).
these results are in agreement with the key findings observed in section 3.2 and summarized in figure 3. firstly,
the magnitude of summary treatment effect estimate was similar throughout, irrespective of model used or estimation
method. secondly, with ml estimation, the stratified intercept model gave narrower 95% cis and smaller estimates of the
between trial variance than the random intercept model, especially with k = 20 trials. thirdly, using reml overcame this
discrepancy, with now very similar results between the random and stratified intercept models; in addition, the 95% cis
were wider when using reml, due to the larger estimates of the between trial variance. fourthly, applying a kr or
satterthwaite correction in addition to reml further widened the 95% cis. finally, although 95% cis were slightly wider
when using a kr correction instead of satterthwaite, results were generally similar from these two corrections, especially
with k = 20 trials.
5 discussion
5.1 key findings
in summary, we have conducted an extensive simulation study to examine the estimation of a summary treatment effect
using a one-stage ipd meta-analysis model for a continuous outcome. specifically, we examined different options for
specifying the trial-specific intercepts and compared different options for parameter estimation and ci derivation. fourteen different scenarios were tested (varying the number of trials, number of participants per trial, and heterogeneity of
parameters), for each of three different data generating mechanisms (encompassing a common and random treatment
effect with a normally distributed intercept, as well as a beta distributed intercept and random treatment effect). all scenarios assumed a 1:1 treatment:control allocation ratio, and a data generating mechanism that was based on a random
intercept model; hence, our conclusions are restricted to this context.
our key findings, for settings with heterogeneity in treatment effect, were illustrated using a real example, and these
are summarized in figure 3. firstly, the results suggest that, as long as the same estimation method is used, there are no
important differences between the stratified and random intercept models in terms of bias, empirical se or mse for the
summary treatment effect estimate. indeed, the mean bias in 𝜃
̂ was close to zero throughout, which is perhaps expected
given the statistical theory underpinning linear mixed models. furthermore, when using reml (with or without a ci
correction), there were generally no important differences in coverage performance between the stratified and random
intercept models. interestingly, the random intercept model (which assumes normality of the intercept) performed well
even when the trial intercepts were drawn from a highly asymmetric beta distribution. kahan and morris25 also found
that misspecifying the random intercept distribution of random effects models did not impact treatment effect results.
secondly, the kr and satterthwaite corrections generally performed similarly in terms of improving the coverage and
were especially effective for scenarios involving at least 10 trials with a mix of small and large sample sizes, but also
considerably increased mean run time in these instances (see web tables c.v to c.vii). the satterthwaite correction
always had a similar or quicker average run time than kr (sometimes by more than eight times). one could surmise from
the similarity in coverage performance that the main impact of both corrections is in the use of a t-distribution to derive
cis and that the kr adjusted variance of the summary estimate has relatively less impact.
thirdly, when using ml estimation, the random intercept model always showed better or comparable coverage to the
stratified intercept model (closer to 95%). this is likely due to the random intercept model having a reduced number of
.
table 4 results from baseline weight adjusted individual participant data meta-analysis of i-wip data: summary treatment effect estimate (
̂
𝜃) with 95% confidence interval and
between-trial variance of treatment effects estimate (̂𝜏2). from meta-analysis with different numbers of trials (k = 5, 10, or 20), and assuming a random treatment effect and a common
residual variance throughout
̂
𝜽 (95% ci); ̂𝝉𝟐
method for modeling stratified random
intercept intercept intercept
estimation ml reml reml+kr reml+satt ml reml reml+kr reml+satt
number of trials
5 −1.172 −1.172 −1.172 −1.172 −1.170 −1.171 −1.171 −1.171
(−1.811,−0.534); (−1.815,−0.530); (−3.114, 0.770); (−2.712, 0.367); (−1.811,−0.529); (−1.813,−0.528); (−3.072, 0.731); (−2.681, 0.340);
8.58e−17 3.94e−15 3.94e−15 3.94e−15 2.95e−14 4.51e−12 4.51e−12 4.51e−12
10 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972
(−1.479,−0.465); (−1.482,−0.462); (−1.740,−0.204); (−1.653,−0.291); (−1.481,−0.462); (−1.482,−0.462); (−1.731,−0.212); (−1.646,−0.298);
1.97e−16 2.58e−12 2.58e−12 2.58e−12 9.52e−11 5.94e−16 5.94e−16 5.94e−16
20 −0.821 −0.820 −0.820 −0.820 −0.830 −0.830 −0.830 −0.830
(−1.102,−0.540); (−1.243,−0.396); (−1.298,−0.342); (−1.286,−0.354); (−1.217,−0.442); (−1.235,−0.426); (−1.290,−0.370); (−1.276,−0.384);
1.11e−14 0.317 0.317 0.317 0.210 0.258 0.258 0.258
ci = confidence interval
options: ml, maximum likelihood estimation with standard ci derivation; reml, restricted maximum likelihood estimation with standard ci derivation; reml+kr, reml estimation with kenward-roger
ci derivation; reml+satt, reml estimation with satterthwaite ci derivation.
parameters, and thus improved ml estimation of the between-trial variance. a similar finding was also recently shown
by jackson et al26 for one-stage meta-analysis models for a binary outcome. nevertheless, even the random intercept
model produced downwardly biased estimates of the between-trial variance using ml and low coverage. using reml
is therefore important, to improve on this coverage. indeed, coverage is more consistently near 95% when using reml
with either a kr or satterthwaite correction. however, on some occasions (particularly, when there are a low number of
trials), the kr and satterthwaite corrections lead to over-coverage. this is similar to the hartung-knapp sidik-jonkman
correction to 95% cis following a two-stage analysis,27,28 which generally gives a more suitable coverage than a standard
95% ci, although on occasion is overly conservative.29
if there is genuinely no heterogeneity in treatment effect across trials, however, our findings suggest that there are
generally no differences in mean bias, empirical se, mse, or coverage for the treatment effect between the stratified
and random intercept models, for any estimation method, ci derivation approach, and under any simulation scenario.
however, in our experience, situations of completely homogeneous treatment effects are unlikely.
unreported simulations
following recent work by morris et al5 and jackson et al,26 which considered an alternative coding for the binary treatment group variable (+0.5/−0.5 for treatment/control groups, respectively), we also tested this treatment group coding
for our reml estimation simulation results but found only small differences in performance results compared to the 1/0
coding. hence, we did not present the results here. we also tested using stratified (instead of common) residual variances
for both the data generating mechanisms and models fitted. again, no difference in performance of the summary treatment effect estimate was observed, suggesting that the ipd model may be robust to the (mis) specification of the residual
variances. morris et al5 also found that assuming common or distinct residual variances, in a common treatment effect
ipd meta-analysis setting, has very little impact on the precision of the summary effect when the number of patients per
trial is over 25. in general, from a point of principle, we recommend a separate residual variance for each trial, but in situations where this has convergence problems, a common residual variance would seem apt. further research of this issue
would be welcome.
5.2 recommendation
for researchers conducting a one-stage ipd meta-analysis of randomized trials with a 1:1 treatment:control allocation
ratio and a continuous outcome and aiming to estimate a summary treatment effect that is heterogeneous across trials,
we recommend that either a stratified or random intercept model is used, and estimated using reml, ideally followed
by a 95% ci derived using either the kr or satterthwaite approaches. in our simulations, this approach gave close to zero
mean bias in the summary treatment effect estimate and coverage generally close to 95%, except in a few situations where
there was over-coverage (particularly, when there were a low number of trials).
using reml with a kr correction for linear mixed models based on continuous outcomes has already been proposed
by some researchers,30,31 while literature advocating the merits of the satterthwaite correction is less common. however,
in our simulations, we found that the satterthwaite correction generally obtains similar results to the kr approach, hence
making for an excellent alternative.
5.3 limitations and further research
throughout this simulation study, we focused solely on synthesizing trials containing a 1:1 treatment:control allocation ratio; hence, an important limitation is that our conclusions may not hold under settings involving other treatment
allocation ratios.
in addition, we have focused solely on ipd of continuous outcomes, hence another important limitation is that our
conclusions are not necessarily generalizable to other popular outcome types in the meta-analytical field, such as binary
and time-to-event outcomes. binary outcomes, for example, are more complex to deal with than continuous outcomes,
as a logistic mixed effects model is nonlinear, and hence, the corresponding maximum likelihood function has no closed
form. jackson et al26 recently investigated the use of ml estimation and found that a stratified intercept model leads
to substantial downward bias in between-trial variance estimates and under-coverage of cis for the summary result,
which increases as the number of trials (and thus parameters) increases. interestingly, the issue was resolved when using
a + 0.5/−0.5 coding for the treatment variable, rather than a 1/0 coding, or when placing random effects on the trial
intercept.26 mcneish32 investigated logistic mixed models by either retaining the nonlinearity of the model and making
an approximation for the likelihood function or linearly approximating the model to give the likelihood function a closed
form (pseudo-likelihood approach). the latter option was shown to be favorable (under the specific conditions of the
study), by use of a residual penalized quasi-likelihood with a kr correction.
while our simulation study did consider an extensive range of scenarios—we varied the number of trials, number of
participants per trial, and heterogeneity of parameters—we recognize that our conclusions were based on a final score
model that did not adjust for baseline outcome value. often, the ancova model should be used, as in our applied
example, because there will be baseline imbalances in practice. however, as baseline values did not vary across individuals in our simulation study, using a final score analysis model rather than ancova was appropriate. when using
one-stage ancova ipd models, an additional issue is using stratified adjustment terms or placing a random effect on
the adjustment term. based on our study findings, we expect that, with reml, either approach should be suitable.
we also assumed independence of the two random effects (ie, a covariance of zero) when assuming random intercept
and random treatment effects, both in the data generating mechanism and when fitting the corresponding model. their
correlation could be taken into account if deemed sensible13; however, we did not consider this alternative assumption
in our simulation study, largely due to the added complexity and difficulty in estimating the correlation parameter in
practice with few trials. importantly, it is perhaps likely that the effect of treatment could be correlated with the control
group outcome, and therefore, the most appropriate assumption needs further consideration.
another limitation is that we did not consider prediction intervals. these allow us to make predictive inferences of
the potential treatment effect in a single setting of application.33 some researchers argue that prediction intervals offer
a more appropriate summary of trial findings than cis of the average effect.34 however, partlett and riley18 showed in a
two-stage ipd meta-analysis setting that there was considerable under-coverage of prediction intervals in some situations.
for example, under-coverage was observed in settings involving a low heterogeneity or with varied trial sample sizes and
was not improved upon by increasing the number of trials or using ci corrections such as kr. hence, we did not consider
it useful to consider prediction intervals in our study.
finally, we could have considered a bayesian approach to our simulation study, which is an alternative to frequentist
methods, and a natural way to account for all parameter uncertainty, to make predictions and to derive (joint) probabilistic
statements regarding parameters of interest. however, we deemed this extension to be beyond the scope of this paper.
yet, if a bayesian approach is to be used in practice, bayesians still need to choose between random or stratified intercept
one-stage ipd models, which is something that our work can help with.
5.4 conclusions
in an ipd meta-analysis of trials with a 1:1 treatment:control allocation ratio and a continuous outcome, aiming to estimate a summary treatment effect that is heterogeneous across trials, our findings suggest that researchers use either a
stratified or random intercept model with reml estimation and ideally derive 95% cis using either the kr or satterthwaite approach. further work is needed to improve upon coverage in a few situations where the kr and satterthwaite
intervals are overly conservative. such situations include when there are a low number of trials; these are also situations
where corrections to cis in a two-stage ipd meta-analysis are overly conservative.18


<|EndOfText|>

the value of preseason screening for injury prediction: the development and internal validation of a multivariable prognostic model to predict indirect muscle injury risk in elite football (soccer) players

abstract
background
in elite football (soccer), periodic health examination (phe) could provide prognostic factors to predict injury risk.

objective
to develop and internally validate a prognostic model to predict individualised indirect (non-contact) muscle injury (imi) risk during a season in elite footballers, only using phe-derived candidate prognostic factors.

methods
routinely collected preseason phe and injury data were used from 152 players over 5 seasons (1st july 2013 to 19th may 2018). ten candidate prognostic factors (12 parameters) were included in model development. multiple imputation was used to handle missing values. the outcome was any time-loss, index indirect muscle injury (i-imi) affecting the lower extremity. a full logistic regression model was fitted, and a parsimonious model developed using backward-selection to remove factors that exceeded a threshold that was equivalent to akaike’s information criterion (alpha 0.157). predictive performance was assessed through calibration, discrimination and decision-curve analysis, averaged across all imputed datasets. the model was internally validated using bootstrapping and adjusted for overfitting.

results
during 317 participant-seasons, 138 i-imis were recorded. the parsimonious model included only age and frequency of previous imis; apparent calibration was perfect, but discrimination was modest (c-index = 0.641, 95% confidence interval (ci) = 0.580 to 0.703), with clinical utility evident between risk thresholds of 37–71%. after validation and overfitting adjustment, performance deteriorated (c-index = 0.589 (95% ci = 0.528 to 0.651); calibration-in-the-large = − 0.009 (95% ci = − 0.239 to 0.239); calibration slope = 0.718 (95% ci = 0.275 to 1.161)).

conclusion
the selected phe data were insufficient prognostic factors from which to develop a useful model for predicting imi risk in elite footballers. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.

trial registration
nct03782389

key points
factors measured through preseason screening generally have weak prognostic strength for future indirect muscle injuries, and further research is needed to identify novel, robust prognostic factors.

because of sample size restrictions and until the evidence base improves, it is likely that any further attempts at creating a prognostic model at individual club level would also suffer from poor performance.

the value of using preseason screening data to make injury predictions or to select bespoke injury prevention strategies remains to be demonstrated, so screening should only be considered as useful for detection of salient pathology or for rehabilitation/performance monitoring purposes at this time.

background
in elite football (soccer), indirect (non-contact) muscle injuries (imis) predominantly affect the lower extremities and account for 30.3 to 47.9% of all injuries that result in time lost to training or competition [1,2,3,4,5]. reduced player availability negatively impacts upon medical [6] and financial resources [7, 8] and has implications for team performance [9]. therefore, injury prevention strategies are important to professional teams [9].

periodic health examination (phe), or screening, is a key component of injury prevention practice in elite sport [10]. specifically, in elite football, phe is used by 94% of teams and consists of medical, musculoskeletal, functional and performance tests that are typically evaluated during preseason and in-season periods [11]. phe has a rehabilitation and performance monitoring function [12] and is also used to detect musculoskeletal or medical conditions that may be dangerous or performance limiting [13]. another perceived role of phe is to recognise and manage factors that may increase, or predict, an athlete’s future injury risk [10], although this function is currently unsubstantiated [13].

phe-derived variables associated with particular injury outcomes (such as imis) are called prognostic factors [14], which can be used to identify risk differences between players within a team [12]. single prognostic factors are unlikely to satisfactorily predict an individual’s injury risk if used independently [15]. however, several factors could be combined in a multivariable prognostic prediction model to offer more accurate personalised risk estimates for the occurrence of a future event or injury [15, 16]. such models could be used to identify high-risk individuals who may require an intervention that is designed to reduce risk [17], thus assisting decisions in clinical practice [18]. despite the potential benefits of using prognostic models for injury risk prediction, we are unaware of any that have been developed using phe data in elite football [19].

therefore, the aim of this study was to develop and internally validate a prognostic model to predict individualised imi risk during a season in elite footballers, using a set of candidate prognostic factors derived from preseason phe data.

methods
the methods have been described in a published protocol [20] so will only be briefly outlined. this study has been registered on clinicaltrials.gov (identifier: nct03782389) and is reported according to the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) statement [21, 22].

data sources
this study was a retrospective cohort design. eligible participants were identified from a population of male elite footballers, aged 16–40 years old at manchester united football club. a dataset was created using routinely collected injury and preseason phe data over 5 seasons (1st july 2013 to 19th may 2018). for each season, which started on 1st july, participants completed a mandatory phe during week 1 and were followed up to the final first team game of the season. if eligible participants were injured at the time of phe, a risk assessment was completed by medical staff. only tests that were appropriate and safe for the participant’s condition were completed; examiners were not blinded to injury status.

participants and eligibility criteria
during any season, participants were eligible if they (1) were not a goalkeeper and (2) participated in phe for the relevant season. participants were excluded if they were not contracted to the club for the forthcoming season at the time of phe.

ethics and data use
informed consent was not required as data were captured from the mandatory phe completed through the participants’ employment. the data usage was approved by the club and university of manchester research ethics service.

outcome
the outcome was any time-loss, index imi (i-imi) of the lower extremity. that is, any i-imi sustained by a participant during matches or training, which affected lower abdominal, hip, thigh, calf or foot muscle groups and prohibited future football participation [23]. i-imis were graded by a club doctor or physiotherapist according to the validated munich consensus statement for the classification of muscle injuries in sport [24, 25], during routine assessments undertaken within 24 h of injury. these healthcare professionals were not blinded to phe data.

sample size
we allowed a maximum of one candidate prognostic factor parameter per 10 i-imis, which at the time of protocol development, was the main recommendation to minimise overfitting (additional file 1) [20, 26]. the whole dataset was used for model development and internal validation, which agrees with methodological recommendations [27].

candidate prognostic factors
the available dataset contained 60 candidate factors [20]. because of the sample size considerations, before any analysis, the set of candidate factors was reduced. initially, an audit was conducted to quantify missing values and to determine the measurement reliability of the eligible candidate factors [20]. any candidate factors which had greater than 15% missing data or where reliability was classed as fair to poor (intraclass correlation coefficient < 0.70) were excluded [20] (additional file 2). of the remaining 45 eligible factors, previous evidence of prognostic value [19] and clinical reasoning were used to select candidate prognostic factors suitable for inclusion [20]. this process left a final set of 10 candidate factors, represented by 12 model parameters (table 1). the 35 factors that were not included in model development are also listed in additional file 2, and will be utilised in a related, forthcoming exploratory study which aims to examine their association with indirect muscle injuries in elite football players.

table 1 set of candidate prognostic factors (with corresponding number of parameters) for model development
full size table
statistical analysis
data handling—outcome measures
each participant-season was treated as independent. participants who sustained an i-imi were no longer considered at risk for that season and were included for further analysis at the start of the next season if still eligible. any upper limb imi, trunk imi or non-imi injuries were ignored, and participants were still considered at risk.

eligible participants who were loaned to another club throughout that season, but had not sustained an i-imi prior to the loan, were still considered at risk. i-imis that occurred whilst on loan were included for analysis, as above. permanently transferred participants (who had not sustained an i-imi prior to leaving) were recorded as not having an i-imi during the relevant season and exited the cohort at the season end.

data handling—missing data
missing values were assumed to be missing at random [20]. the continuous parameters generally demonstrated non-normal distributions, so were transformed using normal scores [35] to approximate normality before imputation, and back-transformed following imputation [36]. multivariate normal multiple imputation was performed, using a model that included all candidates and i-imi outcomes. fifty imputed datasets were created in stata 15.1 (statacorp llc, texas, usa) and analysed using the mim module.

prognostic model development
continuous parameters were retained on their original scales, and their effects assumed linear [22]. a full multivariable logistic regression model was constructed, which contained all 12 parameters. parameter estimates were combined across imputed datasets using rubin’s rules [37]. to develop a parsimonious model that would be easier to utilise in practice, backward variable selection was performed using estimates pooled across the imputed datasets at each stage of the selection procedure to successively remove non-significant factors with p values > 0.157. this threshold was selected to approximate equivalence with akaike’s information criterion [38, 39]. multiple parameters representing the same candidate factor were tested together so that the whole factor was either retained or removed. candidate interactions were not examined, and no terms were forced into the model. all analyses were conducted in stata 15.1.

assessment of model performance
the full and parsimonious models were used to predict i-imi risk over a season, for every participant-season in all imputed datasets. for all performance measures, each model’s apparent performance was assessed in each imputed dataset and then averaged across all imputed datasets using rubin’s rules [37]. discrimination determines a model’s ability to differentiate between participants who have experienced an outcome compared to those who have not [40], quantified using the concordance index (c-index). this is equivalent to the area under the receiver operating characteristic (roc) curve for logistic regression, where 1 demonstrates perfect discrimination, whilst 0.5 indicates that discrimination is no better than chance [41].

calibration determines the agreement between the model’s predicted outcome risks and those observed [42], evaluated using an apparent calibration plot in each imputed dataset. all predicted risks were divided into ten groups defined by tenths of predicted risk. the mean predicted risks for the groups were plotted against the observed group outcome proportions with corresponding 95% confidence intervals (cis). a loess smoothing algorithm showed calibration across the range of predicted values [43]. for grouped and smoothed data points, perfect predictions lie on the 45° line (i.e. a slope of 1).

the systematic (mean) error in model predictions was quantified using calibration-in-the-large (citl), which has an ideal value of 0 [40, 42], and the expected/observed (e/o) statistic, which is the ratio of the mean predicted risk against the mean observed risk (ideal value of 1) [40, 42]. the degree of over or underfitting was determined using the calibration slope, where a value of 1 equals perfect calibration on average across the entire range of predicted risks [22]. nagelkerke’s pseudo-r2 was also calculated, which quantifies the overall model fit, with a range of 0 (no variation explained) to 1 (all variation explained) [44].

assessment of clinical utility
decision-curve analysis was used to assess the parsimonious model’s apparent clinical usefulness in terms of net benefit (nb) if used to allocate possible preventative interventions. this assumed that the model’s predicted risks were classed as positive (i.e. may require a preventative intervention) if greater than a chosen risk threshold, and negative otherwise. nb is then the difference between the proportion of true positives and false positives, where both were weighted by the odds of the chosen risk threshold and also divided by the sample size [45]. positive nb values suggest the model is beneficial compared to treating none, which has no benefit to the team but with no negative cost and efficiency implications. the maximum possible nb value is the proportion with the outcome in the dataset.

the model’s nb was also compared to the nb of delivering an intervention to all individuals. this is considered a treat-all strategy, offering maximum benefit to the team, but with maximum negative cost and efficiency implications [17]. a model has potential clinical value if it demonstrates higher nb than the default strategies over the range of risk thresholds which could be considered as high risk in practice [46].

internal validation and adjustment for overfitting
to examine overfitting, the parsimonious model was internally validated using 200 bootstrap samples, drawn from the original dataset with replacement. in each sample, the complete model-building procedure (including multiple imputation, backward variable selection and performance assessment) was conducted as described earlier. the difference in apparent performance (of a bootstrap model in its bootstrap sample) and test performance (of the bootstrap model in the original dataset) was averaged across all samples. this generated optimism estimates for the calibration slope, citl and c-index statistics. these were subtracted from the original apparent calibration slope, citl and c-index statistics to obtain final optimism-adjusted performance estimates. the nagelkerke r2 was adjusted using a relative reduction equivalent to the relative reduction in the calibration slope.

to produce a final model adjusted for overfitting, the regression coefficients produced in the parsimonious model were multiplied by the optimism-adjusted calibration slope (also termed a uniform shrinkage factor), to adjust (or shrink) for overfitting [47]. finally, the citl (also termed model intercept) was then re-estimated to give the final model, suitable for evaluation in other populations or datasets.

complete case and sensitivity analyses
to determine the effect of multiple imputation and player transfer assumptions on model stability, the model development process was repeated: (1) as a complete case analysis and (2) as sensitivity analyses which excluded all participant-seasons where participants had not experienced an i-imi up to the point of loan or transfer, which were performed as both multiple imputation and complete case analyses.

results
participants
during the five seasons, 134 participants were included, contributing 317 participant-seasons and 138 imis in the primary analyses (fig. 1). three players were classified as injured when they took part in phe (which affected three participant-seasons). this meant they were unavailable for full training or to play matches at that time. however, these players had commenced football specific, field-based rehabilitation around this time, so also had similar exposure to training activities as the uninjured players. as such, these players were included in the cohort because it was reasonable to assume that they could also be considered at risk of an i-imi event even during their rehabilitation activities.

fig. 1
figure1
participant flow chart. key: n = participants; i-imi = index indirect muscle injury

full size image
table 2 describes the frequency of included participant-seasons, and the frequency and proportion of recorded i-imi outcomes across all five seasons. for the sensitivity analyses (excluding loans and transfers), 260 independent participant-seasons with 129 imis were included; 36 participants were transferred on loan, whilst 14 participants were permanently transferred during a season, which excluded 57 participant-seasons in total (fig. 1). table 2 also describes the frequency of excluded participant-seasons where players were transferred either permanently or on loan, across the 5 seasons.

table 2 frequency of included participant-seasons, i-imi outcomes and participant-seasons affected by transfers, per season (primary analysis)
full size table
table 3 shows anthropometric and all prognostic factor characteristics for participants included in the primary analyses. these were similar to those included in the sensitivity analyses (additional file 3).

table 3 characteristics of included participants in the primary analysis
full size table
missing data and multiple imputation
all i-imi, age and previous muscle injury data were complete (table 3). for all other candidates, missing data ranged from 6.31 (for hip internal and external rotation difference) to 13.25% for countermovement jump (cmj) power (table 3). the distribution of imputed values approximated observed values (additional file 4), confirming their plausibility.

model development
table 4 shows the parameter estimates for the full model and parsimonious model after variable selection (averaged across imputations).

table 4 results of the full and parsimonious multivariable logistic regression models, with prediction formulae
full size table
for both models, only age and frequency of previous imis had a statistically significant (but modest) association with increased i-imi risk (p < 0.157). no clear evidence for an association was observed for any other candidate factor.

model performance assessment and clinical utility
table 4 shows the apparent performance measures for the full and parsimonious models, all of which were similar. figure 2 shows the apparent calibration of the parsimonious model in the dataset used to develop the model (i.e. before adjustment for overfitting). these were identical across all imputed datasets because the retained prognostic factors contained no missing values. the parsimonious model had perfect apparent overall citl and calibration slope by definition, but calibration was more variable around the 45° line between the expected risk ranges of 28 to 54%. discrimination was similarly modest for the full (c-index = 0.670, 95% ci = 0.609 to 0.731) and parsimonious models (c-index = 0.641, 95% ci = 0.580–0.703). the apparent overall model fit was low for both models, indicated by nagelkerke r2 values of 0.120 for the full model and 0.089 for the parsimonious model.

fig. 2
figure2
apparent calibration of the parsimonious model (before adjustment for overfitting). key: e:o = expected to observed ratio; ci = confidence interval; i-imi = index indirect muscle injury

full size image
figure 3 displays the decision-curve analysis. the nb of the parsimonious model was comparable to the treat-all strategy at risk thresholds up to 31%, marginally greater between 32 and 36% and exceeded the nb of either default strategies between 37 and 71%.

fig. 3
figure3
decision curve analysis for the parsimonious model (before adjustment for overfitting)

full size image
internal validation and adjustment for overfitting
table 4 shows the optimism-adjusted performance statistics for the parsimonious model, with full internal validation results shown in additional file 9. after adjustment for optimism, the overall model fit and the model’s discrimination performance deteriorated (nagelkerke r2 = 0.064; c-index = 0.589 (95% ci = 0.528 to 0.651). furthermore, bootstrapping suggested the model would be severely overfitted in new data (calibration slope = 0.718 (95% ci = 0.275 to 1.161)), so a shrinkage factor of 0.718 was applied to the parsimonious parameter estimates, and the model intercept re-estimated to produce our final model (table 4).

complete case and sensitivity analyses
the full and parsimonious models were robust to complete case analyses and excluding loans and transfers, with comparable apparent performance estimates. for the full models, the c-index range was 0.675 to 0.705, and nagelkerke r2 range was 0.135 to 0.178, whilst for the parsimonious models, the c-index range was 0.632 to 0.691, and nagelkerke r2 range was 0.102 to 0.154 (additional files 5, 6, 7, 8 and 9). the same prognostic factors were selected in all parsimonious models. the degree of estimated overfitting observed in the complete case and sensitivity analyses was comparable to that observed in the main analysis (calibration slope range = 0.678 to 0.715) (additional files 5, 6, 7, 8 and 9).

discussion
we have developed and internally validated a multivariable prognostic model to predict individualised i-imi risk during a season in elite footballers, using routinely, prospectively collected preseason phe and injury data that was available at manchester united football club. this is the only study that we know of that has developed a prognostic model for this purpose, so the results cannot be compared to previous work.

we included both a full model which did not include variable selection and a parsimonious model, which included a subset of variables that were statistically significant. the full model was included because overfitting is likely to increase when variable inclusion decisions are based upon p values. in addition, the use of p value thresholds for variable selection is somewhat arbitrary. however, the overfitting that could have arisen in the parsimonious model after using p values in this way was accounted for during the bootstrapping process, which replicated the variable selection strategy based on p values in each bootstrap sample.

the performance of the full and parsimonious models was similar, which means that utilising all candidate factors offered very little advantage over using two for making predictions. indeed, variable selection eliminated 8 candidate prognostic factors that had no clear evidence for an association with i-imis. our findings confirm previous suggestions that phe tests designed to measure modifiable physical and performance characteristics typically offer poor predictive value [10]. this may be because unless particularly strong associations are observed between a phe test and injury outcome, the overlap in scores between individuals who sustain a future injury and those who do not results in poor discrimination [10]. additionally, after measurement at a single timepoint (i.e. preseason), it is likely that the prognostic value of these modifiable factors may vary over time [48] due to training exposure, environmental adaptations and the occurrence of injuries [49].

the variable selection process resulted in a model which included only age and the frequency of previous imis within the last 3 years, which are simple to measure and routinely available in practice. our findings were similar to the modest association previously observed between age and hamstring imis in elite players [19]. however, whilst a positive previous hamstring imi history has a confirmed association with future hamstring imis [19], we found that for lower extremity i-imis, cumulative imi frequency was preferred to the time proximity of any previous imi as a multivariable prognostic factor. nevertheless, the weak prognostic strength of these factors explains the parsimonious model’s poor discrimination and low potential for clinical utility.

our study is the first to utilise decision-curve analysis to examine the clinical usefulness of a model for identifying players at high risk of imis and who may benefit from preventative interventions such as training load management, strength and conditioning or physiotherapy programmes. our parsimonious model demonstrated no clinical value at risk thresholds of less than 36%, because its nb was comparable to that of providing all players with an intervention. indeed, the only clinically useful thresholds that would indicate a high-risk player would be 37–71%, where the model’s nb was greater than giving all players an intervention. however, because of the high baseline imi risk in our population (approximately 44% of participant-seasons affected), the burden of imis [1,2,3,4,5] and the minimal costs [10] versus the potential benefits of such preventative interventions in an elite club setting, these thresholds are likely to be too high to be acceptable in practice. accordingly, it would be inappropriate to allocate or withhold interventions based upon our model’s predictions.

because of severe overfitting our parsimonious model was optimistic, which means that if used with new players, prediction performance is likely to be worse [39]. although our model was adjusted to account for overfitting and hence improve its calibration performance in new datasets, given the limitations in performance and clinical value, we cannot recommend that it is validated externally or used in clinical practice.

this study has some limitations. we acknowledge that the development of our model does not formally take account of the use of existing injury prevention strategies, including those informed by phe, and their potential effects on the outcome. rather, we predicted i-imis under typical training and match exposure and under routine medical care. in addition, it should be noted that injury risk predictions at an elite level football club may not generalise to other types of football clubs or sporting institutions, where ongoing injury prevention strategies may not be comparable in terms of application and equipment.

we measured candidate factors at one timepoint each season and assumed that participant-seasons were independent. whilst statistically complex, future studies may improve predictive performance and external validity by harnessing longitudinal measurements and incorporating between-season correlations.

we did not perform a competing risks analysis to account for players not being exposed to training and match play due to injuries other than i-imis. that is, our approach predicted the risk of i-imis in the follow up of players, allowing other injury types to occur and therefore possibly limiting the opportunity for i-imis during any rehabilitation period. the competing risk of the occurrence of non-imis was therefore not explicitly modelled and players remained in the risk set after a non-imi had occurred.

we also merged all lower extremity i-imis rather than using specific muscle group outcomes. although less clinically meaningful, this was necessary to maximise statistical power. nevertheless, our limited sample size prohibited examination of complex non-linear associations and only permitted a small number of candidates to be considered. a lack of known prognostic factors [19] meant that selection was mainly guided by data quality control processes and clinical reasoning, so it is possible that important factors were not included.

risk prediction improves when multiple factors with strong prognostic value are used [15]. therefore, future research should aim to identify novel prognostic factors, so that these can be used to develop models with greater potential clinical benefit. this may also allow updating of our model to improve its performance and clinical utility [50].

until the evidence base improves, and because of sample size limitations, it is likely that any further attempts to create a prognostic model at individual club level would suffer similar issues. importantly, this means that for any team, the value of using preseason phe data to make individualised predictions or to select bespoke injury prevention strategies remains to be demonstrated. however, the pooling of individual participant data from several participating clubs may increase sample sizes sufficiently to allow further model development studies [51], where a greater number of candidate factors could be utilised.

conclusion
using phe and injury data available preseason, we have developed and internally validated a prognostic model to predict i-imi risk in players at an elite club, using current methodological best practice. the paucity of known prognostic factors and data requirements for model building severely limited the model’s performance and clinical utility, so it cannot be recommended for external validation or use in practice. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.