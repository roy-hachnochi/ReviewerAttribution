abstract why might the average paediatrician need to get involved in understanding statistics? what do they need to know? are there simple rules that can be followed in determining the appropriate analyses? where can help be found? these are the questions that we aim to answer in this short review of how to design and analyze research studies. keywords statistical-analyses; study-design; critical-appraisal introduction in the modern day there is a plethora of information available at the practicing clinicians’ fingertips. between <UNK> and <UNK> paediatric journals produce on average about <UNK> publications related to child health annually and these, together with research presented in journals not solely confined to paediatric medicine, contribute roughly <UNK> million paediatric related articles within pubmed online resource. in the us there are currently around <UNK> clinical trials being undertaken on children and these are mainly initiated by universities and institutes rather than drug companies. so, with all this happening, why is there any need to be able to understand the research process and statistics?why not just leave it to the experts to do the work and provide the results? the problem is that not all published, or even all peer-reviewed, information is good information. studies are not always optimally performed and analyses may be sub-standard. anyone wishing to use the available literature to inform practice needs to be able to critically appraise content and not just skim abstracts. there is usually no malicious intent with badly performed research, but nonetheless it frequently exists and the reader needs to be able to identify potential flaws as well as useful and applicable research. even if a study has been well designed, executed and presented, there will need to be verification that it is indeed generalisable to any context the reader may wish to apply the results in. to be fully commensurate with all aspects of medical statistics from research design through to analyses and interpretation of results however is too much of a daunting task for the majority of practicing clinicians. thankfully, such in-depth knowledge is not necessary to be able to discern the good from the bad and to gain useful insights into the quality of publications, nor indeed to perform ones own research study. what is necessary is to understand the basics of design and also the thinking that underlines statistical analyses. this, plus the awareness to recognize where a fairly basic analysis may not be appropriate and it is time to enlist more expert help, should stand most readers in good stead. what do we want to know? whether reviewing the work of others or aiming to undertake our own research, the first thing we need to do is to establish the correct research question. sometimes data is collected primarily for clinical purposes and research a by-product i.e. individuals’ have information documented for their own benefit, but these individual bits may be combined within a research project. alternatively, data may be collected specifically to address the research question posed. in the latter case data may be obtained that can more accurately address the research question. it is generally useful to consider a pico breakdown of the research question: p e patient, problem or population i e intervention or exposure of interest c e comparison o e outcome for example, we may be interested in how to cure nits or find the risk factors for early onset asthma but these questions are too loose and it is not easy to see how they may be answered via a single study. by contrast, the questions: “in primary school children, does combing with conditioner post shampooing (as opposed to just shampooing) lead to less infestations of nits?” and “are family history and introduction of solid food prior to <UNK> months associated with onset of asthma before age <UNK> are clearly answerable with obvious pico elements. note that not all elements occur in all research questions. for example, “what is the prevalence of asthma in <UNK> year olds?” is an answerable well-defined question that has a ‘p’ <UNK> year olds) and an ‘o’ (asthma yes/no), but no i or c. it is, however, useful to consider in each case whether there should be a p, i, c and o and what these are. design, design, design! design is of over-riding importance in any study. with even the most sophisticated elegant analyses, if basic data collection set up is less than ideal, then the conclusions must be tempered to take account of this. there are oftenmany differentways to address the same research question. if you are undertaking your own research, then youwould want to ensure that the most efficient and valid feasible design is used. if you are evaluating published research, then the decision to be made is whether the design they have used, even if not optimal, can usefully address the research question posed. in either case, it is worth remembering that it may be impossible to implement the best theoretical design due to practical, financial or ethical limitations. not all designs fit into a neat framework. figure <UNK> shows the most commonly cited designs where two groups of individuals are to be compared. they are split into observational studies, where the researcher does not change things but merely observes what is happening, and experimental where there is some manipulation of individuals for the study purposes. when observing individuals there are various approaches that can be taken: i) consider past habits etc. that are associated with current outcomes (case-control). ii) consider associations in the present time (cross-sectional). iii) classify individuals and follow forward in time to observe outcomes (cohort). with experimental studies the researcher gets to decide who goes into which group. the split can be done systematically (nonrandomized) or randomly (randomized controlled trial). a before and after trial is a form of non-randomized experimental study which is rarely recommended as there will be no measure of what would have happened in the absence of treatment. randomized controlled trials (rct) where individuals are consented to the study and then randomly allocated to a treatment arm are generally considered the most effective way to show causal relationships. a crossover (or within person) rct is where each individual receives both treatments in random order. a crossover trial is less likely to have hidden confounders but will only be appropriate to investigate a treatment giving short term relief of chronic symptoms. sometimes the associations between behavior and outcomes of individuals are considered on a grouped basis: if the study is observational then this form of study is often known as ecological. for example, an ecological study compared childhood cancer rates between those born in hospitals with differing intramuscular vitamin k policies. no evidence was found of an association and this counteracted previous suggestions that administration of vitamin k may increase childhood cancer risk. if the study is a rct, then the groups will be randomized en masse to different treatment arms and this is known as a cluster randomized trial. for example, villages were randomized to different educational programs in a bid to reduce infant mortality in rural areas of nepal. the outcome for babies born in villages which received the new educational program were compared to those in the villages which did not receive additional input (controls). note that the main named study types are given here but there are plenty of studies that do not easily pigeonhole into these mainstays and yet are equally valid. is there anything that might get in the way of any comparisons? the majority of studies are designed to address comparative research questions. for example: do children with sickle cell anaemia have different <UNK> and/or plasma haptoglobin levels? does giving antenatal corticosteroids (ac) reduce respiratory disorders in late preterm infants? whether the study is observational or experimental, things we are not interested in may get in the way. they do this by having different distributions in the groups being compared and also being associated with outcome. when a variable behaves like this it is known as a confounder. often studies are designed specifically to avoid confounding. groups may be chosen to be similar with respect to the potential confounder(s), for example age and sex matched pairs, stratified randomization. a problem is that there may be many potential confounders, some of which are not even known, and it is not possible to consciously correct for all. a strength of randomized trials is that in a large enough trial all factors will be evened out between the groups and hence there will not be confounding. for example to address the question as to whether ac reduces respiratory disorders in late preterm infants, we need two groups of preterm infants: those given ac or not. there are several ways in which these two groups can be chosen relating to different types of study. i.e.: a) an ecological study: outcomes compared between hospitals with different ac policies. b) a case-control study: babies with and without respiratory disorders compared with respect to ac history. c) a cohort study: ac and not ac groups classified then followed to see who develops respiratory disorders. d) a randomized controlled trial of ac versus not ac. in designs (a)e(c), the observational studies, there is far more capacity for confounding and a causal relationship cannot be proved. with (d) there is less chance of confounding and it is possible to infer causality if a difference is found. how generalisable are any results? generalisability is an issue whether the study is observational or experimental. a random sample of the target population (of the research question) should ideally be drawn whatever the study design. it is worth remembering that such an ideal sample is rarely obtained. for example, individuals were randomly sampled from those registered with the two largest unions in hong kong to investigate associations between lifestyle and obesity; schools were randomly selected in the athens region when assessing the validity of a food questionnaire amongst school children; different doses of vitamin d were compared in a rct in schoolchildren in taleghan near tehran. although these may all provide suitably representative samples, we should consider whether they actually are generalisable outside of the groups the selection was from (unionregistered individuals in hong kong, schoolchildren in the athens region or taleghan). very often there is no need to worry with rcts in particular since what works in one sub-population will probably also work elsewhere. for example, if a rct shows that a preparation is beneficial to asthmatics in liverpool, then it will probably also be beneficial to asthmatics in the rest of the country (and possibly the world), so long as our population of asthmatics is similar. with observational studiesmore caution may be warranted as there is greater potential for confounding and effects may not be constant across different communities. similarly, we should be happy that any sampling is representative on a time basis. for example, if a drug is trialed exclusively in summer, will the effects be similar in winter? when dealing with children there are often differences in diet and exercise during term time and outside which may confound the results if not properly adjusted for. weekends and weekday habits also often differ: the greek children were asked to complete food questionnaires on two consecutive weekdays and one weekend day. if comparisons are made over a long time period then causal agents may be very difficult to infer as there may be underlying improvements for all kinds of related reasons. for example, survival rates increased and levels of severe neurodevelopmental impairment decreased substantially between <UNK> and <UNK> for inborn infants with birthweight below <UNK> g, but it would be hard to pinpoint one single causal factor. summaries for any quantitative study, information is recorded for each individual and the values combined within the study groups to provide summaries of similarities and differences. the appropriate form of summary depends on whether the particular piece of information is numeric (such as height, weight, <UNK> or categoric (such as family history: yes/no, severity of disease: mild/moderate/severe, gender:male/female).acategoric variable with only two categories (eg. yes/no, male/female) is known as binary. numeric data should be summarized as either mean and standard deviation or median and range or inter-quartile range. the median is the <UNK> centile, with half the values being higher than this and half lower, whatever the distribution. the inter-quartile range is the <UNK> centiles i.e. the ‘middle half’ of the data if we chop of the top and bottom quarters. if the distribution of the variable is skew (tailing off in one direction), then the mean and standard deviation should not be used. for example, height does not have a skew distribution since there are as many short as there are tall people whereas incomes do have a skew distribution as there is a minimum level of earning and relatively few very high earners. for height, the mean and median will be approximately equal and either provides a good summary of average, the standard deviation can also be used. for income levels, the mean will not be representative of average earnings as it will be skewed by the few high earners, whereas the median and inter-quartile range will give a useful summary. differences between groups can be quantified as the difference in means or medians as appropriate. categoric data can be summarized using proportions who fall into one category and differences between groups quantified as either the absolute or relative difference. these are sometimes known as the arr (absolute risk reduction) and rr (relative risk) respectively. for example, a recent study showed antenatal corticosteroid treatment at <UNK> weeks gestation did not reduce the incidence of respiratory morbidity <UNK> ¼ <UNK> corticosteroid group; <UNK> <UNK> placebo group). hence arr¼ <UNK> ¼ <UNK> and rr¼ <UNK> ¼ <UNK> respiratory distress syndrome (rds) incidences were also similar <UNK> ¼ <UNK> and <UNK> ¼ <UNK> respectively, yielding an arr of <UNK> and rr of <UNK> ¼ <UNK> note that no difference in absolute terms is given by zero, whereas for relative comparisons, no difference is defined as <UNK> an approximation to the rr which is often used is the odds ratio (or), for which a value of <UNK> similarly means no difference between groups. how many is enough? it is important to study enough individuals to address the research question but unethical to waste time and subject more individuals to scrutiny than is necessary. all studies should have some stated rationale for the proposed numbers to be included prior to study commencement. there are many different sample size formulae and the correct one to use depends on the nature of the outcome (numeric or categoric) and the purpose of the study (to identify a difference between groups or to estimate a quantity with sufficient precision). each formula will require you to provide several pieces of information, such as the size of difference that you want to detect, or the precision required. note however, that there is a circular argument to sample size estimation which may initially appear confusing. calculation is also based on outcome and if you had all the information to do the calculation properly you wouldn’t need to do the study! hence, these calculations can only ever be educated guesswork but nonetheless it is always worth guessing. some web links which contain suitable applications are given at the end of this article. analyses figure <UNK> shows when to use the seven most commonly cited tests for comparing a single outcome between two groups. the aim is either to compare the proportion in one category of a categoric outcome (left hand branch) or the mean (right hand branch, left side) or median (right hand branch, right side) between the groups. if the groups are paired (for example, age and sex matched pairs of individuals in the two groups or a crossover treatment trial yielding paired measurements within person) then the appropriate test is mcnemars (categoric outcome), paired t-test or wilcoxon’s test. if there is no pairing then the appropriate test to compare the groups is chi-square or fishers exact (categoric), two sample t-test or mannewhitney u test respectively. for example: chi-square test was used to compare respiratory morbidity between ac and placebo treated groups <UNK> vs <UNK> p ¼ <UNK> the lownumbers with rds <UNK> and <UNK> meant that fishers exact test was used for this comparison <UNK> a two sample t-test would be used to compare <UNK> levels between patients with and without sickle cell anaemia. mannewhitney u test would be used to compare plasma haemoglobin levels which have a skew distribution. if each child with sickle cell anaemia had a matched control selected from their class at school, then the data would consist of matched pairs and the appropriate tests would be a paired t-test and wilcoxon paired test for the <UNK> and plasma haemoglobin comparisons respectively. if the aim is to make a comparison after adjustment for other factors or to build a predictive model, then regression will be appropriate. the type of regression to use depends on the nature of the outcome. table <UNK> gives an overview of what type of regression to use when. for example: to compare respiratory morbidity rates between those given ac or not, adjusting for gestational age and gender, we would use a logistic regression since the outcome (respiratory morbidity) is binary (yes/no). poisson regression is used for discrete (count) data and results are given as relative risks (rr). for example, a recent study showed that there was a significant decrease in mortality in the delivery room for babies with an estimated ga of <UNK> weeks between <UNK> and <UNK> rr ¼ <UNK> <UNK> ci <UNK> to <UNK> linear regression would be used to compare <UNK> between sickle cell children and controls taking into account age and weight. the extent to which onset of childhood asthma (yes/no) is associated with family history and early diet and infections can be investigated using a logistic regression model. not the whole story papers that only cite p-values as the results of statistical analyses, although commonplace, are flawed. it is not possible to ascertain the clinical impact of a finding using the p-value alone. the p-value merely shows how compatible the data obtained is with the hypothesis of no difference (average difference or arr ¼ <UNK> rr or or ¼ <UNK> but not how likely other scenarios might be. estimates of effect size should always be given and presented with confidence intervals. a <UNK> confidence interval gives the range of population scenarios that the sample data is compatible with <UNK> confidence. for example: neither respiratorymorbidity norrds were significantly different in the trial of corticosteroid versus placebo (rr <UNK> <UNK> and <UNK> p ¼ <UNK> respectively). however, confidence intervals for therrreveal the range ofvalueswhich the trial is compatible with and paint a different picture. the <UNK> intervals for the rr are <UNK> <UNK> formorbidity and <UNK> <UNK> for rds.we are farmore confident in excluding large differences inmorbidity but for rds the interval is very wide (due to the small number of events) and almost <UNK> differences cannot be excluded even though the difference is statistically non-significant. filling the gaps rarely is a study as simple as the textbook examples often given where a single outcome is compared between two or more groups of well-defined individuals. more commonly, there is a whole host of information collected from each participant that will be combined to give an overall picture. some pieces of information may correlate with others to give a more comprehensive overview of the issues. although this is a good thing to do and may lend validity to the ‘main’ question, the collection of large amounts of information carries with it pitfalls. most notably, there is greater capacity for some individuals to have incomplete data. unfortunately most statistical analyses require each individual to supply values for all variables included and so it is necessary to either fill the gaps or exclude those individuals with missing data. figure <UNK> gives a guide to useful and not so useful techniques that can be used. replacing the missing values by some substitute with no allowance for potential error in doing so is always wrong (left hand branch) since this will lead to overly precise results and possibly also bias. if the data are missing completely at random, then removing any individuals with any missing data (listwise deletion) will not introduce bias but precision may be less than it needs to be. ‘missing at random’ means that the missing data could be inferred from that available. for example, if we have the age and gender of a child then we would be able to make a reasonable guess at their height. for data missing at random, or missing completely at random, the preferred option is multiple imputation and this will give unbiased results with the best possible precision. multiple imputation techniques are now available in most large packages and details can be found in the references at the end of this article. particular issues for paediatricians mostly problems are the same as for any discipline. when dealing with frontline clinical medicine, as opposed to the laboratory scientist, there are the additional issues to face around recruitment of people who may choose not to be part of the study or may agree and then not provide all required data. they may not provide valid information (for example, they may be inaccurate in reporting gestation and self-reported smoking histories have long been known to be open to bias) or adhere to preferred time-scales (for example, a scheduled annual follow up may need to be undertaken much earlier or later due to illness or holidays). when dealing with children there are the added problems of whether ethics require assent from the child and/or consent from the parent/guardians, plus the aging process may mean that they are not a homogenous group. the latter is evidenced in the need for age (or growth) related standards to interpret individual data. for example, it is usual to interpret childhood lung function measurements with reference to the height of the child. hence adjustment is often necessary for comparing groups of children unless those groups are only within a relatively small age range (in which case the results would not be generalisable outside of the range). this infers a need for more complex analyses. making sense of it all! in this short article we have given an overviewof some issues facing the paediatrician either trying to interpret published research or undertaking their own study. it cannot be comprehensive but aims to provide a starting point. further recommended reading is given below. to summarize, we hope that: p aediatricians have become more i nformed after reading this article c ompared to those who have not read it, with the longer term o utcome of better understanding of published research.

<|EndOfText|>

fear or favour? statistics in pathology statistics have a role to play in most areas of medical research including the field of pathology. we have come a long way since <UNK> when the british medical journal published excerpts from a debate held by the study circle on medical statistics as to whether the then growing influence of statistics in medicine was, in fact, <UNK> one speaker declared that, “medicine was an art, statistics a science; he conceded that the latter had its uses, but when it came to mixing science and art, statistics was as out of place as a skillet in a crown derby tea-service.” he concluded that “statistics might be all very well for the elite but were a menace to the mob.” someone else “referred darkly to the deliberate misuse of statistics, fostered—for what purpose ?—by statisticians themselves. statistical publications, he said, could be recognised by the prolixity of their tables. in his view no papers should contain any tables at all.” the debate concluded with the motion that the influence of statistics should be welcomed in all branches of medicine and this was carried by a narrow majority on a show of hands. in the intervening <UNK> years there has been a mushrooming of statistical literature designed to assist the medical researcher, with numerous articles highlighting misuses of statistics and giving pointers towards improvement. there has been a growing understanding that statisticians are concerned with the whole process of research, from study design through to final conclusions, and are not merely purveyors of p values and analytical methodology. the more recent evidence based medicine movement has served to further publicise this recognition. as h g wells predicted, “statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.” the lessons have been numerous and only the major developments are reviewed here. emphasis has increasingly been placed on identifying a well defined and answerable research question before undertaking any study. this seemingly obvious prerequisite may be the hardest part, finding the right question often being more troublesome than finding the right answer. in the words of einstein, “the formulation of a problem is often more essential than its solution which may be merely a matter of mathematical or experimental skill.” the incorporation of suitable control groups and some quantification, before starting a study, of the necessary sample size required to conclusively answer the research question have been stressed. the need for some formal statistical comparison of the results, usually resulting in a p value, has also been encouraged. an informal review of the publications in this journal over the last <UNK> years shows that this latter point has indeed been taken on board. the majority of the papers published in jcp in <UNK> contained little or no formal statistical analysis: during the whole of that year only <UNK> papers contained any p values and most of those had only one. by contrast, in the current editions most submissions have at least some formal analysis and usually contain several p values. when interpreting the clinical value of results statisticians have, in more recent years, stressed the importance of quantifying the size of any effect, rather than merely relying on a significance level as given by a p value. to this end, the current guidelines for this journal state that <UNK> confidence intervals should be used wherever appropriate.” this has certainly led to a dramatic increase in usage. during the whole of <UNK> there was only one confidence interval presented in jcp, compared with the current situation where they are to be found in most issues. so, in common with most others, this journal has seen a secular trend in the use of statistics, and the statistical quality of published research is undoubtedly superior to that seen <UNK> years ago. as we enter the new millennium is there anything that can be done to assist yet further improvements? one potential barrier to such improvement is the dearth of statistical guidance aimed specifically at pathologists. it may seem strange to suggest that the wealth of published statistical literature is not directly applicable to research in the field of pathology. every discipline tends to use certain types of study design and forms of statistical analyses more than others. the necessary information is out there but it may be somewhat off-putting to have to delve through a mountain of irrelevant material to find it. by identifying the areas of main interest we can considerably reduce the ground that must be covered to gain the necessary knowledge to produce good quality research that answers useful questions in our particular area. for example, the majority of medical statistics texts aimed at the non-statistician urge us to perform randomised controlled trials, anything else being considered inferior, yet these are hardly ever used by and are largely irrelevant to pathologists. most of the research questions addressed by studies in this journal concern the comparison between two or more previously defined groups (for example, diseased and healthy, different diseased groups, or those at different stages of the same disease); assessing the reliability/ validity/reproducibility of measurements; predicting time to death/recovery/relapse/infection or using new measurements to improve diagnostic accuracy. the rest of this paper will focus on two aspects of medical statistics that are relevant to all of these scenarios yet are still widely misunderstood or poorly presented. the aim is to provide a further learning brick to build on the improvements that have already been seen over the last two decades. choice of sample(s) individuals when we perform studies we are trying to find out what happens in the population. for example, do two measuring instruments give the same readings when applied to patients in disease group x? can we accurately and consistently measure y? can we predict survival or time to relapse from p, q, and z? we cannot measure the whole population, so we observe a subset or sample of individuals and from these infer what we think happens in the population as a whole. statistical analyses are used to make this inference. studies can be irretrievably ruined by the biased choice of samples, particularly if we are unaware of the size and direction of any bias. however, choice of sample appears to receive little thought and is often made according for convenience rather than representativeness. commonly all available samples from a given laboratory or hospital may be included. while this is all that may be feasible within the time and practicality constraints imposed on the researchers, there should be some attempt to identify whether this sample is in fact representative of the population of interest. for example, does this hospital tend to get referred patients at all stages of this disease or is it biased towards the more symptomatic? is the area which this laboratory/hospital serves socially and ethnically representative? given this kind of information, the reader can decide whether the results are likely to apply to their own population. quite commonly it is a subgroup of patients over a certain time frame who are included, these being chosen according to availability of blocks, tissue samples, or data. in this case there needs to be some discussion of the representativeness of the subgroup. for example, were those with available tissue more severely ill? do they tend to have different underlying diseases? were data more carefully recorded in the more unusual diagnostic cases? control groups which consist of “healthy volunteers” may be used. this group should ideally be similar to the disease group except for the presence of disease. it is of interest to know precisely how this group has been recruited to help determine whether it constitutes a reasonable comparison group. for example, sometimes laboratory staff or patients admitted for reasons unrelated to the present study interest are used as controls. in fact either of these might be considered unsuitable. the former, laboratory staff, may be younger than the study disease group and the latter may not be entirely normal with respect to the study measures. volunteers, whether from the disease or the control group, may differ from non-volunteers, and it will usually be impossible to assess the extent of that difference. it is best to avoid advertising for volunteers as a means of recruitment. if <UNK> of those approached to participate refuse then at least we have some measure of how representative the final sample is. when the sample is to consist of a subset of some larger group of eligible individuals, then this subset should be randomly chosen, that is in a manner unbiased by the characteristics of the individuals and in a non-systematic way. random selections must be made using either tables or suitable software and this should be made explicit in the description of the sample selection process. specimens within individuals having identified individuals to be included in the study, there may be the further selection of a particular specimen to be analysed. this has been <UNK> to be “the most observer dependent and therefore most subjective step.” many studies state that “representative sections” or “systematically selected areas” were chosen, but precisely how this was done is not made explicit. if there is a system it should be clearly outlined so that others can make comparable choices. if selection is random, the methodology should be specified. to summarise, while the ideal of comparable and representative samples from the study groups concerned (for example, disease x and healthy controls, disease x and disease y) is rarely attained, we can improve published studies by giving full details of the selection process for both the individuals and specimens from those individuals. the representativeness of these should be discussed to enable readers to identify applicability and potentially confounding variables. a further and related point is that comparability can be improved by ensuring that those who make the assessments are blind to the study group. where several assessors are involved then there should be high inter-rater reliability. size of sample(s) samples are used to estimate population effects. it is intuitively obvious that a larger sample will give a more precise estimation of the population value. for example, if <UNK> of the population display trait x then in a sample of <UNK> from this population we would not be surprised to find anywhere between <UNK> and <UNK> individuals with the trait <UNK> if we sample <UNK> individuals we would not be surprised to observe anywhere between <UNK> and <UNK> <UNK> with the trait; more extreme numbers may lead us to doubt whether the true prevalence of x is actually <UNK> the thinking is similar when we present confidence intervals with sample estimates. from our sample we estimate the population value (for example, the mean or proportion; the difference in means or proportions between normal and diseased individuals or the median survival in different groups). we do not expect this estimate to be exact, although we know that the larger the statistics in pathology <UNK> sample the more precise it will be.a confidence interval gives the range of population values (or differences) that our sample(s) are compatible with: <UNK> confidence intervals give the range within which we are <UNK> confident the population value lies. clearly the addition of a confidence interval facilitates the clinical interpretation of the results and highlights any limitations caused by sample size. most statistical packages now give confidence intervals as standard; details of calculation can be found <UNK> <UNK> the power of a study is its ability to detect a difference of a given <UNK> for example, suppose qrz in the population of individuals with disease k is on average <UNK> and always varies between <UNK> and <UNK> compared with an average of <UNK> and range of <UNK> for normal individuals. (note that the ranges are not symmetric around the averages and this is to stress the idea that this thinking applies not only to normally or symmetrically distributed data.) we may by chance randomly sample disease k patients with a tendency to higher values and normal controls with a tendency to lower values and hence there will be no significant difference in the average values in the samples. we may therefore wrongly conclude that there is no difference in average qrz between the groups. of course this will not always happen and how often it does depends on both the sample sizes (the larger the samples the more closely they tend to approximate their respective population means) and the variability of the measurements in the populations (if disease k measures are mostly between <UNK> and <UNK> and the normals between <UNK> and <UNK> then it will be less likely to happen than if the values of qrz are more evenly spread across the range in each group). the power of the study, usually expressed as a percentage, tells us how often a given difference will be detected for a certain variability and sample size. as the variability of a measure is fixed (that is, it exists in the population and there is nothing we can do about it except perhaps choose a more homogeneous population which will change the research question), the aim is to choose a sample size that will detect a clinically important difference with reasonable power. power is usually set at <UNK> or above. a value of <UNK> means that four times out of five the study will detect the difference if it exists. it is now accepted as standard practice that all published randomised controlled trials should include some statement regarding the power of the <UNK> it is less well recognised that a similar proviso would benefit the presentation of all studies, including non-randomised and single group descriptive studies. such a policy safeguards the researcher against wasting time with a sample that is too small to give conclusive answers. it also serves to assist interpretation where results are nonsignificant, in which case we want to know the power that the study had to detect a difference. the combination of no power calculations, p value reporting, and interpretation with little or no use of confidence intervals is a recipe for potential disaster. <UNK> gives a good overview of the most commonly used power and precision calculations. other useful <UNK> are for ordinal <UNK> <UNK> reliability <UNK> <UNK> survival <UNK> <UNK> and for testing <UNK> <UNK> (which requires larger samples than to show a difference). conclusion if we are to have a new year’s resolution for statistics in pathology let it be that we will use unbiased sample selection methods which are fully reported, perform power calculations, and present results with confidence intervals. in this way we can ensure that we are selecting and interpreting our data without fear or favour of being misled by biased samples or mystical p values. conclusions based solely on the latter should be d valued forthwith.

<|EndOfText|>

research is only worth doing if it provides useful information. medical research usually consists of studying groups of individuals with the aim of answering a predefined research question. most commonly in the field of sexually transmitted infections (sti), the prevalence of a virus or abnormality is to be estimated or prevalences compared, either over time or between different groups of people. alternatively, several therapies may be compared within a randomised controlled trial. one question that arises at the start of any study is “how many individuals should be included in this study?” there are several ways of answering this question. the number included may be based on practical issues—for instance, the length of time available to the researcher together with the time taken to recruit, treat, and test each individual and the expected patient accrual rate. these factors will vary from researcher to researcher and between different sources of patients—for example, accrual rates will differ between different hospitals. to use such variable quantities to determine the number needed to effectively answer a given research question is clearly flawed. ethically it is wrong to either underrecruit or overrecruit. on the one hand we may be left with insufficient numbers to conclusively answer the question. on the other hand, if we overrecruit, then the best scenario is not only do we waste time, but also we subject more individuals than necessary to any inconvenience associated with being studied. in the worst scenario, we may be allowing individuals to receive inferior treatment after sufficient numbers have been recruited to ensure that the best treatment is known. many researchers associate sample size calculation purely with randomised controlled trials. most of the studies presented in this journal do not fall into this category. however sample size estimation before study commencement is important for all types of study, including prevalence studies and observational comparisons. this article highlights the need for consideration of study size over and above issues of feasibility and practicality. information is presented on how to determine an appropriate sample size for the most commonly used study designs within the field of sti. why size matters a prevalence study finds that <UNK> of women in the outer hebrides have <UNK> antibodies. we cannot interpret this information without knowing the numbers this figure was based on. for example, one out of four <UNK> is a much less precise estimate than <UNK> out of <UNK> <UNK> a proper presentation of the results would include a confidence interval. the <UNK> confidence interval for the first scenario is <UNK> <UNK> and for the second <UNK> <UNK> for each of the examples, these are the ranges within which we are <UNK> confident the population prevalence lies. as we would expect, with a much larger sample size, in the latter scenario we can make a more precise statement about the likely population prevalence. <UNK> of pregnant outer hebrideans have <UNK> antibodies compared with <UNK> of inner hebrideans. if the above statement were true and we randomly sampled and tested <UNK> individuals in each group, then we would expect to see about six antibody positive individuals in each group. however, we would not be unduly surprised to find four individuals with antibodies in one of the groups and eight in the other. however, we can quantify how likely, in the absence of a difference, we are to falsely conclude from our study that there is one. the statistical significance of a study is the probability that we will falsely identify a difference when none exists. the larger the study, the less likely this is to happen. <UNK> of pregnant outer hebrideans have <UNK> antibodies compared with <UNK> of mainland scots. if the above statement were true and we randomly sampled and tested <UNK> individuals in each group, then we would expect to see about six antibody positive individuals in the first group and three in the latter. however, we would not be unduly surprised to find four individuals with antibodies in each of the groups. key messages + sample size is an important issue for all contributors of studies to this journal. + the interpretation of study results depends on the sample size included that study. + sample size formulas are given for the most common scenarios encountered in the field of sti. question: how do we know that our study will not indicate there are differences when none exist? answer:we don’t. however, we can quantify how likely we are to find a difference of a given size if it exists. the power of a study, usually represented as a percentage, is the ability of a study of a given size to detect a difference of a given magnitude. the larger the difference the smaller the number needed. simple sums for sample sizing choosing the best sample size is not a precise art. equations exist for calculating the sample sizes needed to obtain a specified precision or to identify differences of a given size. the latter of these are called power calculations. sample size calculation relies on “guesstimates” of unknown quantities and hence obtained sizes are by definition unlikely to be correct. the extent to which sample size determination is influenced by erroneous estimates can be investigated by trying out different guesstimates in the formulas. below are details of sample size calculation for the most common scenarios in sti studies. (i) the approximate number of individuals required to estimate a prevalence within ± e% is given by the formula: the number required depends on the prevalence the study is designed to estimate! note that if the prevalence is x% then the sample size required is the same as if estimating a prevalence of <UNK> − x)%. for example, if the true prevalence is <UNK> and we want to estimate this prevalence to within <UNK> then individuals are required. of course we do not know this prevalence before we undertake the study. our guesstimate of <UNK> may be inaccurate. if the true prevalence is actually <UNK> then individuals would be required to give the same level of precision. if only <UNK> individuals are included (based on the guesstimate of <UNK> then the prevalence will be estimated less precisely than anticipated. figure <UNK> shows the numbers required to detect various prevalences to within <UNK> for example, to estimate a prevalence of <UNK> with this precision will require approximately <UNK> individuals. (note that the same number would be required to estimate <UNK> —that is, <UNK> − <UNK> with the same precision.) if a prevalence needs to be estimated with greater precision then the sample size must be increased, for less precision smaller sample sizes are required. to estimate prevalences more or less precisely, the estimates given for <UNK> can be divided by the required precision squared. for example: (a) to estimate to within <UNK> the numbers given for <UNK> need to be divided by <UNK> <UNK> this curve is shown on the figure. to estimate <UNK> to within <UNK> requires approximately <UNK> or <UNK> individuals. (b) to estimate to within <UNK> the numbers given for <UNK> need to be divided by <UNK> (= <UNK> part of this curve is shown on the figure. to estimate <UNK> to within <UNK> requires approximately <UNK> or <UNK> <UNK> individuals. (ii) if the prevalences within different groups are <UNK> and <UNK> the approximate numbers of individuals required to detect this difference with <UNK> power at the <UNK> significance level are: table <UNK> shows n for selected <UNK> and <UNK> for greater power the sample sizes need to be increased. similarly, larger samples are required to detect smaller differences between prevalences. for <UNK> power, the samples need to be increased by about one third. for example, if <UNK> of hebrideans and <UNK> of mainland scots have <UNK> infection, then <UNK> hebridean and <UNK> mainland scots need question: how do we know that our study will not fail to identify a difference of clinical importance that truly exists? answer:we don’t. figure <UNK> number need to estimate a single prevalence to within plus or minus <UNK> <UNK> and <UNK> with <UNK> confidence. <UNK> <UNK> <UNK> <UNK> estimate to within: ± <UNK> ± <UNK> ± <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> percentage prevalence number needed <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> table <UNK> number required in each group to detect differences with <UNK> power at the <UNK> significance level given that group prevalences are <UNK> and <UNK> prevalence <UNK> prevalence <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> to be tested to detect this difference with <UNK> power at the <UNK> significance level. for <UNK> power a total of <UNK> (= <UNK> ´ <UNK> will need to be tested in each group. as before, the sample size estimate is based on guesstimates of the study outcome. for the purposes of sample size determination, the magnitude of the difference in prevalences between groups may be chosen on the basis of what is a clinically important difference that we want to detect. for example, the best available evidence may suggest that the hebrideans only have <UNK> fewer <UNK> infections but we decide to undertake a study to detect a difference of <UNK> since this could represent a clinically important difference (in terms of allocating resources, etc), whereas <UNK> would not. often there is more than one factor to consider. for example, outer hebrideans may be older than the average scottish person and it would be of interest to know whether the difference in prevalence of <UNK> can be explained by the difference in ages of the groups. sample size estimation in these scenarios is more complex and will depend on the extent to which the factors are <UNK> more of one than the other when comparisons are made between two or more groups, power will be maximised (for a given overall total number enrolled) if each group is of the same size. to accommodate an imbalance in numbers while retaining the same power, the total sample size needs to be increased <UNK> ones that got away calculations give the estimated numbers required for statistical analysis. sometimes it will be necessary to recruit many more to ensure that sufficient are obtained for that analysis. on the basis of the expected refusal and compliance rates, the study should be designed so that sufficient are contacted/approached/recruited to account for these losses. ones that never happen in some studies, individuals are recruited and monitored for variable lengths of time with the aim of comparing times to some defined event, such as death or infection, between subgroups of individuals. the nature of the outcome is that not all individuals will have the event and a straightforward comparison of percentages dying or becoming infected is invalidated because of the variable follow up times. similarly, comparing average times to death/ infection would also be invalid. a special type of analysis is <UNK> and the sample size will be based on the number of individuals for whom the event <UNK> so for a rare event larger numbers will be required. extending the length of follow up may be used to increase the numbers of events occurring. ones that flock together if treatments are allocated on a group basis, the effective sample size will not equal the total number of individuals. adjustment must be made according to the extent to which individual outcomes are influenced by the fact that they form part of a homogeneous <UNK> <UNK> for example, the introduction of trained specialists within randomly selected <UNK> to identify whether these specialists have an effect on adverse outcomes. individuals from the same clinic may be more alike, perhaps because of social and ethnic similarities, than those from different clinics in terms of their tendency to be recorded as having a problem even before any intervention. a study that randomises by clinic will effectively be putting groups of similar individuals into the same arm of the trial en bloc. a similar situation occurs when individuals within the same family, ward/hospital, etc, are jointly allocated to treatments. summary in conclusion, this short article has addressed the issues surrounding the determination of appropriate study size. only the sample size equations relating to the most common study types in the field of sti are presented. for more complex but common situations, the important issues to consider are highlighted. many other sample size equations exist—for example, when the outcome is continuous, such as <UNK> count, or more than two groups are compared. several useful references are given in the further reading section. this article raises awareness of the need to perform studies of the correct size for the given purpose and informs the researcher in sexually transmitted infections of the particular points that they may need to consider.

<|EndOfText|>

derivation versus validation assessing the probable clinical course of children presenting to their care is one of the day to day dilemmas facing the practising paediatrician. with accurate prognostication, invasive or expensive treatment may be targeted towards those most likely to benefit. there is little point in expending vital resources on, or administering invasive treatments to, children who are likely to recover without such intervention. children with certain characteristics may tend to do better or worse than others. for example, younger children or those with specific clinical signs might be expected to deteriorate more rapidly. past experience often acts as a guide for the experienced paediatrician. algorithms (or prognostic models) can be developed to provide a means of transferring expert knowledge to the novice. for example, apgar scores are routinely used to assess the health of newborn babies, and apache scores can be used as a measure of prognostication among admissions to paediatric intensive <UNK> in this issue, brogan and <UNK> present an algorithm for identifying children presenting to a&e with fever and petechiae who are at increased risk of significant bacterial sepsis. it is interesting to note that of the many prognostic models that are published each <UNK> <UNK> relatively few are sufficiently validated and fewer still find their way into clinical <UNK> determining which patients will benefit from treatment is only one use of prognostic algorithms. they also provide a means of informing parents of the likely outcome and can be used in research to give baseline measures of severity in different groups. derivation of prognostic models an experienced clinician accumulates knowledge via the patients that he or she has cared for from initial presentation through to final outcome. what has happened to these cases will inform the future decisions that the clinician makes. the process can be formalised by documenting information on newly referred patients who are then tracked until their outcome is known. when a suitably sized sample has been collected, statistical techniques may be used to build an algorithm designed to predict poor outcome in future patients. the most common way of deriving a prognostic model from existing data is via regression analyses. the development of prognostic models is extensively covered in many medical statistics texts. from the competing models one may be chosen that is both practical and statistically acceptable. in particular, models may be preferred on the grounds that they require only routinely collected and reliable data and do not have substantially less predictive ability than alternative models that require invasive or non-routine data. decisions may need to be made quickly and algorithms should be simple and user friendly. one problem with the development of algorithms for prognostication is that derivation will be driven by the available dataset. quirks individual to that dataset may appear to be prognostic and be encapsulated in the predictive algorithm. for example, time between symptoms and presentation may be unrelated to outcome but by chance those patients with the poorest outcome in the available dataset tended to present soon after the first appearance of symptoms. a prognostic model designed to be applied at presentation which incorporates time from symptoms as a factor would under diagnose those presenting late. unusual features or extreme but random differences between prognostic groups within the development dataset may not be replicated elsewhere, leading to the creation of a non-transportable model. where there are many variables competing for inclusion this problem may be extreme. hence, analyses that are not pre-specified but are data dependent are liable to give a better fit than is obtained when the model is applied elsewhere. the problem is further compounded when cut off points for continuous variables are selected to give the best prognostication based on the development <UNK> for example, respiratory rate was dichotomised at <UNK> <UNK> <UNK> <UNK> and <UNK> to identify the cut off point giving the best sensitivity and specificity for predicting hypoxia in acutely ill <UNK> although arbitrary thresholds for continuous variables are not generally <UNK> they are often preferred because of the simplicity that they confer on the final algorithm. the converse problem is that of under fitting. important prognostic variables may not be identified in the derivation dataset and there are several reasons this could occur. random or chance variation may mean that the values of a truly prognostic variable are not significantly different between prognostic groups within the available dataset. for example, suppose time from first symptoms to presentation is predictive of outcome but that, in the development dataset, it just happens that the patients with the worse outcome were unusual in that they tended to present early. alternatively prognostic factors may not be identified because the patient set used for derivation is limited in some way. for example, age may be highly prognostic but does not enter into the algorithm because the model was derived from data collected only from children in a very narrow age range. methods of model checking are available with most statistical computer packages and tend to be well covered in most regression texts. however, these methods merely detect whether the chosen model adequately describes the trends in the dataset on which it has been developed. they cannot inform on whether the model is suitable for use in clinical practice or an accurate description of the population trends. validation the actual evidence that the application of a prognostic model alters medical practice and improves the outcome of patients has to be established additionally which is in accordance with phase iv studies of diagnostic <UNK> model validation is the process whereby the derived (or fitted) model is shown to be suitable for the purpose for which it was developed. it addresses the question of whether the model has wider applicability. as the aim of most published papers is to present results that will be generally useful, these are questions of major importance that cannot be overlooked. surprisingly few medical statis- tics textbooks discuss techniques for model validation and most do not even mention its importance or relevance to model interpretation. in addition to being user friendly and statistically sound, a prognostic algorithm needs to be generalisable to be clinically useful. a model based on variables which have low reliability will not tend to be valid in a sample other than that for which it was developed. one of the reasons published algorithms may not find their way into standard practice is the lack of evidence that they are applicable to patients from establishments other than where they were developed. generalisability needs to be established by testing the prognostic algorithm in numerous and diverse settings. <UNK> in the words of a recent <UNK> on the subject of model validation, “usefulness is determined by how well a model works in practice, not by how many zeros there are in the associated p-values.” techniques for model validation it is well recognised that deriving and validating a model on the same dataset will by definition lead to over optimistic estimates of the model’s accuracy. an alternative approach is to split the dataset into two parts, one part for derivation and the other for validation. a major drawback is that the precision of the fitted parameters will clearly be reduced as only a portion of the dataset is used for model <UNK> a variety of methods have been advocated for dividing the dataset. automated procedures exist for choosing two halves that are homogeneous but these will also clearly give over optimistic estimates of model validity. if some measure of internal validity is required then it is recommended that the data are split in a non-random way. data from different time periods could be <UNK> and this is the same as validation using a more recent cohort or prospective validation using an algorithm derived from a retrospective dataset. alternatively, one of the less arbitrary “leave one out” approaches may be <UNK> <UNK> external validity the model should be externally validated by assessing its applicability to data collected at another centre or by different individuals. altman and <UNK> present a series of examples of models that have been derived, internally validated, and then validated elsewhere. they note that authors tend to confirm the validity of their own models but that others are less successful at doing so. this finding could be the result of a form of publication bias; if the authors’ internal validation was weak it is doubtful that they would attempt to publish the results. alternatively there may be real differences between centres and the model, while being internally valid, is not transportable and hence of limited use. statistical versus clinical validity in general authors show no appreciation of a distinction between statistically and clinically valid <UNK> a model that is statistically valid will yield unbiased predictions when applied to new datasets. this quality however does not necessarily mean that the model has clinical validity. to be clinically valid the model must be accurate enough to serve the purpose for which it was developed. for example, abnormal white cell might be significantly associated with increased risk of significant bacterial sepsis among children presenting to a&e with rash. this finding, even if replicated across hospitals and hence statistically valid, may be of little clinical relevance if only a few extra children with poor outcome are identified as a result. there are different clinical implications if application of the algorithm means that an additional child in <UNK> or an additional child in four with poor outcome is identified early. consideration must of course also be given to whether there are other children who achieve a worse outcome, perhaps because treatment is withheld that would normally have been administered, as a result of applying the algorithm. similarly a statistically invalid model is not necessarily clinically invalid. the algorithm may not be consistent in the extent to which it identifies children with poor outcome but it may always identify enough to warrant its implementation on a regular basis. scores need to be reliable enough for the purpose they were developed for, even if this reliability is relatively <UNK> there will be a trade off between the additional workload entailed in applying the model and the likely benefit derived by the patients. conclusion simple diagnostic models may be more portable than more complex <UNK> when kennedy et al made the above observation they were commenting on the statistical aspects of deriving a model which is then used elsewhere, but it is of course true in more ways than one. models based only on factors which are highly related to outcome will be more likely to be similarly predictive in another setting. complex models incorporating multiple factors, for some of which any predictive value may be highly specific to that dataset, are less likely to be similarly predictive elsewhere. from a practical viewpoint simple models are more likely to be readily incorporated into clinical practice with minimal disruption. model derivation and validation are two separate and important parts of the same process, the identification of clinically useful models. it is to be remembered that the final test of a model should always be whether it is accurate and generalisable enough for the purpose for which it was derived.

<|EndOfText|>

the journal of clinical pathology publishes about nine papers a month presenting comparative (mostly primary) data. their aim is to inform the clinical readership about the effects of disease or the comparative effectiveness of differing treatments. during the last <UNK> years many internationally recognised reporting guidelines within health research have been developed. the most notable of these are consort (consolidated standards of reporting <UNK> strobe (strengthening the reporting of observational studies in <UNK> and stard (standards for the reporting of diagnostic accuracy <UNK> however, these guidelines have been used infrequently. last year the umbrella network equator (enhancing the quality and transparency of health <UNK> was officially launched with the aim of enhancing the reliability of medical research literature by promoting transparent and accurate reporting of research results. one way the network aimed to do this was by increasing the usage of robust reporting guidelines such as consort, strobe and stard. the j clin pathol supports this initiative, and the equator network is cited within the author instructions. all the guidelines require that estimates should be given with some measure of precision that is dependent on the sample size. this precision usually takes the form of a confidence interval around the point estimate of effect. a review of the <UNK> months april to june <UNK> revealed that of <UNK> j clin pathol articles presenting sample data, only six mentioned confidence intervals, and only four presented them correctly for all relevant estimates. to facilitate adherence to the current guidelines for contributors to j clin pathol, this paper gives an overview of confidence intervals and their interpretation in the most commonly encountered data scenarios. we do not give details of calculation as this can be done by any good statistics package, and the researcher does not need to get involved with the intricacies of the process. the emphasis is on understanding the rationale and application. an illustrative example suppose we want to know what percentage of malignant mesotheliomas (mms) express <UNK> we will look for gata- <UNK> expression in a random sample of mms. the percentage that expresses <UNK> in that sample will give an estimate of the population percentage. the sample estimate will not usually be the same as the population value. how good an estimate the sample yields depends on the sample size. larger samples give more precise estimates. it makes sense to take into account the sample size when interpreting the sample results. what is a confidence interval? a confidence interval gives the range of population scenarios that the sample is compatible with. it is a measure of precision attached to, and built around, a study estimate. suppose in a random sample of <UNK> mms we find that eight express <UNK> our best estimate of the percentage of all mms that express <UNK> is <UNK> but this does not mean that the population percentage is <UNK> we could quite reasonably expect to obtain <UNK> if the population percentage were <UNK> or <UNK> (when we would expect to obtain about seven or nine of our sample expressing <UNK> however, if the population percentage were actually <UNK> for instance, then we would expect to see about two expressing <UNK> and would be surprised to find as many as eight. the confidence interval is built around the sample estimate <UNK> and gives the range of population values that can reasonably be expected to yield a sample estimate of <UNK> from a sample of that size. how ‘‘reasonable’’ it is for the interval to contain the population value is quantified by the percentage confidence interval that we choose to give: c the <UNK> confidence interval gives the range within which we are <UNK> confident the population value lies (based on our sample) c the <UNK> confidence interval is narrower and gives the range within which we are <UNK> confident the population value lies c the <UNK> confidence interval is even narrower and gives the range within which we are <UNK> confident the population value lies. the interval is narrower as we are less confident it actually contains the population value. it is not impossible that the population value lies outside the confidence interval; we just know that it is unlikely with a given level of confidence. a <UNK> confidence interval is estimated from our sample. we may be unlucky and obtain one of the <UNK> of random samples that yield a confidence interval that does not contain the population value. if we calculate the <UNK> confidence interval around our sample estimate of <UNK> it is found to be <UNK> to <UNK> this means that we are <UNK> confident that the population percentage of mms that express <UNK> is between <UNK> and <UNK> the <UNK> confidence interval for the sample estimate <UNK> is <UNK> to <UNK> which, as expected, is narrower, since we are less confident <UNK> as opposed to <UNK> that it contains the population value. it might seem odd that on the basis of <UNK> mms we obtain limits that are not values that could be obtained from a sample of <UNK> (ie, with <UNK> mms we could not obtain a sample estimate of <UNK> as we could have either <UNK> <UNK> or <UNK> <UNK> but not <UNK> of the <UNK> expressing <UNK> this does make sense though because the population value might actually be <UNK> even though we would never obtain this sample estimate from <UNK> mms. as stated above, the sample estimate is unlikely to be exactly the same as the population value, particularly where small numbers are sampled. larger samples if we take a larger sample, the specified percentage confidence interval will be narrower as we will have a more precise estimate. for example, if <UNK> of <UNK> randomly sampled mms express <UNK> our sample estimate is still <UNK> <UNK> but the <UNK> confidence interval is now <UNK> to <UNK> which is more precise than the <UNK> to <UNK> obtained with the smaller sample. the <UNK> confidence interval for the <UNK> <UNK> is <UNK> to <UNK> hence, as we would expect, the larger sample gives a more precise estimate. terminology the edges of the confidence interval are known as the confidence limits. for example, the <UNK> confidence limits for the example above are <UNK> and <UNK> the <UNK> confidence limits are <UNK> and <UNK> sometimes ‘‘confidence interval’’ is abbreviated as ‘‘ci’’ and ‘‘confidence limits’’ as ‘‘cl’’. the limits may be separated by the word ‘‘to’’ as we have done <UNK> to <UNK> or by a comma <UNK> <UNK> although a dash is sometimes used <UNK> this is not recommended as it can cause confusion with minus signs. the width of a confidence interval is the difference between the confidence limits (ie, how far the interval spans). for example, the <UNK> confidence interval in the example based on a sample of <UNK> is <UNK> to <UNK> and hence of width <UNK> <UNK> while based on a sample of <UNK> the width is only <UNK> <UNK> note that it is most common to cite <UNK> confidence intervals and if no percentage is given then <UNK> confidence should be assumed. confidence intervals for other population parameters the example just given considered a single population percentage (the percentage of mms expressing <UNK> and illustrates the simplest case. confidence intervals can, and should, be built around any sample estimate of a population value. for example, the mean age of those expressing <UNK> or the difference in average ages of those presenting with two different diagnoses. how wide a particular <UNK> confidence interval is always depends on the sample estimate and the sample size. for numeric values, such as mean age or <UNK> value, the width of the confidence interval will also depend on the variability of the sample measurements. calculation of confidence intervals for the majority of sample estimates a measure of precision known as the standard error can be calculated. there are two main exceptions to this. first is where no distribution can be assumed and non-parametric estimates such as the median are used. second, if sample numbers are small and the standard error cannot accurately be established, exact methods of confidence interval estimation need to be used. if a standard error can be calculated then this is used to construct a confidence interval for the estimate. the standard error is positive and larger values mean that the estimate is less precise. smaller samples yield larger standard errors and wider confidence intervals. a <UNK> confidence interval is given by (sample <UNK> standard errors) = ((sample <UNK> standard errors) to (sample <UNK> standard errors)). an <UNK> confidence interval is given by (sample <UNK> standard errors) = ((sample <UNK> standard errors) to (sample <UNK> standard errors)). for example, the standard error of the percentage expressing <UNK> based on a sample of <UNK> mms can be calculated to be <UNK> the <UNK> confidence interval for the <UNK> sample estimate is calculated as: <UNK> = <UNK> = <UNK> to <UNK> which can be rounded to <UNK> to <UNK> the <UNK> confidence interval is calculated as: <UNK> = <UNK> = <UNK> to <UNK> which can be rounded to <UNK> to <UNK> clinical interpretation of confidence intervals it is important to give confidence intervals around all sample estimates as they allow clinical interpretation of the study results that take into account the sample size. we should believe in the estimate based on <UNK> mms more than that based on only <UNK> mms. the <UNK> confidence interval for a population estimate gives the range of population scenarios with which the sample values are compatible (with <UNK> confidence). we can reasonably exclude values outside this interval as being unlikely, whereas we should consider the possibility that anything within the interval could reasonably be true. recent published examples florena et <UNK> this study compared megakaryocytes (mkcs) between <UNK> patients with essential thrombocythaemia (et) and <UNK> patients with primary myelofibrosis (pmf). the average difference of <UNK> (pmf <UNK> et <UNK> was not significant <UNK> a <UNK> confidence interval for the difference <UNK> to <UNK> shows the range of average differences with which these two samples are compatible. we cannot exclude an increased average of <UNK> in the pmf group. in order to fully interpret the results we need to consider not only the p value but also the confidence limits, in particular whether <UNK> is a clinically important difference that we would want to investigate further. the sample data are compatible with mkc values for patients with et being on average <UNK> higher and <UNK> lower than for patients with pmf. by contrast, florena et <UNK> also recorded the percentages of mkcs positive for bcl-xi for each of the <UNK> patients and found a significant difference of <UNK> (pmf <UNK> et <UNK> <UNK> with a <UNK> confidence interval of <UNK> to <UNK> this interval shows that the data are compatible with an average difference between pmf and et as small as <UNK> or as large as <UNK> to interpret the data we need to consider the clinical relevance of those limits. interpretation cannot be made merely using the p value. al-mulla et <UNK> this study showed that the median age of onset of breast cancer was <UNK> years for individuals with mutation <UNK> in exon <UNK> <UNK> confidence interval <UNK> to <UNK> it is important to consider the confidence interval as this takes into account the sample size that the median age <UNK> years) is based on. if a much smaller sample were used which yielded the same average but a wider confidence interval of, for example <UNK> to <UNK> years), we would need to interpret the information differently. in the latter scenario we cannot draw any conclusions as the range given <UNK> to <UNK> years) is so wide as to be uninformative. other applications a review of j clin pathol articles in the <UNK> months april to june <UNK> shows that a variety of summary statistics are used regularly in j clin pathol. in all instances confidence intervals are appropriate and should be presented alongside the summary estimates. the main forms are as follows. single percentage prevalence studies yield sample estimates of population percentages. other instances where single percentages are obtained are reliability studies (k) and diagnostic studies (sensitivity, specificity, positive and negative predictive values). note that for diagnostic studies the sample size for the different estimates varies. for example, sensitivity is based on those with the disorder and specificity on those without, the size of these two groups possibly being quite different. single mean or median we should be mindful of whether the measurements are normally distributed (hence the mean of the values will be a valid summary) or not (when the median is a better summary). either way a confidence interval should be given alongside the mean or median. differences in percentages between two groups the analysis here is typically <UNK> or fisher’s exact test. a confidence interval should be given for the percentage difference between the two groups. difference in means or medians between two groups the analysis here is typically two sample t test (means of normally distributed data) or mann–whitney u test (medians of non-normally distributed data). some skew data are actually log-normally distributed and means can be calculated on the transformed scale. a confidence interval should be given for the difference in mean or median between the two groups. for differences in percentages between two groups and difference in means or medians between two groups, note that a common mistake is to calculate confidence intervals for each group separately but this is not what is required. the confidence interval to present is for the difference between groups. correlations the relationship between two variables using a correlation coefficient also requires a confidence interval since it is a sample estimate of the population value. hazard ratios in j clin pathol it is not uncommon to see time to event (survival) data presented. the hazards of the event are compared between groups and it is important that their ratio is given with a measure of precision that is dependent on sample size. overview this article has highlighted an area where vast improvements can be made in the presentation and interpretation of study results in the j clin pathol. we expect authors to follow the guidelines of the equator network, and having a good understanding of confidence intervals is vital to this. our hope is that this article will help researchers to make the best use of the data they have collected.

<|EndOfText|>

abstract background: minimisation can be used within treatment trials to ensure that prognostic factors are evenly distributed between treatment groups. the technique is relatively straightforward to apply but does require running tallies of patient recruitments to be made and some simple calculations to be performed prior to each allocation. as computing facilities have become more widely available, minimisation has become a more feasible option for many. although the technique has increased in popularity, the mode of application is often poorly reported and the choice of input parameters not justified in any logical way. methods: we developed an automated package for patient allocation which incorporated a simulation arm. we here demonstrate how simulation of data can help to determine the input parameters to be used in a subsequent application of minimisation. results: several scenarios were simulated. within the selected scenarios, increasing the number of factors did not substantially adversely affect the extent to which the treatment groups were balanced with respect to the prognostic factors. weighting of the factors tended to improve the balance when factors had many categories with only a slight negative effect on the factors with fewer categories. when interactions between factors were included as minimisation factors, there was no major reduction in the balance overall. conclusion: with the advent of widely available computing facilities, researchers can be better equipped to implement minimisation as a means of patient allocation. simulations prior to study commencement can assist in the choice of minimisation parameters and can be used to justify those selections. background most medical researchers are aware that it is necessary to perform a randomised controlled trial to effectively establish the usefulness of a new treatment. the aim is that treatments are compared on similar groups of patients. completely random allocation of patients to treatments does not, however, ensure that the patient groups are similar with respect to prognostic factors. for example, purely by chance one of the treatment groups may have been allocated older or more severely ill patients. if such an imbalance in prognostic factors has occurred then it may be difficult to attribute any differences to treatment, the analyses will require adjustment and the study will have less power. minimisation <UNK> is a dynamic allocation procedure that ensures treatment groups are similar with respect to a series of pre-specified prognostic factors. as patients are recruited to the trial they are allocated to the treatment group that will 'minimise' the differences in the distribution of those factors between the groups. one pitfall of this process is that the allocation is not random and hence could be predicted. this problem is addressed by randomising each patient but weighting the randomisation towards the minimisation favoured treatment group for that individual. by introducing weighted randomisation, the individual is more likely to be allocated to receive the preferred treatment, but is not guaranteed to do so. there is a trade-off between the size of the weighting used and the ability of the researcher to predict the next allocation. to apply minimisation requires only simple algebra but this may be problematic for the clinician with limited time and resources. the advent of greater access to computing technology has led to an increase in the usage of minimisation, which has previously been implemented relatively rarely. published trials increasingly cite the use of minimisation for patient allocation. for example, falk et al <UNK> minimized patients to receive immediate or delayed radiotherapy with groups balanced according to clinician, histology, presence of metastases and who performance status. pal et al <UNK> used minimisation to ensure even distribution within age groups <UNK> or <UNK> years) and whether or not there was cerebral impairment when conducting a trial of phenobarbital versus phenytoin for seizure control amongst epileptic children in rural india. minimisation can similarly be used for allocation within cluster randomised controlled trials when confounding factors are applicable at the cluster level. for example, hilton et al <UNK> performed a cluster randomised trial of intervention to lessen individuals' cardiovascular risk with general practices allocated to intervention or control using minimisation for jarman score, ratio of patient to practice nurse hours per week and fundholding status. the benefits of minimisation have been debated recently <UNK> and a recent review of the usage of minimisation recommended 'its wider adoption in the conduct of randomized controlled trials' <UNK> the technique has not been without its critics <UNK> as well as advocates, but it is acknowledged to be relatively simple to implement and perform comparably to more complex models where prognostic factors are non-numeric in basis <UNK> a common criticism is that minimisation concentrates only on marginal distributions of prognostic factors and may not ensure that the interactions are similar between groups. for example, although there may be similar numbers of males, females, disease state positive and negative in the treatment groups post allocation, all of the positive males may receive one treatment and all of the positive females the other. if there is an interaction between disease state and gender on outcome, then this difference between the treatment arms may be problematic. however, the likelihood of imbalance is easily countered via a <UNK> minimisation variable: male/ positive, male/negative, female/positive, female/negative. the usefulness of minimisation as an allocation procedure within randomised controlled trials is therefore established and will continue to be recognised and utilised. however, it has been commonplace for published studies that have used minimisation to give little or no information as to how the process has been implemented. often they cite the minimisation variables but do not state what metric has been used to determine the preferred allocation group, whether and what randomisation weightings have been used for the allocations, whether and how the factors have been weighted, whether interactions have been accounted for or, where applicable, how cut-points for continuous prognostic factors were selected. the researcher who wishes to embark on a trial using minimisation as the means of patient allocation often has many questions to ask. several parameters need to be selected for each trial to which minimisation is to be applied. these parameters and, from our experience within the statistical consultancy, the most common associated questions related to the choice of each, are given below: <UNK> number of factors • how many factors can be balanced simultaneously? • how does the balance for an individual factor change as more factors are incorporated? • in particular, what is the effect on other factors of adding a single, many-categoried factor such as centre? <UNK> number of categories for each factor • how does choice of number of categories (for continuous prognostic factors) affect the allocation process? <UNK> whether and how to weight the factors • how should the factors be weighted? <UNK> the randomisation weighting to use • how much should the randomisation be weighted in favour of the preferred group? they also want to know • with a chosen randomisation weighting and given number of minimisation factors, how big a discrepancy can be expected for a specified sample size? and • how does inclusion of interactions between prognostic factors change all this? there is minimal information available in the published literature to inform researchers when addressing the above questions. we have developed a simple computer package to perform minimisation allocations subject to selected values of these <UNK> input parameters. we have also incorporated a simulation element that allows the researcher to investigate the size of discrepancies in allocation of prognostic factors between treatment groups subject to variation in the input parameters. we here present the results of some simulations for a hypothetical proposed trial and show how this process can assist the researcher in deciding on the parameter values to be used in their trial. subject to our chosen criteria for model comparison, we examine the extent to which varying the minimisation parameters may influence the equalisation of prognostic factors between treatment groups. methods for each new patient to be allocated, the process of minimisation considers the imbalance in selected prognostic factors and weights the randomisation of the next patient, according to his or her characteristics, in favour of the treatment that will make the treatment groups most similar with respect to the prognostic factors. to formalize this process, assume a trial where patients are allocated to one of two treatments, <UNK> and <UNK> suppose there are m prognostic factors and cj is the number of categories for the jth prognostic factor (j = <UNK> let anj be the value that the nth patient takes for the jth prognostic factor (anj∈ <UNK> and let dj be a measure of the difference between the numbers of patients allocated to each of the two treatments who are in category anj after allocation of the <UNK> patient. assume dj positive if more patients in category anj are currently receiving <UNK> negative if more are receiving <UNK> and zero if both treatments currently contain equal numbers of patients at this level for the jth confounder. therefore <UNK> = <UNK> <UNK> ..., dm} is the vector of differences between the treatment groups with respect to the m prognostic factors at the levels seen in the nth patient prior to allocation of that patient. using minimisation, according to the size and direction of <UNK> randomisation of the nth patient will be weighted towards the treatment group that will make dn numerically smaller according to some chosen metric ie. the differences between the treatment groups will be minimised with respect to the prognostic factors. for example, consider a study where there are m = <UNK> minimisation criteria: gender, age (under/over <UNK> residency status of patient (in/out) and severity of disease (mild/ moderate/severe). suppose that <UNK> patients have been recruited and allocated to one of <UNK> treatment arms <UNK> per arm) and that they are distributed amongst the minimisation criteria categories as follows: suppose that the <UNK> (n = <UNK> patient to be allocated (n = <UNK> is an adult male in-patient with mild disease. prior to his allocation the numbers of patients falling into these categories is <UNK> = <UNK> in treatment arm <UNK> and <UNK> = <UNK> in treatment arm <UNK> hence <UNK> = <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> <UNK> <UNK> <UNK> is the vector of differences between the treatment groups with respect to the <UNK> prognostic factors at the levels seen in the <UNK> patient prior to allocation of that patient. since there are fewer similar patients in <UNK> randomisation of the <UNK> patient should be weighted towards this group. choice of metric to minimise dn the simplest algorithm for allocation of the nth patient is to weight the randomisation in favour of <UNK> if , in favour of <UNK> if and use simple randomisation (p = <UNK> where p is the probability with which the patient is allocated to the preferred treatment) if . this dj j m > = σ <UNK> <UNK> dj j m > = σ <UNK> <UNK> dj j m > = σ <UNK> <UNK> table <UNK> treatment arm: <UNK> <UNK> gender: male <UNK> <UNK> female <UNK> <UNK> age: under <UNK> <UNK> <UNK> over <UNK> <UNK> <UNK> residency status: in patient <UNK> <UNK> out patient <UNK> <UNK> severity of disease: mild <UNK> <UNK> moderate <UNK> <UNK> severe <UNK> <UNK> prognostic factors. the prognostic factors can be given different weightings (wj) according to their relative importance and used to determine the allocation. there are a variety of ways that the wj may be chosen. one potential system that seems reasonable is to weight according to the number of categories. for example, if <UNK> patients are allocated to two treatment groups then we would expect <UNK> within each category of a binary variable for each treatment group, and <UNK> within each category of a <UNK> variable for each treatment (assuming, without loss of generality, equal probability of the categories occurring within each variable). a maximum difference of the same absolute magnitude between the two treatment allocations for any of the variable categories would probably be more clinically relevant for the <UNK> than for the binary variable: the absolute (unweighted) differences are identical (= <UNK> but for the binary variable <UNK> contains <UNK> more patients within the first category, for the <UNK> variable there are <UNK> more relative to <UNK> the deviations from expected are <UNK> <UNK> and <UNK> <UNK> respectively. weighting the absolute differences (= <UNK> by the number of categories gives weighted differences of <UNK> <UNK> × <UNK> and <UNK> (= <UNK> × <UNK> for the <UNK> and <UNK> category variables respectively. the extent to which randomisation is more likely to favour one treatment over the other depends on the size of the randomisation weighting used. with p = <UNK> (simple randomisation) there is no preference for either group. when p = <UNK> the patient automatically receives the preferred treatment, there is no random element and allocation is said to be deterministic i.e. the researcher could predict the group allocation of the next patient if they knew the previous allocations. selecting a value <UNK> <p < <UNK> will bias allocation in favour of the preferred treatment whilst retaining an element of randomness so that patient allocation cannot be predicted. more extreme values of p (greater bias) will lead to better balancing of prognostic factors between treatment groups. there must be a trade-off between introducing sufficient randomisation weighting (p large enough) and not allowing allocation of the next patient to be predicted (p not close to <UNK> quantification of balance the maximum absolute difference between the patients allocated to each of the categories within a given factor summarises how well that factor has been balanced by the minimisation process and is used as a summary measure of balance for that factor. in our simulations we summarise across factors with the same number of categories. for example, if we minimize according to <UNK> binary variables and one <UNK> factor then there will be <UNK> summary measures of balance achieved for each simulated dataset: the maximum absolute differences allocated to <UNK> any of the binary variables i.e. the <UNK> categories which constitute these <UNK> factors and <UNK> any category within the <UNK> category factor. as previously noted <UNK> it is the behaviour of an individual design that is of interest to the researcher rather than the average over many applications. of interest is the value below which discrepancies are likely to occur for the majority of applications of minimisation with given criteria. an individual investigator is more likely to want to know the maximum discrepancy that s/he can reasonably expect with chosen allocation parameters rather than what the average will be for everyone using those parameters. for these reasons we prefer the <UNK> centile of the simulated distributions as more realistic and relevant measures than the means or medians which have previously been used to describe the success of a selected allocation process. the error in estimating the <UNK> centile is about <UNK> <UNK> times the error when estimating the mean <UNK> but this difference becomes unimportant if a large number of simulations are taken. the <UNK> centiles of the distributions of simulated values are used to compare allocation schemes with varying input parameters (sample size, randomisation weighting, number and type of variables, weighting of variables). simulation options it was estimated that <UNK> simulations would allow quantification of the <UNK> centile of the distribution to within ± <UNK> standard deviations of that distribution with <UNK> confidence. increasing the number of simulations to <UNK> or <UNK> would increase this precision substantially to ± <UNK> and <UNK> respectively. a further doubling of the number of simulations (to <UNK> would only further increase precision by less than <UNK> standard deviations. hence it was wjdj j m = σ <UNK> table <UNK> <UNK> <UNK> absolute difference <UNK> <UNK> absolute difference <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> decided that <UNK> simulations would be adequate for a comparison of minimisation criteria. for each scenario <UNK> simulations were performed using a fortran program incorporating nag subroutines for random selection of patient characteristics. measure of balance the <UNK> centiles of the distributions of maximum absolute differences for each factor type were recorded and classified according to the number of potential categories within which each individual could fall. these differences are expressed as the proportionate difference from that expected by multiplying by the number of categories for that factor and dividing by the total sample size. randomisation weighting all simulations were repeated for p set at <UNK> (simple randomisation) and also for p taken to be <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> values which equate respectively to allocation to the preferred treatment being <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> times as likely as to the alternative. weighting of variables simulations were performed with prognostic factors both unweighted (wj = <UNK> j = <UNK> and with weights equal to the number of categories of the prognostic factor. to address the types of questions posed by potential researchers we here simulate several different scenarios: firstly, we investigate the effect on balance of increasing the number of factors. models with <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> binary prognostic factors are compared. the sample size for each simulation was set at <UNK> since all factors are binary, weighting will not change the results and these models are only presented unweighted. secondly, we chose a study with three binary, one <UNK> and one <UNK> (total <UNK> prognostic factors to simulate. this scenario was chosen as being typical of the problems we were encountering in the local consultancy and not dissimilar to published studies citing minimisation criteria which commonly have several binary criteria and one or more multiple category confounders <UNK> we investigated the extent to which balance was a function of sample size by simulating the scenario with each simulation based on samples of <UNK> and of <UNK> patients. for sample size of <UNK> we also considered the balance achieved when all <UNK> interactions were used as the minimisation criteria (total of <UNK> interactions between the <UNK> confounders). finally, we generated simulations of allocations for a sample size of <UNK> based on <UNK> minimisation factors: the original <UNK> plus an additional <UNK> category confounder. practicalities of application the practicalities of application require some degree of automation. we have developed a package for clinical usage (simin) which both utilises simulations to facilitate the process of specification selection and provides a userfriendly front-end for the subsequent allocations within trial. the fortran programs which generate the simulations are embedded in this package. the purpose of this paper is to illustrate the types of patterns that can easily be determined via simulation and to show how this might assist the researcher, leading to more informed parameter selection and enhanced reporting of the decision process. results all results were equivalent for randomisation weights of <UNK> and <UNK> hence, only the results for <UNK> are shown in the figures. increasing the number of factors figure <UNK> shows the results of the simulations as the number of binary factors is set at <UNK> <UNK> <UNK> and <UNK> the output for <UNK> <UNK> <UNK> and <UNK> binary factors are not shown on the figure. as expected, the proportionate change from expected falls as the randomisation weighting is increased. the largest fall occurs after the introduction of any randomisation weighting i.e. between values of <UNK> (simple randomisation) and <UNK> as the randomisation weighting is increased to <UNK> in favour of the preferred treatment there are further declines and less so between <UNK> and <UNK> after <UNK> the proportionate change appears to have reached an asymptote. the differences in proportionate changes with increasing number of factors are approximately constant with changing randomisation weights. weighting the variables figure <UNK> shows the <UNK> centiles of the distributions of proportionate changes for the <UNK> prognostic factor <UNK> binary, <UNK> <UNK> and <UNK> <UNK> scenario with a sample size of <UNK> the dotted lines show the results when the prognostic factors are weighted according to the number of categories. again, any degree of randomisation weighting is associated with the largest fall in change from expected. proportionate differences increase with the number of categories. weighting of the prognostic factors has the effect of increasing the discrepancies for the binary factors but reducing them for the factors with more categories. changing the sample size figure <UNK> shows the <UNK> centiles of the distributions when a sample of only <UNK> patients is allocated using <UNK> minimisation factors <UNK> binary, <UNK> <UNK> and <UNK> <UNK> the proportionate differences are much larger with the smaller sample size (c.f. figure <UNK> interactions figure <UNK> shows <UNK> centiles of the distributions of proportionate changes when minimisation is used to allocate <UNK> patients to <UNK> groups with the aim of obtaining even distribution of all <UNK> interactions of <UNK> confounders <UNK> binary, <UNK> <UNK> and <UNK> <UNK> if the distributions sof the interactions are similar, then the marginal distributions of the factors will be also. the differences are increased from the non-interaction model (figure <UNK> approximately <UNK> for the lower randomisation weights (and when simple randomisation is used) up to approximately <UNK> for large randomisation weighting. since the proportionate changes are smaller for larger randomisation weights, the absolute difference in the proportionate changes falls with increasing randomisation weights. it should be noted, however, that the interaction factors have more categories and the discrepancies are about the same in terms of the numbers of individuals allocated. the effect of weighting of the prognostic factors is similar to previous models. when factors are weighted according to the number of categories this has the effect of reducing the proportionate discrepancies for the factors with more categories <UNK> × <UNK> and <UNK> × <UNK> interactions) whilst having the opposite effect for the factors with fewer categories <UNK> × <UNK> and <UNK> × <UNK> interactions). ifnicgrueraesi <UNK> the number of binary factors increasing the number of binary factors. <UNK> centiles of the distributions of proportionate changes from expected for randomisation weights from <UNK> to <UNK> obtained from <UNK> simulations and a simulated sample size of <UNK> the number of minimisation variables is increased from <UNK> (black line) to <UNK> (dark green), <UNK> (lime green) and to <UNK> (red). adding a factor with many categories figure <UNK> shows the results when samples of <UNK> are simulated using <UNK> confounders <UNK> binary, <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> the inclusion of the <UNK> variable has had little influence on the proportionate changes for the other factors in the unweighted model (figure <UNK> when the factors are weighted the proportionate changes for the <UNK> factor are increased in the expanded model as opposed to having decreased previously. weighting of the prognostic factors has most effect on the extent to which the <UNK> factor has a similar distribution for the <UNK> treatment groups. discussion in this paper we have simulated some common treatment trial scenarios and compared the results in terms of the distribution of discrepancies between treatment groups. whilst only a selection of potential scenarios can be shown, we have illustrated how prior investigation helps quantify the sensitivity of minimisation to the choice of input parameters (randomisation weights, weighting of prognostic factors, number and type of factors). assuming that numerically small differences between the groups are of little practical clinical importance, then we can make several useful statements regarding the selected scenarios: sfaigmuprlee <UNK> <UNK> <UNK> prognostic factors <UNK> binary, <UNK> <UNK> <UNK> <UNK> sample size <UNK> <UNK> prognostic factors <UNK> binary, <UNK> <UNK> <UNK> <UNK> <UNK> centiles of the distributions of proportionate changes from expected for randomisation weights from <UNK> to <UNK> obtained from <UNK> simulations. dotted lines show results when prognostic factors are weighted according to the number of categories. results for binary factors shown in black, <UNK> in blue and <UNK> in purple. • the number of factors to be taken into account can be increased quite substantially without severely affecting the overall balance. • weighting of the factors had most effect on the factors with many categories and was not highly detrimental to the factors with fewer categories (in keeping with the findings of weir and lees <UNK> • including interaction terms in the minimisation did not greatly increase the overall discrepancies. • even weighting the randomisation by a small amount in favour of the preferred treatment had a large effect on the equality of the distribution of prognostic factors between treatments. • increasing the randomisation weights above <UNK> had little effect on the extent to which the treatment groups were similar. note that for a different number or type of minimisation factors the above statements may not hold. they are not meant to be universally true. these are statements about one particular hypothesised scenario to show how infor- sfaigmuprlee <UNK> <UNK> <UNK> prognostic factors <UNK> binary, <UNK> <UNK> <UNK> <UNK> sample size <UNK> <UNK> prognostic factors <UNK> binary, <UNK> <UNK> <UNK> <UNK> <UNK> centiles of the distributions of proportionate changes from expected for randomisation weights from <UNK> to <UNK> obtained from <UNK> simulations. dotted lines show results when prognostic factors are weighted according to the number of categories. results for binary factors shown in black, <UNK> in blue and <UNK> in purple. mation relating to that scenario can be easily generated via simulations. we believe that prior simulation according to expected sample size will be useful for clinicians embarking on a randomised controlled trial for which prognostic factors exist and should be equalised between treatment groups. simulation will help quantify the effect of different input parameters on the expected discrepancies. it may assist in the choice of randomisation weighting utilised and the trade-off between minimizing for more criteria and/or increasing the categories of minimisation where prognostic factors are continuous. however, it should be noted that the decision of which variables to include in the minimisation process should be informed primarily by the clinical importance of variables and their impact on outcome. vaughan reed and wickham <UNK> give further discussion to the choice of cut-points for continuous prognostic factors when performing minimisation. all minimisation variables should be adjusted for in the final analyses <UNK> and it is therefore important that only necessary variables are included to avoid over-parameterisation of the models. there is a trade-off between incorporating too many variables/unnecessarily increasing the number of categories used and allowing imbalance in important prognostic factors. the results of the mfiignuimreiz <UNK> the difference of the interactions minimizing the difference of the interactions. <UNK> centiles of the distributions of proportionate changes from expected for randomisation weights from <UNK> to <UNK> obtained from <UNK> simulations. all <UNK> <UNK> interactions from <UNK> prognostic factors <UNK> binary, <UNK> <UNK> <UNK> <UNK> used as minimisation criteria. dotted lines show results when prognostic factors are weighted according to the number of categories. results for the <UNK> <UNK> × <UNK> interaction terms shown in black, for the <UNK> <UNK> × <UNK> interactions in dark green, the <UNK> <UNK> × <UNK> in lime green and the <UNK> <UNK> × <UNK> interaction in red. simulation exercises can assist in the process but cannot be used as the sole, or even main, decision criteria for inclusion of a particular variable. in theory, the process of minimisation is relatively simple to undertake <UNK> but in practice we feel that clinicians do not find it easy to keep running totals and perform weighted randomisations. automation of the process (in addition to telephone allocation where available) reduces the propensity for conscious or unconscious interference with the allocation procedure. given the importance of allocation concealment, this is an additional benefit of employing minimisation. furthermore, simulated models are not necessarily straightforward to generate. consequently, we have developed a software package (simin) for clinical usage which enables not only easy minimisation but also generates simulations that may be useful prior to the commencement of the study to help in determining which input parameters to use. it is important that the person performing the allocations is independent of the trial team. having a quick and simple to use automated system makes it easier to enrol suitable individuals for this task. at study commencement it should be possible to justify the number and type of minimisation variables, their faidgduinrge <UNK> <UNK> factor adding a <UNK> factor. <UNK> centiles of the distributions of proportionate change from expected for randomisation weights from <UNK> to <UNK> obtained from <UNK> simulations. dotted lines show results when prognostic factors are weighted according to the number of categories. results for binary factors shown in black, <UNK> in blue, <UNK> in purple and <UNK> in green. weighting and the choice of randomisation weighting. simulation enables estimation of the discrepancies anticipated and their probability. for example, suppose a treatment trial is estimated to require <UNK> patients to be allocated to the new or standard treatments to obtain a reasonable power to detect differences in outcome of clinical importance. potential confounders are sex (male/ female), age (under/over <UNK> whether the individual is an in- or out-patient, severity of disease (mild/moderate/ severe) and ethnicity <UNK> categories). all categories of all minimisation variables are expected to be equally likely. (this latter criterion may not realistic but the density functions can be easily adjusted within the simulations.) if a randomisation weight of <UNK> (p = <UNK> is used then the proportionate change from expected of patients allocated to new and standard treatment is expected to be less than or equal to <UNK> for sex, age group and patient status (in- or out- patient), less than or equal to <UNK> for disease severity and less than or equal to <UNK> for ethnicity for <UNK> of random patient samples (figure <UNK> these are equivalent to absolute difference of <UNK> <UNK> and <UNK> patients respectively. a statement such as this could be incorporated into the protocol in a similar way to having a power calculation. i.e. the protocol could state, "sex, age (under/over <UNK> residency status of patient (in/out), severity of disease (mild/ moderate/severe) and ethnicity <UNK> categories) were included as factors in the minimisation. factors were unweighted and a randomisation weighting of <UNK> was used for the allocations. it was estimated that the discrepancy between patients allocated to the <UNK> treatment groups would not exceed <UNK> <UNK> or <UNK> for the binary variables, disease state and ethnicity respectively with probability <UNK> frequently, very little information is given on randomisation weights, weighting of variables or even the categories used in description of minimisation procedures. whilst it should be possible to justify the minimisation criteria, the precise details of the allocation process should not be widely divulged until after the trial has completed. the less information that is accessible, the less chance there is of recruitment being biased by knowledge of the likely allocation of future patients. we recommend that the details of and justification for the allocation process being employed are documented and given in the final trial reports. however, we also recommend that these details are not revealed to the research team during the trial apart from where this is essential. the international conference on harmonisation <UNK> guidelines <UNK> discuss the importance of minimising bias in the design of trials, with 'bias' defined as "the systematic tendency of any factors associated with the design, conduct, analysis and interpretation of the results of clinical trials to make the estimate of a treatment effect deviate from its true value." these guidelines also state that "good design should generally aim to achieve the same distribution of subjects to treatments within each centre and good management should maintain this design objective." the use of dynamic allocation of patients to treatments is one way to achieve these aims. in this paper we have investigated some of the issues that arise in the practical application of one allocation technique the use of which has risen sharply in recent years. we have performed simulations using the most common scenario of <UNK> treatment groups. a similar process could be done where there are more treatment groups and this option has been incorporated into our software. we have simulated datasets under different minimisation criteria to show how outcomes may vary as the input parameters are changed and suggest that this sort of approach should become standard practice. previously it has been noted that simulations can be usefully employed prior to study commencement to determine the best allocation method to use <UNK> these studies have used relatively complex models to compare not only minimisation parameters but also alternative approaches such as stratification. in some cases they have incorporated existing data <UNK> our results are in keeping with the findings of these more detailed studies. what we have aimed to show in this paper is how a relatively simple automatic algorithm, made available in package form, can be used to assist clinicians when they have decided to utilise minimisation and need to determine the optimal parameters. the package simplifies the practicalities of the process and hence may make this the preferred allocation method even when there are few prognostic factors to be taken into account and stratification is also a feasible option. it is important that researchers justify the choices they make with regards to the procedure for allocating patients. the arguments for this are not dissimilar to the argument for giving a power calculation or describing other details of the study protocol. conclusion the use of minimisation as a means of patient allocation is increasing. decisions need to be made regarding the precise mode of implementation. choice of input parameters may influence the extent to which the process is successful in ensuring equality of prognostic factors between treatment groups. we show how a simple automated package that we have developed locally can be used to allow researchers to investigate the effects of varying the input parameters prior to study commencement. the advent of the wide availability of computing technology makes minimisation a more realistic choice for many researchers. it is important that they utilise the technique most effectively. we have shown how simulations can be used prior to study commencement to ensure that the minimisation has a reasonable chance of providing comparable treatment groups.

<|EndOfText|>

abstract background: a randomised controlled trial of participatory women's groups in rural nepal previously showed reductions in maternal and newborn mortality. in addition to the outcome data we also collected previously unreported information from the subgroup of women who had been pregnant prior to study commencement and conceived during the trial period. to determine the mechanisms via which the intervention worked we here examine the changes in perinatal care of these women. in particular we use the information to study factors affecting positive behaviour change in pregnancy, childbirth and newborn care. methods: women's groups focusing on perinatal care were introduced into <UNK> of <UNK> study clusters (average cluster population <UNK> a total of <UNK> women of reproductive age enrolled in the trial had previously been pregnant and conceived during the trial period. for each of four outcomes (attendance at antenatal care; use of a boiled blade to cut the cord; appropriate dressing of the cord; not discarding colostrum) each of these women was classified as better, good, bad or worse to describe whether and how she changed her pre-trial practice. multilevel multinomial models were used to identify women most responsive to intervention. results: among those not initially following good practice, women in intervention areas were significantly more likely to do so later for all four outcomes (or <UNK> to <UNK> within intervention clusters, women who attended groups were more likely to show a positive change than non-group members with regard to antenatal care utilisation and not discarding colostrum, but non-group members also benefited. conclusion: women's groups promoted significant behaviour change for perinatal care amongst women not previously following good practice. positive changes attributable to intervention were not restricted to specific demographic subgroups. background maternal and newborn mortality rates remain unacceptably high in the developing world. most births and newborn deaths occur outside health facilities, so behaviour change in relation to home care practices and care-seeking behaviour is an essential component of any strategy to reduce deaths. we reported previously a cluster randomised controlled trial of the effects of participatory women's groups on neonatal outcomes in rural <UNK> the trial intervention was a woman facilitator (who was not a trained health worker) within each area paid to instigate and guide women's groups focused on care in the perinatal period. the trial showed significant falls in neonatal <UNK> and maternal mortality <UNK> and appeared to be cost <UNK> married women of reproductive age <UNK> years) living in the study areas at the time of study inception were eligible. before the trial started, each eligible woman was asked about her most recent pregnancy. if this resulted in a stillbirth, infant care practices were asked in respect of the most recent live birth. information as to who was present at the birth and whether it took place in an institution was recorded. in particular it was ascertained whether there was a skilled attendant at the birth. the woman was asked whether she had attended antenatal care, which implement was used to cut the cord, what was applied to the cord after it was cut (the criterion for cleanliness was that either nothing or antiseptic was used) and whether or not she had discarded colostrum before starting to breastfeed, a practice distinct from discarding the foremilk at each feed. the evidence base for deciding which care practices are beneficial for good perinatal outcome is <UNK> however, the practices recorded within the trial (antenatal care, skilled birth attendance, measures of cleanliness and good breastfeeding practice) have long been accepted as <UNK> after the baseline interview each woman became a member of the closed cohort who were randomised within village development committee areas (vdcs) and followed prospectively. in the original trial the efficacy of women's groups was measured for all women living within intervention areas (compared with control areas), even though many did not attend groups. the use of pre-trial pregnancy data allowed us to investigate the precise patterns of behaviour change within individual women and subgroups of women. some women, in both arms of the trial, did not have the capacity for positive change attributable to intervention because they followed good practice in a pre-trial pregnancy. for women who did not follow good practice before the trial, our study gives us greater insight into factors affecting positive behaviour change, such as group membership, socioeconomic status, ethnicity and maternal age. the subset of women used for these analyses had by definition a pre-trial pregnancy and the results are not necessarily generalisable to women whose first pregnancy occurred in the trial. methods details of the original trial are reported <UNK> briefly, <UNK> cluster units comprising village development committee areas (vdcs) – existing geopolitical units of population about <UNK> – were placed into <UNK> matched pairings of similar topography, ethnicity and population densities. one vdc area of each matched pair was randomly assigned to receive the intervention and the other formed a control. all eligible women were identified and details of pregnancies, births and deaths were recorded prospectively for <UNK> months. the analysis includes women who had reported a previous pregnancy and who had a subsequent pregnancy during the surveillance period. twin pregnancies were included only once in the dataset as the process outcomes under consideration mostly related to the delivery or woman at that time rather than the individual child. repeated pregnancies were included in the analysis with the appropriate clustering to account for within-woman correlation of outcomes and responses. statistical analysis the practices undertaken in each trial pregnancy were compared with those practices a woman had reported in her pre-trial pregnancy. each practice was classified for each pregnancy as: <UNK> better – lack of good practice in the preceding pregnancy followed by good practice in the trial pregnancy. <UNK> good – good practice in both preceding and trial pregnancies. <UNK> bad – lack of good practice in both preceding and trial pregnancies. <UNK> worse – good practice in the preceding pregnancy but not in the trial pregnancy. we fitted multilevel multinomial models, taking into account the pairing of vdc area clusters, the clustering of women within vdc areas and households, and the correspondence between repeat prospective pregnancies in the same woman, to the <UNK> outcomes. multinomial models were preferred to logistic regressions of the trial practices corrected for pre-trial behaviour since they distinguished between changes from bad to good or from good to bad practice. multinomial models are extensions of logistic models. associations between the outcomes and various features of the women are quantified and presented as coefficients for the ratios falling into the better category relative to the other categories. this representation of the model results was chosen as being the easiest to interpret clinically. for all <UNK> ratios thus obtained, larger values were associated with more favourable outcome. all coefficients are presented with <UNK> confidence intervals adjusted for the clustered nature of the data. for each feature, separate coefficients are given to quantify the ratios: <UNK> better relative to good – quantifies the extent to which women following good practice in the trial pregnancy were doing so as a result of positive change (as opposed to continuing the good practices they had adopted pre-trial). <UNK> better relative to bad – quantifies the extent to which women who were following bad practice in the pre-trial pregnancy improved their practice within the trial. this coefficient is of particular interest as it describes the extent to which opportunities for positive change were taken. <UNK> better relative to worse – quantifies the extent to which those women who changed practice made a positive, as opposed to negative, change. a series of multilevel multinomial models were fitted to each of the process variables. firstly, models were used to quantify differences in patterns of change between control and intervention clusters and the additional effect of attending a women's group for women within intervention clusters. secondly, a series of models were fitted to investigate whether the effect of intervention varied between women of differing ages, literacy levels and education, or between those living within households of differing ethnicity, assets or food sufficiency. for each of these demographic variables, a model which incorporated the demographic variable, a variable representing intervention status, and a term for the interaction between these two variables was used. the intervention and demographic variables were independently significantly associated with outcomes in all models. the coefficients for the fitted interaction terms showed which groups of women were most likely to respond to intervention and these are presented. coefficients greater than <UNK> indicate that the women in the intervention clusters within that demographic subgroup had a more favourable distribution compared to the baseline category which was over and above any increase in favourable practices that could be attributed to intervention across all subgroups. coefficients less than <UNK> are associated with a less favourable response for that demographic subgroup of women in the intervention compared with control clusters. ethics and consent the study was registered as an international standard randomised controlled trial, number <UNK> it was approved by the nepal health research council and the ethical committee of the institute of child health and great ormond street hospital for children, and was conducted in collaboration with his majesty's government ministry of health, nepal. the aims and design of the trial were discussed at both national and local meetings, after which consent to cluster involvement was given by chairpersons of vdc areas and the makwanpur district development committee. women who chose to participate in the study gave oral consent, were free to decline to be interviewed at any time, and the information they provided remained confidential. results of the women for whom information regarding a previous pregnancy had been recorded, <UNK> had one further pregnancy during the trial surveillance period, <UNK> had <UNK> pregnancies and <UNK> had <UNK> pregnancies. hence, there were a total of <UNK> within-trial pregnancies from women with retrospectively recorded information. most women delivered at home <UNK> without a trained attendant <UNK> or any government health personnel present <UNK> in either the preceding or study pregnancy. the sentinel care practices of antenatal care uptake, use of a clean blade to cut the umbilical cord, appropriate dressing of the cord and feeding of colostrum to the baby were more variably followed. table <UNK> shows the demographic breakdown of the women and the extent to which good practice was being followed prior to commencement of the study. approximately three quarters of the women were appropriately dressing the cord initially and this proportion was fairly constant across demographic subgroups. attendance at antenatal care, boiling of the blade and not discarding colostrum were all more prevalent amongst the more highly educated and literate women from wealthier households. the effect of being in an intervention vdc table <UNK> shows the percentages of pregnancy pairings falling into each of the <UNK> categories (better, good, bad, worse) for the <UNK> outcomes for women in intervention and control arms of the trial. the percentage of women who were following good practice during their trial pregnancies can be obtained by adding together the percentages falling into the better and good categories. for each of the <UNK> outcomes a greater percentage of the women in the intervention clusters followed good practice during the trial. combining the better and bad categories gives the percentage of women who were following bad practice pretrial (and hence had the capacity to change for the better). for all outcomes apart from the discarding of colostrum, control clusters had more women with that capacity than intervention clusters. the percentages of women who recalled discarding colostrum in their preceding births were approximately equal between control and intervention clusters. the percentages lying within the bad category represent missed opportunities for positive change and there were consistently fewer women within intervention clusters falling into this category for each of the <UNK> outcomes. women who changed their practice between preceding and study pregnancies fell into the better and worse categories. the percentage of women in the intervention clusters falling into the better category was greater than the percentage in the worse category, showing that women were more likely to make a positive, as opposed to detrimental, change for all outcomes. women in the control clusters were more likely to stop, as opposed to table <UNK> practices in pre-trial pregnancies according to demographic variables number following good practice pre-trial (% of total) antenatal care attendance boiling the blade appropriate dressing of cord not discarding colostrum n = <UNK> (%) n = <UNK> (%) n = <UNK> (%) n = <UNK> (%) household ethnicity: tamang <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> brahmin-chhetri <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> magar <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> other <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no assets listed <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> clock, radio, iron, bicycle <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> more costly appliances <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> mother illiterate <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> reads with difficulty <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> reads with ease <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no formal education <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> primary schooling only <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> secondary or higher <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> total <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> median (interquartile range) for those following good (g) and bad (b) practice retrospectively: household number of months with g: <UNK> <UNK> g: <UNK> <UNK> g: <UNK> <UNK> <UNK> g: <UNK> <UNK> <UNK> sufficient food b: <UNK> <UNK> b: <UNK> <UNK> b: <UNK> <UNK> <UNK> b: <UNK> <UNK> <UNK> mother age (per additional year) g: <UNK> <UNK> <UNK> g: <UNK> <UNK> <UNK> g: <UNK> <UNK> <UNK> g: <UNK> <UNK> <UNK> b: <UNK> <UNK> <UNK> b: <UNK> <UNK> <UNK> b: <UNK> <UNK> <UNK> b: <UNK> <UNK> <UNK> note: numbers are less than <UNK> for each outcome since some women did not have pregnancies that progressed to the stage for that outcome to be appropriate. for example, <UNK> of the eligible pregnancies did not result in a live birth of a surviving mother and hence the discarding of colostrum was only appropriate as an outcome for <UNK> women. start, appropriate dressing of the cord, but otherwise their changes were similarly more likely to be in a positive direction. the differences between women in the intervention and control vdcs are further quantified by the fitting of multinomial models to the <UNK> outcomes with intervention status as a predictor. the coefficients and confidence intervals are given for the better/bad and better/worse ratios. these are all significantly different to <UNK> for all four practices women who were initially following bad practice were significantly more likely to change to good practice if they lived in an intervention vdc (better/bad ratios). for example, women who did not attend antenatal care in preceding pregnancies were more than twice as likely to do so during the study period if they lived in an intervention area (odds ratio <UNK> <UNK> ci <UNK> <UNK> times)). of the women who changed practice these changes were significantly more likely to be in a positive direction for all outcomes except antenatal care attendance (better/ worse ratio). women attending antenatal care and/or using a boiled blade to cut the cord in pregnancies falling within the study period were significantly less likely to be doing so as a result of a positive change in practice if they lived in an intervention vdc (better/good ratios). these results are not unexpected given the larger percentages of women within the intervention vdcs following good practice for these outcomes pre-trial. the independent effect of attending a women's group about one in twelve married women of reproductive age, and about one third of newly pregnant women in intervention clusters attended the women's groups. there were few differences between the percentages of women who did and did not attend women's groups falling into each of the <UNK> categories. the effect of attending a group over and above the improvements attributable to living within an intervention area was greatest for antenatal care attendance. the percentages of women who attended the groups falling table <UNK> behaviour change over time between pre-trial and trial pregnancies for four perinatal care practices. antenatal care attendance boiling the blade appropriate dressing of cord not discarding colostrum intervention n = <UNK> n = <UNK> n = <UNK> n = <UNK> control n = <UNK> n = <UNK> n = <UNK> n = <UNK> % better intervention <UNK> <UNK> <UNK> <UNK> control <UNK> <UNK> <UNK> <UNK> % good intervention <UNK> <UNK> <UNK> <UNK> control <UNK> <UNK> <UNK> <UNK> % bad intervention <UNK> <UNK> <UNK> <UNK> control <UNK> <UNK> <UNK> <UNK> % worse intervention <UNK> <UNK> <UNK> <UNK> control <UNK> <UNK> <UNK> <UNK> total (%) intervention <UNK> <UNK> <UNK> <UNK> control <UNK> <UNK> <UNK> <UNK> intervention/control comparisons : odds ratios <UNK> confidence interval) %better/%bad ratio * <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> %better/%worse ratio* <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> %better/%good ratio* <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> *results from multilevel multinomial models. the estimates and intervals are adjusted to take account of the correlations between pregnancies within the same women, women from the same household, households from the same vdc and vdcs within the same matched pair. all odds ratios are significantly different to <UNK> coefficients greater than <UNK> indicate that the women in the intervention clusters had a more favourable distribution, those less than <UNK> are associated with a less favourable response for the women in the intervention compared with control clusters. into the better, good, bad and worse categories were <UNK> <UNK> <UNK> and <UNK> respectively, compared to <UNK> <UNK> <UNK> and <UNK> of those within intervention vdcs who did not attend groups. hence, a larger percentage of those attending the women's groups improved their practice <UNK> vs <UNK> or maintained previous good practice <UNK> vs <UNK> the significantly lower odds of making a positive as opposed to negative change (better/ worse ratio) in the intervention vdcs were counter- acted in the subgroup who attended the women's groups. the women who attended the groups were significantly more likely to make positive changes than nonattending women within intervention vdcs (better/ worse ratio <UNK> <UNK> <UNK> similarly, the women within intervention vdcs who attended the groups but did not attend antenatal care in their previous pregnancies were significantly more likely to start doing so than the women within those same vdcs who did not attend (better/bad ratio <UNK> <UNK> <UNK> this difference was additional to the <UNK> fold increase seen in the intervention vdcs overall. the better/good ratio for attenders vs non-attenders was also significant <UNK> <UNK> <UNK> women attending the groups were significantly more likely to make positive changes compared to non-attending women in the same vdcs with respect to discarding colostrum (better/worse ratio <UNK> <UNK> <UNK> and there was some evidence that if they were discarding colostrum previously they were more likely to stop doing so (better/bad ratio <UNK> <UNK> <UNK> there were no other significant differences. were specific subgroups of women with the capacity for positive change more likely to respond to intervention? table <UNK> shows the increase in the better/bad ratios for the intervention group compared to the women in control areas. values greater than <UNK> indicate that the intervention was more successful in those subgroups of women relative to the baseline demographic category. significant differences in the effects of intervention on the four process outcomes were not consistent across demographic subgroups. were women who changed practice more likely to do so positively if they were from specific subgroups? the extent to which women made positive, as opposed to negative, changes in practice is quantified by the better/ worse model coefficients. patterns were not consistent (table <UNK> but they were based on the smallest groups (table <UNK> women from households with more assets within intervention vdcs were significantly more likely to make a positive change to dressing the cord but a negative change with respect to the treatment of colostrum. it was the older women, those who were less literate and the less well educated who were significantly more likely to have stopped, as opposed to started, discarding colostrum if they lived in intervention, as opposed to control, vdcs. were women from specific subgroups who followed good practice during the trial more likely to be doing so as a result of a positive change? these differences are quantified in table <UNK> (better/ good ratios). this table is presented for completeness. however, it is of the least clinical interest due to the dependence on the variability between groups of the percentages who show no changes but continue good practice throughout. discussion within a large scale trial of a community group intervention, women were followed prospectively to document patterns of behaviour change for perinatal care. this helps to understand how primary trial outcomes may be explained by changes in the practices of individuals within the communities. of the <UNK> women who became pregnant and were included in the main trial analyses, a subset of <UNK> <UNK> had a pregnancy pretrial with which to compare their trial pregnancy behaviour. within this subgroup we have investigated the changes for women undergoing their second or subsequent pregnancies. the findings cannot be extrapolated to women in their first pregnancies. as expected, there were strong relationships between past and present behaviour. those who followed good practice in previous pregnancies were likely to do so again, regardless of whether they were allocated to the intervention or control arm of the trial. having a skilled birth attendant is known to be an important indicator of outcome. less than <UNK> in <UNK> of the women had such a person present at either their pre-trial or any trial pregnancy. the numbers therefore were too low to investigate any impact the intervention may have had on improving skilled birth attendance. however, it was possible to investigate changes in other factors known to be important: antenatal care, cleanliness of blade and cord, and discarding of colostrum. the intervention effectively promoted significant change in all four care behaviours amongst the group of women not previously following good practice (table <UNK> positive changes in antenatal care attendance and the discarding of colostrum were more likely to be made by women who attended the groups, but behaviour change in hygienic cutting and dressing is observed generally in the intervention areas. the lack of uniform relationship between group attendance and outcome was expected. the presence of groups in an area has a wider impact than merely on the women who attend. in our study only <UNK> of married women of reproductive age joined our groups, but <UNK> of newly pregnant women attended at least once. whilst group members showed a greater tendency to posbmc itive behaviour change than non-group members, this effect is unlikely to explain the overall improved behaviour change in intervention versus control clusters. our data provides evidence that the activities and existence of the group stimulate wider behaviour change in their communities. the group intervention is a dynamic process that is uniform only in its participatory method, thus further study is necessary to explore these processes of behaviour change. we hoped to bring about behaviour change by giving women and grandmothers the knowledge they need to make informed choices, and by creating favourable social conditions, and an enabling environment in which they could take these <UNK> preliminary analysis of qualitative data, and the data presented here suggest that this has been the case in the intervention areas. most of the responses to intervention were positive. significantly greater percentages of women in intervention vdcs who were following bad practice pre-trial stopped doing so after the commencement of the women's groups. it was surprising that significantly more of the intervention area women stopped as opposed to started attending antenatal care compared to the women within control vdcs. this difference was mostly attributable to the greater proportion of women in intervention vdcs who stopped attending. for the other <UNK> practices a greater percentage of control women stopped previous good practice and for all <UNK> practices there was a lower percentage who started. it is possible that the women in the intervention vdcs saw women's groups as a replacement for antenatal care. this potentially detrimental effect of the intervention requires further investigation, perhaps via the use of focus groups in similar future initiatives. since allocation was random, we would not expect baseline differences between women in control and intervention vdcs. despite similar mortality rates at <UNK> some differences in practices were found in the subgroup with a previous pregnancy reported here. in particular, women within intervention vdcs were more likely to have attended antenatal care <UNK> intervention, <UNK> table <UNK> coefficients and <UNK> confidence intervals for the extent to which women in the intervention vdcs, relative to women in the control vdcs, within different demographic subgroups were more (or less) likely to make a positive change, relative to those in the baseline subgroup, if they were not initially following good practice (%better/%bad ratio) antenatal care attendance n = <UNK> boiling the blade prior to cord cutting n = <UNK> appropriate dressing of the cord n = <UNK> not discarding colostrum n = <UNK> household: ethnicity: tamang <UNK> <UNK> <UNK> <UNK> ...brahmin-chhetri <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> magar <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> other <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no assets listed <UNK> <UNK> <UNK> <UNK> clock, radio, iron, bicycle <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> more costly appliances <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> number of months with sufficient food <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> mother: age (per additional year) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> illiterate <UNK> <UNK> <UNK> <UNK> reads with difficulty <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> reads with ease <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no formal education <UNK> <UNK> <UNK> <UNK> primary schooling only <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> secondary or higher <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> (results from multilevel multinomial models. the estimates and intervals are adjusted to take account of the correlations between pregnancies within the same women, women from the same household, households from the same vdc and vdcs within the same matched pair. significant differences are shown in bold.) control) and to have boiled the blade <UNK> and <UNK> respectively) in their pre-trial pregnancies. the analyses presented in this paper show that multigravid women in intervention vdcs were significantly more likely to continue or begin good practices after accounting for baseline differences. these significant differences in behaviour within this subgroup of just over three-quarters of the women who fell pregnant within the trial period are compatible with the reductions in major outcomes found within the trial. we have presented secondary analyses of the dataset. many comparisons are presented and these are meant to be interpreted in unison and with the main outcome analyses. the study was not originally designed to detect subgroup differences and the results should not be interpreted as though they were primary objectives. what we have aimed to do is to identify patterns that might be clinically relevant and informative to future studies. we have not identified any major consistent patterns, a finding which is itself of interest. having identified significant differences which could be attributed to the intervention[ <UNK> this analysis investigates the modes via which those differences may have been achieved. we would expect to observe differences in process outcomes since these are known to be related to mortality outcomes. the finding that the intervention was associated with increased uptake of good practices in those previously not following them is both important and as expected: in this paper we attempt to quantify the degree of difference. it is also important to note that no tendency for the intervention to target only subgroups of privileged or non-privileged women was found. prior to performing these analyses we had no notion of the direction that any intervention bias might fall. if women were already following good practice, the capacity for the women's groups to effect positive change was limited. it was important that those following good practices continued to do so. therefore, we have considered all patterns of change and how they related to features of the mother, the household in which she lived and whether or not she resided in an intervention area. four pre-trial practices were found to have a large capacity for positive change and for there to have been significant alterations during the study period. the women's groups discussed table <UNK> coefficients and <UNK> confidence intervals for the extent to which women in the intervention vdcs, relative to women in the control vdcs, within different demographic subgroups were more (or less) likely to make positive as opposed to negative changes, relative to those in the baseline subgroup, (%better/%worse ratio) ante-natal care attendance n = <UNK> boiling the blade prior to cord cutting n = <UNK> appropriate dressing of the cord n = <UNK> not discarding colostrum n = <UNK> household: ethnicity: tamang <UNK> <UNK> <UNK> <UNK> ...brahmin-chhetri <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> magar <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> other <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no assets listed <UNK> <UNK> <UNK> <UNK> clock, radio, iron, bicycle <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> more costly appliances <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> number of months with sufficient food <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> mother: age (per additional year) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> illiterate <UNK> <UNK> <UNK> <UNK> reads with difficulty <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> reads with ease <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no formal education <UNK> <UNK> <UNK> primary schooling only <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> secondary or higher <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> (results from multilevel multinomial models. the estimates and intervals are adjusted to take account of the correlations between pregnancies within the same women, women from the same household, households from the same vdc and vdcs within the same matched pair. significant differences are shown in bold.) the prevention of neonatal deaths, home care practices that might help, and the use of health services for either routine or emergency care. the issues of antenatal care, the use of clean cord-cutting implements, avoidance of unhygienic dressings and the benefits of colostrum feeding arose as subjects of discussion on many occasions. these issues could, and were, easily translated into specific actions. it was clear that the less educated and illiterate women were less likely to be following good practice initially. although these women were significantly targeted by the intervention for some outcomes, the differences were not uniform. there were benefits across all of the demographic subgroups of women. conclusion in conclusion, peer-education and empowerment of women through women's groups has positive effects on perinatal care practices for women in their second or subsequent pregnancies. both group members and nongroup members in the locality benefit from this intervention.

<|EndOfText|>

are adolescents with constipation more likely to suffer psychological <UNK> what percentage of chiari i-type headaches show improvement after foramen magnum decompression <UNK> does bcg vaccination reduce early childhood hospitalisation in <UNK> is diagnosis of coeliac disease associated with differences in adolescent <UNK> does visual feedback affect the rate of chest <UNK> these are all questions asked in recent issues of this journal. in each case the authors collated information from a sample of individuals to yield an answer to their question. differing study types were used ranging from observational audits and surveys through to randomised parallel and crossover trials. the study designs, participants, settings, sample sizes and key statistics are summarised in table <UNK> table <UNK> description of the five studies study design participants setting sample size key statistics <UNK> cross-sectional survey <UNK> year olds <UNK> schools <UNK> <UNK> constipated) <UNK> of constipated and <UNK> of non-constipated had maladjustment: or <UNK> <UNK> <UNK> <UNK> retrospective review of audit database/ before-after study chiari i-type headache cases having fmd tertiary hospital <UNK> <UNK> showed improvement post fmd <UNK> randomised controlled trial newborn babies <UNK> danish hospitals <UNK> children <UNK> bcg, <UNK> controls) <UNK> hospitalisations bcg group vs <UNK> controls. hr <UNK> <UNK> <UNK> <UNK> general health examination database <UNK> yr old israeli jews eligibility assessment for military service <UNK> adolescents; <UNK> with coeliac disease (cd)) boys with cd had lower bmi (average <UNK> vs <UNK> cd girls were shorter <UNK> vs <UNK> cm on average) <UNK> randomised crossover trial hospital staff tertiary hospital <UNK> pairs of measurements – with and without visual feedback rate of chest compressions was lower and less variable in those receiving feedback despite these differences, the same basic principle is followed for each. a sample of the relevant group of individuals is identified and from observing what happens to this sample, inferences are made about the wider population. the inferences may be beneficial to similar individuals and those involved in their care. for example, clinicians trying to determine whether to perform <UNK> or parents considering the pros and cons of bcg vaccination. <UNK> how well a question is answered by the study depends on how large a sample was studied in conjunction with other factors such as the variability of the measurements and/or event rates. both researchers and patients intuitively understand that findings based on a larger sample are likely to be more accurate, and will have more confidence in results based on a randomised trial of <UNK> individuals than if only <UNK> patients had been recruited. what is less intuitive is that if a treatment is not shown to be effective in a small sample, it may still have benefits. similarly, we are generally less inclined to also apply such intuitive logic to observational descriptive studies. this article will explain the rationale behind consideration of sample size for all types of research study, wherever data is collated to address a proposed research question. making statements based on the available sample to explore these concepts further, consider the chiari i-type headache <UNK> here <UNK> of individuals showed improvement post fmd, which gives a sample estimate of <UNK> improvement rate. a <UNK> confidence interval for the rate is <UNK> <UNK> which means that we are <UNK> confident that at least <UNK> of the population (in this case, chiari i-type headache sufferers) will improve. if we wanted a more accurate estimate, then a larger sample size would give this. for example, suppose <UNK> had been assessed and of these <UNK> had improved, then this would still be a rate of <UNK> but the <UNK> confidence interval would now be <UNK> <UNK> and we would therefore be <UNK> confident that at least <UNK> would improve. the data that we have is compatible with population estimates anywhere within the <UNK> confidence interval at the <UNK> significance level. any hypothesised population estimate that we test against outside that interval will yield a p-value < <UNK> indicating a statistically significant difference and non-compatibility. there is a conceptual distinction between significance tests, which determine compatibility with a pre-specified hypothesised value, and confidence intervals, which have no prior hypothesis and seek only to identify a range of compatibility. for some study types one approach may be favoured over the other. significance tests are generally considered when presenting the results of a randomised trial, whereas an observational study may omit this approach entirely depending on the precise research question.for example, suppose that prior to the study the authors ascertained that improvement in at least <UNK> of patients would warrant routine use of fmd. when testing against a hypothesised value of <UNK> the sample of <UNK> would have yielded a p-value <UNK> a non-significant difference, indicating compatibility with <UNK> (as also shown by the confidence interval which contains this value). for the sample of <UNK> (where the <UNK> confidence interval does not contain <UNK> a significant result would be obtained <UNK> indicating non-compatibility. this example illustrates the correspondences in interpretation between p-values and confidence intervals. larger samples lead to narrower confidence intervals as they enable more population scenarios to be excluded. note that if interest lies on the difference between two groups, for example the heights of <UNK> year olds with and without coeliac disease, then the hypothesised population value will be zero (ie. no difference in average height between the two groups). the confidence interval will be for the difference in means (or percentages) between the two groups. if this interval excludes zero then the difference is statistically significant. what do power calculations have to do with this? studies don’t always give a definitive, or even very useful, answer to the research question posed. for example, if the aim of the headache study had been to determine whether the improvement rate was at least <UNK> then the sample of <UNK> would not have been sufficient. even though performing the significance test gives a non-significant p-value <UNK> this does not necessarily mean that the improvement rate is lower than <UNK> the <UNK> confidence interval <UNK> <UNK> shows that the data are in fact compatible with an improvement rate as high as <UNK> (as well as values below <UNK> we still don’t know whether fmd yields the necessary improvement rate to warrant usage as the answer given from the available sample is too imprecise. if a larger sample had been studied then the results would have been more conclusive. before starting a study a decision needs to be made as to whether it is feasible to collect a sufficiently sized sample to address the research question within a plausible time frame. power calculations can tell us how many subjects are needed to obtain a significant p-value if there is a difference of a given size from the hypothesised value <UNK> in example above). alternatively, power calculations can give the number needed to estimate a specified size of estimate (again <UNK> in above example) with given precision. finding the right number there are formulae, that are known as power calculations, which can be used to identify how many need to be sampled to make the estimates sufficiently precise and/or statistical significance likely. these formulae can readily be identified in appropriate statistics textbooks and <UNK> or via the many online calculators available, identified by a simple google search. hence in theory this is a relatively easy process – locate a formula or online calculator, plug in a few values and get a number of individuals/items/things that you should collect information on to answer your specified research question. however, there are decisions to make as to which formula and/or online calculator to use, what values to plug into this and how to properly interpret the number that magically comes out. the aim of this short paper is to guide the reader through some calculations so that they are better equipped to address these issues. formulae are given and these relate to those found on our online calculators <UNK> (http://tinyurl.com/samplesizecasc ). using a formula/online calculator there are two types of sample size formulae. the first of these is aligned with significance testing and providing p-values, the second is akin to confidence intervals. although within the results you will probably present both of these, you need only do one type of sample size calculation. it depends on whether the emphasis in your study is on detecting a difference, or on estimating parameters with sufficient precision. in this paper, both types of formulae are presented for both binary and numeric outcomes. a binary outcome is one which takes one of two values. for example, the headache study had a binary outcome - headaches either improved or not and the study was interested to identify the percentage who <UNK> by contrast, a numeric outcome is usually summarised by the mean of the values. for example, the average of the changes in rates of chest compressions for a clinician using two feedback <UNK> often the population parameter being estimated is the difference between two distinct groups of patients (with and without disease; treated vs control). in this case it is the difference in percentages or means between groups for binary and numeric outcomes respectively that is of interest. the difference may be between distinct groups of individuals, for example the percentage difference between those with and without <UNK> or the difference in mean bmi between boys with coeliac disease or <UNK> alternatively, measurements may be paired, within individual or by taking matched pairs of individuals, and these differences summarised, for example the rate of chest compression when the same clinician uses visual feedback or <UNK> formulae are shown for these scenarios. only formulae for binary outcomes and means are given in this paper. whilst these are the most commonly used and widely applicable examples of power calculations, it should be noted that formulae exist for many other scenarios, such as non-normal data, hazard ratios, odds ratios and/or hierarchical data. the principles of these formulae are similar although they relate to more complex situations, and are hence beyond the scope of this paper. terms to understand if the outcome is numeric, then an estimate will be needed of the variability of the measurements. this is expressed as the standard deviation (sd). larger values of the sd indicate greater variation. with a numeric outcome, we can express the difference we would like to detect as a standardised difference (sdiff), which is the difference divided by the sd. for example, if the sd of the change in rates of chest compression using two methods is <UNK> a difference of <UNK> will be equivalent to a sdiff of <UNK> <UNK> power is the ability of a study to detect a difference if it exists, and is usually set at <UNK> <UNK> or <UNK> a power of <UNK> means that if the difference exists there is an <UNK> chance that this study will identify it. hence there is a one in <UNK> <UNK> = <UNK> chance of not identifying the difference, so although a power of <UNK> is often used (and will require smaller numbers), it is generally better to have a higher power. significance level is usually set at <UNK> this is the chance of falsely declaring a difference when the population value is actually as hypothesised. precision is a measure of how closely we would to estimate the true value. the width of the confidence interval is synonymous with precision. when using the confidence interval based formulae, the precision needs to be expressed in the same terms as the outcome (percentage or mean range) and is defined, in the formulae presented here (and associated weblink given above) as half the width of the resultant confidence interval. for example, to estimate the difference in average height of coeliac and non-coeliac <UNK> year olds to within ± <UNK> the precision is entered as <UNK> and we expect to obtain a confidence interval of width <UNK> some power calculations some commonly used formula are presented here with examples. as explained above, the appropriate formula depends on whether it is a difference that is to be detected or a precision attained, and whether the outcome is numeric or categoric. the formulae are organised accordingly. detecting a difference <UNK> numeric outcomes to detect a specified difference sdiff with <UNK> power between two groups at the <UNK> significance level requires 𝟐𝟏𝒔𝒅𝒊𝒇𝒇𝟐 in each group. reduce this number by a quarter for <UNK> power and add a quarter for <UNK> power. for example, the sd of heights of <UNK> year olds is known to be about <UNK> and a study aims to detect a difference in average height of coeliac and non-coeliac <UNK> year olds of <UNK> cm or more. this is a sdiff of <UNK> = <UNK> the sample size required to do this is <UNK> per group ie. a total of <UNK> <UNK> year olds, half of whom have coeliac disease. to detect the same difference with <UNK> power, would require <UNK> per group <UNK> x <UNK> for <UNK> power, the numbers per group need to be increased to <UNK> <UNK> x <UNK> paired data if there are pairs of observations and it is the mean difference between pairs that is to be compared to an average of zero (implying no difference), then half the sample size given above is the number of paired observations that need to be made. note that the sdiff must be based on the sd of the within pair differences, which will be different to the sd within each group as the pairing removes variation due to differences between groups (such as differences in age, sex, diet, exercise level, disease status). for example, with the rate of chest compressions crossover trial, patients are measured using different methods but it is the difference that is of interest ie. each individual contributes one difference to the dataset despite having <UNK> measures made. to detect a difference between methods of <UNK> sdiff with <UNK> power at the <UNK> significance level will require <UNK> clinicians to make paired measurements. to detect the difference with <UNK> or <UNK> power requires <UNK> and <UNK> clinicians respectively. <UNK> binary outcomes the percentages in each group with the outcome are to be compared. if these are <UNK> and <UNK> then this difference can be detected with <UNK> power at the <UNK> significance level if the following number are assessed in each group: 𝟏𝟎.𝟓{%𝟏(𝟏𝟎𝟎−%𝟏)+%𝟐(𝟏𝟎𝟎−%𝟐)}(%𝟏−%𝟐)𝟐 as before, this number reduces by a quarter for <UNK> and increases by a quarter for <UNK> power. for example: i) the randomised trial of bcg vaccination aimed to detect a fall in the hospitalisation rate of <UNK> to <UNK> or lower. for <UNK> power, <UNK> significance, this requires: <UNK> = <UNK> per group, a total of <UNK> randomised to <UNK> equal sized groups (bcg vaccinated or not). ii) assuming <UNK> of normal children and <UNK> of children with constipation have psychological maladjustment, this difference can be identified with <UNK> power at the <UNK> significance level with two groups of <UNK> children <UNK> for <UNK> power the sample could be reduced to <UNK> per group. notice that the above two examples relate to quite different study forms, a rct and an observational study, yet the sample size formula required is the same. the formula required depends on the outcome (in this case difference in percentages in two groups) and the power and significance required. being sufficiently precise divide the quantity stated by the precision required squared (ie. <UNK> to give the sample size required to estimate with <UNK> confidence. <UNK> numeric outcomes <UNK> per group for example, to estimate the difference in average height between coeliac and non-coeliac <UNK> year olds to within ± <UNK> ( precision = <UNK> assuming a sd of <UNK> cm, will require <UNK> 𝑥 <UNK> per group, total <UNK> for a more precise estimate within ± <UNK> cm, requires a larger sample of <UNK> 𝑥 <UNK> per group, total <UNK> paired data for paired measurements, the number of pairs required is half of the sample size per group as given above (ie. <UNK> paired measurements). for example, to estimate the average difference between rates of compression using <UNK> methods to within ± <UNK> where the sd of the within pair differences is <UNK> requires <UNK> 𝑥 <UNK> clinicians making paired assessments. <UNK> binary outcomes to estimate a single percentage ( <UNK> ): <UNK> <UNK> <UNK> - <UNK> for example, if <UNK> of patients with chiari i-type headaches improve then this can be estimated to within ± <UNK> with <UNK> confidence using a sample of <UNK> 𝑥 <UNK> <UNK> patients. to estimate a difference in percentages <UNK> <UNK> : <UNK> <UNK> <UNK> - <UNK> <UNK> <UNK> - <UNK> for example, to estimate a fall in percentages hospitalised when given bcg vaccination from <UNK> to <UNK> with a precision of <UNK> (ie. confidence interval width <UNK> would require a sample of <UNK> 𝑥 <UNK> per group, a total of <UNK> children. unequal groups sometimes when two groups are to be compared it is not anticipated that they will be of equal size. for example, in the study to compare psychological maladjustment rates between adolescents with and without constipation, this was a survey across schools and the numbers in the two groups would not be expected to be equal. an imbalance can be adjusted for by increasing the overall sample <UNK> if the above formulae estimate that n per group is required assuming equal sized groups, this is a total sample of <UNK> to account for an imbalance between groups of <UNK> the total sample size will need to be increased to 𝒏(𝟏+𝒌)𝟐𝟐𝒌 for example, in the constipation study it was anticipated that about <UNK> of the children would have constipation. this is an imbalance of <UNK> ie. for every <UNK> that are healthy, there will be <UNK> who is constipated and hence <UNK> the previous formula showed that <UNK> per group were required to detect a difference of <UNK> in psychological maladjustment <UNK> and <UNK> per group) with <UNK> power and <UNK> significance. this is a total sample of <UNK> to adjust for the smaller numbers of constipated children, the sample needs to be increased to <UNK> 𝑥 <UNK> 𝑥 <UNK> which will consist of <UNK> with constipation <UNK> and <UNK> without. some points to note each of the formulae requires estimation of some quantities, for example the sd of the measures or the percentages in each group. this may appear nonsensical. for example, if we knew the percentages of bcg vaccinated and non-vaccinated who were hospitalised (to put into the formula), we would not be doing a study to estimate the percentage difference between the groups! sample size estimation is not a precise art. it can give guidance and ball-park figures, but if all information for exact calculation were available we would not need to do the study. this does not mean that sample size calculation is not worth doing. it is important to ensure that a study is likely to have a reasonable chance of yielding useful information. it is unlikely that we would have no idea of the likely range of estimates required and these can be used to inform calculation. the size of difference to be detected, sdiff or the difference in percentages, should be informed by an understanding of the minimal clinically important difference. it is important that all estimates are clinically plausible and suitably justified, with estimation never determined with reference to the available or preferred sample size. in the above calculations, the formulae give the minimum number required and in all cases, where this is not a whole number, has been rounded up. the number given is the minimum number for statistical analyses, consideration needs to be given when designing the study to the proportion of eligible participants that will refuse to take part and the likelihood of missing data and/or loss to follow up. these factors will impact on the feasibility of attaining the necessary sample size in the timeframe available. there are some relatively minor discrepancies with the sample size estimates given in the example papers and the numbers given here, but this does not indicate errors. all calculations agree to a reasonable extent, with differences attributable to differing approximations in the formulae used. there are often other factors that should be considered. for example, if we compare heights between those with and without coeliac disease, we may wish to adjust for differences in age, sex and ethnicity of children as well as other potential confounders associated with anthropometry. it is worth noting that in this case, taking into account these covariables will only serve to reduce overall variation (sd), so estimation not taking them into account will be conservative. the formulae given are based on a single primary outcome. if there are multiple comparisons, then the sample sizes will need to be larger. as noted earlier, only the most basic calculations are given in this paper, further formulae exist for more complex scenarios. no amount of increasing the sample size can account for biases in the data. “but i’m not doing a clinical trial so this is irrelevant” it’s a common misconception that power calculations are only required for clinical trials of the randomised controlled type variety. this is untrue. wherever sampled data is used to address a research question, the sample size needs to be adequate to give a useable answer to that question. in this paper, a variety of examples are given to illustrate applicability. the headache study was a single sample descriptive study, the differences in constipated and non-constipated an observational comparison. conclusions this article should provide a large enough sample of information to enable the reader to understand that sample size calculation should always be given consideration before commencing a study with <UNK> confidence. it should be borne in mind that having an adequate sample size is only one important facet of research design. samples selected should be representative and measures of outcome both reliable and valid. no amount of increasing the sample can correct for such deficits in design and doing so will merely result in obtaining a more precise estimate of the wrong answer, or the right answer to the wrong question. when preparing a manuscript for publication this journal refers authors equator <UNK> which aim to ensure that whatever type of study is being undertaken all important statistical considerations are made. the majority of the guidelines make reference to sample size determination, including those for observational <UNK> and diagnostic testing <UNK> studies. this paper explains the basis of sample size calculation with simple examples given as manual calculations, replicable via online calculators if preferred. sample size estimation need not be overly complex nor a black-box affair. the aim of this paper is to enhance adherence to guidelines and incorporation of this important element of research in practice.

<|EndOfText|>

our one-day introduction to sample size course has run <UNK> times since <UNK> comprehensive written notes are given for use in conjunction with custom made excel sheets to perform calculations and these are openly available via a weblink. immediate feedback for the course is generally excellent. to gauge the extent to which course materials had ongoing usage, and whether post leaving the course perception of the material and its’ usefulness remained, a short survey was emailed to all <UNK> course participants from the <UNK> courses. attendees noted ongoing usage of the course materials, with over two thirds of respondents subsequently referring to these frequently or occasionally. even the most conservative estimate showed a substantial proportion still gaining direct benefit after <UNK> years. background for <UNK> years the ucl centre for applied statistics courses (casc) has presented <UNK> to <UNK> day courses primarily aimed at non-statisticians in the workplace. attendees often aim to leave a course equipped to undertake their own research, as well as having an improved understanding of published research. the one-day introduction to sample size estimation has proved highly popular and runs on average three or four times per year with an audience of up to <UNK> participants. numbers are restricted by the fact that teaching takes place within a cluster room and participants are directed to custom made excel sheets to perform calculations. the excel sheets were originally available via cds given to students but are now accessed via a web link to which there is open access (http://www.ucl.ac.uk/ich/short-courses-events/about-stats-courses/samplesize ). a folder of comprehensive course notes is given to each course participant detailing all course material and usage of the excel sheets with examples. all casc courses request feedback from students via opinio, a web-based tool used to gather ratings and comments. this feedback, collected shortly after the day of the course, is consistently positive. there remains the question of how useful the participants find the training and material on an ongoing basis. the sample size course is unique in that the excel sheets are presented and their usage integral to the estimation processes introduced to the students. furthermore, the excel sheets remain available to the students for as long as they wish to use them. therefore, the sample size course, with its very specific estimation tools as well as written notes, was ideal for beginning an investigation into the ongoing potential benefits of our training we decided to contact attendees to the sample size courses held over the last five years to evaluate whether they continued to use the excel sheets and whether the course had in fact been of direct relevance to their work. the aim of this survey was to address the research questions: i) did participants of the one day sample size course utilise the information and tools given in the course in their work life? ii) was usage related to time since attendance at the course? data collection an online survey was developed asking whether individuals had referred back to the course material and whether they had continued to use the excel spreadsheets. both of these questions had the response options: ‘yes frequently’, ‘yes, occasionally’, ‘rarely’ and ‘no’. the individuals were also asked whether what they had learnt on the sample size course had been of direct relevance and use to their work, with response options ‘yes’ or ‘no’. all questions had space for comments and there was an additional overall comment box at the end asking for any other suggestions that may help improve the one-day introduction to sample size course. potential participants were sent an email requesting that they complete the web survey (via a link), which would consist of only <UNK> questions. by making the survey short and informing of length in the accompanying email, it was hoped to maximize the response rate (galesic and bosniak, <UNK> the survey was sent via email to all <UNK> participants of the <UNK> one day courses held between <UNK> and april <UNK> there were <UNK> emails <UNK> returned undeliverable with the proportion falling over time from <UNK> <UNK> attendees at the three courses in <UNK> to only one of the <UNK> <UNK> in the two courses in the first half of <UNK> since the survey was anonymous, and we did not wish to alienate future participants by repeated emails, no additional requests were made to those who may not have responded. results from the <UNK> emails that were successfully sent, responses were received from <UNK> <UNK> most of the respondents <UNK> <UNK> completed the form in a minute or less and over <UNK> in less than <UNK> minutes <UNK> two outliers were <UNK> and <UNK> minutes. over <UNK> <UNK> replied within <UNK> hours, and all but one within eight days. response rates were not dependent on time since course attendance being between <UNK> and <UNK> for all but two years, which were <UNK> <UNK> and <UNK> <UNK> two people skipped the question about whether what they had learnt on the one-day sample size course had been of direct relevance to their work. of the <UNK> that submitted a completed survey form, the overwhelming response was ‘yes’ <UNK> <UNK> and this was consistent across years. the five who did not find the course relevant, rarely <UNK> or never <UNK> used the course material and also rarely <UNK> or never <UNK> returned to use the excel sheets. all individuals answered the usage questions. most referred back to the course material and the excel spreadsheets occasionally or frequently. there was a strong correspondence between responses from the two questions (table <UNK> two individuals occasionally referred back to the course material but did not use the excel spreadsheets, a single individual made occasional use of the spreadsheets but did not refer to the course material. table <UNK> ongoing usage of course material and excel spreadsheets responses have you returned to the excel spreadsheets and used these? yes, frequently yes, occasionally rarely no total have you referred back to the course material? yes, frequently yes, occasionally rarely no total <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> when analysed according to year of attendance, there was no consistent pattern of more usage either if the course was more recent nor with increasing time since receiving (figure <UNK> figure <UNK> temporal changes in responses to usage questions comments a total of <UNK> comments were left by <UNK> individuals. comments attached to the usage questions were most often given by those who responded ‘rarely’ or ‘no’. there were six comments following the question about usage of course material with half from individuals who cited rare usage but explained their lack of need rather than problems with the material. one person who said they never referred back to the course material commented that they disliked using excel, whereas another person who rarely used the course material stated difficulties following the concepts. the final comment was from someone who occasionally referred back to the course material <UNK> colleagues have asked me to help them with a power calculation and one colleague borrowed the course file”). there were <UNK> comments given following the question about usage of the spreadsheets, including the person who disliked excel and reiterated their previous comment. two of those commenting on this question stated that they had gone on to use stata (occasional user) or g power (never user). two individuals said they occasionally used the excel sheets and gave positive comments. the remaining three comments were very positive despite recording rare (“however, i think that this material it's very useful for working with”) or no (“i think it will come useful when i need to calculate sample size“ and “while i am very glad to have them and the process of reviewing them certainly helped understanding, the design of of (stet) research does require the calculations used the spread sheet“) of the <UNK> comments left after the relevance and usefulness in work question, eight were from individuals who answered positively and the only minor criticism was that stata may have been better to use on the course(“it was interesting but using more traditional software such as stata would have been far more helpful.”). two comments were from the five who said the course was not of direct relevance and use to their work. they said, “i typically use a repeated measure design, which was not covered in the workshop” and “it will be useful when i get to design my own study and as i have been developing a proteomics method i have not needed to use the course info”. <UNK> of the <UNK> respondents left an overall comment at the end of the survey. of these <UNK> made a comment that was purely positive with half of these specifically mentioning the usefulness of the excel sheets. there was only one negative comment, from someone who attended the course in <UNK> rarely used material or excel sheet and did not find the course relevant to their work they said they “did not learn anything as it was not explained”. however, a review of the opinio feedback from <UNK> did not yield any such negativity. three comments raised the issue of alternative software, with <UNK> of these also praising the excel sheets. one user of gpower mentioned the need to quote software used for publication, “i currently use <UNK> as it is more flexible than the spreadsheets. although the spreadsheets were useful i wonder if sometime spent on recognised software would be useful. i say this because, as part of getting published i have had to quote the software i used and the parameters i put in to generate the sample size and using <UNK> the reviewers can check my work”. five comments were from individuals who would like more advanced topics covered: these were sample size calculations for repeat measures <UNK> comments), non-inferiority trials (these are given a brief mention in the course), unspecified ‘epidemiological studies’ and multiple groups (such as <UNK> ethnic groups). despite each formula included in the course having at least one fully worked through example of usage, one participant wanted more. their comment was “more worked examples with answers would be useful when returning to the spreadsheets to be sure of using the correct one and inputting the correct figures”. the final comment felt the course might be shorter, but seemed generally positive, “was good, could be shorter, or just a half-day for more advanced users of excel/stats -overall it was very useful, if anything, it highlighted an often neglected issue in science ”. discussion a substantial proportion of those responding to the survey continued to refer to the course material and/or to use the online excel spreadsheets for calculation. some of those that did not currently use the material still cited the potential usefulness and intended to do so. almost all felt that the course had been of direct relevance and useful to their work. there was no evidence of these findings being related to time since course attendance. strengths and limitations the response rate was low at <UNK> but this is to be expected with a one shot survey of this type. online survey response rates below <UNK> are not uncommon and rates are declining due to the volume that individuals currently receive (van mol, <UNK> those responding may have been biased towards those who did engage with the material. conversely though they may have been biased towards those who found the course not very helpful. however, low response does not necessarily infer bias (fosnacht et al, <UNK> we had access to email addresses of course attendees over the last five years. all presentations of the course were led by the same teacher and the content of the course remained similar throughout. we were asking about current usage and so we could evaluate the long term effect of the course. the numbers of non-deliverable emails declined over time as would be expected, but the response rates within those whose emails which were delivered did not show a temporal pattern. if we consider the numbers responding positively from those where an email was delivered (ie. assume a worse-case scenario that those who did not respond rarely or never used the materials or spreadsheets) there remains <UNK> <UNK> ci <UNK> <UNK> of those who attended the course five years ago still gaining direct benefit occasionally or frequently from the course materials and the excel sheets. we can consider this a very conservative lower limit. conclusion this study gives evidence of a long term benefit of attendance at the one day sample size course, with over two thirds of respondents who had attended the course stating that they subsequently referred to the course materials frequently or occasionally. even the most conservative estimate shows a substantial proportion of attendees continuing to use the course material directly in their work after <UNK> years.

<|EndOfText|>

summary cross-sectional covariate-related reference ranges are widely used in clinical medicine to put individual observations in the context of population values. usually, such reference ranges are created from data sets of independent observations. if multiple measurements per individual are available, then ignoring the within-person correlation between repeats will lead to overestimation of centile precision. furthermore, if abnormal measurements have triggered more frequent assessment, the data set will be biased thus producing biased centiles. where multiple measures per individual exist, the methods commonly used are either randomly or systematically to select one observation per individual or to model individual trajectories and combine these. the first of these approaches may result in discarding a large proportion of the available data and may itself cause bias and the latter requires the form of the changes within individuals to be characterized. we have developed an approach to the modeling of the median, spread, and skew across individuals using maximum likelihood, which can incorporate correlations between dependent observations. heavily biased data sets are simulated to illustrate how the methodology can eliminate the biases inherent in the data collection process and produce valid centiles plus estimates of the within-person correlations. the “select one per individual” approach is shown to be liable to bias and to produce less precise centiles.we recommend that the maximum likelihood method incorporating correlations be used with existing data sets. furthermore, this is a potentially more efficient approach to be considered when planning the future collection of data solely for the purposes of creating cross-sectional covariate-related reference ranges. keywords: age-related reference ranges; correlated measurements; dependence; serial measures; unbiased; z-scores. <UNK> introduction covariate-adjusted reference ranges may be used to assess individuals at a single point in time (crosssectional) or to monitor changes within individuals over time (velocity or conditional). most commonly, the covariate used is age. if an individual presents for diagnosis/assessment and repeat measurements are available, then it will generally be advisable to utilize all of these. however, there are many occasions on which only a single measurement is available and this needs to be evaluated against population values using a covariate-adjusted cross-sectional reference range. the methodologies for constructing population-based covariate-adjusted cross-sectional reference centiles are now well established and were recently reviewed by the world health organization (borghi and others, <UNK> for the world health organization [who] multicentre growth reference study group). these methodologies commonly assume that the measurements used for construction are independent. if the data set contains serial measurements from individuals, then these will be correlated within person and hence the independence assumption is not satisfied. one approach has been to circumvent the problem by systematically or randomly selecting one observation per individual to create a data set of independent measurements (e.g. kurmanavicius, wright, royston, wisser and others, <UNK> kurmanavicius, wright, royston, zimmermann and others, <UNK> wade and ades, <UNK> however, this is wasteful of the data and may lead to bias. this paper is concerned specifically with the construction of cross-sectional reference ranges using serial measurements from individuals. the need for any marginal analyses to include assumptions about the form of the correlation has been well documented within other applications (diggle and others, <UNK> the laird–ware model <UNK> gives a general framework for modeling which allows for variable spacing of observations and varying structures between individuals. the estimation of parameters for this model form has received much coverage (davidian and giltinan, <UNK> hand and crowder, <UNK> vonesh and chinchilli, <UNK> lindsey, <UNK> diggle and others, <UNK> however, very little of the available literature applies specifically to the reference range problem. within the majority of texts, characterization of the average pattern is of primary importance followed by estimation of the covariance/correlation structure between repeats within individuals where these exist. when reference ranges are to be constructed, the estimation of any skewness and the spread of values at each covariate are at least as important as quantification of the median. this shift of emphasis is necessary as it is usually the extreme centiles that are of most clinical use. by contrast, estimation of the covariance/correlation structure is generally of little or no direct interest in this scenario. the who review recommended the use of methodologies that model the covariate-related changes in distributional features and then combine these to obtain centiles. commonly, the underlying distribution is assumed to be some transformation of the normal distribution and the kurtosis, skew, spread, and median are modeled. the form of the models used for the distributional features and the mode of identifying the best fit parameters vary according to the specific method chosen (cole, <UNK> cole and green, <UNK> wright and royston, <UNK> previously, we used this methodology within a maximum likelihood framework with exponential models to create age-related centiles for <UNK> counts (wade and ades, <UNK> randomly selecting one measurement per child. we subsequently extended the approach by incorporating suitable correlation structure into the likelihood and thus additionally modeled the correlation between repeats from the same individual as smooth functions of age and time (wade and ades, <UNK> hence utilizing the entire data set. this extension may be viewed as a generalization of models developed to identify trends in longitudinal data with explicit modeling of the serial correlation (diggle, <UNK> despite a strong correlation structure, incorporation did not substantially alter the fitted centiles. this finding was not unexpected because measurements were made at ages defined within a strict protocol, and hence, frequency of measuring was independent of previous measurements. a similar approach that has been used is systematically, as opposed to randomly, to select one measurement per individual. for example, kurmanavicius, wright, royston, wisser and others <UNK> and kurmanavicius, wright, royston, zimmermann and others <UNK> used only the first of serial measurements made during pregnancy to create cross-sectional centiles for fetal biometry. while it is generally appreciated that incorporation of dependent observations without adjustment for correlations will lead to overestimation of centile precision, the propensity for bias invalidating the centiles has received little discussion. when the number and/or timing of observations are related to outcome, for example, when an abnormal measurement is likely to trigger more frequent assessment for clinical purposes, then the incorporation of correlations may have a large impact on the centiles by reducing or removing the bias inherent in the collection process. the problem is one of informative observation times, whereby future measurement frequency is related to the values of existing measurements for that individual (lin and others, <UNK> the who review (borghi and others, <UNK> identified the following <UNK> approaches that incorporated correlated measurements into the construction of cross-sectional reference ranges. laird and ware <UNK> proposed <UNK> random-effects models, while goldstein <UNK> proposed a more general framework of multilevel models which could be parameterized to allow for complex covariance structures and multiple explanatory variables. marginal distributions obtained from these models would identify cross-sectional patterns of change (pan and goldstein, <UNK> while these models are flexible and present a solution to the specific problem posed here, they require explicit characterization of a common underlying form for expected trajectories within individuals. goldstein and others <UNK> recognized that the methodology for conditional (longitudinal) references can theoretically yield cross-sectional references. the second approach cited by the who review was our previously described maximum likelihood method (wade and ades, <UNK> requiring characterization only of population changes irrespective of how individual trajectories vary. in this paper, we illustrate how biases may be removed and precision increased via appropriate modeling even for heavily biased simulated data sets. we compare the precision with which centiles are estimated when correlations are incorporated versus the alternative systematic or random “select one” approach. the methodology is illustrated by application to serial fetal ultrasound measurements collected at the university hospital in zurich (uhz) and previously modeled using only a subset of the data (kurmanavicius, wright, royston, wisser and others, <UNK> kurmanavicius, wright, royston, zimmermann and others, <UNK> <UNK> methods <UNK> statistical methods in previous papers, we have demonstrated the use of splines, fractional polynomials, and exponentials within the maximum likelihood methodology. any data collection protocol can be accommodated, as can any amount of variation between the number and timing of measurements per individual. formal significance tests are easily performed between nested models and confidence intervals constructed for the model parameters and/or the centiles (wade and ades, <UNK> <UNK> thompson and fatti <UNK> extended the methodology to create multivariate centile charts. in the analyses presented in this paper, we assume that a transformation of the normal distribution is appropriate at each covariate value and we model the changes in the skewness, spread, and median. we maximize the likelihood incorporating a correlation structure between repeated measurements within the same individual. we used fortran programs incorporating numerical algorithms group subroutines, which are available within the supplementary material, available at biostatistics online. an alternative would be to use generalized additive models for location, scale, and shape (gamlss; rigby and stasinopoulis, <UNK> the gamlss command in r can be used to fit centiles with incorporation of random effects for individuals to account for serial correlation. <UNK> simulations full details and results from the simulations are given in the supplementary material, available at biostatistics online. the features and findings were as follows. underlying models were assumed so that median and spread were both increasing with gestational age, as this would be typical in most applications. simulations were based around a scenario often encountered during pregnancy where measurements are made between <UNK> and <UNK> weeks and abnormally low values trigger additional repeated measurements. repeated measurements within an individual were generated with an exponentially decaying correlation function. the extent of bias in the data sets was dependent on how the frequency of repeat measurements was determined. fitted centiles based on an assumption of independence were heavily biased. the extent of the correlations between repeats was typically underestimated within the correlation models, although the centiles obtained were not biased. both precision and accuracy were improved for the correlation model compared to that assuming independence. with “select one”, the precision of the centile estimates was reduced and the centile estimates were biased. this latter finding shows that selecting a subset of independent measurements does not necessarily yield unbiased centiles. at later gestations, there were more measurements from those fetuses previously with abnormally low values and hence there was more chance of selecting biased assessments in this gestational age range. <UNK> application to ultrasound data set <UNK> the data set ultrasound measurements were taken from clinic records of pregnant women examined at the uhz, where routine examinations were performed at <UNK> <UNK> and <UNK> weeks of gestation. high-risk pregnancies were examined at shorter intervals, every <UNK> or <UNK> weeks until delivery. referrals at later gestations from other ultrasound centers were also included. a relatively common reason for such referral would be suspected intrauterine growth retardation (iugr) due to placental insufficiency which manifests after <UNK> weeks of gestation with reduced growth of the fetal abdomen. such women then undergo repeat tests until a definitive diagnosis is made. small values of abdominal circumference (ac) indicate potential iugr, whereas biparietal diameter (bpd), a measure of skull size, is not expected to be affected by iugr. the only measurements excluded from the data set were for fetuses found to have a congenital abnormality. the original analysis used the first fetal measurements made between <UNK> and <UNK> weeks from <UNK> pregnant women <UNK> bpds and <UNK> acs). fractional polynomials were used to model age-related changes in the mean and standard deviation, and shapiro–francia w test was used to check the normality of the z-scores. for these <UNK> fetal measurements, a linear cubic in age for the mean and linear model for the standard deviation were found to be suitable (kurmanavicius, wright, royston, wisser and others, <UNK> kurmanavicius, wright, royston, zimmermann and others, <UNK> the current data set, which has expanded since its use in <UNK> consists of information from <UNK> <UNK> women measured between <UNK> and <UNK> times. a total of <UNK> <UNK> bpds and <UNK> <UNK> acs are included. hence, any select one approach would utilize only about <UNK> of the available measurements. since the purpose was to illustrate the effects of this modeling, we used the same model forms as kurmanavicius and others had previously (linear-cubic model for the mean, linear model for the standard deviation, and no skew) and estimated only their parameters. we allowed the correlation between repeats from the same individual to fall as the time between those repeats increased and characterized this as a <UNK> exponential model <UNK> hence, we estimated <UNK> parameters for the independence and select one models <UNK> for the mean and <UNK> for the standard deviation) and an additional <UNK> (for the correlation structure) for the correlation incorporated models. we compare the fitted centiles with those previously presented by kurmanavicius, wright, royston, wisser and others <UNK> and kurmanavicius, wright, royston, zimmermann and others <UNK> <UNK> fitted models figures <UNK> and (b) show the <UNK> <UNK> and <UNK> centiles from the independence models, the exponentially correlated models, and those previously presented by kurmanavicius, wright, royston, wisser and others <UNK> and kurmanavicius, wright, royston, zimmermann and others <UNK> for both ac and bpd, taking into account the correlations reduces the centile range at earlier gestations. this pattern is compatible with a greater frequency of measurement of fetuses with extreme values. the centiles fitted by kurmanavicius, wright, royston, wisser and others <UNK> and kurmanavicius, wright, royston, zimmermann and others <UNK> lie between the correlation and independence models at early gestations but become increasingly like the independence centiles at later ages. the effect of incorporating correlations on the centiles at later gestations differs between ac and bpd. the increased ac <UNK> centile beyond <UNK> weeks of gestation may be explained by the inclusion of late referrals to uhz for suspected iugr. by contrast, bpd is a skull measurement, large values of which may be of greater concern near to term <UNK> weeks of gestation). the pattern of differences shown in figure <UNK> suggests that the fetuses with larger bpd were more likely to be measured more frequently in the last <UNK> weeks of pregnancy. incorporation of correlations reduces the effect of these larger measurements, and the centiles based on the correlation model are lower. figures <UNK> and (b) show how incorporation of correlations can remove the selection bias inherent in clinical data sets. the patterns observed were not anticipated but with hindsight have clinically valid fig. <UNK> estimated <UNK> <UNK> and <UNK> centiles. the solid lines show the estimated centiles when all measures are assumed independent and the thick dashed lines when correlations between repeats are incorporated. the lighter dashed lines show the fitted centiles as presented by kurmanavicius, wright, royston, wisser and others <UNK> and kurmanavicius, wright, royston, zimmermann and others <UNK> (a) ac obtained using <UNK> <UNK> ultrasound measurements from <UNK> <UNK> pregnancies. (b) bpd obtained using <UNK> <UNK> ultrasound measurements from <UNK> <UNK> pregnancies. explanations. the results show that the adjustments for correlation will not be uniform in either direction or quantity even for seemingly highly related measurements, that is, different assessments of growth from the same ultrasounds in the same group of women. it is perhaps surprising that the centiles based on the first measurement from each fetus (kurmanavicius, wright, royston, wisser and others, <UNK> kurmanavicius, wright, royston, zimmermann and others, <UNK> were more akin to those obtained from the data set of all measurements. however, this is compatible with the finding of the simulation study. selecting an independent subset does not necessarily remove bias as late referrals to uhz are probably atypical. <UNK> discussion our simulations and application demonstrate that incorporation of a correlation structure within the fitting algorithm is to be preferred to the select one approach. although select one is computationally simpler, our analysis shows that the precision and accuracy of the centiles may be severely affected. the simulations were developed to represent typical clinical scenarios. the irremovable bias obtained was not expected, although easily explained with hindsight. it is important to note that such biases may occur in any data set and will yield biased centiles if select one is used. simulations are necessarily limited by choice of the parameters and assumptions incorporated. in particular, we assumed very similar correlation structures within the data generation and fitting phases.we do know that if the correlation structure is severely misspecified, then this will lead to invalid estimation. the comparison with the independence model of zero correlation between repeats within individuals is an extreme case and clearly illustrates this point. there must therefore be some degree of misspecification that can lead to invalidation of the centiles. for all cases where we modeled the correlation structure, we assumed exponential decline with increasing time, a reasonable assumption for clinical applications. the extent of any decline was estimated, and estimates were often biased. however, the model did allow for any extent of decline (including zero), and the simulations illustrate that biased parameter estimates do not necessarily lead to biased centile estimates. when constructing cross-sectional reference ranges from correlated data, estimation of the correlation structure is not of direct interest, but rather is a means toward the important end of obtaining unbiased estimates of centiles. the data set we describe consists of longitudinal measurements with informative observation times. techniques that specifically model the timing mechanism could be employed and would give additional information. however, this would necessitate the specification of the conditional distribution of an observation given the history of the process and the centile estimates may not be robust to misspecification (lin and others, <UNK> for the purposes of creating cross-sectional covariate-related reference ranges, the biases in the measurement process that we wish to eliminate are a function of the clinical scenario. our simulations show that the method we present is capable of producing unbiased centiles from messy data sets of the form likely to be found in clinical practice. the extent and direction of centile adjustment when correlations are incorporated may yield information about the nature of biases inherent in the data set. despite the numerous advantages of incorporating some form of correlation, the technique has not been widely used, perhaps because of a mistaken belief that the added complexity of modeling is not warranted. in this paper, we have shown that simpler methods applied to data with informative observation times lead to invalid centile estimation. the process of selecting the first observation per individual, as previously used by kurmanavicius, wright, royston, wisser and others <UNK> and kurmanavicius, wright, royston, zimmermann and others <UNK> may lead to a biased solution and necessitates discarding a large proportion of the data. the resulting reduction in precision will be a function of the percentage of data discarded and the extent of the within-individual correlation. the reduction may not be uniform across the age range since selecting the first measurement from each pregnancy will lead to greater loss of precision at later gestations where there will be fewer women presenting. the uhz received referrals for suspected problems, based on abnormal ultrasound measurements, from other hospitals. while fetuses subsequently found to have a congenital abnormality were excluded, biases are still likely to remain. the simulation results show that incorporating correlation structure into the modeling has the capacity to reduce biases such as these in estimating centiles. the extent of the bias reduction will depend on the degree to which the correlation structure has been adequately modeled. if data are to be collected specifically for the purposes of creating cross-sectional reference ranges, then the most precise centiles for a given total number of observations will be obtained when these observations are independent. however, there may be a trade-off between the recruitment cost per individual and the cost of following individuals serially (goldstein, <UNK> if subsequent measurements for a recruited individual are easier and/or cheaper to obtain than measurements from new recruits, then some consideration should be given at the design stage to the most efficient way to proceed. it may be ethically easier to justify serial collection from a smaller pool of prospective subjects, or the pool may be limited (e.g. children born to <UNK> mothers). often, the remit is to produce both cross-sectional and conditional or velocity references (borghi and others, <UNK> in this case, the optimal approach will be to incorporate serial observations into the cross-sectional references using appropriate adjustment for correlation within individuals. the precision with which centiles are estimated under differing correlation structures, model forms, and/or sample sizes can be readily compared using simulations to identify the most appropriate recruitment method to use.