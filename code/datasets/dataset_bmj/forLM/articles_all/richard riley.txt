individual participant data meta-analysis of continuous
outcomes: a comparison of approaches for specifying
and estimating one-stage models

one-stage individual participant data meta-analysis models should account for
within-trial clustering, but it is currently debated how to do this. for continuous outcomes modeled using a linear regression framework, two competing approaches are a stratified intercept or a random intercept. the stratified
approach involves estimating a separate intercept term for each trial, whereas
the random intercept approach assumes that trial intercepts are drawn from
a normal distribution. here, through an extensive simulation study for continuous outcomes, we evaluate the impact of using the stratified and random
intercept approaches on statistical properties of the summary treatment effect
estimate. further aims are to compare (i) competing estimation options for the
one-stage models, including maximum likelihood and restricted maximum likelihood, and (ii) competing options for deriving confidence intervals (ci) for
the summary treatment effect, including the standard normal-based 95% ci,
and more conservative approaches of kenward-roger and satterthwaite, which
inflate cis to account for uncertainty in variance estimates. the findings reveal
that, for an individual participant data meta-analysis of randomized trials with a
1:1 treatment:control allocation ratio and heterogeneity in the treatment effect,
(i) bias and coverage of the summary treatment effect estimate are very similar when using stratified or random intercept models with restricted maximum
likelihood, and thus either approach could be taken in practice, (ii) cis are generally best derived using either a kenward-roger or satterthwaite correction,
although occasionally overly conservative, and (iii) if maximum likelihood is
required, a random intercept performs better than a stratified intercept model.
an illustrative example is provided.
keywords
continuous outcomes, estimation, individual participant data, ipd, meta-analysis

1 introduction
individual participant data (ipd) meta-analysis involves obtaining and then synthesizing raw individual-level data from
multiple related studies, to produce summary results that inform clinical decision making.1 the ipd approach is increasingly popular and has many potential advantages over a traditional meta-analysis of published aggregate data, such as
increased power to detect treatment-covariate interactions and avoiding reliance on published results.2
statistical methods to perform an ipd meta-analysis involve either a one-stage or two-stage approach.3 generally, these
approaches give very similar meta-analysis results, especially when they use the same modeling assumptions and/or
estimation methods.4,5 however, the one-stage approach has become increasingly popular over the past decade.6 it conveniently allows all studies to be analyzed simultaneously and avoids the assumption of normally distributed study effect
estimates with known variances that is usually made in the second stage of the two-stage approach. it also allows greater
flexibility of parameter specification over the two-stage approach.6
when conducting a one-stage ipd meta-analysis, it is important to account for clustering of participants within studies,
to correctly condition an individual's response to the study they are in. ignoring clustering and analyzing ipd as if coming
from a single study can result in misleading conclusions. for example, abo-zaid et al7 showed that family history of
thrombophilia was statistically significant as a diagnostic marker of deep vein thrombosis when clustering was accounted
for (odds ratio = 1.30; 95% confidence interval (ci): 1.00, 1.70; p value = 0.05) but not when clustering was ignored (odds
ratio = 1.06; 95% ci: 0.83, 1.37; p value = 0.64). while it is well established that clustering should be accounted for, it is
debatable exactly how this should be done. in particular, there are two competing approaches to account for clustering in
a one-stage model: a stratified intercept or a random intercept. the stratified approach involves a separate intercept term
being estimated for each study; thus, if there are 10 studies, 10 intercept terms would be estimated (one for each study). in
the random intercept approach, the intercepts are assumed to be drawn from some distribution, typically normal with an
underlying mean value and variance. the advantage of the stratified intercept approach is that it makes no assumptions
about the distribution of intercepts across studies. in contrast, the advantage of the random intercept approach is that it
requires fewer parameters to be estimated.
in this article, we evaluate through an extensive simulation study the impact of using either the stratified or random
intercept approach on the statistical properties of the summary treatment effect estimate (for example, in terms of bias,
precision, mean square error (mse), and coverage). this is considered in the context of randomized trials with a continuous outcome and a 1:1 treatment:control allocation ratio, assuming either common or random treatment effects across
trials. two further aims are to (i) compare competing estimation options for the one-stage models, including maximum
likelihood (ml) and restricted maximum likelihood (reml) and (ii) compare competing options for deriving confidence intervals for the summary treatment effect, including the standard normal-based 95% ci, and (for reml, but not
ml estimation) the kenward-roger (kr)8 and satterthwaite9 corrections that inflate confidence intervals to account for
uncertainty in variance estimates.
this paper is structured as follows. in section 2, we introduce the two competing one-stage ipd meta-analysis models of
interest that account for clustering, as well as the competing estimation and ci derivation options. in section 3, we outline
how the simulation study was conducted and present the results, and in section 4, we provide a real example to illustrate
the methods considered. finally, in section 5, we conclude with a discussion of the key findings and limitations and offer
a recommendation for those conducting one-stage ipd meta-analysis of randomized trials with 1:1 treatment:control
allocation ratio and with a continuous outcome.
2 introducing different model specification and
estimation options
consider that ipd have been obtained from i = 1 to k related randomized trials, each investigating a treatment effect
based on a continuous outcome y (say, blood pressure); that is, the mean difference in outcome value between a treatment
and a control group. suppose that there are ni participants in trial i. let yfi j be the end-of-trial (f used to denote final)
continuous outcome value, for participant j in trial i, and ybi j (b to denote baseline) be the pre-treatment outcome value.
let treati j take the value 1 or 0 for participants in the treatment or control group, respectively.
given such ipd, there are several ways in which researchers can use a one-stage meta-analysis to model the summary
treatment effect across trials. we focus initially on presenting one-stage analysis of covariance (ancova) mixed models,
which either use a stratified intercept or a random intercept to account for clustering of participants within trials. we also
assume a random treatment effect since heterogeneity is usually expected.
2.1 model (1): stratified intercept
with the following approach, a stratified intercept is used to account for within-trial clustering.
y𝐹 𝑖𝑗 = 𝛽i + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + ui) treat𝑖𝑗 + e𝑖𝑗 (1)
ui ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
here, 𝛽i denotes the intercept term for trial i (expected final outcome value for participants in the control group in trial i
who have the mean baseline outcome value), and the distinct intercept for each trial is used to account for within trial
clustering. the term 𝜆i denotes a trial-specific adjustment term for the baseline outcome value (here, centered at the
mean for each trial (y𝐵𝑖) to aid interpretation of the trial-specific intercepts). for example, when there are k = 10 trials,
there would be 10 𝛽i terms and 10 𝜆i terms. of main interest is an estimate of the model parameter 𝜃, as this denotes
the summary (average) treatment effect. the random effect, ui, indicates that the true treatment effects in each trial are
assumed to arise from a distribution of true effects with mean 𝜃 and between-trial variance 𝜏2. this assumption could be
constrained if considered appropriate, with a common (fixed) treatment effect (ie, constrain 𝜏2 = 0). lastly, 𝜎2
i denotes a
distinct residual variance per trial.
the flexibility of the one-stage ipd approach allows us to make further modifications by considering, for example, a
common baseline adjustment term (ie, 𝜆i= 𝜆) across trials, or common residual variances (ie, 𝜎2
i = 𝜎2) if necessary5,10,11;
however, this should be justified (eg, based on computational reasons or estimation problems), and sensitivity analysis to
the choice of assumptions is often sensible.
2.2 model (2): random intercept
when there are a large number of trials to be synthesized, a stratified intercept approach to clustering can be computationally intensive (as equation (1) requires estimation of 3 k + 2 parameters).4 an alternative approach for dealing with
clustering, which is preferred by some researchers,12 is to use a random intercept term.
y𝐹 𝑖𝑗 = (𝛽 + u1i) + 𝜆i
(
y𝐵𝑖𝑗 − y𝐵𝑖)
+ (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗 (2)
u1i ∼ n
(
0, 𝜏2
𝛽
)
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n (
0, 𝜎2
i
)
parameters are as in equation (1), except that within-trial clustering has now been accounted for by a random (instead
of stratified) intercept term, with 𝜏2
𝛽 denoting the between trial variance in the intercept about the mean intercept (𝛽).
equation (2) assumes independence of the two random effects (ie, a covariance of zero), but their correlation could be
accounted for assuming a bivariate random effect distribution; indeed, this might be of special interest when evaluating
the relationship across trials of mean baseline in the control group and true treatment effect.13
compared to equation (1), the number of parameters to be estimated has been reduced, with only 𝛽 and 𝜏𝛽 for the
intercept, instead of k separate terms. therefore, fewer estimation problems might be anticipated than in equation (1). on
the downside, equation (2) makes a strong and potentially unnecessary assumption that control group means are drawn
from a normal distribution with a common mean and variance. furthermore, the estimation of an additional random
effect term might increase computational intensity.
2.3 options for estimation and ci derivation
the parameters in models (1) and (2) are typically estimated using either a ml or reml approach. ml is known to
produce downwardly biased estimates of between trial variance when there are few trials,14-16 whereas reml addresses
the downward bias and is thus generally preferred.17,18
in addition to competing options for model parameter estimation, there are also competing options to subsequently derive (1 − 𝛼)100% cis for the true summary treatment effect (𝜃). standard cis are based on large-sample
inference and assume 𝜃
̂ is approximately normally distributed:
𝜃
̂± z1−𝛼
2
√
var(𝜃
̂), (3)
where 𝜃
̂is the estimate of 𝜃, var(𝜃
̂) is its variance, and z1−𝛼
2
is the upper 1− 𝛼
2 quantile of the standard normal distribution.
this standard approach may produce cis that are too narrow, as var(𝜃
̂) does not account for the uncertainty in the estimate
of the between trial variation of 𝜃
̂.
4,18
to address this, more conservative options are available based on small-sample inference, which define the uncertainty
around 𝜃
̂ using approximations based on a t-distribution, such as the kr8 and satterthwaite9 corrections, which are also
known as denominator-degrees-of-freedom adjustments.
the kr corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
varkr(𝜃
̂), (4)
where 𝜃
̂ is as before, but now a bias-adjusted (inflated) variance (varkr(𝜃
̂)) is used, and t𝜐;1−𝛼
2
(the upper 1−𝛼
2 quantile of
the t-distribution with an adjusted degrees of freedom, 𝜐) instead of z1−𝛼
2
.
for a single parameter of interest (as in our case), the satterthwaite corrected (1 − 𝛼)100% ci is given by
𝜃
̂± t𝜐;1−𝛼
2
√
var(𝜃
̂), (5)
where t𝜐;1−𝛼
2
is as in the kr correction, but the original (unadjusted) variance of 𝜃
̂is used. note that, while the denominator
degrees of freedom calculated from the kr and satterthwaite corrections are the same for single hypothesis tests, the kr
correction uses a bias-adjusted variance; therefore, cis derived using equations (4) and (5) will potentially differ, with the
one using the kr correction (equation (4)) leading to slightly wider intervals.19
although schaalje et al20 recommend kr over satterthwaite in special cases when the sampling distribution of the
test statistic is known, there remains debate over the best method, and a lack of literature in this area in regard to ipd
meta-analysis for estimation of a parameter of interest.
3 simulation study
we now perform a simulation study to examine the statistical performance of the summary treatment effect estimate (𝜃
̂)
from a one-stage ipd meta-analysis across a range of scenarios. our aim is to assess the different model specifications,
parameter estimation methods and ci derivation options described in section 2. that is, we compare the following: stratified or random intercept specifications; ml or reml estimation options; and, for reml estimation, 95% cis based on
asymptotic formula (equation (3)) or with either kr or satterthwaite corrections (equations (4) and (5), respectively)).
3.1 methods
provided is a step-by-step guide to our simulation study. for simplicity, and to considerably speed up the many simulations, we removed the baseline adjustment term in models (1) and (2), such that it does not exist in any of the data
generating mechanisms or models fitted in our simulations. in other words, we generate data without baseline imbalances
and thus analyze the data according to a final score ipd meta-analysis model, which is appropriate in this situation.
21 for
similar reasons of simplicity and computational complexity, we assumed a common residual variance across trials (both
in data generation and models fitted). extension to different residual variances is considered in our discussion (section 5).
to inform the true parameter values for the simulation, we used a previous ipd meta-analysis of treatment for lower
blood pressure outcomes.22
all analyses were conducted using stata v.14.2 (stata corporation, tx, usa).23
3.1.1 scenario 1 (base case)
the simulation process is now explained, in the context of an initial base case scenario with ipd from 10 trials and a
relatively simple data generating mechanism. extensions to other more complex scenarios are described afterwards.
step 1: data generating mechanism for one ipd meta-analysis of 10 trials
consider that an ipd meta-analysis of i = 1 to k related trials is of interest, with the goal to summarize a treatment
effect on a continuous outcome. to generate such data for the base case of this simulation study, we started by setting the
number of trials, k, to 10. we set a fixed number of participants, n = 100 in each trial, and assumed a fixed randomization
of 1:1 in each trial; that is, on average, 50% of participants within any given trial are allocated to a treatment group, and
the remaining 50% to a control group. this gave us a triali (trial 1/0 indicator) and treatij (treatment group 1/0 indicator)
value for each of 100 participants in each of 10 trials.
next, based on the previous meta-analysis,22 we set the true parameter values for this simulation to be as follows:
𝜃 = −9.66 (summary treatment effect; negative value favors treatment group), 𝜏2 = 7.79 (between trial variation in the
treatment effect), 𝛽 = 159.73 (mean blood pressure response in control group), 𝜏2
𝛽 = 233.99 (between trial variation in the
intercept), and 𝜎2 = 333.74 (residual variance).
we then used these parameter values to generate further terms, beginning with using 𝜎2 to generate an error term ei j,
for the jth participant from the ith trial
e𝑖𝑗 ∼ n(0, 𝜎2
). (6)
then, we generated the trial level values for the random parts of the intercept and treatment effect terms, u1i and u2i,
respectively,
u1i ∼ n
(
0, 𝜏2
𝛽
)
(7)
u2i ∼ n(0, 𝜏2
).
finally, with all the parameters defined (𝛽, u1i, 𝜃, u2i, treati j, and ei j), we generated the end-of-trial continuous outcome
value yfi j, under the random intercept model (2) (with no baseline adjustment term and assuming a common residual
variance)
y𝐹 𝑖𝑗 = (𝛽 + u1i) + (𝜃 + u2i) treat𝑖𝑗 + e𝑖𝑗. (8)
this gave one complete ipd meta-analysis dataset of 1000 total participants, containing 100 participants in each of
10 trials, consisting of the following data for each individual: a trial indicator (triali), a treatment group indicator (treati j),
and an end-of-trial continuous outcome value (yfi j).
step 2: model fit and replication
using the generated data, we fitted a stratified intercept model (1) and a random intercept model (2) (without the
baseline adjustment term and assuming a common residual variance) separately to this simulated ipd, under all the combinations of estimation and ci derivation methods outlined in section 2. figure 1 provides a flow diagram summarizing
the possible combinations. each time a model was fitted (under a particular combination of estimation and ci derivation
methods), we stored the following: the summary treatment effect estimate, 𝜃
̂; its corresponding 95% ci; a binary indicator variable for coverage of 𝜃
̂ (ie, the value 1 if the 95% ci of 𝜃
̂ contained the true 𝜃, and 0 otherwise); estimates of any
variance parameters; model run time (from start of model fit to end of post estimation); and model convergence (1/0 for
convergence within 100 iterations/nonconvergence, respectively).
standard
ci
stratified
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
intercept
option
estimation
method
ci method standard
ci
random
intercept model
ml reml
standard
ci
kr
ci
satterthwaite
ci
figure 1 flow diagram of possible combinations of intercept option, estimation, and ci methods. ci, confidence interval;
kr, kenward-roger correction; ml, maximum likelihood estimation; reml, restricted maximum likelihood
for each model (stratified or random intercept) fitted to the data, this enabled us to obtain two estimates of 𝜃
(one each for the models fitted using ml and reml estimation, respectively) and four 95% cis for 𝜃
̂ (one for ml estimation with a standard ci derivation, and then one each for reml estimation with the standard, kr-corrected, and
satterthwaite-corrected ci derivations).
step 3: simulation replications
steps 1 and 2 were repeated until 1000 ipd meta-analysis datasets had been generated using the true parameter values
and procedure as outlined thus far, followed by application of the various intercept option, estimation, and ci methods to
each of the 1000 replicated datasets (note: 1000 simulations were chosen to give a monte carlo error of 0.7% on a coverage
of 95%).
step 4: summarizing performance
using the results obtained after step 3, the statistical properties of 𝜃
̂ under the different model specification and estimation options were assessed by summarizing the 1000 results obtained using the following metrics: mean percentage
(%) bias, empirical standard error (se), mse, coverage (separately for each ci method), convergence, and mean run time
(separately for each ci method). additionally, we considered the median percentage bias in the heterogeneity (𝜏2) of the
true treatment effects also. definitions of these performance measures are provided in web appendix a.
3.1.2 extended set of 38 scenarios changing number of trials, participants, between-trial
distributions, and data generating mechanisms
the base case scenario defined in section 3.1.1 was extended to further settings, leading to an extensive range of
38 scenarios in total (see table 1), which we now summarize.
we varied the number of trials (scenarios a1 and a2), so that k = 5, 10, and 20 were considered, which cover the typical
sizes of ipd meta-analyses in our experience. we also considered trials with differing sample sizes within an ipd, so that
ni (number of participants within trial i) was drawn from a uniform distribution, ni∼u(a, b). fixing a = 30, b = 1000
(scenario b1) allowed for mixed sample sizes, and having 5 trials with a = 30, b = 100 and 5 with a = 900, b = 1000 within
an ipd (scenario b2) tested the effect of a mix of small and large sample sizes only. lastly, fixing a = 30, b = 100 tested
the effect of having only small trials (scenario b3).
we also tested the combined effect of varying the number of trials and number of participants per trial simultaneously
(scenarios b1-a1, b1-a2, b2-a1, b2-a2), and we tested the effects of adjusting the magnitude of the intercept or treatment
effect heterogeneity (scenarios c1, c2, d1, d2).
scenarios 15 to 38 replicate the first 14 scenarios where possible, for modifications to the base case data generating
mechanism. first, to test the robustness of the normality of the intercept assumption in the random intercept model, we
altered the final step of the data generating mechanism in equation (8), so that the final outcome was calculated by
y𝐹 𝑖𝑗 = 𝛽i + (𝜃 + u2i)treat𝑖𝑗 + e𝑖𝑗 (9)
𝛽i ∼ (beta (15, 3)) × 220
u2i ∼ n(0, 𝜏2
)
e𝑖𝑗 ∼ n(0, 𝜎2
).
therefore, the intercept term 𝛽i was now derived from a beta distribution with shape parameters of 15 and 3, which
represent a negatively skewed distribution that was then scaled by 220 to give sensible values for systolic blood pressure
(the outcome upon which the hypothetical data is based). an example density plot of this beta distribution for modeling
the intercept term is shown in web figure a.1.
secondly, we also considered a data generating mechanism with a common (fixed) treatment effect (ie, 𝜏2 = 0). here,
the fitted stratified and random intercept models were also modified to have a common treatment effect.
3.2 results
simulation results are shown in tables 2 and 3, covering most of the scenarios under the normal and beta distribution
intercept data generating mechanisms, across all options for specifying and estimating the intercept. these tables show the
mean percentage bias of the summary treatment effect estimate (𝜃
̂) (table 2) and the median percentage bias in its heterogeneity (𝜏̂2) (table 3). figure 2 graphically depicts the percentage coverage of the summary treatment effect estimate (𝜃
̂).
.
table 1 summary of the different simulation scenarios*
scenario data generation details modification from base case scenario
base case (i) number of trials, k = 10 -
(ii) number of participants in trial i, ni = 100 (fixed across all trials)
(iii) fixed treatment exposure of 50%
(iv) 𝜃 = −9.66 (summary treatment effect; negative value favors treatment group)
(v) 𝜏2 = 7.79 (between trial variation in 𝜃)
(vi) 𝛽 = 159.73 (mean response in control group)
(vii) 𝜏𝜷 2 = 233.99 (between trial variation in 𝛽)
(viii) 𝜎2 = 333.74 (residual variance)
a1 same as base case, except changed (i) k = 5
a2 same as base case, except changed (i) k = 20
b1 same as base case, except changed (ii) n ∼i u(30, 1000)
b2 same as base case, except changed (ii) n ∼i u(30, 100) for trials 1 to 5,
n ∼i u(900, 1000) for trials 6 to 10
b1-a1 same as base case, except changed (i) and (ii) k = 5 and n ∼i u(30, 1000)
b1-a2 same as base case, except changed (i) and (ii) k = 20 and n ∼i u(30, 1000)
b2-a1 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 and 2, n ∼i u(900, 1000) for trials 3 to 5
b2-a2 same as base case, except changed (i) and (ii) n ∼i u(30, 100) for trials 1 to 10, n ∼i u(900, 1000) for trials 11 to 20
b3 same as base case, except changed (ii) n ∼i u(30, 100)
c1 same as base case, except changed (vii) halving 𝜏𝛽 2 to 117
c2 same as base case, except changed (vii) doubling 𝜏𝛽 2 to 468
d1 same as base case, except changed (v) halving 𝜏2 to 3.9
d2 same as base case, except changed (v) doubling 𝜏2 to 15.6
*each scenario was repeated under the following data generating mechanisms: (1) random treatment effect with a normally distributed intercept, (2) random treatment effect with a
220*beta(15, 3) distribution for the intercept (except scenarios c1 and c2), and (3) common treatment effect with a normally distributed intercept (except scenarios d1 and d2).
abbreviations: k = number of trials, ni = number of participants in trial i, 𝜃 = summary treatment effect, 𝜏2 = between trial variation in summary treatment effect, 𝛽 = mean response in
control group, 𝜏𝛽 2 = between trial variation in mean response in control group, 𝜎2 = residual variance, u (a, b) = uniform distribution over the interval (a, b).
table 2 mean percentage bias of the summary treatment effect estimate (𝜃
̂) under different scenarios, for the
random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified (1) and random (2) intercept models, under each of the different
estimation options considered
mean percentage bias of 𝜽̂
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −0.01 0.00 −0.01 −0.01 0.34 0.31 0.33 0.29
a1 −0.90 −0.90 −0.90 −0.90 −0.02 0.13 −0.06 0.10
a2 0.15 0.18 0.16 0.18 −0.48 −0.41 −0.47 −0.40
b1 0.67 0.58 0.68 0.58 −0.57 −0.63 −0.58 −0.63
b2 −0.47 −0.59 −0.47 −0.56 0.29 0.27 0.33 0.28
b1-a1 0.54 0.53 0.53 0.53 −0.14 −0.27 −0.11 −0.24
b1-a2 −0.10 −0.11 −0.10 −0.11 0.08 −0.01 0.10 0.01
b2-a1 −0.41 −0.43 −0.37 −0.41 0.46 0.52 0.05 −0.17
b2-a2 −0.45 −0.41 −0.44 −0.42 −0.45 −0.37 −0.37 −0.34
b3 0.19 0.24 0.21 0.25 1.36 1.34 1.20 1.20
c1 −0.03 −0.02 −0.03 −0.02 n/a n/a n/a n/a
c2 −0.01 0.07 −0.01 0.07 n/a n/a n/a n/a
d1 −0.10 −0.13 −0.10 −0.13 0.26 0.34 0.24 0.32
d2 0.13 0.12 0.13 0.12 0.46 0.49 0.45 0.46
* see table 1 for full data generation details relating to each scenario. true value for 𝜃 is −9.66.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
we focus on the results when assuming a random treatment effect. further results assuming a common treatment effect
data generating mechanism and for additional performance measures (percentage convergence of models, numerical
percentage coverage of the summary treatment effect estimate, average run time of simulations, and empirical se and
mse of the summary treatment effect estimate) are shown in the supplementary material (web appendices b and c,
respectively). in the following, we summarize the key findings.
3.2.1 convergence of models
under a random treatment effect data generating mechanism, the proportion of models that converged was consistently
high, with a minimum convergence of 94.3% across all situations (web table c.i).
note that all other performance measures to follow are estimated conditional on model convergence.
3.2.2 bias of summary treatment effect estimate
generally, there were negligible differences in mean percentage bias of 𝜃
̂ between ml and reml estimation options for
either model (stratified or random intercept), under any given scenario and data generating mechanism (table 2 and
web table b.i). nor were there any important differences in the mean percentage bias of 𝜃
̂ between the stratified model
and random intercept model. furthermore, mean bias was close to zero in all situations and only reached a maximum
absolute percentage of 1.36%.
3.2.3 bias of estimated between-trial variance of treatment effects
for either model (stratified or random intercept), under any given scenario and data generating mechanism, using ml
always produced more downwardly biased estimates than reml (table 3), as expected.14-18 for example, for the base
table 3 median percentage bias of the between-trial variance of treatment effects (𝜏̂2), under different scenarios
for the random treatment effect with normal and beta distributions for the intercept data generating mechanisms.
results shown separately for stratified and random intercept models, under each of the estimation options considered
median percentage bias of ̂𝝉𝟐
intercept normal distribution beta distribution
generating
mechanism
method for stratified intercept random intercept stratified intercept random intercept
modeling
intercept
estimation ml reml ml reml ml reml ml reml
scenario*
base case −100.00 −16.86 −41.50 −15.85 −100.00 −14.36 −56.17 −32.86
a1 −100.00 −36.88 −80.33 −33.10 −100.00 −73.62 −100.00 −80.15
a2 −100.00 −8.59 −20.86 −7.74 −100.00 −13.96 −39.78 −25.06
b1 −49.64 −10.74 −22.94 −10.09 −100.00 −14.23 −28.91 −16.39
b2 −56.93 −18.03 −35.11 −17.84 −72.90 −17.92 −35.95 −19.81
b1-a1 −77.28 −19.57 −42.62 −18.45 −100.00 −27.27 −54.23 −30.91
b1-a2 −40.64 −5.83 −13.08 −6.31 −88.22 −9.50 −19.59 −13.53
b2-a1 −72.73 −28.35 −56.23 −28.10 −100.00 −34.30 −64.33 −32.61
b2-a2 −36.66 −4.98 −14.36 −5.39 −50.99 −4.84 −12.86 −5.35
b3 −100.00 −28.68 −61.86 −24.74 −100.00 −17.42 −81.05 −48.75
c1 −100.00 −16.72 −39.54 −14.38 n/a n/a n/a n/a
c2 −100.00 −16.86 −40.56 −15.94 n/a n/a n/a n/a
d1 −100.00 −19.20 −66.97 −24.63 −100.00 −33.67 −99.98 −62.07
d2 −100.00 −11.65 −30.04 −11.79 −100.00 −10.50 −37.84 −22.19
* see table 1 for full data generation details relating to each scenario. true value for 𝜏2 is 7.79, except scenarios d1 and d2 where 𝜏2
is equal to 3.9 and 15.6, respectively.
n/a = not applicable, since there is no 𝜏𝛽 2 to vary when a beta distribution is used for the intercept data generating mechanism.
options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation.
case scenario with the random intercept model, under the normal intercept data generating mechanism, the median
percentage bias using reml estimation was −15.9% compared to −41.5% using ml estimation. the bias was worse when
using a stratified intercept model (due to the extra number of parameters to estimate), as ml estimation often produced
a downward median bias of 100%.
when using reml estimation, there were generally only small differences between random and stratified intercept
models in terms of bias of the between-trial variance of treatment effects; however, while better than ml, downward bias
was not removed entirely with reml. furthermore, the overall size of the bias was typically greater in the beta distribution
intercept case than in the normal distribution intercept case, regardless of which model was used.
3.2.4 empirical se and mse of summary treatment effect estimate
there were negligible differences in empirical se or mse of 𝜃
̂ between the two models (stratified or random intercept),
under any given scenario and data generating mechanism (web tables c.viii to c.x).
3.2.5 coverage of summary treatment effect estimate
there were marked differences observed in the coverage of 𝜃
̂ across the different estimation approaches (ml or reml)
and ci derivations (standard, kr, or satterthwaite), as now explained.
(i) under a normal distribution intercept generating mechanism
we consider first the normal distribution intercept generating mechanism (figure 2a and web table c.ii). across both
models and all scenarios, ml with standard ci (ml + standard) derivation always exhibited under-coverage compared to
the other options (reml+standard, reml+kr, reml+satterthwaite). for example, for scenario b2 using the stratified
intercept model, the percentage coverage using ml + standard was 81.3% compared to 88.8%, 95.8%, and 94.8% using
figure 2 percentage coverage of the summary treatment effect estimate (𝜃
̂) under different scenarios for the random treatment effect
with normal (figure 2a) and beta distributions (figure 2b) for the intercept data generating mechanisms, for stratified (left) and random
(right) intercept models, under each of the estimation and ci derivation options considered. options: ml, maximum likelihood estimation
with standard confidence interval (ci) derivation; reml, restricted maximum likelihood estimation with standard ci derivation;
reml+kr, reml estimation with kenward-roger ci derivation; reml+satt, reml estimation with satterthwaite ci derivation [colour
figure can be viewed at wileyonlinelibrary.com]
reml+standard, reml+kr and reml+satterthwaite, respectively. the random intercept model always performed
better with respect to coverage under ml than the stratified intercept model under ml, likely due to the reduction in the
number of parameters that needed estimation. for example, when considering only small trials (scenario b3), percentage
coverage improved from 91.3% to 94.5% (close to the nominal 95% level), when comparing the stratified to a random
intercept model with ml estimation.
using reml substantially improved on the coverage obtained from ml and removed any important differences
between the stratified and random intercept models. however, for either model (stratified or random intercept),
reml+standard still had important under-coverage in some scenarios. for example, in scenario b2-a1, a percentage
coverage of 85.7% and 85.8% was observed, under a stratified and random intercept model, respectively.
the reml+kr approach generally improved on the coverage compared to reml+standard, again with no important
differences observed between the stratified and random intercept models. percentage coverage ranged from 95.1 to 98.8%
using reml+kr, while the percentage coverage ranged from 85.7% to 94.9% using reml+standard. the improvement
gained by using reml+kr was especially important for scenarios that involved at least 10 trials and a large variation
in sample sizes (b1, b2, b1-a2, b2-a2). for example, for scenario b2 (five small and five large sample sized trials, with
average sample size 66 and 949 in the small and large trials, respectively), percentage coverage from the stratified intercept
model was 88.8%, using reml+standard, but 95.8% using reml+kr.
using reml+satterthwaite gave very similar results to reml+kr. occasionally, there was some over-coverage using
reml+kr or reml+satterthwaite, particularly when using a low number of trials (k = 5). for example, coverage was
close to 99% (regardless of which model was used), in a setting of k = 5 trials with an equal number of participants
per trial (scenario a1; ni = 100), and in a setting of k = 5 trials with some small-sized and some large-sized trials (scenario
b2-a1; 2 small trials where ni∼u(30, 100), and 3 large trials where ni∼u(900, 1000)).
(ii) under a beta distribution intercept generating mechanism
for the beta distribution intercept generating mechanism (figure 2b and web table c.iii), using reml+standard again
gave better coverage than using ml, and using reml+kr or reml+satterthwaite generally further improved upon this
coverage (ie, moved it closer to 95%), especially with scenarios concerning at least 10 trials that had a large variation in
sample sizes.
as before, under ml estimation, the random intercept model showed better estimates of between-trial variance and
improved coverage (closer to 95%) than the stratified intercept model. however, differences between the two models were
generally small for estimation under reml (with or without a 95% ci correction).
3.2.6 common treatment effect data generating mechanism
results based on a common (fixed) treatment effect data generating mechanism are shown in web appendix b. all fitted
models assumed a common treatment effect and converged every time (ie, 100% convergence), and there was negligible
difference in mean percentage bias of 𝜃
̂ between ml and reml estimation options for either model (stratified or
random intercept), or between either model (web table b.1). the percentage coverage results were stable across all comparisons, ranging from 93.8 to 96.0%, with negligible differences between the various models and estimation options
(web figure b.1).
3.2.7 key findings
a summary of the key findings from this simulation study for settings with between-trial heterogeneity in the treatment
effect is given in figure 3.
figure 3 key simulation findings and recommendation for estimating a summary treatment effect based on a one-stage individual
participant data (ipd) meta-analysis of randomized trials with a 1:1 treatment:control allocation ratio and a continuous outcome, with
between-study heterogeneity in the treatment effect. ci, confidence interval; ml, maximum likelihood estimation; mse, mean square error;
reml, restricted maximum likelihood; se, standard error
4 illustration of methods and key findings in a real example
the international weight management in pregnancy (i-wip) collaborative group dataset includes ipd from 36 trials
(12,447 women), collected for a health technology assessment report in 2017.24 the authors investigated the association
between diet and lifestyle interventions to prevent weight gain in pregnancy and several other primary outcomes. here, we
present ipd meta-analysis results using the i-wip dataset, for illustration purposes only, to demonstrate the key findings
from our simulation study. we include only trials that collected follow-up outcome values for weight in pregnancy and
apply a one-stage ipd model for this continuous outcome, with model assumptions in line with our simulation study
analysis. however, while we did not generate any baseline imbalances in our simulation data, baseline weight imbalance
was present in some trials from the i-wip data. to remove this imbalance, we apply a baseline adjustment in our models,
as is recommended.21
table 4 shows ipd meta-analysis results for a random sample of 5 and 10 trials that investigated exercise interventions
and for 20 trials (using all 15 exercise trials, plus 5 additional trials that investigated mixed interventions).
these results are in agreement with the key findings observed in section 3.2 and summarized in figure 3. firstly,
the magnitude of summary treatment effect estimate was similar throughout, irrespective of model used or estimation
method. secondly, with ml estimation, the stratified intercept model gave narrower 95% cis and smaller estimates of the
between trial variance than the random intercept model, especially with k = 20 trials. thirdly, using reml overcame this
discrepancy, with now very similar results between the random and stratified intercept models; in addition, the 95% cis
were wider when using reml, due to the larger estimates of the between trial variance. fourthly, applying a kr or
satterthwaite correction in addition to reml further widened the 95% cis. finally, although 95% cis were slightly wider
when using a kr correction instead of satterthwaite, results were generally similar from these two corrections, especially
with k = 20 trials.
5 discussion
5.1 key findings
in summary, we have conducted an extensive simulation study to examine the estimation of a summary treatment effect
using a one-stage ipd meta-analysis model for a continuous outcome. specifically, we examined different options for
specifying the trial-specific intercepts and compared different options for parameter estimation and ci derivation. fourteen different scenarios were tested (varying the number of trials, number of participants per trial, and heterogeneity of
parameters), for each of three different data generating mechanisms (encompassing a common and random treatment
effect with a normally distributed intercept, as well as a beta distributed intercept and random treatment effect). all scenarios assumed a 1:1 treatment:control allocation ratio, and a data generating mechanism that was based on a random
intercept model; hence, our conclusions are restricted to this context.
our key findings, for settings with heterogeneity in treatment effect, were illustrated using a real example, and these
are summarized in figure 3. firstly, the results suggest that, as long as the same estimation method is used, there are no
important differences between the stratified and random intercept models in terms of bias, empirical se or mse for the
summary treatment effect estimate. indeed, the mean bias in 𝜃
̂ was close to zero throughout, which is perhaps expected
given the statistical theory underpinning linear mixed models. furthermore, when using reml (with or without a ci
correction), there were generally no important differences in coverage performance between the stratified and random
intercept models. interestingly, the random intercept model (which assumes normality of the intercept) performed well
even when the trial intercepts were drawn from a highly asymmetric beta distribution. kahan and morris25 also found
that misspecifying the random intercept distribution of random effects models did not impact treatment effect results.
secondly, the kr and satterthwaite corrections generally performed similarly in terms of improving the coverage and
were especially effective for scenarios involving at least 10 trials with a mix of small and large sample sizes, but also
considerably increased mean run time in these instances (see web tables c.v to c.vii). the satterthwaite correction
always had a similar or quicker average run time than kr (sometimes by more than eight times). one could surmise from
the similarity in coverage performance that the main impact of both corrections is in the use of a t-distribution to derive
cis and that the kr adjusted variance of the summary estimate has relatively less impact.
thirdly, when using ml estimation, the random intercept model always showed better or comparable coverage to the
stratified intercept model (closer to 95%). this is likely due to the random intercept model having a reduced number of
.
table 4 results from baseline weight adjusted individual participant data meta-analysis of i-wip data: summary treatment effect estimate (
̂
𝜃) with 95% confidence interval and
between-trial variance of treatment effects estimate (̂𝜏2). from meta-analysis with different numbers of trials (k = 5, 10, or 20), and assuming a random treatment effect and a common
residual variance throughout
̂
𝜽 (95% ci); ̂𝝉𝟐
method for modeling stratified random
intercept intercept intercept
estimation ml reml reml+kr reml+satt ml reml reml+kr reml+satt
number of trials
5 −1.172 −1.172 −1.172 −1.172 −1.170 −1.171 −1.171 −1.171
(−1.811,−0.534); (−1.815,−0.530); (−3.114, 0.770); (−2.712, 0.367); (−1.811,−0.529); (−1.813,−0.528); (−3.072, 0.731); (−2.681, 0.340);
8.58e−17 3.94e−15 3.94e−15 3.94e−15 2.95e−14 4.51e−12 4.51e−12 4.51e−12
10 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972 −0.972
(−1.479,−0.465); (−1.482,−0.462); (−1.740,−0.204); (−1.653,−0.291); (−1.481,−0.462); (−1.482,−0.462); (−1.731,−0.212); (−1.646,−0.298);
1.97e−16 2.58e−12 2.58e−12 2.58e−12 9.52e−11 5.94e−16 5.94e−16 5.94e−16
20 −0.821 −0.820 −0.820 −0.820 −0.830 −0.830 −0.830 −0.830
(−1.102,−0.540); (−1.243,−0.396); (−1.298,−0.342); (−1.286,−0.354); (−1.217,−0.442); (−1.235,−0.426); (−1.290,−0.370); (−1.276,−0.384);
1.11e−14 0.317 0.317 0.317 0.210 0.258 0.258 0.258
ci = confidence interval
options: ml, maximum likelihood estimation with standard ci derivation; reml, restricted maximum likelihood estimation with standard ci derivation; reml+kr, reml estimation with kenward-roger
ci derivation; reml+satt, reml estimation with satterthwaite ci derivation.
parameters, and thus improved ml estimation of the between-trial variance. a similar finding was also recently shown
by jackson et al26 for one-stage meta-analysis models for a binary outcome. nevertheless, even the random intercept
model produced downwardly biased estimates of the between-trial variance using ml and low coverage. using reml
is therefore important, to improve on this coverage. indeed, coverage is more consistently near 95% when using reml
with either a kr or satterthwaite correction. however, on some occasions (particularly, when there are a low number of
trials), the kr and satterthwaite corrections lead to over-coverage. this is similar to the hartung-knapp sidik-jonkman
correction to 95% cis following a two-stage analysis,27,28 which generally gives a more suitable coverage than a standard
95% ci, although on occasion is overly conservative.29
if there is genuinely no heterogeneity in treatment effect across trials, however, our findings suggest that there are
generally no differences in mean bias, empirical se, mse, or coverage for the treatment effect between the stratified
and random intercept models, for any estimation method, ci derivation approach, and under any simulation scenario.
however, in our experience, situations of completely homogeneous treatment effects are unlikely.
unreported simulations
following recent work by morris et al5 and jackson et al,26 which considered an alternative coding for the binary treatment group variable (+0.5/−0.5 for treatment/control groups, respectively), we also tested this treatment group coding
for our reml estimation simulation results but found only small differences in performance results compared to the 1/0
coding. hence, we did not present the results here. we also tested using stratified (instead of common) residual variances
for both the data generating mechanisms and models fitted. again, no difference in performance of the summary treatment effect estimate was observed, suggesting that the ipd model may be robust to the (mis) specification of the residual
variances. morris et al5 also found that assuming common or distinct residual variances, in a common treatment effect
ipd meta-analysis setting, has very little impact on the precision of the summary effect when the number of patients per
trial is over 25. in general, from a point of principle, we recommend a separate residual variance for each trial, but in situations where this has convergence problems, a common residual variance would seem apt. further research of this issue
would be welcome.
5.2 recommendation
for researchers conducting a one-stage ipd meta-analysis of randomized trials with a 1:1 treatment:control allocation
ratio and a continuous outcome and aiming to estimate a summary treatment effect that is heterogeneous across trials,
we recommend that either a stratified or random intercept model is used, and estimated using reml, ideally followed
by a 95% ci derived using either the kr or satterthwaite approaches. in our simulations, this approach gave close to zero
mean bias in the summary treatment effect estimate and coverage generally close to 95%, except in a few situations where
there was over-coverage (particularly, when there were a low number of trials).
using reml with a kr correction for linear mixed models based on continuous outcomes has already been proposed
by some researchers,30,31 while literature advocating the merits of the satterthwaite correction is less common. however,
in our simulations, we found that the satterthwaite correction generally obtains similar results to the kr approach, hence
making for an excellent alternative.
5.3 limitations and further research
throughout this simulation study, we focused solely on synthesizing trials containing a 1:1 treatment:control allocation ratio; hence, an important limitation is that our conclusions may not hold under settings involving other treatment
allocation ratios.
in addition, we have focused solely on ipd of continuous outcomes, hence another important limitation is that our
conclusions are not necessarily generalizable to other popular outcome types in the meta-analytical field, such as binary
and time-to-event outcomes. binary outcomes, for example, are more complex to deal with than continuous outcomes,
as a logistic mixed effects model is nonlinear, and hence, the corresponding maximum likelihood function has no closed
form. jackson et al26 recently investigated the use of ml estimation and found that a stratified intercept model leads
to substantial downward bias in between-trial variance estimates and under-coverage of cis for the summary result,
which increases as the number of trials (and thus parameters) increases. interestingly, the issue was resolved when using
a + 0.5/−0.5 coding for the treatment variable, rather than a 1/0 coding, or when placing random effects on the trial
intercept.26 mcneish32 investigated logistic mixed models by either retaining the nonlinearity of the model and making
an approximation for the likelihood function or linearly approximating the model to give the likelihood function a closed
form (pseudo-likelihood approach). the latter option was shown to be favorable (under the specific conditions of the
study), by use of a residual penalized quasi-likelihood with a kr correction.
while our simulation study did consider an extensive range of scenarios—we varied the number of trials, number of
participants per trial, and heterogeneity of parameters—we recognize that our conclusions were based on a final score
model that did not adjust for baseline outcome value. often, the ancova model should be used, as in our applied
example, because there will be baseline imbalances in practice. however, as baseline values did not vary across individuals in our simulation study, using a final score analysis model rather than ancova was appropriate. when using
one-stage ancova ipd models, an additional issue is using stratified adjustment terms or placing a random effect on
the adjustment term. based on our study findings, we expect that, with reml, either approach should be suitable.
we also assumed independence of the two random effects (ie, a covariance of zero) when assuming random intercept
and random treatment effects, both in the data generating mechanism and when fitting the corresponding model. their
correlation could be taken into account if deemed sensible13; however, we did not consider this alternative assumption
in our simulation study, largely due to the added complexity and difficulty in estimating the correlation parameter in
practice with few trials. importantly, it is perhaps likely that the effect of treatment could be correlated with the control
group outcome, and therefore, the most appropriate assumption needs further consideration.
another limitation is that we did not consider prediction intervals. these allow us to make predictive inferences of
the potential treatment effect in a single setting of application.33 some researchers argue that prediction intervals offer
a more appropriate summary of trial findings than cis of the average effect.34 however, partlett and riley18 showed in a
two-stage ipd meta-analysis setting that there was considerable under-coverage of prediction intervals in some situations.
for example, under-coverage was observed in settings involving a low heterogeneity or with varied trial sample sizes and
was not improved upon by increasing the number of trials or using ci corrections such as kr. hence, we did not consider
it useful to consider prediction intervals in our study.
finally, we could have considered a bayesian approach to our simulation study, which is an alternative to frequentist
methods, and a natural way to account for all parameter uncertainty, to make predictions and to derive (joint) probabilistic
statements regarding parameters of interest. however, we deemed this extension to be beyond the scope of this paper.
yet, if a bayesian approach is to be used in practice, bayesians still need to choose between random or stratified intercept
one-stage ipd models, which is something that our work can help with.
5.4 conclusions
in an ipd meta-analysis of trials with a 1:1 treatment:control allocation ratio and a continuous outcome, aiming to estimate a summary treatment effect that is heterogeneous across trials, our findings suggest that researchers use either a
stratified or random intercept model with reml estimation and ideally derive 95% cis using either the kr or satterthwaite approach. further work is needed to improve upon coverage in a few situations where the kr and satterthwaite
intervals are overly conservative. such situations include when there are a low number of trials; these are also situations
where corrections to cis in a two-stage ipd meta-analysis are overly conservative.18


<|EndOfText|>

the value of preseason screening for injury prediction: the development and internal validation of a multivariable prognostic model to predict indirect muscle injury risk in elite football (soccer) players

abstract
background
in elite football (soccer), periodic health examination (phe) could provide prognostic factors to predict injury risk.

objective
to develop and internally validate a prognostic model to predict individualised indirect (non-contact) muscle injury (imi) risk during a season in elite footballers, only using phe-derived candidate prognostic factors.

methods
routinely collected preseason phe and injury data were used from 152 players over 5 seasons (1st july 2013 to 19th may 2018). ten candidate prognostic factors (12 parameters) were included in model development. multiple imputation was used to handle missing values. the outcome was any time-loss, index indirect muscle injury (i-imi) affecting the lower extremity. a full logistic regression model was fitted, and a parsimonious model developed using backward-selection to remove factors that exceeded a threshold that was equivalent to akaike’s information criterion (alpha 0.157). predictive performance was assessed through calibration, discrimination and decision-curve analysis, averaged across all imputed datasets. the model was internally validated using bootstrapping and adjusted for overfitting.

results
during 317 participant-seasons, 138 i-imis were recorded. the parsimonious model included only age and frequency of previous imis; apparent calibration was perfect, but discrimination was modest (c-index = 0.641, 95% confidence interval (ci) = 0.580 to 0.703), with clinical utility evident between risk thresholds of 37–71%. after validation and overfitting adjustment, performance deteriorated (c-index = 0.589 (95% ci = 0.528 to 0.651); calibration-in-the-large = − 0.009 (95% ci = − 0.239 to 0.239); calibration slope = 0.718 (95% ci = 0.275 to 1.161)).

conclusion
the selected phe data were insufficient prognostic factors from which to develop a useful model for predicting imi risk in elite footballers. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.

trial registration
nct03782389

key points
factors measured through preseason screening generally have weak prognostic strength for future indirect muscle injuries, and further research is needed to identify novel, robust prognostic factors.

because of sample size restrictions and until the evidence base improves, it is likely that any further attempts at creating a prognostic model at individual club level would also suffer from poor performance.

the value of using preseason screening data to make injury predictions or to select bespoke injury prevention strategies remains to be demonstrated, so screening should only be considered as useful for detection of salient pathology or for rehabilitation/performance monitoring purposes at this time.

background
in elite football (soccer), indirect (non-contact) muscle injuries (imis) predominantly affect the lower extremities and account for 30.3 to 47.9% of all injuries that result in time lost to training or competition [1,2,3,4,5]. reduced player availability negatively impacts upon medical [6] and financial resources [7, 8] and has implications for team performance [9]. therefore, injury prevention strategies are important to professional teams [9].

periodic health examination (phe), or screening, is a key component of injury prevention practice in elite sport [10]. specifically, in elite football, phe is used by 94% of teams and consists of medical, musculoskeletal, functional and performance tests that are typically evaluated during preseason and in-season periods [11]. phe has a rehabilitation and performance monitoring function [12] and is also used to detect musculoskeletal or medical conditions that may be dangerous or performance limiting [13]. another perceived role of phe is to recognise and manage factors that may increase, or predict, an athlete’s future injury risk [10], although this function is currently unsubstantiated [13].

phe-derived variables associated with particular injury outcomes (such as imis) are called prognostic factors [14], which can be used to identify risk differences between players within a team [12]. single prognostic factors are unlikely to satisfactorily predict an individual’s injury risk if used independently [15]. however, several factors could be combined in a multivariable prognostic prediction model to offer more accurate personalised risk estimates for the occurrence of a future event or injury [15, 16]. such models could be used to identify high-risk individuals who may require an intervention that is designed to reduce risk [17], thus assisting decisions in clinical practice [18]. despite the potential benefits of using prognostic models for injury risk prediction, we are unaware of any that have been developed using phe data in elite football [19].

therefore, the aim of this study was to develop and internally validate a prognostic model to predict individualised imi risk during a season in elite footballers, using a set of candidate prognostic factors derived from preseason phe data.

methods
the methods have been described in a published protocol [20] so will only be briefly outlined. this study has been registered on clinicaltrials.gov (identifier: nct03782389) and is reported according to the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) statement [21, 22].

data sources
this study was a retrospective cohort design. eligible participants were identified from a population of male elite footballers, aged 16–40 years old at manchester united football club. a dataset was created using routinely collected injury and preseason phe data over 5 seasons (1st july 2013 to 19th may 2018). for each season, which started on 1st july, participants completed a mandatory phe during week 1 and were followed up to the final first team game of the season. if eligible participants were injured at the time of phe, a risk assessment was completed by medical staff. only tests that were appropriate and safe for the participant’s condition were completed; examiners were not blinded to injury status.

participants and eligibility criteria
during any season, participants were eligible if they (1) were not a goalkeeper and (2) participated in phe for the relevant season. participants were excluded if they were not contracted to the club for the forthcoming season at the time of phe.

ethics and data use
informed consent was not required as data were captured from the mandatory phe completed through the participants’ employment. the data usage was approved by the club and university of manchester research ethics service.

outcome
the outcome was any time-loss, index imi (i-imi) of the lower extremity. that is, any i-imi sustained by a participant during matches or training, which affected lower abdominal, hip, thigh, calf or foot muscle groups and prohibited future football participation [23]. i-imis were graded by a club doctor or physiotherapist according to the validated munich consensus statement for the classification of muscle injuries in sport [24, 25], during routine assessments undertaken within 24 h of injury. these healthcare professionals were not blinded to phe data.

sample size
we allowed a maximum of one candidate prognostic factor parameter per 10 i-imis, which at the time of protocol development, was the main recommendation to minimise overfitting (additional file 1) [20, 26]. the whole dataset was used for model development and internal validation, which agrees with methodological recommendations [27].

candidate prognostic factors
the available dataset contained 60 candidate factors [20]. because of the sample size considerations, before any analysis, the set of candidate factors was reduced. initially, an audit was conducted to quantify missing values and to determine the measurement reliability of the eligible candidate factors [20]. any candidate factors which had greater than 15% missing data or where reliability was classed as fair to poor (intraclass correlation coefficient < 0.70) were excluded [20] (additional file 2). of the remaining 45 eligible factors, previous evidence of prognostic value [19] and clinical reasoning were used to select candidate prognostic factors suitable for inclusion [20]. this process left a final set of 10 candidate factors, represented by 12 model parameters (table 1). the 35 factors that were not included in model development are also listed in additional file 2, and will be utilised in a related, forthcoming exploratory study which aims to examine their association with indirect muscle injuries in elite football players.

table 1 set of candidate prognostic factors (with corresponding number of parameters) for model development
full size table
statistical analysis
data handling—outcome measures
each participant-season was treated as independent. participants who sustained an i-imi were no longer considered at risk for that season and were included for further analysis at the start of the next season if still eligible. any upper limb imi, trunk imi or non-imi injuries were ignored, and participants were still considered at risk.

eligible participants who were loaned to another club throughout that season, but had not sustained an i-imi prior to the loan, were still considered at risk. i-imis that occurred whilst on loan were included for analysis, as above. permanently transferred participants (who had not sustained an i-imi prior to leaving) were recorded as not having an i-imi during the relevant season and exited the cohort at the season end.

data handling—missing data
missing values were assumed to be missing at random [20]. the continuous parameters generally demonstrated non-normal distributions, so were transformed using normal scores [35] to approximate normality before imputation, and back-transformed following imputation [36]. multivariate normal multiple imputation was performed, using a model that included all candidates and i-imi outcomes. fifty imputed datasets were created in stata 15.1 (statacorp llc, texas, usa) and analysed using the mim module.

prognostic model development
continuous parameters were retained on their original scales, and their effects assumed linear [22]. a full multivariable logistic regression model was constructed, which contained all 12 parameters. parameter estimates were combined across imputed datasets using rubin’s rules [37]. to develop a parsimonious model that would be easier to utilise in practice, backward variable selection was performed using estimates pooled across the imputed datasets at each stage of the selection procedure to successively remove non-significant factors with p values > 0.157. this threshold was selected to approximate equivalence with akaike’s information criterion [38, 39]. multiple parameters representing the same candidate factor were tested together so that the whole factor was either retained or removed. candidate interactions were not examined, and no terms were forced into the model. all analyses were conducted in stata 15.1.

assessment of model performance
the full and parsimonious models were used to predict i-imi risk over a season, for every participant-season in all imputed datasets. for all performance measures, each model’s apparent performance was assessed in each imputed dataset and then averaged across all imputed datasets using rubin’s rules [37]. discrimination determines a model’s ability to differentiate between participants who have experienced an outcome compared to those who have not [40], quantified using the concordance index (c-index). this is equivalent to the area under the receiver operating characteristic (roc) curve for logistic regression, where 1 demonstrates perfect discrimination, whilst 0.5 indicates that discrimination is no better than chance [41].

calibration determines the agreement between the model’s predicted outcome risks and those observed [42], evaluated using an apparent calibration plot in each imputed dataset. all predicted risks were divided into ten groups defined by tenths of predicted risk. the mean predicted risks for the groups were plotted against the observed group outcome proportions with corresponding 95% confidence intervals (cis). a loess smoothing algorithm showed calibration across the range of predicted values [43]. for grouped and smoothed data points, perfect predictions lie on the 45° line (i.e. a slope of 1).

the systematic (mean) error in model predictions was quantified using calibration-in-the-large (citl), which has an ideal value of 0 [40, 42], and the expected/observed (e/o) statistic, which is the ratio of the mean predicted risk against the mean observed risk (ideal value of 1) [40, 42]. the degree of over or underfitting was determined using the calibration slope, where a value of 1 equals perfect calibration on average across the entire range of predicted risks [22]. nagelkerke’s pseudo-r2 was also calculated, which quantifies the overall model fit, with a range of 0 (no variation explained) to 1 (all variation explained) [44].

assessment of clinical utility
decision-curve analysis was used to assess the parsimonious model’s apparent clinical usefulness in terms of net benefit (nb) if used to allocate possible preventative interventions. this assumed that the model’s predicted risks were classed as positive (i.e. may require a preventative intervention) if greater than a chosen risk threshold, and negative otherwise. nb is then the difference between the proportion of true positives and false positives, where both were weighted by the odds of the chosen risk threshold and also divided by the sample size [45]. positive nb values suggest the model is beneficial compared to treating none, which has no benefit to the team but with no negative cost and efficiency implications. the maximum possible nb value is the proportion with the outcome in the dataset.

the model’s nb was also compared to the nb of delivering an intervention to all individuals. this is considered a treat-all strategy, offering maximum benefit to the team, but with maximum negative cost and efficiency implications [17]. a model has potential clinical value if it demonstrates higher nb than the default strategies over the range of risk thresholds which could be considered as high risk in practice [46].

internal validation and adjustment for overfitting
to examine overfitting, the parsimonious model was internally validated using 200 bootstrap samples, drawn from the original dataset with replacement. in each sample, the complete model-building procedure (including multiple imputation, backward variable selection and performance assessment) was conducted as described earlier. the difference in apparent performance (of a bootstrap model in its bootstrap sample) and test performance (of the bootstrap model in the original dataset) was averaged across all samples. this generated optimism estimates for the calibration slope, citl and c-index statistics. these were subtracted from the original apparent calibration slope, citl and c-index statistics to obtain final optimism-adjusted performance estimates. the nagelkerke r2 was adjusted using a relative reduction equivalent to the relative reduction in the calibration slope.

to produce a final model adjusted for overfitting, the regression coefficients produced in the parsimonious model were multiplied by the optimism-adjusted calibration slope (also termed a uniform shrinkage factor), to adjust (or shrink) for overfitting [47]. finally, the citl (also termed model intercept) was then re-estimated to give the final model, suitable for evaluation in other populations or datasets.

complete case and sensitivity analyses
to determine the effect of multiple imputation and player transfer assumptions on model stability, the model development process was repeated: (1) as a complete case analysis and (2) as sensitivity analyses which excluded all participant-seasons where participants had not experienced an i-imi up to the point of loan or transfer, which were performed as both multiple imputation and complete case analyses.

results
participants
during the five seasons, 134 participants were included, contributing 317 participant-seasons and 138 imis in the primary analyses (fig. 1). three players were classified as injured when they took part in phe (which affected three participant-seasons). this meant they were unavailable for full training or to play matches at that time. however, these players had commenced football specific, field-based rehabilitation around this time, so also had similar exposure to training activities as the uninjured players. as such, these players were included in the cohort because it was reasonable to assume that they could also be considered at risk of an i-imi event even during their rehabilitation activities.

fig. 1
figure1
participant flow chart. key: n = participants; i-imi = index indirect muscle injury

full size image
table 2 describes the frequency of included participant-seasons, and the frequency and proportion of recorded i-imi outcomes across all five seasons. for the sensitivity analyses (excluding loans and transfers), 260 independent participant-seasons with 129 imis were included; 36 participants were transferred on loan, whilst 14 participants were permanently transferred during a season, which excluded 57 participant-seasons in total (fig. 1). table 2 also describes the frequency of excluded participant-seasons where players were transferred either permanently or on loan, across the 5 seasons.

table 2 frequency of included participant-seasons, i-imi outcomes and participant-seasons affected by transfers, per season (primary analysis)
full size table
table 3 shows anthropometric and all prognostic factor characteristics for participants included in the primary analyses. these were similar to those included in the sensitivity analyses (additional file 3).

table 3 characteristics of included participants in the primary analysis
full size table
missing data and multiple imputation
all i-imi, age and previous muscle injury data were complete (table 3). for all other candidates, missing data ranged from 6.31 (for hip internal and external rotation difference) to 13.25% for countermovement jump (cmj) power (table 3). the distribution of imputed values approximated observed values (additional file 4), confirming their plausibility.

model development
table 4 shows the parameter estimates for the full model and parsimonious model after variable selection (averaged across imputations).

table 4 results of the full and parsimonious multivariable logistic regression models, with prediction formulae
full size table
for both models, only age and frequency of previous imis had a statistically significant (but modest) association with increased i-imi risk (p < 0.157). no clear evidence for an association was observed for any other candidate factor.

model performance assessment and clinical utility
table 4 shows the apparent performance measures for the full and parsimonious models, all of which were similar. figure 2 shows the apparent calibration of the parsimonious model in the dataset used to develop the model (i.e. before adjustment for overfitting). these were identical across all imputed datasets because the retained prognostic factors contained no missing values. the parsimonious model had perfect apparent overall citl and calibration slope by definition, but calibration was more variable around the 45° line between the expected risk ranges of 28 to 54%. discrimination was similarly modest for the full (c-index = 0.670, 95% ci = 0.609 to 0.731) and parsimonious models (c-index = 0.641, 95% ci = 0.580–0.703). the apparent overall model fit was low for both models, indicated by nagelkerke r2 values of 0.120 for the full model and 0.089 for the parsimonious model.

fig. 2
figure2
apparent calibration of the parsimonious model (before adjustment for overfitting). key: e:o = expected to observed ratio; ci = confidence interval; i-imi = index indirect muscle injury

full size image
figure 3 displays the decision-curve analysis. the nb of the parsimonious model was comparable to the treat-all strategy at risk thresholds up to 31%, marginally greater between 32 and 36% and exceeded the nb of either default strategies between 37 and 71%.

fig. 3
figure3
decision curve analysis for the parsimonious model (before adjustment for overfitting)

full size image
internal validation and adjustment for overfitting
table 4 shows the optimism-adjusted performance statistics for the parsimonious model, with full internal validation results shown in additional file 9. after adjustment for optimism, the overall model fit and the model’s discrimination performance deteriorated (nagelkerke r2 = 0.064; c-index = 0.589 (95% ci = 0.528 to 0.651). furthermore, bootstrapping suggested the model would be severely overfitted in new data (calibration slope = 0.718 (95% ci = 0.275 to 1.161)), so a shrinkage factor of 0.718 was applied to the parsimonious parameter estimates, and the model intercept re-estimated to produce our final model (table 4).

complete case and sensitivity analyses
the full and parsimonious models were robust to complete case analyses and excluding loans and transfers, with comparable apparent performance estimates. for the full models, the c-index range was 0.675 to 0.705, and nagelkerke r2 range was 0.135 to 0.178, whilst for the parsimonious models, the c-index range was 0.632 to 0.691, and nagelkerke r2 range was 0.102 to 0.154 (additional files 5, 6, 7, 8 and 9). the same prognostic factors were selected in all parsimonious models. the degree of estimated overfitting observed in the complete case and sensitivity analyses was comparable to that observed in the main analysis (calibration slope range = 0.678 to 0.715) (additional files 5, 6, 7, 8 and 9).

discussion
we have developed and internally validated a multivariable prognostic model to predict individualised i-imi risk during a season in elite footballers, using routinely, prospectively collected preseason phe and injury data that was available at manchester united football club. this is the only study that we know of that has developed a prognostic model for this purpose, so the results cannot be compared to previous work.

we included both a full model which did not include variable selection and a parsimonious model, which included a subset of variables that were statistically significant. the full model was included because overfitting is likely to increase when variable inclusion decisions are based upon p values. in addition, the use of p value thresholds for variable selection is somewhat arbitrary. however, the overfitting that could have arisen in the parsimonious model after using p values in this way was accounted for during the bootstrapping process, which replicated the variable selection strategy based on p values in each bootstrap sample.

the performance of the full and parsimonious models was similar, which means that utilising all candidate factors offered very little advantage over using two for making predictions. indeed, variable selection eliminated 8 candidate prognostic factors that had no clear evidence for an association with i-imis. our findings confirm previous suggestions that phe tests designed to measure modifiable physical and performance characteristics typically offer poor predictive value [10]. this may be because unless particularly strong associations are observed between a phe test and injury outcome, the overlap in scores between individuals who sustain a future injury and those who do not results in poor discrimination [10]. additionally, after measurement at a single timepoint (i.e. preseason), it is likely that the prognostic value of these modifiable factors may vary over time [48] due to training exposure, environmental adaptations and the occurrence of injuries [49].

the variable selection process resulted in a model which included only age and the frequency of previous imis within the last 3 years, which are simple to measure and routinely available in practice. our findings were similar to the modest association previously observed between age and hamstring imis in elite players [19]. however, whilst a positive previous hamstring imi history has a confirmed association with future hamstring imis [19], we found that for lower extremity i-imis, cumulative imi frequency was preferred to the time proximity of any previous imi as a multivariable prognostic factor. nevertheless, the weak prognostic strength of these factors explains the parsimonious model’s poor discrimination and low potential for clinical utility.

our study is the first to utilise decision-curve analysis to examine the clinical usefulness of a model for identifying players at high risk of imis and who may benefit from preventative interventions such as training load management, strength and conditioning or physiotherapy programmes. our parsimonious model demonstrated no clinical value at risk thresholds of less than 36%, because its nb was comparable to that of providing all players with an intervention. indeed, the only clinically useful thresholds that would indicate a high-risk player would be 37–71%, where the model’s nb was greater than giving all players an intervention. however, because of the high baseline imi risk in our population (approximately 44% of participant-seasons affected), the burden of imis [1,2,3,4,5] and the minimal costs [10] versus the potential benefits of such preventative interventions in an elite club setting, these thresholds are likely to be too high to be acceptable in practice. accordingly, it would be inappropriate to allocate or withhold interventions based upon our model’s predictions.

because of severe overfitting our parsimonious model was optimistic, which means that if used with new players, prediction performance is likely to be worse [39]. although our model was adjusted to account for overfitting and hence improve its calibration performance in new datasets, given the limitations in performance and clinical value, we cannot recommend that it is validated externally or used in clinical practice.

this study has some limitations. we acknowledge that the development of our model does not formally take account of the use of existing injury prevention strategies, including those informed by phe, and their potential effects on the outcome. rather, we predicted i-imis under typical training and match exposure and under routine medical care. in addition, it should be noted that injury risk predictions at an elite level football club may not generalise to other types of football clubs or sporting institutions, where ongoing injury prevention strategies may not be comparable in terms of application and equipment.

we measured candidate factors at one timepoint each season and assumed that participant-seasons were independent. whilst statistically complex, future studies may improve predictive performance and external validity by harnessing longitudinal measurements and incorporating between-season correlations.

we did not perform a competing risks analysis to account for players not being exposed to training and match play due to injuries other than i-imis. that is, our approach predicted the risk of i-imis in the follow up of players, allowing other injury types to occur and therefore possibly limiting the opportunity for i-imis during any rehabilitation period. the competing risk of the occurrence of non-imis was therefore not explicitly modelled and players remained in the risk set after a non-imi had occurred.

we also merged all lower extremity i-imis rather than using specific muscle group outcomes. although less clinically meaningful, this was necessary to maximise statistical power. nevertheless, our limited sample size prohibited examination of complex non-linear associations and only permitted a small number of candidates to be considered. a lack of known prognostic factors [19] meant that selection was mainly guided by data quality control processes and clinical reasoning, so it is possible that important factors were not included.

risk prediction improves when multiple factors with strong prognostic value are used [15]. therefore, future research should aim to identify novel prognostic factors, so that these can be used to develop models with greater potential clinical benefit. this may also allow updating of our model to improve its performance and clinical utility [50].

until the evidence base improves, and because of sample size limitations, it is likely that any further attempts to create a prognostic model at individual club level would suffer similar issues. importantly, this means that for any team, the value of using preseason phe data to make individualised predictions or to select bespoke injury prevention strategies remains to be demonstrated. however, the pooling of individual participant data from several participating clubs may increase sample sizes sufficiently to allow further model development studies [51], where a greater number of candidate factors could be utilised.

conclusion
using phe and injury data available preseason, we have developed and internally validated a prognostic model to predict i-imi risk in players at an elite club, using current methodological best practice. the paucity of known prognostic factors and data requirements for model building severely limited the model’s performance and clinical utility, so it cannot be recommended for external validation or use in practice. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.

<|EndOfText|>

temporal recalibration for improving prognostic model development and risk predictions in settings where survival is improving over time 

abstract
background
prognostic models are typically developed in studies covering long time periods. however, if more recent years have seen improvements in survival, then using the full dataset may lead to out-of-date survival predictions. period analysis addresses this by developing the model in a subset of the data from a recent time window, but results in a reduction of sample size.

methods
we propose a new approach, called temporal recalibration, to combine the advantages of period analysis and full cohort analysis. this approach develops a model in the entire dataset and then recalibrates the baseline survival using a period analysis sample.

the approaches are demonstrated utilizing a prognostic model in colon cancer built using both cox proportional hazards and flexible parametric survival models with data from 1996–2005 from the surveillance, epidemiology, and end results (seer) program database. comparison of model predictions with observed survival estimates were made for new patients subsequently diagnosed in 2006 and followed-up until 2015.

results
period analysis and temporal recalibration provided more up-to-date survival predictions that more closely matched observed survival in subsequent data than the standard full cohort models. in addition, temporal recalibration provided more precise estimates of predictor effects.

conclusion
prognostic models are typically developed using a full cohort analysis that can result in out-of-date long-term survival estimates when survival has improved in recent years. temporal recalibration is a simple method to address this, which can be used when developing and updating prognostic models to ensure survival predictions are more closely calibrated with the observed survival of individuals diagnosed subsequently.

prognostic models, temporal recalibration, period analysis, up-to-date survival predictions, flexible parametric survival models, cox proportional hazards models
topic: patient prognosisdatasets
issue section: original article
key messages
if survival has been improving over time, standard full cohort models can under-estimate survival.

period analysis uses a more recent subset of data to produce survival estimates which are more up-to-date, however it reduces the sample size and number of events used in the analysis.

temporal recalibration combines the sample size advantages associated with full cohort analysis with the up-to-date estimates produced with period analysis.

temporal recalibration can be used at the model development stage or to update existing prognostic models when new data becomes available.

introduction
for individuals diagnosed with a particular disease or health condition, prognostic models can provide outcome predictions and aid treatment decisions.1,2 in this article, we focus on the outcome of time-until-death from colon cancer and survival predictions, however the approach can be generalized. prognostic models contain multiple predictors and are typically developed using a regression format such as logistic, cox or a parametric survival model. it is often of interest to provide survival predictions at different time points, such as 1, 5 and 10 years after diagnosis. for 10-year predictions, it is necessary to have a model development dataset that includes individuals who were diagnosed at least 10 years ago, such that the analysis has sufficient follow-up length. however, this can lead to out-of-date (miscalibrated) survival predictions for recently diagnosed individuals if there have been improvements in survival over calendar time: e.g. in recent years treatment may have improved survival compared with 5 or 10 years earlier. improvements in survival for colorectal cancer have been reported in a number of different countries.3–6

with the development of online tools and apps, survival estimates from prognostic models have become more accessible. some models such as predict, a prognostic model for breast cancer,7 and qcancer, a prognostic model for colorectal cancer,8 are freely available online for both clinicians and the public. the survival estimates produced from these, and many other webtools, are from a standard full cohort analysis approach. such models may produce survival predictions that under-estimate the true survival probability of recently diagnosed patients (and conversely over-estimate the actual risk of adverse outcomes).

period analysis has been used in population-based cancer studies to obtain up-to-date estimates of survival9–12 and in this article we explore its use in the development and updating of prognostic models. period analysis defines a recent time window and only the risk-time and events that fall within this window contribute to the estimates of the hazard rates and predictor effects.13 this method is not commonly used for prognostic models, however keogh et al.14 produced survival predictions for cystic fibrosis patients using period analysis. a disadvantage with period analysis is that it results in a reduction of sample size for model development. this could be particularly problematic in small datasets, when there are rare predictor patterns or rare events, and may lead to a low number of events per predictor parameter, which increases the potential for model overfitting.15

in this article we introduce a new approach, called temporal recalibration, that combines the use of full cohort analysis, period analysis and recalibration methods. specifically it aims to maximize the use of data toward model development, with the full dataset used to model predictor effects and the baseline survival recalibrated in a recent time window to produce more up-to-date survival predictions for new individuals. we illustrate and compare these methods using an example of colon cancer from the surveillance, epidemiology, and end results (seer) program database.16

methods
cox proportional hazards models and post-estimation of the baseline
cox proportional hazards (ph) models are frequently used to develop prognostic models.17 the model is of the form:  
h(t;xi)=h0(t)eβxi 
with h(t;xi) the hazard function, h0(t) the baseline hazard function and βxi the prognostic index.18
the cumulative hazard function h(t;xi) must be approximated to calculate survival predictions as it is not directly modelled. this can be achieved post-estimation using a non-parametric approach, or by a smoother using fractional polynomials or splines.7,19 in this article, restricted cubic splines are used to create a smooth approximation of the log cumulative baseline hazard post-estimation. the same knot locations as the flexible parametric survival models (fpms) (see supplementary file 1, available as supplementary data at ije online) were used to ensure a fair comparison. the baseline survival curve was approximated by sˆ0(t)=e−h0ˆ(t)⁠, and survival predictions for individuals with different values of the prognostic index by sˆ(t;xi)=sˆ0(t)eβˆxi⁠.

it is possible to extend these models to include time-dependent predictor effects (i.e. non-proportional hazards). period analysis20 (see the period analysis section) can be performed using delayed entry techniques.

flexible parametric survival models
although cox models are widely used for prognostic modelling, fpms have several advantages. fpms directly model the log baseline cumulative hazard function which allows for smooth survival curves to be produced during model development, without the need for post-estimation smoothing.21 it remains straightforward to include time-dependent predictor effects22 and incorporate delayed entry. fpms use restricted cubic splines to directly model the baseline ln[h0(t;xi)] (see supplementary file 1, available as supplementary data at ije online). a prognostic model can be written in the following form where ζ(ln(t)|γ,k0) is the restricted cubic spline function and βxi is the prognostic index.23 
ln[h(t;xi)]=ζ(ln(t)|γ,k0)+βxi
period analysis
period analysis, in the context of population-based cancer data, was developed by brenner and gefeller.20 only individuals who contribute follow-up time during the period window are included in the analysis to estimate predictor effects and baseline survival (see table 1). this reduces the sample size since people who experienced the event before the window (e.g. participant b, see figure 1) are excluded. only the events that occur within the window are considered in the analysis and therefore the choice of window width is a balance between ensuring up-to-date survival estimates and having sufficient events (and events per predictor parameter). the width of the window could be determined by meeting the criteria defined by riley et al.15 further details and a sensitivity analysis of using different window widths are included in supplementary file 4, available as supplementary data at ije online.

figure 1
contribution of follow-up time from four hypothetical participants (diagnosed 1 january) to a 2-year period window of 2004–05.
open in new tabdownload slide
contribution of follow-up time from four hypothetical participants (diagnosed 1 january) to a 2-year period window of 2004–05.

table 1.
summary of the data used for the estimation of the baseline and predictor effects for each method

method	baseline	predictor effects
full cohort 	full 	full 
temporal recalibration 	recent 	full 
period analysis 	recent 	recent 
open in new tab
delayed entry techniques are used to left truncate the follow-up time of people diagnosed before the window so that the short-term hazard rates are only estimated from those diagnosed within or shortly before the period window (e.g. participant d, see figure 1).

this method has been shown to produce more up-to-date survival estimates than full cohort analysis in population-based cancer settings for many types of cancer in different countries9–12 and is used routinely within international cancer survival comparisons.3,24

temporal recalibration
a key disadvantage with period analysis is the reduction in sample size and number of events for model estimation. to address this, we propose temporal recalibration, which combines the sample size advantages associated with the full cohort analysis with the up-to-date predictions from period analysis.

the process of fitting a temporal recalibration model is as follows. (i) fit a survival model using the full cohort dataset to estimate the predictor effects using all individuals. (ii) recalibrate the model by re-estimating the baseline using the subset of individuals from a period analysis sample, while holding the predictor effect estimates from step (i) fixed.

recalibrating the baseline in a recent period analysis sample allows for improvements in survival to be captured and leads to more up-to-date predictions. under proportional hazards the model can be written in the following form for fpms:  
ln[hnew(t;xi)]=ζnew(ln(t)|γ,k0)+offset(pii)
where ζnew(ln(t)|γ,k0) is the updated spline function for the log cumulative baseline hazard function estimated in the recent period data, k0 are the knot locations from the full cohort model and offset(pii) is the prognostic index estimated from the full cohort model as an offset term. fixing the predictor effects with constraints when fitting in the period analysis sample would offer an equivalent approach.
for a cox ph model it can be written as:  
hnew(t;xi)=h0new(t)eoffset(pii) 
where hnew(t;xi) and h0new(t) are the hazard and baseline hazard functions respectively, estimated on the recent time window, and offset(pii) is the prognostic index estimated from the full cohort model as an offset term.
as with period analysis, the choice of the window width is a bias-variance trade-off (see supplementary file 4, available as supplementary data at ije online). the width of the window could possibly be reduced compared with a standard period analysis approach as it is only necessary to have a sufficient number of events to estimate the baseline (and not the predictor effects). in temporal recalibration we explicitly assume the predictor effects are the same as they were in the full cohort model (see table 1).

assessing the performance of predictions
marginal survival (i.e. average across all individuals) can be calculated both within-sample (i.e. in the same dataset used to develop the model) and out-of-sample (i.e. in new individuals) by calculating every individual’s predicted survival over time, and then averaging the survival curves:25 
sˆ¯¯¯(t)=1n∑ni=1sˆ(t;xi)
out-of-sample marginal survival predictions can be compared with the observed survival (kaplan–meier estimates) to determine the calibration of a model’s survival predictions for a new group of individuals.

studying the marginal survival only assesses how well the model performs on average (sometimes referred to as calibration-in-the-large1,26), whereas calibration plots can be used to determine the model’s performance in different risk groups at particular time points. in this article the risk groups were defined by dividing the prognostic index from the full cohort models into 10 equally sized risk groups.

the e/o statistic quantifies calibration-in-the-large by comparing predicted or expected (e) outcome risk to the observed (o) risk through eo(t)=1−s exp (t)1−sobs(t)⁠. e is calculated from the marginal survival prediction from the model [sexp(t)] and o is from the observed kaplan–meier curve [sobs(t)]. a value of 1 indicates agreement1,27

harrell’s c-index can be used to assess the concordance of survival predictions from proportional hazards models. a value of 1 indicates perfect concordance.28

we now compare full cohort, temporal recalibration and period analysis approaches using an illustrative example of colon cancer.

example
data
we used the public-access seer database from the usa.16 the seer program covers ∼34% of the us population and collects population-based data on all reported cases of cancer within the cancer registries included in the seer program.29 the analysis was restricted to adults who were aged 18–99 years at the time of their diagnosis of colon cancer (icd10 codes c18.0–c18.9). if there were any duplicates of the patient id, only the first record was retained. patients with an unknown survival time (recorded to the nearest month) or incomplete dates for their diagnosis or death were also excluded. data from 1996–2015 were available for this analysis. as the aim was to identify which model gave better long-term survival predictions in new data, the data were split at 2005 for illustration purposes. data from 1996–2005 were used to develop the models and a 2 year period window from 1 january 2004 to 31 december 2005 was used to fit the temporal recalibration and period analysis models. the data from 2006–15 were then used to validate the models. baseline characteristics for the development dataset can be found in table 2.


table 2.
baseline characteristics of the 48 861 participants in the development dataset once participants with missing predictor values were removed. mean (sd) is presented for continuous variables and n (%) for categorical variables

variable	mean (sd) or n (%)
age 	70.1 (13.0) 
sex 	 
 male 	23 674 (48.5%) 
 female 	25 187 (51.5%) 
race 	 
 white 	42 296 (86.6%) 
 black 	6565 (13.4%) 
stage at diagnosis 	 
 stage 1 	18 469 (37.8%) 
 stage 2 	21 529 (44.1%) 
 stage 3 	8863 (18.1%) 
grade of tumour at diagnosis 	 
 grade 1 	5496 (11.2%) 
 grade 2 	32 992 (67.5%) 
 grade 3 	9871 (20.2%) 
 grade 4 	502 (1.0%) 
open in new tab
models
cause-specific cox and fpms were fitted, meaning that deaths due to causes other than colon cancer were censored. age at diagnosis, stage at diagnosis (localized, regional, distant), grade of the tumour (i–iv), sex and race (restricted to white and black patients only) were included as predictors. age was modelled using restricted cubic splines with three degrees of freedom, and stage, grade, sex and race were modelled categorically. all predictors were forced to be included (i.e. there was no variable selection). for the fpms, five degrees of freedom were used to model the log baseline cumulative hazard and, to simplify the process of recalibration, the baseline splines were not orthogonalized. example code used to fit these models is provided in supplementary file 2, available as supplementary data at ije online. in this illustrative example, any participants with missing predictor values were excluded in order to more easily compare the approaches, though in practice multiple imputation is usually preferable.

age at diagnosis was winsorized30 to provide more stability in the extremes by adding an additional constraint forcing the splines to be constant for the top and bottom 2% of the age distribution.31 in further analyses, the ph assumption was relaxed using time-dependent predictor effects for age and stage. to compare the model predictions from these three approaches, the marginal predicted survival for the 5601 patients diagnosed in 2006 was calculated using each model and compared with the observed kaplan–meier estimates. this was further assessed through calibration plots at 10 years after diagnosis.

all analyses were performed using stata version 15.0.32 fpms were fitted using the user-written package stpm233 and harrell’s c-index was calculated for these models using the user-written package stcstat2.34

results
in terms of predictor effect estimates, the log hazard ratios and standard errors were very similar regardless of whether cox models or fpms were used (table 3). the log hazard ratios were fairly similar for full cohort and period analysis, however the standard errors from the period analysis approaches were around twice as large due to the reduction in sample size. overfitting was minimial due to the large number of events relative to the number of predictor parameters, highlighted by a uniform shrinkage factor35 for the full cohort model of 0.999.


table 3.
comparison of the sample size, number of events, log hazard ratios (hr) and standard errors (s.e.) of the log hazard ratios for the categorical predictors in each model

flexible parametric survival model
cox proportional hazards model
full cohort	period analysis	full cohort	period analysis
sample size 	48 861 	33 197 	48 861 	33 197 
number of events 	12 040 	2900 	12 040 	2900 
predictor effects: log hr (s.e. of log hr) 	female 	−0.05 (0.018) 	−0.10 (0.038) 	−0.05 (0.018) 	−0.10 (0.038) 
black 	0.24 (0.025) 	0.27 (0.051) 	0.24 (0.025) 	0.27 (0.051) 
stage 2 	1.15 (0.031) 	1.18 (0.060) 	1.15 (0.031) 	1.18 (0.060) 
stage 3 	2.98 (0.031) 	2.92 (0.062) 	2.96 (0.031) 	2.90 (0.062) 
grade 2 	0.22 (0.039) 	0.09 (0.073) 	0.22 (0.039) 	0.10 (0.073) 
grade 3 	0.68 (0.041) 	0.55 (0.078) 	0.67 (0.041) 	0.54 (0.078) 
grade 4 	0.81 (0.088) 	0.78 (0.146) 	0.79 (0.088) 	0.75 (0.146) 
open in new tab
similar marginal survival predictions were produced regardless of whether cox or fpms were used when predicting for patients diagnosed in 2006 (figure 2). the marginal survival predictions for temporal recalibration and period analysis were very similar and consistently provided more well-calibrated estimates than the standard full cohort model. the survival probability is under-estimated for all risk groups in the full cohort analysis models, and in 9 of the 10 groups the predictions are the furthest from the reference line. however, using temporal recalibration, all the predicted survival estimates increase and agree more closely with the kaplan–meier estimates. although the marginal survival predictions from the temporal recalibration and period analysis models are very similar, small differences in predicted survival can be seen for the highest risk groups. including time-dependent effects for age and stage in the fpm improves the calibration in the third highest risk group, however there is very little difference in the marginal survival estimates, see supplementary file 3, available as supplementary data at ije online.

figure 2
external validation of the models to assess the calibration of survival predictions for new patients (diagnosed in 2006 with follow-up data until 2015). top: comparison of marginal observed (kaplan–meier) and predicted survival from each model. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. bottom: 10-year calibration plots comparing the observed and predicted cancer-specific survival probabilities from each model.
open in new tabdownload slide
external validation of the models to assess the calibration of survival predictions for new patients (diagnosed in 2006 with follow-up data until 2015). top: comparison of marginal observed (kaplan–meier) and predicted survival from each model. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. bottom: 10-year calibration plots comparing the observed and predicted cancer-specific survival probabilities from each model.


a comparison of the model performance in terms of calibration and concordance of survival predictions is displayed in table 4. calibration improves by 0.02 by performing temporal recalibration which is large at the population level and improves the net benefit of the model.36 in other scenarios, the difference may be greater if there have been more substantial changes in baseline survival over calendar time. as the predictor effects for the temporal recalibration models are constrained to be the same as those from the full cohort model, harrell’s c-index will always be the same for these models. in this example, the predictor effects for the period analysis models were also very similar and therefore harrell’s c-index is the same to three decimal places.



table 4.
comparison of model performance in the validation dataset. the difference in observed and predicted marginal survival at 10 years after diagnosis [sobs(10) – sexp(10)], the ratio of expected to observed risk at 10 years after diagnosis (e/o) and harrell’s c-index

model	sobs(10) – sexp(10)a	eo(10)	harrell’s c-index
full cohort: fpm 	0.056 	1.169 	0.788 
full cohort: cox 	0.051 	1.155 	0.788 
temporal recalibration: fpm 	0.031 	1.094 	0.788 
temporal recalibration: cox 	0.031 	1.095 	0.788 
period analysis: fpm 	0.032 	1.098 	0.788 
period analysis: cox 	0.033 	1.101 	0.788 
a
sobs(10), kaplan–meier estimate at 10 years after diagnosis; sexp(10), 10 year marginal survival prediction from the model.

open in new tab
updating prognostic models
temporal recalibration can also be used to produce up-to-date survival estimates when new data become available by simply re-estimating the baseline without the need for repeating the model-building process or re-estimating the predictor effects. this is akin to previous work by riley et al.,37 schuetz et al.38 and steyerberg26 that show how recalibrating the baseline hazard in new (local) settings can be important. to illustrate this, prognostic models were fitted using fpms with data from 1986–95, and data from 1996–2005 was used to update these models. as stage was only available from 1995 onwards, only age, sex, race and grade were included as predictors, and for simplicity phs was assumed. table 5 defines the models m1-m6 that were compared in this analysis.


table 5.
comparison of the data used to estimate the predictor effects and baseline of each flexible parametric survival model

model	description	data for predictor effects	data for baseline
m1 	original full cohort model 	1986–95 	1986–95 
m2 	full cohort model with all available data 	1986–2005 	1986–2005 
m3 	full cohort model with most recent data 	1996–2005 	1996–2005 
m4 	temporal recalibration of m1 	1986–95 	period window 2004–05 
m5 	temporal recalibration of m3 	1996–2005 	period window 2004–05 
m6 	period analysis 	period window 2004–05 	period window 2004–05 
open in new tab
to illustrate the difference in survival predictions for these models, 10-year survival was estimated for patients diagnosed in 2006 and compared with the kaplan–meier estimates for these patients, see figure 3.

figure 3
comparison of marginal observed (kaplan–meier curve) and predicted survival from the original and updated models. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly.
open in new tabdownload slide
comparison of marginal observed (kaplan–meier curve) and predicted survival from the original and updated models. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly.


using the original model (m1) resulted in a difference between the observed and predicted survival of 0.12, which was reduced to 0.08 by using a longer timespan (m2) and 0.05 by using a more recent cohort (m3). temporal recalibration models (m4 and m5) and the period analysis model (m6) produced the closest estimates which differed by <0.02. performing temporal recalibration improves the calibration of the full cohort models by at least 0.03 and a larger improvement of >0.10 is observed when recalibrating the original full cohort model. despite different models being temporally recalibrated, the predictions overlaid exactly. this demonstrates that temporal recalibration is appropriate in this example since the predictor effects do not greatly change over time and therefore it is only necessary to re-estimate the baseline.

discussion
often there are large underlying improvements in survival over the follow-up available in a model development dataset, which presents a challenge for subsequently making predictions for newly diagnosed patients. we have shown that survival predictions from prognostic models developed using a standard full cohort approach underestimate survival of recently diagnosed patients. however, more up-to-date, and thus accurate, survival predictions can be produced by developing prognostic models using temporal recalibration, where the baseline hazard is recalibrated in a subset of most recent data. this idea is similar to the approach of period analysis, but has the additional benefit of more precisely estimating predictor effects as it uses all the data to estimate the prognostic index.

unlike period analysis, it is possible to directly apply temporal recalibration to a range of existing prognostic models (i.e. cox ph models, fpms with time-dependent effects) to update the survival predictions without the need of repeating the model-building process or re-estimating predictor effects. no additional data are required, only a period analysis sample of the most recent data is needed to re-estimate the baseline and produce more up-to-date predictions which better reflect the survival of those currently being diagnosed. we have also shown the importance of regularly updating prognostic models when new data become available and how this can easily be achieved using temporal recalibration.

we have used seer public use data for colon cancer patients, with a range of predictors in order to illustrate the approach. for cancer sites and settings with smaller improvements over calendar time, the predicted survival estimates from a standard and temporally recalibrated approach would differ less. however, the approach would still be valid in this case. in this example we only showed complete case analysis, however, temporal recalibration could be performed on imputed datasets and the survival predictions from the models could be combined using rubin’s rules.39,40 example code for fitting these models is included in supplementary file 2, available as supplementary data at ije online.

temporal recalibration assumes that the predictor effects are the same in the recent data as in the full cohort and therefore do not change as a function of diagnosis date. this is in contrast to period analysis which updates both the baseline and the prognostic index. therefore, the parameter estimates from the full cohort and period analysis models can be informally compared to verify that this assumption is plausible. further, careful consideration should be given to the consistency of predictor’s values over time, but this is an issue generally and not specific to the approach we outline here.

temporal recalibration is a similar concept to model updating,41 in which the calibration of predictions from a previously developed prognostic model are externally validated using new data obtained from a more recent time point. in that setting, if the model consistently under or over predicts survival, it is recalibrated; typically predictor effects are kept fixed (i.e. as originally estimated), but the baseline is updated. the difference with temporal recalibration is that the period analysis sample (used for the recalibration) is not a separate dataset and has already been included in the full cohort model to estimate predictor effects.

an alternative to temporal recalibration and period analysis would be to model the year of diagnosis directly and then predict survival using the most recent year included in the model. this approach would make developing and updating existing prognostic models more challenging as it would require the year of diagnosis to be modelled appropriately, which may include time-dependent effects and non-linear terms. this method would also rely more heavily on extrapolation of effects when producing long-term survival predictions for the most recent calendar year. however, with temporal recalibration the long-term hazards are estimated directly from those included in the period window. with both temporal recalibration and modelling the year of diagnosis it may be necessary to consider interactions between predictor effects and year of diagnosis.42

many existing prognostic models use the standard full cohort approach. we have illustrated that using temporal recalibration could update these survival predictions and be a more accurate reflection of the prognosis of patients who are currently being diagnosed.

<|EndOfText|>

calculating the sample size required for developing a clinical prediction model

summary points
patients and healthcare professionals require clinical prediction models to accurately guide healthcare decisions
larger sample sizes lead to the development of more robust models
data should be of sufficient quality and representative of the target population and settings of application
it is better to use all available data for model development (ie, avoid data splitting), with resampling methods (such as bootstrapping) used for internal validation
when developing prediction models for binary or time-to-event outcomes, a well known rule of thumb for the required sample size is to ensure at least 10 events for each predictor parameter
the actual required sample size is, however, context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the model
we propose to use such information to tailor sample size requirements to the specific setting of interest, with the aim of minimising the potential for model overfitting while targeting precise estimates of key parameters
our proposal can be implemented in a four step procedure and is applicable for continuous, binary, or time-to-event outcomes
the pmsampsize package in stata or r allows researchers to implement the procedure
clinical prediction models are needed to inform diagnosis and prognosis in healthcare.123 well known examples include the wells score,45 qrisk,67 and the nottingham prognostic index.89 such models allow health professionals to predict an individual’s outcome value, or to predict an individual’s risk of an outcome being present (diagnostic prediction model) or developed in the future (prognostic prediction model). most prediction models are developed using a regression model, such as linear regression for continuous outcomes (eg, pain score), logistic regression for binary outcomes (eg, presence or absence of pre-eclampsia), or proportional hazards regression models for time-to-event data (eg, recurrence of venous thromboembolism).10 an equation is then produced that can be used to predict an individual’s outcome value or outcome risk conditional on his or her values of multiple predictors, which might include basic characteristics such as age, weight, family history, and comorbidities; biological measurements such as blood pressure and biomarkers; and imaging or other test results. supplementary material s1 shows examples of regression equations.

developing a prediction model requires a development dataset, which contains data from a sample of individuals from the target population, containing their observed predictor values (available at the intended moment of prediction11) and observed outcome. the sample size of the development dataset must be large enough to develop a prediction model equation that is reliable when applied to new individuals in the target population. what constitutes an adequately large sample size for model development is, however, unclear,12 with various blanket “rules of thumb” proposed and debated.1314151617 this has created confusion about how to perform sample size calculations for studies aiming to develop a prediction model.

in this article we provide practical guidance for calculating the sample size required for the development of clinical prediction models, which builds on our recent methodology papers.1314151618 we suggest that current minimum sample size rules of thumb are too simplistic and outline a more scientific approach that tailors sample size requirements to the specific setting of interest. we illustrate our proposal for continuous, binary, and time-to-event outcomes and conclude with some extensions.

moving beyond the 10 events per variable rule of thumb
in a development dataset, the effective sample size for a continuous outcome is determined by the total number of study participants. for binary outcomes, the effective sample size is often considered about equal to the minimum of the number of events (those with the outcome) and non-events (those without the outcome); time-to-event outcomes are often considered roughly equal to the total number of events.10 when developing prediction models for binary or time-to-event outcomes, an established rule of thumb for the required sample size is to ensure at least 10 events for each predictor parameter (ie, each β term in the regression equation) being considered for inclusion in the prediction model equation.192021 this is widely referred to as needing at least 10 events per variable (10 epv). the word “variable” is, however, misleading as some predictors actually require multiple β terms in the model equation—for example, two β terms are needed for a categorical predictor with three categories (eg, tumour grades i, ii, and iii), and two or more β terms are needed to model any non-linear effects of a continuous predictor, such as age or blood pressure. the inclusion of interactions between two or more predictors also increases the number of model parameters. hence, as prediction models usually have more parameters than actual predictors, it is preferable to refer to events per candidate predictor parameter (epp). the word candidate is important, as the amount of model overfitting is dictated by the total number of predictor parameters considered, not just those included in the final model equation.

the rule of at least 10 epp has been widely advocated perhaps as a result of its simplicity, and it is regularly used to justify sample sizes within published articles, grant applications, and protocols for new model development studies, including by ourselves previously. the most prominent work advocating the rule came from simulation studies conducted in the 1990s,192021 although this work actually focused more on the bias and precision of predictor effect estimates than on the accuracy of risk predictions from a developed model. the adequacy of the 10 epp rule has often been debated. although the rule provides a useful starting point, counter suggestions include either lowering the epp to below 10 or increasing it to 15, 20, or even 50.102223242526 these inconsistent recommendations reflect that the required epp is actually context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the model.1314151617 this finding is unsurprising as sample size considerations for other study designs, such as randomised trials of interventions, are all context dependent and tailored to the setting and research question. rules of thumb have also been advocated in the continuous outcome setting, such as two participants per predictor,27 but these share the same concerns as for 10 epp.16

sample size calculation to ensure precise predictions and minimise overfitting
recent work by van smeden et al1314 and riley et al1516 describe how to calculate the required sample size for prediction model development, conditional on the user specifying the overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit (r2). these authors’ approaches can be implemented in a four step procedure. each step leads to a sample size calculation, and ultimately the largest sample size identified is the one required. we describe these four steps, and, to aid general readers, provide the more technical details of each step in the figures.

step 1: what sample size will produce a precise estimate of the overall outcome risk or mean outcome value?
fundamentally, the sample size must allow the prediction model’s intercept to be precisely estimated, to ensure that the developed model can accurately predict the mean outcome value (for continuous outcomes) or overall outcome proportion (for binary or time-to-event outcomes). a simple way to do this is to calculate the sample size needed to precisely estimate (within a small margin of error) the intercept in a model when no predictors are included (the null model).15figure 1 shows the calculation for binary and time-to-event outcomes, and we generally recommend aiming for a margin of error of ≤0.05 in the overall outcome proportion estimate. for example, with a binary outcome that occurs in half of individuals, a sample size of at least 385 people is needed to target a confidence interval of 0.45 to 0.55 for the overall outcome proportion, and thus an error of at most 0.05 around the true value of 0.5. to achieve the same margin of error with outcome proportions of 0.1 and 0.2, at least 139 and 246 participants, respectively, are required.

fig 1
fig 1
calculation of sample size required for precise estimation of the overall outcome probability in the target population

download figure open in new tab download powerpoint
for time-to-event outcomes, a key time point needs to be identified, along with the anticipated outcome event rate. for example, with an anticipated event rate of 10 per 100 person years of the entire follow-up, the sample size must include a total of 2366 person years of follow-up to ensure an expected margin of error of ≤0.05 in the estimate of a 10 year outcome probability of 0.63, such that the expected confidence interval is 0.58 to 0.68.

for continuous outcomes, the anticipated mean and variance of outcome values must be prespecified, alongside the anticipated percentage of variation explained by the prediction model (see supplementary material s2 for details).16

step 2: what sample size will produce predicted values that have a small mean error across all individuals?
in addition to predicting the average outcome value precisely (see step 1), the sample size for model development should also aim for precise predictions across the spectrum of predicted values. for binary outcomes, van smeden et al use simulation across a wide range of scenarios to evaluate how the error of predicted outcome probabilities from a developed model depends on various characteristics of the development dataset sampled from a target population.14 they found that the number of candidate predictor parameters, total sample size, and outcome proportion were the three main drivers of a model’s mean predictive accuracy. this led to a sample size formula (fig 2) to help ensure that new prediction models will, on average, have a small prediction error in the estimated outcome probabilities in the target population (as measured by the mean absolute prediction error, mape). the calculation requires the number of candidate predictor parameters and the anticipated outcome proportion in the target population to be prespecified. for example, with 10 candidate predictor parameters and an outcome proportion of 0.3, a sample size of at least 461 participants and 13.8 epp is required to target a mean absolute error of 0.05 between observed and true outcome probabilities (see fig 2 for calculation). the calculation is available as an interactive tool (https://mvansmeden.shinyapps.io/beyondepv/) and applicable to situations with 30 or fewer candidate predictors. ongoing work aims to extend to larger numbers of candidate predictors and also to time-to-event outcomes.

fig 2
fig 2
sample size required to help ensure a developed prediction model of a binary outcome will have a small mean absolute error in predicted probabilities when applied in other targeted individuals

download figure open in new tab download powerpoint
for continuous outcomes, accurate predictions across the spectrum of predicted values require the standard deviation of the residuals to be precisely estimated.1016 supplementary material s3 shows that to target a less than 10% multiplicative error in the estimated residual standard deviation, the required sample size is simply 234+p, where p is the number of predictor parameters considered.

step 3: what sample size will produce a small required shrinkage of predictor effects?
our third recommended step is to identify the sample size required to minimise the problem of overfitting.28 overfitting is when a developed model’s predictions are more extreme than they ought to be for individuals in a new dataset from the same target population. for example, an overfitted prediction model for a binary outcome will give a predicted outcome probability too close to 1 for individuals with a higher than the average outcome probability and too close to 0 for individuals with a lower than the average outcome probability. overfitting notably occurs when the sample size is too small. in particular, when the number of candidate predictor parameters is large relative to the number of participants in total (for continuous outcomes) or to the number of participants with the outcome event (for binary or time-to-event outcomes). a consequence of overfitting is that a developed model’s apparent predictive performance (as observed in the development dataset itself) will be optimistic (ie, too high), and its actual predictive performance in new data from the same target population will be lower (ie, worse).

shrinkage (also known as penalisation or regularisation) methods deal with the problem of overfitting by reducing the variability in the developed model’s predictions such that extreme predictions (eg, predicted probabilities close to 0 or 1) are pulled back toward the overall average.293031323334 however, there is no guarantee that shrinkage will fully overcome the problem of overfitting when developing a prediction model. this is because the shrinkage or penalty factors (which dictate the magnitude of shrinkage required) are also estimated from the development dataset and, especially when the sample size is small, are often imprecise and so fail to tackle the magnitude of overfitting correctly in a particular application.30 furthermore, a negative correlation tends to occur between the estimated shrinkage required and the apparent performance of a model. if the apparent model performance is excellent simply by chance, the required shrinkage is typically estimated too low.30 thus, ironically, in those situations when overfitting is of most concern (and thus shrinkage is most urgently needed), the prediction model developer has insufficient assurance in selecting the proper amount of shrinkage to cancel the impact of overfitting.

riley et al therefore suggest identifying the sample size and number of candidate predictors that correspond to a small amount of desired shrinkage (≤10%) during model development.1516 the sample size calculation (fig 3) requires the researcher to prespecify the number of candidate predictor parameters and, for binary or time-to-event outcomes, the anticipated outcome proportion or rate, respectively, in the target population. in addition, a (conservative) value for the anticipated model performance is required, as defined by the cox-snell r squared statistic (r2cs).1535 the anticipated value of r2cs is important because it reflects the signal:noise ratio, which has an impact on the estimation of multiple parameters and the potential for overfitting. when the signal:noise ratio is anticipated to be high (eg, r2cs is close to 1 for a prediction model with a continuous outcome), true patterns are easier to detect and so overfitting is less of a concern, such that more predictor parameters can be estimated. however, when the signal:noise ratio is low (ie, r2cs is anticipated to be close to 0), true patterns are harder to identify and there is more potential for overfitting, such that fewer predictor parameters can be estimated reliably.

fig 3
fig 3
how to calculate the sample size needed to target a small magnitude of required shrinkage of predictor effects (to minimise potential model overfitting) for binary or time-to-event outcomes

download figure open in new tab download powerpoint
in the continuous outcome setting, r2cs is simply the coefficient of determination r2, which quantifies the proportion of the variance of outcome values that is explained by the prediction model and thus is between 0 and 1. for example, when developing a prediction model for a continuous outcome with up to 30 predictor parameters and an anticipated r2cs of 0.7, a sample size of 206 participants is required to ensure the expected shrinkage is 10% (see supplementary material s4 for full calculation). this corresponds to about seven participants for each predictor parameter considered.

the r2cs statistic generalises to non-continuous outcomes and allows sample size calculations to minimise the expected shrinkage when developing a prediction model for binary and time-to-event outcomes (fig 3). for example, when developing a new logistic regression model with up to 20 candidate predictor parameters and an anticipated r2cs of at least 0.1, a sample size of 1698 participants is required to ensure the expected shrinkage is 10% (see fig 3 for full calculation). if the target setting has an outcome proportion of 0.3, this corresponds to an epp of 25.5. the required sample size and epp are sensitive to the choice of r2cs, with lower anticipated values of r2cs leading to higher required sample sizes. therefore, a conservative choice of r2cs is recommended (fig 4).

fig 4
fig 4
how to decide on the model’s anticipated r2cs in advance of data collection

download figure open in new tab download powerpoint
as in sample size calculations for randomised trials evaluating intervention effects, external evidence and expert opinion are required to inform the values that need specifying in the sample size calculator. figure 4 provides guidance for specifying r2cs. importantly, unlike for continuous outcomes when r2cs is bounded between 0 and 1, the r2cs is bounded between 0 and max(r2cs) for binary and time-to-event outcomes. the max(r2cs) denotes the maximum possible value of r2cs, which is dictated by the overall outcome proportion or rate in the development dataset and is often much less than 1. supplementary material s5 shows the calculation of max(r2cs). for logistic regression models with outcome proportions of 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, and 0.01, the corresponding max(r2cs) values are 0.75, 0.74, 0.71, 0.63, 0.48, 0.33, and 0.11, respectively. thus the anticipated r2cs might be small, even for a model with potentially good performance.

step 4: what sample size will produce a small optimism in apparent model fit?
the sample size should also ensure a small difference in the developed models apparent and optimism adjusted values of r2nagelkerke (ie, r2cs/max(r2cs)), as this is a fundamental overall measure of model fit.1038 the apparent r2nagelkerke value is simply the model’s observed performance in the same data as used to develop the model, whereas the optimism adjusted r2nagelkerke value is a more realistic (approximately unbiased) estimate of the model’s fit in the target population. the sample size calculations are shown in supplementary material s6 for continuous outcomes and in figure 5 for binary and time-to-event outcomes. as before, they require the user to specify the anticipated r2cs and the max(r2cs), as described in figure 4. for example, when developing a logistic regression model with an anticipated r2cs of 0.2, and in a setting with an outcome proportion of 0.05 (such that the max(r2cs) is 0.33), 1079 participants are required to ensure the expected optimism in the apparent r2nagelkerke is just 0.05 (see figure 5 for calculation).

fig 5
fig 5
how to calculate the sample size needed to target a small optimism in model fit (to minimise potential model overfitting) for binary and time-to-event outcomes

download figure open in new tab download powerpoint
recommendations and software
box 1 summarises our recommended steps for calculating the minimum sample size required for prediction model development. this involves four calculations for binary outcomes (b1 to b4), three for time-to-event outcomes (t1 to t3), and four for continuous outcomes (c1 to c4). to implement the calculations, we have written the pmsampsize package for stata and r. the software calculates the sample size needed to meet all the criteria listed in box 1 (except b2, which is available at https://mvansmeden.shinyapps.io/beyondepv/), conditional on the user inputting values of required parameters such as the number of candidate predictors, the anticipated outcome proportion in the target population, and the anticipated r2cs. the calculations are especially helpful when prospective data collection (eg, new cohort study) are required before model development; however, they are also relevant when existing data are available to guide the number of predictors that can be considered.

box 1 recommendations for calculating the sample size needed when developing a clinical prediction model for continuous, binary, and time-to-event outcomes
to increase the potential for developing a robust prediction model, the sample size should be at least large enough to minimise model overfitting and to target sufficiently precise model predictions
binary outcomes
for binary outcomes, ensure the sample size is enough to:
estimate the overall outcome proportion with sufficient precision (use equation in figure 1) (b1)
target a small mean absolute prediction error (use equation in figure 2, if number of predictor parameters is ≤30) (b2)
target a shrinkage factor of 0.9 (use equation in figure 3) (b3)
target small optimism of 0.05 in the apparent r2nagelkerke (use equation in figure 5) (b4)
time-to-event outcomes
for time-to-event outcomes, ensure the sample size is enough to:
estimate the overall outcome proportion with sufficient precision at one or more key time-points in follow-up (use equation in figure 1) (t1)
target a shrinkage factor of 0.9 (use equation in figure 3) (t2)
target small optimism of 0.05 in the apparent r2nagelkerke (use equation in figure 5) (t3)
continuous outcomes
for continuous outcomes, ensure the sample size is enough to:
estimate the model intercept precisely (see supplementary material 1) (c1)
estimate the model residual variance with sufficient precision (see supplementary material 2) (c2)
target a shrinkage factor of 0.9 (use equation in figure 3) (c3)
target small optimism of 0.05 in the apparent r2nagelkerke (use equation in figure 5) (c4)
these approaches require researchers to specify the anticipated overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit (r2cs). when the choice of values is uncertain, we generally recommend being conservative and so taking those values (eg, smallest r2cs) that give larger sample sizes
when an existing dataset is already available (such that sample size is already defined), the calculations can be used to identify if the sample size is sufficient to estimate the overall outcome risk or the mean outcome value, and how many predictor parameters can be considered before overfitting becomes a concern
applied examples
we now illustrate the recommendations in box 1 by using three examples.

example 1: binary outcome
north et al developed a model predicting pre-eclampsia in pregnant women based on clinical predictors measured at 15 weeks’ gestation,43 including vaginal bleeding, age, previous miscarriage, family history, smoking, and alcohol consumption. the model included 13 predictor parameters and had a c statistic of 0.71. emerging research aims to improve this and other pre-eclampsia prediction models by including additional predictors (eg, biomarkers and ultrasound measurements).

as the outcome is binary, the sample size calculation for a new prediction model needs to examine criteria b1 to b4 in box 1. this requires us to input the overall proportion of women who will develop pre-eclampsia (0.05) and the number of candidate predictor parameters (assumed to be 30 for illustration). for an outcome proportion of 0.05, the max(r2cs) value is 0.33 (see supplementary material s5). if we assume, conservatively, that the new model will explain 15% of the variability, the anticipated r2cs value is 0.15×0.33=0.05. now we can check criteria b1, b3, and b4 by typing in stata:

pmsampsize, type(b) rsquared(0.05) parameters(30) prevalence(0.05)
this indicates that at least 5249 women are required, corresponding to 263 events and an epp of 8.75. this is driven by criterion b3, to ensure the expected shrinkage required is just 10% (to minimise the potential overfitting). to check criterion b2 in box 1, we can apply the formula in figure 2. this suggests that 544 women are needed to target a mean absolute error in predicted probabilities of ≤0.05. this is much lower than the 5249 women needed to meet criterion b3.

if recruiting 5249 women is impractical (eg, because of time, cost, or practical constraints for data collection), the sample size required can be reduced by identifying a smaller number of candidate predictors (eg, based on existing evidence from systematic reviews44). for example, with 20 rather than 30 candidate predictors, the required sample size to meet all four criteria is at least 3500 women and 175 events (still 8.75 epp).

example 2: time-to-event outcome
many prognostic models are available for the risk of a recurrent venous thromboembolism (vte) after cessation of treatment for a first vte.45 for example, the model of ensor et al included predictors of age, sex, site of first clot, d-dimer level, and the lag time from cessation of treatment until measurement of d-dimer (often around 30 days).46 the model’s c statistic was 0.69 and the adjusted r2cs was 0.051 (corresponding to 8% of the total variation). emerging research aims to extend such models by including additional predictors.

the sample size required for a new model must at least meet criteria t1 to t3.15 this requires us to input a key time point for prediction of vte recurrence risk (eg, two years), alongside the number of candidate predictor parameters (n=30), the anticipated mean follow-up (2.07 years), and outcome event rate (0.065, or 65 vte recurrences for every 1000 person years of follow-up), and the conservative value of r2cs (0.051), with all chosen values based on ensor et al.46 now criteria t1 to t3 can be checked, for example by typing in stata:

pmsampsize, type(s) rsquared(0.051) parameters(30) rate(0.065) timepoint(2) meanfup
(2.07)
this indicates that at least 5143 participants are required, corresponding to 692 events and an epp of 23.1. this is considerably more than 10 epp, and is driven by a desired shrinkage factor (criterion t2) of only 10% to minimise overfitting based on just 8% of variation explained by the model. if the number of candidate predictor parameters is lowered to 20, the required sample size is reduced to 3429 (still an epp of 23.1).

example 3: continuous outcome
hudda et al developed a prediction model for fat free mass in children and adolescents aged 4 to 15 years, including 10 predictor parameters based on height, weight, age, sex, and ethnicity.47 the model is needed to provide an estimate of an individual’s current fat mass (=weight minus predicted fat free mass). on external validation, the model had an r2cs of 0.90. let us assume that the model will need updating (eg, in 10 years owing to changes in the population behaviour and environment), and that an additional 10 predictor parameters (and thus a total of 20 parameters) will need to be considered in the model development.

the sample size for a model development dataset must at least meet the four criteria of c1 to c4 in box 1. this requires us to specify the anticipated r2cs (0.90), number of candidate predictor parameters (n=20), and mean (26.7 kg) and standard deviation (8.7 kg) of fat free mass in the target population (taken from hudda et al47). for example, in stata, after installation of pmsampsize (type: ssc install pmsampsize), we can type:

pmsampsize, type(c) rsquared(0.9) parameters(20) intercept(26.7) sd
(8.7)
this returns that at least 254 participants are required, and so 12.7 participants for each predictor parameter. the sample size of 254 is driven by the number needed to precisely estimate the model standard deviation (criterion c3), as only 68 participants are needed to minimise overfitting (criteria c1 and c2).

extensions and further topics
ensuring accurate predictions in key subgroups
alongside the criteria outlined in box 1, a more stringent task is to ensure model predictions are accurate in key subgroups defined by particular values or categories of included predictors.48 one way to tackle this is to ensure predictor effects in the model equation are precisely estimated, at least for key subgroups of interest.1516 for binary and time-to-event outcomes, the precision of a predictor’s effect depends on its magnitude, the variance of the predictor’s values, the predictor’s correlation with other predictors in the model, the sample size, and the outcome proportion or rate in the study.495051 for continuous outcomes, it depends on the sample size, the residual variance, the correlation of the predictor with other included predictors, and the variance of the predictor’s values.4852535455 note that for important categorical predictors large sample sizes might be needed to avoid separation issues (ie, where no events or non-events occur in some categories),13 and potential bias from sparse events.56

sample size considerations when using an existing dataset
our proposed sample size calculations (ie, based on the criteria in box 1) are still useful in situations when an existing dataset is already available, with a specific number of participants and predictors. firstly, the calculations might identify that the dataset is too small (for example, if the overall outcome risk cannot be estimated precisely) and so the collection of further data is required.5758 secondly, the calculations might help identify how many predictors can be considered before overfitting becomes a concern. the shrinkage estimate obtained from fitting the full model (including all predictors) can be used to gauge whether the number of predictors could be reduced through data reduction techniques such as principal components analysis.10 this process should be done blind to the estimated predictor effects in the full model, as otherwise decisions about predictor inclusion will be influenced by a “quick look” at the results (which increases the overfitting).

sample size requirements when using variable selection
further research on sample size requirements with variable selection is required, especially for the use of more modern penalisation methods such as the lasso (least absolute shrinkage and selection operator) or elastic net.3359 such methods allow shrinkage and variable selection to operate simultaneously, and they even allow the consideration of more predictor parameters than number of participants or outcome events (ie, in high dimensional settings). however, there is no guarantee such models solve the problem of overfitting in the dataset at hand. as mentioned, they require penalty and shrinkage factors to be estimated using the development dataset, and such estimates will often be hugely imprecise. also, the subset of included predictors might be highly unstable60616263; that is, if the prediction model development was repeated on a different sample of the same size, a different subset of predictors might be selected and important predictors missed (especially if sample size is small). in healthcare the final set of predictors is a crucial consideration, owing to their cost, time, burden (eg, blood test, invasiveness), and measurement requirements.

larger sample sizes might be needed when using machine learning approaches to develop risk prediction models
an alternative to regression based prediction models are those based on machine learning methods, such as random forests and neural networks (of which “deep learning” methods are a special case).64 when the focus is on individualised outcome risk prediction, it has been shown that extremely large datasets might be needed for machine learning techniques. for binary outcomes, machine learning techniques could need more than 10 times as many events for each predictor to achieve a small amount of overfitting compared with classic modelling techniques such as logistic regression, and might show instability and a high optimism even with more than 200 epp.26 a major cause of this problem is that the number of predictor (“feature”) parameters considered by machine learning approaches will usually far exceed that for regression, even when the same set of predictors is considered, particularly because they routinely examine multiple interaction terms and categorise continuous predictors.

therefore, machine learning methods are not immune to sample size requirements, and actually might need truly “big data” to ensure their developed models have small overfitting, and for their potential advantages (eg, dealing with highly non-linear relations and complex interactions) to reach fruition. the size of most medical research datasets is better suited to using regression (including penalisation and shrinkage approaches),65 especially as regression also leads to a transparent model equation that facilitates implementation, validation, and graphical displays.

sample size for model updating
when an existing prediction model is updated, the existing model equation is revised using a new dataset. the required sample size for this dataset depends on how the model is to be updated and whether additional predictors are to be included. in our worked examples, we assumed that all parameters in the existing model will be re-estimated using the model updating dataset. in that situation, the researcher can still follow the guidance in box 1 for calculating the required sample size, with the total predictor parameters the same as in the original model plus those new parameters required for any additional predictors.

sometimes, however, only a subset of the existing model’s parameters is to be updated.6667 in particular, to deal with calibration-in-the-large, researchers might only want to revise the model intercept (or baseline survival), while constraining the other parameter estimates to be the same as those in the existing model. in this case the required sample size only needs to be large enough to estimate the mean outcome value or outcome risk precisely (ie, to meet criteria c1, b1, or t1 in box 1). even if researchers also want to update the existing predictor effects, they might decide to constrain their updated values to be equal to the original values multiplied by a constant. then, the sample size only needs to be large enough to estimate one predictor parameter (ie, the constant) for the existing predictors, plus any new parameters the researchers decide to add. such model updating techniques therefore reduce the sample size needed (to meet the criteria in box 1) compared with when every predictor parameter is re-estimated without constraint.

conclusion
patients and healthcare professionals require clinical prediction models to accurately guide healthcare decisions.1 larger sample sizes lead to more robust models being developed, and our guidance in box 1 outlines how to calculate the minimum sample size required. clearly, the more data for model development the better; so if larger sample sizes are achievable than our guidance suggests, use it! of course, any data collected should be of sufficient quality and representative of the target population and settings of application.6869

after data collection, careful model building is required using appropriate methods.1310 in particular, we do not recommend data splitting (eg, into model training and testing samples), as this is inefficient and it is better to use all the data for model development, with resampling methods (such as bootstrapping) used for internal validation.7071 sometimes external information might be used to supplement the development dataset further.727374 lastly, sample size requirements when externally validating an existing prediction model require a different approach, as discussed elsewhere.

<|EndOfText|>

a guide to systematic review and meta-analysis of prognostic factor studies

prognostic factors are associated with the risk of future health outcomes in individuals with a particular health condition or some clinical start point (eg, a particular diagnosis). research to identify genuine prognostic factors is important because these factors can help improve risk stratification, treatment, and lifestyle decisions, and the design of randomised trials. although thousands of prognostic factor studies are published each year, often they are of variable quality and the findings are inconsistent. systematic reviews and meta-analyses are therefore needed that summarise the evidence about the prognostic value of particular factors. in this article, the key steps involved in this review process are described.

systematic reviews and meta-analyses are common in the medical literature, routinely appearing in specialist and general medical journals, and forming the cornerstone of cochrane. the majority of systematic reviews focus on summarising the benefit of one or more therapeutic interventions for a particular condition. however, they are also important for summarising other evidence, such as the accuracy of screening and diagnostic tests,1 the causal association of risk factors for disease onset, and the prognostic ability of bespoke factors and biomarkers. prognostic evidence arises from prognosis studies, which aim to examine and predict future outcomes (such as death, disease progression, side effects or medical complications like pre-eclampsia) in people with a particular health condition or start point (such as those developing a certain disease, undergoing surgery, or women who are pregnant).

the progress (prognosis research strategy) framework defines four types of prognosis research objectives: (a) to summarise overall prognosis (eg, overall risk or rate) of health outcomes for groups with a particular health condition2; (b) to identify prognostic factors associated with changes in health outcomes3; (c) to develop, validate, and examine the impact of prognostic models for individualised prediction of such outcomes4; and (d) to identify predictors of an individual’s response to treatment.5 each objective requires specific methods and tools for conducting a systematic review and meta-analysis. two recent articles provided a guide to undertaking reviews and meta-analysis of prognostic (prediction) models.67 in this article, we focus on prognostic factors.

a prognostic factor is any variable that is associated with the risk of a subsequent health outcome among people with a particular health condition. different values or categories of a prognostic factor are associated with a better or worse prognosis of future health outcomes. for example, in many cancers, tumour grade at the time of histological examination is a prognostic factor because it is associated with time to disease recurrence or death. each grade represents a group of patients with a different prognosis, and the risk or rate (hazard) of the outcome increases with higher grades. many routinely collected patient characteristics are prognostic, such as sex, age, body mass index, smoking status, blood pressure, comorbidities, and symptoms. many researched prognostic factors are biomarkers, which include a diverse range of blood, urine, imaging, electrophysiological, and physiological variables.

prognostic factors have many potential uses, including aiding treatment and lifestyle decisions, improving individual risk prediction, providing novel targets for new treatment, and enhancing the design and analysis of randomised trials.3 this motivates so-called “prognostic factor research” to identify genuine prognostic factors (sometimes also called “predictor finding studies”8).9 although thousands of such studies are published each year, often they are of variable quality and have inconsistent findings. systematic reviews and meta-analyses are therefore urgently needed to summarise the evidence about the prognostic value of particular factors.101112 in this article, we provide a step-by-step guide on conducting such reviews. our aim is to help readers, healthcare providers, and researchers understand the key principles, methods, and challenges of reviews of prognostic factor studies.

summary points
primary studies to identify prognostic factors are abundant, but often findings are inconsistent and quality is variable. systematic reviews and meta-analyses are urgently needed to identify, evaluate, and summarise prognostic factor studies and their findings.
a clear review question should be defined using the picots system (population, index prognostic factor, comparator prognostic factors, outcome, timing, setting), and a transparent search undertaken for eligible articles. broad search strings may be required, leading to a large number of articles to screen.
a data extraction phase is needed to obtain the relevant information from each study. a modification of charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) can be used for prognostic factors (charms-pf).
the quips tool (quality in prognostic factor studies) can be used to examine each study’s risk of bias. unfortunately, many primary studies may have a high risk of bias because of poor design standards, conduct, and analysis. applicability of a study should also be checked.
if appropriate, meta-analysis can be used to combine prognostic effect estimates (such as hazard ratios, risk ratios, or odds ratios) across studies to produce an overall summary of a factor’s prognostic effect. between-study heterogeneity should be expected and accounted for.
ideally separate meta-analyses should be performed for unadjusted and adjusted prognostic effect estimates; adjusted estimates are important to examine a factor’s independent prognostic value over and above (that is, after adjustment for) other prognostic factors.
separate meta-analyses may also be required for each method of measurement (for factors and outcomes), each approach to handling continuous factors, and each type of estimate (such as hazard ratios or odds ratios).
publication bias is a major threat to the validity of meta-analyses of prognostic factor studies based on published evidence, and may cause small-study effects (asymmetry on a funnel plot).
remark (reporting recommendations for tumour marker prognostic studies) and prisma (preferred reporting items for systematic reviews and meta-analyses) can be used to guide the reporting of the systematic review and meta-analysis of prognostic factor studies; the degree of confidence in the summary results from the review may be examined by use of adapted forms of grade (grades of recommendation, assessment, development, and evaluation) for interventions and diagnostic test accuracy studies.
availability of individual participant data from primary prognostic factor studies may alleviate many of the challenges.
step 1: defining the review question
the first step is to define the review question. a review of prognostic factor studies falls within the second objective of the progress framework2 because it aims to summarise the prognostic value of a particular factor (or each of multiple factors) for relevant health outcomes and time points in people with a specific health condition (eg, disease). some reviews are broad; for example, riley and colleagues aimed to identify any prognostic factor for overall and disease free survival in children with neuroblastoma or ewing’s sarcoma.13 other reviews have a narrower focus; for example, hemingway and colleagues aimed to summarise the evidence on whether c reactive protein (crp) is a prognostic factor for fatal and non-fatal events in patients with stable coronary disease.14 this crp review is used as an example throughout this article.

charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) provides guidance for formulating a review question (table 1 in the article by moons and colleagues15). although charms was developed15 and refined6 for reviews of prediction model studies, it can also be used to define and frame the question for reviews of prognostic factor studies. charms15 and subsequent improvements6 propose a modification of the traditional pico system (population, index intervention, comparison, and outcome) used in systematic reviews of therapeutic intervention studies. the modification is called picots, because it also considers timing and setting (box 1). in the context of prognostic factor reviews, the “p” of population and the “o” of outcome remain largely the same as in the original pico system, but the “i” refers to index prognostic factors and the “c” refers to other prognostic factors that can be considered as comparators in some way. for example, the aim may be to compare the prognostic ability of a certain index factor with one or more other (that is, comparator) prognostic factors; or to investigate the adjusted prognostic value of a particular index factor over and above (adjusted for) other (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, which is not generally recommended, then no comparator factor is being considered. the “t” denotes timing and refers to two concepts of time. firstly, at what time point the prognostic factors under review are to be measured or assessed (that is, the time point at which prognosis information is required); and secondly, over what time period the outcomes are predicted by these factors. the “s” of setting refers to the setting or context in which the index prognostic factors are to be used because the prognostic ability of a factor may change across healthcare settings.

table 1 charms-pf checklist of key items to be extracted from primary studies of prognostic factors, based on additions and modifications of the original charms checklist for primary studies of prediction models15
view popupview inline
box 1
six items (picots) defining the question for systematic reviews of prognostic factor studies, based on charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies)615 and applied to a review of the adjusted prognostic value of c reactive protein (crp)14
population: define the target population for which prognostic factors under review are to be used. for example, crp review: patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of acute coronary syndrome at least two weeks before prognostic factor (crp) measurement.
index prognostic factor: define the factors for which prognostic value is under review. for example, crp review: crp was the single biomarker reviewed for its prognostic value.
comparator prognostic factors: comparator prognostic factors can be considered in a review in various ways. for example, the aim could be to compare the prognostic ability of a certain index factor with two or more other (that is, comparator) prognostic factors; or to review the adjusted prognostic value of a particular index factor—that is, over and above (adjusted for, independent of) other existing (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, then no comparator factor is being considered. for example, crp review: the focus was on the adjusted prognostic value of crp—that is, its prognostic effect after adjusting for existing (comparator) prognostic factors. in particular, adjustment for the following conventional prognostic factors was of interest: age, sex, smoking status, obesity, diabetes, and one or more lipid variables (from total cholesterol, low density lipoprotein cholesterol, high density lipoprotein cholesterol, triglycerides) and inflammatory markers (fibrinogen, interleukin 6, white cell count).
outcome: define the outcomes for which the prognostic ability of the factor(s) under review are of interest. for example, crp review: outcome events were defined as coronary (coronary death, sudden cardiac death, acute non-fatal myocardial infarction, primary percutaneous coronary intervention, unplanned emergency admissions with unstable angina), cardiovascular (when coronary events were reported in combination with heart failure, stroke, or peripheral arterial disease), and all cause mortality.
timing: define firstly at what time points the prognostic factors (index and comparators) are to be used (that is, the time point of prognostication), and secondly over what time period the outcomes are predicted by these factors. for example, crp review: the crp measurement had to be done at least two weeks after diagnosis and all follow-up information on the outcomes (all time periods) was extracted from the studies.
setting: define the intended setting and role of the prognostic factors under review. for example, crp review: crp measurement was studied in primary and secondary care to provide prognostic information about patients diagnosed with coronary heart disease; this information may be useful for healthcare professionals treating and managing such patients.
return to text
an important component of reviews of prognostic factors is whether unadjusted or adjusted estimates of the index prognostic factors will be summarised, or both. we recommend that reviewers primarily focus on adjusted prognostic factor estimates because they reveal whether a certain index factor contributes independently to the prediction of the outcome over and above (that is, after adjustment for) other prognostic factors. in particular, for each clinical scenario there are often so-called “established” or “conventional” prognostic factors that are always measured. therefore, for prognostic factors under review, it is important to understand whether they contribute additional (sometimes called “independent”) prognostic information to the routinely measured ones. this means that reviewers need adjusted (and not unadjusted or crude) prognostic effect estimates to be estimated and reported in primary prognostic factor studies. such adjusted prognostic estimates are typically derived from a multivariable regression model containing the established prognostic factors plus each index prognostic factor of interest.

for example, consider a logistic regression of a binary outcome including three adjustment factors (a1, a2, and a3) and one new index prognostic factor (x1), which is expressed as:

ln(p/(1−p)) = α+β1a1+β2a2+β3a3+β4x1
here, “p” is the probability of the outcome. after estimation of all the unknown parameters (that is, α, β1, β2, β3, β4), of key interest is the estimated β4. this parameter provides the adjusted prognostic effect of the index prognostic factor and reveals its independent contribution to the prediction of the outcome over and above the prognostic effects of the other (established comparator) factors a1, a2, and a3 combined.

the need to focus on adjusted prognostic effects is no different from (systematic reviews of) aetiological studies, in which the focus is on estimating the association of a certain causal risk factor after adjustment for other risk factors. in such causal research, these factors are usually referred to as “confounders” rather than as “other prognostic factors,” which is the term typically used for prognosis research. the crude (unadjusted) prognostic effect of some index factors may completely disappear after adjustment and is therefore rather uninformative, especially because prognostication in healthcare is rarely based on a single prognostic factor but rather on the information from multiple prognostic factors.4

this article focuses on systematic reviews to summarise prognostic factor effect estimates. some primary studies may also evaluate an index factor’s added value in terms of improvement in risk classification and clinical use (eg, measures such as net reclassification improvement and net benefit), and change in prediction model performance (eg, by calculating the change in the concordance index, also known as the c statistic or area under the receiver operating characteristics (roc) curve).17181920 however, this is beyond the scope of this article, and we refer the reader to other relevant sources.62122

application to crp review
crp is widely studied for its prognostic value in patients with coronary disease. however, there is uncertainty whether crp is useful because us and european clinical practice guidelines recommend measurement but clinical practice varies widely. this uncertainty motivated the systematic review by hemingway and colleagues,14 with the corresponding picots system presented in box 1. no studies were excluded on the basis of methodological standards, sample size, duration of follow-up, publication year, or language of publication.

step 2: searching for and selection of eligible studies
the next step is to identify primary studies that are eligible for review; studies that address the review question defined in step 1 following the picots framework. unfortunately, it is more difficult to identify prognostic factor studies than randomised trials of interventions. prognosis studies do not tend to be indexed (“tagged”) because a taxonomy of prognosis research is not widely recognised. moreover, compared with studies of interventions, there is much more variation in the design of prognostic factor studies (eg, data from cohort studies, randomised trials, routine care registries, and case-control studies can all be used), patient inclusion criteria, prognostic factor and outcome measurement, follow-up time, methods of statistical analysis, and adjustment of (and number of) other prognostic factors (covariates). between-study heterogeneity is therefore the rule rather than the exception in prognostic factor research. it is essential that systematic reviews of prognostic factor studies define the study inclusion and exclusion criteria based on the picots structure (step 1) because this determines the study search and selection strategy.

typically, broad search and selection filters are required that combine terms related to prognosis research (such as prognostic, predict, predictor, factor, independent) with domain or disease specific terms (such as the name of prognostic factors and the targeted disease or patient population).23 a broad search comes at the (often considerable) expense of retrieving many irrelevant records. geersing and colleagues24 validated various existing search strategies for prognosis studies and suggested a generic filter for identifying studies of prognostic factors,232526 which extended the work of ingui, haynes, and wong.232526 when tested in a single review of prognostic factors, this generic filter had a number needed to read of 569 to identify one relevant article, emphasising the difficulty in targeting prognostic factor articles. the number needed to read could be considerably reduced when specific factors or populations are added to the filter. even then, care is needed to be inclusive because multiple terms are often used for the same meaning; for example, biomarker mycn is also referred to as n-myc and nmyc, among other terms.13

once the search is complete, each potentially relevant study must be screened for its applicability to the review question. because of the heterogeneity in prognostic factor studies, during this study selection phase more deviations from the defined picots (in step 1) are possible (far greater than what is typically encountered during the selection of randomised intervention studies). the applicability of this primary study selection should firstly be based on title and abstract screening, followed by full text screening, both ideally done by two researchers independently. any discrepancies should be resolved through discussion, potentially with a third reviewer. to check if any relevant articles have been missed, it is helpful to share the list of identified articles with researchers in the field to examine the reference lists of these articles and to perform a citation search.

application to crp review
hemingway and colleagues included any prospective observational study that reported risk of subsequent events among patients with stable coronary disease in relation to measured crp values.14 eligible studies had to include patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of previous acute coronary syndrome at least 2 weeks before crp measurement. hemingway and colleagues searched medline between 1966 and 25 november 2009 and embase between 1980 and 17 december 2009, using a search string containing terms for coronary disease, prognostic studies, and crp. the search identified 1566 articles, of which 83 fulfilled the inclusion criteria. if specific terms for crp had not been included in the search string, then the total number of identified articles would have far exceeded 1566.

step 3: data extraction
the next step is to extract key information from each selected study. data extraction provides the necessary data from each study, which enables reviewers to examine their (eventual) applicability to the review question and their risk of bias (see step 4). this step also provides the information required for subsequent qualitative and quantitative (meta-analysis) synthesis of the evidence across studies. the charms checklist gives explicit guidance (table 2 in the article by moons and colleagues15) about which key items across 11 domains should be extracted from primary studies of prediction models, and for what reason (that is, to provide general information about the primary study, to guide risk of bias assessment, or to assess applicability of the primary study to the review question). based on our experience of conducting systematic reviews of prognostic factor studies, we modified the original charms checklist for prediction model studies to make it suitable for data extraction in reviews of prognostic factors (here referred to as charms-pf; table 1). this basically means that three domains typically addressing multivariable prediction modelling aspects were combined to one overall analysis domain, while other domain names and key items were slightly reworded or extended. reasons for extraction of each key item are similar to charms for prediction models. because we developed the original charms checklist, a wider consensus of the charms-pf content was not considered necessary.

table 2 quips tool (quality in prognostic factor studies), which can be used to classify risk of bias of prognostic factor studies
view popupview inline
reviewers should extract fundamental information from the primary prognostic factor studies, such as the dates, setting, study design, definitions of start points, outcomes, follow-up length, and prognostic factors; reviewers will often find large heterogeneity in this information across studies. the extracted information can be summarised in tables of study characteristics. in addition, more specific information is needed to properly assess applicability and risk of bias (see step 4), such as methods used to measure prognostic factors and outcomes, handling missing data, attrition (loss to follow-up), and whether estimated associations of the prognostic factors under review were adjusted for other prognostic factors. this information also enhances the potential for meta-analysis and the presentation and interpretation of subsequent summary results (see steps 5-8).

to enable meta-analysis of prognostic factor studies, the key elements to extract are estimates, and corresponding standard errors or confidence intervals, of the prognostic effect for each factor of interest; for example, the estimated risk ratio or odds ratio (for binary outcomes), hazard ratio (for time-to-event outcomes), or mean difference (for continuous outcomes). as most prognostic factor studies consider time-to-event outcomes (including censored observations and different follow-up lengths for patients), hazard ratios are often the most suitable effect measure. a concern is that hazard ratios may not be constant over time, and therefore any evaluations of non-proportional hazards (that is, non-constant hazard ratios for the prognostic factors of interest) should also be extracted; however, such information is rarely reported in sufficient detail.

unfortunately, many prognostic factor studies do not adequately report estimated prognostic effect measures or their precision. for this reason, methods are available to restore the missing information upon data extraction. in particular, parmar and colleagues28 and tierney and colleagues29 describe how to obtain unadjusted hazard ratio estimates (and their variances) when they are not reported directly. for example, under assumptions, the number of outcomes (events) and an available p value (eg, from a log rank test or cox regression) can be used to indirectly estimate the unadjusted hazard ratio between two groups defined by a particular factor (eg, “high” versus “normal” levels). perneger and colleagues30 report how to derive unadjusted hazard ratios from survival proportions, and pérez and colleagues suggest using a simulation approach.31 even with such indirect estimation methods, not all results can be obtained. for example, in a systematic review of 575 studies investigating prognostic factors in neuroblastoma,32 the methods of parmar and colleagues were used to obtain 204 hazard ratio estimates and their confidence intervals; but this represented only 35.5% of the potential evidence.

although indirect estimation methods help retrieve unadjusted prognostic factor effect estimates, they often have limited value for obtaining adjusted effect estimates. furthermore, even when multiple studies provide the adjusted prognostic effect of a particular factor, the set of adjustment factors will usually differ across studies, which complicates the interpretation of subsequent meta-analysis results. we recommend that reviewers predefine the core set of prognostic factors for the outcome of interest (eg, age, sex, smoking status, disease stage) that represents the desired “minimal” set of adjustment factors. an agreed process among health professionals and researchers in the field could be required to define this set. for example, a list of established prognostic factors could be identified that are routinely used within current prognostication of the clinical population of interest.

it may also be necessary to standardise the extracted estimates to ensure they all relate to the same scale and direction in each study. in particular, the direction of the prognostic effect will need standardising if one study compares the hazard rate in a factor’s “high” versus “normal” group, whereas another study compares the hazard rate in the factor’s “normal” versus “high” group. when the outcome is defined differently across studies, approaches to convert effect measures on different outcome scales could be useful.33 also, to deal with different cutpoint levels for values of a particular factor,34 the prognostic effects of “high” versus “normal” could be converted to prognostic effects relating to a 1 unit increase in the factor. this requires assumptions about the underlying distribution of the factor. such an approach was used by hemingway and colleagues.14 of concern, however, is that the actual distribution of a prognostic factor may be unknown (or even vary across studies). finally, it is also possible to derive standardised effect estimates by standardising the corresponding regression coefficients.35

application to crp review
hemingway and colleagues extracted background information such as year of study start, number of included patients, mean age, baseline coronary morbidity (eg, proportion with stable angina), average levels of biomarker at baseline, method of crp measurement, follow-up duration, and number and type of events. basic information was often missing. for example, nearly a fifth of studies did not report the method of measurement, and only a quarter gave the number of patients included in the analyses and reasons for dropout. prognostic effect estimates for crp were extracted in terms of the reported risk ratio, odds ratio, or hazard ratio (labelled generally as “risk ratio” in this article), and 95% confidence intervals. these effect estimates were then converted to a standardised scale comparing the highest third with the lowest third of the (log transformed) crp distribution. if available, separate prognostic effect estimates were extracted for different degrees of adjustment for other prognostic factors.

step 4: evaluating applicability and risk of bias of primary studies
once eligible studies are identified and data are extracted, an important next step is to assess the applicability and risk of bias (quality) of each study in the review. as for steps 2 and 3, ideally this is done by two reviewers, independently, with any discrepancies resolved. applicability refers to the extent to which a selected study (in step 2) matches the review question in terms of the population, timing, prognostic factors, and outcomes (endpoints) of interest. just because a study is eligible for inclusion does not mean it is free from applicability concerns. some aspects of a study may be applicable (eg, correct condition at start point, with prognostic factors of interest evaluated) but not others (eg, incorrect population or setting, inappropriate outcome definition, different follow-up time, lack of adjustment for conventional prognostic factors). applicability is typically first assessed during title and abstract screening, and then during this step, so that it is based on full text screening and determined by picots (step 1) and inclusion and exclusion criteria of studies (step 2).

risk of bias refers to the extent to which flaws in the study design or analysis methods could lead to bias in estimates of the prognostic factor effects. unfortunately, based on growing empirical evidence from systematic reviews examining methodology quality, many primary studies will be at high risk of bias.832363738394041424344 for prognostic factor studies, hayden and colleagues developed the quips checklist (quality in prognostic factor studies) for examining risk of bias across six domains27: study participation, study attrition, prognostic factor measurement, outcome measurement, adjustment for other prognostic factors, and statistical analysis and reporting. table 2 shows the signalling items within these domains to help guide reviewers in making low, unclear, or high risk of bias classifications. additional guidance may be found in general tools examining the quality of observational studies,4546 and the remark guideline (reporting recommendations for tumour marker prognostic studies) for reporting of primary prognostic factor studies.4748

we recommend that users first operationalise criteria to assess the signalling items and domains for the specific review question. for example, with the study participation and attrition domains, this includes defining a priori the most important characteristics that could indicate a systematic bias in study recruitment (study participation domain) and loss to follow-up (study attrition domain). defining these characteristics ahead of time will facilitate assessment and consensus related to the importance of potential differences that could influence the observed association between the index prognostic factors and outcomes of interest. definitions of sufficiently valid and reliable measurement of the index prognostic factors and outcomes should also be specified at the protocol stage. similarly, the core set of other (adjustment) prognostic factors that are deemed necessary for the primary studies to have adjusted for, should be predefined to facilitate judgment related to risk of bias in domain 5.

overall assessment of the six risk of bias domains is undertaken by considering the risk of bias information from the signalling items for each domain, rated as low, moderate, and high risk of bias. occasionally, item information needed to assess the bias domains is not available in the study report. when this occurs, other publications that may have used the same dataset (which often occurs in prognostic studies based on large existing cohorts) should be consulted and study authors should be contacted for additional information. an informed judgment about the potential risk of bias for each bias domain should be made independently by two reviewers, and discussed to reach consensus. each of the six domains needs to be rated and reported separately because this will inform readers, flag improvements needed for subsequent primary studies, and facilitate future meta-epidemiological research. we recommend defining studies with an overall “low risk of bias” as those studies where all, or the most important domains (as determined a priori), are rated as having low (or low to moderate) risk of bias.

application to crp review
hemingway and colleagues assessed the quality of included studies by the quality of their reporting on 17 items derived from the remark guideline.48 the median number of study quality items reported was seven of a possible 17, and standards did not change between 1997 and 2009. only two studies referred to a study protocol, with none referring to a statistical analysis plan. hemingway and colleagues noted that this “makes it difficult to know what the specific research objectives were at the start of cohort recruitment, at the time of crp measurement, or at the onset of the statistical analysis.”14 only two studies reported the time elapsed between first lifetime presentation with coronary disease and assessment of crp and this raised applicability concerns.

step 5: meta-analysis
meta-analysis of prognostic factor studies aims to summarise the (adjusted) prognostic effect of each factor of interest. in addition to missing estimates, challenges for the meta-analyst include (a) having different types of prognostic effect measures (eg, odds ratios and hazard ratios), which are not necessarily comparable30; (b) estimates without standard errors, which is a problem because meta-analysis methods typically weight each study by (a function of) their standard error; (c) estimates relating to various time points of the outcome occurrence or measurement; (d) different methods of measurement for prognostic factors and outcomes; (e) various sets of adjustment factors; and (f) different approaches to handling continuous prognostic factors (eg, categorisation, linear, non-linear trends), including the choice of cutpoint value when dichotomising continuous values into “low” and “normal” groups. many of these issues lead to substantial heterogeneity and if a meta-analysis is performed, summary results cannot be directly interpreted.

generally, meta-analysis results will be most interpretable, and therefore useful, when a separate meta-analysis is undertaken for groups of “similar” prognostic effect measures. in particular, we suggest considering a meta-analysis for:

hazard ratios, odds ratios, and risk ratios separately
unadjusted and adjusted associations separately
prognostic factor effects at distinct cutpoints (or groups of similar cutpoints) separately
prognostic factor effects corresponding to a linear trend (association) separately
prognostic factor effects corresponding to non-linear trends separately
each method of measurement (for factors and outcomes) separately.
ideally a meta-analysis of adjusted results should ensure that all included estimates are adjusted for the same set of other prognostic factors. this situation is unlikely and so a compromise could be to ensure that all adjusted estimates in the same meta-analysis have adjusted for at least a (predefined) minimum set of adjustment factors (that is, a core set of established prognostic factors).

even when adhering to this guidance, unexplained heterogeneity is likely to remain because of other reasons (eg, differences in length of follow-up or in treatments received during follow-up). therefore, if a meta-analysis is performed, a random effects approach is essential to allow for unexplained heterogeneity across studies (box 2), as previously described in the bmj.53 this approach provides a summary estimate of the average prognostic effect of the index factor and the variability in effect across studies. also potentially useful are meta-analysis methods to estimate the trend (eg, linear effect) of a prognostic factor that has been grouped into three or more categories within studies (with each category compared with the reference category). these methods generally model the estimated prognostic effect sizes in each category as a function of “exposure” level (eg, midpoint or median prognostic factor value in the category) and account for within-study correlation and between-study heterogeneity.5455565758 to apply these methods, some additional knowledge of the factor’s underlying distribution is usually needed to help define the “exposure” level because the chosen value can have an impact on the results.56

box 2
explanation of a random effects meta-analysis of prognostic factor effect estimates
the true prognostic effect of a factor is likely to vary from study to study; therefore assuming a common (fixed) prognostic effect is not sensible. if yi and var(yi) denote the prognostic effect estimate (eg, ln(hazard ratio), ln(odds ratio), ln(risk ratio), or mean difference) and its variance in study i, then a general random effects meta-analysis model can be specified as:

yi ~n(μ,var(yi)+τ2).

most researchers use either restricted maximum likelihood or the approach of dersimonian and laird to estimate this model,49 but other options are available, including a bayesian approach.50 of key interest is the estimate of μ, which reveals the summary (average) prognostic effect of the index prognostic factor of interest. the standard deviation of this prognostic factor effect across studies is denoted by τ, and non-zero values suggest there is between-study heterogeneity. confidence intervals for µ should ideally account for uncertainty in estimated variances (in particular τ),51 and we have found the approach of hartung-knapp to be robust for this purpose in most settings.1652 when synthesising prognostic effects on the log scale, the summary results and confidence intervals require back transformation (using the exponential function) to the original scale.

return to text
advanced multivariate meta-analysis methods are also available to handle multiple cutpoints,59 multiple methods of measurement,59 or different adjustment factors in prognostic factor studies.60 an introduction to multivariate meta-analysis has been published in the bmj.61

application to crp review
hemingway and colleagues14 applied a random effects meta-analysis to combine 53 adjusted prognostic effect estimates for crp from studies that adjusted for at least one of six conventional risk factors (age, sex, smoking status, diabetes, obesity, and lipids). the summary meta-analysis result was a risk ratio of 1.97 (95% confidence interval 1.78 to 2.17), which gives the average prognostic effect of crp (for those in the top v bottom third of crp distribution), and suggests larger crp values are associated with higher risk. although there was substantial between-study heterogeneity, nearly all estimates were in the same direction (that is, risk ratio >1). when restricting meta-analysis to just the 13 studies that adjusted for at least all six conventional prognostic factors, the summary risk ratio decreased to 1.65 (95% confidence interval 1.39 to 1.96), and the between-study heterogeneity reduced. using the study specific estimates given by hemingway and colleagues, we updated this meta-analysis (fig 1), obtaining the same summary result but a wider confidence interval (1.34 to 2.04) through the hartung-knapp approach.16

fig 1
fig 1
forest plot showing the study specific estimates and meta-analysis summary result of the adjusted prognostic effect (risk ratio) of c reactive protein taken from the review of hemingway and colleagues14; all studies were adjusted for a core set of existing prognostic factors (age, sex, smoking status, diabetes, obesity, and lipids), plus up to 14 other prognostic factors. meta-analysis results shown are based on a random effects meta-analysis model with dersimonian and laird estimation of the between-study variances. the summary result is identical to hemingway and colleagues,14 but the confidence interval is wider because we used the hartung-knapp approach to account for uncertainty in variance estimates.16although “risk ratio” is used, the estimates actually correspond to a mixture of risk ratios, odds ratios, and hazard ratios

download figure open in new tab download powerpoint
step 6: quantifying and examining heterogeneity
for all meta-analyses, when there is large heterogeneity across included studies, it may be better not to synthesise the study results, but rather display the variability in estimates on a forest plot without showing an overall pooled estimate. when a meta-analysis is performed in the face of heterogeneity, it is important to quantify and report the magnitude of heterogeneity itself; for example, through the estimate of (the between-study variance),62 or an approximate 95% prediction interval indicating the potential true prognostic effect of a factor in a new population.5363

subgroup analyses and meta-regression can be used to examine or explore the causes of heterogeneity. a subgroup analysis performs a separate meta-analysis for categories defined by a particular characteristic, such as those with a low risk of bias, those with a follow-up of less than one year or of at least one year, or those set in countries in europe. a better approach is meta-regression, which extends the meta-analysis equation shown in box 2 by including study level covariates,64 and allows a formal comparison of meta-analysis results across groups defined by covariates (eg, low risk of bias studies v studies at higher risk of bias). unfortunately, subgroup analyses and meta-regression are often problematic. there will often be few studies per subgroup and low power to detect genuine causes of heterogeneity. furthermore, study level confounding will be rife so that it is difficult to disentangle the associations for one covariate from another. for example, studies with a low risk of bias may also have a different length of follow-up or a particular cutpoint level compared with studies at higher risk of bias.

application to crp review
hemingway and colleagues reported that meta-regression identified four study level covariates that explained some between-study heterogeneity in the prognostic effect of crp: definition of comparison group, number of adjustment factors, the (log) number of events, and the proportion of patients with stable coronary disease (reflecting study size).14 studies originally reporting unequal crp groups had stronger effects than those reporting crp on a continuous scale. for each additional adjustment factor, the summary risk ratio decreased by 3%. the summary risk ratio was smaller among studies with more than the median number of outcome events, and smaller among studies confined to stable coronary disease. there was no evidence that the crp effect differed according to the number of quality items reported by a study, or by the type of prognostic effect measure provided (that is, risk ratio, odds ratio, or hazard ratio).

step 7: examining small-study effects
the term “small-study effects” refers to when there is a systematic difference in prognostic effect estimates for small studies and large studies.65 a particular concern is when small studies (especially those that are exploratory because these often evaluate many potential prognostic factors with relatively few outcome events) show larger prognostic effects than larger studies. this difference may be due to chance or heterogeneity, but a major threat here is publication bias and selective reporting, which are endemic in prognosis research.363738 such reporting biases lead to smaller studies, with (statistically) significant or larger prognostic factor effect estimates being more likely to be published or reported in sufficient detail, and thus included in a meta-analysis, than smaller studies with non-significant or smaller prognostic effect estimates. this bias is a potential concern for unadjusted and adjusted prognostic effects. a primary study usually estimates an unadjusted prognostic effect for each of multiple prognostic factors, but study authors may only report effects that are statistically significant. in addition, adjusted results are often only reported for prognostic factors that retain statistical significance in univariable and multivariable analysis. a consequence is that meta-analysis results will be biased, with larger summary prognostic effects than in reality, and potentially some factors being deemed to have clinical value when actually they do not.

the evidence for small-study effects is usually considered on a funnel plot, which shows the study estimates (x axis) against their precision (y axis). a funnel plot is usually recommended if there are 10 or more studies.65 the plot should ideally show a symmetric, funnel like shape, with results from larger studies at the centre of the funnel and smaller studies spanning out in both directions equally. asymmetry will arise if there are small-study effects, with a greater proportion of smaller studies in one particular direction. statistical tests for asymmetry in risk, odds and hazard ratios can be used, such as peter’s and debray’s test.6667 contour enhanced funnel plots also show the statistical significance of individual studies, and “missing” studies are perhaps more likely to fall within regions of non-significance if publication bias was the cause of small-study effects. an example is shown in figure 2.

fig 2
fig 2
evidence of funnel plot asymmetry (small-study effects) in the c reactive protein meta-analysis shown in figure 1. the smaller studies (with higher standard errors) have risk ratio (rr) estimates mainly to the right of the larger studies, and therefore give the largest prognostic effect estimates. a concern is that this is due to publication bias, with “missing” studies potentially falling to the left side of the larger studies and in the lighter shaded regions denoting non-significant rr estimates

download figure open in new tab download powerpoint
as mentioned, small-study effects may also arise due to heterogeneity. therefore, it is difficult to disentangle publication bias from heterogeneity in a single review. for example, if smaller studies used an analysis with fewer adjustment factors, then this may cause larger prognostic factor effects in such studies, rather than it being caused by publication bias. a multivariate meta-analysis could reduce the impact of small-study effects by “borrowing strength” from related information.61

a related concern is that smaller prognostic factor studies are generally at higher risk of bias than larger studies. smaller studies tend to be more exploratory in nature and typically based on a convenient sample, often examining many (sometimes hundreds of) potential prognostic factors, with relatively few outcome events. this design leads to spurious (due to chance) and potentially biased (due to poor estimation properties68) prognostic effect estimates, which are more prone to selective reporting. in contrast, larger studies are often confirmatory studies focusing on one or a few prognostic factors, and are more likely to adopt a protocol driven and prospective approach, with clearer reporting regardless of their findings.3 therefore, larger studies are less likely to identify spurious prognostic factor effect estimates. it is helpful to examine small-study effects (potential publication bias) when restricting analysis to the subset of studies at low risk of bias. if this approach resolves previous issues of small-study effects in the full meta-analysis, then it gives even more credence to focus conclusions and recommendations on the meta-analysis results based only on the higher quality studies.

application to crp review
figure 2 shows a funnel plot of the study estimates from the crp meta-analysis shown in figure 1. there is clear asymmetry, which shows the strong potential for publication bias. there was an insufficient number of studies considered at low risk of bias to evaluate small-study effects in a subset of higher quality studies.

step 8: reporting and interpretation of results
as with all research studies, clear and complete reporting is essential for reviews of prognostic factor studies. most of the reporting guidelines of prisma (preferred reporting items for systemic reviews and meta-analyses) and moose (meta-analysis of observational studies in epidemiology) will be relevant,6970 and should be complemented by remark,4748 which was aimed at primary prognostic factor studies. more specific guidance for reporting systematic reviews of prognostic factor studies is under development.

interpretation and translation of summary meta-analysis results is an important final step. the guidance in the previous steps is the essential input for this step. discussion is necessary on whether and how the prognostic factors identified may be useful in practice (that is, translation of results to clinical practice), and what further research is necessary. ideally impact studies (eg, randomised trials that compare groups which do and do not use a prognostic factor to inform clinical practice) are needed before strong recommendations for clinical practice are made; however, these studies are rare and outside the scope of the review framework outlined in this article.

to interpret the certainty (confidence) of the summary results of a review of intervention effectiveness, grade (grades of recommendation, assessment, development, and evaluation) was developed. this approach assesses the overall quality of and certainty in evidence for the summary estimates of the intervention effects by addressing five domains: risk of bias, inconsistency, imprecision, indirectness, and publication bias. the grade domains can be assessed using the information obtained by the tools and methods described in the above steps. however, it is not known whether these domains, developed for reviews of interventions, are equally applicable to assessing the certainty of summary results of systematic reviews of prognostic factor studies. compared with reviews of intervention studies, allowing for heterogeneity (the inconsistency domain) might be more acceptable in reviews of prognostic factor studies because of the inevitable heterogeneity caused by study differences in methods of measurement, adjustment factors, and statistical analysis methods, among others. furthermore, the threat of selective reporting or publication bias in reviews of prognostic factor studies may be more severe than in reviews of intervention studies because of the problems of exploratory studies, poor reporting, and biased analysis methods.

there is limited empirical evidence for using the existing domains to grade the certainty of summary estimates of prognostic factor studies, although a first attempt has been made71; in addition, an assessment has been performed on grading the certainty of evidence of summary estimates of overall prognosis studies.72 reviewers need to be especially cautious when comparing the adjusted prognostic value of multiple index factors, for example, to conclude whether the summary adjusted hazard ratio for prognostic factor a is larger than that for factor b. usually different sets of studies will be available for each index factor, and so the comparison will be indirect and potentially biased. moreover, the studies evaluating factor a may often have used different sets of adjustment factors (other prognostic factors) than those evaluating factor b. it will be rare to find studies on different index factors that used exactly the same set of adjustment factors. we therefore recommend reviewers restrict comparisons (of the adjusted prognostic value) of two or more index factors to those studies that at least used a similar, minimally required set of adjustment factors.73 even then, due to different scales and distributions of each factor (eg, continuous or binary), a simple comparison of the prognostic effect sizes (eg, hazard ratio for factor a v hazard ratio for factor b) may not be straightforward.

application to crp review
the meta-analysis results suggest crp is a prognostic factor for the risk of death and non-fatal cardiovascular events, even when only including the largest studies that adjusted for all six conventional prognostic factors. in their discussion, hemingway and colleagues downgraded the meta-analysis findings because of a strong concern about the quality and reliability of the underlying evidence.14 the absence of prespecified protocols, poor and potentially biased reporting, and strong potential for publication bias prevented the authors from making firm conclusions about whether crp has prognostic value after adjustment for established prognostic factors. they state that the concerns “explicitly challenge the statement for healthcare professionals made by the centers for disease control that measuring crp is both ‘useful’ and ‘independent’ as a marker of prognosis.”74

summary
in this article, we described the key steps and methods for conducting a systematic review and meta-analysis of prognostic factor studies. current reviews are often limited by the quality and heterogeneity of primary studies.7576 we expect the prevalence of such reviews to grow rapidly, especially as cochrane has recently embarked on prognosis reviews (see also the cochrane prognosis methods group website www.methods.cochrane.org/prognosis).77 our guidance will help researchers to write grant applications for reviews of prognostic factor studies, and to develop protocols and conduct such reviews. protocols of prognostic factor reviews should be published ideally at the same time as the review is registered, for example within prospero, the international prospective register of systematic reviews (www.crd.york.ac.uk/prospero/), or the cochrane database.77 our guidance will also allow readers and healthcare providers to better judge reports of prognostic factor reviews.

finally, we note that some of the limitations described (eg, use of different cutpoint values across studies) could be alleviated if the individual participant data were obtained from primary prognostic factor studies78 rather than being extracted from study publications; although, this may not solve all problems (eg, quality of original study, availability of different adjustment factors).79 further discussion on individual participant data meta-analysis of prognostic factor studies is given elsewhere.80