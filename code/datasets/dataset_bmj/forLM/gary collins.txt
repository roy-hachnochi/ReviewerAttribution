prognostic factors are associated with the risk of future health outcomes in individuals with a particular health condition or some clinical start point (eg, a particular diagnosis). research to identify genuine prognostic factors is important because these factors can help improve risk stratification, treatment, and lifestyle decisions, and the design of randomised trials. although thousands of prognostic factor studies are published each year, often they are of variable quality and the findings are inconsistent. systematic reviews and meta-analyses are therefore needed that summarise the evidence about the prognostic value of particular factors. in this article, the key steps involved in this review process are described. systematic reviews and meta-analyses are common in the medical literature, routinely appearing in specialist and general medical journals, and forming the cornerstone of cochrane. the majority of systematic reviews focus on summarising the benefit of one or more therapeutic interventions for a particular condition. however, they are also important for summarising other evidence, such as the accuracy of screening and diagnostic <UNK> the causal association of risk factors for disease onset, and the prognostic ability of bespoke factors and biomarkers. prognostic evidence arises from prognosis studies, which aim to examine and predict future outcomes (such as death, disease progression, side effects or medical complications like pre-eclampsia) in people with a particular health condition or start point (such as those developing a certain disease, undergoing surgery, or women who are pregnant). the progress (prognosis research strategy) framework defines four types of prognosis research objectives: (a) to summarise overall prognosis (eg, overall risk or rate) of health outcomes for groups with a particular health <UNK> ; (b) to identify prognostic factors associated with changes in health <UNK> ; (c) to develop, validate, and examine the impact of prognostic models for individualised prediction of such <UNK> ; and (d) to identify predictors of an individual’s response to treatment summary points • primary studies to identify prognostic factors are abundant, but often findings are inconsistent and quality is variable. systematic reviews and meta-analyses are urgently needed to identify, evaluate, and summarise prognostic factor studies and their findings. • a clear review question should be defined using the picots system (population, index prognostic factor, comparator prognostic factors, outcome, timing, setting), and a transparent search undertaken for eligible articles. broad search strings may be required, leading to a large number of articles to screen. • a data extraction phase is needed to obtain the relevant information from each study. a modification of charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) can be used for prognostic factors (charms-pf). • the quips tool (quality in prognostic factor studies) can be used to examine each study’s risk of bias. unfortunately, many primary studies may have a high risk of bias because of poor design standards, conduct, and analysis. applicability of a study should also be checked. • if appropriate, meta-analysis can be used to combine prognostic effect estimates (such as hazard ratios, risk ratios, or odds ratios) across studies to produce an overall summary of a factor’s prognostic effect. between-study heterogeneity should be expected and accounted for. • ideally separate meta-analyses should be performed for unadjusted and adjusted prognostic effect estimates; adjusted estimates are important to examine a factor’s independent prognostic value over and above (that is, after adjustment for) other prognostic factors. • separate meta-analyses may also be required for each method of measurement (for factors and outcomes), each approach to handling continuous factors, and each type of estimate (such as hazard ratios or odds ratios). • publication bias is a major threat to the validity of meta-analyses of prognostic factor studies based on published evidence, and may cause small-study effects (asymmetry on a funnel plot). • remark (reporting recommendations for tumour marker prognostic studies) and prisma (preferred reporting items for systematic reviews and meta-analyses) can be used to guide the reporting of the systematic review and meta-analysis of prognostic factor studies; the degree of confidence in the summary results from the review may be examined by use of adapted forms of grade (grades of recommendation, assessment, development, and evaluation) for interventions and diagnostic test accuracy studies. • availability of individual participant data from primary prognostic factor studies may alleviate many of the challenges. <UNK> each objective requires specific methods and tools for conducting a systematic review and meta-analysis. two recent articles provided a guide to undertaking reviews and meta-analysis of prognostic (prediction) <UNK> <UNK> in this article, we focus on prognostic factors. a prognostic factor is any variable that is associated with the risk of a subsequent health outcome among people with a particular health condition. different values or categories of a prognostic factor are associated with a better or worse prognosis of future health outcomes. for example, in many cancers, tumour grade at the time of histological examination is a prognostic factor because it is associated with time to disease recurrence or death. each grade represents a group of patients with a different prognosis, and the risk or rate (hazard) of the outcome increases with higher grades. many routinely collected patient characteristics are prognostic, such as sex, age, body mass index, smoking status, blood pressure, comorbidities, and symptoms. many researched prognostic factors are biomarkers, which include a diverse range of blood, urine, imaging, electrophysiological, and physiological variables. prognostic factors have many potential uses, including aiding treatment and lifestyle decisions, improving individual risk prediction, providing novel targets for new treatment, and enhancing the design and analysis of randomised <UNK> this motivates so-called “prognostic factor research” to identify genuine prognostic factors (sometimes also called “predictor finding <UNK> <UNK> although thousands of such studies are published each year, often they are of variable quality and have inconsistent findings. systematic reviews and meta-analyses are therefore urgently needed to summarise the evidence about the prognostic value of particular <UNK> in this article, we provide a step-by-step guide on conducting such reviews. our aim is to help readers, healthcare providers, and researchers understand the key principles, methods, and challenges of reviews of prognostic factor studies. step <UNK> defining the review question the first step is to define the review question. a review of prognostic factor studies falls within the second objective of the progress <UNK> because it aims to summarise the prognostic value of a particular factor (or each of multiple factors) for relevant health outcomes and time points in people with a specific health condition (eg, disease). some reviews are broad; for example, riley and colleagues aimed to identify any prognostic factor for overall and disease free survival in children with neuroblastoma or ewing’s <UNK> other reviews have a narrower focus; for example, hemingway and colleagues aimed to summarise the evidence on whether c reactive protein (crp) is a prognostic factor for fatal and nonfatal events in patients with stable coronary <UNK> this crp review is used as an example throughout this article. charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) provides guidance for formulating a review question (table <UNK> in the article by moons and <UNK> although charms was <UNK> and <UNK> for reviews of prediction model studies, it can also be used to define and frame the question for reviews of prognostic factor studies. <UNK> and subsequent <UNK> propose a modification of the traditional pico system (population, index intervention, comparison and outcome) used in systematic reviews of therapeutic intervention studies. the modification is called picots, because it also considers timing and setting (box <UNK> in the context of prognostic factor reviews, the “p” of population and the “o” of outcome remain largely the same as in the original pico system, but the “i” refers to index prognostic factors and the “c” refers to other prognostic factors that can be considered as comparators in some way. for example, the aim may be to compare the prognostic ability of a certain index factor with one or more other (that is, comparator) prognostic factors; or to investigate the adjusted prognostic value of a particular index factor over and above (adjusted for) other (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, which is not generally recommended, then no comparator factor is being considered. the “t” denotes timing and refers to two concepts of time. firstly, at what time point the prognostic factors under review are to be measured or assessed (that is, the time point at which prognosis information is required); and secondly, over what time period the outcomes are predicted by these factors. the “s” of setting refers to the setting or context in which the index prognostic factors are to be used because the prognostic ability of a factor may change across healthcare settings. an important component of reviews of prognostic factors is whether unadjusted or adjusted estimates of the index prognostic factors will be summarised, or both. we recommend that reviewers primarily focus on adjusted prognostic factor estimates because they reveal whether a certain index factor contributes independently to the prediction of the outcome over and above (that is, after adjustment for) other prognostic factors. in particular, for each clinical scenario there are often so-called “established” or “conventional” prognostic factors that are always measured. therefore, for prognostic factors under review, it is important to understand whether they contribute additional (sometimes called “independent”) prognostic information to the routinely measured ones. this means that reviewers need adjusted (and not unadjusted or crude) prognostic effect estimates to be estimated and reported in primary prognostic factor studies. such adjusted prognostic estimates are typically derived from a multivariable regression model containing the established prognostic factors plus each index prognostic factor of interest. for example, consider a logistic regression of a binary outcome including three adjustment factors <UNK> , <UNK> , and <UNK> ) and one new index prognostic factor <UNK> ), which is expressed as: <UNK> = <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> here, “p” is the probability of the outcome. after estimation of all the unknown parameters (that is, α, <UNK> , <UNK> , <UNK> , <UNK> ), of key interest is the estimated <UNK> . this parameter provides the adjusted prognostic effect of the index prognostic factor and reveals its independent contribution to the prediction of the outcome over and above the prognostic effects of the other (established comparator) factors <UNK> , <UNK> , and <UNK> combined. the need to focus on adjusted prognostic effects is no different from (systematic reviews of) aetiological studies, in which the focus is on estimating the association of a certain causal risk factor after adjustment for other risk factors. in such causal research, these factors are usually referred to as “confounders” rather than as “other prognostic table <UNK> | charms-pf checklist of key items to be extracted from primary studies of prognostic factors, based on additions and modifications of the original charms checklist for primary studies of prediction <UNK> domain and key items general applicability risk of bias source of data: source of data (eg, cohort, case control, randomised trial, or registry data) x x x participants: participant eligibility and recruitment method (eg, consecutive participants, location, number of centres, setting, inclusion and exclusion criteria) x x x participant description x x details of treatments received (if relevant) x x study dates x x outcomes to be predicted: definition and method for measurement of outcomes x x was the same outcome definition (and method for measurement) used in all participants? x types of outcomes (eg, single or combined endpoints)? x x were the outcomes assessed without knowledge of the candidate prognostic factors (that is, blinded)? x were candidate prognostic factors part of the outcome (eg, when using a panel or consensus outcome measurement)? x time of outcome occurrence or summary of duration of follow-up x x x prognostic factors (index and comparator prognostic factors): number and type of prognostic factors (eg, obtained from demographics, patient history, physical examination, additional testing, disease characteristics) x x definition and method for measurement of prognostic factors x x timing of prognostic factor measurement (eg, at patient presentation, diagnosis, treatment initiation, at the end of surgery) x x were prognostic factors assessed blinded for outcome, and for each other (if relevant)? x handling of prognostic factors in the analysis (eg, continuous, linear, non-linear transformations or categorised) x sample size: was a sample size calculation conducted and, if so, how? x number of participants and number of outcomes or events x number of outcomes or events in relation to the number of candidate prognostic factors (events per variable) x missing data: number of participants with any missing value (in the prognostic factors and outcomes) x x number of participants with missing data for each prognostic factor of interest x details of attrition (loss to follow-up) and, for time-to-event outcomes, number of censored observations (ideally in each category for those categorical prognostic factors of interest) x handling of missing data (eg, complete case analysis, imputation, or other methods) x analysis: modelling method (eg, linear, logistic, cox, parametric survival, competing risks) regression) x x how modelling assumptions were checked; in particular, for time-to-event outcomes and the analysis of hazard ratios, the method for assessing non-proportional hazards (non-constant hazard ratios over time) x method for selection of prognostic factors for inclusion in multivariable modelling (eg, all candidate prognostic factors considered, preselection of established prognostic factors, retain only those significant from univariable analysis) x method for selection or exclusion of prognostic factors (including those of interest and those used as adjustment factors) during multivariable modelling (eg, backward or forward selection, or full model approach including all factors regardless), and criteria used for any selection or exclusion (eg, p value, akaike information criterion) x method of handling each continuous prognostic factor (eg, dichotomisation, categorisation, linear, non-linear), including values of any cutpoints used and their justification; for non-linear trends, the method of identifying non-linear relationships (eg, splines, fractional polynomials) x results: unadjusted and adjusted prognostic effect estimates (eg, risk ratios, odds ratios, hazard ratios, mean differences) for each prognostic factor of interest, and the corresponding <UNK> confidence interval (or variance or standard error). details of any non-linear relationships and whether modelling assumptions hold; in particular, for time-to-event outcomes, any evidence of non-proportional hazards (non-constant hazard ratios) for each prognostic factor of interest x x x for each extracted adjusted prognostic effect estimate of interest, the set of adjustment factors used x x x interpretation and discussion: interpretation of presented results x x comparison with other studies, discussion of generalisability, strengths and limitations x x charms=checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies. charms-pf enables reviewers to describe, assess (eg, for applicability or risk of bias), and summarise (individually and within a meta-analysis) primary studies. factors,” which is the term typically used for prognosis research. the crude (unadjusted) prognostic effect of some index factors may completely disappear after adjustment and is therefore rather uninformative, especially because prognostication in healthcare is rarely based on a single prognostic factor but rather on the information from multiple prognostic <UNK> this article focuses on systematic reviews to summarise prognostic factor effect estimates. some primary studies may also evaluate an index factor’s added value in terms of improvement in risk classification and clinical use (eg, measures such as net reclassification improvement and net benefit), and change in prediction model performance (eg, by calculating the change in the concordance index, also known as the c statistic or area under the receiver operating characteristics (roc) <UNK> however, this is beyond the scope of this article, and we refer the reader to other relevant <UNK> <UNK> <UNK> application to crp review crp is widely studied for its prognostic value in patients with coronary disease. however, there is uncertainty whether crp is useful because us and european clinical practice guidelines recommend measurement but clinical practice varies widely. this uncertainty motivated the systematic review by hemingway and <UNK> with the corresponding picots system presented in box <UNK> no studies were excluded on the basis of methodological standards, sample size, duration of follow-up, publication year, or language of publication. van der harst <UNK> blankenberg <UNK> speidl <UNK> kinjo <UNK> espinola-klein <UNK> hoffmeister <UNK> momiyama <UNK> sabatine <UNK> haim <UNK> palazzuoli <UNK> lee <UNK> brodov <UNK> minoretti <UNK> overall <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> study adjusted risk ratio <UNK> ci) adjusted risk ratio <UNK> ci) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no of adjustment factors additional to core set <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> weight (%) fig <UNK> | forest plot showing the study specific estimates and meta-analysis summary result of the adjusted prognostic effect (risk ratio) of c reactive protein taken from the review of hemingway and <UNK> all studies were adjusted for a core set of existing prognostic factors (age, sex, smoking status, diabetes, obesity, and lipids), plus up to <UNK> other prognostic factors. meta-analysis results shown are based on a random effects meta-analysis model with dersimonian and laird estimation of the between-study variances. the summary result is identical to hemingway and <UNK> but the confidence interval is wider because we used the hartung-knapp approach to account for uncertainty in variance <UNK> although “risk ratio” is used, the estimates actually correspond to a mixture of risk ratios, odds ratios, and hazard ratios box <UNK> six items (picots) defining the question for systematic reviews of prognostic factor studies, based on charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling <UNK> <UNK> and applied to a review of the adjusted prognostic value of c reactive protein <UNK> • population: define the target population forwhich prognostic factors under revieware to be used. for example, crp review: patientswith stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of acute coronary syndrome atleasttwo weeks before prognostic factor (crp) measurement. • index prognostic factor: define the factors forwhich prognostic value is under review. for example, crp review: crpwas the single biomarker reviewed for its prognostic value. • comparator prognostic factors: comparator prognostic factors can be considered in a reviewin variousways. for example, the aim could be to compare the prognostic ability of a certain index factorwith two or more other (thatis, comparator) prognostic factors; or to reviewthe adjusted prognostic value of a particular index factor—thatis, over and above (adjusted for, independent of) other existing (thatis, comparator) prognostic factors. ifthe only aim is to summarise the unadjusted prognostic effect of a particular index factor, then no comparator factor is being considered. for example, crp review: the focuswas on the adjusted prognostic value of crp—thatis, its prognostic effect after adjusting for existing (comparator) prognostic factors. in particular, adjustmentfor the following conventional prognostic factorswas of interest: age, sex, smoking status, obesity, diabetes, and one or more lipid variables (from total cholesterol, lowdensity lipoprotein cholesterol, high density lipoprotein cholesterol, triglycerides) and inflammatory markers (fibrinogen, interleukin <UNK> cell count). • outcome: define the outcomes forwhich the prognostic ability ofthe factor(s) under revieware of interest. for example, crp review: outcome events were defined as coronary (coronary death, sudden cardiac death, acute non-fatal myocardial infarction, primary percutaneous coronary intervention, unplanned emergency admissionswith unstable angina), cardiovascular (when coronary eventswere reported in combinationwith heartfailure, stroke, or peripheral arterial disease), and all cause mortality. • timing: define firstly atwhattime points the prognostic factors (index and comparators) are to be used (thatis, the time point of prognostication), and secondly overwhattime period the outcomes are predicted by these factors. for example, crp review: the crp measurement had to be done at leasttwoweeks after diagnosis and all follow-up information on the outcomes (alltime periods)was extracted from the studies. • setting: define the intended setting and role ofthe prognostic factors under review. for example, crp review: crp measurementwas studied in primary and secondary care to provide prognostic information about patients diagnosedwith coronary heart disease; this information may be useful for healthcare professionals treating and managing such patients. step <UNK> searching for and selection of eligible studies the next step is to identify primary studies that are eligible for review; studies that address the review question defined in step <UNK> following the picots framework. unfortunately, it is more difficult to identify prognostic factor studies than randomised trials of interventions. prognosis studies do not tend to be indexed (“tagged”) because a taxonomy of prognosis research is not widely recognised. moreover, compared with studies of interventions, there is much more variation in the design of prognostic factor studies (eg, data from cohort studies, randomised trials, routine care registries, and case-control studies can all be used), patient inclusion criteria, prognostic factor and outcome measurement, follow-up time, methods of statistical analysis, and adjustment of (and number of) other prognostic factors (covariates). between-study heterogeneity is therefore the rule rather than the exception in prognostic factor research. it is essential that systematic reviews of prognostic factor studies define the study inclusion and exclusion criteria based on the picots structure (step <UNK> because this determines the study search and selection strategy. typically, broad search and selection filters are required that combine terms related to prognosis research (such as prognostic, predict, predictor, factor, independent) with domain or disease specific terms (such as the name of prognostic factors and the targeted disease or patient <UNK> a broad search comes at the (often considerable) expense of retrieving many irrelevant records. geersing and <UNK> validated various existing search strategies for prognosis studies and suggested a generic filter for identifying studies of prognostic <UNK> <UNK> <UNK> which extended the work of ingui, haynes, and <UNK> <UNK> <UNK> when tested in a single review of prognostic factors, this generic filter had a number needed to read of <UNK> to identify one relevant article, emphasising the difficulty in targeting prognostic factor articles. the number needed to read could be considerably reduced when specific factors or populations are added to the filter. even then, care is needed to be inclusive because multiple terms are often used for the same meaning; for example, biomarker mycn is also referred to as n-myc and nmyc, among other <UNK> once the search is complete, each potentially relevant study must be screened for its applicability to the review question. because of the heterogeneity in prognostic factor studies, during this study selection phase more deviations from the defined picots (in step <UNK> are possible (far greater than what is typically encountered during the selection of randomised intervention studies). the applicability of this primary study selection should firstly be based on title and abstract screening, followed by full text screening, both ideally done by two researchers independently. any discrepancies should be resolved through discussion, potentially with a third reviewer. to check if any relevant articles have been missed, it is helpful to share the list of identified articles with researchers in the field to examine the reference lists of these articles and to perform a citation search. application to crp review hemingway and colleagues included any prospective observational study that reported risk of subsequent events among patients with stable coronary disease in relation to measured crp <UNK> eligible studies had to include patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of previous acute coronary syndrome at least <UNK> weeks before crp measurement. hemingway and colleagues searched medline between <UNK> and <UNK> november <UNK> and embase between <UNK> and <UNK> december <UNK> using a search string containing terms for coronary disease, prognostic studies, and crp. the search identified <UNK> articles, of which <UNK> fulfilled the inclusion criteria. if specific terms for crp had not been included in the search string, then the total number of identified articles would have far exceeded <UNK> step <UNK> data extraction the next step is to extract key information from each selected study. data extraction provides the necessary data from each study, which enables reviewers to examine their (eventual) applicability to the review question and their risk of bias (see step <UNK> this step also provides the information required for subsequent qualitative and quantitative (meta-analysis) synthesis of the evidence across studies. the charms checklist gives explicit guidance (table <UNK> in the article by moons and <UNK> about which key items across <UNK> domains should be extracted from primary studies of prediction models, and for what reason (that is, to provide general information about the primary study, to guide risk of bias assessment, or to assess applicability of the primary study to the review question). based on our experience of conducting systematic reviews of prognostic factor studies, we modified the original charms checklist for prediction model studies to make it suitable for data extraction in reviews of prognostic factors (here referred to as charms-pf; table <UNK> this basically means that three domains typically addressing multivariable prediction modelling aspects were combined to one overall analysis domain, while other domain names and key items were slightly reworded or extended. reasons for extraction of each key item are similar to charms for prediction models. because we developed the original charms checklist, a wider consensus of the charmspf content was not considered necessary. reviewers should extract fundamental information from the primary prognostic factor studies, such as the dates, setting, study design, definitions of start points, outcomes, follow-up length, and prognostic factors; reviewers will often find large heterogeneity in this information across studies. the extracted information can be summarised in tables of study characteristics. in addition, more specific information is needed to properly assess applicability and risk of bias (see step <UNK> such as methods used to measure prognostic factors and outcomes, handling missing data, attrition (loss to follow-up), and whether estimated associations of the prognostic factors under review were adjusted for other prognostic factors. this information also enhances the potential for meta-analysis and the presentation and interpretation of subsequent summary results (see steps <UNK> to enable meta-analysis of prognostic factor studies, the key elements to extract are estimates, and corresponding standard errors or confidence intervals, of the prognostic effect for each factor of interest; for example, the estimated risk ratio or odds ratio (for binary outcomes), hazard ratio (for time-to-event outcomes), or mean difference (for continuous outcomes). as most prognostic factor studies consider time-to-event outcomes (including censored observations and different follow-up lengths for patients), hazard ratios are often the most suitable effect measure. a concern is that hazard ratios may not be constant over time, and therefore any evaluations of non-proportional hazards (that is, non-constant hazard ratios for the prognostic factors of interest) should also be extracted; however, such information is rarely reported in sufficient detail. unfortunately, many prognostic factor studies do not adequately report estimated prognostic effect measures or their precision. for this reason, methods are available to restore the missing information upon data extraction. in particular, parmar and <UNK> and tierney and <UNK> describe how to obtain unadjusted hazard ratio estimates (and their variances) when they are not reported directly. for example, under assumptions, the number of outcomes (events) and an available p value (eg, from a log rank test or cox regression) can be used to indirectly estimate the unadjusted hazard ratio between two groups defined by a particular factor (eg, “high” versus “normal” levels). perneger and <UNK> report how to derive unadjusted hazard ratios from survival proportions, and pérez and colleagues suggest using a simulation <UNK> even with such indirect estimation methods, not all results can be obtained. for example, in a systematic review of <UNK> studies investigating prognostic factors in <UNK> the methods of parmar and colleagues were used to obtain <UNK> hazard ratio estimates and their confidence intervals; but this represented only <UNK> of the potential evidence. although indirect estimation methods help retrieve unadjusted prognostic factor effect estimates, they often have limited value for obtaining adjusted effect table <UNK> | quips tool (quality in prognostic factor studies), which can be used to classify risk of bias of prognostic factor studies domains signalling items risk of bias ratings <UNK> study participation (a) adequate participation in the study by eligible persons (b) description of the target population or population of interest (c) description of the baseline study sample (d) adequate description of the sampling frame and recruitment (e) adequate description of the period and place of recruitment (f) adequate description of inclusion and exclusion criteria high: the relationship between the pf and outcome is very likely to be different for participants and eligible non-participants moderate: the relationship between the pf and outcome may be different for participants and eligible non-participants low: the relationship between the pf and outcome is unlikely to be different for participants and eligible non-participants <UNK> study attrition (a) adequate response rate for study participants (b) description of attempts to collect information on participants who dropped out (c) reasons for loss to follow-up are provided (d) adequate description of participants lost to follow-up (e) there are no important differences between participants who completed the study and those who did not high: the relationship between the pf and outcome is very likely to be different for completing and non-completing participants moderate: the relationship between the pf and outcome may be different for completing and non-completing participants low: the relationship between the pf and outcome is unlikely to be different for completing and non-completing participants <UNK> prognostic factor measurement (a) a clear definition or description of the pf is provided (b) method of pf measurement is adequately valid and reliable (c) continuous variables are reported or appropriate cutpoints are used (d) the method and setting of measurement of pf is the same for all study participants (e) adequate proportion of the study sample has complete data for the pf (f) appropriate methods of imputation are used for missing pf data high: the measurement of the pf is very likely to be different for different levels of the outcome of interest moderate: the measurement of the pf may be different for different levels of the outcome of interest low: the measurement of the pf is unlikely to be different for different levels of the outcome of interest <UNK> outcome measurement (a) a clear definition of the outcome is provided (b) method of outcome measurement used is adequately valid and reliable (c) the method and setting of outcome measurement is the same for all study participants high: the measurement of the outcome is very likely to be different related to the baseline level of the pf moderate: the measurement of the outcome may be different related to the baseline level of the pf low: the measurement of the outcome is unlikely to be different related to the baseline level of the pf <UNK> adjustment for other prognostic factors (a) all other important pfs are measured (b) clear definitions of the important pfs measured are provided (c) measurement of all important pfs is adequately valid and reliable (d) the method and setting of pf measurement are the same for all study participants (e) appropriate methods are used to deal with missing values of pfs, such as multiple imputation (f) important pfs are accounted for in the study design (g) important pfs are accounted for in the analysis high: the observed effect of the pf on the outcome is very likely to be distorted by another factor related to pf and outcome moderate: the observed effect of the pf on outcome may be distorted by another factor related to pf and outcome low: the observed effect of the pf on outcome is unlikely to be distorted by another factor related to pf and outcome <UNK> statistical analysis and reporting (a) sufficient presentation of data to assess the adequacy of the analytic strategy (b) strategy for model building is appropriate and is based on a conceptual framework or model (c) the selected statistical model is adequate for the design of the study (d) there is no selective reporting of results high: the reported results are very likely to be spurious or biased related to analysis or reporting moderate: the reported results may be spurious or biased related to analysis or reporting low: the reported results are unlikely to be spurious or biased related to analysis or reporting pf=prognostic factor. some wording from hayden and <UNK> has been modified to be consistent with the terminology used in this <UNK> estimates. furthermore, even when multiple studies provide the adjusted prognostic effect of a particular factor, the set of adjustment factors will usually differ across studies, which complicates the interpretation of subsequent meta-analysis results. we recommend that reviewers predefine the core set of prognostic factors for the outcome of interest (eg, age, sex, smoking status, disease stage) that represents the desired “minimal” set of adjustment factors. an agreed process among health professionals and researchers in the field could be required to define this set. for example, a list of established prognostic factors could be identified that are routinely used within current prognostication of the clinical population of interest. it may also be necessary to standardise the extracted estimates to ensure they all relate to the same scale and direction in each study. in particular, the direction of the prognostic effect will need standardising if one study compares the hazard rate in a factor’s “high” versus “normal” group, whereas another study compares the hazard rate in the factor’s “normal” versus “high” group. when the outcome is defined differently across studies, approaches to convert effect measures on different outcome scales could be <UNK> also, to deal with different cutpoint levels for values of a particular <UNK> the prognostic effects of “high” versus “normal” could be converted to prognostic effects relating to a <UNK> unit increase in the factor. this requires assumptions about the underlying distribution of the factor. such an approach was used by hemingway and <UNK> of concern, however, is that the actual distribution of a prognostic factor may be unknown (or even vary across studies). finally, it is also possible to derive standardised effect estimates by standardising the corresponding regression <UNK> application to crp review hemingway and colleagues extracted background information such as year of study start, number of included patients, mean age, baseline coronary morbidity (eg, proportion with stable angina), average levels of biomarker at baseline, method of crp measurement, follow-up duration, and number and type of events. basic information was often missing. for example, nearly a fifth of studies did not report the method of measurement, and only a quarter gave the number of patients included in the analyses and reasons for dropout. prognostic effect estimates for crp were extracted in terms of the reported risk ratio, odds ratio, or hazard ratio (labelled generally as “risk ratio” in this article), and <UNK> confidence intervals. these effect estimates were then converted to a standardised scale comparing the highest third with the lowest third of the (log transformed) crp distribution. if available, separate prognostic effect estimates were extracted for different degrees of adjustment for other prognostic factors. step <UNK> evaluating applicability and risk of bias of primary studies once eligible studies are identified and data are extracted, an important next step is to assess the applicability and risk of bias (quality) of each study in the review. as for steps <UNK> and <UNK> ideally this is done by two reviewers, independently, with any discrepancies resolved. applicability refers to the extent to which a selected study (in step <UNK> matches the review question in terms of the population, timing, prognostic factors, and outcomes (endpoints) of interest. just because a study is eligible for inclusion does not mean it is free from applicability concerns. some aspects of a study may be applicable (eg, correct condition at start point, with prognostic factors of interest evaluated) but not others (eg, incorrect population or setting, inappropriate outcome definition, different follow-up time, lack of adjustment for conventional prognostic factors). applicability is typically first assessed during title and abstract screening, and then during this step, so that it is based on full text screening and determined by picots (step <UNK> and inclusion and exclusion criteria of studies (step <UNK> risk of bias refers to the extent to which flaws in the study design or analysis methods could lead to bias in estimates of the prognostic factor effects. unfortunately, based on growing empirical evidence from systematic reviews examining methodology quality, many primary studies will be at high risk of <UNK> <UNK> <UNK> for prognostic factor studies, hayden and colleagues developed the quips checklist (quality in prognostic factor studies) for examining risk of bias across six <UNK> study participation, study attrition, prognostic factor measurement, outcome measurement, adjustment for other prognostic factors, and statistical analysis and reporting. table <UNK> shows the signalling items within these domains to help guide reviewers in making low, unclear, or high risk of bias classifications. additional guidance may be found in general tools examining the quality of observational <UNK> <UNK> and the remark guideline (reporting recommendations for tumour marker prognostic studies) for reporting of primary prognostic factor <UNK> <UNK> we recommend that users first operationalise criteria to assess the signalling items and domains for the specific review question. for example, with the study participation and attrition domains, this includes defining a priori the most important characteristics that could indicate a systematic bias in study recruitment (study participation domain) and loss to follow-up (study attrition domain). defining these characteristics ahead of time will facilitate assessment and consensus related to the importance of potential differences that could influence the observed association between the index prognostic factors and outcomes of interest. definitions of sufficiently valid and reliable measurement of the index prognostic factors and outcomes should also be specified at the protocol stage. similarly, the core set of other (adjustment) prognostic factors that are deemed necessary for the primary studies to have adjusted for, should be predefined to facilitate judgment related to risk of bias in domain <UNK> overall assessment of the six risk of bias domains is undertaken by considering the risk of bias information from the signalling items for each domain, rated as low, moderate, and high risk of bias. occasionally, item information needed to assess the bias domains is not available in the study report. when this occurs, other publications that may have used the same dataset (which often occurs in prognostic studies based on large existing cohorts) should be consulted and study authors should be contacted for additional information. an informed judgment about the potential risk of bias for each bias domain should be made independently by two reviewers, and discussed to reach consensus. each of the six domains needs to be rated and reported separately because this will inform readers, flag improvements needed for subsequent primary studies, and facilitate future meta-epidemiological research. we recommend defining studies with an overall “low risk of bias” as those studies where all, or the most important domains (as determined a priori), are rated as having low (or low to moderate) risk of bias. application to crp review hemingway and colleagues assessed the quality of included studies by the quality of their reporting on <UNK> items derived from the remark <UNK> the median number of study quality items reported was seven of a possible <UNK> and standards did not change between <UNK> and <UNK> only two studies referred to a study protocol, with none referring to a statistical analysis plan. hemingway and colleagues noted that this “makes it difficult to know what the specific research objectives were at the start of cohort recruitment, at the time of crp measurement, or at the onset of the statistical <UNK> only two studies reported the time elapsed between first lifetime presentation with coronary disease and assessment of crp and this raised applicability concerns. step <UNK> meta-analysis meta-analysis of prognostic factor studies aims to summarise the (adjusted) prognostic effect of each factor of interest. in addition to missing estimates, challenges for the meta-analyst include (a) having different types of prognostic effect measures (eg, odds ratios and hazard ratios), which are not necessarily <UNK> (b) estimates without standard errors, which is a problem because meta-analysis methods typically weight each study by (a function of) their standard error; (c) estimates relating to various time points of the outcome occurrence or measurement; (d) different methods of measurement for prognostic factors and outcomes; (e) various sets of adjustment factors; and (f) different approaches to handling continuous prognostic factors (eg, categorisation, linear, non-linear trends), including the choice of cutpoint value when dichotomising continuous values into “high” and “normal” groups. many of these issues lead to substantial heterogeneity and if a meta-analysis is performed, summary results cannot be directly interpreted. generally, meta-analysis results will be most interpretable, and therefore useful, when a separate meta-analysis is undertaken for groups of “similar” prognostic effect measures. in particular, we suggest considering a meta-analysis for: • hazard ratios, odds ratios, and risk ratios separately • unadjusted and adjusted associations separately • prognostic factor effects at distinct cutpoints (or groups of similar cutpoints) separately • prognostic factor effects corresponding to a linear trend (association) separately • prognostic factor effects corresponding to non-linear trends separately • each method of measurement (for factors and outcomes) separately. ideally a meta-analysis of adjusted results should ensure that all included estimates are adjusted for the same set of other prognostic factors. this situation is unlikely and so a compromise could be to ensure that all adjusted estimates in the same meta-analysis have adjusted for at least a (predefined) minimum set of adjustment factors (that is, a core set of established prognostic factors). even when adhering to this guidance, unexplained heterogeneity is likely to remain because of other reasons (eg, differences in length of follow-up or in treatments received during follow-up). therefore, if a meta-analysis is performed, a random effects approach is essential to allow for unexplained heterogeneity across studies (box <UNK> as previously described in the bmj. <UNK> this approach provides a summary estimate of the average prognostic effect of the index factor and the variability in effect across studies. also potentially useful are meta-analysis methods to estimate the trend (eg, linear effect) of a prognostic factor that has been grouped into three or more categories within studies (with each category compared with the reference category). these methods generally model the estimated prognostic effect sizes in each category as a function of “exposure” level (eg, midpoint or median prognostic factor value in the category) and account for within-study correlation and betweenstudy <UNK> to apply these methods, some additional knowledge of the factor’s underlying distribution is usually needed to help define the “exposure” level because the chosen value can have an impact on the <UNK> application to crp review hemingway and <UNK> applied a random effects meta-analysis to combine <UNK> adjusted prognostic effect estimates for crp from studies that adjusted for at least one of six conventional risk factors (age, sex, smoking status, diabetes, obesity, and lipids). the summary meta-analysis result was a risk ratio of <UNK> <UNK> confidence interval <UNK> to <UNK> which gives the average prognostic effect of crp (for those in the top v bottom third of crp distribution), and suggests larger crp values are associated with higher risk. although there was substantial between-study heterogeneity, nearly all estimates were in the same direction (that is, risk ratio <UNK> when restricting meta-analysis to just the <UNK> studies that adjusted for at least all six conventional prognostic factors, the summary risk ratio decreased to <UNK> <UNK> confidence interval <UNK> to <UNK> and the between-study heterogeneity reduced. using the study specific estimates given by hemingway and colleagues, we updated this meta-analysis (fig <UNK> obtaining the same summary result but a wider confidence interval <UNK> to <UNK> through the hartung-knapp <UNK> step <UNK> quantifying and examining heterogeneity for all meta-analyses, when there is large heterogeneity across included studies, it may be better not to synthesise the study results, but rather display the variability in estimates on a forest plot without showing an overall pooled estimate. when a metaanalysis is performed in the face of heterogeneity, it is important to quantify and report the magnitude of heterogeneity itself; for example, through the estimate of (the between-study <UNK> or an approximate <UNK> prediction interval indicating the potential true prognostic effect of a factor in a new <UNK> <UNK> subgroup analyses and meta-regression can be used to examine or explore the causes of heterogeneity. a subgroup analysis performs a separate meta-analysis for categories defined by a particular characteristic, such as those with a low risk of bias, those with a follow-up of less than one year or of at least one year, or those set in countries in europe. a better approach is meta-regression, which extends the meta-analysis equation shown in box <UNK> by including study level <UNK> and allows a formal comparison of metaanalysis results across groups defined by covariates (eg, low risk of bias studies v studies at higher risk of bias). unfortunately, subgroup analyses and metaregression are often problematic. there will often be few studies per subgroup and low power to detect genuine causes of heterogeneity. furthermore, study level confounding will be rife so that it is difficult to disentangle the associations for one covariate from another. for example, studies with a low risk of bias may also have a different length of follow-up or a particular cutpoint level compared with studies at higher risk of bias. application to crp review hemingway and colleagues reported that metaregression identified four study level covariates that explained some between-study heterogeneity in the prognostic effect of crp: definition of comparison group, number of adjustment factors, the (log) number of events, and the proportion of patients with stable coronary disease (reflecting study <UNK> studies originally reporting unequal crp groups had stronger effects than those reporting crp on a continuous scale. for each additional adjustment factor, the summary risk ratio decreased by <UNK> the summary risk ratio was smaller among studies with more than the median number of outcome events, and smaller among studies confined to stable coronary disease. there was no evidence that the crp effect differed according to the number of quality items reported by a study, or by the type of prognostic effect measure provided (that is, risk ratio, odds ratio, or hazard ratio). step <UNK> examining small-study effects the term “small-study effects” refers to when there is a systematic difference in prognostic effect estimates for small studies and large <UNK> a particular concern is when small studies (especially those that are exploratory because these often evaluate many potential prognostic factors with relatively few outcome events) show larger prognostic effects than larger studies. this difference may be due to chance or heterogeneity, but a major threat here is publication bias and selective reporting, which are endemic in prognosis <UNK> such reporting biases lead to smaller studies, with (statistically) significant or larger prognostic factor effect estimates being more likely to be published or reported in sufficient detail, and thus included in a meta-analysis, than smaller studies with non-significant or smaller prognostic effect estimates. this bias is a potential concern for unadjusted and adjusted prognostic effects. a primary study usually estimates an unadjusted prognostic effect for each of multiple prognostic factors, but study authors may only report effects that are statistically significant. in addition, adjusted results are often only reported for prognostic factors that retain statistical significance in univariable and multivariable analysis. a consequence is that meta-analysis results will be biased, with larger summary prognostic effects than in reality, and potentially some factors being deemed to have clinical value when actually they do not. the evidence for small-study effects is usually considered on a funnel plot, which shows the study estimates (x axis) against their precision (y axis). a funnel plot is usually recommended if there are <UNK> box <UNK> explanation of a random effects meta-analysis of prognostic factor effect estimates the true prognostic effect of a factor is likely to vary from study to study; therefore assuming a common (fixed) prognostic effectis not sensible. if yi and var(yi ) denote the prognostic effect estimate (eg, ln(hazard ratio), ln(odds ratio), ln(risk ratio), or mean difference) and its variance in study i, then a generalrandom effects meta-analysis model can be specified as: yi ~n(μ,var(yi <UNK> ). mostresearchers use either restricted maximum likelihood or the approach of dersimonian and laird to estimate this model, <UNK> but other options are available, including a bayesian <UNK> key interestis the estimate of μ,which reveals the summary (average) prognostic effect ofthe index prognostic factor of interest. the standard deviation ofthis prognostic factor effect across studies is denoted by τ, and non-zero values suggestthere is between-study heterogeneity. confidence intervals for µ should ideally accountfor uncertainty in estimated variances (in particular <UNK> andwe have found the approach ofhartung-knapp to be robustfor this purpose in most <UNK> <UNK> when synthesising prognostic effects on the log scale, the summary results and confidence intervals require back transformation (using the exponential function) to the original scale. advanced multivariate meta-analysis methods are also available to handle multiple <UNK> multiple methods of <UNK> or different adjustmentfactors in prognostic factor <UNK> an introduction to multivariate meta-analysis has been published in the bmj. <UNK> or more <UNK> the plot should ideally show a symmetric, funnel like shape, with results from larger studies at the centre of the funnel and smaller studies spanning out in both directions equally. asymmetry will arise if there are small-study effects, with a greater proportion of smaller studies in one particular direction. statistical tests for asymmetry in risk, odds and hazard ratios can be used, such as peter’s and debray’s <UNK> <UNK> contour enhanced funnel plots also show the statistical significance of individual studies, and “missing” studies are perhaps more likely to fall within regions of non-significance if publication bias was the cause of small-study effects. an example is shown in figure <UNK> as mentioned, small-study effects may also arise due to heterogeneity. therefore, it is difficult to disentangle publication bias from heterogeneity in a single review. for example, if smaller studies used an analysis with fewer adjustment factors, then this may cause larger prognostic factor effects in such studies, rather than it being caused by publication bias. a multivariate meta-analysis could reduce the impact of small-study effects by “borrowing strength” from related <UNK> a related concern is that smaller prognostic factor studies are generally at higher risk of bias than larger studies. smaller studies tend to be more exploratory in nature and typically based on a convenient sample, often examining many (sometimes hundreds of) potential prognostic factors, with relatively few outcome events. this design leads to spurious (due to chance) and potentially biased (due to poor estimation <UNK> prognostic effect estimates, which are more prone to selective reporting. in contrast, larger studies are often confirmatory studies focusing on one or a few prognostic factors, and are more likely to adopt a protocol driven and prospective approach, with clearer reporting regardless of their <UNK> therefore, larger studies are less likely to identify spurious prognostic factor effect estimates. it is helpful to examine small-study effects (potential publication bias) when restricting analysis to the subset of studies at low risk of bias. if this approach resolves previous issues of small-study effects in the full meta-analysis, then it gives even more credence to focus conclusions and recommendations on the meta-analysis results based only on the higher quality studies. application to crp review figure <UNK> shows a funnel plot of the study estimates from the crp meta-analysis shown in figure <UNK> there is clear asymmetry, which shows the strong potential for publication bias. there was an insufficient number of studies considered at low risk of bias to evaluate smallstudy effects in a subset of higher quality studies. step <UNK> reporting and interpretation of results as with all research studies, clear and complete reporting is essential for reviews of prognostic factor studies. most of the reporting guidelines of prisma (preferred reporting items for systemic reviews and meta-analyses) and moose (meta-analysis of observational studies in epidemiology) will be <UNK> <UNK> and should be complemented by <UNK> <UNK> which was aimed at primary prognostic factor studies. more specific guidance for reporting systematic reviews of prognostic factor studies is under development. interpretation and translation of summary metaanalysis results is an important final step. the guidance in the previous steps is the essential input for this step. discussion is necessary on whether and how the prognostic factors identified may be useful in practice (that is, translation of results to clinical practice), and what further research is necessary. ideally impact studies (eg, randomised trials that compare groups which do and do not use a prognostic factor to inform clinical practice) are needed before strong recommendations for clinical practice are made; however, these studies are rare and outside the scope of the review framework outlined in this article. to interpret the certainty (confidence) of the summary results of a review of intervention effectiveness, grade (grades of recommendation, assessment, development, and evaluation) was developed. this approach assesses the overall quality of and certainty in evidence for the summary estimates of the intervention effects by addressing five domains: risk of bias, inconsistency, imprecision, indirectness, and publication bias. the grade domains can be assessed using the information obtained by the tools and methods described in the above steps. however, it is not known whether these domains, developed for reviews of interventions, are equally applicable to assessing the certainty of summary results of systematic reviews of prognostic factor studies. compared with reviews of intervention studies, allowing for heterogeneity (the inconsistency domain) might be more acceptable in reviews of in (rr) standard error of in (rr) studies p < <UNK> <UNK> < p < <UNK> <UNK> < p < <UNK> p > <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> fig <UNK> | evidence of funnel plot asymmetry (small-study effects) in the c reactive protein meta-analysis shown in figure <UNK> the smaller studies (with higher standard errors) have risk ratio (rr) estimates mainly to the right of the larger studies, and therefore give the largest prognostic effect estimates. a concern is that this is due to publication bias, with “missing” studies potentially falling to the left side of the larger studies and in the lighter shaded regions denoting non-significant rr estimates prognostic factor studies because of the inevitable heterogeneity caused by study differences in methods of measurement, adjustment factors, and statistical analysis methods, among others. furthermore, the threat of selective reporting or publication bias in reviews of prognostic factor studies may be more severe than in reviews of intervention studies because of the problems of exploratory studies, poor reporting, and biased analysis methods. there is limited empirical evidence for using the existing domains to grade the certainty of summary estimates of prognostic factor studies, although a first attempt has been <UNK> in addition, an assessment has been performed on grading the certainty of evidence of summary estimates of overall prognosis <UNK> reviewers need to be especially cautious when comparing the adjusted prognostic value of multiple index factors, for example, to conclude whether the summary adjusted hazard ratio for prognostic factor a is larger than that for factor b. usually different sets of studies will be available for each index factor, and so the comparison will be indirect and potentially biased. moreover, the studies evaluating factor a may often have used different sets of adjustment factors (other prognostic factors) than those evaluating factor b. it will be rare to find studies on different index factors that used exactly the same set of adjustment factors. we therefore recommend reviewers restrict comparisons (of the adjusted prognostic value) of two or more index factors to those studies that at least used a similar, minimally required set of adjustment <UNK> even then, due to different scales and distributions of each factor (eg, continuous or binary), a simple comparison of the prognostic effect sizes (eg, hazard ratio for factor a v hazard ratio for factor b) may not be straightforward. application to crp review the meta-analysis results suggest crp is a prognostic factor for the risk of death and non-fatal cardiovascular events, even when only including the largest studies that adjusted for all six conventional prognostic factors. in their discussion, hemingway and colleagues downgraded the meta-analysis findings because of a strong concern about the quality and reliability of the underlying <UNK> the absence of prespecified protocols, poor and potentially biased reporting, and strong potential for publication bias prevented the authors from making firm conclusions about whether crp has prognostic value after adjustment for established prognostic factors. they state that the concerns “explicitly challenge the statement for healthcare professionals made by the centers for disease control that measuring crp is both ‘useful’ and ‘independent’ as a marker of <UNK> summary in this article, we described the key steps and methods for conducting a systematic review and meta-analysis of prognostic factor studies. current reviews are often limited by the quality and heterogeneity of primary <UNK> <UNK> we expect the prevalence of such reviews to grow rapidly, especially as cochrane has recently embarked on prognosis reviews (see also the cochrane prognosis methods group website www.methods. <UNK> our guidance will help researchers to write grant applications for reviews of prognostic factor studies, and to develop protocols and conduct such reviews. protocols of prognostic factor reviews should be published ideally at the same time as the review is registered, for example within prospero, the international prospective register of systematic reviews (www.crd.york.ac.uk/prospero/), or the cochrane <UNK> our guidance will also allow readers and healthcare providers to better judge reports of prognostic factor reviews. finally, we note that some of the limitations described (eg, use of different cutpoint values across studies) could be alleviated if the individual participant data were obtained from primary prognostic factor <UNK> rather than being extracted from study publications; although, this may not solve all problems (eg, quality of original study, availability of different adjustment <UNK> further discussion on individual participant data meta-analysis of prognostic factor studies is given <UNK>

<|EndOfText|>

bstract objective to review and appraise the validity and usefulness of published and preprint reports of prediction models for diagnosing coronavirus disease <UNK> <UNK> in patients with suspected infection, for prognosis of patients with <UNK> and for detecting people in the general population at increased risk of becoming infected with <UNK> or being admitted to hospital with the disease. design living systematic review and critical appraisal by the covid-precise (precise risk estimation to optimise <UNK> care for infected or suspected patients in diverse settings) group. data sources pubmed and embase through ovid, arxiv, medrxiv, and biorxiv up to <UNK> may <UNK> study selection studies that developed or validated a multivariable <UNK> related prediction model. data extraction at least two authors independently extracted data using the charms (critical appraisal and data extraction for systematic reviews of prediction modelling studies) checklist; risk of bias was assessed using probast (prediction model risk of bias assessment tool). results <UNK> <UNK> titles were screened, and <UNK> studies describing <UNK> prediction models were included. the review identified four models for identifying people at risk in the general population; <UNK> diagnostic models for detecting <UNK> <UNK> were based on medical imaging, nine to diagnose disease severity); and <UNK> prognostic models for predicting mortality risk, progression to severe disease, intensive care unit admission, ventilation, intubation, or length of hospital stay. the most frequently reported predictors of diagnosis and prognosis of <UNK> are age, body temperature, lymphocyte count, and lung imaging features. flulike symptoms and neutrophil count are frequently predictive in diagnostic models, while comorbidities, sex, c reactive protein, and creatinine are frequent prognostic factors. c index estimates ranged from <UNK> to <UNK> in prediction models for the general population, from <UNK> to more than <UNK> in diagnostic models, and from <UNK> to <UNK> in prognostic models. all models were rated at high risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, high risk of model overfitting, and vague reporting. most reports did not include any description of the study population or intended use of the models, and calibration of the model predictions was rarely assessed. conclusion prediction models for <UNK> are quickly entering the academic literature to support medical decision making at a time when they are urgently needed. this review indicates that proposed models are poorly reported, at high risk of bias, and their reported for numbered affiliations see end of the article what is already known on this topic the sharp recent increase in coronavirus disease <UNK> <UNK> incidence has put a strain on healthcare systems worldwide; an urgent need exists for efficient early detection of <UNK> in the general population, for diagnosis of <UNK> in patients with suspected disease, and for prognosis of <UNK> in patients with confirmed disease viral nucleic acid testing and chest computed tomography imaging are standard methods for diagnosing <UNK> but are time consuming earlier reports suggest that elderly patients, patients with comorbidities (chronic obstructive pulmonary disease, cardiovascular disease, hypertension), and patients presenting with dyspnoea are vulnerable to more severe morbidity and mortality after infection what this study adds four models identified patients at risk in the general population (using proxy outcomes for <UNK> ninety one diagnostic models were identified for detecting <UNK> <UNK> were based on medical images; nine were for severity classification); and <UNK> prognostic models for predicting, among others, mortality risk, progression to severe disease proposed models are poorly reported and at high risk of bias, raising concern that their predictions could be unreliable when applied in daily practice performance is probably optimistic. hence, we do not recommend any of these reported prediction models for use in current practice. immediate sharing of well documented individual participant data from <UNK> studies and collaboration are urgently needed to develop more rigorous prediction models, and validate promising ones. the predictors identified in included models should be considered as candidate predictors for new models. methodological guidance should be followed because unreliable predictions could cause more harm than benefit in guiding clinical decisions. finally, studies should adhere to the tripod (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) reporting guideline. systematic review registration protocol <UNK> registration https:// <UNK> readers’ note this article is a living systematic review that will be updated to reflect emerging evidence. updates may occur for up to two years from the date of original publication. this version is update <UNK> of the original article published on <UNK> april <UNK> (bmj <UNK> and previous updates can be found as data supplements (https://www.bmj.com/ <UNK> introduction the novel coronavirus disease <UNK> <UNK> presents an important and urgent threat to global health. since the outbreak in early december <UNK> in the hubei province of the people’s republic of china, the number of patients confirmed to have the disease has exceeded <UNK> <UNK> <UNK> in <UNK> countries, and the number of people infected is probably much higher. more than <UNK> <UNK> people have died from <UNK> (up to <UNK> june <UNK> despite public health responses aimed at containing the disease and delaying the spread, several countries have been confronted with a critical care crisis, and more countries could <UNK> outbreaks lead to important increases in the demand for hospital beds and shortage of medical equipment, while medical staff themselves could also get infected. to mitigate the burden on the healthcare system, while also providing the best possible care for patients, efficient diagnosis and information on the prognosis of the disease is needed. prediction models that combine several variables or features to estimate the risk of people being infected or experiencing a poor outcome from the infection could assist medical staff in triaging patients when allocating limited healthcare resources. models ranging from rule based scoring systems to advanced machine learning models (deep learning) have been proposed and published in response to a call to share relevant <UNK> research findings rapidly and openly to inform the public health response and help save <UNK> many of these prediction models are published in open access repositories, ahead of peer review. we aimed to systematically review and critically appraise all currently available prediction models for <UNK> in particular models to predict the risk of developing <UNK> or being admitted to hospital with <UNK> models to predict the presence of <UNK> in patients with suspected infection, and models to predict the prognosis or course of infection in patients with <UNK> we included model development and external validation studies. this living systematic review, with periodic updates, is being conducted by the covid-precise (precise risk estimation to optimise <UNK> care for infected or suspected patients in diverse settings) group in collaboration with the cochrane prognosis methods group. methods we searched pubmed and embase through ovid, biorxiv, medrxiv, and arxiv for research on <UNK> published after <UNK> january <UNK> we used the publicly available publication list of the <UNK> living systematic <UNK> this list contains studies on <UNK> published on pubmed and embase through ovid, biorxiv, and medrxiv, and is continuously updated. we validated whether the list is fit for purpose (online supplementary material) and further supplemented it with studies on <UNK> retrieved from arxiv. the online supplementary material presents the search strings. additionally, we contacted authors for studies that were not publicly available at the time of the <UNK> <UNK> and included studies that were publicly available but not on the living systematic <UNK> list at the time of our <UNK> we searched databases repeatedly up to <UNK> may <UNK> (supplementary table <UNK> all studies were considered, regardless of language or publication status (preprint or peer reviewed articles; updates of preprints are only included and reassessed after publication in a peer reviewed journal). we included studies if they developed or validated a multivariable model or scoring system, based on individual participant level data, to predict any <UNK> related outcome. these models included three types of prediction models: diagnostic models for predicting the presence or severity of <UNK> in patients with suspected infection; prognostic models for predicting the course of infection in patients with <UNK> and prediction models to identify people at increased risk of <UNK> in the general population. no restrictions were made on the setting (eg, inpatients, outpatients, or general population), prediction horizon (how far ahead the model predicts), included predictors, or outcomes. epidemiological studies that aimed to model disease transmission or fatality rates, diagnostic test accuracy, and predictor finding studies were excluded. starting with the second update, retrieved records were initially screened by a text analysis tool developed by artificial intelligence to prioritise sensitivity (supplementary material). titles, abstracts, and full texts were screened for eligibility in duplicate by independent reviewers (pairs from lw, bvc, mvs) using <UNK> and discrepancies were resolved through discussion. data extraction of included articles was done by two independent reviewers (from lw, bvc, gsc, tpad, mch, gh, kgmm, rdr, es, ljms, ews, kies, cw, al, jm, tt, jaad, kl, jbr, lh, cs, ms, mch, ns, nk, smjvk, jcs, pd, clan, rw, gpm, it, jyv, dld, jw, fsvr, ph, vmtdj, and mvs). reviewers used a standardised data extraction form based on the charms (critical appraisal and data extraction for systematic reviews of prediction modelling studies) <UNK> and probast (prediction model risk of bias assessment tool) for assessing the reported prediction <UNK> we sought to extract each model’s predictive performance by using whatever measures were presented. these measures included any summaries of discrimination (the extent to which predicted risks discriminate between participants with and without the outcome), and calibration (the extent to which predicted risks correspond to observed risks) as recommended in the tripod (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) <UNK> discrimination is often quantified by the c index (c <UNK> if the model discriminates perfectly; c <UNK> if discrimination is no better than chance). calibration is often quantified by the calibration intercept (which is zero when the risks are not systematically overestimated or underestimated) and calibration slope (which is one if the predicted risks are not too extreme or too <UNK> we focused on performance statistics as estimated from the strongest available form of validation (in order of strength: external (evaluation in an independent database), internal (bootstrap validation, cross validation, random training test splits, temporal splits), apparent (evaluation by using exactly the same data used for development)). any discrepancies in data extraction were discussed between reviewers, and remaining conflicts were resolved by lw and mvs. the online supplementary material provides details on data extraction. we considered aspects of prisma (preferred reporting items for systematic reviews and <UNK> and <UNK> in reporting our article. patient and public involvement it was not possible to involve patients or the public in the design, conduct, or reporting of our research. the study protocol and preliminary results are publicly available on <UNK> and medrxiv. results we retrieved <UNK> titles through our systematic search (of which <UNK> were included in the present update; supplementary table <UNK> fig <UNK> two additional unpublished studies were made available on request (after a call on social media). we included a further six studies that were publicly available but were not detected by our search. of <UNK> titles, <UNK> studies were retained for abstract and full text screening (of which <UNK> in the present update). one hundred seven studies describing <UNK> prediction models met the inclusion criteria (of which <UNK> papers and <UNK> models added in the present update, supplementary table <UNK> <UNK> these studies were selected for data extraction and critical appraisal (table <UNK> table <UNK> table <UNK> and table <UNK> primary datasets forty five studies used data on patients with <UNK> from china (supplementary table <UNK> six from <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> three from <UNK> <UNK> <UNK> three from <UNK> <UNK> <UNK> three from the united <UNK> <UNK> <UNK> two from south <UNK> one from <UNK> one from the <UNK> one from the united <UNK> one from <UNK> one from <UNK> and one from <UNK> twenty two studies used international data (supplementary table <UNK> and two studies used simulated <UNK> <UNK> three studies used proxy data to estimate <UNK> related risks (eg, medicare claims data from <UNK> to <UNK> <UNK> <UNK> twelve studies were not clear on the origin of <UNK> data (supplementary table <UNK> based on <UNK> studies that reported study dates, data were collected between <UNK> december <UNK> and <UNK> april <UNK> four studies reported median followup time <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> while another study reported a follow-up of at least five <UNK> some centres provided data to multiple studies and several studies used open <UNK> or <UNK> data repositories (version or date of access often unspecified), and so it was unclear how much these datasets overlapped across our identified studies (supplementary table <UNK> one <UNK> developed prediction models for use in paediatric patients. the median age in studies on adults varied from <UNK> to <UNK> years, and the proportion of men varied from <UNK> to <UNK> although this information was often not reported at all (supplementary table <UNK> among the studies that developed prognostic models to predict mortality risk in people with confirmed or suspected infection, the percentage of deaths varied between <UNK> and <UNK> (table <UNK> this wide variation is partly because of substantial sampling bias caused by studies excluding participants who still had the disease at the end of the study period (that is, they had neither recovered nor <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> additionally, length of follow-up could have varied between studies (but was rarely reported), and there might be local and temporal variation in how people were diagnosed as having <UNK> or were admitted to the hospital (and therefore recruited for the studies). among the diagnostic model studies, only nine reported on the prevalence of <UNK> and used a cross sectional or cohort design; the prevalence varied between <UNK> and <UNK> (table <UNK> because <UNK> diagnostic studies used either case-control sampling or an unclear method of data collection, the prevalence in these diagnostic studies might not have been representative of their target population. table <UNK> table <UNK> and table <UNK> give an overview of the <UNK> prediction models reported in the <UNK> identified studies. supplementary table <UNK> provides modelling details and box <UNK> discusses the availability of models in a format for use in clinical practice. models to predict risks of <UNK> in the general population we identified four models that predicted risk of <UNK> in the general population. three models from one study used hospital admission for non- tuberculosis pneumonia, influenza, acute bronchitis, or upper respiratory tract infections as proxy outcomes in a dataset without any patients with <UNK> among the predictors were age, sex, previous hospital admissions, comorbidity data, and social determinants of health. the study reported c indices of <UNK> <UNK> and <UNK> a fourth model used deep learning on thermal videos from the faces of people wearing facemasks to determine abnormal breathing (not covid related) with a reported sensitivity of <UNK> diagnostic models to detect <UNK> in patients with suspected infection we identified <UNK> multivariable models to diagnose <UNK> most models targeted patients with suspected <UNK> reported c index values ranged between <UNK> and <UNK> a few models also evaluated calibration and reported good <UNK> <UNK> <UNK> the most frequently used diagnostic predictors (at least <UNK> times) were flu-like signs and symptoms (eg, shiver, fatigue), imaging features (eg, pneumonia signs on computed tomography scan), age, body temperature, lymphocyte count, and neutrophil count (table <UNK> nine studies aimed to diagnose severe disease in patients with <UNK> eight in adults with <UNK> with reported c indices between value of <UNK> and <UNK> and one in paediatric patients with reported perfect <UNK> predictors of severe <UNK> used more than once were comorbidities, liver enzymes, c reactive protein, imaging features, and neutrophil count. sixty prediction models were proposed to support the diagnosis of <UNK> or <UNK> pneumonia (and some also to monitor progression) based on images. most studies used computed tomography images or chest radiographs. others used spectrograms of cough <UNK> and lung <UNK> the predictive performance varied widely, with estimated c index values ranging from <UNK> to more than <UNK> prognostic models for patients with diagnosis of <UNK> we identified <UNK> prognostic models (table <UNK> for patients with a diagnosis of <UNK> the intended use of these models (that is, when to use them, and for whom) was often not clearly described. prediction horizons varied between one and <UNK> days, but were often unspecified. of these models, <UNK> estimated mortality risk and eight aimed to predict progression to a severe or critical state (table <UNK> the remaining studies used other outcomes (single or as part of a composite) including recovery, length of hospital stay, intensive care unit admission, intubation, (duration of) mechanical ventilation, and acute respiratory distress syndrome. one study used data from <UNK> to <UNK> to predict mortality and prolonged assisted mechanical ventilation (as a <UNK> proxy <UNK> additional records identified through other sources articles excluded not a prediction model development or validation study epidemiological model to estimate disease transmission or case fatality rate commentary, editorial or letter methods paper duplicate article <UNK> <UNK> <UNK> <UNK> <UNK> records screened records identified through database searching records excluded articles assessed for eligibility studies included in review (with <UNK> models) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> diagnostic models (including <UNK> severity models and <UNK> imaging studies) prognostic models (including <UNK> for mortality, <UNK> for progression to severe or critical state) models to identify subjects at risk in general population <UNK> <UNK> <UNK> <UNK> fig <UNK> | prisma (preferred reporting items for systematic reviews and meta-analyses) flowchart of study inclusions and exclusions the most frequently used prognostic factors (for any outcome, included at least <UNK> times) included comorbidities, age, sex, lymphocyte count, c reactive protein, body temperature, creatinine, and imaging features (table <UNK> studies that predicted mortality reported c indices between <UNK> and <UNK> some studies also evaluated <UNK> <UNK> <UNK> when applied to new patients, the model by xie et al yielded probabilities of mortality that were too high for low risk patients and too low for high risk patients (calibration slope <UNK> despite excellent <UNK> the mortality model by zhang et al also showed miscalibrated (overfitted and underestimated) risks at external <UNK> while the model by barda et al showed <UNK> the studies that developed models to predict progression to a severe or critical state reported c indices between <UNK> and <UNK> three of these studies also reported good calibration, but this was evaluated internally (eg, <UNK> or in an unclear <UNK> <UNK> reported c indices for other outcomes varied between <UNK> and <UNK> singh et al and zhang et al also evaluated calibration externally (in new patients). singh showed that the epic deterioration index overestimated the risk or a poor outcome, while the poor outcome model by zhang et al underestimated the risk of a poort <UNK> <UNK> risk of bias all studies were at high risk of bias according to assessment with probast (table <UNK> table <UNK> and table <UNK> which suggests that their predictive performance when used in practice is probably lower than that reported. therefore, we have cause for concern that the predictions of the proposed models are unreliable when used in other people. box <UNK> gives details on common causes for risk of bias for each type of model. fifty three of the <UNK> studies had a high risk of bias for the participants domain (table <UNK> which indicates that the participants enrolled in the studies might not be representative of the models’ targeted populations. unclear reporting on the inclusion of participants prohibited a risk of bias assessment in <UNK> studies. fifteen of the <UNK> studies had a high risk of bias for the predictor domain, which indicates that predictors were not available at the models’ intended time of use, not clearly defined, or influenced by the outcome measurement. one diagnostic imaging study used a simple scoring rule and was scored at low predictor risk of bias. the diagnostic model studies that used medical images as predictors in artificial intelligence were all scored as unclear on the predictor domain. the publications often lacked clear information on the preprocessing steps (eg, cropping of images). moreover, complex machine learning algorithms transform images into predictors in a complex way, which makes it challenging to fully apply the probast predictors section for such imaging studies. most studies used outcomes that are easy to assess (eg, death, presence of <UNK> by laboratory confirmation). nonetheless, there was cause for concern about bias induced by the outcome measurement in <UNK> studies, for example due to the use of subjective or proxy outcomes (eg, non <UNK> severe respiratory infections). all but one of these <UNK> were at high risk of bias for the analysis domain (table <UNK> many table <UNK> | overview of prediction models for use in the general population study; setting; and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) general population original review decaprio et <UNK> ; data from us general population; hospital admission for <UNK> pneumonia (proxy events)† age, sex, number of previous hospital admissions, <UNK> diagnostic features, interactions between age and diagnostic features <UNK> million (unknown) training test split <UNK> <UNK> (unknown) c index <UNK> high decaprio et <UNK> ; data from us general population; hospital admission for <UNK> pneumonia (proxy events)† age and <UNK> features related to diagnosis history <UNK> million (unknown) training test split <UNK> <UNK> (unknown) c index <UNK> high decaprio et <UNK> ; data from us general population; hospital admission for <UNK> pneumonia (proxy events)† <UNK> undisclosed features, including age, diagnostic history, social determinants of health, charlson comorbidity index <UNK> million (unknown) training test split <UNK> <UNK> (unknown) c index <UNK> high update <UNK> jiang et <UNK> data from china, respiratory patients versus healthy volunteers; detection of respiratory diseases such as <UNK> infrared/thermal video of face unknown training test split not applicable sensitivity <UNK> ppv <UNK> high npv=negative predictive value; ppv=positive predictive value; probast=prediction model risk of bias assessment tool. *performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is reported. apparent performance is the performance observed in the development data. †proxy events used: pneumonia (except from tuberculosis), influenza, acute bronchitis, or other specified upper respiratory tract infections (no patients with <UNK> pneumonia in data). setting; and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) diagnosis original review feng et <UNK> data from china, patients presenting at fever clinic; suspected <UNK> pneumonia age, temperature, heart rate, diastolic blood pressure, systolic blood pressure, basophil count, platelet count, mean corpuscular haemoglobin content, eosinophil count, monocyte count, fever, shiver, shortness of breath, headache, fatigue, sore throat, fever classification, interleukin <UNK> <UNK> <UNK> temporal validation <UNK> (unclear) c index <UNK> high lopez-rincon et <UNK> data from international genome sequencing data repository, target population unclear; <UNK> diagnosis specific sequences of base pairs <UNK> <UNK> <UNK> cross validation not applicable c index <UNK> <UNK> specificity <UNK> high meng et <UNK> data from china, asymptomatic patients with suspected <UNK> <UNK> diagnosis age, activated partial thromboplastin time, red blood cell distribution width sd, uric acid, triglyceride, serum potassium, albumin/globulin, <UNK> serum calcium <UNK> <UNK> external validation <UNK> <UNK> c index <UNK> high song et <UNK> data from china, inpatients with suspected <UNK> <UNK> diagnosis fever, history of close contact, signs of pneumonia on ct, neutrophil to lymphocyte ratio, highest body temperature, sex, age, meaningful respiratory syndromes <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> <UNK> to <UNK> high update <UNK> martin et <UNK> simulated patients with suspected <UNK> <UNK> diagnosis unknown not applicable external validation only (simulation) not applicable sensitivity <UNK> specificity <UNK> high sun et <UNK> data from singapore, patients with suspected infection presenting at infectious disease clinic; <UNK> diagnosis age, sex, temperature, heart rate, systolic blood pressure, diastolic blood pressure, sore throat <UNK> <UNK> leave-one-out cross validation not applicable c index <UNK> <UNK> to <UNK> high sun et <UNK> data from singapore, patients with suspected infection presenting at infectious disease clinic; <UNK> diagnosis sex, temperature, heart rate, respiration rate, diastolic blood pressure, sore throat, sputum production, shortness of breath, gastrointestinal symptoms, lymphocytes, neutrophils, eosinophils, creatinine <UNK> <UNK> leave-one-out cross validation not applicable c index <UNK> <UNK> to <UNK> high sun et <UNK> data from singapore, patients with suspected infection presenting at infectious disease clinic; <UNK> diagnosis sex, temperature, heart rate, respiration rate, diastolic blood pressure, sputum production, gastrointestinal symptoms, chest radiograph or ct scan suggestive of pneumonia, neutrophils, eosinophils, creatinine <UNK> <UNK> leave-one-out cross validation not applicable c index <UNK> <UNK> to <UNK> high sun et <UNK> data from singapore, patients with suspected infection presenting at infectious disease clinic; <UNK> diagnosis sex, <UNK> case contact, travel to wuhan, travel to china, temperature, heart rate, respiration rate, diastolic blood pressure, sore throat, sputum production, gastrointestinal symptoms, chest radiograph or ct scan suggestive of pneumonia, neutrophils, eosinophils, creatinine, sodium <UNK> <UNK> leave-one-out cross validation not applicable c index <UNK> <UNK> to <UNK> high wang et <UNK> data from china, patients with suspected <UNK> <UNK> pneumonia epidemiological history, wedge shaped or fan shaped lesion parallel to or near the pleura, bilateral lower lobes, ground glass opacities, crazy paving pattern, white blood cell count <UNK> <UNK> external validation <UNK> <UNK> c index <UNK> calibration slope <UNK> high wu et <UNK> data from china, inpatients with suspected <UNK> <UNK> diagnosis lactate dehydrogenase, calcium, creatinine, total protein, total bilirubin, basophil, platelet distribution width, kalium, magnesium, creatinine kinase isoenzyme, glucose <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> high update <UNK> batista et <UNK> data from brazil, inpatients with suspected <UNK> admitted to the emergency care department; <UNK> diagnosis age, sex, haemoglobin, platelets, red blood cells, mean corpuscular haemoglobin concentration, mean corpuscular haemoglobin, red cell distribution width , mean corpuscular volume, leukocytes, lymphocytes, monocytes, basophils, eosinophils and c reactive protein <UNK> <UNK> training test split <UNK> (unknown) c index <UNK> sensitivity <UNK> specificity <UNK> high table <UNK> | overview of prediction models for diagnosis of <UNK> (continued) and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) brinati et <UNK> data from italy, inpatients with suspected <UNK> <UNK> diagnosis age, aspartate aminotransferase, lymphocytes, lactodehydrogenase, pcr, wbc count, eosinophils, alanine transaminase, neutrophils, gamma-glutamyltransferase, monocytes, basophils, alkaline phosphatase, platelets <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> high brinati et <UNK> data from italy, inpatients with suspected <UNK> <UNK> diagnosis age, aspartate aminotransferase, lymphocytes, lactodehydrogenase, pcr, wbc count, eosinophils, alanine transaminase, neutrophils, gamma-glutamyltransferase, monocytes, basophils, alkaline phosphatase, platelets <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> high chen et <UNK> data from china, inpatients with suspected <UNK> <UNK> diagnosis total number of mixed ggo in peripheral area, tree-in-bud, offending vessel augmentation in lesions, respiration, heart ratio, temperature, wbc count, cough, fatigue, lymphocyte count <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high diaz-quijano et <UNK> data from brazil, inpatients with suspected <UNK> <UNK> diagnosis age, days after reporting first confirmed case in federal unit, fever, cough, sore throat, diarrhoea, coryza, chills, pulmonary manifestation, other signs, hiv, kidney diseaes, trip outside brazil up to <UNK> days before onset <UNK> <UNK> external validation (new centres, brazil) <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high kurstjens et <UNK> data from the netherlands, inpatients with suspected <UNK> <UNK> diagnosis age, sex, crp, ld, ferritin, absolute neutrophil count, absolute lymphocyte count, chest radiograph <UNK> <UNK> external (unclear) <UNK> <UNK> c index <UNK> <UNK> to <UNK> high mei et <UNK> data from china; inpatients with suspected <UNK> <UNK> diagnosis age, sex, ct imaging, exposure history, symptoms (present or absent of fever, cough and/or sputum), wbc counts, neutrophil count, percentage neutrophils, lymphocyte counts, percentage lymphocytes <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> ppv <UNK> <UNK> to <UNK> npv <UNK> <UNK> to <UNK> high menni et <UNK> data from uk and usa, suspected <UNK> <UNK> diagnosis age, sex, loss of smell and taste, severe or significant persistent cough, severe fatigue, skipped meals <UNK> <UNK> <UNK> external validation (new centres, usa) <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> ppv <UNK> <UNK> to <UNK> npv <UNK> <UNK> to <UNK> high soares et <UNK> data from brazil; patients with suspected infection presenting at triage centre; <UNK> diagnosis age, red blood cells, mean corpuscular volume, mean corpuscular haemoglobin concentration, mean corpuscular haemoglobin, red blood cell distribution width, leukocytes, basophils, monocytes, lymphocytes, platelets, mean platelet volume, creatinine, potassium, sodium, crp <UNK> <UNK> repeated <UNK> cross validation not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> npv <UNK> <UNK> to <UNK> ppv <UNK> <UNK> to <UNK> high tordjman et <UNK> data from france; suspected patients; <UNK> diagnosis eosinophils, lymphocytes, neutrophils, basophils <UNK> <UNK> external validation (new centres, france) <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> high zhao et <UNK> data from china; inpatients with suspected <UNK> <UNK> diagnosis fever, chest ct, crp, pct, wbc <UNK> (unknown) training test split <UNK> (unknown) c index <UNK> <UNK> to <UNK> high diagnostic severity classification original review yu et <UNK> data from china, paediatric inpatients with confirmed <UNK> severe disease (yes/no) defined based on clinical symptoms direct bilirubin, alanine transaminase <UNK> <UNK> apparent performance only not applicable <UNK> score <UNK> high update <UNK> zhou et <UNK> data from china, inpatients with confirmed <UNK> severe pneumonia age, sex, onset-admission time, high blood pressure, diabetes, chd, copd, white blood cell counts, lymphocyte, neutrophils, alanine transaminase, aspartate aminotransferase, serum albumin, serum creatinine, blood urea nitrogen, crp <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high table <UNK> | continued and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) update <UNK> benchoufi et <UNK> data from france, inpatients with suspected or confirmed <UNK> lung injury severity (pathologic vs normal) lung ultrasound scores for <UNK> quadrants in a global score <UNK> (unknown) internal validation by resampling (bootstrap) not applicable c index <UNK> sensitivity <UNK> specificity <UNK> high chassagnon et <UNK> data from france, inpatients with confirmed <UNK> severe <UNK> unclear <UNK> (unknown) external validation (new centres, france) <UNK> (unknown) c index <UNK> sensitivity <UNK> specificity <UNK> high li et <UNK> data from china, target population unclear; severe <UNK> portion of infection, average infection hounsfield unit, a measure of radio density <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> high lyu et <UNK> data from china, target population unclear; severe/critical <UNK> pneumonia unclear <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high lyu et <UNK> data from china , target population unclear; critical <UNK> pneumonia unclear <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high wang et <UNK> data from china, inpatients with confirmed <UNK> severe <UNK> neutrophil-to-lymphocyte ratio, red cell volume distribution width <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> npv <UNK> high zhu et <UNK> data from china, inpatients with confirmed <UNK> severe <UNK> peripheral blood cytokine <UNK> crp, hypertension <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> high diagnostic imaging original review barstugan et <UNK> data from italy, patients with suspected <UNK> <UNK> diagnosis not applicable <UNK> (not applicable) cross validation not applicable sensitivity <UNK> specificity <UNK> high chen et <UNK> data from china, people with suspected <UNK> pneumonia; <UNK> pneumonia not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> high gozes et <UNK> data from china and us,§ patients with suspected <UNK> <UNK> diagnosis not applicable <UNK> (unknown) external validation with chinese cases and us controls unclear c index <UNK> <UNK> to <UNK> high jin et <UNK> data from china, us, and switzerland,¶ patients with suspected <UNK> <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> high jin et <UNK> data from china, patients with suspected <UNK> <UNK> pneumonia not applicable <UNK> <UNK> training test split <UNK> <UNK> c index: <UNK> sensitivity <UNK> specificity <UNK> high li et <UNK> data from china, patients with suspected <UNK> <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> high shan et <UNK> data from china, people with confirmed <UNK> segmentation and quantification of infection regions in lung from chest ct scans not applicable <UNK> (not applicable) training test split <UNK> (not applicable) dice similarity coefficient <UNK> high shi et <UNK> data from china, target population unclear; <UNK> pneumonia five categories of location features from imaging: volume, number, histogram, surface, radiomics <UNK> <UNK> fivefold cross validation not applicable c index <UNK> high table <UNK> | continued final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) wang et <UNK> data from china, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> internal, other images from same people not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high xu et <UNK> data from china, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> ppv <UNK> high song et <UNK> data from china, target population unclear; diagnosis of <UNK> v healthy controls not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> high song et <UNK> data from china, target population unclear; diagnosis of <UNK> v bacterial pneumonia not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> high zheng et <UNK> data from china, target population unclear; <UNK> diagnosis not applicable unknown temporal validation unknown c index <UNK> high update <UNK> abbas et <UNK> data from repositories (origin unspecified), target population unclear; <UNK> diagnosis not applicable <UNK> (unknown) training test split <UNK> (unknown) c index <UNK> sensitivity <UNK> specificity <UNK> high apostolopoulos et <UNK> data from repositories (us, italy); patients with suspected <UNK> <UNK> diagnosis not applicable <UNK> <UNK> tenfold cross validation not applicable sensitivity <UNK> specificity <UNK> high bukhari et <UNK> data from canada and us; patients with suspected <UNK> <UNK> diagnosis not applicable <UNK> (unknown) training test split <UNK> <UNK> sensitivity <UNK> ppv <UNK> high chaganti et <UNK> data from canada, us, and european countries; patients with suspected <UNK> percentage lung opacity not applicable <UNK> (not applicable) training test split <UNK> (not applicable) correlation§§ <UNK> high chaganti et <UNK> data from canada, us, and european countries; patients with suspected <UNK> percentage high lung opacity not applicable <UNK> (not applicable) training test split <UNK> (not applicable) correlation§§ <UNK> high chaganti et <UNK> data from canada, us, and european countries; patients with suspected <UNK> severity score not applicable <UNK> (not applicable) training test split <UNK> (not applicable) correlation§§ <UNK> high chaganti et <UNK> data from canada, us, and european countries; patients with suspected <UNK> lung opacity score not applicable <UNK> (not applicable) training test split <UNK> (not applicable) correlation§§ <UNK> high chowdhury et <UNK> data from repositories (italy and other unspecified countries), target population unclear; <UNK> v “normal” not applicable unknown fivefold cross validation not applicable c index <UNK> high chowdhury et <UNK> data from repositories (italy and other unspecified countries), target population unclear; <UNK> v “normal” and viral pneumonia not applicable unknown fivefold cross validation not applicable c index <UNK> high chowdhury et <UNK> data from repositories (italy and other unspecified countries), target population unclear; <UNK> v “normal” not applicable unknown fivefold cross validation not applicable c index <UNK> high table <UNK> | continued (continued) final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) chowdhury et <UNK> data from repositories (italy and other unspecified countries), target population unclear; <UNK> v “normal” and viral pneumonia not applicable unknown fivefold cross validation not applicable c index <UNK> high fu et <UNK> data from china, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> external validation <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> high gozes et <UNK> data from china, people with suspected <UNK> <UNK> diagnosis not applicable <UNK> (unknown) external validation <UNK> <UNK> c index <UNK> <UNK> to <UNK> high imran et <UNK> data from unspecified source, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> twofold cross validation not applicable sensitivity <UNK> specificity <UNK> high li et <UNK> data from china, inpatients with confirmed <UNK> severe and critical <UNK> severity score based on ct scans not applicable external validation of existing score <UNK> (not applicable) c index <UNK> <UNK> to <UNK> high li et <UNK> data from unknown origin, patients with suspected <UNK> <UNK> not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> high hassanien et <UNK> data from repositories (origin unspecified), people with suspected <UNK> <UNK> diagnosis not applicable unknown training test split unknown sensitivity <UNK> specificity <UNK> high tang et <UNK> data from china, patients with confirmed <UNK> <UNK> severe v non-severe not applicable <UNK> <UNK> threefold cross validation not applicable c index <UNK> sensitivity <UNK> specificity <UNK> high wang et <UNK> data from china, inpatients with suspected <UNK> <UNK> not applicable <UNK> <UNK> external validation in other centres <UNK> <UNK> c index (average) <UNK> high zhang et <UNK> data from repositories (origin unspecified), people with suspected <UNK> <UNK> not applicable <UNK> <UNK> twofold cross validation not applicable c index <UNK> sensitivity <UNK> specificity <UNK> high zhou et <UNK> data from china, patients with suspected <UNK> <UNK> diagnosis not applicable <UNK> <UNK> external validation in other centres <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> high update <UNK> angelov et <UNK> data from unknown origin; <UNK> diagnosis not applicable unknown apparent performance only not applicable c index <UNK> sensitivity <UNK> ppv <UNK> high arpan et <UNK> data from repositories (multiple countries); <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> sensitivity <UNK> ppv <UNK> high bai et <UNK> data from china and us, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> sensitivity <UNK> <UNK> to <UNK> , specificity <UNK> <UNK> to <UNK> high bassi et <UNK> data from italy, target population unclear, <UNK> diagnosis not applicable unknown training test split unknown sensitivity <UNK> ppv <UNK> high borghesi et <UNK> data from italy, target population unclear, ; severity of <UNK> pneumonia sum score for lung abnormalities based on chest radiograph not applicable external validation only <UNK> (unknown) agreement, kappa <UNK> <UNK> to <UNK> high born et <UNK> data from repositories (origin unspecified), target population unclear, <UNK> diagnosis not applicable <UNK> <UNK> fivefold cross validation not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high table <UNK> | continued (continued) final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) castiglioni et <UNK> data from italy, inpatients suspected of <UNK> <UNK> diagnosis not applicable <UNK> <UNK> temporal validation <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> ppv <UNK> <UNK> to <UNK> npv <UNK> <UNK> to <UNK> high guiot et <UNK> data from belgium, inpatients suspected of <UNK> <UNK> diagnosis <UNK> radiomics features <UNK> (unknown) training test split <UNK> (unknown) c index <UNK> <UNK> to <UNK> , sensitivity <UNK> specificity <UNK> ppv <UNK> npv <UNK> high hu et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high islam et <UNK> data from unknown origin, inpatients suspected of <UNK> <UNK> diagnosis not applicable <UNK> <UNK> unknown origin <UNK> <UNK> sensitivity <UNK> high kana et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> external validation, different repository (unknown origin) <UNK> <UNK> sensitivity <UNK> specificity <UNK> high karim et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable unknown unknown unknown severe inconsistencies in reported performance: data not extracted high khan et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> ppv <UNK> high kumar et <UNK> data from usa, china and italy, target population unclear; <UNK> diagnosis; <UNK> diagnosis not applicable unknown apparent performance only not applicable c index <UNK> sensitivity <UNK> specificity <UNK> high kumar et <UNK> data from usa, china and italy, target population unclear; <UNK> diagnosis; <UNK> diagnosis not applicable unknown apparent performance only not applicable c index <UNK> sensitivity <UNK> specificity <UNK> high <UNK> data from repositories, target population unclear; <UNK> pneumonia not applicable <UNK> <UNK> training test split <UNK> (unknown) sensitivity <UNK> high ozturk et <UNK> data from repositories, target population unclear; <UNK> pneumonia not applicable <UNK> <UNK> fivefold cross validation not applicable sensitivity <UNK> specificity <UNK> ppv <UNK> high rahimzadeh et <UNK> data from repositories, target population unclear; <UNK> pneumonia not applicable <UNK> <UNK> fivefold cross validation not applicable sensitivity <UNK> specificity <UNK> ppv <UNK> high rehman et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> high rehman et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> high rehman et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> high rehman et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> high table <UNK> | continued research studies had small sample sizes (table <UNK> table <UNK> table <UNK> which led to an increased risk of overfitting, particularly if complex modelling strategies were used. three studies did not report the predictive performance of the developed model, and four studies reported only the apparent performance (the performance with exactly the same data used to develop the model, without adjustment for optimism owing to potential overfitting). only <UNK> studies assessed <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> but the method to check calibration was probably suboptimal in two <UNK> <UNK> twenty five models were developed and externally validated in the same study (in an independent dataset, excluding random training test splits and temporal <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> however, in <UNK> of these models, the datasets used for the external validation were likely not representative of the target <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and in one study, data from before the <UNK> crisis were <UNK> consequently, predictive performance could differ if the models are applied in the targeted population. in one study, commonly used performance statistics for prognosis (discrimination, calibration) were not <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> had satisfactory predictive performance on an external validation set, but it is unclear how the data for the external validation were collected (eg, whether the patients were consecutive), and whether they are representative. <UNK> <UNK> <UNK> <UNK> and <UNK> obtained satisfactory discrimination on probably unbiased validation datasets, but each of these had fewer than the recommended number of events for external validation <UNK> <UNK> diaz-quijano externally validated a diagnostic model in a large registry with reasonable discrimination, but many patients had to be excluded because no polymerase chain reaction (pcr) testing was <UNK> one study presented a small external validation <UNK> participants) that reported satisfactory predictive performance of a model originally developed for avian influenza <UNK> pneumonia. however, patients who had not recovered at the end of the study period were excluded, which again led to a selection <UNK> another study was a small scale external validation study <UNK> participants) of an existing severity score for lung computed tomography images with satisfactory reported <UNK> three studies validated existing early warning or severity scores to predict in-hospital mortality or <UNK> <UNK> <UNK> they had satisfactory discrimination but less than the recommended number of events for <UNK> <UNK> or unclear sample sizes, excluded patients who remained in hospital at the end of the study period, or had an unclear study design. discussion in this systematic review of prediction models related to the <UNK> pandemic, we identified and critically appraised <UNK> studies that described <UNK> models. study; setting; and outcome predictors in final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensitivity (%), specificity (%), ppv/npv (%), calibration slope, other <UNK> ci, if reported)) rehman et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> high singh et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable unknown twentyfold cross validation not applicable sensitivity <UNK> specificity <UNK> high ucar et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable unknown training test split unknown sensitivity <UNK> specificity <UNK> ppv <UNK> high wu et <UNK> data from unknown origin, target population unclear; <UNK> diagnosis not applicable <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> high table <UNK> | continued chd=coronary heart disease; copd=chronic obstructive pulmonary disease; <UNK> disease <UNK> crp=c reactive protein; ct=computed tomography; ggo=ground glass opacities; npv=negative predictive value; ppv=positive predictive value; probast=prediction model risk of bias assessment tool; pcr=polymerase chain reaction; wbc=white blood cells. *performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is reported. apparent performance is the performance observed in the development data. ‡calibration plot presented, but unclear which data were used. §the development set contains scans from chinese patients, the testing set contains scans from chinese cases and controls, and us controls. ¶data contain mixed cases and controls. chinese data and controls from us and switzerland. **describes similarity between segmentation of the ct scan by a medical doctor and automated segmentation. §§pearson correlation between the predicted and ground truth scores for patients with lung abnormalities. ¶¶performance similar for models with different non-cases (healthy, bacterial pneumonia, and viral pneumonia). final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other <UNK> ci, if reported)) prognosis original review bai et <UNK> data from china, inpatients at admission with mild confirmed <UNK> deterioration into severe/critical disease (period unspecified) combination of demographics, signs and symptoms, laboratory results and features derived from ct images <UNK> <UNK> unclear not applicable c index <UNK> <UNK> to <UNK> high caramelo et <UNK> data from china, target population unclear; mortality (period unspecified)†† age, sex, presence of any comorbidity (hypertension, diabetes, cardiovascular disease, chronic respiratory disease, cancer)†† unknown not reported not applicable not reported high lu et <UNK> data from china, inpatients at admission with suspected or confirmed <UNK> mortality (within <UNK> days) age, crp <UNK> <UNK> not reported not applicable not reported high qi et <UNK> data from china, inpatients with confirmed <UNK> at admission; hospital stay <UNK> days <UNK> features derived from ct images‡‡ (logistic regression model) <UNK> <UNK> fivefold cross validation not applicable c index <UNK> high qi et <UNK> data from china, inpatients with confirmed <UNK> at admission; hospital stay <UNK> days <UNK> features derived from ct images‡‡ (random forest) <UNK> <UNK> <UNK> fold cross validation not applicable c index <UNK> high shi et <UNK> data from china, inpatients with confirmed <UNK> at admission; death or severe <UNK> (period unspecified) age (dichotomised), sex, hypertension <UNK> <UNK> validation in less severe cases <UNK> <UNK> not reported high xie et <UNK> data from china, inpatients with confirmed <UNK> at admission; mortality (in hospital) age, ldh, lymphocyte count, <UNK> <UNK> <UNK> external validation (other chinese centre) <UNK> <UNK> c index <UNK> <UNK> to <UNK> calibration slope <UNK> <UNK> to <UNK> high yan et <UNK> data from china, inpatients suspected of <UNK> mortality (period unspecified) ldh, lymphocyte count, high sensitivity crp <UNK> <UNK> temporal validation, selecting only severe cases <UNK> <UNK> sensitivity <UNK> ppv <UNK> high yuan et <UNK> data from china, inpatients with confirmed <UNK> at admission; mortality (period unspecified) clinical scorings of ct images (zone, left/right, location, attenuation, distribution of affected parenchyma) not applicable external validation of existing model <UNK> <UNK> c index <UNK> <UNK> to <UNK> high update <UNK> huang et <UNK> data from china, inpatients with confirmed <UNK> at admission; severe symptoms three days after admission underlying diseases, fast respiratory rate <UNK> elevated crp level <UNK> mg/dl), elevated ldh level <UNK> u/l) <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high pourhomayoun et <UNK> data from <UNK> countries, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) unknown unknown <UNK> cross validation not applicable c index <UNK> sensitivity <UNK> specificity <UNK> high sarkar et <UNK> data from several continents (australia, asia, europe, north america), inpatients with <UNK> symptoms; death v recovery (period unspecified) age, days from symptom onset to hospitalisation, from wuhan, sex, visit to wuhan <UNK> <UNK> apparent performance only not applicable c index <UNK> high wang et <UNK> data from china, inpatients with confirmed <UNK> length of hospital stay age and ct features <UNK> (not applicable) not reported not applicable not reported high zeng et <UNK> data from china, inpatients with confirmed <UNK> severe disease progression (period unspecified) ct features <UNK> <UNK> cross validation (number of folds unclear) not applicable c index <UNK> high zeng et <UNK> data from china, inpatients with confirmed <UNK> severe disease progression (period unspecified) ct features and laboratory markers <UNK> <UNK> cross validation (number of folds unclear) not applicable c index <UNK> high update <UNK> al-najjar et <UNK> data from south korea, target population unclear; recovery from <UNK> (period unspecified) birth year (age), sex, country, group, infection reason, confirmed date <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> high table <UNK> | overview of prediction models for prognosis of <UNK> (continued) final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other <UNK> ci, if reported)) al-najjar et <UNK> data from south korea, target population unclear; mortality (period unspecified) age, sex, country, region, infection reason, confirmed date <UNK> <UNK> training test split <UNK> <UNK> sensitivity <UNK> specificity <UNK> high barda et <UNK> data from israel, patients with confiremed <UNK> mortality (period unspecified) age, sex, pack years, copd, number of wheezing/ dyspnea diagnoses, albumin, red cell distribution width, c reactive peptide, urea, lymphocyte, chloride, creatinine, high density lipoprotein, duration of hospital admissions, count of hospital admissions, count of ambulance rides, count of sulfonamide dispenses, count of anticholinergic dispenses, count of glucocorticoid dispenses, chronic respiratory disease, cardiovascular disease, diabetes, malignancy, hypertension <UNK> <UNK> other (specify in column cl) january <UNK> to april <UNK> inclusion, follow-up until april <UNK> <UNK> (covid cases for external validation) <UNK> <UNK> c index <UNK> <UNK> to <UNK> , sensitivity <UNK> <UNK> to <UNK> ppv <UNK> <UNK> to <UNK> high bello-chavolla et <UNK> data from mexico, confirmed <UNK> patients presenting at gp; <UNK> mortality age, pregnancy, diabetes, obesity, pneumonia, ckd, copd, immunosuppression <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> somer’s d <UNK> high carr et <UNK> data from united kingdom, inpatients with confirmed <UNK> progression to severe <UNK> (period unspecified) age, national early warning score (news) <UNK> crp, neutrophil, egfr, albumin <UNK> <UNK> temporal validation <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> high chassagnon et <UNK> data from france, inpatients with confirmed <UNK> composite, <UNK> intubation or mortality unclear <UNK> <UNK> external validation (new centres, france) <UNK> <UNK> sensitivity <UNK> specificity <UNK> high colombi et <UNK> data from italy, inpatients with confirmed <UNK> icu admission or in-hospital (period unspecified) age, cardiovascular comorbidities, median platelet count, crp, visual assessment of well aerated lung % <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> ppv <UNK> <UNK> to <UNK> npv <UNK> <UNK> to <UNK> high colombi et <UNK> data from italy, inpatients with confirmed <UNK> icu admission or in-hospital mortality (period unspecified) age, cardiovascular comorbidities, median platelet count, ldh, crp, software assessment of well aerated lung absolute volume, adipose tissue <UNK> <UNK> apparent performance only not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> ppv <UNK> <UNK> to <UNK> npv <UNK> <UNK> to <UNK> high das et <UNK> data from south korea, inpatients with confirmed <UNK> icu admission or in-hospital mortality (period unspecified) age, sex, province, date of diagnosis, place of exposure to <UNK> <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> high gong et <UNK> data from china, target population unclear; <UNK> progression to severe <UNK> age, direct bilirubin, red cell distribution width, blood urea nitrogen, crp, lactate dehydrogenase, albumin <UNK> <UNK> external validation (new centres, china) <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> high guo et <UNK> data from china, inpatients with confirmed <UNK> <UNK> progression to severe <UNK> age, chronic illness, neutrophil to lymphocyte ratio, crp, d-dimer <UNK> <UNK> external validation (new centres, china) <UNK> <UNK> c index <UNK> <UNK> to <UNK> high hu et <UNK> data from china inpatients with confirmed <UNK> mortality (period unspecified) age, high-sensitivity crp, lymphocyte count, d-dimer <UNK> <UNK> external validation (new centres, china) <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> high hu et <UNK> data from china, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) modified early warning score (mews): heart rate, systolic blood pressure, respiratory rate, body temperature, consciousness not applicable external validation only <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> npv <UNK> high hu et <UNK> data from china, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) rapid emergency medicine score (rems): mean arterial pressure, pulse rate, respiratory rate, oxygen saturation, gcs, age not applicable external validation only <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> npv <UNK> high ji et <UNK> data from china, inpatients with confirmed <UNK> <UNK> progression to severe <UNK> comorbidity, age, lymphocyte count, lactate dehydrogenase <UNK> <UNK> internal validation by resampling (bootstrap) not applicable c index <UNK> <UNK> to <UNK> sensitivity <UNK> <UNK> to <UNK> specificity <UNK> <UNK> to <UNK> high table <UNK> | continued (continued) final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other <UNK> ci, if reported)) jiang et <UNK> data from china, inpatients with confirmed <UNK> acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count <UNK> <UNK> <UNK> cross validation not applicable classification accuracy <UNK> high jiang et <UNK> data from china, inpatients with confirmed <UNK> acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count <UNK> <UNK> <UNK> cross validation not applicable classification accuracy <UNK> high jiang et <UNK> data from china, inpatients with confirmed <UNK> acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count <UNK> <UNK> <UNK> cross validation not applicable classification accuracy <UNK> high jiang et <UNK> data from china, inpatients with confirmed <UNK> acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count <UNK> <UNK> <UNK> cross validation not applicable classification accuracy <UNK> high jiang et <UNK> data from china, inpatients with confirmed <UNK> acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count <UNK> <UNK> <UNK> cross validation not applicable classification accuracy <UNK> high jiang et <UNK> data from china, inpatients with confirmed <UNK> acute respiratory distress syndrome*** alanine aminotransferase, myalgias, haemoglobin, sex, temp, na+, k+, lymphocyte count, creatinine, age, white blood count <UNK> <UNK> <UNK> cross validation not applicable classification accuracy <UNK> high levy et <UNK> data from usa, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) age, serum blood urea nitrogen, emergency severity index, red cell distribution width, absolute neutrophil count, serum bicarbonate, glucose unknown leave-one-out cross validation not applicable c index <UNK> high levy et <UNK> data from usa, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) sofa score not applicable external validation only unclear c index <UNK> high levy et <UNK> data from usa, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) <UNK> score not applicable external validation only unclear c index <UNK> high levy et <UNK> data from usa, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) sofa+ score not applicable external validation only unclear c index <UNK> high liu et <UNK> data from china, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) age, underlying disease status, helper t cells, helper t cells and suppressor t cells ratio <UNK> <UNK> apparent performance only not applicable mcfadden pseudo r-squared <UNK> high mcrae et <UNK> data from china, inpatients with confirmed <UNK> in-hospital mortality (period unspecified) age, sex, cardiac troponin i, crp, procalcitonin, myoglobin <UNK> <UNK> new centres in china, case series <UNK> (unknown) c index <UNK> <UNK> to <UNK> high singh et <UNK> data from usa, inpatients with confirmed <UNK> icu-level care, mechanical ventilation or in-hospital mortality (period unspecified) epic deterioration index unknown external validation only <UNK> <UNK> c index <UNK> <UNK> to <UNK> sensitivity <UNK> ppv <UNK> high table <UNK> | continued final model sample size: total no of participants for model development set (no with outcome) predictive performance on validation overall risk of bias using probast type of validation* sample size: total no of participants for model validation (no with outcome) performance* (c index, sensi- tivity (%), specificity (%), ppv/ npv (%), calibration slope, other <UNK> ci, if reported)) vaid et <UNK> data from usa, inpatients with confirmed <UNK> intubation, discharge to hospice care or mortalit (period unspecified) sex, race, ethnicity, age, hypertension, atrial fibrillation, coronary artery disease, heart failure, stroke, chronic kidney disease, diabetes, asthma, copd, cancer, heart rate, pulse, oximetry, respiration rate, temperature, systolic blood pressure, diastolic blood pressure, body weight, sodium, potassium, creatinine, lactate, white blood cells, lymphocyte percentage, haemoglobin, red blood cell distribution width, platelets, alanine, aminotransferase, aspartate, aminotransferase, albumin, total bilirubin, prothrombin time, partial thromboplastin time, <UNK> ph, crp, ferritin, d-dimer, creatinine phosphokinase, lactate dehydrogenase, procalcitonin, troponin i <UNK> <UNK> external validation, new centres (usa) <UNK> (unknown) c index <UNK> sensitivity <UNK> specificity <UNK> high vazquez guillamet et <UNK> data from usa, target population unclear; in-hospital mortality (period unspecified) age, immunosuppression, copd, congestive heart failure, bmi, sex, time to mechanical ventilation (days), length of hospital stay prior to hospital admission, <UNK> glasgow coma scale, maximum heart rate, maximum respiratory rate, minimum mean arterial blood pressure, maximum temperature, minimum albumin, minimum ph <UNK> <UNK> external validation, new centres (usa) <UNK> <UNK> c index <UNK> ppv <UNK> npv <UNK> high vazquez guillamet et <UNK> data from usa, target population unclear; mechanical ventilation <UNK> hours age, immunosuppression, copd, congestive heart failure, bmi, sex, time to mechanical ventilation (days), length of hospital stay before hospital admission, <UNK> glasgow coma scale, maximum heart rate, maximum respiratory rate, minimum mean arterial blood pressure, maximum temperature, minimum albumin, minimum ph <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> high vazquez guillamet et <UNK> data from usa, target population unclear; mechanical ventilation <UNK> hours age, immunosuppression, copd, congestive heart failure, bmi, sex, time to mechanical ventilation (days), length of hospital stay prior to hospital admission, <UNK> glasgow coma scale, maximum heart rate, maximum respiratory rate, minimum mean arterial blood pressure, maximum temperature, minimum albumin, minimum ph <UNK> <UNK> training test split <UNK> <UNK> c index <UNK> high zhang et <UNK> data from china and united kingdom, inpatients with confirmed <UNK> in hospital mortality (period unspecified) age, sex, neutrophil count, lymphocyte count, platelet count, crp, creatinine <UNK> <UNK> external validation (new centres, different country) <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> npv <UNK> high zhang et <UNK> data from china, inpatients with confirmed <UNK> ards, intubation or ecmo, icu admission, in hospital mortality (period unspecified) age, sex, chronic lung disease, diabetes mellitus, malignancy, cough, dyspnoea, immunocompromised, hypertension, heart disease, chronic renal disease, fever, fatigue, diarrhoea <UNK> <UNK> repeated five-fold cross validation not applicable c index <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> npv <UNK> high zhang et <UNK> data from china and united kingdom, inpatients with confirmed <UNK> ards, intubation or ecmo, icu admission, in hospital mortality (period unspecified) age, sex, neutrophil count, lymphocyte count, platelet count, crp, creatinine <UNK> <UNK> external validation (new centres, different country) <UNK> <UNK> c index <UNK> sensitivity <UNK> specificity <UNK> ppv <UNK> npv <UNK> high ards=acute respiratory distress syndrome; bmi=body mass index; copd=chronic obstructive pulmonary disease; <UNK> disease <UNK> crp=c reactive protein; ct=computed tomography; ecmo=extracorporal membrane oxygenation; icu=intensive care unit; ldh=lactate dehydrogenase; npv=negative predictive value; <UNK> ratio of arterial oxygen partial pressure to fractional inspired oxygen; <UNK> pressure of carbon dioxide; ppv=positive predictive value; probast=prediction model risk of bias assessment tool; sofa=sequential organ failure assessment score; <UNK> saturation; na+=sodium; k+=potassium. table <UNK> | continued *performance is given for the strongest form of validation reported. this is indicated in the column “type of validation.” when a training test split was used, performance on the test set is reported. apparent performance is the performance observed in the development data. ††outcome and predictor data were simulated. ‡‡wavelet-hlh_gldm_smalldependencelowgraylevelemphasis, wavelet-lhh_glcm_correlation, wavelet-lhl_glszm_graylevelvariance, wavelet-llh_glszm_sizezonenonuniformitynormalized, wavelet-llh_glszm_smallareaemphasis, wavelet-llh_ glcm_correlation. ***each model uses a different predictive algorithm. these prediction models can be divided into three categories: models for the general population to predict the risk of having <UNK> or being admitted to hospital for <UNK> models to support the diagnosis of <UNK> in patients with suspected infection; and models to support the prognostication of patients with <UNK> all models reported moderate to excellent predictive performance, but all were appraised to have high risk of bias owing to a combination of poor reporting and poor methodological conduct for participant selection, predictor description, and statistical methods used. models were developed on data from different countries, but the majority used data from china or public international data repositories. with few exceptions, the available sample sizes and number of events for the outcomes of interest were limited. this is a well known problem when building prediction models and increases the risk of overfitting the <UNK> a high risk of bias implies that the performance of these models in new samples will probably be worse than that reported by the researchers. therefore, the estimated c indices, often close to <UNK> and indicating near perfect discrimination, are probably optimistic. the majority of studies developed new models, only <UNK> carried out an external validation, and calibration was rarely assessed. we reviewed <UNK> studies that used advanced machine learning methodology on medical images to diagnose <UNK> <UNK> related pneumonia, or to assist in segmentation of lung images. the predictive performance measures showed a high to almost perfect ability to identify <UNK> although these models and their evaluations also had a high risk of bias, notably because of poor reporting and an artificial mix of patients with and without <UNK> therefore, we do not recommend any of the <UNK> identified prediction models to be used in practice. challenges and opportunities the main aim of prediction models is to support medical decision making. therefore, it is vital to identify a target population in which predictions serve a clinical need, and a representative dataset (preferably comprising consecutive patients) on which the prediction model can be developed and validated. this target population must also be carefully described so that the performance of the developed or validated model can be appraised in context, and users know which people the model applies to when making predictions. unfortunately, the studies included in our systematic review often lacked an adequate description of the study population, which leaves users of these models in doubt about the models’ applicability. although we recognise that all studies were done under severe time constraints, we recommend that any studies currently in preprint and all future studies should adhere to the tripod reporting <UNK> to improve the description of their study population and their modelling choices. tripod translations (eg, in chinese and japanese) are also available at https:// www.tripod-statement.org. authors risk of bias participants predictors outcome analysis hospital admission in general population original review decaprio et <UNK> high low high high update <UNK> jiang et <UNK> high unclear high high diagnosis original review feng et <UNK> low unclear high high lopez-rincon et <UNK> unclear low low high meng et <UNK> high low high high song et <UNK> high unclear low high update <UNK> martin et <UNK> high high high high sun et <UNK> low low unclear high wang et <UNK> low unclear unclear high wu et <UNK> high unclear low high update <UNK> batista et <UNK> unclear unclear low high brinati et <UNK> unclear unclear low high chen et <UNK> high high low high diaz-quijano et <UNK> high high low high kurstjens et <UNK> unclear low high high mei et <UNK> high unclear unclear high menni et <UNK> high unclear unclear high soares et <UNK> unclear unclear low high tordjman et <UNK> low unclear unclear high zhao et <UNK> high high unclear high diagnosis of severity original review yu et <UNK> unclear unclear unclear high update <UNK> zhou et <UNK> unclear low high high update <UNK> benchoufi et <UNK> high low low high chassagnon et <UNK> low low low high li et <UNK> unclear unclear unclear high lyu et <UNK> low unclear unclear high wang et <UNK> unclear high low high zhu et <UNK> low low high high diagnostic imaging original review barstugan et <UNK> unclear unclear unclear high chen et <UNK> high unclear low high* gozes et <UNK> unclear unclear high high jin et <UNK> high unclear unclear high† jin et <UNK> high unclear high high* li et <UNK> low unclear low high shan et <UNK> unclear unclear high high† shi et <UNK> high unclear low high wang et <UNK> high unclear low high xu et <UNK> high unclear high high song et <UNK> unclear unclear low high zheng et <UNK> unclear unclear high high update <UNK> abbas et <UNK> high unclear unclear high apostolopoulos et <UNK> high unclear high high bukhari et <UNK> unclear unclear unclear high chaganti et <UNK> high unclear low unclear chowdhury et <UNK> high unclear unclear high fu et <UNK> high unclear unclear high gozes et <UNK> high unclear unclear high imran et <UNK> high unclear unclear high* li et <UNK> low low unclear high li et <UNK> high unclear high high* hassanien et <UNK> unclear unclear unclear high* tang et <UNK> unclear unclear high high table <UNK> | risk of bias assessment (using probast) based on four domains across <UNK> studies that created prediction models for coronavirus disease <UNK> a better description of the study population could also help us understand the observed variability in the reported outcomes across studies, such as <UNK> related mortality and <UNK> prevalence. the variability in prevalence could in part be reflective of different diagnostic standards across studies. note that the majority of diagnostic models use viral nucleic acid test results as the gold standard, which may have unacceptable false negative rates. <UNK> prediction problems will often not present as a simple binary classification task. complexities in the data should be handled appropriately. for example, a prediction horizon should be specified for prognostic outcomes (eg, <UNK> day mortality). if study participants have neither recovered nor died within that time period, their data should not be excluded from analysis, which most reviewed studies have done. instead, an appropriate time to event analysis should be considered to allow for administrative <UNK> censoring for other reasons, for instance because of quick recovery and loss to follow-up of patients who are no longer at risk of death from <UNK> could necessitate analysis in a competing risk <UNK> a prediction model applied in a new healthcare setting or country often produces predictions that are <UNK> and might need to be updated before it can safely be applied in that new <UNK> this requires data from patients with <UNK> to be available from that system. instead of developing and updating predictions in their local setting, individual participant data from multiple countries and healthcare systems might allow better understanding of the generalisability and implementation of prediction models across different settings and populations. this approach could greatly improve the applicability and robustness of prediction models in routine <UNK> the evidence base for the development and validation of prediction models related to <UNK> will quickly increase over the coming months. together with the increasing evidence from predictor finding <UNK> and open peer review initiatives for <UNK> related <UNK> data <UNK> <UNK> <UNK> are being set up. to maximise the new opportunities and to facilitate individual participant data metaanalyses, the world health organization has released a new data platform to encourage sharing of anonymised <UNK> clinical <UNK> to leverage the full potential of these evolutions, international and interdisciplinary collaboration in terms of data acquisition, model building and validation is crucial. study limitations with new publications on <UNK> related prediction models rapidly entering the medical literature, this systematic review cannot be viewed as an up-to-date list of all currently available <UNK> related prediction models. also, <UNK> of the studies we reviewed were only available as preprints. these studies might improve after peer review, when they enter the official medical literature; we will reassess these peer reviewed publications in future updates. we also found other prediction models that are authors risk of bias participants predictors outcome analysis wang et <UNK> low unclear unclear high zhang et <UNK> high unclear high high zhou et <UNK> high unclear high high* update <UNK> angelov et <UNK> high unclear high high arpan et <UNK> unclear unclear unclear high bai et <UNK> high unclear high high bassi et <UNK> high unclear high high borghesi et <UNK> high unclear unclear high born et <UNK> high unclear unclear high castiglioni et <UNK> unclear unclear low high guiot et <UNK> high unclear low high hu et <UNK> high unclear high high islam et <UNK> high unclear high high kana et <UNK> high unclear high high* karim et <UNK> high unclear high high khan et <UNK> high unclear high high* kumar et <UNK> high unclear unclear high* <UNK> unclear unclear unclear high ozturk et <UNK> high unclear unclear high rahimzadeh et <UNK> high unclear unclear high rehman et <UNK> high unclear unclear high singh et <UNK> high unclear unclear high ucar et <UNK> high unclear unclear high wu et <UNK> high unclear unclear high prognosis original review bai et <UNK> low unclear unclear high caramelo et <UNK> high high high high lu et <UNK> low low low high qi et <UNK> unclear low low high shi et <UNK> high high high high xie et <UNK> low low low high yan et <UNK> low high low high yuan et <UNK> low high low high update <UNK> huang et <UNK> unclear unclear unclear high pourhomayoun et <UNK> low low unclear high sarkar et <UNK> high high high high wang et <UNK> low low low high zeng et <UNK> low low low high update <UNK> al-najjar et <UNK> unclear unclear unclear high barda et <UNK> low low high high bello-chavolla et <UNK> unclear unclear low high carr et <UNK> low low low high chassagnon et <UNK> low low low high colombi et <UNK> high unclear unclear high das et <UNK> low low low high gong et <UNK> low low high high guo et <UNK> low high unclear high hu et <UNK> high low low high hu et <UNK> low unclear low high ji et <UNK> low low low high jiang et <UNK> unclear unclear unclear high levy et <UNK> low low low high liu et <UNK> low low low high mcrae et <UNK> high high high high singh et <UNK> low unclear high high vaid et <UNK> unclear high high high vazquez guillamet et <UNK> high low unclear high zhang et <UNK> low unclear unclear/low‡ high probast=prediction model risk of bias assessment tool. *risk of bias high owing to calibration not being evaluated. if this criterion is not taken into account, analysis risk of bias would have been unclear. †risk of bias high owing to calibration not being evaluated. if this criterion is not taken into account, analysis risk of bias would have been low. ‡zhang et al evaluated two outcomes: death (low risk of bias) and a composite poor outcome (unclear risk of bias). table <UNK> | continued currently being used in clinical practice without scientific <UNK> and web risk calculators launched for use while the scientific manuscript is still under review (and unavailable on request). these unpublished models naturally fall outside the scope of this review of the <UNK> as we have argued extensively <UNK> transparent reporting that enables validation by independent researchers is key for predictive analytics, and clinical guidelines should only recommend publicly available and verifiable algorithms. implications for practice all <UNK> reviewed prediction models were found to have a high risk of bias, and evidence from independent external validation of the newly developed models is currently lacking. however, the urgency of diagnostic and prognostic models to assist in quick and efficient triage of patients in the <UNK> pandemic might encourage clinicians and policymakers to prematurely implement prediction models without sufficient documentation and validation. earlier studies have shown that models were of limited use in the context of a <UNK> and they could even cause more harm than <UNK> therefore, we cannot recommend any model for use in practice at this point. the current oversupply of insufficiently validated models is not useful for clinical practice. future studies should focus on validating, comparing, improving, and updating promising available prediction models, rather than developing new <UNK> for example, diaz-quijano developed and externally validated a diagnostic model using brazilian surveillance data with reasonable discrimination, but many patients had to be excluded because no pcr testing was performed, hence this model needs further <UNK> two other models to diagnose <UNK> also showed promising discrimination at external validation in small unselected <UNK> <UNK> an externally validated model that used computed tomography based total severity scores showed good discrimination between patients with mild, common, and severe-critical <UNK> two models to predict progression to severe <UNK> within two weeks showed promising discrimination when validated externally on unselected <UNK> <UNK> another model discriminated well between survivors and non-survivors among confirmed cases, but the prediction horizon was not specified, and the study had many missing values for key <UNK> because reporting in each of these studies was insufficiently detailed and the validation was in datasets with fewer than <UNK> events in the smallest outcome category, validation in larger, international datasets is needed. such external validations should assess not only discrimination, but also calibration and clinical utility (net <UNK> <UNK> <UNK> owing to differences between healthcare systems (eg, chinese and european) in when patients are admitted to and discharged from hospital, as well as the testing criteria for patients with suspected <UNK> we anticipate most existing models will be miscalibrated, but this can usually be solved by updating and adjustment to the local setting. when creating a new prediction model, we recommend building on previous literature and expert opinion to select predictors, rather than selecting predictors in a purely data driven <UNK> this is especially important for datasets with limited sample <UNK> based on the predictors included in multiple models identified by our review, we encourage researchers to consider incorporating several candidate predictors. common predictors include age, body temperature, lymphocyte count, and lung imaging features. flulike signs and symptoms and neutrophil count are frequently predictive in diagnostic models, while comorbidities, sex, c reactive protein, and creatinine are frequently reported prognostic factors. by pointing to the most important methodological challenges and issues in design and reporting of the currently available models, we hope to have provided a useful starting point for further studies aiming to develop new models, or to validate and update existing ones. this living systematic review has been conducted in collaboration with the cochrane prognosis methods group. we will update this review and appraisal continuously to provide up-to-date information for healthcare decision makers and professionals as more international research emerges over time. box <UNK> availability of models in format for use in clinical practice several studies presented their models in a format for use in clinical practice. however, because all models were at high risk of bias, we do not recommend their routine use before they are properly externally validated. models to predict risk of developing coronavirus disease <UNK> <UNK> or of hospital admission for <UNK> in general population the <UNK> vulnerability index” to detect hospital admission for <UNK> pneumonia from other respiratory infections (eg, pneumonia, influenza) is available as an online <UNK> diagnostic models several sum <UNK> <UNK> <UNK> <UNK> and model <UNK> <UNK> are available to support the diagnosis. graphical diagnostic aids include <UNK> <UNK> <UNK> and a decision <UNK> the <UNK> diagnosis aid” app is available on ios and android devices to diagnose <UNK> in asymptomatic patients and those with suspected <UNK> additionally, online tools are <UNK> <UNK> <UNK> <UNK> <UNK> classification in terms of disease severity can be done using a published <UNK> a decision tree to detect severe disease for paediatric patients with confirmed <UNK> is also available in an <UNK> diagnostic models based on images five artificial intelligence models to assist with diagnosis based on medical images are available through web <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> one model is deployed in <UNK> hospitals, but the authors do not provide any usable tools in their <UNK> two papers includes a severity scoring system to classify patients based on <UNK> prognostic models to assist in the prognosis of mortality, a <UNK> a decision <UNK> a score <UNK> online <UNK> <UNK> <UNK> <UNK> <UNK> and a computed tomography based scoring rule are available in the <UNK> other online tools predict in-hospital death and the need for prolonged mechanical <UNK> <UNK> or in-hospital death and a composite of poor <UNK> <UNK> additionally <UNK> <UNK> <UNK> <UNK> and a model <UNK> are available to predict progression to severe <UNK> several studies made their code available on <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> seventy four studies did not include any usable equation, format, code, or reference for use or validation of their prediction model. conclusion several diagnostic and prognostic models for <UNK> are currently available and they all report moderate to excellent discrimination. however, these models are all at high risk of bias, mainly because of nonrepresentative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, and model overfitting. therefore, their performance estimates are probably optimistic and misleading. the covid-precise group does not recommend any of the current prediction models to be used in practice. future studies aimed at developing and validating diagnostic or prognostic models for <UNK> should explicitly address the concerns raised. sharing data and expertise for the validation and updating of <UNK> related prediction models is urgently needed.

<|EndOfText|>

summary points patients and healthcare professionals require clinical prediction models to accurately guide healthcare decisions larger sample sizes lead to the development of more robust models data should be of sufficient quality and representative of the target population and settings of application it is better to use all available data for model development (ie, avoid data splitting), with resampling methods (such as bootstrapping) used for internal validation when developing prediction models for binary or time-to-event outcomes, a well known rule of thumb for the required sample size is to ensure at least <UNK> events for each predictor parameter the actual required sample size is, however, context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the model we propose to use such information to tailor sample size requirements to the specific setting of interest, with the aim of minimising the potential for model overfitting while targeting precise estimates of key parameters our proposal can be implemented in a four step procedure and is applicable for continuous, binary, or time-to-event outcomes the pmsampsize package in stata or r allows researchers to implement the procedure clinical prediction models are needed to inform diagnosis and prognosis in <UNK> well known examples include the wells <UNK> <UNK> <UNK> <UNK> and the nottingham prognostic <UNK> <UNK> such models allow health professionals to predict an individual’s outcome value, or to predict an individual’s risk of an outcome being present (diagnostic prediction model) or developed in the future (prognostic prediction model). most prediction models are developed using a regression model, such as linear regression for continuous outcomes (eg, pain score), logistic regression for binary outcomes (eg, presence or absence of pre-eclampsia), or proportional hazards regression models for time-to-event data (eg, recurrence of venous <UNK> an equation is then produced that can be used to predict an individual’s outcome value or outcome risk conditional on his or her values of multiple predictors, which might include basic characteristics such as age, weight, family history, and comorbidities; biological measurements such as blood pressure and biomarkers; and imaging or other test results. supplementary material <UNK> shows examples of regression equations. developing a prediction model requires a development dataset, which contains data from a sample of individuals from the target population, containing their observed predictor values (available at the intended moment of <UNK> and observed outcome. the sample size of the development dataset must be large enough to develop a prediction model equation that is reliable when applied to new individuals in the target population. what constitutes an adequately large sample size for model development is, however, <UNK> with various blanket “rules of thumb” proposed and <UNK> this has created confusion about how to perform sample size calculations for studies aiming to develop a prediction model. in this article we provide practical guidance for calculating the sample size required for the development of clinical prediction models, which builds on our recent methodology <UNK> <UNK> we suggest that current minimum sample size rules of thumb are too simplistic and outline a more scientific approach that tailors sample size requirements to the specific setting of interest. we illustrate our proposal for continuous, binary, and time-to-event outcomes and conclude with some extensions. moving beyond the <UNK> events per variable rule of thumb in a development dataset, the effective sample size for a continuous outcome is determined by the total number of study participants. for binary outcomes, the effective sample size is often considered about equal to the minimum of the number of events (those with the outcome) and non-events (those without the outcome); time-to-event outcomes are often considered roughly equal to the total number of <UNK> when developing prediction models for binary or time-to-event outcomes, an established rule of thumb for the required sample size is to ensure at least <UNK> events for each predictor parameter (ie, each β term in the regression equation) being considered for inclusion in the prediction model <UNK> this is widely referred to as needing at least <UNK> events per variable <UNK> epv). the word “variable” is, however, misleading as some predictors actually require multiple β terms in the model equation—for example, two β terms are needed for a categorical predictor with three categories (eg, tumour grades i, ii, and iii), and two or more β terms are needed to model any non-linear effects of a continuous predictor, such as age or blood pressure. the inclusion of interactions between two or more predictors also increases the number of model parameters. hence, as prediction models usually have more parameters than actual predictors, it is preferable to refer to events per candidate predictor parameter (epp). the word candidate is important, as the amount of model overfitting is dictated by the total number of predictor parameters considered, not just those included in the final model equation. the rule of at least <UNK> epp has been widely advocated perhaps as a result of its simplicity, and it is regularly used to justify sample sizes within published articles, grant applications, and protocols for new model development studies, including by ourselves previously. the most prominent work advocating the rule came from simulation studies conducted in the <UNK> although this work actually focused more on the bias and precision of predictor effect estimates than on the accuracy of risk predictions from a developed model. the adequacy of the <UNK> epp rule has often been debated. although the rule provides a useful starting point, counter suggestions include either lowering the epp to below <UNK> or increasing it to <UNK> <UNK> or even <UNK> <UNK> these inconsistent recommendations reflect that the required epp is actually context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the <UNK> this finding is unsurprising as sample size considerations for other study designs, such as randomised trials of interventions, are all context dependent and tailored to the setting and research question. rules of thumb have also been advocated in the continuous outcome setting, such as two participants per <UNK> but these share the same concerns as for <UNK> <UNK> sample size calculation to ensure precise predictions and minimise overfitting recent work by van smeden et <UNK> <UNK> and riley et <UNK> <UNK> describe how to calculate the required sample size for prediction model development, conditional on the user specifying the overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit <UNK> ). these authors’ approaches can be implemented in a four step procedure. each step leads to a sample size calculation, and ultimately the largest sample size identified is the one required. we describe these four steps, and, to aid general readers, provide the more technical details of each step in the figures. step <UNK> what sample size will produce a precise estimate of the overall outcome risk or mean outcome value? fundamentally, the sample size must allow the prediction model’s intercept to be precisely estimated, to ensure that the developed model can accurately predict the mean outcome value (for continuous outcomes) or overall outcome proportion (for binary or time-to-event outcomes). a simple way to do this is to calculate the sample size needed to precisely estimate (within a small margin of error) the intercept in a model when no predictors are included (the null <UNK> <UNK> shows the calculation for binary and time-to-event outcomes, and we generally recommend aiming for a margin of error of <UNK> in the overall outcome proportion estimate. for example, with a binary outcome that occurs in half of individuals, a sample size of at least <UNK> people is needed to target a confidence interval of <UNK> to <UNK> for the overall outcome proportion, and thus an error of at most <UNK> around the true value of <UNK> to achieve the same margin of error with outcome proportions of <UNK> and <UNK> at least <UNK> and <UNK> participants, respectively, are required. for time-to-event outcomes, a key time point needs to be identified, along with the anticipated outcome event rate. for example, with an anticipated event rate of <UNK> per <UNK> person years of the entire follow-up, the sample size must include a total of <UNK> person years of follow-up to ensure an expected margin of error of <UNK> in the estimate of a <UNK> year outcome probability of <UNK> such that the expected confidence interval is <UNK> to <UNK> for continuous outcomes, the anticipated mean and variance of outcome values must be prespecified, alongside the anticipated percentage of variation explained by the prediction model (see supplementary material <UNK> for <UNK> step <UNK> what sample size will produce predicted values that have a small mean error across all individuals? in addition to predicting the average outcome value precisely (see step <UNK> the sample size for model development should also aim for precise predictions across the spectrum of predicted values. for binary outcomes, van smeden et al use simulation across a wide range of scenarios to evaluate how the error of predicted outcome probabilities from a developed model depends on various characteristics of the development dataset sampled from a target <UNK> they found that the number of candidate predictor parameters, total sample size, and outcome proportion were the three main drivers of a model’s mean predictive accuracy. this led to a sample size formula (fig <UNK> to help ensure that new prediction models will, on average, have a small prediction error in the estimated outcome probabilities in the target population (as measured by the mean absolute prediction error, mape). the calculation requires the number of candidate predictor parameters and the anticipated outcome proportion in the target population to be prespecified. for example, with <UNK> candidate predictor parameters and an outcome proportion of <UNK> a sample size of at least <UNK> participants and <UNK> epp is required to target a mean absolute error of <UNK> between observed and true outcome probabilities (see fig <UNK> for calculation). the calculation is available as an interactive tool (https://mvansmeden.shinyapps.io/beyondepv/ ) and applicable to situations with <UNK> or fewer candidate predictors. ongoing work aims to extend to larger numbers of candidate predictors and also to time-to-event outcomes. for continuous outcomes, accurate predictions across the spectrum of predicted values require the standard deviation of the residuals to be precisely <UNK> <UNK> supplementary material <UNK> shows that to target a less than <UNK> multiplicative error in the estimated residual standard deviation, the required sample size is simply <UNK> where p is the number of predictor parameters considered. step <UNK> what sample size will produce a small required shrinkage of predictor effects? our third recommended step is to identify the sample size required to minimise the problem of <UNK> overfitting is when a developed model’s predictions are more extreme than they ought to be for individuals in a new dataset from the same target population. for example, an overfitted prediction model for a binary outcome will give a predicted outcome probability too close to <UNK> for individuals with a higher than the average outcome probability and too close to <UNK> for individuals with a lower than the average outcome probability. overfitting notably occurs when the sample size is too small. in particular, when the number of candidate predictor parameters is large relative to the number of participants in total (for continuous outcomes) or to the number of participants with the outcome event (for binary or time-to-event outcomes). a consequence of overfitting is that a developed model’s apparent predictive performance (as observed in the development dataset itself) will be optimistic (ie, too high), and its actual predictive performance in new data from the same target population will be lower (ie, worse). shrinkage (also known as penalisation or regularisation) methods deal with the problem of overfitting by reducing the variability in the developed model’s predictions such that extreme predictions (eg, predicted probabilities close to <UNK> or <UNK> are pulled back toward the overall <UNK> however, there is no guarantee that shrinkage will fully overcome the problem of overfitting when developing a prediction model. this is because the shrinkage or penalty factors (which dictate the magnitude of shrinkage required) are also estimated from the development dataset and, especially when the sample size is small, are often imprecise and so fail to tackle the magnitude of overfitting correctly in a particular <UNK> furthermore, a negative correlation tends to occur between the estimated shrinkage required and the apparent performance of a model. if the apparent model performance is excellent simply by chance, the required shrinkage is typically estimated too <UNK> thus, ironically, in those situations when overfitting is of most concern (and thus shrinkage is most urgently needed), the prediction model developer has insufficient assurance in selecting the proper amount of shrinkage to cancel the impact of overfitting. riley et al therefore suggest identifying the sample size and number of candidate predictors that correspond to a small amount of desired shrinkage <UNK> during model <UNK> <UNK> the sample size calculation (fig <UNK> requires the researcher to prespecify the number of candidate predictor parameters and, for binary or time-to-event outcomes, the anticipated outcome proportion or rate, respectively, in the target population. in addition, a (conservative) value for the anticipated model performance is required, as defined by the cox-snell r squared statistic <UNK> cs <UNK> <UNK> the anticipated value of <UNK> cs is important because it reflects the signal:noise ratio, which has an impact on the estimation of multiple parameters and the potential for overfitting. when the signal:noise ratio is anticipated to be high (eg, <UNK> cs is close to <UNK> for a prediction model with a continuous outcome), true patterns are easier to detect and so overfitting is less of a concern, such that more predictor parameters can be estimated. however, when the signal:noise ratio is low (ie, <UNK> cs is anticipated to be close to <UNK> true patterns are harder to identify and there is more potential for overfitting, such that fewer predictor parameters can be estimated reliably. in the continuous outcome setting, <UNK> cs is simply the coefficient of determination <UNK> , which quantifies the proportion of the variance of outcome values that is explained by the prediction model and thus is between <UNK> and <UNK> for example, when developing a prediction model for a continuous outcome with up to <UNK> predictor parameters and an anticipated <UNK> cs of <UNK> a sample size of <UNK> participants is required to ensure the expected shrinkage is <UNK> (see supplementary material <UNK> for full calculation). this corresponds to about seven participants for each predictor parameter considered. the <UNK> cs statistic generalises to non-continuous outcomes and allows sample size calculations to minimise the expected shrinkage when developing a prediction model for binary and time-to-event outcomes (fig <UNK> for example, when developing a new logistic regression model with up to <UNK> candidate predictor parameters and an anticipated <UNK> cs of at least <UNK> a sample size of <UNK> participants is required to ensure the expected shrinkage is <UNK> (see fig <UNK> for full calculation). if the target setting has an outcome proportion of <UNK> this corresponds to an epp of <UNK> the required sample size and epp are sensitive to the choice of <UNK> cs , with lower anticipated values of <UNK> cs leading to higher required sample sizes. therefore, a conservative choice of <UNK> cs is recommended (fig <UNK> as in sample size calculations for randomised trials evaluating intervention effects, external evidence and expert opinion are required to inform the values that need specifying in the sample size calculator. figure <UNK> provides guidance for specifying <UNK> cs . importantly, unlike for continuous outcomes when <UNK> cs is bounded between <UNK> and <UNK> the <UNK> cs is bounded between <UNK> and <UNK> cs ) for binary and time-to-event outcomes. the <UNK> cs ) denotes the maximum possible value of <UNK> cs , which is dictated by the overall outcome proportion or rate in the development dataset and is often much less than <UNK> supplementary material <UNK> shows the calculation of <UNK> cs ). for logistic regression models with outcome proportions of <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> the corresponding <UNK> cs ) values are <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> respectively. thus the anticipated <UNK> cs might be small, even for a model with potentially good performance. step <UNK> what sample size will produce a small optimism in apparent model fit? the sample size should also ensure a small difference in the developed models apparent and optimism adjusted values of r <UNK> nagelkerke (ie, <UNK> cs <UNK> cs )), as this is a fundamental overall measure of model <UNK> <UNK> the apparent <UNK> nagelkerke value is simply the model’s observed performance in the same data as used to develop the model, whereas the optimism adjusted <UNK> nagelkerke value is a more realistic (approximately unbiased) estimate of the model’s fit in the target population. the sample size calculations are shown in supplementary material <UNK> for continuous outcomes and in figure <UNK> for binary and time-to-event outcomes. as before, they require the user to specify the anticipated <UNK> cs and the <UNK> cs ), as described in figure <UNK> for example, when developing a logistic regression model with an anticipated <UNK> cs of <UNK> and in a setting with an outcome proportion of <UNK> (such that the <UNK> cs ) is <UNK> <UNK> participants are required to ensure the expected optimism in the apparent <UNK> nagelkerke is just <UNK> (see figure <UNK> for calculation). recommendations and software box <UNK> summarises our recommended steps for calculating the minimum sample size required for prediction model development. this involves four calculations for binary outcomes <UNK> to <UNK> three for time-to-event outcomes <UNK> to <UNK> and four for continuous outcomes <UNK> to <UNK> to implement the calculations, we have written the pmsampsize package for stata and r. the software calculates the sample size needed to meet all the criteria listed in box <UNK> (except <UNK> which is available at https://mvansmeden.shinyapps.io/beyondepv/), conditional on the user inputting values of required parameters such as the number of candidate predictors, the anticipated outcome proportion in the target population, and the anticipated <UNK> cs . the calculations are especially helpful when prospective data collection (eg, new cohort study) are required before model development; however, they are also relevant when existing data are available to guide the number of predictors that can be considered. box <UNK> recommendations for calculating the sample size needed when developing a clinical prediction model for continuous, binary, and time-to-event outcomes to increase the potential for developing a robust prediction model, the sample size should be at least large enough to minimise model overfitting and to target sufficiently precise model predictions binary outcomes for binary outcomes, ensure the sample size is enough to: estimate the overall outcome proportion with sufficient precision (use equation in figure <UNK> <UNK> target a small mean absolute prediction error (use equation in figure <UNK> if number of predictor parameters is <UNK> <UNK> target a shrinkage factor of <UNK> (use equation in figure <UNK> <UNK> target small optimism of <UNK> in the apparent <UNK> nagelkerke (use equation in figure <UNK> <UNK> time-to-event outcomes for time-to-event outcomes, ensure the sample size is enough to: estimate the overall outcome proportion with sufficient precision at one or more key time-points in follow-up (use equation in figure <UNK> <UNK> target a shrinkage factor of <UNK> (use equation in figure <UNK> <UNK> target small optimism of <UNK> in the apparent <UNK> nagelkerke (use equation in figure <UNK> <UNK> continuous outcomes for continuous outcomes, ensure the sample size is enough to: estimate the model intercept precisely (see supplementary material <UNK> <UNK> estimate the model residual variance with sufficient precision (see supplementary material <UNK> <UNK> target a shrinkage factor of <UNK> (use equation in figure <UNK> <UNK> target small optimism of <UNK> in the apparent <UNK> nagelkerke (use equation in figure <UNK> <UNK> these approaches require researchers to specify the anticipated overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit <UNK> cs ). when the choice of values is uncertain, we generally recommend being conservative and so taking those values (eg, smallest <UNK> cs ) that give larger sample sizes when an existing dataset is already available (such that sample size is already defined), the calculations can be used to identify if the sample size is sufficient to estimate the overall outcome risk or the mean outcome value, and how many predictor parameters can be considered before overfitting becomes a concern applied examples we now illustrate the recommendations in box <UNK> by using three examples. example <UNK> binary outcome north et al developed a model predicting pre-eclampsia in pregnant women based on clinical predictors measured at <UNK> weeks’ <UNK> including vaginal bleeding, age, previous miscarriage, family history, smoking, and alcohol consumption. the model included <UNK> predictor parameters and had a c statistic of <UNK> emerging research aims to improve this and other pre-eclampsia prediction models by including additional predictors (eg, biomarkers and ultrasound measurements). as the outcome is binary, the sample size calculation for a new prediction model needs to examine criteria <UNK> to <UNK> in box <UNK> this requires us to input the overall proportion of women who will develop pre-eclampsia <UNK> and the number of candidate predictor parameters (assumed to be <UNK> for illustration). for an outcome proportion of <UNK> the <UNK> cs ) value is <UNK> (see supplementary material <UNK> if we assume, conservatively, that the new model will explain <UNK> of the variability, the anticipated <UNK> cs value is <UNK> now we can check criteria <UNK> <UNK> and <UNK> by typing in stata: pmsampsize, type(b) <UNK> <UNK> <UNK> this indicates that at least <UNK> women are required, corresponding to <UNK> events and an epp of <UNK> this is driven by criterion <UNK> to ensure the expected shrinkage required is just <UNK> (to minimise the potential overfitting). to check criterion <UNK> in box <UNK> we can apply the formula in figure <UNK> this suggests that <UNK> women are needed to target a mean absolute error in predicted probabilities of <UNK> this is much lower than the <UNK> women needed to meet criterion <UNK> if recruiting <UNK> women is impractical (eg, because of time, cost, or practical constraints for data collection), the sample size required can be reduced by identifying a smaller number of candidate predictors (eg, based on existing evidence from systematic <UNK> for example, with <UNK> rather than <UNK> candidate predictors, the required sample size to meet all four criteria is at least <UNK> women and <UNK> events (still <UNK> epp). example <UNK> time-to-event outcome many prognostic models are available for the risk of a recurrent venous thromboembolism (vte) after cessation of treatment for a first <UNK> for example, the model of ensor et al included predictors of age, sex, site of first clot, d-dimer level, and the lag time from cessation of treatment until measurement of d-dimer (often around <UNK> <UNK> the model’s c statistic was <UNK> and the adjusted <UNK> cs was <UNK> (corresponding to <UNK> of the total variation). emerging research aims to extend such models by including additional predictors. the sample size required for a new model must at least meet criteria <UNK> to <UNK> this requires us to input a key time point for prediction of vte recurrence risk (eg, two years), alongside the number of candidate predictor parameters <UNK> the anticipated mean follow-up <UNK> years), and outcome event rate <UNK> or <UNK> vte recurrences for every <UNK> person years of follow-up), and the conservative value of <UNK> cs <UNK> with all chosen values based on ensor et <UNK> now criteria <UNK> to <UNK> can be checked, for example by typing in stata: pmsampsize, type(s) <UNK> <UNK> <UNK> <UNK> <UNK> this indicates that at least <UNK> participants are required, corresponding to <UNK> events and an epp of <UNK> this is considerably more than <UNK> epp, and is driven by a desired shrinkage factor (criterion <UNK> of only <UNK> to minimise overfitting based on just <UNK> of variation explained by the model. if the number of candidate predictor parameters is lowered to <UNK> the required sample size is reduced to <UNK> (still an epp of <UNK> example <UNK> continuous outcome hudda et al developed a prediction model for fat free mass in children and adolescents aged <UNK> to <UNK> years, including <UNK> predictor parameters based on height, weight, age, sex, and <UNK> the model is needed to provide an estimate of an individual’s current fat mass (=weight minus predicted fat free mass). on external validation, the model had an <UNK> cs of <UNK> let us assume that the model will need updating (eg, in <UNK> years owing to changes in the population behaviour and environment), and that an additional <UNK> predictor parameters (and thus a total of <UNK> parameters) will need to be considered in the model development. the sample size for a model development dataset must at least meet the four criteria of <UNK> to <UNK> in box <UNK> this requires us to specify the anticipated <UNK> cs <UNK> number of candidate predictor parameters <UNK> and mean <UNK> kg) and standard deviation <UNK> kg) of fat free mass in the target population (taken from hudda et <UNK> for example, in stata, after installation of pmsampsize (type: ssc install pmsampsize), we can type: pmsampsize, type(c) <UNK> <UNK> <UNK> <UNK> this returns that at least <UNK> participants are required, and so <UNK> participants for each predictor parameter. the sample size of <UNK> is driven by the number needed to precisely estimate the model standard deviation (criterion <UNK> as only <UNK> participants are needed to minimise overfitting (criteria <UNK> and <UNK> extensions and further topics ensuring accurate predictions in key subgroups alongside the criteria outlined in box <UNK> a more stringent task is to ensure model predictions are accurate in key subgroups defined by particular values or categories of included <UNK> one way to tackle this is to ensure predictor effects in the model equation are precisely estimated, at least for key subgroups of <UNK> <UNK> for binary and time-to-event outcomes, the precision of a predictor’s effect depends on its magnitude, the variance of the predictor’s values, the predictor’s correlation with other predictors in the model, the sample size, and the outcome proportion or rate in the <UNK> for continuous outcomes, it depends on the sample size, the residual variance, the correlation of the predictor with other included predictors, and the variance of the predictor’s <UNK> <UNK> note that for important categorical predictors large sample sizes might be needed to avoid separation issues (ie, where no events or non-events occur in some <UNK> and potential bias from sparse <UNK> sample size considerations when using an existing dataset our proposed sample size calculations (ie, based on the criteria in box <UNK> are still useful in situations when an existing dataset is already available, with a specific number of participants and predictors. firstly, the calculations might identify that the dataset is too small (for example, if the overall outcome risk cannot be estimated precisely) and so the collection of further data is <UNK> <UNK> secondly, the calculations might help identify how many predictors can be considered before overfitting becomes a concern. the shrinkage estimate obtained from fitting the full model (including all predictors) can be used to gauge whether the number of predictors could be reduced through data reduction techniques such as principal components <UNK> this process should be done blind to the estimated predictor effects in the full model, as otherwise decisions about predictor inclusion will be influenced by a “quick look” at the results (which increases the overfitting). sample size requirements when using variable selection further research on sample size requirements with variable selection is required, especially for the use of more modern penalisation methods such as the lasso (least absolute shrinkage and selection operator) or elastic <UNK> <UNK> such methods allow shrinkage and variable selection to operate simultaneously, and they even allow the consideration of more predictor parameters than number of participants or outcome events (ie, in high dimensional settings). however, there is no guarantee such models solve the problem of overfitting in the dataset at hand. as mentioned, they require penalty and shrinkage factors to be estimated using the development dataset, and such estimates will often be hugely imprecise. also, the subset of included predictors might be highly <UNK> that is, if the prediction model development was repeated on a different sample of the same size, a different subset of predictors might be selected and important predictors missed (especially if sample size is small). in healthcare the final set of predictors is a crucial consideration, owing to their cost, time, burden (eg, blood test, invasiveness), and measurement requirements. larger sample sizes might be needed when using machine learning approaches to develop risk prediction models an alternative to regression based prediction models are those based on machine learning methods, such as random forests and neural networks (of which “deep learning” methods are a special <UNK> when the focus is on individualised outcome risk prediction, it has been shown that extremely large datasets might be needed for machine learning techniques. for binary outcomes, machine learning techniques could need more than <UNK> times as many events for each predictor to achieve a small amount of overfitting compared with classic modelling techniques such as logistic regression, and might show instability and a high optimism even with more than <UNK> <UNK> a major cause of this problem is that the number of predictor (“feature”) parameters considered by machine learning approaches will usually far exceed that for regression, even when the same set of predictors is considered, particularly because they routinely examine multiple interaction terms and categorise continuous predictors. therefore, machine learning methods are not immune to sample size requirements, and actually might need truly “big data” to ensure their developed models have small overfitting, and for their potential advantages (eg, dealing with highly non-linear relations and complex interactions) to reach fruition. the size of most medical research datasets is better suited to using regression (including penalisation and shrinkage <UNK> especially as regression also leads to a transparent model equation that facilitates implementation, validation, and graphical displays. sample size for model updating when an existing prediction model is updated, the existing model equation is revised using a new dataset. the required sample size for this dataset depends on how the model is to be updated and whether additional predictors are to be included. in our worked examples, we assumed that all parameters in the existing model will be re-estimated using the model updating dataset. in that situation, the researcher can still follow the guidance in box <UNK> for calculating the required sample size, with the total predictor parameters the same as in the original model plus those new parameters required for any additional predictors. sometimes, however, only a subset of the existing model’s parameters is to be <UNK> <UNK> in particular, to deal with calibration-in-the-large, researchers might only want to revise the model intercept (or baseline survival), while constraining the other parameter estimates to be the same as those in the existing model. in this case the required sample size only needs to be large enough to estimate the mean outcome value or outcome risk precisely (ie, to meet criteria <UNK> <UNK> or <UNK> in box <UNK> even if researchers also want to update the existing predictor effects, they might decide to constrain their updated values to be equal to the original values multiplied by a constant. then, the sample size only needs to be large enough to estimate one predictor parameter (ie, the constant) for the existing predictors, plus any new parameters the researchers decide to add. such model updating techniques therefore reduce the sample size needed (to meet the criteria in box <UNK> compared with when every predictor parameter is re-estimated without constraint. conclusion patients and healthcare professionals require clinical prediction models to accurately guide healthcare <UNK> larger sample sizes lead to more robust models being developed, and our guidance in box <UNK> outlines how to calculate the minimum sample size required. clearly, the more data for model development the better; so if larger sample sizes are achievable than our guidance suggests, use it! of course, any data collected should be of sufficient quality and representative of the target population and settings of <UNK> <UNK> after data collection, careful model building is required using appropriate <UNK> <UNK> <UNK> in particular, we do not recommend data splitting (eg, into model training and testing samples), as this is inefficient and it is better to use all the data for model development, with resampling methods (such as bootstrapping) used for internal <UNK> <UNK> sometimes external information might be used to supplement the development dataset <UNK> lastly, sample size requirements when externally validating an existing prediction model require a different approach, as discussed <UNK>

<|EndOfText|>

artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies abstract objective to systematically examine the design, reporting standards, risk of bias, and claims of studies comparing the performance of diagnostic deep learning algorithms for medical imaging with that of expert clinicians. design systematic review. data sources medline, embase, cochrane central register of controlled trials, and the world health organization trial registry from <UNK> to june <UNK> eligibility criteria for selecting studies randomised trial registrations and non-randomised studies comparing the performance of a deep learning algorithm in medical imaging with a contemporary group of one or more expert clinicians. medical imaging has seen a growing interest in deep learning research. the main distinguishing feature of convolutional neural networks (cnns) in deep learning is that when cnns are fed with raw data, they develop their own representations needed for pattern recognition. the algorithm learns for itself the features of an image that are important for classification rather than being told by humans which features to use. the selected studies aimed to use medical imaging for predicting absolute risk of existing disease or classification into diagnostic groups (eg, disease or non-disease). for example, raw chest radiographs tagged with a label such as pneumothorax or no pneumothorax and the cnn learning which pixel patterns suggest pneumothorax. review methods adherence to reporting standards was assessed by using consort (consolidated standards of reporting trials) for randomised studies and tripod (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) for nonrandomised studies. risk of bias was assessed by using the cochrane risk of bias tool for randomised studies and probast (prediction model risk of bias assessment tool) for non-randomised studies. results only <UNK> records were found for deep learning randomised clinical trials, two of which have been published (with low risk of bias, except for lack of blinding, and high adherence to reporting standards) and eight are ongoing. of <UNK> non-randomised clinical trials identified, only nine were prospective and just six were tested in a real world clinical setting. the median number of experts in the comparator group was only four (interquartile range <UNK> full access to all datasets and code was severely limited (unavailable in <UNK> and <UNK> of studies, respectively). the overall risk of bias was high in <UNK> of <UNK> studies and adherence to reporting standards was suboptimal <UNK> adherence for <UNK> of <UNK> tripod items). <UNK> of <UNK> studies stated in their abstract that performance of artificial intelligence was at least comparable to (or better than) that of clinicians. only <UNK> of <UNK> studies <UNK> stated that further prospective studies or trials were required. conclusions few prospective deep learning studies and randomised trials exist in medical imaging. most nonrandomised trials are not prospective, are at high risk of bias, and deviate from existing reporting standards. data and code availability are lacking in most studies, and human comparator groups are often small. future studies should diminish risk of bias, enhance real world clinical relevance, improve reporting and transparency, and appropriately temper conclusions. study registration prospero <UNK> introduction the digitisation of society means we are amassing data at an unprecedented rate. healthcare is no exception, with ibm estimating approximately one million gigabytes accruing over an average person’s lifetime and the overall volume of global healthcare data doubling every few <UNK> to make sense of these big data, clinicians are increasingly collaborating with computer scientists and other allied disciplines to for numbered affiliations see end of the article. what is already known on this topic the volume of published research on deep learning, a branch of artificial intelligence (ai), is rapidly growing media headlines that claim superior performance to doctors have fuelled hype among the public and press for accelerated implementation what this study adds few prospective deep learning studies and randomised trials exist in medical imaging most non-randomised trials are not prospective, are at high risk of bias, and deviate from existing reporting standards data and code availability are lacking in most studies, and human comparator groups are often small future studies should diminish risk of bias, enhance real world clinical relevance, improve reporting and transparency, and appropriately temper conclusions make use of artificial intelligence (ai) techniques that can help detect signal from <UNK> a recent forecast has placed the value of the healthcare ai market as growing from <UNK> <UNK> <UNK> in <UNK> to <UNK> by <UNK> with a <UNK> compound annual growth <UNK> deep learning is a subset of ai which is formally defined as “computational models that are composed of multiple processing layers to learn representations of data with multiple levels of <UNK> in practice, the main distinguishing feature between convolutional neural networks (cnns) in deep learning and traditional machine learning is that when cnns are fed with raw data, they develop their own representations needed for pattern recognition; they do not require domain expertise to structure the data and design feature <UNK> in plain language, the algorithm learns for itself the features of an image that are important for classification rather than being told by humans which features to use. a typical example would be feeding in raw chest radiographs tagged with a label such as either pneumothorax or no pneumothorax and the cnn learning which pixel patterns suggest pneumothorax. fields such as medical imaging have seen a growing interest in deep learning research, with more and more studies being <UNK> some media headlines that claim superior performance to doctors have fuelled hype among the public and press for accelerated implementation. examples include: “google says its ai can spot lung cancer a year before doctors” and “ai is better at diagnosing skin cancer than your doctor, study <UNK> <UNK> the methods and risk of bias of studies behind such headlines have not been examined in detail. the danger is that public and commercial appetite for healthcare ai outpaces the development of a rigorous evidence base to support this comparatively young field. ideally, the path to implementation would involve two key steps. firstly, well conducted and well reported development and validation studies that describe an algorithm and its properties in detail, including predictive accuracy in the target setting. secondly, well conducted and transparently reported randomised clinical trials that evaluate usefulness in the real world. both steps are important to ensure clinical practice is determined based on the best evidence <UNK> our systematic review seeks to give a contemporary overview of the current standards of deep learning research for clinical applications. specifically, we sought to describe the study characteristics, and evaluate the methods and quality of reporting and transparency of deep learning studies that compare diagnostic algorithm performance with human clinicians. we aim to suggest how we can move forward in a way that encourages innovation while avoiding hype, diminishing research waste, and protecting patients. methods the protocol for this study was registered in the online prospero database <UNK> before search execution. the supplementary appendix gives details of any deviations from the protocol. this manuscript has been prepared according to the prisma (preferred reporting items for systematic reviews and meta-analyses) guidelines and a checklist is available in the supplementary <UNK> study identification and inclusion criteria we performed a comprehensive search by using free text terms for various forms of the keywords “deep learning” and “clinician” to identify eligible studies. appendix <UNK> presents the exact search strategy. several electronic databases were searched from <UNK> to june <UNK> medline, embase, cochrane central register of controlled trials (central), and the world health organization international clinical trials registry platform (who-ictrp) search portal. additional articles were retrieved by manually scrutinising the reference lists of relevant publications. we selected publications for review if they satisfied several inclusion criteria: a peer reviewed scientific report of original research; english language; assessed a deep learning algorithm applied to a clinical problem in medical imaging; compared algorithm performance with a contemporary human group not involved in establishing the ground truth (the true target disease status verified by best clinical practice); and at least one human in the group was considered an expert. we included studies when the aim was to use medical imaging for predicting absolute risk of existing disease or classification into diagnostic groups (eg, disease or non-disease). exclusion criteria included informal publication types (such as commentaries, letters to the editor, editorials, meeting abstracts). deep learning for the purpose of medical imaging was defined as computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction (in practice through a cnn; see box <UNK> a clinical problem was defined as a situation in which a patient would usually see a medical professional to improve or manage their health (this did not include segmentation tasks, eg, delineating the borders of a tumour to calculate tumour volume). an expert was defined as an appropriately board certified specialist, attending physician, or equivalent. a real world clinical environment was defined as a situation in which the algorithm was embedded into an active clinical pathway. for example, instead of an algorithm being fed thousands of chest radiographs from a database, in a real world implementation it would exist within the reporting software used by radiologists and be acting or supporting the radiologists in real time. study selection and extraction of data after removal of clearly irrelevant records, four people (mn, yc, cal, dina radenkovic) independently screened abstracts for potentially eligible studies so that each record was reviewed by at least two people. full text reports were then assessed for eligibility with disagreements resolved by consensus. at least two people (mn, yc, cal) extracted data from study reports independently and in duplicate for each eligible study, with disagreements resolved by consensus or a third reviewer. adherence to reporting standards and risk of bias we assessed reporting quality of non-randomised studies against a modified version of the tripod (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) <UNK> this statement aims to improve the transparent reporting of prediction modelling studies of all types and in all medical <UNK> the tripod statement consists of a <UNK> item checklist <UNK> total points when all subitems are included), but we considered some items to be less relevant to deep learning studies (eg, points that related to predictor variables). deep learning algorithms can consider multiple predictors; however, in the cases we assessed, the only predictors (almost exclusively) were the individual pixels of the image. the algorithm did not typically receive information on characteristics such as patient age, sex, and medical history. therefore, we used a modified list of <UNK> total points (see appendix <UNK> the aim was to assess whether studies broadly conformed to reporting recommendations included in tripod, and not the detailed granularity required for a full assessment of <UNK> we assessed risk of bias for non-randomised studies by applying probast (prediction model risk of bias assessment <UNK> probast contains <UNK> signalling questions from four domains (participants, predictors, outcomes, and analysis) to allow assessment of the risk of bias in predictive modelling <UNK> we did not assess applicability (because no specific therapeutic question existed for this systematic review) or predictor variables (these are less relevant in deep learning studies on medical imaging; see appendix <UNK> we assessed the broad level reporting of randomised studies against the consort (consolidated standards of reporting trials) statement. risk of bias was evaluated by applying the cochrane risk of bias <UNK> <UNK> data synthesis we intentionally planned not to conduct formal quantitative syntheses because of the probable heterogeneity of specialties and outcomes. patient and public involvement patients were not involved in any aspect of the study design, conduct or in the development of the research question or outcome measures. results study selection our electronic search, which was last updated on <UNK> june <UNK> retrieved <UNK> records <UNK> study records and <UNK> trial registrations; see fig <UNK> of the <UNK> study records, we assessed <UNK> full text articles; <UNK> were excluded, which left <UNK> non-randomised studies for analysis. of the <UNK> trial registrations, we assessed <UNK> in full; <UNK> were excluded, which left <UNK> trial registrations that related to deep learning. box <UNK> deep learning in imaging with examples deep learning is a subset of artificial intelligence that is formally defined as “computational models that are composed of multiple processing layers to learn representations of data with multiple levels of <UNK> a deep learning algorithm consists of a structure referred to as a deep neural network of which a convolutional neural network (cnn) is one particular type frequently used in imaging. cnns are structurally inspired by the hierarchical arrangement of neurons within the brain. they can take many nuanced forms but the basic structure consists of an input layer, multiple hidden layers, and a final output layer. each hidden layer responds to a different aspect of the raw input. in the case of imaging, this could be an edge, colour, or specific pattern. the key difference between deep learning and other types of machine learning is that cnns develop their own representations needed for pattern recognition rather than requiring human input to structure the data and design feature extractors. in plain language, the algorithm learns for itself the features of an image that are important for classification. therefore, the algorithm has the freedom to discover classification features that might not have been apparent to humans (particularly when datasets are large) and thereby improve the performance of image classification. cnns use raw image data that have been labelled by humans in a process known as supervised learning. each image is fed into the input layer of the algorithm as raw pixels and then processed sequentially through the layers of the cnn. the final output is a classification likelihood of the image belonging to a prespecified group. some examples from this review include the following: • feeding in raw chest radiographs tagged with a label (pneumothorax or no pneumothorax) and the cnn learning which pixel patterns suggest pneumothorax. when fed with new untagged images, the cnn outputs a likelihood of the new image containing a pneumothorax or not. • feeding in raw retinal images tagged with the stage of age related macular degeneration and the cnn learning which pixel patterns suggest a particular stage. when fed with new untagged images, the cnn outputs a likelihood of the new image containing a specific stage of age related macular degeneration. • feeding in optical coherence tomography scans tagged with a management decision (urgent referral, semi urgent referral, routine referral, observation). when fed with new untagged images, the cnn outputs a likelihood of the most appropriate management decision. randomised clinical trials table <UNK> summarises the <UNK> trial registrations. eight related to gastroenterology, one to ophthalmology, and one to radiology. eight were from china, one was from the united states, and one from taiwan. two trials have completed and published their results (both in <UNK> three are recruiting, and five are not yet recruiting. the first completed trial enrolled <UNK> paediatric patients who attended ophthalmology clinics in china. these patients underwent cataract assessment with or without an ai platform (using deep learning) to diagnose and provide a treatment recommendation (surgery or <UNK> the authors found that accuracy (defined as proportion of true results) of cataract diagnosis and treatment recommendation with ai were <UNK> (sensitivity <UNK> specificity <UNK> and <UNK> (sensitivity <UNK> specificity <UNK> respectively. these results were significantly lower than accuracy of diagnosis <UNK> sensitivity <UNK> specificity <UNK> and treatment recommendation <UNK> sensitivity <UNK> specificity <UNK> by senior consultants <UNK> for both); and also lower than the results for the same ai when tested in a nonrandomised clinical trial setting <UNK> and <UNK> respectively). the mean time for receiving a diagnosis with the ai platform was faster than diagnosis by consultants <UNK> v <UNK> minutes, <UNK> the authors suggested that this might explain why patients were more satisfied with ai (mean satisfaction score <UNK> v <UNK> <UNK> risk of bias was low in all domains except for blinding of participants and personnel. the reporting showed high adherence <UNK> of <UNK> items, <UNK> to the consort checklist (which was included with the manuscript). the second completed trial enrolled <UNK> patients who underwent a colonoscopy with or without the assistance of a real time automatic polyp detection system, which provided simultaneous visual and sound alerts when it found a <UNK> the authors reported that the detection system resulted in a significant increase in the adenoma detection rate <UNK> v <UNK> <UNK> and an increase in the number of hyperplastic polyps identified <UNK> v <UNK> <UNK> risk of bias was low in all domains except for blinding of participants, personnel, and outcome assessors. one of the other trial registrations belongs to the same author group. these authors are performing a additional records identified through trial registry full text articles excluded not contemporary comparison, not only human or human involved with ground truth not a clinical problem not english language not an article no experts not deep learning <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> records screened aer duplicates removed records identified through publication databases records excluded full text articles assessed for eligibility records included in qualitative synthesis <UNK> studies <UNK> trial registrations <UNK> <UNK> quantitative synthesis (meta-analysis) not performed <UNK> <UNK> <UNK> <UNK> full text trial registrations excluded not randomised not deep learning <UNK> <UNK> <UNK> <UNK> <UNK> fig <UNK> | prisma (preferred reporting items for systematic reviews and meta-analyses) flowchart of study records double blind randomised clinical trial with sham ai to overcome the blinding issue in the previous study. the reporting showed high adherence <UNK> of <UNK> items, <UNK> to the consort checklist (though the consort checklist itself was not included or referenced by the manuscript). non-randomised studies general characteristics table <UNK> and table <UNK> summarise the basic characteristics of the <UNK> non-randomised studies. nine of <UNK> <UNK> non-randomised studies were prospective, but only six of these nine were tested in a real world clinical environment. the us and asia accounted for <UNK> of studies, with the top four countries as follows: us <UNK> <UNK> china <UNK> <UNK> south korea <UNK> <UNK> and japan <UNK> <UNK> the top five specialties were radiology <UNK> <UNK> ophthalmology <UNK> <UNK> dermatology <UNK> <UNK> gastroenterology <UNK> <UNK> and histopathology <UNK> <UNK> eighteen <UNK> studies compared how long a task took in ai and human arms in addition to accuracy or performance metrics. funding was predominantly academic <UNK> <UNK> as opposed to commercial <UNK> <UNK> or mixed <UNK> <UNK> twelve studies stated they had no funding and another <UNK> did not report on funding. a detailed table with further information on the <UNK> studies is included as an online supplementary file. in <UNK> of <UNK> studies, a specific comment was included in the abstract about the comparison between ai and clinician performance. ai was described as superior in <UNK> <UNK> comparable or better in <UNK> <UNK> comparable in <UNK> <UNK> able to help a clinician perform better in <UNK> <UNK> and not superior in two <UNK> only nine studies added a caveat in the abstract that further prospective trials were required (this was missing in all <UNK> studies that reported ai was superior to clinician performance). even in the discussion section of the paper, a call for prospective studies (or trials in the case of existing prospective work) was only made in <UNK> of <UNK> <UNK> studies. seven of <UNK> <UNK> studies claimed in the discussion that the algorithm could now be used in clinical practice despite only two of the seven having been tested prospectively in a real world setting. concerning reproducibility, data were public and available in only four studies <UNK> code (for preprocessing of data and modelling) was available in only six studies <UNK> both raw labelled data and code were available in only one <UNK> methods and risk of bias most studies developed and validated a model <UNK> <UNK> compared with development only by using validation through resampling <UNK> <UNK> or validation only <UNK> <UNK> when validation occurred in a separate dataset, this dataset was from a different geographical region in <UNK> of <UNK> <UNK> studies, from a different time period in <UNK> of <UNK> <UNK> and a combination of both in five of <UNK> <UNK> in studies that did not use a separate dataset for validation, the most common method of internal validation was split sample <UNK> followed by cross validation <UNK> and then bootstrapping <UNK> some studies used more than one method (box <UNK> sample size calculations were reported in <UNK> of <UNK> <UNK> studies. dataset sizes were as follows (when reported): training, median <UNK> (interquartile range <UNK> validation, <UNK> <UNK> <UNK> and test, <UNK> <UNK> the median event rate for development, validation, and test sets was <UNK> <UNK> and <UNK> respectively, when a binary outcome was assessed <UNK> as opposed to a multiclass classification <UNK> forty one of <UNK> studies used data augmentation (eg, flipping and inverting images) to increase the dataset size. the human comparator group was generally small and included a median of five clinicians (interquartile range <UNK> range <UNK> of which a median of four were experts (interquartile range <UNK> range <UNK> the number of participating non-experts varied from <UNK> to <UNK> (median <UNK> interquartile range <UNK> experts were used exclusively in <UNK> of <UNK> studies, but in the <UNK> studies that included non-experts, <UNK> had separate performance data available which were exclusive to the expert group. in most studies, every human (expert or non-expert) rated the test dataset independently (blinded to all other clinical information except the image in <UNK> studies). the volume and granularity of the separate data for experts varied considerably among studies, with some reporting individual performance metrics for each human (usually in supplementary appendices). the overall risk of bias assessed using probast led to <UNK> of <UNK> <UNK> studies being classified as high risk (fig <UNK> the analysis domain was most commonly rated to be at high risk of bias (as opposed to participant or outcome ascertainment domains). major deficiencies in the analysis domain related to probast items <UNK> (were there a reasonable number of participants?), <UNK> (were all enrolled participants included in the analysis?), <UNK> (were relevant model performance measures evaluated appropriately?), and <UNK> (were model overfitting and optimism in model performance accounted for?). adherence to reporting standards adherence to reporting standards was poor <UNK> adherence) for <UNK> of <UNK> tripod items (see fig <UNK> overall, publications adhered to between <UNK> and <UNK> of the tripod items: median <UNK> (interquartile range <UNK> eight tripod items were reported in <UNK> or more of the <UNK> studies, and five items in less than <UNK> (fig <UNK> a flowchart for the flow of patients or data through the study was only present in <UNK> of <UNK> <UNK> studies. we also looked for reporting of the hardware that was used for developing or validating the algorithm, although this was not specifically requested in the tripod statement. only <UNK> of <UNK> <UNK> studies reported this information and in most cases <UNK> it related only to the graphics processing unit rather than providing full details (eg, random access memory, central processing unit speed, configuration settings). table <UNK> | randomised trial registrations of deep learning algorithms trial registration title status record last updated country specialty planned sample size intervention control blinding primary outcome anticipated completion <UNK> a colorectal polyps auto-detection system based on deep learning to increase polyp detection rate: a prospective clinical study completed, published <UNK> july <UNK> china gastroenterology <UNK> ai assisted colonoscopy standard colonoscopy none polyp detection rate and adenoma detection rate <UNK> february <UNK> <UNK> comparison of artificial intelligent clinic and normal clinic completed, published <UNK> july <UNK> china ophthalmology <UNK> ai assisted clinic normal clinic double (investigator and outcomes assessor) accuracy for congenital cataracts <UNK> may <UNK> <UNK> breast ultrasound image reviewed with assistance of deep learning algorithms recruiting <UNK> october <UNK> us radiology <UNK> computer aided detection system manual ultrasound imaging review double (participant and investigator) concordance rate <UNK> july <UNK> <UNK> adenoma detection rate using ai system in china not yet recruiting <UNK> february <UNK> china gastroenterology <UNK> csk ai system assisted colonoscopy standard colonoscopy none adenoma detection rate <UNK> march <UNK> <UNK> computer-aided detection for colonoscopy not yet recruiting <UNK> february <UNK> taiwan gastroenterology <UNK> computer aided detection standard colonoscopy double (participant, care provider) adenoma detection rate <UNK> december <UNK> <UNK> the impact of a computer aided diagnosis system based on deep learning on increasing polyp detection rate during colonoscopy, a prospective double blind study not yet recruiting <UNK> february <UNK> china gastroenterology <UNK> ai assisted colonoscopy standard colonoscopy double polyp detection rate and adenoma detection rate <UNK> january <UNK> <UNK> a multicenter randomised controlled study for evaluating the effectiveness of artificial intelligence in improving colonoscopy quality recruiting <UNK> march <UNK> china gastroenterology <UNK> endoangel assisted colonoscopy colonoscopy double (participants and evaluators) polyp detection rate <UNK> december <UNK> <UNK> development and validation of a deep learning algorithm for bowel preparation quality scoring not yet recruiting <UNK> april <UNK> china gastroenterology <UNK> ai assisted scoring group conventional human scoring group single (outcome assessor) adequate bowel preparation <UNK> april <UNK> <UNK> quality measurement of esophagogastroduodenoscopy using deep learning models recruiting <UNK> april <UNK> china gastroenterology <UNK> dcnn model assisted egd conventional egd double (participant, care provider) detection of upper gastrointestinal lesions <UNK> may <UNK> <UNK> prospective clinical study for artificial intelligence platform for lymph node pathology detection of gastric cancer not yet recruiting <UNK> may <UNK> china gastroenterology <UNK> pathological diagnosis of artificial intelligence traditional pathological diagnosis not stated clinical prognosis <UNK> august <UNK> ai=artificial intelligence; csk=commonsense knowledge; dcnn=deep convolutional neural network; egd=esophagogastroduodenoscopy. table <UNK> | characteristics of non-randomised studies lead author year country study type specialty disease outcome caveat in discussion* suggestion in discussion† abramoff <UNK> us prospective real world ophthalmology diabetic retinopathy more than mild diabetic retinopathy no yes arbabshirani <UNK> us prospective real world radiology intracranial haemorrhage haemorrhage yes no arji <UNK> japan retrospective radiology oral cancer cervical lymph node metastases no no becker <UNK> switzerland retrospective radiology breast cancer bi-rads category <UNK> yes no becker <UNK> switzerland retrospective radiology breast cancer bi-rads category <UNK> yes no bien <UNK> us retrospective radiology knee injuries abnormality on mri no no brinker <UNK> germany retrospective dermatology skin cancer melanoma no no brinker <UNK> germany retrospective dermatology skin cancer melanoma yes no brown <UNK> us retrospective ophthalmology retinopathy of prematurity plus disease no no burlina <UNK> us retrospective ophthalmology macular degeneration armd stage no no burlina <UNK> us retrospective ophthalmology macular degeneration intermediate or advanced stage armd no no burlina <UNK> us retrospective ophthalmology macular degeneration armd stage no no bychov <UNK> finland retrospective histopathology colorectal cancer low or high risk for <UNK> year survival no no byra <UNK> us retrospective radiology breast cancer bi-rads category <UNK> or more no no cha <UNK> us retrospective radiology bladder cancer <UNK> status post chemotherapy no no cha <UNK> south korea retrospective radiology lung cancer nodule operability yes no chee <UNK> south korea retrospective radiology osteonecrosis of the femoral head stage of osteonecrosis yes no chen <UNK> taiwan prospective gastroenterology colorectal cancer neoplastic polyp no no choi <UNK> south korea retrospective radiology liver fibrosis fibrosis stage no no choi <UNK> south korea retrospective radiology breast cancer malignancy no no chung <UNK> south korea retrospective orthopaedics humerus fractures proximal humerus fracture yes no ciompi <UNK> netherlands/italy retrospective radiology lung cancer nodule type no no ciritsis <UNK> switzerland retrospective radiology breast cancer bi-rads stage no no de fauw <UNK> uk retrospective ophthalmology retinopathy diagnosis and referral decision yes no ehtesham bejnordii <UNK> netherlands retrospective histopathology breast cancer metastases yes no esteva <UNK> us retrospective dermatology skin cancer lesion type yes no fujioka <UNK> japan retrospective radiology breast cancer bi-rads malignancy no no fujisawa <UNK> japan retrospective dermatology skin cancer malignancy classification yes no gan <UNK> china retrospective orthopaedics wrist fractures fracture yes no gulshan <UNK> india prospective real world ophthalmology retinopathy moderate or worse diabetic retinopathy or referable macula oedema yes no haenssle <UNK> germany retrospective dermatology skin cancer malignancy classification and management decision yes no hamm <UNK> us retrospective radiology liver cancer li-rads category no no han <UNK> south korea retrospective dermatology skin cancer cancer type no no han <UNK> south korea retrospective dermatology onchomycosis onchomycosis diagnosis no no hannun <UNK> us retrospective cardiology arrhythmia arrhythmia classification no no he <UNK> china retrospective radiology bone cancer recurrence of giant cell tumour no no hwang <UNK> taiwan retrospective ophthalmology macular degeneration classification and type of armd no yes hwang <UNK> south korea retrospective radiology tuberculosis tb presence yes no hwang <UNK> south korea retrospective radiology pulmonary pathology abnormal chest radiograph yes no kim <UNK> south korea retrospective radiology sinusitis maxillary sinusitis label no no kise <UNK> japan retrospective radiology sjogren’s syndrome sjogren’s syndrome presence no no kooi <UNK> netherlands retrospective radiology breast cancer classification of mammogram no no krause <UNK> us retrospective ophthalmology diabetic retinopathy diabetic retinopathy stage no no kuo <UNK> taiwan retrospective nephrology chronic kidney disease <UNK> <UNK> no yes lee <UNK> us prospective radiology intracranial haemorrhage haemorrhage yes no li <UNK> china prospective oncology nasopharyngeal cancer malignancy no yes li <UNK> china retrospective ophthalmology glaucoma glaucoma no no li <UNK> china retrospective radiology thyroid cancer malignancy yes no armd=age related macular degeneration; bi-rads=breast imaging reporting and data system; egfr=estimated glomerular filtration rate; li-rads=liver imaging reporting and data system; mri=magnetic resonance imaging; tb=tuberculosis. *caveat mentioned in discussion about need for further prospective work or trials. †suggestion in discussion that algorithm can now be used clinically. table <UNK> | characteristics of non-randomised studies lead author year country study type specialty disease outcome caveat in discussion* suggestion in discussion† long <UNK> china prospective real world ophthalmology congenital cataracts detection of congenital cataracts no no lu <UNK> china retrospective ophthalmology macular pathologies classification of macular pathology no no marchetti <UNK> us retrospective dermatology skin cancer malignancy (melanoma) yes no matsuba <UNK> japan retrospective ophthalmology macular degeneration wet amd no no mori <UNK> japan prospective real world gastroenterology polyps neoplastic polyp yes yes nagpal <UNK> us retrospective histopathology prostate cancer gleason score no no nakagawa <UNK> japan retrospective gastroenterology oesophageal cancer cancer invasion depth stage <UNK> no no nam <UNK> south korea retrospective radiology pulmonary nodules classification and localisation of nodule yes no nirschl <UNK> us retrospective histopathology heart failure heart failure (pathologically) no yes olczak <UNK> sweden retrospective orthopaedics fractures fracture no yes park <UNK> us retrospective radiology cerebral aneurysm aneurysm presence yes no poedjiastoeti <UNK> thailand retrospective oncology jaw tumours malignancy no no rajpurkar <UNK> us retrospective radiology pulmonary pathology classification of chest radiograph pathology yes no raumviboonsuk <UNK> thailand prospective real world ophthalmology diabetic retinopathy moderate or worse diabetic retinopathy yes no rodriguez-ruiz <UNK> netherlands retrospective radiology breast cancer classification of mammogram yes no sayres <UNK> us retrospective ophthalmology diabetic retinopathy moderate or worse non-proliferative diabetic retinopathy no no shichijo <UNK> japan retrospective gastroenterology gastritis helicobacter pylori gastritis no no singh <UNK> us retrospective radiology pulmonary pathology chest radiograph abnormality no no steiner <UNK> us retrospective histopathology breast cancer metastases yes no ting <UNK> singapore retrospective ophthalmology retinopathy, glaucoma, macular degeneration referable pathology for retinopathy, glaucoma, macular degeneration yes no urakawa <UNK> japan retrospective orthopaedics hip fractures intertrochanteric hip fracture no no van grinsven <UNK> netherlands retrospective ophthalmology fundal haemorrhage fundal haemorrhage no no walsh <UNK> uk/italy retrospective radiology fibrotic lung disease fibrotic lung disease no no wang <UNK> china retrospective radiology thyroid nodule nodule presence yes no wang <UNK> china retrospective radiology lung cancer invasive or preinvasive adenocarcinoma nodule no no wu <UNK> us retrospective radiology bladder cancer <UNK> response to chemotherapy no no xue <UNK> china retrospective orthopaedics hip osteoarthritis radiograph presence of hip osteoarthritis no no ye <UNK> china retrospective radiology intracranial haemorrhage presence of intracranial haemorrhage yes no yu <UNK> south korea retrospective dermatology skin cancer malignancy (melanoma) no no zhang <UNK> china retrospective radiology pulmonary nodules presence of a malignant nodule yes no zhao <UNK> china retrospective radiology lung cancer classification of nodule invasiveness no no zhu <UNK> china retrospective gastroenterology gastric cancer tumour invasion depth (deeper than <UNK> no no zucker <UNK> us retrospective radiology cystic fibrosis brasfield score yes no amd=age related macular degeneration. *caveat mentioned in discussion about need for further prospective work or trials. †suggestion in discussion that algorithm can now be used clinically. discussion we have conducted an appraisal of the methods, adherence to reporting standards, risk of bias, and claims of deep learning studies that compare diagnostic ai performance with human clinicians. the rapidly advancing nature and commercial drive of this field has created pressure to introduce ai algorithms into clinical practice as quickly as possible. the potential consequences for patients of this implementation without a rigorous evidence base make our findings timely and should guide efforts to improve the design, reporting, transparency, and nuanced conclusions of deep learning <UNK> <UNK> principal findings five key findings were established from our review. firstly, we found few relevant randomised clinical trials (ongoing or completed) of deep learning in medical imaging. while time is required to move from development to validation to prospective feasibility testing before conducting a trial, this means that claims about performance against clinicians should be tempered accordingly. however, deep learning only became mainstream in <UNK> giving a lead time of approximately five years for testing within clinical environments, and prospective studies could take a minimum of one to two years to conduct. therefore, it is reasonable to assume that many similar trials will be forthcoming over the next decade. we found only one randomised trial registered in the us despite at least <UNK> deep learning algorithms for medical imaging approved for marketing by the food and drug administration (fda). these algorithms cover a range of fields from radiology to ophthalmology and <UNK> <UNK> secondly, of the non-randomised studies, only nine were prospective and just six were tested in a real world clinical environment. comparisons of ai performance against human clinicians are therefore difficult to evaluate given the artificial in silico context in which clinicians are being evaluated. in much the same way that surrogate endpoints do not always reflect clinical <UNK> a higher area under the curve might not lead to clinical benefit and could even have unintended adverse effects. such effects could include an unacceptably high false positive rate, which is not apparent from an in silico evaluation. yet it is typically retrospective studies that are usually cited in fda approval notices for marketing of algorithms. currently, the fda do not mandate peer reviewed publication of these studies; instead internal review alone is <UNK> <UNK> however, the fda has recognised and acknowledged that their traditional paradigm of medical device regulation was not designed for adaptive ai and machine learning technologies. non-inferior ai (rather than superior) performance that allows for a lower burden on clinician workflow (that is, being quicker with similar accuracy) might warrant further investigation. however, less than a quarter of studies reported time taken for task completion in both the ai and human groups. ensuring fair comparison between ai and clinicians is arguably done best in a randomised clinical trial (or at the very least prospective) setting. however, it should be noted that prospective testing is not necessary to actually develop the model in the first place. even in a randomised clinical trial setting, ensuring that functional robustness tests are present is crucial. for example, does the algorithm produce the correct decision for normal anatomical variants and is the decision independent of the camera or imaging software used? thirdly, limited availability of datasets and code makes it difficult to assess the reproducibility of deep learning research. descriptions of the hardware used, when present, were also brief and this vagueness might affect external validity and implementation. reproducible research has become a pressing issue across many scientific disciplines and efforts to encourage data and code sharing are <UNK> even when commercial concerns exist about intellectual property, strong arguments exist for ensuring that algorithms are non-proprietary and available for <UNK> commercial companies could collaborate with non-profit third parties for independent prospective validation. fourthly, the number of humans in the comparator group was typically small with a median of only four experts. there can be wide intra and inter case variation even between expert clinicians. therefore, an appropriately large human sample for comparison is essential for ensuring reliability. inclusion of nonexperts can dilute the average human performance and potentially make the ai algorithm look better than it otherwise might. if the algorithm is designed specifically to aid performance of more junior clinicians or non-specialists rather than experts, then this should be made clear. box <UNK> specific terms • internal validation: evaluation of model performance with data used in development process • external validation: evaluation of model performance with separate data not used in development process • cross validation: internal validation approach in which data are randomly split into n equally sized groups; the model is developed in <UNK> of n groups, and performance evaluated in the remaining group with the whole process repeated n times; model performance is taken as average over n iterations • bootstrapping: internal validation approach similar to cross validation but relying on random sampling with replacement; each sample is the same size as model development dataset • split sample: internal validation approach in which the available development dataset is divided into two datasets: one to develop the model and the other to validate the model; division can be random or non-random. fifthly, descriptive phrases that suggested at least comparable (or better) diagnostic performance of an algorithm to a clinician were found in most abstracts, despite studies having overt limitations in design, reporting, transparency, and risk of bias. caveats about the need for further prospective testing were rarely mentioned in the abstract (and not at all in the <UNK> studies that claimed superior performance to a clinician). accepting that abstracts are usually word limited, even in the discussion sections of the main text, nearly two thirds of studies failed to make an explicit recommendation for further prospective studies or trials. one retrospective study gave a website address in the abstract for patients to upload their eye scans and use the algorithm <UNK> overpromising language leaves studies vulnerable to being misinterpreted by the media and the public. although it is clearly beyond the power of authors to control how the media and public interpret their findings, judicious and responsible use of language in studies and press releases that factor in the strength and quality of the evidence can <UNK> this issue is especially concerning given the findings from new research that suggests patients are more likely to consider a treatment beneficial when news stories are reported with spin, and that false news spreads much faster online than true <UNK> <UNK> policy implications the impetus for guiding best practice has gathered pace in the last year with the publication of a report that proposes a framework for developing transparent, replicable, ethical, and effective research in healthcare ai <UNK> this endeavour is led by a multidisciplinary team of clinicians, methodologists, statisticians, data scientists, and healthcare policy makers. the guiding questions of this framework will probably feed into the creation of more specific reporting standards such as a tripod extension for machine learning <UNK> key to the success of these efforts will be high visibility to researchers and possibly some degree of enforcement by journals in a similar vein to preregistering randomised trials and reporting them according to the consort <UNK> <UNK> enthusiasm exists to speed up the process by which medical devices that feature ai are approved for <UNK> <UNK> better design and more transparent reporting should be seen eventually as a facilitator of the innovation, validation, and translation process, and could help avoid hype. study limitations our findings must be considered in light of several limitations. firstly, although comprehensive, our search might have missed some studies that could have been included. secondly, the guidelines that we used to assess non-randomised studies (tripod and probast) were designed for conventional prediction modelling studies, and so the adherence levels we found should be interpreted in this context. thirdly, we focused specifically on deep learning for diagnostic medical imaging. therefore, it might not be appropriate risk of bias percentage participants outcomes analysis overall high unclear low <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> fig <UNK> | probast (prediction model risk of bias assessment tool) risk of bias assessment for non-randomised studies adherence (%) title abstract introduction - context introduction - objectives methods - study design methods - study dates methods - study setting methods - eligibility criteria methods - outcome predicted methods - blinding methods - sample size methods - missing data methods - model building methods - validation predictions methods - model performance methods - model updating methods - data differences results - flow of data results - characteristics results - validation results - numbers results - model performance results - model updating discussion - limitations discussion - development v validation discussion - interpretation discussion - clinical use supplementary data funding <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> fig <UNK> | completeness of reporting of individual tripod (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) items for non-randomised studies to generalise our findings to other types of ai, such as conventional machine learning (eg, an artificial neural network based mortality prediction model that uses electronic health record data). similar issues could exist in many other types of ai paper, however we cannot definitively make this claim from our findings because we only assessed medical imaging studies. moreover, nomenclature in the field is sometimes used in non-standardised ways, and thus some potentially eligible studies might have been presented with terminology that did not lead to them being captured with our search strategy. fourthly, risk of bias entails some subjective judgment and people with different experiences of ai performance could have varying perceptions. conclusions deep learning ai is an innovative and fast moving field with the potential to improve clinical outcomes. financial investment is pouring in, global media coverage is widespread, and in some cases algorithms are already at marketing and public adoption stage. however, at present, many arguably exaggerated claims exist about equivalence with or superiority over clinicians, which presents a risk for patient safety and population health at the societal level, with ai algorithms applied in some cases to millions of patients. overpromising language could mean that some studies might inadvertently mislead the media and the public, and potentially lead to the provision of inappropriate care that does not align with patients’ best interests. the development of a higher quality and more transparently reported evidence base moving forward will help to avoid hype, diminish research waste, and protect patients.

<|EndOfText|>

abstract prediction models are developed to aid health care providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision making. however, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes. this article describes how the tripod statement was developed. an extensive list of items based on a review of the literature was created, which was reduced after a web-based survey and revised during a <UNK> meeting in june <UNK> with methodologists, health care professionals, and journal editors. the list was refined during several meetings of the steering group and in e-mail discussions with the wider group of tripod contributors. the resulting tripod statement is a checklist of <UNK> items, deemed essential for transparent reporting of a prediction model study. the tripod statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. the tripod statement is best used in conjunction with the tripod explanation and elaboration document. to aid the editorial process and readers of prediction model studies, it is recommended that authors include a completed checklist in their submission (also available at www.tripod-statement.org). editors’ note: in order to encourage dissemination of the tripod statement, this article is freely accessible on the annals of internal medicine web site (www.annals.org) and will be also published in bjog, british journal of cancer, british journal of surgery, bmc medicine, british medical journal, circulation, diabetic medicine, european journal of clinical investigation, european urology, and journal of clinical epidemiology. the authors jointly hold the copyright of this article. an accompanying explanation and elaboration article is freely available only on www.annals.org; annals of internal medicine holds copyright for that article. background in medicine, patients with their care providers are confronted with making numerous decisions on the basis of an estimated risk or probability that a specific disease or condition is present (diagnostic setting) or a specific event will occur in the future (prognostic setting) (figure <UNK> in the diagnostic setting, the probability that a particular disease is present can be used, for example, to inform the referral of patients for further testing, initiate treatment directly, or reassure patients that a serious cause for their symptoms is unlikely. in the prognostic setting, predictions can be used for planning lifestyle or therapeutic decisions based on the risk for developing a particular outcome or state of health within a specific period <UNK> such estimates of risk can also be used to risk-stratify participants in therapeutic clinical trials <UNK> figure <UNK> <UNK> schematic representation of diagnostic and prognostic prediction modeling studies. the nature of the prediction in diagnosis is estimating the probability that a specific outcome or disease is present (or absent) within an individual, at this point in time—that is, the moment of prediction (t = <UNK> in prognosis, the prediction is about whether an individual will experience a specific event or outcome within a certain time period. in other words, in diagnostic prediction the interest is in principle a cross-sectional relationship, whereas prognostic prediction involves a longitudinal relationship. nevertheless, in diagnostic modeling studies, for logistical reasons, a time window between predictor (index test) measurement and the reference standard is often necessary. ideally, this interval should be as short as possible and without starting any treatment within this period. full size image in both the diagnostic and prognostic setting, estimates of probabilities are rarely based on a single predictor <UNK> doctors naturally integrate several patient characteristics and symptoms (predictors, test results) to make a prediction (see figure <UNK> for differences in common terminology between diagnostic and prognostic studies). prediction is therefore inherently multivariable. prediction models (also commonly called “prognostic models,” “risk scores,” or “prediction rules” <UNK> are tools that combine multiple predictors by assigning relative weights to each predictor to obtain a risk or probability <UNK> well-known prediction models include the framingham risk score <UNK> ottawa ankle rules <UNK> euroscore <UNK> nottingham prognostic index <UNK> and the simplified acute physiology score <UNK> figure <UNK> <UNK> similarities and differences between diagnostic and prognostic prediction models. full size image prediction model studies prediction model studies can be broadly categorized as model development <UNK> model validation (with or without updating) <UNK> or a combination of both (figure <UNK> model development studies aim to derive a prediction model by selecting the relevant predictors and combining them statistically into a multivariable model. logistic and cox regression are most frequently used for short-term (for example, disease absent vs. present, <UNK> mortality) and long-term (for example, <UNK> risk) outcomes, respectively <UNK> studies may also focus on quantifying the incremental or added predictive value of a specific predictor (for example, newly discovered) to a prediction model <UNK> figure <UNK> <UNK> types of prediction model studies covered by the tripod statement. d = development data; v = validation data. full size image quantifying the predictive ability of a model on the same data from which the model was developed (often referred to as apparent performance) will tend to give an optimistic estimate of performance, owing to overfitting (too few outcome events relative to the number of candidate predictors) and the use of predictor selection strategies <UNK> studies developing new prediction models should therefore always include some form of internal validation to quantify any optimism in the predictive performance (for example, calibration and discrimination) of the developed model. internal validation techniques use only the original study sample and include such methods as bootstrapping or cross-validation. internal validation is a necessary part of model development <UNK> overfitting, optimism, and miscalibration may also be addressed and accounted for during the model development by applying shrinkage (for example, heuristic or based on bootstrapping techniques) or penalization procedures (for example, ridge regression or lasso) <UNK> after developing a prediction model, it is strongly recommended to evaluate the performance of the model in other participant data than was used for the model development. such external validation requires that for each individual in the new data set, outcome predictions are made using the original model (that is, the published regression formula) and compared with the observed outcomes <UNK> external validation may use participant data collected by the same investigators, typically using the same predictor and outcome definitions and measurements, but sampled from a later period (temporal or narrow validation); by other investigators in another hospital or country, sometimes using different definitions and measurements (geographic or broad validation); in similar participants but from an intentionally different setting (for example, model developed in secondary care and assessed in similar participants but selected from primary care); or even in other types of participants (for example, model developed in adults and assessed in children, or developed for predicting fatal events and assessed for predicting nonfatal events) <UNK> in case of poor performance, the model can be updated or adjusted on the basis of the validation data set <UNK> reporting of multivariable prediction model studies studies developing or validating a multivariable prediction model share specific challenges for researchers <UNK> several reviews have evaluated the quality of published reports that describe the development or validation prediction models <UNK> for example, mallett and colleagues <UNK> examined <UNK> reports published in <UNK> presenting new prediction models in cancer. reporting was found to be poor, with insufficient information described in all aspects of model development, from descriptions of patient data to statistical modeling methods. collins and colleagues <UNK> evaluated the methodological conduct and reporting of <UNK> reports published before may <UNK> describing the development of models to predict prevalent or incident type <UNK> diabetes. reporting was also found to be generally poor, with key details on which predictors were examined, the handling and reporting of missing data, and model-building strategy often poorly described. bouwmeester and colleagues <UNK> evaluated <UNK> reports, published in <UNK> in <UNK> high-impact general medical journals, and likewise observed an overwhelmingly poor level of reporting. these and other reviews provide a clear picture that, across different disease areas and different journals, there is a generally poor level of reporting of prediction model studies <UNK> furthermore, these reviews have shown that serious deficiencies in the statistical methods, use of small data sets, inappropriate handling of missing data, and lack of validation are common <UNK> such deficiencies ultimately lead to prediction models that are not or should not be used. it is therefore not surprising, and fortunate, that very few prediction models, relative to the large number of models published, are widely implemented or used in clinical practice <UNK> prediction models in medicine have proliferated in recent years. health care providers and policy makers are increasingly recommending the use of prediction models within clinical practice guidelines to inform decision making at various stages in the clinical pathway <UNK> it is a general requirement of reporting of research that other researchers can, if required, replicate all the steps taken and obtain the same results <UNK> it is therefore essential that key details of how a prediction model was developed and validated be clearly reported to enable synthesis and critical appraisal of all relevant information <UNK> reporting guidelines for prediction model studies: the tripod statement we describe the development of the tripod (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) statement, a guideline specifically designed for the reporting of studies developing or validating a multivariable prediction model, whether for diagnostic or prognostic purposes. tripod is not intended for multivariable modeling in etiologic studies or for studies investigating single prognostic factors <UNK> furthermore, tripod is also not intended for impact studies that quantify the impact of using a prediction model on participant or doctors’ behavior and management, participant health outcomes, or cost-effectiveness of care, compared with not using the model <UNK> reporting guidelines for observational (the strengthening the reporting of observational studies in epidemiology [strobe]) <UNK> tumor marker (reporting recommendations for tumour marker prognostic studies [remark]) <UNK> diagnostic accuracy (standards for the reporting of diagnostic accuracy studies [stard]) <UNK> and genetic risk prediction (genetic risk prediction studies [grips]) <UNK> studies all contain many items that are relevant to studies developing or validating prediction models. however, none of these guidelines are entirely appropriate for prediction model studies. the <UNK> guidelines most closely related to prediction models are remark and grips. however, the focus of the remark checklist is primarily on prognostic factors and not prediction models, whereas the grips statement is aimed at risk prediction using genetic risk factors and the specific methodological issues around handling large numbers of genetic variants. to address a broader range of studies, we developed the tripod guideline: transparent reporting of a multivariable prediction model for individual prognosis or diagnosis. tripod explicitly covers the development and validation of prediction models for both diagnosis and prognosis, for all medical domains and all types of predictors. tripod also places much more emphasis on validation studies and the reporting requirements for such studies. the reporting of studies evaluating the incremental value of specific predictors, beyond established predictors or even beyond existing prediction models <UNK> also fits entirely within the remit of tripod (see the accompanying explanation and elaboration document <UNK> developing the tripod statement we convened a <UNK> meeting with an international group of prediction model researchers, including statisticians, epidemiologists, methodologists, health care professionals, and journal editors (from annals of internal medicine, bmj, journal of clinical epidemiology, and plos medicine) to develop recommendations for the tripod statement. we followed published guidance for developing reporting guidelines <UNK> and established a steering committee (drs. collins, reitsma, altman, and moons) to organize and coordinate the development of tripod. we conducted a systematic search of medline, embase, psychinfo, and web of science to identify any published articles making recommendations on reporting of multivariable prediction models (or aspects of developing or validating a prediction model), reviews of published reports of multivariable prediction models that evaluated methodological conduct or reporting and reviews of methodological conduct and reporting of multivariable models in general. from these studies, a list of <UNK> possible checklist items was generated. the steering committee then merged related items to create a list of <UNK> candidate items. twenty-five experts with a specific interest in prediction models were invited by e-mail to participate in the web-based survey and to rate the importance of the <UNK> candidate checklist items. respondents <UNK> of <UNK> included methodologists, health care professionals, and journal editors. (in addition to the <UNK> meeting participants, the survey was also completed by <UNK> statistical editors from annals of internal medicine). the results of the survey were presented at a <UNK> meeting in june <UNK> in oxford, united kingdom; it was attended by <UNK> of the <UNK> invited participants <UNK> of whom had participated in the survey). during the <UNK> meeting, each of the <UNK> candidate checklist items was discussed in turn, and a consensus was reached on whether to retain, merge with another item, or omit the item. meeting participants were also asked to suggest additional items. after the meeting, the checklist was revised by the steering committee during numerous face-to-face meetings, and circulated to the participants to ensure it reflected the discussions. while making revisions, conscious efforts were made to harmonize our recommendations with other reporting guidelines, and where possible we chose the same or similar wording for items <UNK> tripod components the tripod statement is a checklist of <UNK> items that we consider essential for good reporting of studies developing or validating multivariable prediction models (table <UNK> the items relate to the title and abstract (items <UNK> and <UNK> background and objectives (item <UNK> methods (items <UNK> through <UNK> results (items <UNK> through <UNK> discussion (items <UNK> through <UNK> and other information (items <UNK> and <UNK> the tripod statement covers studies that report solely development <UNK> both development and external validation, and solely external validation (with or without updating), of a prediction model <UNK> (figure <UNK> therefore, some items are relevant only for studies reporting the development of a prediction model (items <UNK> <UNK> <UNK> and <UNK> and others apply only to studies reporting the (external) validation of a prediction model (items <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> all other items are relevant to all types of prediction model development and validation studies. items relevant only to the development of a prediction model are denoted by d, items relating solely to a validation of a prediction model are denoted by v, whereas items relating to both types of study are denoted d;v. table <UNK> checklist of items to include when reporting a study developing or validating a multivariable prediction model for diagnosis or prognosis* full size table the recommendations within tripod are guidelines only for reporting research and do not prescribe how to develop or validate a prediction model. furthermore, the checklist is not a quality assessment tool to gauge the quality of a multivariable prediction model. an ever-increasing number of studies are evaluating the incremental value of specific predictors, beyond established predictors or even beyond existing prediction models <UNK> reporting of these studies fits entirely within the remit of tripod (see accompanying explanation and elaboration document <UNK> the tripod explanation and elaboration document in addition to the tripod statement, we produced a supporting explanation and elaboration document <UNK> in a similar style to those for other reporting guidelines <UNK> each checklist item is explained and accompanied by examples of good reporting from published articles. in addition, because many such studies are methodologically weak, we also summarize the qualities of good (and the limitations of less good) studies, regardless of reporting <UNK> a comprehensive evidence base from existing systematic reviews of prediction models was used to support and justify the rationale for including and illustrating each checklist item. the development of the explanation and elaboration document was completed after several face-to-face meetings, teleconferences, and iterations among the authors. additional revisions were made after sharing the document with the whole tripod group before final approval. role of the funding source there was no explicit funding for the development of this checklist and guidance document. the consensus meeting in june <UNK> was partially funded by a national institute for health research senior investigator award held by dr. altman, cancer research uk, and the netherlands organization for scientific research. drs. collins and altman are funded in part by the medical research council. dr. altman is a member of the medical research council prognosis research strategy (progress) partnership. the funding sources had no role in the study design, data collection, analysis, preparation of the manuscript, or decision to submit the manuscript for publication. discussion many reviews have showed that the quality of reporting in published articles describing the development or validation of multivariable prediction models in medicine is poor <UNK> in the absence of detailed and transparent reporting of the key study details, it is difficult for the scientific and health care community to objectively judge the strengths and weaknesses of a prediction model study <UNK> the explicit aim of this checklist is to improve the quality of reporting of published prediction model studies. the tripod guideline has been developed to support authors in writing reports describing the development, validation or updating of prediction models, aid editors and peer reviewers in reviewing manuscripts submitted for publication, and help readers in critically appraising published reports. the tripod statement does not prescribe how studies developing, validating, or updating prediction models should be undertaken, nor should it be used as a tool for explicitly assessing quality or quantifying risk of bias in such studies <UNK> there is, however, an implicit expectation that authors have an appropriate study design and conducted certain analyses to ensure all aspects of model development and validation are reported. the accompanying explanation and elaboration document describes aspects of good practice for such studies, as well as highlighting some inappropriate approaches that should be avoided <UNK> tripod encourages complete and transparent reporting reflecting study design and conduct. it is a minimum set of information that authors should report to inform the reader about how the study was carried out. we are not suggesting a standardized structure of reporting, rather that authors should ensure that they address all the checklist items somewhere in their article with sufficient detail and clarity. we encourage researchers to develop a study protocol, especially for model development studies, and even register their study in registers that accommodate observational studies (such as clinicaltrials.gov) <UNK> the importance of also publishing protocols for developing or validating prediction models, certainly when conducting a prospective study, is slowly being acknowledged <UNK> authors can also include the study protocol when submitting their article for peer review, so that readers can know the rationale for including individuals into the study or whether all of the analyses were prespecified. to help the editorial process; peer reviewers; and, ultimately, readers, we recommend submitting the checklist as an additional file with the report, indicating the pages where information for each item is reported. the tripod reporting template for the checklist can be downloaded from <UNK> announcements and information relating to tripod will be broadcast on the tripod twitter address (@tripodstatement). the enhancing the quality and transparency of health research (equator) network <UNK> will help disseminate and promote the tripod statement. methodological issues in developing, validating, and updating prediction models evolve. tripod will be periodically reappraised, and if necessary modified to reflect comments, criticisms, and any new evidence. we therefore encourage readers to make suggestions for future updates so that ultimately, the quality of prediction model studies will improve.

<|EndOfText|>

statistical issues in the development of <UNK> prediction models to the editor, clinical prediction models to aid diagnosis, assess disease severity, or prognosis have enormous potential to aid clinical decision making during the coronavirus disease <UNK> <UNK> pandemic. a living systematic review has, so far, identified <UNK> <UNK> prediction models published (or preprinted) between <UNK> january and <UNK> may <UNK> despite the considerable interest in developing <UNK> prediction models, the review concluded that all models to date, with no exception, are at high risk of bias with concerns related to data quality, flaws in the statistical analysis, and poor reporting, and none are recommended for <UNK> disappointingly, the recent study by yang et <UNK> describing the development of a prediction model to identify <UNK> patients with severe disease is no different. the study has failed to report important information needed to judge the study findings, but numerous methodological problems are <UNK> our first point relates to the sample size. the sample size requirements in a prediction model study are largely influenced by the number of individuals experiencing the event to be predicted (in yang's study, those with mild <UNK> disease, as this is the smaller of the two outcome categories). using published sample size formulae for developing prediction <UNK> based on information reported in the yang study <UNK> predictors, outcome prevalence of <UNK> the minimum sample size in the most optimistic scenario would be <UNK> individuals <UNK> events). to precisely estimate the intercept alone requires <UNK> individuals <UNK> events). the study by yang included <UNK> individuals, where <UNK> had the outcome of mild disease, substantially lower than required. developing a prediction model with a small sample size and a large number of predictors will result in a model that is overfit, including unimportant or spurious predictors, and overestimating the regression coefficients. this means that the model will appear to fit the data (used in its development) too well—leading to a model that has poor predictive accuracy in new data. an important step in all model development studies is to carry out an internal validation of the model building process (using either bootstrapping or cross‐ validation), whereby the overestimation in regression coefficients can be determined and shrunk as well as estimating the optimism in model <UNK> this important step is absent in the study of yang, who reported an area under the curve of <UNK> in the same data used to develop their model—this will almost certainly be substantially overestimated. another concern is the actual model. the final model contains seven predictors and the authors have fully reported this permitting individualized prediction. however, an obvious and major concern is the regression coefficient reported for procalcitonin, with a value of <UNK> and accompanying odds ratio with a confidence interval of <UNK> <UNK> <UNK> (sic). this is clearly nonsensical, and to put it bluntly, makes the model unusable. the reason for the large regression value (standard error and confidence interval) is due to an issue called separation. <UNK> this occurred because there was little or no overlap in the procalcitonin values between individuals with mild and severe disease. the statistical software used by the authors, sas, will report odds ratios as greater than <UNK> when this occurs. instead of retaining this in the model as is, one preferred approach would be to use firth's correction, available in both sas and <UNK> the authors used the model to develop an early warning score—this score has not been presented by the authors—and we caution against such an approach with a preference for alternative formats that permit estimation of absolute <UNK> other concerns include the handling of missing data. while the authors mention discarding observed values with more than <UNK> missing—it is unclear whether individuals were omitted, or whether entire predictors were omitted. regardless, one can only assume a complete‐case analysis was conducted in preference for more suitable approaches using multiple <UNK> finally, we note the use of univariate screening, whereby predictors are omitted based on the lack of statistical association. this approach is largely discredited, as predictors can be spuriously retained or <UNK> we urge the authors and other investigators developing <UNK> prediction models to read the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) statement (www.tripod-statement.org) for key information to report when describing their study so that readers have the minimal information required to judge the quality of the <UNK> the accompanying tripod explanation and elaboration paper describes the rationale of the importance of transparent reporting, examples of good reporting, but also discusses methodological <UNK> until improved methodological standards are adopted, we should not expect prediction models to benefit patients, and should consider the possibility that they might do more harm than good.

<|EndOfText|>

risk factors for the progression of fnger interphalangeal joint osteoarthritis: a systematic review abstract progressive hand interphalangeal joint (ipj) osteoarthritis is associated with pain, reduced function and impaired quality of life. however, the evidence surrounding risk factors for ipj osteoarthritis progression is unclear. identifying risk factors for ipj osteoarthritis progression may inform preventative strategies and early interventions to improve long-term outcomes for individuals at risk of ipj osteoarthritis progression. the objectives of the study were to describe methods used to measure the progression of ipj osteoarthritis and identify risk factors for ipj osteoarthritis progression. medline, embase, scopus, and the cochrane library were searched from inception to <UNK> february <UNK> (prospero <UNK> eligible studies assessed potential risk factor/s associated with ipj osteoarthritis progression. risk of bias was assessed using a modifed quips tool, and a best evidence synthesis was performed. of eight eligible studies, all measured osteoarthritis progression radiographically, and none considered symptoms. eighteen potential risk factors were assessed. diabetes (adjusted mean diference between <UNK> and <UNK> and larger fnger epiphyseal index in males (regression coefcient <UNK> and females <UNK> were identifed as risk factors (limited evidence). older age in men and women showed mixed results; <UNK> variables were not risk factors (all limited evidence). patients with diabetes and larger fnger epiphyseal index might be at higher risk of radiographic ipj osteoarthritis progression, though evidence is limited and studies are biased. studies assessing symptomatic ipj osteoarthritis progression are lacking. keywords hand interphalangeal joint · osteoarthritis · risk factors · disease progression introduction osteoarthritis is one of the leading causes of worldwide disability <UNK> and, in the usa alone, carries a cost of <UNK> billion just from economic loss <UNK> hand osteoarthritis is one of the most common types of radiographic osteoarthritis <UNK> hand osteoarthritis also presents in a younger population than osteoarthritis at other joints, with a prevalence of <UNK> in men and <UNK> in women aged <UNK> years <UNK> <UNK> it is considered a chronic disease, with some cases progressing and the prevalence increasing to <UNK> in men and <UNK> in women aged <UNK> years <UNK> symptomatic treatment for progressive hand osteoarthritis is limited, with patients often requiring surgical management, such as arthrodesis or arthroplasty <UNK> measuring progressive hand osteoarthritis is difcult, with no consensus for defning or quantifying worsening of disease <UNK> the osteoarthritis research society international (oarsi) <UNK> task force described hand osteoarthritis progression as being joint specifc, whereby osteoarthritis in one hand joint evolves independently from other hand joints <UNK> however, analysis from a large cohort study suggests there are patterns of symmetry, osteoarthritis clustering by row (across distal interphalangeal joints (dipjs) or across proximal interphalangeal joints (pipjs)), and clustering by ray (within a fnger) also exist <UNK> there is also poor correlation between radiographic and symptomatic disease <UNK> <UNK> rheumatology international conference abstract presentations some data from this manuscript has been published as a conference abstract: shah k et al <UNK> a systematic review of risk factors and diagnostic methods for hand interphalangeal joint osteoarthritis progression. osteoarthr cartil <UNK> <UNK> electronic supplementary material the online version of this article <UNK> contains supplementary material, which is available to authorized users. rheumatology international <UNK> <UNK> the aetiology for the progression of hand osteoarthritis is also poorly understood, and therefore identifying patients at highest risk for needing surgical management is limited. when managing hip and knee osteoarthritis surgically, shared decision making between clinicians and patients has been shown to be benefcial <UNK> in the hand, a better understanding of whether a patient is at increased risk of progressive disease would help to inform shared decision making. in particular, it would enable earlier investigations, more personalised treatment pathways, and targeted interventions for prevention and treatment. these priorities have been highlighted by the recent commission on the future of surgery <UNK> similarly, being able to identify patients with osteoarthritis who will not progress will prevent the over-investigation and excessive medical treatment of these patients. a review has found that abnormal scintigraphy scans, higher australian/canadian hand osteoarthritis index (auscan) scores, number of osteoarthritis joints at baseline, more pain, and nodal osteoarthritis were risk factors for the progression of radiographic or clinical hand osteoarthritis <UNK> however, this review combined interphalangeal joint (ipj) and base of thumb [frst carpometacarpal joint (cmcj)] osteoarthritis under the umbrella of ‘hand osteoarthritis.’ finger ipj and frst cmcj osteoarthritis are now thought to be diferent subsets of the disease, with diferent risk factors, pathophysiology and patterns of progression <UNK> therefore, the primary aim of this systematic review is to identify risk factors for the progression of fnger ipj osteoarthritis. the secondary aim is to describe the measurements used to defne the progression of ipj osteoarthritis. methods the reporting of this systematic review followed the preferred reporting items for systematic reviews and meta-analysis (prisma) statement <UNK> the protocol was prospectively registered on prospero <UNK> <UNK> search strategy the search strategy was constructed with the assistance of a specialist health-care librarian. the search was conducted in four electronic databases: <UNK> medline by ovid, <UNK> embase by ovid, <UNK> scopus, <UNK> the cochrane library. the search string included a range of search terms for <UNK> hands and fngers, <UNK> osteoarthritis, and <UNK> progression, and was amended for each database (electronic supplementary material <UNK> the picos tool <UNK> was used to frame the search strategy as follows: population: adults with ipj osteoarthritis, intervention/prognostic factor: potential risk factor(s) for ipj osteoarthritis progression, comparison: no exposure to the risk factor(s), outcome: progression of ipj osteoarthritis, study type: quantitative methodology. the search was conducted on <UNK> october <UNK> and duplicates were removed. the search was updated on <UNK> february <UNK> the reference lists of all eligible articles were manually assessed for additional studies. rayyan qcri tool was used to import all papers <UNK> two groups of reviewers (group <UNK> ks, group <UNK> xy and jcel) independently screened titles and abstracts for eligibility. any articles with insufcient title or abstract information were referred for full text review. articles for which the full text was not available were requested directly from the authors. any disagreements in eligibility assessment was resolved at a consensus meeting by a third reviewer (srf). study eligibility criteria studies were considered eligible if they (a) included participants with evidence of radiographic or clinical ipj osteoarthritis at baseline; (b) the participants were followed up for at least <UNK> year (as it has been shown that progression of radiographic hand osteoarthritis can be detected over a <UNK> year time frame <UNK> (c) ipj osteoarthritis (separate from frst cmcj osteoarthritis) progression was measured at follow-up, using radiographic and/or symptomatic criteria (ipj osteoarthritis progression was defned as an increase in radiographic or symptomatic criteria/score at follow-up compared to baseline); (d) the association between a potential risk factor and the progression of ipj osteoarthritis was investigated at follow-up. case reports were excluded. letters to editors might contain important information about studies, such as new information or discussions of further weaknesses of original studies <UNK> therefore, letters to editors which exist in the context of original studies, included in our review, were examined to inform the risk of bias assessment and as additional sources of information <UNK> conference abstracts are considered to have high variability in terms of data reliability, accuracy and detail, and therefore these were excluded <UNK> <UNK> studies of infammatory arthritis, erosive arthritis, with participants under the age of <UNK> years (to avoid confounding by juvenile arthritis), and studies where ipj osteoarthritis results could not be separated from other joints including the frst cmcj, and were not provided on request of the corresponding author within <UNK> months were excluded. animal, cadaver, and cell studies were excluded. articles not in english, and articles which lacked accessible full texts (online or in paper copy throughout the uk, or after requesting them from the corresponding author with no reply within <UNK> months) were excluded. rheumatology international <UNK> <UNK> data extraction one reviewer (ks) independently extracted participant demographics (e.g. age and sex), study characteristics (e.g. study design), the potential risk factor/s assessed, effect measure and size/s and the definition/s used to measure osteoarthritis progression. a potential risk factor was defined as any factor investigated for an association with ipj osteoarthritis progression. if data was reported at multiple time points, results from all time points were extracted. if articles or supplementary material did not contain sufficient data, the corresponding author was contacted to request additional data, with a <UNK> month turnaround policy. for any articles which reported data from a study described in detail elsewhere, the source of the data was retrieved and data extracted as appropriate. data extraction was input into a microsoft excel file and cross-checked by a second independent reviewer (xy). risk of bias assessment two independent reviewers (ks and xy) rated the risk of bias of included studies using a modified version of the quality in prognosis studies (quips) risk of bias tool <UNK> (electronic supplementary material <UNK> the following five domains were assessed: <UNK> study participation, <UNK> study attrition, <UNK> prognostic factor measurement, <UNK> outcome measurement, <UNK> statistical analysis and reporting <UNK> we excluded the domain assessing ‘confounding factors’, as confounders can themselves be considered to be prognostic factors, and thus the term ‘confounders’ is a misnomer in prognostic factor studies <UNK> as there is currently limited established literature in the field of ipj osteoarthritis progression, any ‘confounder’ identified in the literature was treated as a potential risk factor for this review. each domain was given an overall score of ‘low’, ‘moderate’ or ‘high’ risk of bias (electronic supplementary material <UNK> the overall risk of bias of a study was classified by examining the risk of bias in each of the five domains. if one or more domains were classified as having high risk of bias, then this study was classified as having an overall high risk of bias <UNK> <UNK> if three or more domains were classified as having a moderate risk of bias, then this study was classified as having an overall moderate risk of bias <UNK> <UNK> <UNK> <UNK> if all domains were classified as having a low risk of bias, or less than three domains had a moderate risk of bias, then this study was classified as having an overall low risk of bias <UNK> <UNK> any disagreement between reviewers was discussed at a consensus meeting with a third reviewer (srf). analysis and best evidence synthesis risk factors for all defnitions of ipj osteoarthritis progression were identifed, followed by a subgroup analysis for dipj and pipj separately. if studies were homogenous with regard to study populations, potential risk factors assessed, efect measures used, and measurements of ipj osteoarthritis progression, a pooled meta-analysis was considered using review manager software <UNK> and the grading of recommendations, assessment, development and evaluation (grade) approach was used to assess the quality of evidence <UNK> if studies were heterogeneous, we chose not to report effect measures of different types and instead used a qualitative narrative summary. the association between a potential risk factor and ipj osteoarthritis progression was categorised as: (a) a risk factor: positive efect measure. (b) not a risk factor: negative efect measure; or, no statistical association. (c) conficting evidence: efect measures not in the same direction. a best evidence synthesis was used to summarise the data for each potential risk factor assessed <UNK> the criteria were applied sequentially. if multiple analyses were performed within one study, the consistent fndings approach described below was applied to the study to decide whether it showed consistent or mixed evidence. this was then used to calculate the overall best evidence synthesis across studies. (a) consistent evidence:≥ <UNK> of studies reported the same direction of efect (either positive or negative/no association). (b) mixed <UNK> of analyses reported the same direction of efect. if consistent evidence was found, the strength of evidence was assessed: (i) strong <UNK> studies with low risk of bias. (ii) moderate evidence: <UNK> study with low risk of bias. and <UNK> other study; <UNK> studies with moderate or high risk of bias. (iii) limited evidence with low risk of bias: <UNK> study with low risk of bias. (iv) limited <UNK> studies with moderate or high risk of bias. rheumatology international <UNK> <UNK> results studies included combining results from the search in october <UNK> and the updated search in february <UNK> <UNK> <UNK> titles were identifed through the search strategy, with <UNK> remaining after removal of duplicates. after screening titles and abstracts, the full text of <UNK> articles was evaluated, and eight articles met the inclusion criteria (fig. <UNK> no additional articles were found by reviewing the reference lists of eligible studies. study characteristics eight prospective cohort studies were included <UNK> (table <UNK> five studies included men and women <UNK> whilst three studies included only men <UNK> the smallest study included <UNK> participants <UNK> whilst the largest study included <UNK> participants <UNK> the shortest follow-up period was a mean of <UNK> years <UNK> and the longest followup was reported as a mean (standard deviation) of <UNK> <UNK> years <UNK> risk of bias seven studies were rated as having overall high risk of bias <UNK> <UNK> and one study was of moderate risk of bias <UNK> (table <UNK> ‘study participation’ was of high risk of bias in four studies due to studies not adequately reporting recruitment periods and places of recruitment <UNK> <UNK> <UNK> <UNK> in the ‘study attrition’ domain, plato et al. and kallman et al. did not clearly report response rates and reasons for participants with loss to follow-up <UNK> <UNK> whilst haugen et al. and marshall et al. had less than <UNK> response rates and also did not report reasons for loss to follow-up <UNK> <UNK> when assessing the ‘statistical analysis and reporting’ domain, it was found that plato et al., busby et al., and kalichman et al. did not provide efect measures, but only reported p values or stated whether results were ‘signifcant or not signifcant’ <UNK> <UNK> <UNK> fig. <UNK> preferred reporting items for systematic reviews and meta-analysis (prisma) fowchart of study selection screening inclusion eligibilit y iden fica on records aer duplicates removed (n = <UNK> abstracts/tles screened (n = <UNK> records excluded (n = <UNK> full-text arcles assessed for eligibility (n = <UNK> studies included in systemac review (n = <UNK> combined records idenfied through database searching on <UNK> oct <UNK> and <UNK> feb <UNK> (n= <UNK> medline (n = <UNK> embase (n = <UNK> scopus (n = <UNK> cochrane (n = <UNK> full-text arcles excluded (n = <UNK> -ineligible study design <UNK> -ineligible study outcome <UNK> rheumatology international <UNK> <UNK> table <UNK> characteristics of studies investigating risk factors for the progression of fnger interphalangeal joint osteoarthritis authors population length of follow-up (years) age (years) (mean) female (%) inclusion criteria exclusion criteria n (n) criteria for ipj oa progression risk factor assessed plato et al. <UNK> white middle class volunteers participated in the blsa in the usa group <UNK> <UNK> group <UNK> <UNK> group <UNK> <UNK> group <UNK> <UNK> ns <UNK> ns ns <UNK> (ns) increase <UNK> grade from the highest kl <UNK> grade at baseline in any dipj older age in men kallman et al. <UNK> white middle class volunteers who participated in the blsa in the usa <UNK> <UNK> <UNK> <UNK> years) ns <UNK> ns maximum kl score <UNK> at baseline (per patient); not specifed <UNK> <UNK> increase <UNK> grade from the highest kl <UNK> <UNK> grade at baseline in any pipj older age in men busby et al. <UNK> white middle class volunteers who participated in the blsa in the usa <UNK> ns <UNK> ns joints with kl score of <UNK> at baseline <UNK> (ns) outcome <UNK> increase <UNK> grade from the highest kl <UNK> grade at baseline in any ipj (dipj and pipj assessed separately) outcome <UNK> increase in number of ipjs with kl <UNK> <UNK> (dipj and pipj assessed separately) older age in men kalichman et al. <UNK> chuvashians; village; randomly recruited <UNK> men: <UNK> women: <UNK> <UNK> ns ns <UNK> <UNK> increase in number of ipjs with kl <UNK> <UNK> (dipj and pipj assessed separately) alcohol, anthropometric features, familial relationship, gender (female), older age in men, older age in women, smoking kalichman et al. <UNK> chuvashians; village; randomly recruited <UNK> men: <UNK> women: <UNK> <UNK> ns bone disease, amenorrhoea, hormone replacement therapy, steroids <UNK> <UNK> increase <UNK> grade in a cumulative kl <UNK> sum score <UNK> <UNK> and <UNK> pipjs) epiphyseal index (larger) rheumatology international <UNK> <UNK> blsa baltimore longitudinal study of aging, bmi body mass index, casha clinical assessment studies of the hand, cask clinical assessment studies of the knee, dipj distal interphalangeal joint, gp general practice, ipj interphalangeal joint, kl kellgren–lawrence atlas, pipj proximal interphalangeal joint, n number at baseline, n number at follow-up, ns not specifed, oa osteoarthritis, usa united states of america, x-rays plain flm radiographs table <UNK> (continued) authors population length of follow-up (years) age (years) (mean) female (%) inclusion criteria exclusion criteria n (n) criteria for ipj oa progression risk factor assessed hoeven et al. <UNK> rotterdam <UNK> men: <UNK> women: <UNK> <UNK> <UNK> years, living <UNK> year in ommoord, knee, hip, hand x-rays no x-rays, rheumatoid, fractures <UNK> <UNK> increase <UNK> kl <UNK> grade <UNK> ipj, <UNK> ipj had kl <UNK> <UNK> at baseline (dipj and pipj assessed separately) atherosclerosis haugen et al. <UNK> usa; hospital study sites <UNK> <UNK> <UNK> ns systemic infammatory arthritis, bilateral end stage knee oa, inability to walk without aids, contraindication to mri <UNK> <UNK> increase <UNK> grade in a cumulative modifed kl <UNK> <UNK> sum score (dipj and pipj assessed together) alcohol (higher intake), bmi (higher)—at age <UNK> bmi (higher)—current, smoking, waist circumference (higher) marshall et al. <UNK> from casha and cask cohorts; gp community <UNK> <UNK> <UNK> age <UNK> years at baseline, reported hand pain in last month infammatory arthritis, all hand joints afected with <UNK> at baseline, deaths/untraceable/address unknown, severe/terminal illness <UNK> <UNK> outcome <UNK> increase <UNK> grade in a cumulative kl <UNK> sum score (dipj and pipj assessed together) outcome <UNK> increase in number of ipjs with kl <UNK> <UNK> (dipj and pipj assessed together) bmi (higher)— current, diabetes type <UNK> fasting glucose, dyslipidaemia, hypertension, number of metabolic factors (higher) rheumatology international <UNK> <UNK> measurements for the progression of fnger interphalangeal joint osteoarthritis all studies assessed osteoarthritis radiographically, using a version of the kellgren and lawrence (kl) classifcation <UNK> <UNK> (table <UNK> three studies measured ipj osteoarthritis progression as <UNK> grade increase from the highest kl grade at baseline <UNK> three studies measured it as an increase in the total number of ipjs with kl <UNK> <UNK> <UNK> <UNK> one study measured progression as <UNK> grade kl increase in <UNK> ipj <UNK> and three studies measured it <UNK> grade increase in a cumulative kl sum score <UNK> <UNK> <UNK> no studies measured osteoarthritis progression through a deterioration in symptomatic scoring. risk factors for the progression of fnger interphalangeal joint osteoarthritis eighteen potential risk factors were assessed, most commonly in one study only (efect measures shown in electronic supplementary material <UNK> for potential risk factors assessed by more than one study, due to heterogeneity in the defnitions of the risk factor/s, statistical tests, and osteoarthritis defnitions, a best evidence synthesis was performed. three risk factors were identifed: diabetes type <UNK> fasting glucose (ifg) <UNK> and larger epiphyseal index (ei) in males <UNK> and in females <UNK> (all with limited evidence) (table <UNK> older age in men <UNK> <UNK> and in women <UNK> showed mixed results (table <UNK> diabetes type <UNK> fasting glucose (ifg) marshall et al. assessed diabetes type <UNK> compared to not having these conditions in a total of <UNK> participants <UNK> (efect measures shown in electronic supplementary material <UNK> in a complete case analysis, these conditions were associated with an increase <UNK> grade in a cumulative kl <UNK> sum score for all ipjs [adjusted mean diference <UNK> confdence interval) <UNK> <UNK> <UNK> however, there was no association following multiple imputation <UNK> (− <UNK> to <UNK> <UNK> diabetes type <UNK> was associated with an increase in the number of ipjs with kl <UNK> <UNK> following multiple imputation and complete case analysis <UNK> <UNK> and <UNK> <UNK> respectively] <UNK> large fnger epiphyseal index (ei) kalichman et al. investigated larger ei in <UNK> participants <UNK> (electronic supplementary material <UNK> a positive association was found in both males (multiple regression coefcient, <UNK> <UNK> ci not reported) and females <UNK> <UNK> ci not reported), between larger ei and ipj osteoarthritis progression (measured as an increase <UNK> grade in a cumulative kl <UNK> sum score for pipjs in the assessed digits). dipj and pipj subgroup analysis in the dipj subgroup analysis, eight potential risk factors were assessed, and only older age in women was found to be a risk factor (correlation coefcient <UNK> <UNK> (limited evidence) (table <UNK> in the pipj subgroup analysis, <UNK> potential risk factors were assessed, and larger ei in males <UNK> <UNK> ci not reported) and females <UNK> <UNK> ci not reported) were identifed as risk factors <UNK> (limited evidence for both) (table <UNK> discussion osteoarthritis is one of the largest health-care burdens, and radiographic hand osteoarthritis is highly prevalent, afecting more than one out of fve adult americans <UNK> osteoarthritis is considered to be progressive in some cases. however, table <UNK> risk of bias for studies assessing potential risk factors for the progression of fnger interphalangeal joint osteoarthritis, assessed using a modifed quality in prognosis studies (quips) tool a biases from modifed quality in prognosis studies (quips) tool: <UNK> study participation; <UNK> study attrition; <UNK> prognostic factor measurement; <UNK> outcome measure; <UNK> statistical analysis and reporting authors biasesa overall risk of bias <UNK> <UNK> <UNK> <UNK> <UNK> plato et al. <UNK> high high moderate moderate high high kallman et al. <UNK> high high low low moderate high busby et al. <UNK> moderate moderate low moderate high high kalichman et al. <UNK> moderate low moderate moderate high high kalichman et al. <UNK> high high moderate low moderate high hoeven et al. <UNK> moderate low moderate low moderate moderate haugen et al. <UNK> high high moderate moderate moderate high marshall et al. <UNK> moderate high low moderate low high rheumatology international <UNK> <UNK> table <UNK> potential risk factors for the progression of fnger interphalangeal joint osteoarthritis, assessed using a best evidence synthesis consistent evidence for a risk factor consistent evidence for not being a risk factor mixed evidence strong evidence moderate evidence limited evidence with low risk of bias limited evidence strong evidence moderate evidence limited evidence with low risk of bias limited evidence using all defnitions of ipj osteoarthritis progression diabetes/impaired fasting glucose <UNK> higher alcohol intake <UNK> <UNK> older age in men <UNK> <UNK> a larger epiphyseal index in females <UNK> anthropometric features <UNK> older age in women <UNK> a larger epiphyseal index in males <UNK> atherosclerosis <UNK> larger bmi—at age <UNK> years <UNK> larger bmi—current <UNK> <UNK> dyslipidaemia <UNK> familial relationship <UNK> gender (female) <UNK> gender (male) <UNK> hypertension <UNK> higher number of metabolic factors <UNK> smoking <UNK> <UNK> larger waist circumference <UNK> in dipjs only older age in women <UNK> higher alcohol intake <UNK> gender (female) <UNK> anthropometric features <UNK> older age in men <UNK> <UNK> atherosclerosis <UNK> familial relationship <UNK> gender (male) <UNK> in pipjs only larger epiphyseal index in females <UNK> higher alcohol intake <UNK> older age in men <UNK> <UNK> a larger epiphyseal index in males <UNK> anthropometric features <UNK> older age in women <UNK> atherosclerosis <UNK> rheumatology international <UNK> <UNK> there is no unifed method to measure the progression of hand osteoarthritis, and ipj osteoarthritis is now considered to be a diferent disease subset from frst cmcj osteoarthritis. as ipj osteoarthritis progresses, it can be treated surgically, and there are currently no disease-modifying drugs. risk factors which increase the chance of ipj osteoarthritis progression in patients have been studied in the literature. we identifed eight studies (seven high risk of bias) investigating potential risk factors for the progression of fnger ipj osteoarthritis <UNK> all studies measured osteoarthritis progression radiographically, using a version of the kl classifcation system <UNK> <UNK> our review found that patients with diabetes/ifg <UNK> and both male and females with a larger fnger ei <UNK> are at increased risk of ipj osteoarthritis progression (limited evidence), whilst older age in men <UNK> <UNK> and in women <UNK> showed mixed evidence. results were largely similar when dipj and pipj osteoarthritis when assessed separately. the kl classifcation system <UNK> <UNK> was used to measure osteoarthritis progression by all studies <UNK> <UNK> the kl classifcation system <UNK> <UNK> is a sensitive method for measuring the progression of radiographic hand osteoarthritis over a <UNK> time frame <UNK> all of the studies included in this review were longitudinal studies, and the shortest follow-up period had a mean of <UNK> years <UNK> therefore, all studies would have adequately detected any radiographic ipj osteoarthritis progression. however, the defnitions of each measure of progression varied across studies. some studies measured an increase in kl grade <UNK> whilst others measured it as an increase in the number of joints with a particular kl grade <UNK> <UNK> <UNK> <UNK> and still other studies measured it as an increase in a cumulative kl sum score <UNK> <UNK> <UNK> (which is dependent on either an increase in kl grade of already afected joints, or an increase in the number of joints with a particular kl grade). the sensitivity of the kl classifcation system <UNK> <UNK> in detecting ipj osteoarthritis progression measured in these diferent ways has not yet been investigated. additionally, potential risk factors that occur at a localised joint level (such as joint trauma) could also be risk factors for isolated ipj osteoarthritis progression. however, localised risk factors were not assessed by studies in this review. further research is required to understand whether there are any joint-specifc risk factors for ipj osteoarthritis progression, and whether these might cause osteoarthritis to progress at one joint independently of other ipjs. diabetes/ifg was found to be a risk factor for ipj osteoarthritis progression <UNK> marshall et al. suggest diabetes/ ifg might be a risk factor for the progression of osteoarthritis due to hyperglycaemia <UNK> hyperglycaemia has been shown to induce reactive oxygen species and the production of cytokines, which result in joint infammation and in the production of proteolytic enzymes that degrade cartilage <UNK> however, in a delphi study consisting of a panel of hand surgeons, the use of diabetic medication and abnormal fasting glucose were not identifed as risk factors for fnger ipj osteoarthritis progression <UNK> this suggests that though diabetes/ifg might have a relationship with ipj osteoarthritis on a molecular level, in a clinical context the efect is not yet well recognised. our results also found that larger fnger ei is a risk factor for ipj osteoarthritis progression <UNK> in hip and knee osteoarthritis, larger cross-sectional areas in the femoral neck and proximal femoral shaft and in the tibial plateau, respectively, have also been described <UNK> <UNK> additionally, in knee osteoarthritis, a loss of articular cartilage coupled with larger bone epiphyseal area results in a change of loading and force across a joint, further contributing to the progression of osteoarthritis <UNK> however, in the hands, and particularly the fnger ipjs, the load across the joint is much lower, suggesting that there might be other mechanisms which contribute to the relationship between ei and ipj osteoarthritis progression. given the limited evidence reported in our systematic review, further high-quality studies are needed to assess this relationship. table <UNK> (continued) consistent evidence for a risk factor consistent evidence for not being a risk factor mixed evidence strong evidence moderate evidence limited evidence with low risk of bias limited evidence strong evidence moderate evidence limited evidence with low risk of bias limited evidence familial relationship <UNK> gender (female) <UNK> gender (male) <UNK> smoking <UNK> bmi body mass index, dipj distal interphalangeal joints, ipj interphalangeal joint, pipj proximal interphalangeal joints a

<|EndOfText|>

sample size considerations for the external validation of a multivariable prognostic model: a resampling study after developing a prognostic model, it is essential to evaluate the performance of the model in samples independent from those used to develop the model, which is often referred to as external validation. however, despite its importance, very little is known about the sample size requirements for conducting an external validation. using a large real data set and resampling methods, we investigate the impact of sample size on the performance of six published prognostic models. focussing on unbiased and precise estimation of performance measures (e.g. the c-index, d statistic and calibration), we provide guidance on sample size for investigators designing an external validation study. our study suggests that externally validating a prognostic model requires a minimum of <UNK> events and keywords: prognostic model; sample size; external validation <UNK> introduction prognostic models are developed to estimate an individual’s probability of developing a disease or outcome in the future. a vital step toward accepting a model is to evaluate its performance on similar individuals separate from those used in its development, which is often referred to as external validation or transportability <UNK> however, despite the widespread development of prognostic models in many areas of medicine <UNK> very few been externally validated <UNK> to externally validate a model is to evaluate its predictive performance (calibration and discrimination) using a separate data set from that used to develop the model <UNK> it is not repeating the entire modelling process on new data, refitting the model to new ‘validation’ data, or fitting the linear predictor (prognostic index) from the original model as a single predictor to new data <UNK> it is also not necessarily comparing the similarity in performance to that obtained during the development of the prognostic model. whilst in some instances a difference in the performance can be suggestive of deficiencies in the development study, the performance in the new data may still be sufficiently good enough for the model to be potentially useful. the case-mix (i.e., the distribution of predictors included in the model) will influence the performance of the model <UNK> it is generally unlikely that the external validation data set will have an identical casemix to the data used for development. indeed, it is preferable to use a slightly different case-mix in external validation to judge model transportability. successful external validation studies in diverse settings (with different case-mix) indicate that it is more likely that the model will be generalizable to plausibly related, but untested settings <UNK> despite the clear importance of external validation, the design requirements for studies that attempt to evaluate the performance of multivariable prognostic models in new data have been little explored <UNK> published studies evaluating prognostic models are often conducted using sample sizes that are clearly inadequate for this purpose, leading to exaggerated and misleading performance of the prognostic model <UNK> finding such examples is not difficult <UNK> for example, a modified thoracoscore, to predict in-hospital mortality after general thoracic surgery, was evaluated using <UNK> patients, but included only eight events (deaths). a high c-index value was reported, <UNK> <UNK> confidence interval <UNK> to <UNK> <UNK> in the most extreme case, a data set with only one outcome event was used to evaluate a prognostic model <UNK> in this particular study, an absurd value of the c-index was reported, <UNK> <UNK> confidence interval <UNK> to <UNK> concluding predictive accuracy, and thus that the model is fit for purpose, on such limited data is nothing but misleading. the only guidance for sample size considerations that we are aware of is based on a hypothesis testing framework (i.e. to detect pre-specified changes in the c-statistic) and recommends that models developed using logistic regression are evaluated with a minimum of <UNK> events <UNK> however, a recent systematic review evaluating the methodological conduct of external validation studies found that just under half of the studies evaluated models on fewer than <UNK> events <UNK> it is therefore important to provide researchers with appropriate guidance on sample size considerations when evaluating the performance of prognostic models in an external validation study. when validating a prognostic model, investigators should clearly explain how they determined their study size, so that their findings can be placed in context <UNK> our view is that external validation primarily concerns the accurate (unbiased) estimation of performance measures (e.g., the c-index). it does not necessarily include formal statistical hypothesis testing, although this may be useful in some situations. therefore sample size considerations should be based on estimating performance measures that are sufficiently close to the true underlying population values (i.e., unbiased) along with measures of uncertainty that are sufficiently narrow (i.e., precise estimates) so that meaningful conclusions on the model’s predictive accuracy in the target population can be drawn <UNK> the aim of this article is to examine sample size considerations for studies that attempt to externally validate prognostic models and to illustrate that many events are required to provide reasonable estimates of model performance. our study uses published prognostic models <UNK> <UNK> qdscore <UNK> and the cox framingham risk score <UNK> to illustrate sample size considerations using a resampling design from a large data set <UNK> million) of general practice patients in the uk. the structure of the paper is as follows. section <UNK> describes the clinical data set and the prognostic models. section <UNK> describes the design of the study, the assessment of predictive performance and the methods used to evaluate the resampling results. section <UNK> presents the results from the resampling study, which are then discussed in section <UNK> <UNK> data set and prognostic models <UNK> study data: the health improvement network the health improvement network (thin) is a large database of anonymized primary care records collected at general practice surgeries around the uk. the thin database currently contains medical records on approximately <UNK> of the uk population. clinical information from over <UNK> million individuals (from <UNK> general practices) registered between june <UNK> and june <UNK> form the data set. the data have previously been used in the external validation of a number of prognostic models (including those considered in this study) <UNK> there are missing data for various predictors needed to use the prognostic models. for simplicity, we have used one of the imputed data sets from the published external validation studies, where details on the imputation strategy can be found <UNK> <UNK> prognostic models at the core of the study are six sex-specific published models for predicting the <UNK> risk of developing cardiovascular disease (cvd) <UNK> <UNK> and cox framingham <UNK> and the <UNK> risk of developing type <UNK> diabetes (qdscore <UNK> all six prognostic models are all predicting time-to-event outcomes using cox regression. none of these models were developed using thin, but thin has previously been used to evaluate their performance in validation studies <UNK> <UNK> was developed using <UNK> million general practice patients aged between <UNK> and <UNK> years <UNK> million person years of observation) contributing <UNK> <UNK> cardiovascular events from the qresearch database <UNK> separate models are available for women <UNK> <UNK> cvd events) and men <UNK> <UNK> cvd events), containing <UNK> predictors, <UNK> interactions and fractional polynomial terms for age and body mass index (www.qrisk.org). <UNK> cox framingham was developed using <UNK> framingham study participants aged <UNK> to <UNK> years contributing <UNK> cardiovascular events <UNK> separate models are available for women <UNK> cvd events) and men <UNK> cvd events), each containing <UNK> predictors. qdscore was developed on <UNK> million general practice patients aged between <UNK> and <UNK> years <UNK> million person years of observation) contributing <UNK> <UNK> incident diagnoses of type <UNK> diabetes from the qresearch database <UNK> separate models are available for women and men, each containing <UNK> predictors, <UNK> interactions and fractional polynomial terms for age and body mass index (www.qdscore.org). <UNK> methods <UNK> resampling strategy a resampling strategy was applied to examine the influence of sample size (more specifically, the number of events) on the bias and precision in evaluating the performance of published prognostic models. samples were randomly drawn (with replacement) from the thin data set so that the number of events in each sample was fixed at <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> or <UNK> by stratified sampling according to the outcome ensuring that the proportion of events in each sample was the same as the overall proportion of events in the thin data set (table i). the sample sizes for each prognostic model at each value of number of events can be found in the supporting information. for each scenario (i.e., for each sample size), <UNK> <UNK> samples (denoted b) were randomly drawn and performance measures were calculated for each sample. <UNK> performance measures the performance of the prognostic models was quantified by assessing aspects of model discrimination (the c-index <UNK> and d statistic <UNK> calibration <UNK> and other performance measures <UNK> d <UNK> <UNK> oxs <UNK> and the brier score for censored data <UNK> discrimination is the ability of a prognostic model to differentiate between people with different outcomes, such that those without the outcome (e.g., alive) have a lower predicted risk than those with the outcome (e.g., dead). for the survival models used within this study, which are time-to-event based, discrimination is evaluated using harrell’s c-index, which is a generalization of the area under the receiver operating characteristic curve for binary outcomes (e.g., logistic regression) <UNK> harrell’s c-index can be interpreted as the probability that, for a randomly chosen pair of patients, the patient who actually experiences the event of interest earlier in time has a lower predicted value. the c-index and its standard error were calculated using the rcorr.cens function in the rms library in r. we also examined the d statistic, which can be interpreted as the separation between two survival curves (i.e., a difference in log hr) for two equal size prognostic groups derived from cox regression <UNK> it is closely related to the standard deviation of the prognostic index (pi = <UNK> <UNK> βkxk), which is a weighted sum of the variables (xi) in the model, where the weights are the regression coefficients (βi). d is calculated by ordering the values from the prognostic index, transforming them using expected standard normal order statistics, dividing the result by κ ¼ ffiffiffiffiffiffiffi <UNK> p <UNK> and fitting this in a single term cox regression. d and its standard error are given by the coefficient and standard error in the single term cox regression model. table i. ‘true’ values based on the entire thin validation cohort. number of individuals number of events (%) performance measure c-index d statistic <UNK> d <UNK> oxs brier score calibration slope <UNK> <UNK> women <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> men <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> cox framingham <UNK> women <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> men <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> qdscore <UNK> women <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> men <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> the calibration slope was calculated by estimating the regression coefficient in a cox regression model with the prognostic index (the linear predictor) as the only covariate. if the slope is <UNK> discrimination is poorer in the validation data set (regression coefficients are on average smaller than the development data set), and conversely, it is better in the validation data set if the slope is <UNK> coefficients are on average larger than the development data set) <UNK> we also examined the calibration of the models over the entire probability range at a single time point (at <UNK> years) using the val.surv function in the rms library in r, which implements the hare function from the polspline package for flexible adaptive hazard regression <UNK> in summary, for each random sample, hazard regression using linear splines are used to relate the predicted probabilities from the models at <UNK> years to the observed event times (and censoring indicators) to estimate the actual event probability at <UNK> years as a function of the estimate event probability at <UNK> years. to investigate the influence of sample size on calibration, for each event size, plots of observed outcomes against predicted probabilities were drawn and overlaid for each of the <UNK> <UNK> random samples. we examined two <UNK> -type measures <UNK> (explained variation <UNK> and explained randomness <UNK> and the brier score <UNK> royston and sauerbrei’s <UNK> d is the proportion of the that is explained by the prognostic model <UNK> and is given by <UNK> d ¼ <UNK> <UNK> þ <UNK> where d is the value of the d statistic <UNK> <UNK> <UNK> <UNK> and κ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi <UNK> p . the measure of explained randomness, <UNK> k of o’quigley et al. <UNK> is defined as <UNK> oxs ¼ <UNK> exp <UNK> k lβ <UNK>    where k is the number of outcome events, and lb and <UNK> are the log partial likelihoods for the prognostic model and the null model respectively. standard errors of <UNK> k were calculated using the nonparametric bootstrap <UNK> bootstrap replications). the brier score for survival data is a measure of the average discrepancy between the true disease status <UNK> or <UNK> and the predicted probability of developing the disease <UNK> defined as a function of time <UNK> bs tð þ ¼ <UNK> n ∑ n <UNK> s t ^ð þ jxi <UNK> i tð þ i ≤ t; δi ¼ <UNK> ĝ tð þi þ <UNK> s t ^ð þ jxi <UNK> i tð þ i > t ĝð þt " # where ŝ(· |xi) is the predicted probability of an event for individual i; ĝ is the kaplan–meier estimate of the censoring distribution, which is based on the observations (ti, <UNK> is the censoring indicator and i denotes the indicator function. <UNK> the brier score is implemented in the function sbrier from the package ipred in r. <UNK> evaluation the objective of our study was to evaluate the impact of sample size (more precisely the number of events) on the accuracy, precision and variability of model performance. we examined the sample size requirements using the guidance by burton et al. <UNK> we calculated the following quantities for each of the performance measures over the b simulations (defined in the preceding section): • percentage bias, which is the relative magnitude of the raw bias to the true value, defined as ^θ - θ  =θ  . • standardized bias, which is the relative magnitude of the raw bias to the standard error, defined as ^θ - θ  =se ^θ  . a standardized bias of <UNK> percent implies that the estimate lies one quarter of a standard error below the true value. • root mean square error, which incorporates both measures of bias and variability of the estimate, defined as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi <UNK> b ∑ b <UNK> ^θi θ <UNK> s . <UNK> • estimated coverage rate of the <UNK> confidence interval for the c-index, d statistic, <UNK> d and <UNK> oxs, which indicate the proportion of times that a confidence interval contains the true value (θ). an acceptable coverage should not fall outside of approximately two standard errors of the nominal coverage probability ð þp ; se pð þ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi pð þ <UNK> p =b p <UNK> • average width of the confidence interval, defined as <UNK> ∑ b <UNK> <UNK> ^θi   . the true values (θ) of the performance measures were obtained using the entire thin data set for each model (table i). ^θ - ¼ ∑ b <UNK> ^θi=b, where b is the number of simulations performed and ^θi is the performance measure of interest for each of the i = <UNK> <UNK> <UNK> simulations. the empirical standard error, se ^θ , is the square root of the variance of over all b-simulated ^θ values. if, for the d statistic <UNK> d, the modelbased standard error is valid, then its mean over the <UNK> <UNK> simulations should be close to the empirical standard error se ^θ . <UNK> result figure <UNK> presents the empirical values, with boxplots overlaid, for the c-index, d statistic, <UNK> d, <UNK> oxs brier score and calibration slope for <UNK> (women), describing pure sampling variation. as expected, considerable variation in the sample values for each of the six performance measures are observed when the number of events is small. thus, inaccurate estimation of the true performance is more likely in studies with low numbers of events. the mean percent bias, standardized bias and rmse of the performance measures are displayed graphically in figure <UNK> for all of the models, the mean percent bias of both the c-index and brier score are within <UNK> when the number of events reaches <UNK> at <UNK> events, the average bias of the d statistic, <UNK> d and calibration slope is within <UNK> of the true value. the mean standardized bias for all of the models and performance measures drops below <UNK> once the number of events increases to <UNK> because of the skewness in bias at small values of number of events, the median percent bias and standardized bias of the performance measures are also presented (supporting information). for all of the performance measures, the median bias drops below <UNK> as the number of events reaches <UNK> similarly, figure <UNK> empirical performance of <UNK> (women), measured using the c-index, d statistic, <UNK> d, <UNK> oxs, brier score and calibration slope. <UNK> the median standardized bias drops below <UNK> for all of the performance measures and models when the number of events approaches <UNK> as expected, the rmse decreases as the number of events increases for all six performance measures (figure <UNK> the same pattern is observed for all six prognostic models. coverage of the confidence intervals for the c-index, d statistic and <UNK> d are displayed in figure <UNK> acceptable coverage of the c-index at the nominal level of <UNK> percent is achieved as the number of events approaches and exceeds <UNK> however, the d statistic confidence interval exhibits over-coverage regardless of sample size. there is under-coverage of <UNK> d at less than <UNK> events and over-coverage as the number of events increases (for four of the six prognostic models examined). the mean widths of the <UNK> confidence intervals for all of the models are displayed in figure <UNK> a steep decrease is observed in the mean width for all models as the number of events approaches <UNK> within this range, the decrease in mean width becomes smaller with more events. a similar pattern is observed in the width variability, as shown in figure <UNK> for <UNK> (women). figure <UNK> mean percent, standardized bias and rmse of the c-index, d statistic, <UNK> d, <UNK> oxs, brier score and calibration slope. <UNK> the effect of sample size on the performance of the hazard regression assessment of calibration of <UNK> (women) is described in figure <UNK> for each panel (i.e., each event size), <UNK> <UNK> calibration lines have been plotted and a diagonal (dashed) line going through the origin with slope <UNK> has been superimposed, which depicts perfect calibration. furthermore, we have overlaid a calibration line using the entire thin data set to judge convergence of increasing event size. for data sets with <UNK> or fewer numbers of events, the ability to assess calibration was poor. for predicted probabilities greater than <UNK> there was modest to substantial variation between the fitted calibration curves, which decreased as the number of events increased. the calibration line (blue line) using the entire thin data set shows overestimation towards the upper tail of the distribution, whilst some overestimation is captured, from event sizes in excess of <UNK> the true magnitude of overestimation in using <UNK> (women) in the thin data set is not fully captured even when the number of events reach <UNK> calibration plots for two of the five prediction models <UNK> men and cox framingham women) show similar patterns, figure <UNK> coverage rates and <UNK> confidence interval widths for the c-index, d statistic, <UNK> d, <UNK> oxs and calibration slope. [bootstrap standard errors for <UNK> oxs based on <UNK> simulations and <UNK> bootstrap replications]. <UNK> whilst for the remaining three models accurate assessment of calibration is achieved when the number of events reach <UNK> (data not shown). figure <UNK> displays the proportion of simulations in which the performance estimates are within <UNK> <UNK> <UNK> and <UNK> of the true performance measure as the number of events increases. fewer events are required to obtain precise estimates of the c-index than of the other performance measures. for example, at <UNK> events, over <UNK> of simulations yield estimates of the c-index within <UNK> of the true value and over <UNK> of simulations yield values within <UNK> of the true value. considerably more events are required for the d statistic, <UNK> d, brier score and calibration slope. <UNK> additional analyses as observed in figure <UNK> coverage of the d statistic is larger than the nominal <UNK> level regardless of the number of events. similarly, <UNK> d coverage tends to be larger than the nominal <UNK> level as the number of events increases. therefore, we carried out further analyses to investigate the model-based standard error and the nonparametric bootstrap standard error of the d statistic and <UNK> d <UNK> the results are shown in table ii. figure <UNK> width of the <UNK> confidence interval of the c-index, d statistic <UNK> d , <UNK> oxs and calibration slope <UNK> women). [bootstrap standard errors for <UNK> oxs based on <UNK> simulations and <UNK> bootstrap replications]. <UNK> the results from the additional simulations indicate that the model-based standard error is overestimated. there is good agreement between the empirical and bootstrap standard errors, with coverage using the bootstrap standard errors close to the nominal <UNK> percent (table iii). <UNK> discussion external validation studies are a vital step in introducing a prognostic model, as they evaluate the performance and transportability of the model using data that were not involved in its development <UNK> the performance of a prognostic model is typically worse when evaluated on samples independent of the sample used to develop the model <UNK> therefore, the more external validation studies that demonstrate satisfactory performance, the more likely the model will be useful in untested populations, and ultimately, the more likely it will be used in clinical practice. however, despite their clear importance, multiple (independent) external validation studies are rare. many prognostic models are only subjected to a single external validation study and are abandoned if that study gives poor results. other investigators then proceed in developing yet another new model, discarding previous efforts, and the cycle begins again <UNK> however, systematic reviews examining methodological conduct and reporting have shown that many external validation studies are fraught with deficiencies, including inadequate sample size <UNK> the results from our study indicate that small external validation studies are unreliable, inaccurate and possibly biased. we should avoid basing the decision to discard or recommend a prognostic model on an external validation study with a small sample size. an alternative approach that could be used to determine an appropriate sample size for an external validation study is to focus on the ability to detect a clinically relevant deterioration in model performance <UNK> whilst this approach may seem appealing, it requires the investigator to pre-specify a performance measure to base this decision on and to justify the amount of deterioration that will indicate a lack of validation. neither of these conditions are necessarily straightforward, particularly when the figure <UNK> calibration plots for <UNK> (women). the red dashed line denoted perfect prediction. the blue line is the model calibration using the entire data set. <UNK> case-mix is different or the underlying population in the validation data set is different to that from which the model was originally developed <UNK> we take the view that a single external validation is generally insufficient to warrant widespread recommendation of a prognostic model. the case-mix in a development sample does not necessarily reflect the case-mix of the intended population for which the model is being developed, as studies developing a prognostic model are rarely prospective and typically use existing data collected for an entirely different purpose. a prognostic model should be evaluated on multiple validation samples with different case-mixes from the sample used to develop the model, thereby allowing a more thorough investigation into the performance of the model, possibly using meta-analysis methods. a strength of our study is the use of large data sets, multiple prognostic models and evaluating seven performance measures (c-index, d statistic, <UNK> d, <UNK> oxs, brier score, calibration slope and calibration plots). figure <UNK> proportion of estimates within <UNK> <UNK> <UNK> <UNK> and <UNK> of the true value for <UNK> (women). <UNK> we also showed that the analytical standard error for the d statistic (and <UNK> d) are too large, but could be rectified by calculating bootstrap standard errors. fundamental issues in the design of external validation studies have received little attention. existing studies examining the sample size requirements of multivariable prognostic models have focused on models developed using logistic regression <UNK> adopting a hypothesis testing framework, vergouwe and colleagues suggested that a minimum of <UNK> events and <UNK> non-events are required for external validation of prediction models developed using logistic regression <UNK> peek and colleagues examined the influence of sample size when comparing multiple prediction models, including examining the accuracy of performance measures, and concluded that a substantial sample size is required <UNK> our study took the approach that the sample size of an external validation study should be guided by the premise of producing accurate and precise estimates of model performance that reasonably reflect the true underlying population estimate. despite the differences taken in approach, our recommendations coincide. our study focused on prognostic models predicting time-to-event outcomes, whilst we don’t expect any discernable differences, further studies are required to evaluate models predicting binary events. we suggest that externally validating a prognostic model requires a minimum of <UNK> events, preferably <UNK> or more events.

<|EndOfText|>

quantifying the impact of different approaches for handling continuous predictors on the performance of a prognostic model continuous predictors are routinely encountered when developing a prognostic model. investigators, who are often non-statisticians, must decide how to handle continuous predictors in their models. categorising continuous measurements into two or more categories has been widely discredited, yet is still frequently done because of its simplicity, investigator ignorance of the potential impact and of suitable alternatives, or to facilitate model uptake. we examine three broad approaches for handling continuous predictors on the performance of a prognostic model, including various methods of categorising predictors, modelling a linear relationship between the predictor and outcome and modelling a nonlinear relationship using fractional polynomials or restricted cubic splines. we compare the performance (measured by the c-index, calibration and net benefit) of prognostic models built using each approach, evaluating them using separate data from that used to build them. we show that categorising continuous predictors produces models with poor predictive performance and poor clinical usefulness. categorising continuous predictors is unnecessary, biologically implausible and inefficient and should not be used in prognostic model development. keywords: prognostic modelling; continuous predictors; dichotomisation <UNK> introduction categorising continuous measurements in regression models has long been regarded as problematic (e.g., biologically implausible particularly when dichotomising), highly inefficient and unnecessary <UNK> in the context of clinical prediction, investigators developing new prognostic models frequently categorise continuous predictors into two or more categories <UNK> categorisation is often carried out with no apparent awareness of its consequences or in a misguided attempt to facilitate model interpretation, use and uptake. however, categorisation causes a loss of information, and therefore reduces the statistical power to identify a relationship between a continuous measurement and patient outcome <UNK> as studies developing new prognostic models are already generally quite small, it seems unwise to discard any information <UNK> despite the numerous cautions and recommendations not to categorise continuous measurements, there have been relatively few quantitative assessments of the impact of the choice of approach for handling continuous measurements on the performance of a prognostic model <UNK> <UNK> research article determining the functional form of a variable is an important, yet often overlooked step in modelling <UNK> it is often insufficient to assume linearity, and categorising continuous variables should be avoided. alternative approaches that allow more flexibility in the functional form of the association between predictors and outcome should be considered, particularly if they improve model fit and model predictions <UNK> two commonly used approaches are fractional polynomials and restricted cubic splines <UNK> however, few studies have examined how the choice of approach for handling continuous variables affects the performance of a prognostic model. two single case studies demonstrated that dichotomising continuous predictors resulted in a loss in information and a decrease in the predictive ability of a prediction model <UNK> however, neither study was exhaustive, and both studies only examined the impact on model performance using the same data that the models were derived from. in a more recent study, nieboer and colleagues compared the effect of using log transformations, fractional polynomials and restricted cubic splines against retaining continuous predictors as linear on the performance of logistic-based prediction models, but they did not also examine models that categorised one or more continuous predictors <UNK> when developing a new prognostic model, investigators can broadly choose to (i) dichotomise, or more generally categorise, a continuous predictor using one or more cut-points; (ii) leave the predictor continuous but assume a linear relationship with the outcome; or (iii) leave the predictor continuous but allow a nonlinear association with the outcome, such as by using fractional polynomials or restricted cubic splines. how continuous measurements are included will affect the generalisability and transportability of the model <UNK> a key test of a prognostic model is to evaluate its performance on an entirely separate data set <UNK> it is therefore important that the associations are appropriately modelled, to improve the likelihood that the model will predict sufficiently well using data with different case-mix. the aim of this article is to quantitatively illustrate the impact of the choice of approach for handling continuous predictors on the apparent performance (based on the development data set) and validation performance (in a separate data set) of a prognostic model. <UNK> methods <UNK> study data: the health improvement network the health improvement network (thin) is a large database of anonymised electronic primary care records collected at general practice surgeries around the united kingdom (england, scotland, wales and northern ireland). the thin database contains medical records on approximately <UNK> of the united kingdom population. clinical information from over <UNK> million individuals (from <UNK> general practices) registered between june <UNK> and june <UNK> form the data set. the data have previously been used in the external validation of a number of risk prediction models, including those considered in this study <UNK> there are some missing data for the predictors of systolic blood pressure, body mass index and cholesterol. for simplicity and convenience, we have used an imputed data set used in published external validation studies (details on the imputation strategy are reported elsewhere <UNK> <UNK> prognostic models we used cox regression to develop prognostic models that predict the <UNK> risk of cardiovascular disease and <UNK> risk of hip fracture. we split the thin database geographically by pulling out two cohorts: general practices from england and general practices from scotland. data from the england cohort were used to develop the models, whilst data from scotland were used to validate the model. using data for <UNK> patients (men and women) from england (with <UNK> outcome events) to develop cardiovascular prognostic models. model performance was evaluated on <UNK> patients from scotland (with <UNK> outcome events). similarly, <UNK> women (with <UNK> outcome events) in the thin database from england were used to develop hip fracture models. the model performance was evaluated on data from <UNK> women from scotland (with <UNK> outcome events). all data come from the same underlying computer system used to collect the patient level data, and thus splitting this large database geographically is a weak form of external validation (sometimes referred to as narrow validation), although given the health systems in the two countries are ultimately the same the case-mix is not dissimilar. in this instance, the validation is closer to an assessment of reproducibility than transportability <UNK> for convenience and simplicity, the models were developed using a subset of variables that were chosen from the total list of predictors contained in the <UNK> <UNK> and qfracture <UNK> risk prediction models. the cardiovascular disease models used age (continuous), sex (binary), family history of cardiovascular disease (binary), serum cholesterol (continuous), systolic blood pressure (continuous), body <UNK> mass index (continuous) and treated hypertension (binary). the hip fracture models used age (continuous), body mass index (continuous), townsend score (categorical), diagnosis of asthma (binary) and prescription of tricyclic antidepressants (binary). table i shows the distribution of the model variables in the thin data set for both outcomes. <UNK> resampling strategy a resampling study was performed to examine the impact of different strategies for handling continuous predictors in the development of prognostic models on the performance of the model when evaluated in a separate data set. two hundred samples were randomly drawn (with replacement) from the thin data set so that the number of events in each sample was fixed at <UNK> <UNK> <UNK> and <UNK> individuals in each sample were chosen by sampling a constant fraction of those who experienced the event and those who did not, according to the overall proportion of events in the thin data set, for the corresponding prognostic model outcome (cardiovascular disease or hip fracture). models were developed for each sample as described in sections <UNK> and <UNK> model performance (section <UNK> was evaluated on the same data used to derive the model, to give the apparent performance, and on separate data, for validation performance (section <UNK> <UNK> approaches for handling continuous predictors we considered three broad approaches for handling continuous predictors: <UNK> categorise each continuous predictor into equally sized groups, using the median value of the predictor to form two groups, the tertile values to form three groups, the quartile values to form four groups or the quintile values to form five groups. we further categorised the age predictor by grouping individuals into <UNK> or <UNK> age intervals. we also categorised the continuous predictors based on ‘optimal’ cut-points, using a cut-point that minimised the p-value from the logrank test for over <UNK> of the observations (the lower and upper <UNK> of observations removed) <UNK> <UNK> model each continuous predictor by assuming that it has a linear relationship with the outcome. <UNK> model each continuous predictor by assuming that it has a non-linear relationship with the outcome, using fractional polynomials <UNK> and restricted cubic splines <UNK> fractional polynomials are a set of flexible power transformations that describe the relationship between a continuous predictor and the outcome. fractional polynomials of degree one <UNK> and two <UNK> are defined as <UNK> þ¼ x <UNK> ; <UNK> þ¼ x <UNK> þ <UNK> ; <UNK> <UNK> þ <UNK> lnx; <UNK> ¼ <UNK> ¼ p  where the powers p, <UNK> <UNK> ∈ s = <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> ln x. we used the multivariable fractional polynomial (mfp) procedure in r to model potentially nonlinear effects while performing table i. characteristics of the individuals in the thin data set, used as predictors in the developed prognostic models; sd: standard deviation. variable cardiovascular disease hip fracture development (england) (n = <UNK> validation (scotland) (n = <UNK> development (england) (n = <UNK> validation (scotland) (n = <UNK> mean age in years (sd) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> men <UNK> <UNK> <UNK> <UNK> — — family history of cardiovascular disease <UNK> <UNK> <UNK> <UNK> — — mean serum cholesterol (sd) <UNK> <UNK> <UNK> <UNK> — — mean systolic blood pressure (sd) <UNK> <UNK> <UNK> <UNK> — — mean body mass index (sd) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> treated for hypertension <UNK> <UNK> <UNK> <UNK> — — diagnosis of asthma — — <UNK> <UNK> <UNK> <UNK> history of falls — — <UNK> <UNK> <UNK> <UNK> prescription of tricyclic antidepressants — — <UNK> <UNK> <UNK> <UNK> <UNK> variable selection <UNK> mfp formally tests for deviations from linearity using fractional polynomials. it incorporates a closed test procedure that preserves the ‘familywise’ nominal significance level. the default <UNK> with significance level of <UNK> was chosen to build the prognostic models. the restricted cubic splines method places knots along the predictor value range and between two adjacent knots, where the association between the predictor and the outcome is modelled using a cubic polynomial. beyond the outer two knots, the relationship between the predictor and outcome is modelled as a linear association. three to five knots are often sufficient to model the complex associations that are usually based on the percentiles of the predictor values. we used the rcs function in the rms package in r <UNK> prior to conducting the simulations, preliminary analyses suggested that four degrees of freedom should be used to model each continuous predictor with fractional polynomials (i.e., <UNK> models) and that three knots should be used in the restricted cubic splines approach. the models with the highest degrees of freedom (df) were those categorising the continuous predictors into fifths (cvd: df = <UNK> hip fracture: df = <UNK> model using fractional polynomials also had a maximum potential degrees of freedom also of <UNK> and <UNK> for the cvd and hip fracture models respectively. the fewest degrees of freedom were used by the models that assumed a linear relationship with the outcome, those dichotomising (cvd: df = <UNK> hip fracture: df = <UNK> see supplementary table <UNK> figure <UNK> shows how the relationship between the continuous predictors (age, systolic blood pressure, body mass index and total serum cholesterol) and cardiovascular disease when estimated, assuming linearity, nonlinearity (fractional polynomials and restricted cubic splines) and using categorisation (supplementary figure <UNK> shows the corresponding relationship for continuous age and body mass index with hip fracture). age is one of the main risk factors for many diseases, including cardiovascular disease <UNK> and hip fractures <UNK> we therefore examined models whereby all continuous predictors apart from age were assumed linear, whilst age was included using the approaches described above. all of the continuous predictors in the development and validation data sets were centred and scaled before modelling. scaling and centring reduce the chance of numerical underflow or overflow, which can cause inaccuracies and other problems in model estimation <UNK> the values used for transformation were obtained from the development data set and were also applied to the validation data set. <UNK> performance measures model performance was assessed in terms of discrimination, calibration, overall model performance and clinical utility. discrimination is the ability of a prognostic model to differentiate between people with different outcomes, such that those without the outcome have a lower predicted risk than those with the outcome. the survival models in this study were based on the time-to-event. discrimination was thus evaluated using harrell’s c-index, which is a generalisation of the area under the receiver operating characteristic curve for binary outcomes (e.g. logistic regression) <UNK> we graphically examined the calibration of the models at a single time point, <UNK> years, using the val. surv function in the rms library in r. for each random sample, hazard regression with linear splines was used to relate the predicted probabilities from the models at <UNK> years to the observed event times (and censoring indicators). the actual event probability at <UNK> years was estimated as a function of the estimate event probability at <UNK> years. we investigated the influence of the approach used to handle continuous predictors on calibration by overlaying plots of observed outcomes against predicted probabilities for each of the <UNK> random samples. we evaluated the clinical usefulness, or net benefit, of the models using decision curve analysis <UNK> net benefit is the difference between the number of true-positive results and the number of false-positive results, weighted by a factor that gives the cost of a false-positive relative to a false-negative result <UNK> and is defined as: net benefit ¼ true positives n false positives n pt <UNK> pt : the numbers of true and false positives were estimated using the kaplan–meier estimates of the percentage surviving at <UNK> years among those with calculated risks greater than the threshold probability. n is the total number of people. pt is the threshold on the probability scale that defines high risk and is used to weight false positives to false negative results. to calculate the net benefit for survival time data <UNK> subject to censoring <UNK> we defined x = <UNK> if an individual had a predicted probability from the model ≥ pt (the threshold probability), and x = <UNK> otherwise. s(t) is the kaplan–meier survival probability at time t (t = <UNK> years) and n is the number of individuals in the data set. the number of true positives is given by <UNK> | x = <UNK> × p(x = <UNK> × n and the number of false positives by (s(t) | x = <UNK> × p(x = <UNK> × n. a decision curve was produced by plotting across a range of pt values. <UNK> results tables ii and iii show the mean (standard deviation) of the c-index over the <UNK> samples (each containing <UNK> <UNK> <UNK> and <UNK> events) for the models produced by each approach that predict the hip fracture and cardiovascular outcomes, respectively. for the hip fracture models, there was a large difference of <UNK> between the mean c-index produced by the approaches that did not categorise the continuous predictors and the approach that dichotomised the continuous predictors at the median. the same c-index difference was observed in the apparent performance (i.e. the development performance) and the geographical validation performance. a similar pattern was observed for other performance measures including the d-statistic <UNK> <UNK> <UNK> and brier score (see supplementary tables <UNK> the differences in all the measures (c-index, d-statistic, <UNK> and brier score <UNK> all favoured the approaches that kept the variables continuous. the approaches that assumed a linear relationship between the predictor and outcome and the approaches that used either fractional polynomials or restricted cubic splines had similar results for all of the performance measures. the variability (as reflected in the sd) in the four model performance metrics was small relative to the mean value for all of the methods of handling continuous predictors. similar patterns in model performance were observed in the cardiovascular models. for example, a difference of <UNK> in the c-index between the approaches that assumed a linear or nonlinear relationship between the predictor and outcome and the approach that dichotomised at the median predictor value was observed in both the apparent performance and geographical validation. figure <UNK> shows the geographical validation calibration plots for the cardiovascular models. the models using fractional polynomials or restricted cubic splines were better calibrated than the models produced by other approaches, including the models that assumed a linear relationship between the predictor and outcome. a similar pattern was observed in the hip fracture models, although there was more similarity between the linear and nonlinear approaches (see supplemental figure <UNK> figure <UNK> shows the distribution of the predicted <UNK> cardiovascular risk for a subset of the approaches. the approaches that kept the measurements continuous (linear, fractional polynomial and restricted cubic splines) showed similar predicted risk spreads. the approaches that categorised the continuous predictors showed a noticeably wide predicted risk spread in those who did not experience the outcome, which was widest when the predictor was dichotomised, and showed a noticeable narrow spread for those who did experience the outcome. as the number of categories increases the spread of predicted risk in those who did not have an event decreases, whilst the spread of predicted risk in those who did have an event increases. models developed with smaller sample sizes <UNK> <UNK> and <UNK> events) showed higher performance measure values when they were evaluated using the development data set than the models developed figure <UNK> calibration plots of cardiovascular disease risk in the validation cohort <UNK> events). <UNK> with <UNK> events. this pattern was observed for both the hip fracture and cardiovascular disease models, regardless of the how continuous predictors were included in the models. in contrast, lower values were observed in the geographical validation of models developed with smaller sample sizes than the models developed with <UNK> events, suggesting overfitting. similarly, models developed with fewer events showed worse calibration on the geographical validation data than the models developed with <UNK> events. the models developed by categorising continuous predictors showed the greatest variability in performance. table iv shows the clinical consequences of the different strategies for handling the continuous predictors in the cardiovascular models, using dichotomising at the median predictor value as the reference method. similar findings were observed in the hip fracture models. over a range of probability thresholds (e.g. <UNK> to <UNK> an additional net <UNK> to <UNK> cardiovascular disease cases per <UNK> were found during geographical validation if models that implemented fractional polynomials or restricted cubic splines were used, rather than models that dichotomised all of the continuous predictors at the median without conducting any unnecessary treatment. for models that categorised continuous predictors, more additional cases per <UNK> found as the number of categories increases. the models that used fractional figure <UNK> boxplot of the predicted cardiovascular disease risks in the validation cohort <UNK> events). table iv. range of additional net cases per <UNK> found when using each approach for handling continuous predictors, compared with categorising all of the continuous predictors at the median. models developed using <UNK> outcome events. range of thresholds <UNK> to <UNK> for cardiovascular disease and <UNK> to <UNK> for hip fracture. model cardiovascular disease hip fracture development validation development validation linear <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> fractional polynomials <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> restricted cubic splines <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> age categories <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> age categories <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> thirds <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> fourths <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> fifths <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> to <UNK> <UNK> <UNK> polynomials or restricted cubic splines, or that assumed a linear relationship between the predictor and outcome all showed a higher net benefit, over a range of thresholds <UNK> to <UNK> than the categorising approaches (see supplementary figure <UNK> <UNK> discussion we examined the impact of the choice of approach for handling continuous predictors when developing a prognostic model and evaluating it on a separate data set. we developed models using data sets of varying size to illustrate the influence of sample size. the predictive ability of the models was evaluated on a large geographical validation data set, using performance measures that have been recommended for evaluation and reporting <UNK> we have demonstrated that categorising continuous predictors, particularly dichotomising at the median value of the predictor, produces models that have substantially weaker predictive performance than models produced with alternative approaches that retain the predictor on a continuous scale <UNK> it is not surprising that categorising continuous predictors leads to poor models, as it forces an unrealistic, biologically implausible and ultimately incorrect (step) relationship onto the predictor and discard information. it may appear to be sensible to use two or more quantiles (or similar apparently meaningful cutpoints, such as a particular age) to categorise an outcome into three or more groups, but this approach does not reflect the actual predictor–outcome relationship. individuals whose predictor values are similar but are either side of a cut-point are assigned different levels of risk, which has clear implications on how the model will be used in clinical practice. the fewer the cut-points, the larger the difference in risk between two individuals with similar predictor values immediately either side of a cut-point. we observed only small differences between the methods that retained the variables as continuous (assuming linearity or nonlinearity). larger differences would be expected if the relationship between a continuous variable and the outcome were markedly nonlinear. continuous predictors are often categorised as the approach is intuitive, simple to implement and researchers may expect it to improve model use and uptake. however, categorising comes at a substantial cost to the predictive performance. we believe that this cost is detrimental and counterproductive, as the resulting models have weak predictive accuracy and are unpopular for clinical use. if ease of use is required, we recommend leaving predictors as continuous during modelling and instead simplifying the final model, using a points system to allow finer calculation of an individual’s risk <UNK> we focused on exploring the differences in the predictive performance of models produced by each approach for handling continuous predictors. focusing on a fixed and small number of variables, we circumvented issues around variable selection procedures for which we anticipate the differences in performance to be more marked. using restricted cubic splines on very small data sets may produce wiggly functions, and fractional polynomials can result in poorly estimated tails. both problems lead to unstable models that have poor predictive ability when evaluated on separate data in a geographical validation. we observed very little difference in model performance when using either fractional polynomials or restricted cubic splines. similarly, when a data set is small, the centile values for dichotomising or categorising continuous predictor values may vary considerably from sample to sample, again leading to unstable models and poor predictive performance. we did, however, also examine the influence of smaller sample sizes on model performance and observed greater variability in model performance in small samples with few outcome events than in larger samples. furthermore, the apparent performance on smaller data sets was higher compared to using larger data set (which decreased as sample size increased), but at the expense of poorer performance in the geographical validation, where recent guidance suggests that a minimum of <UNK> events are required <UNK> systematic reviews have highlighted numerous methodological flaws, including small sample size, missing data and inappropriate statistical analyses in studies that describe the development or validation of prognostic models <UNK> contributing to why many studies have methodological shortcomings is because it is generally easy to create a (poorly performing) prognostic model. a model that has been developed using poor methods is likely to produce overoptimistic and misleading performance. as noted by andrew vickers, ‘we have a data set, and a statistical package, and add the former to the latter, hit a few buttons and voila, we have another paper’ <UNK> it is therefore both unsurprising and fortunate that most prognostic models never get used. however, some of these poorly performing or suboptimal prognostic models will undoubtedly be used, which may have undesirable clinical consequences. we have clearly demonstrated that categorising continuous predictor <UNK> <UNK> outcomes is an inadequate approach. investigators who wish to develop a new prognostic model for use on patients that is fit for purpose should follow the extensive methodological guidance on model building that discourages categorisation and use an approach that keeps the outcome continuous <UNK>

<|EndOfText|>

minimum sample size for developing a multivariable prediction model: part ii - binary and time-to-event outcomes when designing a study to develop a new prediction model with binary or time-to-event outcomes, researchers should ensure their sample size is adequate in terms of the number of participants (n) and outcome events (e) relative to the number of predictor parameters (p) considered for inclusion. we propose that the minimum values of n and e (and subsequently the minimum number of events per predictor parameter, epp) should be calculated to meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global shrinkage factor of <UNK> (ii) small absolute difference of ≤ <UNK> in the model's apparent and adjusted nagelkerke's <UNK> and (iii) precise estimation of the overall risk in the population. criteria (i) and (ii) aim to reduce overfitting conditional on a chosen p, and require prespecification of the model's anticipated cox-snell <UNK> which we show can be obtained from previous studies. the values of n and e that meet all three criteria provides the minimum sample size required for model development. upon application of our approach, a new diagnostic model for chagas disease requires an epp of at least <UNK> and a new prognostic model for recurrent venous thromboembolism requires an epp of at least <UNK> this reinforces why rules of thumb (eg, <UNK> epp) should be avoided. researchers might additionally ensure the sample size gives precise estimates of key predictor effects; this is especially important when key categorical predictors have few events in some categories, as this may substantially increase the numbers required. keywords binary and time-to-event outcomes, logistic and cox regression, multivariable prediction model, pseudo r-squared, sample size, shrinkage <UNK> introduction statistical models for risk prediction are needed to inform clinical diagnosis and prognosis in <UNK> for example, they may be used to predict an individual's risk of having an undiagnosed disease or condition (“diagnostic prediction model”), or to predict an individual's risk of experiencing a specific event in the future (“prognostic prediction model”). they are typically developed using a multivariable regression framework, such as logistic or cox (proportional hazards) regression, which provides an equation to estimate an individual's risk based on their values of multiple predictors (such as age and smoking, or biomarkers and genetic information). well-known examples are the wells score for predicting the presence of a pulmonary <UNK> ; the framingham risk score and <UNK> which estimate the <UNK> risk of developing cardiovascular disease (cvd); and the nottingham prognostic index, which predicts the <UNK> survival probability of a woman with newly diagnosed breast <UNK> researchers planning or designing a study to develop a new multivariable prediction model must consider sample size requirements for their development data set. our related paper considered this issue for prediction models of a continuous outcome using linear <UNK> here, we focus on binary and time-to-event outcomes, such as the risk of already having a pulmonary embolism, or the risk of developing cvd in the next <UNK> years. in this situation, the effective sample size is often considered to be the number of outcome events (eg, the number with existing pulmonary embolism, or the number diagnosed with cvd during follow-up). in particular, a well-used “rule of thumb” for sample size is to ensure at least <UNK> events per candidate predictor <UNK> where “candidate” indicates a predictor in the development data set that is considered, before any variable selection, for inclusion in the final model. note that, if a predictor is categorical with three of more categories, or continuous and modelled as a nonlinear trend, then including the predictor will require two or more parameters being included in the model. therefore, we refer to events per predictor parameter (epp) here, rather than events per variable. the <UNK> epp rule has generated much debate. some authors claim that the epp can sometimes be lowered below <UNK> in contrast, harrell generally recommends at least <UNK> <UNK> and others identify situations where at least <UNK> epp or up to <UNK> epp are <UNK> however, a concern is that any blanket rule of thumb is too simplistic, and that the number of participants required will depend on many intricate aspects, including the magnitude of predictor effects, the overall outcome risk, the distribution of predictors, and the number of events for each category of categorical <UNK> for example, courvoisier et <UNK> concluded that “there is no single rule based on epp that would guarantee an accurate estimation of logistic regression parameters.” a new sample size approach is needed to address this. in this article, we propose the sample size (n) and number of events (e) in the model development data set must, at the very least, meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global shrinkage factor of <UNK> (ii) small absolute difference of ≤ <UNK> in the model's apparent and adjusted nagelkerke's <UNK> and (iii) precise estimation of the overall risk or rate in the population (or similarly, precise estimation of the model intercept when predictors are mean centred). the values of n and e (and subsequently epp) that meet all three criteria provide the minimum values required for model development. criteria (i) and (ii) aim to reduce the potential for a developed model to be overfitted to the development data set at hand. overfitting leads to model predictions that are more extreme than they ought to be when applied to new individuals, and most notably occurs when the number of candidate predictors is large relative to the number of outcome events. a consequence is that a developed model's apparent predictive performance (as observed in the development data set itself) will be optimistic, and its performance in new data will usually be lower. therefore, it is good practise to reduce the potential for overfitting when developing a prediction <UNK> which criteria (i) and (ii) aim to achieve. in addition, criterion (iii) aims to ensure that the overall risk (eg, by a key time point for prediction) is estimated precisely, as fundamentally, before tailoring predictions to individuals, a model must be able to reliably predict the overall or mean risk in the target population. the article is structured as follows. section <UNK> introduces our proposed criterion (i), for which key concepts of a global shrinkage factor and the cox-snell <UNK> are <UNK> the latter needs to prespecified to utilise our sample size formula, and so in section <UNK> we suggest how realistic values of the cox-snell <UNK> can be obtained in advance of any data collection, eg, by using published information from an existing model in the same field, including values of the c statistic or alternative <UNK> measures. extension to criteria (ii) and (iii) is then made in section <UNK> section <UNK> then provides two examples, which demonstrate our sample size approach for diagnostic and prognostic models. section <UNK> raises a potential additional criteria to consider: ensuring precise estimates of key predictor effects, to help ensure precise predictions across the entire spectrum of predicted risk. section <UNK> concludes with discussion. <UNK> sample size required to minimise overfitting of predictor effects to adjust for overfitting during model development (and thereby improve the model's predictive performance in new individuals), statistical methods for penalisation of predictor effect estimates are available, where regression coefficients are shrunk toward zero from their usual estimated value (eg, from standard maximum likelihood <UNK> van houwelingen notes that “ … shrinkage works on the average but may fail in the particular unique problem on which the statistician is <UNK> therefore, it is important to minimise the potential for overfitting during model development, and this criterion forms the basis of our first sample size calculation. our approach is motivated by the concept of a global shrinkage factor (a measure of overfitting), and so we begin by introducing this, before then deriving a sample size formula. <UNK> concept of a global shrinkage for logistic and cox regression the concept of shrinkage (penalisation) was outlined in our accompanying <UNK> and is explained in detail <UNK> here, we focus on using a global shrinkage factor (s), sometimes referred to as a uniform shrinkage factor. consider a logistic regression model has been fitted using standard maximum likelihood estimation (ie, traditional and unpenalised estimation). subsequently, s can be estimated (eg, using <UNK> or via a closed-form solution; see section <UNK> and applied to the estimated predictor effects, so that the revised model is 𝑙𝑛 ( pi <UNK> − pi ) = 𝛼∗ + s ( 𝛽̂ <UNK> + 𝛽̂ <UNK> + 𝛽̂ <UNK> +···) . <UNK> here, pi is the outcome probability for the ith individual, the 𝛽̂ terms denote the original predictor effect estimates (ln odds ratios) from maximum likelihood, and 𝛼* is the intercept that has been re-estimated (after shrinkage of predictor effects) to ensure perfect calibration-in-the-large, such that, the overall predicted risk still agrees with the overall observed risk in the development data set (for details on how to do this, we refer to the works of <UNK> and <UNK> ). similarly, after fitting a proportional hazards (cox) regression model using standard maximum likelihood, the model can be revised using hi(t) = <UNK> ∗ exp ( s ( 𝛽̂ <UNK> + 𝛽̂ <UNK> + 𝛽̂ <UNK> +···)) , <UNK> where hi(t) is the hazard rate of the outcome over time (t) for the ith individual and ho(t) * is the baseline hazard function re-estimated (after shrinkage of predictor effects) to ensure the predicted and observed outcome rates agree for the development data set as whole. compared to the original (nonpenalised) models, the revised models <UNK> and <UNK> will shrink predicted probabilities away from zero and one, toward the overall mean outcome probability in the development data set. example of a global shrinkage factor van diepen et al developed a prognostic model for <UNK> mortality risk in patients with diabetes starting <UNK> they use a logistic regression framework, with backwards selection to choose predictors in a dataset of <UNK> patients with <UNK> deaths by <UNK> year, and the estimated model is shown in table <UNK> to examine overfitting, the authors use bootstrapping to estimate a global shrinkage factor of <UNK> indicating that the original model was slightly overfitted to the data. therefore, a revised prediction model was produced by multiplying the original 𝛽̂ coefficients (ln odds ratios) from the original logistic regression model by a global shrinkage factor of s = <UNK> table <UNK> example of global shrinkage applied to a prognostic model for <UNK> mortality risk in patients with diabetes starting <UNK> developed (unpenalised) model final (penalised) model adjusted for overfitting intercept 𝜶 𝜶 ̂ * <UNK> <UNK> predictor 𝜷 ̂ s𝜷 ̂ = <UNK> ̂ age (years) <UNK> <UNK> smoking <UNK> <UNK> macrovascular complications <UNK> <UNK> duration of diabetes mellitus (years) <UNK> <UNK> karnofsky scale <UNK> <UNK> haemoglobin level (g/dl) <UNK> <UNK> albumin level (g/l) <UNK> <UNK> <UNK> expressing sample size in terms of a global shrinkage factor bootstrapping is an excellent way to calculate the shrinkage factor postestimation, but (as it is a resampling method) is not useful for us in advance of data collection. an alternative approach to calculating a global shrinkage factor is to use the closed form “heuristic” shrinkage factor of van houwelingen and le <UNK> defined by svh = <UNK> − p lr , <UNK> where p is the total number of predictor parameters for the full set of candidate predictors (ie, all those considered for inclusion in the model) and lr is the likelihood ratio (chi-squared) statistic for the fitted model defined as lr = <UNK> (ln lnull − ln lmodel) , <UNK> where ln lnull is the log-likelihood of a model with no predictors (eg, intercept-only logistic regression model), and ln lmodel is the log-likelihood of the final model. in our related paper on linear regression, we used the copas shrinkage estimate that is similar to equation <UNK> but with p replaced by p + <UNK> in our experience, svh performs better for generalised linear models than the copas estimate, with svh further from <UNK> and closer to the corresponding estimate obtained from bootstrapping. copas also notes that, unlike for linear regression, a formal justification for replacing p by p + <UNK> in equation <UNK> has not been proved for logistic <UNK> hence, we use equation <UNK> as our shrinkage estimate (ie, our measure of overfitting) for logistic and cox regression models, which now motivates our sample size approach to meet criterion (i). first, let us re-express the right-hand side of equation <UNK> in terms of sample size (n), number of candidate predictor parameters (p), and the cox-snell generalised <UNK> <UNK> the latter is also known as the maximum likelihood <UNK> the likelihood ratio <UNK> or magee's <UNK> <UNK> and it provides a generalisation (eg, to logistic and cox regression models) of the well-known proportion of variance explained for linear regression models. let us use <UNK> cs_app to denote the apparent (“app”) estimate of a prediction model's cox-snell (“cs”) <UNK> performance as obtained from the model development data set. it can be shown (eg, see the works of <UNK> or hendry and <UNK> that the lr statistic can be expressed in terms of the sample size (n) and <UNK> cs_app as follows: lr = −n ln ( <UNK> − <UNK> cs_app) . <UNK> this leads to the cox-snell generalised definition of the apparent <UNK> expressed in terms of the lr value for any regression model, including logistic and cox regression <UNK> cs_app = <UNK> − exp (−lr n ) . <UNK> applying equation <UNK> within equation <UNK> the van houwelingen and le cessie shrinkage factor becomes svh = <UNK> + p n ln ( <UNK> − <UNK> cs_app) . <UNK> <UNK> criterion (i): calculating sample size to ensure a shrinkage factor ≥ <UNK> equation <UNK> provides a closed-form solution for the expected shrinkage conditional on n, p, and <UNK> cs_app. therefore, if we could specify a realistic value for <UNK> cs_app in advance of our study starting, we could identify values of n and p that correspond to a desired shrinkage factor (eg, <UNK> thus informing the required sample size. however, a major problem is that <UNK> cs_app is a postestimation measure of model fit, whereas for a sample size calculation, this needs to be specified in advance of collecting the data when designing a new study. furthermore, due to overfitting in the model development data set, the observed <UNK> cs_app is generally an upwardly biased (optimistic) estimate of the cox-snell <UNK> as it is estimated in the same data used to develop the model. thus, in new data, the actual cox-snell <UNK> peformance is likely to be lower. therefore, we need to re-express svh in terms of <UNK> cs_adj, an adjusted (approximately unbiased) estimate of the model's expected <UNK> cs performance in new individuals from the same population. in other words, <UNK> cs_adj is a modification of <UNK> cs_app to adjust for optimism (caused by overfitting) in the model development data set. for generalised linear models such as logistic regression, mittlboeck and heinzl suggest that <UNK> cs_adj can be obtained <UNK> <UNK> cs_adj = <UNK> cs_app <UNK> as the expected value of this <UNK> cs_adj corresponds to the underlying population <UNK> by rearranging equation <UNK> we can express <UNK> cs_app in terms of <UNK> cs_adj <UNK> cs_app = <UNK> cs_adj svh . <UNK> applying equation <UNK> within equation <UNK> we can now express svh in terms of <UNK> cs_adj, rather than <UNK> cs_app svh = <UNK> + p n ln ( <UNK> − <UNK> cs_adj svh ). <UNK> finally, a simple rearrangement of equation <UNK> leads to a closed-form solution for the required sample size to develop a prediction model conditional on p, svh and <UNK> cs_adj n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ). <UNK> for example, for developing a new logistic regression model based on up to <UNK> candidate predictor parameters with an anticipated <UNK> cs_adj of at least <UNK> then to target an expected shrinkage of <UNK> we need a sample size of n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> and thus <UNK> individuals. <UNK> translating the calculated sample size to the number of events and epp it may be surprising that the overall outcome proportion (or overall outcome rate) is not directly included in the right-hand side of the sample size equation <UNK> especially because the total number of events, e, (which depends on the outcome proportion or rate) is often considered the effective sample size for binary and time-to-event <UNK> however, the outcome proportion (rate) is indirectly accounted for in the sample size calculation via the chosen <UNK> cs_adj, as the maximum value of <UNK> cs_adj for the intended population of the model depends on the overall outcome proportion (rate) for that population. as the outcome proportion decreases, the maximum value of <UNK> cs decreases. this is explained further in section <UNK> therefore, after n is derived from the sample size equation <UNK> e can be obtained by combining the calculated n with the outcome proportion (rate) for the intended population. similarly, epp can be obtained. for example for binary outcomes, e = n𝜙 and epp = n𝜙/p, where 𝜙 is the overall outcome proportion in the target population (ie, the overall prevalence for diagnostic models, or the overall cumulative incidence by a key time point for prognostic models). in our aforementioned hypothetical example, where <UNK> subjects were needed based on an <UNK> cs_adj of <UNK> and svh of <UNK> then if the intended setting has 𝜙 of <UNK> (ie, overall outcome risk is <UNK> the required e = <UNK> × <UNK> = <UNK> with <UNK> predictor parameters, the required epp = <UNK> × <UNK> = <UNK> however, if the intended setting has 𝜙 of <UNK> then e = <UNK> and epp = <UNK> the big change in epp is because, although the chosen value of <UNK> cs_adj is fixed at <UNK> the maximum value of <UNK> cs is much higher for the setting with the higher outcome proportion. we can explain this further using nagelkerke's “proportion of total variance <UNK> which is calculated as <UNK> cs_adj∕ <UNK> cs). if two models have the same <UNK> cs_adj (say at <UNK> as in the aforementioned examples), then nagelkerke's measure of predictive performance will be lower for the model whose setting has a higher outcome proportion, as the <UNK> cs) is larger in that setting. models with lower performance have larger overfitting <UNK> and therefore require larger epp to minimise overfitting than models with high performance. hence, explaining why epp was larger when 𝜙 was <UNK> compared with <UNK> in the aforementioned example. this highlights that a blanket rule of thumb (such as at least <UNK> epp) is unlikely to be sensible to meet criterion (i), as the actual epp depends on the setting/population of interest (which dictates the overall outcome proportion or rate) and expected model performance. <UNK> how to prespecify r𝟐 cs_adj based on previous information our sample size proposal in equation <UNK> requires researchers to provide a value for the model's <UNK> cs_adj, that is, to prespecify the anticipated cox-snell <UNK> value if the model was applied to new individuals. how should this be done? we recommend using <UNK> cs_adj values from previous prediction model studies for the same (or similar) population, considering the same (or similar) outcomes and time points of interest. for example, the researcher could consult systematic reviews of existing models and their performance, which are also increasingly <UNK> or registries that record the prediction models available in a particular <UNK> often, a new prediction model is developed specifically to update or improve upon the performance of an existing model, by using additional predictors. then, the existing model's <UNK> cs_adj could be used as a lower bound for the new model's anticipated <UNK> cs_adj. in this situation, if the apparent cox-snell estimate, <UNK> cs_app, is available in an article describing the development of the existing model, then its <UNK> cs_adj can be derived using equation <UNK> as long as the study's n and p can also be obtained. in addition, as in van diepen et al's example (table <UNK> a global shrinkage factor may be reported directly for an existing model development study, and if so, <UNK> cs_adj can be derived from a simple rearrangement of equation <UNK> again as long as the study's n and p are also available. note that, if <UNK> cs_app is available from an external validation study of an existing model, there is no need for adjustment (ie, <UNK> cs_app = <UNK> cs_adj), as the validation dataset provides a direct estimate of the model's performance in new individuals (free from overfitting concerns as there is no model development therein). other options to obtain <UNK> cs_adj from the existing literature are now described. for guidance on choosing an <UNK> cs_adj value in the absence of any prior information, please see our discussion. <UNK> using the lr statistic to derive the cox-snell r𝟐 adj if the <UNK> cs_app or <UNK> cs_adj is not available in the publication of an existing model, the lr value may be reported, which would allow <UNK> cs_app to be derived using equation <UNK> then svh for the model derived using equation <UNK> (assuming the model's n and p are also provided), and finally <UNK> cs_adj using equation <UNK> sometimes the log-likelihood of the final model (lnlmodel) is reported, but not the lr value itself. in this situation, the researcher should calculate ln lnull based on other information in the article, and then calculate lr using equation <UNK> thus allowing <UNK> cs_app and <UNK> cs_adj to be derived using equations <UNK> and <UNK> respectively. for example, in a logistic regression model, the loglnull value can be calculated using ln lnull = e ln (e n ) + (n − e)ln ( <UNK> − e n ) , <UNK> where e is the total number of outcome events. of course, this assumes e and n are actually available in the article. similarly, for an exponential survival model (equivalent to a poisson model with ln (survival time) as an offset), the ln lnull can be calculated using ln lnull = e ln(𝜆) + 𝜆t = e ln (e t ) + e <UNK> as long as 𝜆 (the constant hazard rate), e (the total number of events), and t (the total time at risk, eg, total person-years) are available in the article. note that, for survival models, packages such as sas and stata usually add a constant to the reported log-likelihood to ensure it remains the same value regardless of the time scale used. for example, stata adds the sum of the ln (survival times) for the noncensored individuals to the reported ln lmodel and ln lnull, and so this constant must be either consistently used or consistently removed in each of ln lmodel and ln lnull when deriving the lr value. <UNK> using other <UNK> statistics to derive r𝟐 cs_adj sometimes other <UNK> statistics are reported for logistic and survival models, rather than the cox-snell version specified in equation <UNK> in particular, because <UNK> cs_app has a maximum value less than <UNK> nagelkerke's <UNK> is sometimes <UNK> which divides <UNK> cs_app by the maximum value defined by <UNK> − exp <UNK> ln lnull n ) , as follows: <UNK> nagelkerke_app = <UNK> cs_app max ( <UNK> cs_app) = <UNK> cs_app <UNK> − exp <UNK> ln lnull n ). <UNK> recall that ln lnull is derivable from other information, eg, using equations <UNK> or <UNK> for logistic and exponential (poisson) models, respectively. when nagelkerke's <UNK> ln lnull, and n are available, the <UNK> cs_app can be calculated by rearranging equation <UNK> to give <UNK> cs_app = <UNK> nagelkerke_app ( <UNK> − exp <UNK> ln lnull n )) , <UNK> and then <UNK> cs_adj calculated via equation <UNK> another measure sometimes reported is mcfadden's <UNK> <UNK> <UNK> mcfadden_app = <UNK> − ln lmodel ln lnull . <UNK> as ln lnull is often obtainable (see previous equation), when <UNK> mcfadden_app is reported, we can rearrange equation <UNK> to obtain ln lmodel, and subsequently derive the lr statistic using equation <UNK> the cox-snell <UNK> cs_app from equation <UNK> svh from equation <UNK> (assuming the model's n and p are also provided), and finally <UNK> cs_adj via equation <UNK> for proportional hazards survival models, o'quigley et al suggested to modify <UNK> cs_app by replacing n with the number of events (e) <UNK> <UNK> óquigley_app = <UNK> − exp (−lr e ) . <UNK> therefore, if <UNK> óquigley_app and e were reported, the lr value could be found using lr = −e ln ( <UNK> − <UNK> óquigley_app) , <UNK> and subsequently, <UNK> cs_app can be obtained using equation <UNK> svh using equation <UNK> and finally <UNK> cs_adj using equation <UNK> another measure increasingly being reported for survival models is royston's measure of explained <UNK> which is given by <UNK> royston_app = <UNK> óquigley_app <UNK> óquigley_app + <UNK> <UNK> ) <UNK> − <UNK> óquigley_app). <UNK> when <UNK> royston_app is reported it can be used to obtain <UNK> óquigley_app by rearranging equation <UNK> as <UNK> óquigley_app = <UNK> <UNK> <UNK> royston_app ( <UNK> − <UNK> <UNK> ) <UNK> royston_app − <UNK> . <UNK> this subsequently allows lr, <UNK> cs_app, svh and then <UNK> cs_adj to be derived as explained previously. a similar measure to <UNK> royston is royston and sauerbrei's <UNK> d, <UNK> which can be derived from their proposed d statistic (the ln(hazard ratio) comparing two groups defined by the median value of the model's risk score in the population of application) <UNK> d_app = 𝜋 <UNK> <UNK> <UNK> <UNK> + 𝜋 <UNK> <UNK> . <UNK> in examples shown by <UNK> <UNK> royston_app and <UNK> d_app are reasonably similar, and thus, we tentatively suggest <UNK> d_app as a proxy for <UNK> royston_app when only <UNK> d_app (or d) is reported; though, we recognise that further research is needed on the link between <UNK> d_app and <UNK> royston. table <UNK> predicted values of the d statistic and <UNK> d from equation <UNK> for selected values of the c statistic (values taken from table <UNK> in the work of jinks et <UNK> c dr𝟐 d cdr𝟐 d <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> using values of the c statistic to derive r𝟐 cs_adj jinks et al also proposed the following equation, based on empirical evidence, for predicting royston's d (and thus subsequently <UNK> d_app) when only the c statistic is reported for a survival <UNK> d = <UNK> − <UNK> + <UNK> − <UNK> <UNK> . <UNK> table <UNK> provides values of d (and corresponding values of <UNK> d_app from equation <UNK> predicted from equation <UNK> for selected values of the c statistic, as taken from the work of jinks et <UNK> thus, if only the c statistic is reported, we can use equation <UNK> to predict royston's d statistic and calculate <UNK> d_app (using equation <UNK> as a proxy to <UNK> royston_app, and then <UNK> óquigley_app, lr, <UNK> cs_app and finally <UNK> cs_adj computed sequentially using the equations given previously. further evaluation of the performance of jinks' formula is required, eg, using simulation and across settings with different cumulative outcome incidences. indeed, based on figure <UNK> in the work of jinks et <UNK> the potential error in the predictions of d appears to increase as c increases, and is about +/− <UNK> when c is <UNK> nevertheless, equation <UNK> serves as a good starting point and works well in our applied example (see section <UNK> further research is also needed to ascertain how to predict <UNK> cs from other measures, such as somer's d statistic. <UNK> the anticipated value of r𝟐 cs_adj may be small it is important to emphasise that the cox-snell, <UNK> cs, values for logistic and survival models are usually much lower than for linear regression models, with values often less than <UNK> a key reason is that (unlike for linear regression) the <UNK> cs_app has a maximum value less than <UNK> defined by max( <UNK> cs_app) = <UNK> − <UNK> ln lnull n ) . <UNK> this is because ln lnull is itself bounded for binary and time-to-event outcomes (see equations <UNK> and <UNK> for example, for a logistic regression model with an outcome proportion of <UNK> using equation <UNK> and an arbitrary sample size of <UNK> we have ln lnull = e ln (e n ) + (n − e)ln ( <UNK> − e n ) = <UNK> ln ( <UNK> <UNK> ) + <UNK> − <UNK> ( <UNK> − <UNK> <UNK> ) = <UNK> and therefore, using equation <UNK> max( <UNK> cs_app) = <UNK> − <UNK> ln lnull n ) = <UNK> − <UNK> <UNK> ) = <UNK> however, for an outcome proportion of <UNK> the <UNK> cs_app) is <UNK> and for an outcome proportion of <UNK> the <UNK> cs_app) is <UNK> therefore, especially in situations where the outcome proportion is low, researchers should anticipate a model with a (seemingly) low <UNK> cs_app value, and subsequently a low <UNK> cs_adj value. low values of <UNK> cs_app or <UNK> cs_adj do not necessarily indicate poor model performance. consider the following three examples. first, poppe et al used a cox regression to develop a model (“predict-cvd”) to predict the risk of future cvd events within two years in patients with atherosclerotic <UNK> and directly report an <UNK> cs_app of <UNK> however, the corresponding c statistic is <UNK> which shows discriminatory magnitude typical of many prognostic models used in practice. second, hippisley-cox and coupland use the qresearch database to produce three models (qdiabetes) that estimates the risk of future diabetes in a general <UNK> in their validation of their “model a,” there were <UNK> <UNK> incident cases of diabetes recorded in <UNK> <UNK> <UNK> women <UNK> cases per <UNK> person-years) during follow-up, and the reported <UNK> royston_app was <UNK> using the approach described previously to convert <UNK> royston to lr, this leads to a <UNK> cs_app of <UNK> however, the corresponding d statistic of <UNK> and c statistic of <UNK> are large. third, in a risk prediction model for venous thromboembolism (vte) in women during the first <UNK> weeks after <UNK> <UNK> cs_app was <UNK> due to the extremely low event risk <UNK> per <UNK> <UNK> deliveries), but the model still had important discriminatory ability as the corresponding c statistic was <UNK> <UNK> additional sample size criteria criterion (i) focuses on shrinkage of predictor effects, which is a multiplicative measure of overfitting (ie, on the relative scale). harrell suggests to also evaluate overfitting on the absolute scale and to check key model parameters are estimated <UNK> we now address this with two further criteria. <UNK> criterion (ii): ensuring a small absolute difference in the apparent and adjusted r𝟐 nagelkerke our second criterion for minimum sample size is to ensure a small absolute difference (𝛿) between the model's apparent and adjusted proportion of variance explained. we suggest using nagelkerke's <UNK> for this purpose as, unlike the cox-snell <UNK> value, it can range between <UNK> and <UNK> and so a small difference (say ≤ <UNK> can be ubiquitously defined. based on equation <UNK> the difference in the apparent and adjusted nagelkerke's <UNK> can be defined as <UNK> nagelkerke_app − <UNK> nagelkerke_adj = <UNK> cs_app max ( <UNK> cs_app) − <UNK> cs_adj max ( <UNK> cs_app) = <UNK> cs_adj svh − <UNK> cs_adj max ( <UNK> cs_app) = <UNK> cs_adj <UNK> − s𝑉 𝐻 ) svh max ( <UNK> cs_app), <UNK> where <UNK> cs_app) = <UNK> − exp <UNK> ln lnull n ) , as shown in equation <UNK> therefore, to meet sample size criterion (ii) and ensure the difference is less than a small value (say, 𝛿), we require <UNK> csadj <UNK> − svh) svh max ( <UNK> csapp) ≤ 𝛿. <UNK> we generally recommend 𝛿 is ≤ <UNK> such that the optimism is nagelkerke's percentage of variation explained is ≤ <UNK> rearranging equation <UNK> we find that <UNK> − svh) svh ≤ 𝛿 max ( <UNK> csapp) <UNK> csadj , and therefore, svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp). <UNK> equation <UNK> allows the researcher to calculate the required svh to satisfy criterion (ii), conditional on prespecifying the model's anticipated <UNK> cs_adj (as they did for criterion (i)) and also the value of <UNK> csapp ) as outlined for equation <UNK> then, sample size equation <UNK> can be used to derive the sample size needed to satisfy criterion (ii). this is only necessary when the calculated value of svh from equation <UNK> is larger than that chosen for criterion (i), as then the sample size required to meet criterion (ii) will be larger than that for criterion (i). for example, consider the development of a logistic regression model with anticipated <UNK> cs_adj of at least <UNK> and in a setting with the outcome proportion of <UNK> such that the <UNK> cs_app) is <UNK> then, to ensure 𝛿 is ≤ <UNK> we require svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp) = <UNK> <UNK> + <UNK> × <UNK> = <UNK> therefore, svh must be at least <UNK> to meet criterion (ii). as this is lower than the recommended value of at least <UNK> to meet criterion (i), no further work is required. however, had the anticipated <UNK> cs_adj been <UNK> then svh ≥ <UNK> <UNK> + <UNK> × <UNK> = <UNK> as this is higher than <UNK> we would need to reapply sample size equation <UNK> using <UNK> rather than <UNK> to obtain a sample size that meets both criteria (i) and (ii). <UNK> criterion (iii): ensure precise estimate of overall risk (model intercept) for logistic and time-to-event models, it is fundamental that the available sample size can precisely estimate the overall risk in the population by key time-points of interest. one way to examine this is to calculate the margin of error in outcome proportion estimates (𝜙̂) for a null model (ie, no predictors included). for example, for a binary outcome, an approximate <UNK> confidence interval for the overall outcome proportion is 𝜙̂ ± <UNK> <UNK> − 𝜙̂) n . therefore, the absolute margin of error (𝛿) is <UNK> n , which leads to n = <UNK> 𝛿 <UNK> <UNK> − 𝜙̂) . <UNK> this is largest when the outcome proportion is <UNK> we require <UNK> individuals to ensure a margin of error ≤ <UNK> when the true value is <UNK> however, we recommend a more stringent margin of error ≤ <UNK> which, when the outcome proportion is <UNK> requires n = <UNK> <UNK> <UNK> <UNK> − <UNK> = <UNK> and thus, <UNK> participants (and hence, about <UNK> events) are required. if the outcome proportion is <UNK> then we require <UNK> subjects to ensure a margin of error ≤ <UNK> whilst an outcome proportion of <UNK> requires <UNK> subjects. these sample sizes aim to ensure precise estimation of the overall risk in the population of interest. strictly speaking, we are more interested in precise estimation of the mean risk in an actual model including multiple predictors. if we centre predictors at their mean value, then the model's intercept is the logit risk for an individual with mean predictor values. the corresponding risk for this individual will often be very similar (though not identical) to the mean risk in the overall population. furthermore, the variance of the estimated risk for this individual will be approximately <UNK> n .* *as obtained by inversing the information matrix <UNK> and replacing individual variances defined by <UNK> with a constant variance defined by <UNK> − 𝜙̂). thus, it follows that equation <UNK> is also a good approximation to the sample size required to precisely estimate the mean risk in a model containing predictors centred at their mean. for time-to-event data, we could consider the precision of the estimated cumulative incidence (outcome risk) at a key time point of interest. a simple (and therefore practical) approach is to assume an exponential survival model, for which the estimated cumulative incidence function is f(t) = <UNK> − exp(−𝜆̂ t), where 𝜆̂ is the estimated rate (number of events per person-year). an approximate <UNK> confidence interval for the estimated f(t) is <UNK> ( − ( 𝜆̂ ± <UNK> t ) t ) , where t is the total person-years of follow-up. therefore, to ensure a small absolute margin of error, such that the lower and upper bounds of the confidence interval are ≤ 𝛿 (eg, <UNK> of the true value, we must ensure both the following are satisfied: − exp ( − ( 𝜆̂ + <UNK> 𝜆̂ t ) t ) + exp(−𝜆̂ t) ≤ 𝛿 − exp(−𝜆̂ t) + exp ( − ( 𝜆̂ − <UNK> 𝜆̂ t ) t ) ≤ 𝛿. <UNK> for example, for a constant event rate of <UNK> <UNK> events per <UNK> person-years), then by <UNK> years, the outcome risk is <UNK> = <UNK> − exp <UNK> × <UNK> = <UNK> then, <UNK> person-years of follow-up (and thus <UNK> × <UNK> ≈ <UNK> events) are needed to provide a confidence interval, which has a maximum absolute error of <UNK> from the true value. that is, <UNK> − exp ( − ( 𝜆̂ ± <UNK> 𝜆̂ t ) t ) = <UNK> − exp ( − ( <UNK> ± <UNK> <UNK> <UNK> <UNK> = <UNK> to <UNK> thus, equation <UNK> is satisfied, as both the lower and upper bounds are ≤ <UNK> of the true value of <UNK> more generally, to avoid assuming simple survival distributions like the exponential, harrell suggests using the dvoretzky-kiefer-wolfowitz inequality to estimate the probability of a chosen margin of error anywhere in the estimated cumulative incidence <UNK> <UNK> worked examples to summarise our sample size approach for researchers, we provide a step-by-step guide in figure <UNK> the sample size (and corresponding number of events and epp) that meets criteria (i) to (iii) provides the minimum sample size required for model development. we now present two worked examples to illustrate our approach. <UNK> a diagnostic prediction model for chronic chagas disease our first example considers the minimum sample size required for developing a diagnostic model for predicting a binary outcome (disease: yes or no). brasil et al developed a logistic regression model containing <UNK> predictor parameters for predicting the risk of having chronic chagas disease in patients with suspected chagas <UNK> upon external validation in a cohort of <UNK> participants containing <UNK> with chagas disease, the model had an estimated c statistic of <UNK> and an <UNK> nagelkerke_app of <UNK> consider that a researcher wants to update this model and improve the predictive performance. our sample size approach can be applied as follows. <UNK> steps <UNK> and <UNK> identifying values for p, r𝟐 cs_adj, and max(r𝟐 cs_app) assume that the researcher has identified (eg, based on recent studies) <UNK> additional predictor parameters that they wish to add to the original model. thus, in total, the number of predictor parameters, p, is <UNK> the next step is to identify a sensible value for the anticipated cox-snell <UNK> adj. to achieve this, we can convert the <UNK> nagelkerke_app value for brasil's existing model into a <UNK> cs_app value. assume the disease prevalence is <UNK> as in the brasil validation study, and use equation <UNK> to calculate the log-likelihood for the null model in brasil's validation study ln lnull = e ln (e n ) + (n − e)ln ( <UNK> − e n ) = <UNK> ln ( <UNK> <UNK> ) + <UNK> − <UNK> ( <UNK> − <UNK> <UNK> ) = <UNK> figure <UNK> summary of the steps involved in calculating the minimum sample size required for developing a multivariable prediction model for binary or time-to-event outcomes hence, the <UNK> csapp ) = <UNK> − exp <UNK> ln lnull n ) = <UNK> − exp <UNK> <UNK> ) = <UNK> now, we can use equation <UNK> to obtain <UNK> cs_app = <UNK> nagelkerke_app ( max ( <UNK> csapp)) = <UNK> × <UNK> = <UNK> this apparent cox-snell value of <UNK> can be directly used as an estimate of the model's <UNK> cs_adj, as it was obtained in a different data set to that used for model development. therefore no adjustment is needed, because <UNK> cs_app= <UNK> cs_adj here. <UNK> step <UNK> criterion (i) - ensuring a global shrinkage factor of <UNK> let us assume <UNK> is a lower bound for the <UNK> cs_adj of our new model. we now use equation <UNK> to estimate the sample size required to ensure an expected shrinkage factor (svh = <UNK> conditional on a number of predictor parameters (p= <UNK> n = p (svh − <UNK> ( <UNK> − <UNK> csadj svh ) = <UNK> <UNK> − <UNK> ( <UNK> − <UNK> <UNK> ) = <UNK> thus, <UNK> participants are required to meet criterion (i). <UNK> step <UNK> criterion (ii) - ensuring a small absolute difference in the apparent and adjusted r𝟐 nagelkerke to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of <UNK> or less in the apparent and adjusted <UNK> nagelkerke. using equation <UNK> we obtain svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp) = <UNK> <UNK> + <UNK> × <UNK> = <UNK> this is more stringent than the <UNK> assumed for criterion (i). therefore, we need to reapply equation <UNK> to estimate the sample size required conditional on svh = <UNK> (rather than <UNK> n = p (svh − <UNK> ( <UNK> − <UNK> csadj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> therefore, <UNK> subjects are required to meet criterion (ii), exceeding the <UNK> subjects required for criterion (i). <UNK> step <UNK> criterion (iii) - ensure precise estimate of overall risk (model intercept) assuming the prevalence of chagas disease is <UNK> (as observed from the brasil validation study), then to ensure we estimate this with a margin of error ≤ <UNK> we require (using equation <UNK> n = <UNK> <UNK> <UNK> <UNK> <UNK> − <UNK> = <UNK> and thus <UNK> subjects. this is far fewer than the sample size required to meet criteria (i) and (ii). <UNK> step <UNK> minimum sample size that ensures all criteria are met the largest sample size required was <UNK> subjects to meet criterion (ii), and so this provides the minimum sample size required for developing our new model. it corresponds to <UNK> × <UNK> = <UNK> events, and an epp of <UNK> = <UNK> which is considerably lower than the “epp of at least <UNK> rule of thumb. <UNK> a prognostic model to predict a recurrence of vte our second example considers the sample size required to develop a prognostic model with a time-to-event outcome. ensor et al developed a prognostic time-to-event model for the risk of a recurrent vte following cessation of therapy for a first <UNK> the sample size was <UNK> participants, with a median follow-up of <UNK> months, a total of <UNK> person-years of follow-up, and <UNK> <UNK> of) individuals had a vte recurrence by end of <UNK> the model included predictors of age, gender, site of first clot, d-dimer level, and the lag time from cessation of therapy until measurement of d-dimer (often around <UNK> days). these predictors corresponded to six parameters in the model, which was developed using the flexible parametric survival modelling framework of royston and <UNK> and royston and <UNK> although ensor's model performed well on average, the model's predicted risks did not calibrate well with the observed risks in some <UNK> therefore, new research is needed to update and extend this model, eg, by including additional predictors. we now identify suitable sample sizes to inform such research. <UNK> steps <UNK> and <UNK> identifying values for p, r𝟐 cs_adj and max(r𝟐 cs_app) assume that there are <UNK> potential predictor parameters for inclusion in the new model, and thus, p = <UNK> we next need to identify suitable values for <UNK> cs_adj and <UNK> cs_app). calculating max(r𝟐 cs_app) for the ensor model, <UNK> cs_app was not reported but we should expect it to be quite small because the maximum value of <UNK> cs_app is low. for example, assuming (for simplicity) an exponential survival model was fitted to the ensor data, then using equation <UNK> we have ln lnull = e ln (e t ) + e = <UNK> <UNK> + <UNK> = <UNK> and therefore, using equation <UNK> max ( <UNK> cs_app) = <UNK> − exp <UNK> ln lnull n ) = <UNK> − exp <UNK> × <UNK> <UNK> ) = <UNK> thus, <UNK> cs_app) is considerably less than <UNK> obtaining a sensible value for r𝟐 cs_adj from the study authors as <UNK> cs_app was not reported for the ensor model, we need to obtain it. we contacted the original authors who told us their model's <UNK> cs_app was <UNK> in the development data set. thus, let us use this value to derive <UNK> adj from equation <UNK> based on ensor's sample size of <UNK> and six predictor parameters, we obtain <UNK> cs_adj = <UNK> cs_app = ⎛ ⎜ ⎜ ⎜ ⎝ <UNK> + p n ln ( <UNK> − <UNK> cs_app) ⎞ ⎟ ⎟ ⎟ ⎠ <UNK> cs_app = ( <UNK> + <UNK> <UNK> ln <UNK> − <UNK> ) <UNK> = <UNK> hence, when developing a new model in this field, we could assume <UNK> is a lower bound for the expected <UNK> cs_adj of the new model. this corresponds to nagelkerke's proportion variation explained of <UNK> <UNK> cs_app) ≈ <UNK> = <UNK> (or <UNK> calculating a sensible value for r𝟐 cs_adj from other reported information for illustration, we also consider how <UNK> cs_app could have been estimated indirectly from other available information. the model's reported c statistic was <UNK> and so we can use equation <UNK> to predict the corresponding d statistic d = <UNK> (c − <UNK> + <UNK> − <UNK> <UNK> = <UNK> <UNK> − <UNK> + <UNK> − <UNK> <UNK> = <UNK> the corresponding <UNK> d_app can be derived from equation <UNK> <UNK> d_app = 𝜋 <UNK> <UNK> <UNK> <UNK> + 𝜋 <UNK> <UNK> = 𝜋 <UNK> <UNK> <UNK> <UNK> + 𝜋 <UNK> <UNK> = <UNK> taking <UNK> d_app as a proxy for <UNK> royston_app, we can then use equation <UNK> to obtain <UNK> óquigley_app = <UNK> <UNK> <UNK> royston_app ( <UNK> − <UNK> <UNK> ) <UNK> royston_app − <UNK> = <UNK> <UNK> <UNK> ( <UNK> − <UNK> <UNK> ) <UNK> − <UNK> = <UNK> next, we can use <UNK> óquigley_app and the number of reported events (e = <UNK> to derive the lr statistic from equation <UNK> lr = −e ln ( <UNK> − <UNK> ó quigleyapp) = <UNK> ln <UNK> − <UNK> = <UNK> using equation <UNK> this corresponds to <UNK> cs_app = <UNK> − exp (−lr n ) = <UNK> − exp <UNK> <UNK> ) = <UNK> thus, based on using the reported c statistic, an indirect estimate of the <UNK> cs_app is <UNK> for the ensor model. this is reassuringly close to the estimate of <UNK> provided directly by the study authors. <UNK> step <UNK> criterion (i) - ensuring a global shrinkage factor of <UNK> equation <UNK> can now be applied to derive the required sample size to meet criterion (i). using an <UNK> cs_adj of <UNK> for a model with <UNK> predictor parameters and a targeted expected shrinkage of <UNK> the sample size required is n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> and thus <UNK> participants. <UNK> step <UNK> criterion (ii) - ensuring a small absolute difference in the apparent and adjusted r𝟐 nagelkerke to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of <UNK> or less in the apparent and adjusted <UNK> nagelkerke. recall, assuming an exponential model for simplicity, we calculated that the <UNK> csapp ) = <UNK> then, using equation <UNK> we obtain svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp) = <UNK> <UNK> + <UNK> × <UNK> = <UNK> this is less stringent than the <UNK> assumed for criterion (i), and so no further sample size calculation is required to meet criterion (ii). <UNK> step <UNK> criterion (iii) - ensure precise estimate of overall risk assuming a simple exponential model, we can check the width of the confidence interval for the overall risk at a particular time point based on the sample size identified, using the approach outlined in section <UNK> ensor et <UNK> reported an overall vte recurrence rate of <UNK> = <UNK> with an average follow-up of <UNK> years. therefore, assuming 𝜆 is <UNK> in our new study, and that a predicted risk at <UNK> years is of key interest, an exponential survival model would give the cumulative incidence of <UNK> exp <UNK> × <UNK> based on the calculated sample size of <UNK> participants from criterion (i), and thus an estimated <UNK> = <UNK> person-years of follow-up, the <UNK> confidence interval would be <UNK> − exp ( − ( 𝜆̂ ± <UNK> 𝜆̂ t ) t ) = <UNK> − exp ( − ( <UNK> ± <UNK> <UNK> ) <UNK> ) = <UNK> to <UNK> this is reassuringly narrow, and satisfies equation <UNK> as both the lower and upper bounds are well within an error of <UNK> of the true value of <UNK> <UNK> step <UNK> minimum sample size that ensures all criteria are met the largest sample size required was <UNK> participants to meet criterion (i), which therefore provides the minimum sample size required for developing our new model. this assumes the new cohort will have a similar follow-up, censoring rate, and event rate to that reported by ensor et al, where the mean follow-up per person was <UNK> years, <UNK> of individuals had a vte recurrence by end of follow-up, and the event rate was <UNK> then, the required <UNK> participants corresponds to about <UNK> × <UNK> = <UNK> person-years of follow-up, and <UNK> × <UNK> ≈ <UNK> outcome events, and thus an epp of <UNK> ≈ <UNK> this is over twice the “epp of at least <UNK> rule of thumb. figure <UNK> shows that an epp of <UNK> only ensures a shrinkage factor of <UNK> which would reflect relatively large overfitting. <UNK> what if the sample size is not achievable? if a researcher was restricted in their total sample size, for example, by the time and cost of a new cohort study, then a sample size of <UNK> may not be practical. in this situation, we do not recommend reducing sample size by decreasing sc below <UNK> (as this would reflect larger overfitting) or by assuming a larger <UNK> cs_adj value (as this is anticonservative for criterion (i)). rather, to ensure an svh of <UNK> (ie, an expected shrinkage of <UNK> the researcher should lower p by reducing the number of candidate predictors. for example, predictors could be prioritised based on previous evidence (eg, systematic reviews). after data collection, unsupervised learning techniques such as principal component analysis may be useful, which are blinded to the outcome data. figure <UNK> shows how changing p changes the required sample size to meet criterion (i). for example, if a researcher was restricted to a sample size of about <UNK> participants, then they would need to reduce p to <UNK> to ensure an expected shrinkage of <UNK> this is because, for an svh of <UNK> and <UNK> cs_adj of <UNK> the sample size required is n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> figure <UNK> events per predictor parameter required to achieve various expected shrinkage (svh) values for a new prediction model of venous thromboembolism recurrence risk with an assumed <UNK> cs_adj of <UNK> [colour figure can be viewed at wileyonlinelibrary.com] figure <UNK> sample size required (based on equation <UNK> for a particular number of predictor parameters (p) to achieve a particular value of expected shrinkage (svh), for a new prediction model of venous thromboembolism recurrence risk with an assumed <UNK> cs_adj of <UNK> [colour figure can be viewed at wileyonlinelibrary.com] and so now close to <UNK> figure <UNK> also shows how larger values of svh require larger sample sizes; in particular, the increase in sample size required is substantial when moving from svh of <UNK> to <UNK> values of svh < <UNK> lead to lower sample sizes, but come at the cost of larger expected overfitting, and so are not recommended. therefore, targeting a value of svh of <UNK> would seem a pragmatic choice. <UNK> potential additional criterion: precise estimates of predictor effects ideally, predictions should also be precise across the entire spectrum of predicted values, not just at the mean. this is challenging to achieve, but is helped by ensuring the sample size will give precise estimates of the effects of key <UNK> hence, this may form a further criterion for researchers to check (ie, in addition to criteria (i) to (iii)). briefly, for a particular predictor of a binary or time-to-event outcome, the sample size required to precisely estimate its association with the outcome (ie, an odds ratio or hazard ratio) depends on the assumed magnitude of this effect, the variability of the predictor's values across subjects, the predictor's correlation with other predictors in the model, and the overall outcome proportion in the <UNK> ideally, we want to ensure a sample size that gives a precise confidence interval around the predictor's effect <UNK> however, this is taxing, as closed-form solutions for the variance of adjusted log odds ratio or hazard ratios, from logistic and cox regression, respectively, are nontrivial. one solution is to use simulation-based <UNK> however, perhaps a more practical option is to utilise readily available power-based sample size calculations that calculate the sample size required to detect (based on statistical significance) a predictor's effect for a chosen type i error level (eg, <UNK> and <UNK> as such sample size calculations are likely to be less stringent than those based on confidence interval width (especially for predictors with large effect sizes), we might use a high power, say of <UNK> in the calculation. checking sample size for predictor effects will be laborious with many predictors, and so it may be practical to focus on the subset of key predictors with smallest variance of their values, as these predictors will have the least precision. in particular, when there are important categorical predictors but with few subjects and/or outcome events in some categories, substantially larger sample sizes may be needed to avoid separation issues (ie, no event or nonevents in some <UNK> in addition, any predictors whose effect is small (and thus harder to detect), but still important, may warrant special attention. for example, returning to the vte prediction model from section <UNK> a key predictor in the original model by ensor et al was <UNK> with an adjusted log hazard ratio of <UNK> although this is close to zero, as age is on a continuous scale, the impact of age on outcome risk is potentially large; for example, it corresponds to an adjusted hazard ratio of <UNK> comparing two individuals aged <UNK> years apart. based on the results presented by ensor et <UNK> the standard deviation of age was <UNK> and the overall outcome occurrence by end of follow-up was <UNK> based on these values, and assuming other included predictors explain <UNK> of the variation in age, then the sample size approach of hsieh and <UNK> suggests <UNK> subjects are required to have <UNK> power to detect a prognostic effect for age. this is larger than the <UNK> subjects required to meet criterion (i), and so, to be extra stringent beyond criteria (i) to (iii), the researcher might raise the recommended sample size to <UNK> subjects, if possible. <UNK> discussion sample size calculations for prediction models of binary and time-to-event outcomes are typically based on blanket rules of thumb, such as at least <UNK> epp, which generates much debate and <UNK> in this article, building on our related work for linear <UNK> we have proposed an alternative approach that identifies the sample size, events and epp required to meet three key criteria, which minimise overfitting whilst ensuring precise estimates of overall outcome risk. criterion (i) aims to ensure the optimism of predictor effect estimates is small, as defined by a global shrinkage factor of ≥ <UNK> this idea extends the work of harrell who suggests that, after a model is developed, if the shrinkage estimate “falls below <UNK> for example, we may be concerned with the lack of calibration the model may experience on new <UNK> our premise is the same, except we focused on calculating the expected shrinkage before data collection, to inform sample size calculations for a new study. criterion (ii) extends this idea to ensure the optimism is small on the <UNK> nagelkerke scale, such that there is a difference of ≤ <UNK> in the apparent and adjusted percentage of variation explained by the model. lastly, criterion (iii) ensures the sample size will precisely estimate the overall outcome risk, which is fundamental. by utilising the model's anticipated cox-snell <UNK> the sample size calculations are essentially tailored to the model and setting at hand, because the cox-snell <UNK> reflects many factors including the outcome proportion (ie, outcome prevalence or cumulative incidence) and the overall fit (performance) of the model. it therefore better reflects the trait of a particular model and setting at hand rather than a blanket epp <UNK> in our examples, the sample sizes required often differed considerably from an epp of <UNK> reinforcing the idea that this rule is too <UNK> indeed, the required epp was much higher <UNK> in our second example than our first <UNK> illustrating the problem with a blanket epp rule trying to cover all <UNK> section <UNK> also showed how to obtain a realistic value for cox-snell <UNK> based on previous models to make our proposal more achievable in practice. if no previous prediction model exists for the outcome and setting of interest, then information might be used from studies in a related setting or using a different but similar outcome definition or time points to those intended for the new model. information can also be borrowed from predictor finding studies (eg, studies aiming to estimate the prognostic effect of a particular predictor adjusted for other <UNK> typically, these studies apply multivariable modelling, and although mainly focused on predictor effect estimates, they often report the c statistic and <UNK> values. further research is needed to help researchers when there are no existing studies or information to identify a sensible value of the expected cox-snell <UNK> medical diagnosis and prediction of health-related outcomes are, generally speaking, low signal-to-noise ratio situations. it is not uncommon in these situations to see <UNK> nagelkerke values in the <UNK> to <UNK> range. therefore, in the absence of any other information, we suggest that sample sizes be derived assuming the value of <UNK> cs_adj corresponds to an <UNK> nagelkerke of <UNK> (ie, <UNK> csadj <UNK> csadj) = <UNK> an exception is when predictors include “direct” (mechanistic) measurements, such as including the baseline version of the binary or ordinal outcome (eg, including smoking status at baseline when predicting smoking status at <UNK> year), or direct measures of the processes involved (eg, including physiologic function of patients in intensive care when predicting risk of death within <UNK> hours). then, in this special situation, an <UNK> nagelkerke = <UNK> may be a more appropriate default choice. the rule of having an epp of at least <UNK> stems from limited simulation studies examining the bias and precision of predictor effects in the prediction <UNK> jinks et <UNK> alternatively developed sample size formulae for a time-to-event prediction model based on the d <UNK> they suggest to predefine the d statistic that would be expected, and then, based on a desired significance or confidence interval width, their formulae provide the number of events required to achieve this. however, their method does not account for the number of candidate predictors and does not consider the potential for overfitting when developing a model. our sample size calculations address this, and are meant to be used before any data collection. in situations where a development data set is already available, containing a specific number of participants and predictors, our criteria could be used to identify whether a reduction in the number of predictors is needed before starting model development. indeed, harrell already illustrated this concept by using the shrinkage estimate from the full model (including all predictors) to gauge whether the number of predictors should be reduced via data reduction <UNK> ideally, this should be done blind to the estimated predictor effects (ie, just calculate the shrinkage factor for the full model, but do not observe the predictor effect estimates and associated p-values), as otherwise decisions about predictor inclusion are influenced by a “quick look” at the effect estimates from the full model results. similarly, when planning to use a predictor selection method (such as backwards selection) during model development, researchers should define p as the total number of parameters due to all predictors considered (screened), and not just the subset that are included in the final <UNK> as harrell <UNK> the value of p should be honest. section <UNK> also highlighted the potential additional requirement to ensure precise estimates of key predictor effects. in particular, special attention may be given to those predictors with strong predictive value (and thus most influential to the predicted outcome risk), especially if the variance in their values is small, or when events or nonevents in some categories of the predictor are rare, as this leads to larger sample sizes. for example, van smeden et al highlighted that “separation” between events and nonevents is an important consideration toward the required sample size, which occurs when a single predictor (or a linear combination of multiple predictors) perfectly separates all events from all nonevents, and thus causes estimation <UNK> this may lead to substantially larger epp to resolve the issue (eg, so that all categories of a predictor have both events and nonevents). for such reasons, we labelled our criteria (i) to (iii) proposal as the “minimum” sample size required. further research should identify how our sample size criteria relates to that of the work of van smeden et al, who focused on sample size in regards to the mean squared error in predictions from the <UNK> specifically, they use simulation to evaluate the characteristics that influence the mean squared prediction error of a logistic model, and identify that the outcome proportion and number of predictors are <UNK> in addition to total sample size. this leads to a sample size equation to minimise root mean-squared prediction error in a new model development study. harrell also suggested using simulation to inform sample size, and illustrates this for a logistic regression model with a single <UNK> for example, one could simulate a very large dataset from an assumed prediction model, and quantify the mean square (prediction) error and mean absolute (prediction) error of a model developed from this data set. then, repeat this process each time removing an individual at random, until a sample size is identified below which the mean squared (prediction) error is unacceptable. in summary, we have proposed criteria for identifying the minimum sample size required when developing a prediction model for binary or time-to-event outcomes. we hope this, and our related <UNK> encourages researchers to move away from rules of thumb, and to rather focus on attaining sample sizes that minimise overfitting and ensure precise estimates of overall risk within the model and setting of interest. we are currently writing software modules to implement the approach.