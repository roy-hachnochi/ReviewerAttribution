there are several points/queries to make in addition to those of the reviewers and still of concern in the revised
manuscript:
1. although the authors call this a natural experiment it is still observational and hence use of impact should be
tempered. there was no association found and this was not different to control hospitals over the same period. however,
there is a self-selection problem and no guarantee that the implementation hospitals are not different.
- suggested changes to wordings are for example : objectives (study cannot assess impact), conclusions (no overall
negative ‘association’ were found) and what study adds (cannot evaluate how ‘affected’, nor the ‘impact’).
2. was each implementation hospital within a separate hrr or were some hospitals controls for more than one case? if
the latter, how was this addressed in the analyses?
3. model as defined within statistical analysis section: which factors are considered as random effects and which fixed?
the ‘covariates’ term may be misleading as this will lead to more than one beta value (whereas here it is quite clearly
only one- beta4).
please verify that clustering within the same hrr is accounted for as well as clustering of patients within hospitals and
admissions from the same patient over time. the description sounds as though fixed effects were used, but then this is
given as a sensitivity analysis. beta3 also requires a further subscript i think? removing the equation and giving a
written description may be clearer.
hierarchical logistic regression, with clustering of admissions within patients, patients within hospitals and hospitals
within hrr, adjusting for covariates (as given in table 1) and mdc (how many terms does this entail? what are the
categories?) should be used to model the probability of the outcomes (mortality, readmission, adverse event) to
determine changes over time (pre to post implementation date) according to whether the hospital was an ehr
implementer or control. an interaction term between the time and ehr indicators quantifies the difference-indifferences.
4. not enough information is given to replicate the power calculation. what is the anticipated starting percentage and is
any account taken of icc (different admissions for the same patient or within the same hospital/hrr)?
5. table 1: how useful is it to present significance tests pre-post within the study and control groups, especially given
the large numbers of patients and hence significance of unimportant clinical differences. for example, it is not surprising
that the race breakdown does not differ from pre to post and a relatively small difference in the % females is highly
statistically significant. of more interest perhaps is the fact that the control hospitals tended to have older and/or more
white patients who didn’t stay as long on average and who had a different diagnostic distribution. how might these
differences in patient mix affect interpretation of the results? at the very least, there should be discussion of the
generalisability of results given this selection bias.

<|EndOfText|>

this systematic review and meta-analysis combines results from randomised clinical trials
to evaluate the effectiveness of perioperative interventions in reducing pcas.
the abstract conclusion that ppcs are common is not supported by any information in the
abstract. this statement is again made in the discussion, yet to find the information on
prevalence requires reference to the appendices figures. the authors should perhaps make
the information on the prevalence of ppcs more prominent, rather than emphasising just
the rr and differences in means.
figures 3-7 show the results from meta analyses presented more fully in appendix 2. the
correspondence between these is not always consistent. for example, the values for
post-operative bi-level ni and prophylactic inhaled beta agonist (0.78 (0.32, 1.90) and
0.93 (0.67, 1.29) respectively, figure 3) do not seem to appear in the appendix and hence
their derivation and numbers based on is unclear; in fig 4 incentive spirometry rr is given
as 5.83 (0.63, 26.3), yet this is 5.38 (0.63, 46.30) in appendix 2.
it may help to give the numbers of studies that each of the estimates in figure 3-7 is based
on as well as the total numbers of patients included in those studies.

figure 3 is cited twice as two different figures (from page 16 onwards the figure numbers
should be increased by 1 ie. respiratory infection and atelectasis outcomes are shown in
figure 4 and 5, not 3 and 4, etc.)
for lung protective ventilation, the point estimate of the associations with respiratory
infection and atelectasis outcomes is the same (0.56) with slightly different confidence
intervals (0.28, 1.09) and (0.32, 0.99) respectively, so the interpretation should not be
very different. however, the authors’ dependence on p-values leads them to discount one
as significant and the other not without further discussion, giving the indication they are
critically different.
the paper would benefit from greater coherence between the main body of the text and
the materials given in the appendix. for example, the authors refer to the results for
ambroxol but do not guide the reader to the relevant meta-analysis shown in figure 4.2
appendix 2.
minimal information is given of the tsa and this should be expanded. the daris values
should be given, compared to the actual numbers available, and the implications of these
more fully explored. how many more are required? the authors state that there remains a
need for large, well designed pragmatic trials of ppc prevention strategies. given that
these will be added to the existing information, how large should these be (does this
information follow directly from the daris investigation?)
the authors should also clarify what additional information is given by the tsa/daris
values that is not yielded by an examination of the confidence limits for the comparisons.
the 2nd bullet of what the study adds states that incentive spirometry is equivalent to
standard clinical care. this is again an interpretation based on p-values alone. the point
estimate and confidence interval for developing respiratory infections for instance is 5.83
with 95% ci (0.63, 26.3), which is not evidence on which to argue equivalence since very
large (and undoubtedly clinically important) differences cannot be discounted.
which particular statistics is the 3rd bullet of what the study adds referring to?
how is the conclusion made that bundled groups of interventions (4th bullet) should be
used? there does not appear to be any investigation of bundled groups in this paper on
which to base an inference.
publication bias should have been investigated.
i could not see any available prisma documentation for this study.


<|EndOfText|>

the authors have given good responses to many of the comments and suggestions made. in particular,
the move to poisson regression to validly assess the primary outcome differences is a major
improvement.
i am surprised that the numbers in the 3 arms, for both families and children, are so similar given there
was no stratification or blocking to randomisation (figure 1). it is of course possible, and the author
response suggests that they were fortunate to get such good balance.
i do have some further queries though regarding the current manuscript and the responses given:
1. re the sample size estimation. this is improved, but i still have a few queries. –
- the authors state that to detect a difference of one half of an sd (diff 0.28, sd estimated 0.56) requires
n=80 per group to give 80% power. i assume this would be at 5% significance level. by my calculation
it would be 63 per group. this is not a major difference, i am just curious as to why the difference.
maybe a software issue?
- they then give a number of 150 per group to account for 25% attrition (yielding 107 per group) and
possible clustering within families (indicating a design effect of 1.40, which is arbitrarily chosen?)
- n=2 and an icc of 0.01 would give a design effect of 1.01, and an effective sample size of 1059/1.01
= 1048 (ie. is the design effect given of 1.02 a typo?)
the authors therefore address within family clustering, but was there also within village clustering to
adjust for?
2. since there is clustering, this should be included in the analyses to adjust the confidence intervals and
significance levels accordingly. multilevel models should be used.
3. although in the response the authors discuss the effect size of newsup being 2-4 times greater than
that of fbf, i could not see this information in the manuscript (nor which values they were specifically
referring to). the statements within the manuscript that there were no significant differences between
newsup and fbf should be supported by a difference (rr) and ci. it is accepted that this is an
exploratory analysis without power to detect a difference, as outlined in the methods.
4. primary outcome is given as the number of stickers found at follow-up in the methods and the results
appear to show this, yet the abstract states that the primary outcome is the change in working memory
from baseline to follow up.
5. it is unclear how many children reached the limit of trials (12 for younger, 18 for older) and how
these were treated in the analyses.

6. table 4 should given differences and ci for the newsup vs fbf comparison, not just the p-values.


<|EndOfText|>

angie wade
the authors have made many revisions in the presentation of this study, and although they
have addressed some of my previous concerns, queries remain.
the emphasis in the introduction is now more towards a need to compare different
treatment options rather than being primarily a summary of the effectiveness of available
options. it therefore seems that a network meta analysis may have benefits, the authors
should perhaps explain why this has not been considered.
the revised paper includes an investigation of publication bias via the inclusion of funnel
plots, which is a good addition. further formal testing via significance tests may be invalid
due to small numbers of studies, but the authors should make comment.
the authors present a good case for using tsa and daris to reduce the risk of false positive
results and consider whether larger sample sizes are required to make definitive conclusions.
however, they then use the information from low quality trials with wide and/or inconclusive
confidence intervals to draw conclusions on optimal treatments without reference to how
interpretation of any analysis should be moderated according to the daris findings.
although the authors have made improvements to the presentation of results within the
text, there should ideally be more focus on the clinical implications of the differences seen,
together with the imprecision of estimates. the conclusion section is long and many of the
observations/conclusions based on supposition rather than the data as presented, it needs to
be more focussed on the actual findings of this study (for example, the comments on
cost-effectiveness geared towards choices is more a discussion point than a conclusion from
the data presented).
overall, the authors present a comprehensive analysis with the results clearly displayed in
forest plots and appropriate supplementary material showing individual study results. the

information is limited, as noted by the authors, and interpretations need to be tempered
accordingly


<|EndOfText|>

although the authors have addressed some issues and performed sensitivity analyses in
response to reviewer concerns, there remain problems with the analyses. in particular the
use of mean consumption rather than utilising measures over time via hierarchical models,
and the censoring of patients with other first cancers, which may not satisfy the
requirements of non-informative censoring. the authors should perform the analyses to
address the outstanding concerns of the reviewers, which may affect the results and
conclusions that can be drawn from this data.

<|EndOfText|>

the authors response to point 1, states that one study used or, the rest hr and so to
combine they present rr. a sensitivity analysis removes the single or. point 2 response
states that all studies presented hr and to keep consistency with the primary analysis they
present rr. it is not at all clear why the authors would take this approach rather than
combining hr directly to give overall hr estimates, since this is given in all except one
study whereas rr are not actually presented in any. rr are not identical to hr and any
approximation should be noted and justified. in this case such approximation seems
unnecessary.
the authors have otherwise addressed my comments appropriately.


<|EndOfText|>

the authors have addressed my points and in particular performed modelling with tsdd as a continuum,
which has not changed the conclusions. if there are no objections from the reviewers re author responses
then i have nothing to add. please let me know if you would like me to look at any outstanding issues.

<|EndOfText|>

the authors have responded to all comments. just a couple of relatively minor points:
1. it is usual to give estimate and ci, and not just p-value, in the abstract. hence, the
estimated percentage difference and ci should be added to the sentence that has been
inserted re comparison of traditional and non-traditional criteria (p=0.001).
2. the authors have changed the or for the leiden index to represent an increase of 50
points as suggested. the tables need to make clear that the or given are for 50 points and
not still 1.

<|EndOfText|>

although several issues remain to be resolved with this study, the most crucial is the lack
of information and understanding around the primary outcome, a variant of the spin the
pots task. i could not find the necessary relevant information in the references either.
please explain further: the total number of stickers found is recorded as 1-8 for 15 months
to 3.9 year olds, and 1-9 for 4-7 year olds. why can the number not be zero? there is then
a standardisation by age group to produce z-scores. however, with such a limited number
of integer possibilities (7 for the young and 8 for the older groups), standardisation can
only produce 7 or 8 z-score values and does not turn a discrete outcome into a continuum.
hence, the methods applied for continuous numeric data (linear mixed models) are not
appropriate.
other points that need addressing are given below:
1. there should be some evidence of validation of the outcome measure, which (as noted
above) must be summarised and analysed using methods appropriate to it’s type.
2. secondary outcomes appear to all be continuous numeric and hence the analyses
presented more suited to this type of data. please verify that the models used do satisfy
the assumptions necessary and that model residuals are approximately normally
distributed.
3. some reasoning and justification should be given for the supplementation only being for
5 days of the week. is there any information as to what the children ate on the other 2
days? the potential effects of the 2 day ‘break’ should be discussed.

4. the control breakfast is said to replicate a traditional breakfast for children, but no
reference is given to support this. there should be discussion about any potential bias that
may be introduced by providing a set breakfast rather than testing against the normal
practice without intervention. one reasoning for having a control prepared breakfast may
be to blind participants and evaluators, or to equalise other factors (time of contact etc.)
between the study arms, but this does not appear to be the case.
5. villages were a convenience sample. how representative are they of all rural villages in
guinea-bissau? what evidence is there of this?

6. did any families within the selected villages drop out after enrolment? if so, what biases
might this introduce? (a free breakfast may be more of an incentive for some families than
others. also the free rice on study completion. )
7. was all randomisation simple? ie. there was no blocking or other procedure in place to
ensure that allocations were not widely different (in total number or in confounders)
between the 3 arms. was there any age stratification?
8. the sample size paragraph is very unclear. do the mean differences of 0.56 and 0.28
effects relate to z-score (standardised differences) in newsup and fbf compared to control
respectively? otherwise an estimate of the sd of working memory is required. also required
are estimates (with justification) of the icc within families and the distribution of family
sizes. 80% power is very low.
9. please define what is a clinically important difference in the primary outcome.
10. the title refers to 3 outcomes, yet only one is deemed primary and there is no
corresponding adjustment for multiple. the other outcomes noted are therefore secondary
and should not be viewed with the same level of inference.
11. analyses were mostly conducted with the 2 separate age groups. since there was also
investigation of effect modification by age group via interaction terms, why were the
separate models necessary?
12. the discussion states that newsup caused a marked, significant improvement in
working memory and that the difference of 0.5 sd is better than that of conventional
feeding programs which have a negligible effect of approx 0.09 sds. in this trial fbf is
designed to replicate the supplements often used, and so it is surprising that the
differences between newsup and fbf in this trial are not large (table 3), despite minor
differences in the p-values which deem newsup significant at 5% and fbf not.
13. the authors should present the icc from their study to (a) compare to that used in the
power calculation and (b) help inform others doing similar studies.
14. in the discussion the authors mention the beneficial effect of newsup on hemaglobin in
contrast to no significant difference for fbf, yet the difference between these is not
significant at 5% (p=0.06) and no confidence interval for the difference is given to show
the potential magnitude of any differences. similarly for other newsup and fbf effect
differences that are of importance – average differences and confidence intervals should be
presented to enable informed discussion.

15. there is far too much emphasis on p-values and not enough on the confidence limits.
for example, the data are compatible with differences as small as 0.02 sds for newsup
compared to control. (in what study adds, the 0.4 (0.02, 0.78) difference in working

memory on newsup is deemed a ‘significant beneficial effect’, whereas the 0.39 (-0.01,
0.78) difference for fbf is only a ‘suggestive effect’.)
16. the conclusion that cognitive benefit with newsup was of a magnitude previously seen
with successful psychosocial interventions is not upheld (the data are compatible with
differences as small as only 0.02 sd). nor is the accompanying statement that this was
accompanied by substantial improvements in body composition (many differences shown in
table 4 appear quite small).


<|EndOfText|>

the authors have made extensive revisions and created a more readable paper which
addresses the majority of the reviewer comments. however, i do have some concerns
about the revised paper and these are detailed below (points 3 and 6 in particular are
major concerns re interpretation of the results):
1.
the response to reviewer 4’s query re publication bias and more formal
evaluation/investigation of this is a little weak. the funnel plots should be augmented with
a formal test as appropriate.

2.
the authors state that measures using different scales were converted to a single
scale. if this was done, then further details must be given to support this process.
3.
the authors state that they used change from baseline when available and
otherwise end data. how is this possible? or likely to lead to any useful overall summary?
the figures suggest that a variety of different summaries are combined without
consideration of how the output can be validly interpreted (eg. figure 3: median changes
from baseline, geometric means and medians combined with means). the cited reference
(48) does not appear to lend any support to this approach.
4.
etc.)

i did not understand the relevance of the numbering within figures 1-5 (30.1.1

5.
the values in table 2 for diagnosis of diabetes (rr 1.60 (0.22, 11.77)) do not
match those in figure 2 (1.52 (0.19, 12.05)).

6.
despite the authors being clear that subgrouping results need to be interpreted
with caution, that there were no significant differences between subgroups and that the
result for >2.4 g/d lcn3 was based on few trials, they give this as a recommendation in
the conclusions, policy implications, what this study adds and the abstract. this ‘finding’
needs to be toned down and/or full results given in the main body of the paper (effect size
with ci, results of comparisons with other doses, formal dose analysis). the current
presentation also seems at odds to their response to reviewer 3 re adjustment for multiple
comparisons (ie. this part of the presentation does appear to be based on statistical
significance with no reference to actual effect size, hence adjustment would be necessary).
furthermore, the cut-point of 2.4 in the analyses does not appear justified and additional
tables 2-6 show that the actual categorisations were 2.4-4.4 and above 4.4. the only
significant difference attributable to raised lcn3 in these tables appears to be for >4.4,
associated confidence intervals are generally wide and not at odds with the point estimates
in the lower categories.
