the authors have done a fine job summarizing the state of multimorbidity indices in the
clinical literature. there are however a number of issues that if the authors can address
will make the manuscript much stronger.
currently, the review is quite descriptive, reads more like a catalogue, and as such i’m
unclear who is the intended audience. i don’t think merely listing available tools with some
basic characteristics is enough for someone to decide which tool to use.
i think a more critical overview of the tools would be more useful. many tools (prediction
models) are often poorly developed and poorly reported, there have been numerous
reviews published and thus there is no reason to think why multimorbidity tools won’t
suffer the same shortcomings, and investigating this will make the manuscript stronger.
how the predictive accuracy of the tools were evaluated would be useful to know, this is
often poorly done, often incorrectly/only partially done, and often not following
recommended guidance, e.g., for prediction models, those predicting mortality should
assess both model discrimination (e.g. the auc/c-statistics) but also calibration (see moons
et al, ann intern med 2015). how was internal validation carried out (splitting the data,
bootstrapping, cross-validation).
also do we know from the primary studies, how many reported the full model/index to
allow other investigators to use it, we know from many reviews, that tools developed using
regression often fail to report the full model (e.g., the intercept was not reported, or for a
cox model, the baseline survival at one or more time points wasn’t reported).
when models were predicting mortality. how is mortality defined, is this any mortality at
any time point, is it a 1-year mortality prediction?, maybe include a column in one of the
tables, defining the exact outcome.
similarly, external validation studies are also often poorly done, just because an
investigator has carried out an external validation, doesn’t mean it was done well – they
are often poor too (collins et al, bmc med res methodol 2014), so some discussion on the
quality of external validation studies would be useful. clearly label how many of the tools
have been externally validated (this may have been reported i missed it) – and indicate
how many were independently evaluated by different investigators from those who
developed the index. indicate, when validations were done, the sample size (number of
events, if appropriate), dates, country, performance, etc.
the risk of bias assessment is rather ad-hoc, i suggest the authors to look at the
probast risk of bias tool (wolff et al, ann intern med 2019; moons et al, ann intern med
2019), which is targeted to tools such as this, particularly those focused on prognostic
outcomes (such as mortality). and then discuss the risk of bias in more detail to critically
evaluate the tools, highlight any common issues coming out in the rob assessment (or
within particular rob domains).
categorsing/splitting the indices by what they are tapping into, prognostic outcomes,
mental health etc…might make presentation better, i.e., have separate tables (splitting
table 3), and discuss under separate headings in the main text. depending on the type of
tool, the assessment of performance will be different too (i.e. how you assess a mental
health index is different to how you would assess the performance of a tool predicting
mortality), so might make it easier to read.


<|EndOfText|>

the authors have generally clarified most of my previous concerns.
however, i still remain unclear regarding the inclusion of the prediction model in this paper. it is a
distraction from the main aim of the paper, which is about diagnostic accuracy of ct angiography, mr
angiography etc. the authors are reporting a lot of information which deserves to be split into two papers
so that the message of the two aims do not become diluted.
i fully appreciate that prospectively collecting data to externally validate is not a trivial exercise and will
take time. but suggesting that the model should be used is problematic without demonstrating it ‘works’
- particular when the sample size is moderate. was any attempt made to identify existing datasets that
the model could be evaluated on? developing a prediction model is a separate study and deserves to be
seen as such so that all aspects in deriving and internally validating the model can be explored in more
detail than is currently described here (again adhering to the tripod statement for prediction models;
www.bmj.com/content/350/bmj.g7594 and annals.org/article.aspx?articleid=2088542 - which the
authors appear to have overlooked despite being previously pointed towards tripod). the actual
development of the prediction model looks well done and thus just confirms my opinion that this should
be a separate paper.
as an aside presenting roc curves without labelling particular cut-offs on the curve is uninformative,
labelling the curve can then allow sens/spec to be read off at particular cut-offs. the calibration plot is
ok, but it is widely recommended to also superimpose a lowess calibration curve in addition to the
observed and mean predicted probabilities at fifths of predicted risk. quintiles is incorrectly used, a
quintile is a cut--point to create equal sized groups, the correct term if fifths (see the bmj stats notes,
www.ncbi.nlm.nih.gov/pubmed/7950724). hosmer-lemeshow test is an uninformative measure and does
not assess calibration, widely influence by sample size and grouping, but importantly doesn’t assess
direction of magnitude of (mis)calibration). (see tripod explanation & elaboration paper for more
details; annals.org/article.aspx?articleid=2088542).
the sample size for the main diagnostic accuracy study has now been reported but included in a
supplementary box. why? this should be in the main body of the paper and not tucked away in
supplementary materials.

<|EndOfText|>


the authors have conducted an individual participant data meta-analysis, comprising
15 cohorts from europe and the us. whilst this is an impressive amount of work, i
felt there are a number of details that have been omitted by the authors, that need
clarification and explanation.
how were these cohorts were identified and gathered (obtaining data for an ipd
meta-analysis is notoriously difficult) – are other cohorts available?
were these cohorts identified by a search? what eligibility criteria were used to
include the cohorts?
can the authors describe how these data sets were requested, obtained, managed?,
some details on data checking.
it would be helpful if the authors consulted the prisma-ipd reporting guideline
(jama 2015; 313: 1657-1665) to ensure all relevant bits of information have been
reported.
missing data were both omitted (if they had missing exposure or outcome data) and
included for other variables using the missing indicator approach – why use this
flawed approach (large literature discouraging this type of analysis – groenwold et al
cmaj 2012; 184: 1265-1269)? i also couldn’t see (unless i missed it) what was
missing and how much was missing. can the authors clarify? also why not consider
multiple imputation.
it would be useful to have some more basic information in this table on the cohorts,
dates, country etc. this information is in the supplementary material, but you have
to work to digest it and having it in an easily accessible format would help.
table 1: presumably the +/indicates standard deviations, should avoid +/.
reviewers have pointed out the incorrect hrs in the abstract, and on page 16 of the
text.
the models were adjusted for a number of covariates – were these all recorded in all
cohorts and all on the same scale? i’d be surprised, if they were there covariates
that were recorded as continuous for some cohorts whilst categorical for others? this
is often the case? if so, how was this handled in the analyses? some information on
harmonization of covariates.
also were all covariates available in all cohorts? (or was this part of the inclusion
criteria?) – if not how was this handled? systematically missing covariates is a
problem with ipd ma – though methods to impute these type of covaraites have
recently been published (resche-rignon et al 2013; stat med; 32: 4890-4905).
it’s also usually recommended to conduct a risk of bias assessment – i could see no
evidence of this.
an assessment of the impact of moderator variables was carried – but why was this
done by categorising age and bmi, why not retain on the continuous scale.


<|EndOfText|>

the authors have done a good job in revising their manuscript based on the reviewers
comments. the additional information in the supplementary tables is a good addition, along
with the authors recommendation on their potential usefulness is good.
i probably don't fully agree with not using probast, but am happy with the authors
response, and no further work needs to be done on this.
i still think table 3 is far too long (10 pages!) and would urge you to split this up by type of
index or what their aim is. contrary to the authors response, this would not increase the
length, as no more information is being requested merely an alternative and easier to read
presentation of this table. my comment is to improve readability.
table 1 is not needed - either place into the text or merge into figure 1 (prisma flow
diagram).

<|EndOfText|>

if this systematic review/meta-analysis contained non-overlapping studies, my comments would be relatively minor. however,
the seer database was used in 11 of the included studies. the authors seem to have followed a sensible approach to avoid
over-counting by selecting one study for each scenario, using various criteria defined on page 8. i’m not sure how differently i
would approach this, but could the authors comments on how this will affect their results - if at all? would carrying out
additional sensitivity analyses, using alternative approaches for study selection for the seer studies be useful to confirm their
findings? i fully appreciate, that doing this, things could get quite messy. but it feels messy anyway (not made easier by
examining different primary chance, different lag periods, and different rx modalities), as different seer reports are being used
in different analyses, which whilst they have reported this in the appendix, it naturally feels messy.
p11 l10 - they quote 24 reports from seer - not 11.
figure 2 shows the forest plot for bladder cancer, where are the others for colorectal cancer, rectal cancer, lung cancer etc .
whilst including all the possible forest plots for each cancer and each time point in the main text will be
overwhelming/unnecessary, they should be made available in online appendices (as supporting information). similarly for
radiotherapy modality, summary results are presented in table 3, but the forest plots should also be presented, again as
supplementary material.
adjusted hazard ratios are reported, but what confounders have been adjusted for in those studies which reported
adjustment?
there was no attempt to explore the observed heterogeneity (e.g. conducting a meta-regression), merely a few comments in
the discussion. did the authors not explore this in more detail, and more formally? the risk of bias assessment is all fine, but
as per my previous comment on heterogeneity, could the results from the risk of bias assessment be used (see hopewell et al,
2013). given the nature of the included studies, being data from observational studies/registries,
hopewell s, boutron i, altman dg, ravaud p. incorporation of assessment of risk of bias of primary studies in systematic
reviews of ransdomised trials: a cross-sectional study. bmj open 2013; 3: e003342.

<|EndOfText|>

the authors have generally responded to my earlier comments and the addition of more
contemporary data has resolved some of the problems and this is a good addition to the
analysis.
however, there are still missing data (income and high educational education are missing in
10% of the cohort, and tumour size in nearly 9% of the cohort, with other much smaller
missing data for other predictors), and the other have conducted a 'complete-case' analysis by
omitting those with missing information, therefore i still don't know what the 'final' n used in
the analyses - it clearly isn't 28220 - but if more than 10% of the cohort are omitted (a nonignorable amount), then should we be concerned? - is there something special about those who
were omitted? some sensitivity analyses would be reassuring (i.e. handling the missing data
more appropriately would be preferable).
table 1 is improved by including n, but have n at the top of the column under 'overall',
'additional treatment' and 'disease specific death' would be helpful.

<|EndOfText|>

the authors have done an excellent job in revising what was already a good manuscript. i have 2
comments. (1) could the authors mention they have reported according to tripod (and cite) in the
manuscript, they completed the checklist. (2) the calibration plots, the authors have added a lowess
smoother (at my request), but i'm suspicious as to whether this has been implemented correctly. the
sample size isn't huge, yet the line goes exactly (or appears to) through all points. i wouldn't expect this,
unless the author have fit a smoother to the 10 points - which is incorrect, as it should be on the individual
patients predictions/outcome.

<|EndOfText|>

the authors have satisfactorily responded to my concerns. no outstanding issues.

<|EndOfText|>

the authors have responded to some of my initial concerns, however, a few still remain,
the paper is still overly complex in places (including in the appendix), with many analyses
unnecessarily presented.
the model: table 2b presents the model (i believe) – however if i wanted to validate this
model on my own data i can’t – there is no baseline survival at 7 years. in the footnote to
this tables, some of the acronyms are no longer needed (e.g., bca).
number of candidate predictors. i appreciate the authors clarifying their approach, but
whether you’ve examined their univariate association or adjusted for one or more
variables, you’ve still examined 60 variables? whilst i still disagree with the use of
univariate associations with the outcome as a procedure to select variables – the ultimate
test is if the model works in an external validation – but maybe in the discussion this could
be raised – statistical significance as a criteria to select variable is not ideal.

internal validation: in their response, the authors have clarified that they have repeated
the variable selection/model building process in each of the bootstrap samples – but this is
not reflected in the manuscript.
table 2b. clarification. the model contains 8 variables. in the footnote it states “the final
multivariable model was adjusted with the 8 variables that remained independently
associated with allograft survival.” – this is unclear. if no additional variables are being
adjusted for, i.e., the model only contains those 8 variables, then there is no need to say
the final model was adjusted for 8 variables – as it implies there is a final model (with x
variables) and an additional number of variables have then been adjusted for.
calibration plots. calibration in the derivation cohorts is not particularly interesting, as they
will always appear well calibrated. can the authors clarify how the calibration plots were
created. the calibration in the validation cohorts looks too good? was this carried out using
the val.surv function in the rms library in r and thus accounting for censoring. i’m also not
sure what the black line (observed events) is? a calibration plot is a plot of observed
against predictions, so not sure what the black line is.
clarification is also needed on how the calibration slope and intercept have been calculated
(again in the context of a cox model) – it’s straightforward for a logistic model, but less so
for a cox model. the authors have stated they have been estimated from a linear
regression, this is not the correct way to estimate these values.
i’m still struggling to see the usefulness of the risk groups. those either side of a cutpoint
say from risk strata 1 and 2 will have very similar risk, but by categorising this risk they
will treated differently.
the presentation of the model in the appendix is still in a format that is unhelpfully
complex – this should be simplified. i still don’t understand why this needs to be
normalised on 0-5. provide the baseline survival at 7 years, and then the predictive
probabilities from the model will then range from 0 to 1, and thus more easily understood.
the interface the authors have created does this, it creates a risk from 0 to 100%.


<|EndOfText|>

an interesting and generally well written paper. i have a few comments that the authors should seriously consider to
improve their analyses.
in the methods the authors that labs will be different (time, cytology tests) therefore, i wonder why this wasn’t
accounted for this in the analysis. it would’ve been relatively straight forward to account for this clustering, and this
would strengthen the analysis and results.
it would be nice to know the frequency in which these tests have been administered has changed over time, as i feel
(as noted above) this will need to be factored into the analysis somehow. this information is presented in the
rozmeijer et al (cancer causes control 2016) paper, and should be also included and summarised here to indicate
what is happening.
i find some of the terminology hard to follow. nice clear definitions of screen-detected, and overall interval, and
clinically detected cancer would be useful (unless i missed it).
some clarification on what was imputed (table 1 indicate ses and screening region had some missing data – is that
all?). also how were the 10 imputations combined, there is no information on this and what was done.
is this a conventional way to quantify ses in the netherlands? seems rather crude in that ~80% of the cohort are
classed as ‘middle’, doesn’t seem that ses measured this coarsely could be particularly informative.
choosing confounders based on statistical significance isn’t a particular useful approach. it would be preferable to
identify potential confounders before the analysis and include them in the model regardless of whether they are
statistically significant or not.

<|EndOfText|>

the authors have done a major revision of their original analyses, with a detailed and convincing
response to all the comments raised by the reviewers.
in their revised analyses, they have done (i believe) as much as they can do with the data they
have, and have carried out a number of sensitivity analyses to demonstrate the robustness of their
findings. this is an impressive set of well reported complex analyses on a large cohort and i have no
concerns on the methodology.
apologies to the authors for my slowness in responding.

additional questions:
please enter your name: gary collins
job title: associate professor
institution: university of oxford
reimbursement for attending a symposium?: no
a fee for speaking?: no
a fee for organising education?: no
funds for research?: no
funds for a member of staff?: no
fees for consulting?: no
have you in the past five years been employed by an organisation that may
in any way gain or lose financially from the publication of this paper?: no
do you hold any stocks or shares in an organisation that may in any way
gain or lose financially from the publication of this paper?: no
if you have any competing interests (please see bmj policy) please declare them here:

**information for submitting a revision**
deadline: your revised manuscript should be returned within one month.
how to submit your revised article: log into http://mc.manuscriptcentral.com/bmj and enter your
author center, where you will find your manuscript title listed under "manuscripts with decisions."
under "actions," click on "create a revision." your manuscript number has been appended to denote
a revision.
you will be unable to make your revisions on the originally submitted version of the manuscript.
instead, revise your manuscript using a word processing program and save it on your computer.
once the revised manuscript is prepared, you can upload it and submit it through your author
center. when submitting your revised manuscript, you will be able to respond to the comments
made by the reviewer(s) and committee in the space provided. you can use this space to document
any changes you make to the original manuscript and to explain your responses. in order to expedite
the processing of the revised manuscript, please be as specific as possible in your response to the
reviewer(s). as well as submitting your revised manuscript, we also require a copy of the manuscript
with changes highlighted. please upload this as a supplemental file with file designation ‘revised
manuscript marked copy’. your original files are available to you when you upload your revised
manuscript. please delete any redundant files before completing the submission.
when you revise and return your manuscript, please take note of all the following points about
revising your article. even if an item, such as a competing interests statement, was present and
correct in the original draft of your paper, please check that it has not slipped out during revision.
please include these items in the revised manuscript to comply with bmj style (see:
http://www.bmj.com/about-bmj/resources-authors/article-submission/article-requirements and
http://www.bmj.com/about-bmj/resources-authors/forms-policies-and-checklists).
items to include with your revision (see http://www.bmj.com/about-bmj/resources-authors/articletypes/research):
1. what this paper adds/what is already known box (as described at
http://resources.bmj.com/bmj/authors/types-of-article/research)
2. name of the ethics committee or irb, id# of the approval, and a statement that participants gave
informed consent before taking part. if ethics committee approval was not required, please state so
clearly and explain the reasons why (see http://resources.bmj.com/bmj/authors/editorialpolicies/guidelines.)
3. patient confidentiality forms when appropriate (see
http://resources.bmj.com/bmj/authors/editorial-policies/copy_of_patient-confidentiality).
4. competing interests statement (see http://resources.bmj.com/bmj/authors/editorial-

policies/competing-interests)
5. contributorship statement+ guarantor (see http://resources.bmj.com/bmj/authors/articlesubmission/authorship-contributorship)
6. transparency statement: (see http://www.bmj.com/about-bmj/resources-authors/forms-policiesand-checklists/transparency-policy)
7. copyright statement/licence for publication (see http://www.bmj.com/about-bmj/resourcesauthors/forms-policies-and-checklists/copyright-open-access-and-permission-reuse)
8. data sharing statement (see http://www.bmj.com/about-bmj/resources-authors/articletypes/research)
9. funding statement and statement of the independence of researchers from funders (see
http://resources.bmj.com/bmj/authors/article-submission/article-requirements).
10. patient involvement statement (see http://www.bmj.com/about-bmj/resources-authors/articletypes/research).
11. please ensure the paper complies with the bmj’s style, as detailed below:
a. title: this should include the study design eg "systematic review and meta-analysis.”
b. abstract: please include a structured abstract with key summary statistics, as explained below
(also see http://resources.bmj.com/bmj/authors/types-of-article/research). for every clinical trial and for any other registered study- the last line of the abstract must list the study registration
number and the name of the register.
c. introduction: this should cover no more than three paragraphs, focusing on the research question
and your reasons for asking it now.
d. methods: for an intervention study the manuscript should include enough information about the
intervention(s) and comparator(s) (even if this was usual care) for reviewers and readers to
understand fully what happened in the study. to enable readers to replicate your work or implement
the interventions in their own practice please also provide (uploaded as one or more supplemental
files, including video and audio files where appropriate) any relevant detailed descriptions and
materials. alternatively, please provide in the manuscript urls to openly accessible websites where
these materials can be found.
e. results: please report statistical aspects of the study in line with the statistical analyses and
methods in the published literature (sampl) guidelines http://www.equator-network.org/reportingguidelines/sampl/. please include in the results section of your structured abstract (and, of course, in
the article's results section) the following terms, as appropriate:
i. for a clinical trial: absolute event rates among experimental and control groups; rrr (relative risk
reduction); nnt or nnh (number needed to treat or harm) and its 95% confidence interval (or, if the
trial is of a public health intervention, number helped per 1000 or 100,000.)
ii. for a cohort study: absolute event rates over time (eg 10 years) among exposed and nonexposed groups; rrr (relative risk reduction.)
iii. for a case control study:or (odds ratio) for strength of association between exposure and
outcome.
iv. for a study of a diagnostic test: sensitivity and specificity; ppv and npv (positive and negative
predictive values.)
v. for a systematic review and/or meta-analysis: point estimates and confidence intervals for the
main results; one or more references for the statistical package(s) used to analyse the data, eg
revman for a systematic review. there is no need to provide a formal reference for a very widely
used package that will be very familiar to general readers eg stata, but please say in the text which
version you used. for articles that include explicit statements of the quality of evidence and strength
of recommendations, we prefer reporting using the grade system.
f. discussion: to minimise the risk of careful explanation giving way to polemic, please write the
discussion section of your paper in a structured way. please follow this structure: i) statement of
principal findings of the study; ii) strengths and weaknesses of the study; iii) strengths and
weaknesses in relation to other studies, discussing important differences in results; iv) what your
study adds (whenever possible please discuss your study in the light of relevant systematic reviews
and meta-analyses); v) meaning of the study, including possible explanations and implications for
clinicians and policymakers and other researchers; vi) how your study could promote better
decisions; vi) unanswered questions and future research
g. footnotes and statements
online and print publication: all original research in the bmj is published with open access. our open
access policy is detailed here: http://www.bmj.com/about-bmj/resources-authors/forms-policiesand-checklists/copyright-open-access-and-permission-reuse. the full text online version of your
article, if accepted after revision, will be the indexed citable version (full details are at
http://resources.bmj.com/bmj/about-bmj/the-bmjs-publishing-model). the print and ipad bmj will

carry an abridged version of your article. this abridged version of the article is essentially an
evidence abstract called bmj pico, which we would like you to write using the template downloadable
at http://resources.bmj.com/bmj/authors/bmj-pico. publication of research on bmj.com is definitive
and is not simply interim "epublication ahead of print", so if you do not wish to abridge your article
using bmj pico, you will be able to opt for online only publication. please let us know if you would
prefer this option. if your article is accepted we will invite you to submit a video abstract, lasting no
longer than 4 minutes, and based on the information in your paper’s bmj pico evidence abstract. the
content and focus of the video must relate directly to the study that has been accepted for
publication by the bmj, and should not stray beyond the data.
date sent: 14-apr-2016



<|EndOfText|>

table 1 reports the baseline characteristics of the propensity score match cohort. it’s usually good
practice to report baseline characteristics of the unmatched and matched populations, along with a
metric to show balance across each of the baseline characteristics (i.e. an absolute standardized
difference; not p-values).
the methods are somewhat vague. more information is required on the matching procedure. simply
saying it was 1:1 is vague, what was the caliper width, did they use a greedy matching algorithm etc…no
mention is made in the methods (under ‘propensity scores’) as to how balance was assessed after
matching. there are 7.6 million courses of penicillin, did the authors not consider 1:2 or 1:3 matching, to
make use of more data?
i could not see any mention of missing data/data completeness. there are lots of covariates (table s2).
how was missing data handled? presumably they were omitted, which may be a concern. but we have no
idea how many were omitted.
the unit of analysis is course of fluoroquinolone/penicillin, not at the patient level. so how many patients
did ~2 million courses cover? so in table 1, if n is number of courses and not number of patients, what
is ‘male sex’, because won’t there be over counting (i.e. if one man has 5 courses, has is this counted in
the number of men in table 1, as 5 or 1?), because of multiple courses per patient? similar comment to
age. the analysis uses appropriate methods for multiple courses per patient (i.e. the gee analysis), but
i’m unclear as to whether one needs to balance at the patient or the course level. i think the reader
needs to be reassured that the two populations are comparable. can the authors clarify.
the two countries are quite different in terms of prescribing. sweden had nearly double the
flouroquinolone courses than denmark, but the reverse for penicillin (denmark had over double the
number of courses compared to sweden). does this matter for balancing? can the authors comment on
this, and satisfy the reader that this is not a concern.

<|EndOfText|>

the authors are treading a well-worn path having developed numerous risk scores from the qresearch database.
risk scores for blindness and lower limb amputation have been developed , internally validated and externally
validated on very large datasets.
the methodology is strong and the authors adhering to recommended practices in developing and validating risk
scores. the authors have also carefeully followed the tripod reporting guidelines for prediction models.
my comments are minor.
abstract. make it clear that the patients come from the uk.
page 9/page 10: multiple imputation. some more information would be useful, whether in the main paper or in
supplementary material, that includes what variables were included, any transformations etc.
page 10. confirm that the area under the receiver operating characteristic curve has been calculated for survival
data (and not for binary outcomes).
page 10. why are thresholds of 10% and 20% chosen? is this for illustrative purposes only? if these are thresholds
recommended by the authors then some rationale would be needed. alternatively, the authors could produce net
benefit curves (decision curve analysis, vickers et al 2006; med decis making) to examine this.
page 11. whilst missing data is mentioned for particular variables, how many of the cohort had complete data (or
conversely how many had at least 1 missing variable) in the development cohort.
page 12. whilst missing data is mentioned for particular variables, how many of the cohort had complete data (or
conversely how many had at least 1 missing variable) in the validation cohort.
page 12. whilst the authors mention the availability of the risk score at the start of the manuscript. it would also
seem a natural place to discuss this in the results section, preferably with a brief explanation as to why the model is
not published in the paper. i know the reasons as to why this is the case, but the average reader won’t. the authors
are one of the few teams i am aware that actively maintain and update their models on an annual basis, and this
should be highlighted as it is a particular strength of these models/group.

additional questions:
please enter your name: gary collins
job title: associate professor
institution: university of oxford
reimbursement for attending a symposium?: no
a fee for speaking?: no
a fee for organising education?: no
funds for research?: no
funds for a member of staff?: no
fees for consulting?: no
have you in the past five years been employed by an organisation that may
in any way gain or lose financially from the publication of this paper?: no
do you hold any stocks or shares in an organisation that may in any way
gain or lose financially from the publication of this paper?: no
if you have any competing interests (please see bmj policy) please declare them here: i led the development of the
tripod guidelines for reporting clinical prediction models.



<|EndOfText|>

sample size: no rationale for the effect size (why 13%?), what is this based on? no citations to where
the estimates for the sample size calculation have come from, neither in the paper nor the protocol.
13% is also twice as large as the effect size reported in the introduction in the chinese rct. can the
authors provide the missing information. also can the authors clarify the significance level used in the
calculation? if 0.01 as specified in the paper, then i am unable to reproduce the numbers, if 0.05 (as
specified in the protocol), then i can. i presume the 0.01 in the paper (page 8) is a typo.
table 1: would be useful to include the minimization factors in this table.
table 2, p values should be removed.
table 4: can the authors confirm all these outcomes were pre-specified? from a look at the trial registry
and the protocol, it appears not. e.g., the maternal complications, prematurity. for clarity it would be
easier if all the secondary outcomes were reported in the same table (for the itt analysis).
i couldn’t see the results from the qol?
i’m not sure i agree with the conclusion, based on the findings of this trial. ’currently, there is no
indication supporting the wide use of a freeze-all strategy in the broad population of ivf patients.
present evidence suggests that the freeze-all strategy should be adapted as part of individualised patient
care only in women with pcos or those at increased risk of ohss and not in the general ivf population.’
there is also no discussion on costs-effectiveness (a pre-specified outcome). (note from dr. loder: if
you plan to publish a separate ce analysis, it is fine to simply state that this will be reported in a
separate paper.)

additional questions:
<b><em>the bmj</em> uses compulsory open peer review. your name and institution will be included
with your comments when they are sent to the authors. if the manuscript is accepted, your review,
name and institution will be published alongside the article.</b>

if this manuscript is rejected from <em>the bmj</em>, it may be transferred to another bmj journal
along with your reviewer comments. if the article is selected for publication in another bmj journal,
depending on the editorial policy of the journal your review may also be published. you will be contacted
for your permission before this happens.

for more information, please see our <a href="https://www.bmj.com/about-bmj/resources-reviewers"
target="_blank">peer review terms and conditions</a>.

<b>please confirm that you understand and consent to the above terms and conditions.</b>: i consent
to the publication of this review
please enter your name: gary collins
job title: professor of medical statistics
institution: university of oxford
reimbursement for attending a symposium?: no
a fee for speaking?: no
a fee for organising education?: no
funds for research?: no
funds for a member of staff?: no
fees for consulting?: no
have you in the past five years been employed by an organisation that may
in any way gain or lose financially from the publication of this paper?: no
do you hold any stocks or shares in an organisation that may in any way
gain or lose financially from the publication of this paper?: no
if you have any competing interests <a
href='http://www.bmj.com/about-bmj/resources-authors/forms-policies-and-checklists/declaration-co
mpeting-interests'target='_new'> (please see bmj policy) </a>please declare them here:



<|EndOfText|>

the authors have addressed all my concerns.
regarding the calibration plots, the authors have concern that the revised plots may be difficult to
interpret, primarily the miscalibration at the higher end of risks. personally i would truncate these plots,
the authors can confirm, but there will likely be very few events at this extreme (i.e. not much data, and
therefore the model doesn't appear to work as well). so 1a and 1b have the plot region 0 to 0.3 on both x
and y-axes and 1c and 1.d, have the plot region 0 to 0.2 on both axes. the authors can make a
comment in the paper that there is some miscalibration at the higher end - this is the region where
miscalibration is probably less important anyway, as they are high risk anyway.
presenting merely the calibration plots in groups of ten equal size (which is common practice), some
would (and do) argue could be misleading, because of the arbitrarily grouping of patients...

<|EndOfText|>

the authors have generally clarified most of my previous concerns.
however, i still remain unclear regarding the inclusion of the prediction model in this paper. it is a
distraction from the main aim of the paper, which is about diagnostic accuracy of ct angiography, mr
angiography etc. the authors are reporting a lot of information which deserves to be split into two papers
so that the message of the two aims do not become diluted.
i fully appreciate that prospectively collecting data to externally validate is not a trivial exercise and will
take time. but suggesting that the model should be used is problematic without demonstrating it ‘works’
- particular when the sample size is moderate. was any attempt made to identify existing datasets that
the model could be evaluated on? developing a prediction model is a separate study and deserves to be
seen as such so that all aspects in deriving and internally validating the model can be explored in more
detail than is currently described here (again adhering to the tripod statement for prediction models;
www.bmj.com/content/350/bmj.g7594 and annals.org/article.aspx?articleid=2088542 - which the
authors appear to have overlooked despite being previously pointed towards tripod). the actual
development of the prediction model looks well done and thus just confirms my opinion that this should
be a separate paper.
as an aside presenting roc curves without labelling particular cut-offs on the curve is uninformative,
labelling the curve can then allow sens/spec to be read off at particular cut-offs. the calibration plot is
ok, but it is widely recommended to also superimpose a lowess calibration curve in addition to the
observed and mean predicted probabilities at fifths of predicted risk. quintiles is incorrectly used, a

quintile is a cut--point to create equal sized groups, the correct term if fifths (see the bmj stats notes,
www.ncbi.nlm.nih.gov/pubmed/7950724). hosmer-lemeshow test is an uninformative measure and does
not assess calibration, widely influence by sample size and grouping, but importantly doesn’t assess
direction of magnitude of (mis)calibration). (see tripod explanation & elaboration paper for more
details; annals.org/article.aspx?articleid=2088542).
the sample size for the main diagnostic accuracy study has now been reported but included in a
supplementary box. why? this should be in the main body of the paper and not tucked away in
supplementary materials.