thanks for the suggested amendments.
just a few more minors relating to the same issues i raised before but i don't necessarily
need to see the changes.
the authors have now clarified that logistic regression models were used - this is now
much clearer on what was done. traditionally these models produce odds ratios, but the
authors report risk ratios with 95% confidence intervals (table 4) and the risk rate
difference. in the text these are listed as irr's. for the readers, it might be worth adding
a sentence on how these measures were computed from the logistic regression models
fitted.
please also note you have changed rate ratios to risk ratios - this is not consistently
changed throughout - e.g. table 4 header still reads 'rate'.

additional questions:
please enter your name: jamie kirkham
job title: senior lecturer in medical statistics
institution: university of liverpool
reimbursement for attending a symposium?: no
a fee for speaking?: no
a fee for organising education?: no
funds for research?: no
funds for a member of staff?: no
fees for consulting?: no
have you in the past five years been employed by an organisation that may
in any way gain or lose financially from the publication of this paper?: no
do you hold any stocks or shares in an organisation that may in any way
gain or lose financially from the publication of this paper?: no
if you have any competing interests <a
href='http://www.bmj.com/about-bmj/resources-authors/forms-policies-and-checklists/d
eclaration-competing-interests'target='_new'> (please see bmj policy) </a>please declare
them here:



<|EndOfText|>

the authors have provided some detailed responses to reviewer/editorial comments. some of the
understanding of this paper and indeed the 'science' behind the approach relies on previous research
carried out by the authors - i think this has now been adequately addressed.

however, i do still have a slight niggling issue (as did most reviewers) about the diagnostic test
accuracy measures presented. the most common measures (sensitivity/specificity) and others were not
reported in this paper (even though they were listed in the initial draft). the authors in their response
suggested this was an oversight and referred to a protocol (ref 9). the protocol is however vague and
mentions only 'diagnostic test accuracy measures' will be used. the same authors also conducted a
review and meta-analysis on this topic (ref 5). in this review sens/spec was a measure of choice - this
does raise some signals around selective reporting. perhaps the saving grace is that the authors do
present the tp, fp, fn and tn values so in theory, readers could compute these metrics. does the
presentation of these results tell a different story though? to remove and suspicion, it might be worth
including them given they are relatively simple to compute.
methods - when using backward selection is not more common to uses a retention p-value of <0.1
rather than <0.05. i'm not suggesting repeat the analysis (as this can be seen as arbitrary) - but would
it have made any difference?
appendix 2/3 - what are the denominator values?
the authors use pre-defined rules for plrs and nlrs. this authors comments on the point estimates
which is fine, but you may also want to comment on the uncertainty (i.e. confidence intervals). as an
example, while 'raised calcium' did not reach a plr of 5, the confidence interval includes this value.
i think table 5 is referred to before 4 in the text.

<|EndOfText|>

i'd like to thank the authors for their careful and detailed responses to my initial
comments. the authors present the results in a way that they believe is most informative
to researchers in a similar field. as a result, and from a statistical perspective, i am happy
with the justifications provided on the approach that was taken. importantly the results
presented are data driven and are reported transparently.

<|EndOfText|>

main points:
the original version of this paper is that it contained too much spin. the intervention only
demonstrated a small benefit and emphasis was more focussed on the economic
evaluation. as the intervention was low-cost, the authors were able to demonstrate that
this would reach the thresholds for adoption by nice in the uk, and forms part of a uk
wide health priority goal which is being implemented. in the revision, much of this spin
has now been removed, such that the he analysis does not overshadow the efficacy
benefits.
the protocol was published in july 2016 (trials), and they do mention some health
economics in this - recruitment began about this time, so i think the he evaluation was
always planned in this study (the protocol certainly also contained well known health
economists).
this trial was well done, it’s now much better reported, transparent and balanced – we can
tell what they did and it is important for uk policy at least.


<|EndOfText|>

i would like to thank the authors for addressing my initial statistical comments. here are a few final comments on the
manuscript - these are only minor but i feel they would improve the manuscript. point 10 is the most important to
consider.
1) line 163: change chi-square to chi-squared. similar in table 1. please check elsewhere
2) in the methods section - when referring to your poisson models, i would say that you are estimating relative risks
and 95% ci's
2) please be consistent with the reporting of p-values. for example in the text p<0.0001 is reported compared with
p<0.001 in the table. perhaps the bmj has standard guidance on this.
3) line 201: a p-value is reported as p<0.0001 - from the way this is written, it looks like this is the difference in
fracture rates between the three group but there is no p-value in table 2, it's left as '----'.
4) the use of the way p-values is reported in table 2 is confusing. the non-obese group is used as a reference and then
one p-value is reported. there should be a related p-value for both the obese and bariatric groups. from the ci's i can
see they are both significant and probably have the same p-value as reported, but this doesn't necessarily have to be
the case - same problem in table 4, here there may well be a difference where you report exact p-values. i would also
present the rr and ci's in the text as well as the p-values.
5) from table 2: the period after the index date results are not described in the text as presented in the table, rather a
new result is presented (data not shown in tables) - i found this confusing.
6) table 3 - for clarity i would also label as the pre- and post- surgery period to be consistent with the text. same for
other tables while it's kind of obvious, for clarity i would also label 'central fractures' as you refer to in the text.
7) line 215, i'm not sure i like the term 'increased' or 'gradient' - these terms are usually used to describe a trend - i
think you are merely pointing out that the proportion of fracture rates are 'lower' or 'higher' on average.
8) line 223: you have switched. in line 222 you talk about higher risk in bariatric group and obese group first then on
line 223 you talk about risk reduction in obese then bariatric group - please be consistent in the way you report.
9) line 226: whilst reported correctly, i'm not sure why you switch from reporting the rr and ci for one comparison and
then the % difference for the others.

10) i am confused with figure 2 - what is presented here that is different from table 4 results. based on the numbers,
it's not obvious the message is the same.
11) line: 253. 'surgery groups and all signficantly higher than the non-obese group' significant in a statistical sense?
how do you know? same when you refer to adjusted fracture risk being similar between surgical groups (pre-surgery) are they that similar? most seem closer to the obese group but agp seems stand-alone.

<|EndOfText|>

thanks for making the revisions. in light of the extra clarifications made and author
responses to initial queries, i have a number of further concerns that i would like the
authors to consider.
1. reporting guideline: in the author’s response, they suggest that the review follows
‘prisma protocol’. this is unclear because prisma is the guideline for final reporting of
reviews but prisma-p is the protocol
moher d, shamseer l, clarke m, ghersi d, liberati a, petticrew m, shekelle p, stewart la.
preferred reporting items for systematic review and meta-analysis protocols (prisma-p)
2015 statement. syst rev. 2015;4(1):1
for prisma, please use bmj reference.
2. registration: in the author’s response, they suggest the study was registered with
prospero and the registration is still pending – if this is the case then this should be
followed up. however, it seems unlikely that this would take this long if the study was
registered prior to the reviews search being carried out – were attempts made to register
this study retrospectively rather than prospectively – please be transparent. my experience
is that registration can be rejected by prospero if they believe much of the work has
already been completed.

3. search strategy: in the manuscript, the authors suggest that unpublished studies were
excluded because they may be lower in quality than published studies without peer review.
this is a strong claim - the authors need to provide some references to back this claim.
there is in fact growing evidence that there may be little difference in the quality of articles
without peer review (preprints) compared to those with peer review. the reviewers
response is also confusing because they detail ‘strategies they used to identify unpublished
studies’. so why would you look for unpublished studies then exclude them all based on
quality? if there is a protocol, surely this exclusion criteria would have been documented?
incidentally a good review of how to search for unpublished data was published by bmj.
https://www.bmj.com/content/344/bmj.d8013
4. pg 6. line 84. in the revision, the authors now specify that those studies that reported
on survival rather than mortality were excluded. can the authors explain the rationale for
this? in addition, amongst the ‘effect sizes’ of interest you were including hazard ratios –
these typically correspond to survival outcome.
5. risk of bias assessment: the authors originally used n-o scale which i did suggest was
ok with the suggestion ‘robins’ might be a more up-to-date tool to use. the authors now
use robins-i which is for interventions but they modified it for exposures. how did you do
this? the exposures tool is currently under construction but there are some preliminary
tools available for this purpose (see website).
https://www.bristol.ac.uk/population-health-sciences/centres/cresyda/barr/riskofbias/robins
-e/
this reference might also be helpful:
https://www.ncbi.nlm.nih.gov/pmc/articles/pmc6302384/
6. statistical methods: the authors might want to explain how and the rationale for
converting ors, rrs and hrs from the study papers in to log rr +/- se. is this to obtain a
single ‘effect size’ from each study? this isn’t clear nor while you later refer to (page 8, line
134) log or/rr/hr – have you not already converted these to the same scale?
minor: pg 7 line 115: >50%
pg 8 line 124 ‘pre-defined criteria’ – what is this criteria?
7. results
a) figure 1: the text suggests that 57 full text articles were potentially relevant. figure 1
suggests this is 53. the statement about the total number of included studies also doesn’t
marry quite right. the text suggests there were 31 studies from 30 publications but figure
1 suggests this is 30 studies (not publications). i would update the information in figure 1
for clarity, e.g. 31 studies from 30 publications if this is correct.
b) table 1-3: the male/female split under the sample size column is unclear, e.g. m/f 7216.
how many were m/f?
c) table 1-3: i was also unclear what was meant by the column ‘follow-up’
d) table 1-3: information in the ‘comparison’ column needs better explaining.
e) i’ve done one spot check in the tables. pg 10 line 178 it is reported 16,085 cvd deaths.
if you sum the cases in table 2 this comes to 21,748 – is this correct? please check all
numbers in tables/text as i also believe other’s don’t match.

f) rob assessment ‘pg 11, line 192’. the authors report that none of the included studies
had a low rob in all components. this is a bit misleading as about 11 studies did have an
overall rob assessment as ‘low’ and a ‘low’ assessment in 6 of the 7 domains. in the other
domain, the authors report ‘no information’
from my point in 5) do domains 3 and 4 need to relate to ‘exposure’ and not ‘intervention’
when being referred to in supp table 2. i would use the guidance from this reference again.
perhaps when put into this context there is some information on the ‘change from intended
exposure’. supp table 2 would benefit from some information on how you arrive at the
‘overall judgement’
https://www.ncbi.nlm.nih.gov/pmc/articles/pmc6302384/
g) page 11 ‘findings from the systematic review’ : it is unclear to me which tables i need to
refer to in order to find the information being discussed. this section reports 28 studies on
the association between total protein intake and a/c mortality but table 1 only lists 24
studies? perhaps you can indicate in the table results somehow in the ‘es (95%ci)’ column
the different associations you found, e.g. demarcate ‘non-sig results’, ‘inverse association’
and ‘positive association’.
h) pg 11, line 208. looking at the tables and the forest plot together you have combined
both data from hr and rr – this is not advisable. rr are based on a single time point.
what you describe as the pooled ‘es’ maybe seen as somewhat meaningless. can you not
do a subgroup analysis for those that report rr and hr? perhaps what you have currently
shown can be seen more as a sensitivity. see my point in 6 also where there is a suggestion
you may have used the raw data from the individual studies to calculate the same effect
estimate from each study. this reference might help
https://www.ncbi.nlm.nih.gov/pmc/articles/pmc1920534/pdf/1745-6215-8-16.pdf
the approach in h) will follow on with the subsequent analysis in this paper, mainly the
dose-response analysis.


<|EndOfText|>

this is an interesting study on an important topic area. my comments are aimed at
improving some of the reporting of the results in the paper. most results are crude
estimates - this is perhaps unusual but the authors do present some adjusted
analyses in the supp material and show little differences.

efigure 1: the flow diagram starts with the total number of women with first and
second singleton deliveries and finishes with 302,192 women giving birth (after
exclusions). however, he primary analysis considers only 284,225 women with a
term first birth – this should be reflected in the flow diagram.
the authors fit separate models for each complication with none of the 5
complications as a reference. this seems plausible – the authors then run
additional models when term complications co-occur in order to estimate
associations between having one condition or 2 or more conditions. what were
these additional models fitted? it is assumed that there may be some correlation
between the ‘co-occurrence’ of these complications – how was this handled in the
modelling?
for all models fitted what were the model assumptions and were these assumptions
satisfied.
the impact over time was assessed by looking at three different time periods of the
2nd birth. the three periods are not equally spaced – how were these derived?
why not look at the effect over time as a continuous function given there is nearly
50 years worth of data.
table 1 list the demographics of the data. several other characteristics are used in
the sensitivity analyses. to understand these later analyses better, it might be
beneficial to include the demographics to all these variable used in table 1.
table 1 – for the complications where the denominator is not 301,192, it would be
beneficial to tell us what these are – it’s not clear form the % alone. perhaps list
this next to the complication name.
table 2 (left side) / figure 1 contain the same information. have the authors
considered producing a forest plot that also presents the numerator / denominators
and rr estimates and confidence intervals – similar to those produced in cochrane
reviews. these can be produced in various software.
also consider reporting the
absolute risk reduction with ci’s for each complication compared to no complication
the authors report “in the aggregate, having one of these complications led to an
increased relative risk of preterm of 2.3 (2.2-2.5), while having two or more term
complications produced a relative risk of 3.8 (3.3-4.5)”. i gather this is compare to
having no complications. this seemed quite high level – i guess what would be of
more interest is to understand the relationship between the different complications.
e.g. what combinations of complications lead to preterm birth.
the authors report “there was a general trend for these risks to increase over the
three time periods” i prefer if the term ‘trend’ is not used if this isn’t more formally
tested for. testing for trend is a specific statistical technique – this isn’t done here.
table 2 (right side) – again the forest plot idea above could be utilised here. the
reference categories used weren’t too obvious here. i guess the confusion to the
current table 2 is that it is unclear if we should be looking down the columns or
across the rows.
in all supplementary tables, please present all the numerators/denominators when
computing rr – this is important for reproducibility of results.


<|EndOfText|>

i would like to thanks the authors for using the most up-to-date data in their revision. i
have an number of suggestions for improvement.
on page 12 the tramadol only category was described as the comparison category in yet in
the formal analysis description (pg 13) short acting opioids (excluding tramadol) was the
reference. this needs to be consistent.
i believe that the statistical analysis was carried out correctly and it is likely that
'appropriate' methods were used but the statistical methods section need further clarity to

understand what was done. as an example, the authors mention that 'regression models'
were used but what were these models and what were the assumptions and were these
met?
there is discussion around censoring so were survival models used (or were
censored events ignored in the analysis)? this is inefficient. the section previously on
'definitions of prolonged opioid use' also mentions the use of logistic regression - this
should ideally be put in the stats method section.
figure 1 - perhaps report % on the y-axis and not proportion to be consistent with the
reporting in the manuscript.
table 1 - i would also report the 'male' data. this table is quite large and at the editors
discretion may need modifying.
figure 2 - when describing the results to the higher median discharge fill, you may also
want to comment on the uncertainly - the range is very large. where you provide median
fill data in the text, i would also add the interquartile ranges.
thanks you for also considering variants of the exclusion criteria in your response. for
transparency i would recommend that the main analysis remains as planned using the
updated dataset. this is to avoid any potential for bias from looking at the results form
several analyses.. other analysis could be included if of interest and labelled something
along the lines as 'post-hoc' analysis.


<|EndOfText|>

this is a through examination of the data - the methods in the revision are much
clearer, particularly around the propensity score matching. the results have
been verified in two independent cohorts which is credible. i have no further stats
comments.
there is a lot of material in this paper (tables/figures) particularly with the
analysis of two cohorts analysed in two different way - i.e. the 'primary analysis'
and the 'propensity score' approach - i think the authors would need to work with
editors to determine which material should go into the main paper. table 2/5 i
think are the most important - but these could easily be combined to show the
results to both cohorts in one table.
a couple of minors:

methods - stats section. say that you will report numbers 'and percentages' for
count data. in the results i then think it needs to be clearer in the tables when n
(%) are reported.


<|EndOfText|>

i would like to congratulate the authors for making the necessary amendments to the manuscript. my initial
concern was around the imputation methods used which i am now satisfied with.
one minor point, i would probably prefer to see the section on bias (line 167) in the discussion section.

<|EndOfText|>

general:

1)
‘what is already known on this topic’: bullet point 1), the authors pose a question
– under this section, this should ideally be a factual statement.
statistics:
1)
procedures: can the authors justify the use of the 5 follow-up cohort years from
diabetes onset – how does categorisation increase power?
2)
procedures: the authors suggest that the same patient might be observed in
more than one cohort – how frequent was this? how was this accounted for in the analysis
(i presume this is the gee model) – there is the potential that the inferences made may be
affected if the same participants are used multiple times in the analysis?
3)
procedures: similarly can authors justify the reasons for the categorisation of
hba1c and then using the continuous variable to estimate risk of complications? the latter
is often preferred. there are a lot of numerical results in this paper and the categorisation
clearly adds to this (table 2). what’s more, the number of events in some categories is
small which in some cases leads to a reasonable degree of uncertainty in the results (wide
confidence intervals).
4)
statistics: in the analysis of hypoglycaemia events, how was censoring taken into
account which was described in the ‘procedures’
5)
statistics: the mention that no imputations were made. i did not get a sense of
how much missing data there was. how impactful might this be on the analysis?
6)
units – this is not my area of expertise but can the authors clarify if the data
relates to hbac1 achieved or targeted results? these may not necessarily be the same.
also is there value in presenting results by % hba1c and using mmol/mol
7)
statistics: can the use of 6.5-6.9% be justified as the reference category in the
regression. typically to assist with the inference, a category with the larger number of
events is chosen, this may help with the point made in 3).
8)
the authors present a lot of results in the text – in particular in the abstract i felt
the inclusion of no less than 10 results detracted from the main message. what are the
key results?
9)
there are some disparities in the ‘significance’ of some of the results between
adjusted and unadjusted analysis. it was good to see that the authors presented both
results but i would recommend the use of the term ‘significant’ is dropped when describing
these results. this largely falls back to the uncertainty in the estimates found –the true
differences is best left as presenting the ci’s (though i’m not suggesting the p-values are
removed from the table).
10)
figure 1 – i found this a little bit difficult to interpret given it was a little ‘busy’. i
reviewed in black and white – does colour help with this? the actual numbers are provided
in the supplementary which is helpful.
11)
figure 2.1 and 2.2. this is difficult to understand giving the point you are trying
to make. you report that the deviation occurs at 8.1% for 16-20 years and 8.6% for
shorter follow- up. this maybe true – but this covers a very tiny space on the plots to
see this. you also talk about ‘decrease’ over time – this might be confusing to some
readers as the lines ‘increase’. is there a better way to present/describe this? figure 2.2
– for two of the lines you observe overlap – is there any rationale for this?

additional questions:
please enter your name: jamie kirkham
job title: md
institution: university of liverpool
reimbursement for attending a symposium?:
a fee for speaking?:
a fee for organising education?:
funds for research?:
funds for a member of staff?:
fees for consulting?:
have you in the past five years been employed by an organisation that may
in any way gain or lose financially from the publication of this paper?:
do you hold any stocks or shares in an organisation that may in any way
gain or lose financially from the publication of this paper?:
if you have any competing interests <a
href='http://www.bmj.com/about-bmj/resources-authors/forms-policies-and-checklists/d
eclaration-competing-interests'target='_new'> (please see bmj policy) </a>please declare
them here:

