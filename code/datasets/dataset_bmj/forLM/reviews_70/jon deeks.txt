statistical review
risk of hypoglycaemia related to the addition of dpp-4 inhibitors to sulphonylureas: systematic review and metaanalysis
the authors have taken on board comments about the computations of numbers needed to treat in their revised
manuscript. i still have the following issues.
1) i previously pointed out that computation of an nnt requires stating an expected event rate, and as more events
accumulate the longer a group is followed, any computation of the nnt needs to be conditioned on a stated period of
follow-up. in their response, the authors agree with this, and identified estimates of expected event rates for different
durations. however, most of their nnt computations, including those in the abstract are for studies with mixed and
unstated follow-up, which continues their original error. whilst it is always possible to put together a set of studies with
mixed follow-up to compute an expected event rate, the answer you obtain cannot be applied. i would request that any
nnt that is stated in the paper is specific to a stated follow-up period.
2) the authors have chosen to compute an expected event rate used for computation of nnts using data from a
different meta-analysis, and not the data in their meta-analysis. i am not convinced of why they needed to do this – and
it is notable that the expected event rates in the chosen ma are around twice as high as in their data, leading to the
nnts being twice as strong (e.g. the nnt is 10 rather than an nnt of 20). given that this computation is now post hoc,
a very strong justification needs to be made as to why the studies in their meta-analysis are not suitable for making this
computations, whereas the ones included in someone else’s meta-analysis are preferable.
3) the statistical reporting in the abstract needs improvement, particularly the methods and results.
4) page 9 (all page numbers refer to track changed word file) states “the study was performed in accordance with
prisma”. prisma is a reporting standard not a performance standard, so it could be reported in accordance with
prisma.
5) page 9 – please check whether the abbreviation pbo is defined – i do not think it is now defined in the text.
6) statistical methods – page 10 states that the subgroups were compared with cochrane q test and i2 index. first it is
cochran, not cochrane. second the cochran q test measures the heterogeneity in a group, not a difference between
groups. please refer to the cochrane handbook for the correct way to describe this test.
7) how did you judge that patient characteristics were imbalanced across groups? this is not included in the quality
assessment, and is usually a very subjective judgement.
8) in reporting the quality assessment, the only detail that is given is that three studies were judged to have a high risk
of reporting bias and one a high risk of detection bias. could you please explain why you came to these judgements?
what was it about these studies which supports these judgements?
9) in the discussion page 15 that the risk of bias assessment indicated that study quality was high which was confirmed

by the grade assessment. as grade is based (in part) on the quality assessment, it is rather a nonsense to imply that
grade confirms the quality assessment was correct? clearer expression is needed. there are other points in the
discussion which need more careful wording – for example where the results of sensitivity analyses are interpreted as
saying there was no significant change – no assessment of the significance of differences is undertaken in a sensitivity
analysis.


<|EndOfText|>

bmj.2014.023279.r1
birthweight and later life adherence to healthy lifestyles in predicting type 2 diabetes: a prospective study
the authors have responded to many of the points made by the peer reviewers and editors.
i have several comments concerning the manuscript.
the headlines from the manuscript are based on the calculation of the population attributable risk, which they
state to be 91%. this is quite sensational, and i have concerns about it from several directions.
1) first, is that it is based on the concept that it would be possible for everybody to achieve the baseline lifestyle
and birthweight categorisation. in the cohorts studied, only 1.91% of participants actually were in this category. i
imagine it must be the least likely category to be in. thus it is rather naïve to consider that this is possible –
particularly to adjust birthweights and to reduce your bmi. it would be much more appropriate for the manuscript
to consider the magnitude of reductions which could be achieved by interventions rather than consider a rather
fanciful idea that all risk factors could be removed.
2) second, despite the many comments from the reviewers, the authors do not seriously discuss the possibility
that the relationships observed could in anyway be causal. they are clear that they are making an assumption of
causality, but do not venture into discussion as to how likely it is that this could be the case.
3) third, the analysis of the event rate in the baseline category is based on little data with few events – there are
confidence intervals included, but it is wise to note that it is based on only 19 events across all the cohorts.
4) finally the authors present data (table 4) which make it clear that bmi is the dominant factor which is driving
the lifestyle relationships – adding bmi as a requirement for a healthy lifestyle increases the percentage
attributable risk from 57.2 to 91.2 – thus it represents nearly 40% of the observed attributable risk. this certainly
needs profiling and discussion – currently there is no mention of it in the text.
5) i would also point to the strange use of “confident intervals”


<|EndOfText|>

statistical review
i appreciate the authors’ explanation of the analysis of replacing red meat with other food types.
you.

thank

i think that it is important to draw readers’ attention to many assumptions and much weaker strength of
this replacement food analysis compared to the simple analysis of changes in red meat intake. for
reasons explained below, i propose that this analysis should be completely removed from the paper as i
believe it is likely to be misleading, and currently is over-interpreted. as this appears to be one of the
objectives of this paper, this will be a major change.
the analysis of the red meat changes is based on identifying cohort members who we know did change
their intake and observing their mortality. this change analysis is the main strength of this paper.
however, the replacement analysis is a statistical modelling exercise utilising data across the whole
cohort, without ever identifying whether there is a single individual in the cohort who actually replaced
red meat with any of the other food types. from the authors’ explanation i understand that the analysis
has estimated the change in mortality with unit change in each of the different food types (for example,
an increase in mortality of x% with a unit increase in red meat, a decrease in mortality of y% with a unit
increase in fish) and then has summed the two coefficients to predict what would happen if you made
both changes together. there does not actually need to be any individual in the cohort to have made
that combination of changes to obtain that estimate.
i understand that this analysis is actually assuming that the changes in mortality with each food type are
independent. whilst the uncertainty is the estimate is taking account of shared variance (reflecting the
correlation of changes in consumption of different food types across the cohorts) it is assuming, for

example, that the change of mortality with different changes in fish consumption would be the same
regardless of the changes in consumption of all other food types, including red meat.
understanding how replacement of one food type with another would actually impact on mortality would
require investigation of interaction terms between each combination, for which these data are unlikely to
have statistical power (it will depend on the exact numbers of participants who did actually substitute on
food with another). and the reality is most likely that individuals reducing red meat are likely to replace
it with a selection of different food types, and not a single food type, so such a simple analysis would be
impossible.
i fear that inclusion of the current replacement analysis is creating an illusion of understanding the
actual impact of replacement when all the dataset can provide are independent estimates of how
mortality changes with independent changes in each food type. for example, quoting from the abstract
“a reduction in one serving/day of red meat accompanied by an increase in an alternative food choice
was associated with a lower risk of death”. this is wrong as the model fitted has estimated how a
change in the alternative impacts on risk of death regardless of whether it accompanies a reduction in
red meat intake.
i therefore cannot see what this analysis is adding over a table which summarises the change in
mortality predicted with unit change of each of the food types (i.e. the main effects). these currently
are not reported in the paper at all, although they are the key parameters included in the model from
which these estimates are obtained. i would therefore urge the authors to abandon the reporting of
this replacement analysis, potentially replacing it with reports of the impact of changes in other food
types. interpretation of the “replacement” question can only be speculative.
there is much about this study which is strong and well reported, so i would strongly request that the
authors’ consider this request.


<|EndOfText|>

bmj-2019-051763
statistical review
the authors report on a substantial review of studies which have evaluated the impact of
media portrayal of suicide on subsequent suicide and attempted suicide rates. this is an
important topic and the authors should be congratulated on their work. the review appears
generally well done, but there are important aspects of presentation, analysis, reporting and
interpretation which require attention. currently the reporting does not fully comply with the
prisma guidelines.
major point
1. a major limitation of the current report is the lack of tabulation of the characteristics of
the included studies. its absence creates a barrier for readers to be able to understand the
content and methods of the included studies. please can the authors add such a table? as
a minimum it should describe the participants (setting, location, time, recruitment), the
source of the outcome data, the number and duration of different time periods considered
before and after the media portrayal, the samples sizes before and after, confounders
measured and adjusted for, and methods used to adjust for time trends.
major point
2. readers need to understand more about the study designs included. the authors name
the designs as “ecological” studies, which is a concern, as ecological studies would include
studies which compared average rates between different areas. from the brief descriptions
in the text, i presume that studies which are included are in the same area, but from
time-periods before and after the intervention. these have often been called interrupted
time series (its) studies (a study that uses observations at multiple time points before and
after an intervention (the ‘interruption’) - the design attempts to detect whether the
intervention has had an effect significantly greater than any underlying trend over time.)
please better explain the eligible designs, and reconsider the naming of the studies and give
more information.
major point
3. the authors have developed their own quality assessment process for assessing the
validity of these studies and not used any existing tool. i am not aware that there is a
formalised and validated tool available for assessing study quality from its studies.
however, the cochrane effective practice, organisation of care (epoc) group has suggested
risk of bias criteria for its for epoc reviews on their website
(https://epoc.cochrane.org/sites/epoc.cochrane.org/files/public/uploads/resources-for-autho
rs2017/suggested_risk_of_bias_criteria_for_epoc_reviews.pdf) which the authors may
usefully reflect on. the robins-i tool (https://www.bmj.com/content/355/bmj.i4919) was
designed for cohort type non-randomised studies, and thus does not directly apply, but the
concepts included should be reviewed. i am aware that the robins-i team are developing a

new tool for its studies, but this is not yet published. it might be worth contacting the
authors of robins-i to see whether they have any documents which are publicly available.
4. one key issue that has not been considered are the number of time points before and
after the intervention. it is not possible to adjust for time trends unless there are adequate
numbers of points available. it sounds like there may be studies here with a single time
point before and a single time point after which must be considered carefully as they may
not give valid estimates of effects where there are underlying time trends.
5. the current quality assessment is limited to consideration of three aspects, but these are
not described in adequate detail to allow replication of the assessments, particularly for
confounding and time-trends. there is also no tabulation of the assessments for the
included studies. please could the authors give greater detail of the assessment criteria in
an appendix document (maybe with some examples) and provide tabulations of the ratings
by study and reasons for the judgements made?
6. the process by which screening, eligibility, quality assessment and data extraction
decisions were made in the review is not described. usually we would expect to see that
these processes were done in duplicate independently, involving additional experts to resolve
disagreements. please describe.
7. why was embase not searched? embase contains many journals not included in
medline. is scopus not the platform that hosts web of science? please explain.
8. the exclusion criteria state papers were excluded if they did not include primary or
secondary data. please explain what you mean by primary and secondary data.
9. please can you more fully explain how you categorised study results into “increase”,
“decrease” or “no change”. how were the numerical values used? e.g. what constituted a
numerical “no change”? when there were no numbers presented on what basis was the
categorisation made?
10. please can you explain more what was presented in studies which did not provide
sufficient information to extract quantitative estimates of risk. what stopped you using
these studies further?
11. the data synthesis section states that meta-analysis was done using a random effects
model. later in the heterogeneity section it is stated that pooled estimates were computed
using the knapp-hartung method – i think that this extra important detail belongs in the
data synthesis section and not the heterogeneity section. the meta-regression model used
is not properly described in the heterogeneity section (i expect that it was some form of
random effects model, but please specify).
12. the results section does not state how many potential articles were identified in the
search and how many were considered in full – these details are in the prisma flow diagram
but it would be appropriate to briefly refer to them in the results.
13. figure 2 is labelled as both relative risk and rate ratio. please make it consistent. figure
4 has “rate ratio differences” as a title which is confusing.
14. it would be preferable that the rr anaylsis was used for all studies which requires
appropriate data being obtained from studies only presenting rd values. all but two of the
studies which presented rd values were authored by dr stack, who is a co-author of the
review. is he not able to provide rr
15. for all studies it is important to present the raw results in detail – this may be done in
extra tables if the values cannot be incorporated in figure 2 or figure 3.

16. to interpret funnel plots (figure 4) it is helpful to have the overall effect estimate
plotted, as the consideration of asymmetry relates to the overall effect line and not the null
effect line. the interpretation in the text is also considering asymmetry around the null line,
which is not correct.
17. table 1 reports the investigation of sources of heterogeneity. what statistic does irr
relate to here? these values are ratios of risk ratios. this needs explanation in the table,
and in the text. the interpretation of these figures on page 12 in paragraph 2 under
heterogeneity is wrong. they are being interpreted as if they are actually relative risks.
18. how is the r^2 value calculated for the meta-regression? i am not aware that random
effects meta-regression can produce valid estimates of r^2
major point
19. the sensitivity analysis shows substantial attenuation of effect when low quality studies
and studies with very long follow-up are removed. such findings should be given much
greater emphasis in the interpretation of the results (including the abstract). currently this
sensitivity analysis appears to be disregarded in the interpretation of the results. the
bottom line of the paper, that the risk has increased 50-80% is not supported by this data.
the estimates in the high quality studies are of a 6-13% increase – much much smaller.


<|EndOfText|>

statistical review
bmj.2014.023058.r1
arthroscopic surgery for the degenerative knee: a systematic review and meta-analysis of
benefits and harms
the authors have submitted a strong response to the comments made with an appropriately
revised manuscript. they have updated the analysis as i suggested, and their key findings
remain strong (if not stronger) than in the original manuscript.
i have one comment that i would like them to address:
1) the conversion of the effect size to the vas scale difference is not fully described. it must be
made based upon some presumed value of the standard deviation of the vas scale. as an es of
0.14 corresponds to a vas difference of 2.4mm, so i presume that the sd is about 17mm). it
would be appropriate to fully explain this in the paper and explain where the value of 17mm
has arisen from


<|EndOfText|>

statistical review bmj.2014.023070.r1
consumption of sugar-sweetened beverages, artificially sweetened beverages and fruit juice
and incidence of type 2 diabetes
the authors have submitted a revised manuscript.
the authors have now included a quality assessment process for the included studies which is
based around the cochrane acrobat-nsri tool for assessing risk of bias in non-randomised
intervention studies. i am not completely clear from the manuscript how they have undertaken
this process or that the results of it have been appropriately presented.
1. page 6 – they do not describe whether the acrobat-nsri assessment was undertaken by a
single observer, checked by a second or done independently in duplicate.
2. page 6 -it also state here that sensitivity analysis was undertaken for each of the seven
quality domains in the acrobat-nsri tool, but no results of these are mentioned in the text or
presented in any table.
3. page 9 – a key aspect of the acrobat-nsri tool is documenting which confounders are
balanced/matched/adjusted for in each study. there is no list of the confounders reported – the
best description is given in the middle of page 9 as “socio-demographic variables, clinical
factors (family history of diabetes or prevalent diseases) and lifestyle factors including a diet”.
however, the adjustments used in table 2 are noted in the footer as being only for
“demographic and lifestyle covariates” which isn’t the same. the legend for figure 1 doesn’t

mention adjustment for these factors at all, which i presume is an oversight. i would have
expected, particularly given the extension supplementary material provided, to have a
tabulation of the actual adjustments made study-by-study. table s4 gives the results of
unadjusted and adjusted analyses and does not fully state what was adjusted for.
4. it is also not clear what criteria were used to rate the risk of confounding. in table s2 it is
noted that only one study was rated as being at high risk of bias due to confounding. table s4
lists four studies of omitting adjustment for “diet and clinical factors” so it is not clear why
these are also not flagged as being at risk of bias from confounding. other studies may omit
other key variables, but given that no list of variables is presented we cannot tell.
5. there is no mention whether the analyses of the drink types are mutually adjusted for each
other (for example, is the ssb analysis adjusted for asb and fruit juice?) it is hard to think that
consumption of each drink is independent.
6. table s2 and supplementary material on page 17. overall quality assessment has to make a
leap from the ratings of the individual domains to obtaining an overall assessment of likelihood
of bias. it is not clear what rule the authors used to achieve this. the text on page 17 does not
describe a consistent system for doing this. for example, studies 25 and 48 are stated as being
at high risk of bias because their classification of diet was wrong, but the other seven studies
marked as having high risk of bias on the dietary measures domain are not classified as being
at high risk of bias overall. the same problem appears across multiple domains in the tool.
7. page 13 – the authors indicate that publication bias created a false positive effect for asb.
however, the degree of publication bias seems rather small (not really visible at all in the
funnel plot) and adjustment for it did not substantially change the magnitude of the effect.
8. page 11 - the authors make grade assessment and place two outcomes (asb and fruit
juice) as being of low quality and one (ssb) as being of moderate quality. there is no strong
argument why ssb is argued to be of moderate quality. it is hard to see why one outcome
would differ from the others given that they are all reported in the same studies which were
done using the same methods and adjusted for the same confounders. given that the estimates
for both ssb and asb shift considerably between the analyses adjusting for measurement
error, confounders and publication bias, it is hard to attribute moderate or high credibility to
any of them. the estimate for fruit juice seems to be close to a null effect in all analyses – the
authors seem distracted by it moving either side of the null effect value, but it seems
consistently close to it in all analyses.
9. the population attributable fraction computations are based on assumptions of causality, and
on reducing consumption of the three types of drink to zero. given that the estimates of effect
vary considerably between the sensitivity analyses, they perhaps should investigate how much
the estimates varies. the authors do state the assumptions behind this, and indicate that
causality is a concern, but are keen to promote intense public health interventions based on
this evidence. would not trials of ssb reduction be justified now rather than public health
interventions? also i wonder whether the illustration would be more helpful if it were based on
the sort of magnitude of reduction in ssb that was achievable by a public health intervention
and not an unachievable reduction to zero.

<|EndOfText|>

bmj.2015.025078
consumption of spicy foods and total and cause-specific mortality: a 7-year prospective study of 0.5 million
chinese adults
the authors have made substantial revisions to this manuscript in responses to comments made, and all
addressed all important points.
i have no further comments to make.
my apologies over the time taken for me to complete this review of the revision.

<|EndOfText|>

statistical review bmj.2015.024934.r1
first, please note that i am working with three of the authors of this manuscript on a collaborative project
(ben van calster, dirk timmerman and tom bourne). i believe that i have given a fair judgement of the
paper, but please be aware of this potential conflict of interest.
the authors report the experience of a cohort of women presenting with threatened miscarriage where the
first us scan revealed a pregnancy of uncertain viability, and validate and develop ultrasound criteria that
can be used to identify pregnancies which are nonviable.
1. i found it challenging to identify the flow of women through the initial ultrasound presentations through
to the second ultrasound examination. presentation as a flow chart would greatly assist.
2. the current paper lacks a description of the analytical plan in the methods section. i would have liked to
see a clearer divide in the paper between previously proposed which are being validated, and new criteria
which are being derived from the data collected in this cohort. my understanding is that of the proposed
criteria in table 4, only the 1st, the 3rd and the 4th are validations of existing criteria, all the others appear
to be new or modified criteria. it is also not clear what strategy was used to derive the new criteria.
3. i wondered why the authors did not present 95% confidence intervals of the positive predictive values.
these statistics are the most informative to a woman and clinician, and it is important to be aware of the
uncertainty with which they are estimated. in some cases, the number of women in the cohort meeting
each criterion is very small, and thus the confidence intervals will be quite wide. for example, the initial
scan criteria are based on detecting 0/12 and 0/17. 95% confidence intervals on these predictive values
extend upwards to 26% and 20% respectively, so i am not as confident as the authors that they have
conclusively shown these criteria to be robust. whereas the positive predictive value for absence of heart
activity on two consecutive scans has an upper limit of 2.5%, which is much more convincing.
4. given the difficulties in obtaining evidence for some of the criteria due to the small number of women
involved, i wondered whether combining data with that from other studies in a meta-analysis would be
possible and helpful? perhaps not for this paper.
5. i found it hard to identify data in the paper supporting all of the criteria that are evaluated, particularly
those which combine size by gestation age, size and growth, and size and absence of embryonic heart
activity. there are lots of numbers given in the text, but it would be much preferable to have the evidence
presented in tables like table 2 and 3 for each of the criteria. there is a risk that the “numerical sentences”
in the text end up being ambiguous which presenting the data in a table avoids.
6. it would be interesting to add a discussion about the yield associated with the different criteria. for
example, there appear very few cases where a definitive diagnosis can be made on only the first scan, and
the greatest yield is from noting on the second scan that a miscarriage has occurred (interesting it is not
stated as a criteria that total absence on the second scan is perfectly predictive – is it too obvious? ).
7. the authors have replied to the journal already concerning the informative drop-out. i would encourage
more detail from their additional analysis to make its way into the paper.

<|EndOfText|>

statistical report
the authors now more clearly report the methods which they have followed in producing
this report. it clearly has been a substantial project and the authors should be
congratulated on their work.
however, it is now clear that the reported findings largely depend on the model which has
been fitted to the data, particularly for the countries and regions where data are sparse or

absent. the structure of the model (in terms of the grouping of countries into regions and
regions in super regions) will have had a strong influence on the reported results which
needs consideration and discussion. it is also important to be clear as to what results are
supported by observed data, and which are extrapolations – currently all results are
presented as being equally valid, which is not the case.
1. whilst the use of the hierarchical model is appropriate, the interpretation of results
needs to be made with care. as was raised in the previous comments, there are concerns
that the level of extrapolation that is being used by the model where data are missing for
particular countries, regions or super-regions is inadequately acknowledged by the authors,
and is not suitably flagged to a reader. i would expect to read more about the impact of
the degree of extrapolation in the limitations section of the discussion.
2. the pattern of missing data is very uneven, with some regions and super regions having
far fewer (if any) studies contributing to estimates than others. where no data are
available, my understanding is that the model estimates will entirely be based on
“borrowed evidence” from higher levels in the model, such that the average region value is
used for the country estimate when there is no country specific data, that the average
super region value is used for countries when there are no data within a region, and that
the global average is used when there are no data from within a super region. looking at
the results, particularly in the figures, the data for the regions is presented is the same
way regardless of whether it is based largely on data observed in the countries, or by
“borrowing” from other regions. i believe that it is very important for readers to be able to
distinguish observed results from extrapolated results. could the authors look to augment
the evidence presented in the figures with a summary of the amount of the evidence is
based on observed studies and how much is from borrowing strength? i do not know what
graphical device or statistic could be used, but it is really important to indicate the real
data from the extrapolated data, and the level at which the extrapolation is made.
the authors also need to be clear that the region and super region estimates where data
are sparse are limited in that they are the values based on the small number of studies of
studies in the same grouping. it is important that the reader is not mislead into thinking
that they are based on regional or super regional level studies.
3. i cannot get all the figures in the text to match with those in the tables. for example
page 10 includes the statement:
“psoriasis occurred more frequently in adults than in children. in children, the prevalence
of psoriasis varied between 0.02% (0.01% to 0.04%) in east asia table states to 0.22%
(0.06% to 0.81%) in australasia and 0.21% (0.11% to 0.41%) in western europe (figure
1)”
several of the figures here do not agree with those in etable 7. for example, the estimate
of east asia is 0.03 (0.01 to 0.09). is there an explanation or are these errors? similarly
i cannot locate all the figures in the next statement:
“in adults, the disease varied between 0.14% (0.05% to 0.40%) in east asia to 1.99%
(0.64% to 6.60%) in australasia. other regions with an occurrence of the diseases above
1% were western europe 1.92% (1.07% to 3.46%); central europe 1.83% (0.62% to
5.32%); high-income north america 1.50% (0.63% to 3.60%); high-income
southern-latin america 1.10% (0.36% to 2.96%); figure 2.”
4. there are also issues in the text concerning being clear where the reported values are
from observed data and where it is based on extrapolation from the model. for example,
in the statement immediately above, i do not believe that there were any studies in
southern-latin america which contributed data, so this is an extrapolated value, whereas

other examples are generated from the data. it would be helpful to have some way of
separating out the observed data results from those which are based upon the model.
5. the regions and super regions are not described in the main paper at the moment.
when i found them in the additional document i was rather surprised by their structure, as
they are largely based around income brackets and appear to vary between the different
population groups and do not fit with geography as i recalled it (e.g. argentina being
classified as a north american country in etable 8). it is very important for this structure
to be described as it determines the way in which estimates for missing data are made and
the rationale for this grouping to be explained and justfied. it will have by itself created
the income related pattern that has been commented on by others. if a different structure
were used, estimates for countries with no data will change according to the observations
of other countries within the super region and region. the dependence of the findings
upon both the categorisation of countries into regions and super regions definitely requires
greater discussion and acknowledgement as a limitation in the discussion.
6. i have also observed some discrepancies between the tables in the appendix. for
example, etable 3 lists the countries which provide data on prevalence in children. there
are no datasets listed here from sub-saharan african countries. however, in etable 7
there is a report that data are available for tanzania which contradicts this etable 3.
