i thank the authors for their detailed response. on reading their
response and the revision, some further statistical issues and
clarifications have arisen for them to address going forward, as
follows:
abstract: the design section needs to mention how the cohort study
was set-up or obtained (e.g. prospective, database study, etc),

years of follow-up, representativeness, and that propensity score
matching was done, and briefly how.
abstract and main results: to translate to absolute risk, in addition
to giving the absolute rates, could we also have the probability of
survival at 7 days for each group, and their difference? this would
help the reader know the probability of death is very low.
abstract: give the total number that died.
methods: i cannot see the full list of baseline characteristics given
that were used in the ps approach. it just says: “the ps is the
probability of receiving haloperidol as opposed to one of the three
atypical antipsychotics estimated by logistic regression, given the
baseline characteristics described above.” – but the latter need to
given explicitly, as i can’t find them ‘above’. do you mean to refer
to table s1? i think adjustment factors are crucial, so should be in
the main paper, not the supplementary material.
methods: “we assessed potential confounders for the planned
analyses using information from hospital admission to the day
before antipsychotic initiation.” – how were these assessed?
statistically? clinical judgement? etc
results: i do not follow the comment that “the ps distributions
were largely overlapping with a c-statistic of 0.65” – i think you
need to explain to the reader that the c-statistic is for the logistic
regression model that produces the ps values. it is not a measure
of separation of the ps values for those treated and untreated (i
think? indeed, if it was, this would be cause for concern, because a
c of 0.65 suggests considerable distinction between groups). so this
sentence should be split; explain the ps model and its c-statistic.
then says that ps values were largely overlapping (perhaps a figure
in the sup material to show the 2 distributions for the groups?)
results: in response to my previous comment about the need to
check for non-proportional hazards, it is an important new finding
that there is strong evidence of non-proportional hazards. “the
time by exposure interaction coefficient was statistically significant
(p=0.008), suggesting that the proportional hazards assumption is
not met in the model”. however, the implications of this are only
briefly mentioned in the results and reference given to the
supplementary material. i think more details are needed in the main
text, with results shown for the hr in day 0, 3, 5, 7. 10 as in s4.
indeed, what is rather striking, and worthy of discussion, is that the
hr is close to 1 by day 5, and then becomes < 1 by day 7 and 10,
with almost a significant result in the opposite direction at day 10.
if this is genuine, what are the implications?
i think the issue of non-ph should be given more prominence in the
paper, e.g. what this study adds and the abstract.

subgroup analyses: it is not clear why age, cci, exposure have
been dichotomised. this is not a sensible analysis strategy, and – if
the covariates are of genuine interest – a more powerful and
meaningful analysis would be to consider them on their continuous
scale (at the very least, in addition to the analyses done). this
comment was also made in my earlier review but not addressed as
far as i can tell.
results: “comparison of hrs between patients who were in the icu
(hr=1.11, 0.68-1.81) on the index date and those who were on the
medical ward (hr=2.01, 1.44-2.82) suggests that there might be
effect heterogeneity (p=0.04).” was icu / medical ward part of the
ps matching criteria?
discussion: “the potential adverse effect of haloperidol appeared to
be the strongest during the first few days following initiation,
suggesting an acute harmful effect of haloperidol” – i would tone
down the implied causal language here to something like: “the
association of haloperidol with increased rate of death appeared to
be the strongest during the first few days following initiation, which
if the association was causal, would suggest an acute harmful effect
of haloperidol”
in response to my comment about missing data, the authors say
there is not any missing data in ‘the strict sense.’ so, really no
missing data in any of the variables used in the ps matching model?
i find that hard to believe based on my personal experience, but if
so then this should be added, i.e. that there was complete data for
all individuals. however, in one subgroup analysis they say: “it
should be acknowledged, however, that there is an increased
likelihood of residual confounding in this analysis due to potentially
incomplete baseline information.” is this just the subgroup analysis
where there is missing data then?
i hope these additional comments can be addressed by the authors,
and help improve the article further.
best wishes, richard riley


<|EndOfText|>

response 1: i am satisfied with this response. it is much clearer for me now what the model is
doing, and i agree that it allows patients with no values beyond baseline to still be included. i
had wrongly thought, from the description given, that baseline had also been adjusted for as a
continuous covariate as well as an outcome response. that would be wrong. but i can see that
your use of the word ‘factor’ relates to just an indicator variable, so that you can see the mean
baseline scores and contrast them with follow up scores. this is acceptable, and i am happy
now.
response 2: again, fine, thank you.
i have no further comments, and i thank the authors for their consideration.
professor riley, bmj statistics editor

<|EndOfText|>

this is clearly an important topic. the statistical analyses appear well done, with clustering and
heterogeneity appropriately modelled for example. however, i have some suggestions for improvement:
1) i found this a tough read, and (as a non-specialist) i found it hard to identify the question and
distinguish the definition of ‘at risk’ from the outcome. for example, the language appears inconsistent
(predictors, indicators, patients at risk could all be used to describe the patients of interest, but it took a
long time to realise indicators meant the outcome) such that the start-point and end-point (outcomes)
were not always clear. the ‘indicator’ language is especially problematic, to me.
eg at the start of the stats methods, it says ‘ for each indicator, a binary outcome variable categorised for
each patient at risk as triggering an indicator or not’
so for each indicator, the definition of the outcome is the indicator present or not – but upon a first read, i
thought the indicator was a coding for those identified at risk, and then the outcome was an actual
prescribing error in this subset. indicator is commonly used in the bmj or other journals to indicate those at
higher risk, so this is incredibly confusing and needs to be addressed.
2) key statistical measures like ‘icc’ and ‘reliability’ (based on the ‘prophecy formula’) are hard to translate
to clinical meaning. i would urge the authors to either explain these better, or focus primarily on the
summary %s and the distribution.
3) in relation to this point, the authors focus in tables on the summary %s from the analyses. these are
the summary %s across all the practices. whilst this itself is important, i think of more importance is the
variability (heterogeneity) of the %s across the practices. this is what the authors are alluding to by
focusing on the icc, but it would be far better to actually quantify the range of the %s across practices by
a 95% prediction interval for the potential true % in a new practice. this would reveal, with better clinical
meaning, the distribution of true %s. at the moment, the distribution is summarised by the range of
observed %s. but this is inappropriate as it is too wide, as it accounts for sampling error (chance) due to
a restricted sample size in each practice. therefore, a 95% prediction (akin to that in meta-analysis, see
ref [1] below) would be more intuitive, for the true %s. this can be added to table 2
[1] riley rd, higgins jp, deeks jj. interpretation of random effects meta-analyses. bmj. 2011;342:d549.
4) the design of the study also warrants clarification please. i think it is cross-sectional of sorts, but then
looks back upto 6 months (?) to ascertain prescribing practices. but what time-point is chosen for each
individual to be ‘at risk’. for example, are all times looked at, and what if the patients could be included at
multiple time-points (e.g. at risk at time 1, 5 and 6 – were the same patients then included at multiple
time-points to see if they had the outcome? and if so, how was the correlation of individuals accounted for?
in other words, the sampling frame needs to be much clearer to me. similarly, if individuals were are risk
for 2 or more prescribing errors, then how was the correlation accounted for?
or is the outcome just yes or no for each patient, with ‘yes’ meaning 1 or more errors?
e.g. confusingly the authors say ‘the proportion of patients triggering each indicator was calculated
individually’ – how can the proportion of all patients be calculated individually? do the authors mean, of the
total patients in each practice, they first identified those at risk and then, of these, which were exposed to
a prescribing error (indicator)?
are the ‘at risk’ group all patients in all practices?
5) the analyses seem well done, for example with clustering of patients within practices accounted for, and
the heterogeneity in effects (and prevalences) accounted for across practices. given the authors, i
expected this to be good. but better description of the variability across practices, in terms of the
prediction interval of the true %, is needed.
i hope my comments helps improve the article further.

<|EndOfText|>

the authors have responded very clearly to my comments, and made suitable revision to
the paper. i thank them for their clear responses throughout this process. my only
remaining comment is that throughout the paper the authors discuss results in terms of
risk and lowering risk of t2d. however, i think they mean rates, not risk. please consider
this throughout. the word ‘risk’ should be accompanied by a particular time-point (risk by
what time), whereas rate refers to the whole follow-up.
some examples of this are:

1) “after adjusting for lifestyle and dietary risk factors for diabetes, participants in the
highest quintile of total whole grain consumption had 29% (95% ci: 26% to 33%) lower
risk of t2d comparing with those in the lowest quintile”
- i think the authors mean lower rate and not lower risk
2) “spline regression showed a nonlinear dose-response relationship between total whole
grain intake and t2d risk in that the risk reduction slightly plateaued over two servings/d
(p for curvature < 0.0001)
- again, rate reduction rather than risk reduction?
same applies in many other places too
i thank the authors again.
best wishes, richard riley


<|EndOfText|>

the revision is superb and the response to my comments very clear. i thank the
authors for this. i only have two further minor comments. best wishes, richard
riley
- harrel’s e statistic should be harrell’s e statistic i assume? also, can the
authors very briefly explain to the reader what the e statistic is, as i actually had
never heard of it.
- i-squared is a poor measure of heterogeneity; rather report the estimated
between-study standard deviation (sd), tau, should be reported.

1. rucker g, schwarzer g, carpenter jr, schumacher m. undue reliance on i(2)
in assessing heterogeneity may mislead. bmc med res methodol. 2008;8:79.


<|EndOfText|>

the authors have clearly worked hard to align their paper with our suggestions.
this makes the paper a long read, especially in the methods section, and we now
have most of the original analyses supplemented with the analyses that we
suggested. for example, the multiple imputation follows the description of single
imputation analyses, and the investigation of non-linear associations now follows
the standard linear assumption. so this is much to process for the reader. that
being said, the complexity is a fair reflection on how the findings were derived
and the order the methods were operationalised. i have a few main comments
remaining, which i’m sure the authors can address, and some minor ones.
main comments
•
results ‘main associations …’ section. when the authors say ‘in model 1
…’ – i think it would help to remind the reader what this model is (specifically
what adjustment factors were included). also, there is still no mention of

whether the proportional hazards assumption was sensible for the analyses
reported.
•
and, the linearity assumption is mentioned for the first analysis, but not
for the others. the spline plot is a very nice addition (i thank the authors for
doing this on the log scale now), and shows that the linearity assumption appears
sensible for this one outcome, but what about other outcomes of chd and
cerebrovascular disease?
•
the abstract focuses on hazard ratios. i think it would be beneficial to
also indicate how they translate to change in absolute risk by a particular
time-point. the hazard ratio appears small, and the impact on absolute risk may
be very small. this is an important point to convey to the reader. the results and
discussion may also benefit from this
•
in the discussion, can the authors discuss whether further research
might rather analyse the total amount of ultra-processed food eaten, rather than
as a % of the total diet? i am not sure if this would change the findings or be
relevant to the field, but to me, the absolute amount may be more important (or
also important) than %.
minor comments
•
in the what this study adds: “an absolute increment of 10 in the
percentage of ultra-processed foods in the diet was associated with a >10%
significant increase in the risks of overall cardiovascular, coronary heart, and
cerebrovascular diseases.” i would remove the word significant in this sentence.
•
“absolute cvd risks were 253 for 100000 person years in the whole
population: more specifically, age and sex-corrected absolute cvd risks were 242
for 100000 person years …” – i think cvd risks should be changed to cvd rates
in this sentence.
•
“more specifically, ultra-processed beverages (p=0.004) were associated
with increased overall cvd, ultra-processed fats and sauces (p=0.04) and meats
(0.05) were associated with increased coronary heart diseases, and
ultra-processed beverages (p=0.01), sugary products (p=0.05) and salty snacks
(p=0.04) were associated with increased cerebrovascular diseases” – i think it
would be beneficial for the p-values to be replaced or supplemented with hrs and
cis here.
•
“in contrast, none of these food groups were associated with cvd risk in
their non-ultra-processed form …” – i think the authors are better to say that
there was no strong evidence for an association, rather than there is definitely
not an association.
•
table 2: i still do not find the sex-specific quartiles easy to follow. i
would have thought there were 4 for males and 4 for females, but they seem to
be lumped together? please can the authors clarify how these groupings are
created and analysed in their article?
•
“the association between ultra-processed food and overall cardiovascular
risk was also investigated separately in different strata of the population:
men/women, younger adults (<45y)/older adults (≥45y), participants with a high
lipid intake (>median)/those with a lower one, participants with a bmi<25
kg/m2/those with a bmi≥25 kg/m2, …” – as mentioned before, these groupings
are rather arbitrary, and a better approach would have been to look at the
interaction between the continuous covariate and the effect of ultra-processed
food (rather than creating groups and looking in them separately). but, at this
stage, i do not think it makes sense to ask the authors to do anymore work on
this specific point.
best wishes, richard riley


<|EndOfText|>

the paper is very well written and the analyses well described. the authors have done a very good revision from their
earlier version, and in particular responded well to the truly superb review by efthimiou. i have some further suggestions
for improvement, though these are relatively minor except for comments about ors (see below) which need detailed
thought/justification. the analysis methods appear entirely appropriate and in keeping with the current literature. it is
an immense piece of work and very well reported, with excellent appendices.
major
1) ors are provided, and i think used to translate back to the absolute risk scale (assuming a particular baseline risk).
the authors are therefore assuming that ors are close approximation to rrs. but the event %s appear quite high. i
therefore worry that this is not appropriate. please can the authors justify this, or revise accordingly.
2) why are ors relevant here, when the studies have different follow up lengths? surely hazard ratios would be
preferred. indeed, the authors often refer to rates, but simply give %s, which are not rates but risks. i worry they are
confusing the two here. they say “the primary efficacy outcome was prevention of advanced metachronous neoplasia,
within 3-5 years of the index colonoscopy” – therefore, it could be that ors for 3 years are very different to ors for 5
years? if follow-up length was different for different agents, then this might aversely bias the comparisons? can the
authors address this please, either as a limitation, or to justify why ors were used and what time-point they exactly
relate to.
we certainly need to know the time in each study when the or was measured.
3) a related point, when the absolute risks are presented: what time-point is this the risk of an outcome by?
4) ors require all patients to be followed until the time-point of analysis. therefore, how were patients handled who
were censored before the time-point? i don't know whether censoring was an issue or not, but with 3-5 year follow up it
is possible.
minor
1) sucra is an unusual term for the bmj reader, and the abstract would benefit from either explaining its meaning in
more detail, or removing it.
2) introduction says: “network meta-analyses combine direct and indirect evidence to
establish comparative efficacy and safety across a network of rcts of all agents used in a particular condition” – the
authors should note the assumptions at this point also. we gain more by assuming more.
3) the authors put most of their an analysis description in the supplementary appendix. i strongly suggest this is moved
to the main paper. that is, integrate the ‘emethods’ stats analysis section to the methods of the main paper. there are
many assumptions and explanations therein that are important, and deserve to be in the paper. the bmj has no word
limit.
4) figure 1 – why are the number of studies with direct evidence not simply added to the lines?
i hope my comments help the authors improve the article going forward.
richard riley, bmj statistics editor.

<|EndOfText|>

i thank the authors for considering my comments in detail and revising the article accordingly. i am
generally happy that the analysis methods are now clear, and the limitations / robustness of findings
more clearly detailed. they have clearly done a lot more work to address my views, and i think (and i
hope they agree) that the article is now more transparent, and i hope this improves the translation of
findings for the bmj audience. with that in mind, i have a few final comments to improve clarity further
(without the need for additional analyses):
(1) the language needs to be refined throughout the paper to clarify that the key groups are those
babies delivered or women admitted at weekend versus those delivered or admitted during the
week.. at the moment it sometimes still sounds like the key groups are those who had an outcome at the
weekend itself or during the week (regardless of whether the delivery/admission was in the weekend or
not). here are some examples
abstract: ‘… was significantly worse at weekends’ – i think the authors must be more explicit and say ‘…
was significantly worse for babies delivered at weekend and mothers admitted at weekend’
similar elsewhere: e.g. ‘…the perinatal mortality rate was 7.3 per 1,000 at weekends’; i think the
authors mean ‘…the perinatal mortality rate was 7.3 per 1,000 babies born at the weekend’
and ‘the results would suggest approximately 7750 perinatal deaths and 4750 maternal infections per
year above what might be expected at weekends if performance was consistent across days of the week’
should be: ‘the results would suggest approximately 7750 perinatal deaths and 4750 maternal infections
per year above what might be expected for deliveries/admissions at weekends if performance was
consistent with baby/mother outcomes of deliveries/admissions across days of the week’
please address everywhere in the article for clarity and to ensure correct interpretation
2) for added clarity, the abstract should say that: groups (e.g. weekday versus weekend) were defined
by day of admission (for maternal indicators) or birth (for neonatal records) rather than by day of
complication.
3) methods: i do not follow this sentence ‘where indicators cover national performance across the
measures was calculated, disaggregated by day of delivery’
4) page 11 says ‘we estimated there are some 7750 perinatal deaths (95% ci 720 to 830) per year’ and
later 470 maternal infections above what would be expected if rates were as they are on tuesday. but
when? weekend? all other days? is this the total additional death/infections that relate to the weekend,
or the whole wed-mon period? it’s not clear, though the abstract is more specific in saying that it relates
to weekends.
5) page 12: ‘even through’ should be ‘even though’?
i hope this improves things further before acceptance

<|EndOfText|>

this is an excellent revision. i only have a few further comments:
hazard ratios relate to the whole follow-up period, and yet in various
places (especially the abstract) the authors infer it is time-point specific
(‘… with mesh has a higher chance of having a re-intervention (hr 1.47,
1.21 to 1.79) at one year...). in other words, it does not relate to just
one year, but also at all times up to one year. please address this
throughout by changing the language (e.g. could say: throughout the
first year, patients with mesh had an increased risk ….). if the authors
want to report a comparison at a particular time-point, then the different
in survival % at one year between the matched groups is more pertinent.
i suggest this is also added, as derived from figure 2b.
abstract should detail the matching factors used in the propensity score
approach (i.e. detail the confounders adjusted for)
my biggest issue: the authors do not discuss any other potential
confounders that may still be affecting their results, in their discussion.
the hr is attenuated from the unadjusted analysis to the propensity
score analysis – could additional confounders attenuate it further? this
needs serious discussion and thought.
best wishes, richard

<|EndOfText|>

thank you for the opportunity to review this interesting work. ipd meta-analyses are not an easy
undertaking, and it is encouraging to see the authors have obtained and synthesised ipd from over
100,000 patients and 15 studies. this is commendable. the authors conclude that their data indicate
that genetic risk and dietary fat quality are each associated with t2d incidence. but there is no evidence
to support tailoring dietary fat quality recommendations to individual t2d genetic risk (i.e. because there
was no interaction).
i would like to recommend the authors revise their article after addressing the following comments and
concerns.
1) the genetic risk score was created (grs), though i am still not clear about exactly how.
- “the grs was generated by summing the number of risk alleles at each genetic variant weighted by
the respective allelic effect sizes on risk of t2d (log transformed odds ratio estimates) from the largest
published genome-wide association meta-analysis” – this is not clear to me; why not simply adjust for
these genetic factors in the analysis, rather than create an (unvalidated) risk score. at least in
sensitivity analysis, i would also suggest this is investigated.
2) also, i think the weighting approach needs further explanation, so that it could be replicated by
others. perhaps in a supplementary material? this is an ipd meta-analysis, with clustering by study (and
potential heterogeneity in effects across studies). how was heterogeneity across studies accounted for
when creating the score? or perhaps as the weights are derived from external data, this does not
matter?

3) was imputation of missing data done in each study separately (to correctly preserve clustering and
heterogeneity)? i think it was, but just make this more explicit.
4) in the main analysis, grs and fat are assumed to have linear effects, but this may not be the case.
indeed, their interaction may also be non-linear. a multivariate meta-analysis of splines could be used
to investigate this, within the 2-stage ipd meta-analysis framework [1, 2] the authors must consider
non-linear trends in their revision, as otherwise perhaps they are missing non-linear associations.
5) the authors use a fixed effect meta-analysis, which assumed effects are the same across studies.
given the heterogeneity in observational studies, i do not agree with this assumption. they are from
different countries, using different methods of measurement, and follow up periods, etc. at the very
least, we need some assurance that a random effects approach leads to comparable findings
6) i-squared is a very poor measure of heterogeneity; it is misleading.[3] simply estimate and report
the heterogeneity itself (between-study variance).
7) the meta-regression uses dichotomised variables (e.g. mean age > 55 years or < 55 years), which is
arbitrary and loses power. also such analyses are severely prone to across study confounding, so the
analyses have little credibility. i suggest they are removed or severely downplayed. if included, at least
also consider age as continuous.
8) what is the ‘joint meta-analysis method’? a multivariate meta-analysis? please explain and justify.
9) some details on the length of time to obtain and clean ipd was be welcome.
10) the authors do not investigate non-proportional hazards; that is, whether the hazard ratio of
interest is a constant over time. this must be addressed.
11) often the authors use the term relative risk, but do they mean hazard ratio?
12) peter’s test – i thought this was specific to odds ratios, requiring the 2 by 2 tables? perhaps i am
wrong. debray test is specific to hazard ratios.[4] (this is perhaps a minor point)
13) authors say that missing studies will not be due to reporting bias, as they are missing in areas of
high significance. but i disagree, looking at sup fig 3, many of the ‘missing’ studies would fall in the
white non-significant regions. please can they check this again.
14) the interaction analyses are poorly reported, both in the results (just p-values given) and in the
table 3, where p-values are given and just the direction of interaction effect is reported. what are the
summary interaction estimates and cis?
15) “meta-regressions analyses were conducted to examine the impact of continuous mean age and bmi
of each cohort on the interaction between fat” – but why look at mean age, when the authors have the
ipd and so can assess the interaction between individual age and the grs-fat interaction? the
meta-regression is prone to confounding and ecological bias, whereas the within-study information is
less prone to confounding.[5, 6] as mentioned, i am not sure the meta-regression analyses add
important information.
16) the authors excluded participants who reported implausible baseline energy intake (< 500 or >
4,500 kcal/d), and participants who had missing genetic or dietary fat quality data. can the authors
clarify why they were excluded, rather than included as part of the multiple imputation analysis?

17) “with the available sample size, we had 88% statistical power to detect an interaction effect size of
1.04 hr on t2d risk (type 1 error rate set at 0.05)” – do the authors have a reference for this
calculation? power calculations are awkward in this setting, given the clustering and potential for
heterogeneity. it is even more important to report the interaction estimates and cis in table 3, as
otherwise the cis may be wide, and the conclusion about a lack of interaction may actually be due to
low power. indeed, can we also see forest plots of the interaction estimates please.
in summary, there are quite a few points for clarification and/or updating. i sincerely hope the authors
find my comments helpful and that they lead to improvements. i re-emphasise that the bmj should
recognise the considerable achievement by the authors to have obtained this ipd and undertaken the
meta-analysis, and provide a comprehensive and well-written report of their work. i am confident the
authors can address the comments raised, and i look forward to reading the revision.
best wishes, richard riley
reference list
1.
gasparrini a, armstrong b, kenward mg. multivariate meta-analysis for non-linear and other
multi-parameter associations. stat med 2012; 31: 3821-3839.
2.
gasparrini a, armstrong b. multivariate meta-analysis: a method to summarize non-linear
associations. stat med 2011; 30: 2504-2506.
3.
rucker g, schwarzer g, carpenter jr, schumacher m. undue reliance on i(2) in assessing
heterogeneity may mislead. bmc med res methodol 2008; 8: 79.
4.
debray tpa, moons kgm, riley rd. detecting small-study effects and funnel plot asymmetry in
meta-analysis of survival data: a comparison of new and existing tests. res synth methods 2018; 9:
41-50.
5.
berlin ja, santanna j, schmid ch, szczech la, feldman hi. individual patient- versus
group-level data meta-regressions for the investigation of treatment effect modifiers: ecological bias
rears its ugly head. stat med 2002; 21: 371-387.
6.
schmid ch, stark pc, berlin ja, landais p, lau j. meta-regression detected associations
between heterogeneous treatment effects and study-level, but not patient-level, factors. j clin epidemiol
2004; 57: 683-697.


<|EndOfText|>

i thank the authors again for their revision and clear response. i think we are very close
now, but i would suggest one further revision to address the points below. i thank the
authors in advance for addressing these, and i am confident they will do a good job.
1) in the abstract, need to make it clear whether the summary hazard ratios are adjusted
for confounding factors (other risk factors)
2) in the data extraction of the methods section, please define what is meant by ‘maximally
adjusted’ – the authors refer to the reported hr that adjusted for the most confounders, but
the reader might think that this rather refers to hrs that adjusted for a pre-defined set of
confounders that would the authors considered to be the maximum.
3) explain that by ‘crude estimate’ the authors mean unadjusted estimates
4) stata should be stata
5) in the assessment of heterogeneity section of the methods, start by saying “in each
meta-analysis, heterogeneity was evaluated by using …”

6) methods says: “a p-value of <0.10 indicates the presence of publication bias” – rather
replace this with “a p-value of <0.10 was taken as statistical evidence of the the presence of
small study effects (potential publication bias)”
7) results: “figure 1 shows the shrs and their corresponding …” – please make it clear here
and indeed everywhere that the shrs are summary adjusted hazard ratios
8) in the results, the authors often refer to statistical significance. i would prefer them to
say that there was no strong evidence of association, and also refer to whether the width of
the 95% ci is wide. in other words, don’t infer that there is no association when actually it
may be a low power issue. so, just be cautious about using statistical significance to guide
statements about no associations.
9) no results are embedded within the results text – the tables are referred to, but i think
providing summary results in brackets for a few key variables would make the reading
clearer and flow better.
10) most important point: i think the quality of evidence should come early on in the results.
then, when the authors provide their shrs, they can make an accompanying statement
about the quality of evidence behind it?
11) the previous two points are already done superbly well in the abstract, e.g. “the quality
of evidence was rated high for an inverse association with incidence of t2d for an increased
intake of whole grains (shr for an increment of 30 g/d (95%-ci): 0.87 (0.82-0.93)) and …”
– i would suggest the authors write similarly in the actual results.
at the moment the results are quite disjointed, and hard to piece together the shrs with the
quality angle. starting with quality and the focusing the start of the shrs to be on those
with high quality evidence would really help.
i hope this helps the authors and the bmj going forward
best wishes, richard riley


<|EndOfText|>

thank you for the opportunity to review this interesting and well-written manuscript. i
have focused on statistical aspects, and have the following comments
1) at the start of the results, there needs to be more discussion on the included
populations of the trials and whether they are broadly similar across them. this is crucial to
the understanding of whether these trials and their populations are in some sense
exchangeable, such that the consistency assumption is likely to hold (in advance of data
analysis). i.e. that it is sensible to combine these trials in a network meta-analysis. when i
look at the table of study characteristics i see trials from different countries, different
settings (eg primary and secondary care), different age range (some all ages, other upto
just 45)and different lengths of follow-up (eg 12 and 18 months), which do raise concerns
about the distribution of potential effect modifiers being different for studies that give
direct and indirect evidence.
2) the authors say in the results section that: “direct evidence was therefore available for
nine of the 10 possible comparisons. one rct was a cluster-randomised trial. we therefore
used the cluster size and the intra-cluster correlation coefficient to reduce the size of the
trial to its “effective sample size” before we carried out any data pooling” –the authors
should explain this approach in the methods section.
3) in addition to the heat plot, it would useful to have a statistical measure or test of
whether there is any evidence of inconsistency across the network ma overall. i do not
find the heat plot easy to conclude whether there is inconsistency or not. the consistency
assumption can also be examined across the whole network using design-by-treatment

interaction models, which allow an overall significance test for inconsistency. (of course this
may have low power; see introduction by riley rd, jackson d, salanti g, et al. multivariate
and network meta-analysis of multiple outcomes and multiple treatments: rationale,
concepts, and examples. bmj 2017;358:j3932)
4) the authors say the “the p-score is the probability of each treatment being ranked as
best in the network analysis” – this is not correct, or at least it not quite exact. readers
may interpret this as the probability of being ranked first, when actually the p-score has
the same interpretation as the sucra, which is the area under a line plot of the cumulative
probability over ranks (from most effective to least effective) and is just the mean rank
scaled to be between 0 and 1. i think the mean rank is more interpretable. could this also
be provided? also the sucra graphs are needed, and a graph giving the probability of each
rank (see the aforementioned riley et al. paper for examples)
5) “we performed a network meta-analysis using the frequentist model, with the statistical
package “netmeta”” – we need more details than just referring to a stats package, as there
are many options therein. in particular, what estimation method was used (reml), were
random treatment effects assumed, was the heterogeneity assumed the same for each
treatment effect, what was used in the pooling (rrs or the raw numbers of events/total),
etc? also whether a two-stage or one-stage meta-analysis modelling approach was used,
and whether the confidence intervals accounted for uncertainty in the estimate of
tau-squared, for example using the hartung-knapp method?
https://www.ncbi.nlm.nih.gov/pubmed/30067315
6) i-squared is a poor direct measure of heterogeneity, and we should rather be seeing the
estimate(s) of tau-squared, the actual between-study variance estimate(s). (rucker g,
schwarzer g, carpenter jr, et al. undue reliance on i(2) in assessing heterogeneity may
mislead. bmc med res methodol 2008;8:79)
7) how were multiple (i.e. 2 or more) treatment effects from the same study handled in
the analysis (as they are correlated due to the common control group)?
8) in the results, there is a major emphasis on the p-score, but we do not get told the
magnitude of treatment effects and their cis. this needs to be clearer in the text as well as
the tables.
9) for the outcome of being asymptomatic (see figure 2) we have wide cis that include
the null, and so we might even want further evidence to clarify that there is a benefit for
any of these treatments, before ranking them. whereas there is clearer evidence that
test-treat has significantly fewer endoscopies than other approaches, it less clear for other
outcomes. actually the authors already hint at this when they say “but none of the
strategies was significantly less effective than “test and treat”, or more effective than each
other, on direct or indirect comparison” – so is the following conclusion actually justified:
- ““test and treat” is likely to be the most effective first-line strategy for the management
of uninvestigated dyspepsia in primary care”
cis might be even wider following a bayesian approach or when using the hartng-knapp
correction
10) the last follow-up time is the key time-point, but as mentioned this varies across trials.
this will lead to heterogeneity in the rrs and make it harder to interpret. this should be
discussed. i assume hazard ratios were not available?
11) table 3 and 4– i am not sure the words ‘league table’ are correct or needed – simply
say summary treatment effect estimates from the network meta-analysis. as mentioned,
we need all the treatment effects for all pairs.

12) usually we are shown the direct evidence results too for each treatment effect; a forest
plot for each would be welcome, to reveal study specific results. indeed, i cannot see study
specific results in this article at all.
13) none of the studies were at low risk of bias. thus, in addition to the summary results
from the nma having wide cis that include the null, this perhaps gives extra credence for
not making strong conclusions about the best treatments from these network
meta-analysis results.
14) the abstract does not explain what the p-score is, and i think readers will confuse it
with a p-value. suggest that the authors define it or, better still, rather give the mean
rank. as mentioned, the abstract should also discuss the uncertainty of the ranking, i.e.
that comparisons of key treatments have wide cis
15) the authors say “we used a rr of remaining symptomatic at the final point of
follow-up” – but earlier say “we extracted all endpoints at a minimum of 12 months, even
for rcts providing effectiveness data at other time points. we did this to ensure as much
homogeneity as possible between individual trial results, and to avoid overestimating the
effectiveness of one management strategy relative to another”
- thus taking the final point of follow-up does not seem sensible. why not choose
time-points that were similar to each? eg do a separate meta-analysis at 12 months, and
then at 2 years etc? this is the best way to improve homogeneity, rather than taking the
final follow-up times.
i thank the authors for considering my comments and i hope they are useful going
forward.
best wishes, prof richard riley


<|EndOfText|>

this is an interesting and well conducted trial. here we see secondary (i think, though it could be clearer) analyses of a
previously conducted and published trial. i have reviewed this from a statistical perspective and have some comments and
suggestions as follows:
is this a negative trial? i’m not as sure as the authors are. all of the effect sizes are in the direction that favours bivalirudin,
but are not statistically significant. cis are wide (e.g. 0.70 to 1.16, 0.67 to 1.05, etc). therefore, how can the authors strongly
state: “a bivalirudin monotherapy strategy, as compared with heparin with or without gpi, does not reduce mace or nace in
patients”. i suggest the authors revisit this conclusion. is it not a case that further research is needed should be the message?
i assume the strong statement is based on non-significant p-values (cis containing 1). perhaps low power is causing the nonsignificance of the findings? or are the authors inferring that no clinically relevant values are in the cis? please clarify. a rr of
0.70 for example would seem to be quite beneficial to me (though i’m not a clinician). in short, the authors should be clear
and justified) about whether there conclusion is based on statistical significance, or clinical magnitude of effects, or indeed
both.
at end of the introduction it says ‘here we present the primary results …’ – what does primary mean here? does this mean
primary outcomes of the trial? i think these are not primary analyses of the trial? thus i would be clearer at the end of the
introduction that this is a secondary analysis of a trial but for pres-specified outcomes / groupings. indeed, the statistical
analysis section should make clearer that the trial was powered on a different main analysis (i think?). this issue relates to the
point above about potential low power.
i would also like more motivation in the intro as to why these secondary but pre-specified analyses were not already reported
in the main trial publication.
i do not think it is necessary to present p-values in table 1, as the study is not aimed (powered) to detect differences at
baseline.
in terms of the analyses, what is the mantel-cox method? no reference is given, and i was expecting a cox regression analysis
to have been used to estimate rate ratios.
was the assumption of proportional hazards evaluated for each analysis? that is, were any treatment effects time-dependent
(non-constant hazard ratio)? please discuss in the manuscript.
why did the main analysis not adjust for centre? would conclusions change if the analysis was stratified by centre? these
centres are across different countries, therefore there is the potential for differences in baseline risk, and thus clustering by
centre might be important. further, is there any evidence of heterogeneity in treatment effect across centres (or at least
across countries)? these points could be considered in a sensitivity analysis, with results shown in a supplementary material
perhaps
there are lots of analyses being done in terms of subgroups and interactions and tests for trend (see methods for list and table
2, though i think more analyses have been done than shown in table 2. with tests for trends and interactions). therefore, even
i struggle to believe anything beyond chance to be honest. this weakens the robustness of the paper, in my opinion, and
reflects perhaps the secondary nature of the work compared to the original trial. perhaps i am wrong, and the authors could
respond to this, but this is my instinct from reading.

for example, the authors have a paragraph about a significant interaction effect. they say: “rates of myocardial infarction
were similar between bivalirudin and heparin in acs patients
with and without st elevation (rr 1.24, 95% ci 0.88 to 1.76 and rr 0.95, 95% ci 0.78 to
1.16 respectively, p for interaction=0.19). notably, event rates at 30 day differed markedly
between the two population: 3.3% in patients with steacs and 15% in those with nsteacs.
randomised treatment effect on stroke were directionally opposite in patients with and
without st elevation (p for interaction=0.0052) with bivalirudin associated with lower risk of
stroke as compared to heparin in steacs patients (rr 0.35, 95% ci 0.13 to 0.97, p=0.035)
and higher risk of stroke in nsteacs patients (rr 4.02, 95% ci 0.85 to 18.95, p=0.057).”
- the focus on 30 days is worrying. is this because it was significant? the overall effect (across all times) is noted as not
significant in the first sentence, but then the authors focus on 30 days. also, what is an ‘event rate’ at 30 days. do the authors
mean event risk by 30 days?
the start of the methods intrdocues 3 matrix trials, but then it is not clearly specified which trial is used going forward (just
the matrix anti-thrombin one?).
some results are slipped into the discussion i think. in the discussion it say: “in secondary analyses we observed that
bivalirudin was associated with a reduction in all-cause mortality due to lower cv mortality, a difference that in this analysis
was similar in patients with steacs and with nsteacs (p for interaction=0.72) while in the updated meta-analysis was limited
to patients with steacs (with a p value for difference for steacs vs nsteacs of 0.07)”. i can’t see the meta-analysis
mentioned earlier.
some of the points in the discussion are debatable. “while we did not observe significant differences in the rate of acute stent
thrombosis between randomised strategies in acs patients with and without st elevation, the present analysis indicates that
the risk of acute st with bivalirudin is probably limited to patients with steacs” – why? what results support this.
in summary, in the context of an important and well conducted trial, i do have some concerns about the potential low power
for these additional analyses, and the over-emphasis (at times) of either non-significant findings being conclusively negative,
or subgroup effects being important (beyond chance). therefore i currently struggle to identify firm findings that will be useful
to the bmj readers. further, some analysis issues remain to be addressed / clarified. i hope the authors can take my
comments as constructive and allow them to revise the paper going forward.


<|EndOfText|>

i start by saying that the pdf of the submitted article (which i used for my review below initially)
does not seem to match the track changes version of the submission (labelled v2 in the
supplementary material). that is, the pdf version does not contain most (perhaps any) of the
changes highlighted in yellow. i’m not sure what has gone on there, but i spent 2 hours going
through the pdf version and only at the end, when looking through the supplementary material,
did i identify this discrepancy. i therefore do not know which version is right for me to be
reviewing! however, i have checked whether the track changes version addresses my concerns,
and it does not. therefore, i think my comments below still stand.
following the response to comments, it is interesting to see that the odds of immediate death (on
day of birth) are larger at weekends than during the week. but this information is only given in
the response to comments, and i can’t see it detailed in the actual article! this is one of a few
examples of where the authors have only gone part of the way to addressing my comments. no
propensity score analysis was done; it’s still not clear how missing data was handled (not multiple
imputation it seems); and clustering multiple babies from the same woman is still not accounted
for. as mentioned, the reporting of the methods is still not adequate – for example, even though
they respond to the comments, they have not always revised the paper in the same way (e.g.
details about missing data and clustering are still missing in the paper) therefore, i am not
convinced the article is robust enough for the bmj as it stands. i’m sorry to say that. i suggest
further revisions and clarification are still needed:
1) the authors say that missing data was handled by the regression analysis, but it is not clear
how. they say in their response:
“missing data is not a significant problem in hes data due to the coding process and rules. for
some variables, there are “unknown” responses which are accounted for the regression model.
the extent of “unknown” values is given in table 2. as described above, coding practice is not
influenced the day of admission/birth so missing values are not likely to bias the results”
was ‘missing’ given its own category? would the results be different if a multiple imputation
approach was used? we need to know for robustness. i searched the article for ‘missing’ and
can’t see anything.
2) the authors still do not do a propensity score analysis, which is a real shame as i still feel the
paper would be far stronger as a result. they say: ‘we did not undertake propensity score
analysis but did validate the key results by using alternative methodological approaches (e.g.
difference case-mix adjustment and use of hierarchical models). such analysis would not address
the limitation of residual confounding bias.’
i agree that a propensity score model won’t solve the issue of potential residual confounding, but
when matched propensity score analysis is done, the adjustment for confounding is less prone to

influence of extreme individuals (i.e. those can’t be matched) in one of the groups than the
regression models used. i think i would, at the very least, note the potential to consider
propensity score analyses in the future for this type of study, and to explain why you did not do
this.
3) i do not understand why an analysis that accounts for clustering of multiple babies for the
same woman does not converge. the authors infer this is due to such woman only being 2.9% of
the total number. however, this still represents 36,679 babies, which is enormous. is it because
there are so few events in this set of babies? we need an explanation, and full details in the
paper that you tried to do this but it did not work, as otherwise the paper could be severely
criticised.
4) please present the unadjusted odds ratios in the table; it is only one additional column and it is
not clear, as it stands, whether the ors presented are unadjusted or adjusted anyway.
5) the immediate deaths analysis is important and should be reported in the paper, surely? or
am i missing it somewhere? otherwise, if death occurs 3 days later, then how do we know it was
due to being born on a saturday or something that happened on the monday? the immediate
death is therefore more pertinent.
6) the abstract conclusion infers causality – there is no mention of the potential for residual
confounding. i am disappointed by this, as in their response the authors clearly say that they
have noted the limitations of their work more clearly now; yet this is not evident here.
7) what this study adds says that the paper gives the ‘most comprehensive evaluation …’ – but
with no accounting for clustering, no propensity score analysis, no multiple imputation of missing
values, etc i can’t agree with this statement. please tone down.
8) the abstract says that 750 and 450 perinatal and maternal deaths extra occur at the weekend:
but over what period? one year? each weekend? please be absolutely clear, and give confidence
intervals for these pivotal numbers. and be clear what the population size these numbers relate
to. 750 extra deaths out of how many total deaths, in a population of size of …
9) i am still unclear, and i think the methods are still unclear in the paper, how events were
defined in relation to the day of the week. so if a woman was admitted on a saturday but had the
event on a tuesday, is this classed as a week day event? i think it is, but it needs to be explicitly
stated.
10) further, another important issue that i think is unresolved. please clarify how the days prior
to the event are accounted for (if at all). for example, could the authors count for each individual
how many weekend days they were exposed to? then, they could look at including the number of
weekend days as a covariate in a model, and (after adjustment for other things) ascertain
whether there is an association between the number of weekend days of exposure and the rate of
event.
this is interesting for two reasons:
(i) it is another way to examine if there is an indication that weekend care is having a negative
impact on the whole (not just on the day of delivery itself).
(ii) it may be that, after you adjust for the number of days exposed to weekend care, that the
effect of the actual day of delivery disappears. or it could be that both day of delivery and the
number of exposure days are important, which would be revealing.
11) the analysis described as follows it not clear enough to me still: ‘using regression analysis on
just the reference days (tuesdays), probabilities of in-hospital perinatal death and puerperal
infections (maternal) were derived. by matching these probabilities for each tuesday admission,
based on the mother’s or neonate’s characteristics, indirectly standardised estimates for the
outcomes as if those non-tuesday cases had had similar rates as their tuesday counterparts were
calculated.’
- how was matching done? how close did the probability have to be in order to be matched? was
the matching random?
- for deriving the predictions, what covariates were included? how were missing values dealt
with?
- indirectly standardises estimates … of what? odds ratios? proportions? and how were they
derived?
12) “the most common day for giving birth is a thursday (15%...)” – can we have a ci please for
this estimate, otherwise this is just an observation. the ci should be very narrow given the large
sample size.
13) “much of the difference is explained by number of elective …’ – please refer to the figure
where this is apparent
14) in their responses, the authors make the important statement that immediate death is also
higher in those born at weekends. but
15) the discussion says that the estimates of weekend effect are likely to be an underestimate – i
don’t see the justification for this at all. residual confounding is one reason why it may be an

overestimate.
16) throughout the article, the authors use ‘rate’ when they really mean % or proportion. a rate
is the number of events per period of time. here though they look at the % with an event by one
time-point (e.g. by 3 days).
i'm sorry to be so negative, but these are all genuinely important concerns and areas of worry for
me. i sincerely hope these comments help improve the robustness and interpretation of the
article moving forward.

<|EndOfText|>


the authors have responded well to the comments, and i still believe this is a very good
research study. but some response do not go far enough in my opinion.
1. the decision to report one of the pre-defined primary outcomes as a secondary
outcome is still troublesome, even though the authors note this in the discussion. why not
simply report 2 primary outcomes? the problem is that the primary outcome moved to
become secondary is actually not statistically significant, whilst the retained primary
outcome is. this looks bad, because, it does appear to be a selective decision. however,
both effect sizes are in the same direction. in many ways, they do back each other up,
and why raise any complaints in this regard? just stick with 2 primary outcomes.
2. i find it incredibly reassuring that the new meta-analysis that i requested shows
consistent findings across centres. i strongly recommend this is given in the appendix at
least, and am surprised why the authors do not see why this add credence to their
findings. this analysis also accounts for clustering of patients within centres, which their
main analysis does not.
3. i asked them to consider adjustment for baseline imbalance (e.g. as a sensitivity
analysis), but they have not done this yet. “as this is a randomized controlled trial, our
proposal is not to adjust the
analysis. if the editor thinks this is important to report, we are more than willing to do
so.” i understand the view that this is an rct, and hence randomisation should expect to
balance groups and so estimates are unbiased; but clearly there is slight imbalance.
hence the question is valid: are conclusions robust. thus i do ask again for this in a
sensitivity analysis. as for the multi-centre analysis above, if findings barely change, then
this add credence to the finding. the focus should not be on the change in p-value here,
but rather any chance in the magnitude of effect (and its clinical relevance).
4. the authors changed their analysis methods from the first submission from a complete
case analysis to one that uses multiple imputation to ensure an itt analysis i think. what
assumptions is this imputation analysis making? what if baseline covariates were
themselves missing, from which the outcome is being computed? how do results compare
to the complete case analysis from before? it would be nice to know in the paper.
also, in the discussion they say: “missing data were balanced between groups averting
any bias in outcome.” – but does the multiple imputation not exceed this statement now?
6) re-write: “of the women assigned to atosiban, cesarean delivery was performed in
60% of the women and 55% in the fenoterol group” – it reads like the fenoterol group is a
subset of those in the atosiban group.
7) at start of discussion it says: “although the difference was not statistically significant, it
is likely to be true, as the number of women in cephalic position after the procedure was
significantly decreased after atosiban.” please change to ‘it is highly plausible’ rather than
likely to be true.
i hope this helps the authors improve the article going forward.
best wishes, professor riley, bmj statistics editor.


<|EndOfText|>

i have looked at this from a statistical perspective and have some comments.
1) i understand the reasons why the authors do not perform meta-analysis, due to the heterogeneity in
time-points, analysis results presented, inappropriate tests in primary studies, etc. however, though this
overall decision is perhaps commendable at first glance (and is probably correct!), it is rather ‘broad brush’
as perhaps there were subgroups of studies that were combinable (e.g. outcome measured at same timepoint, analysis methods appropriate and well reported)? further, even when time-points differ, hazard
ratios are combinable as we typically assumed hrs are constant over time.

2) thus, far more details are needed in regard how data extraction was performed regarding the effects of
interest. for example, when hazard ratios are of interest but poorly reported, there is a multitude of
methods to indirectly obtain them from other information (even simply an exact p-value and the numbers
of patients/events in each group) see ref [1]. therefore, i would like reassurance that the authors have
done their utmost to get the right data, before we can be sure that a lack of meta-analysis is entirely
sensible.
3) a related point is that the effect of interest is not clearly defined in the text. are we looking at
continuous outcomes, or binary outcomes? are we looking at mean change, ors, hrs, or …? even though
results are not being combined, we need clarity in the text as to the measures of effect being reported and
extracted. i suggest a section, near the start of the results, that explains what exactly was extractable (in
terms of effect estimates) for each outcomes, the scale(s) of this effect, and the why meta-analysis was
not done.
4) i imagine that the authors are ultimately right to not pool, but as mentioned above we need greater
justification. but the issues of heterogeneity are very common in meta-analyses of lab based research, for
example see all the quotes about the difficulties of doing meta-analysis in the ref [2] paper, so i
sympathise and we need to address the issue of heterogeneity in these types of studies, and make sure
research studies are combinable in the future. prospectively planned ipd meta-analyses would really help
us in this regard [2][3-5]
5) table 2 – why just report the statistically significant findings? i suggest the authors may simply be
promoting selective reporting or chance findings by doing this, and are not being comprehensive in the
review summary. at least, can the full results for each study be given in a supplementary material? this
relates to the point 6 below, where i ask for all the results in the plots to be quantitatively summarised,
where possible, in terms the estimate and ci of the ratio being presented.
6) the authors present the results on a graph (fig 2 and 3) by first creating a ratio of the response in the
vaccine group compared to the non-vaccine group. these are not combinable, as they highlight in the
methods, but allow us to get a feel for the direction of results. i think we need the authors to give far more
details next to the results of each study in each plot, or in a separate companion table. in particular, what
is the ratio of (means, odds, medians, etc) and what is the ci for the ratio (if derivable) for each bubble?
otherwise the reader may try to pool without due thought. in particular, uncertainty of the evidence is
being completed ignored, in terms of the ci around each point. so these figures need to come with a clear
warning that they are just for illustrative purposes, and the limitations of them must be emphasised!
7) figs 2 and 3 also need to be improved in terms of the size of the bubbles. some bubbles are so big that
they encompass other bubbles on lines above them. e.g. page 19, plot a, the bubbles on rows 2 to 4 are
encompassed by one huge bubble, which makes it hard to follow. but i appreciate the authors are
struggling to present the messy data.
i hope my comments help improve the article going forward.
[1] parmar mk, torri v, stewart l. extracting summary statistics to perform meta-analyses of the
published literature for survival endpoints. stat med. 1998;17:2815-34.
[2] riley rd, hayden ja, steyerberg ew, moons kg, abrams k, kyzas pa, et al. prognosis research
strategy (progress) 2: prognostic factor research. plos med. 2013;10:e1001380.
[3] riley rd, lambert pc, abo-zaid g. meta-analysis of individual participant data: rationale, conduct, and
reporting. bmj. 2010;340:c221.
[4] altman dg, riley rd. an evidence-based approach to prognostic markers. nature clinical practice
oncology. 2005;2:466-72.
[5] riley rd, abrams kr, sutton aj, lambert pc, jones dr, heney d, et al. reporting of prognostic
markers: current problems and development of guidelines for evidence-based practice in the future. br j
cancer. 2003;88:1191-8.

<|EndOfText|>

the revision is excellent, and i thanks the authors for answering my comments in detail and revising the
paper accordingly.
i have some further, minor comments
1) why was a 90% ci used when looking at non-inferiority (why not 95%?) – page 16. is this a
standard level used in the non-inferiority field (i am not familiar with this field, but it is important to
clarify here as i don’t see why 95% is not used)
2) “the estimates and 95% confidence intervals at our primary time point of interest were all below our
original sample size estimate (or=0.5)” – do you mean below or above (or estimates are above 0.5)?
please re-write to be clearer
best wishes, richard riley

<|EndOfText|>

thank you for the opportunity to review this interesting paper, on clearly an important topic.
i have been through this from a statistical perspective, and have a number of comments and
suggestions for improvement, as follows.
1) the authors should use prisma-nma, not prisma
(http://www.prisma-statement.org/extensions/networkmetaanalysis.aspx)
2) i-squared is not a test of heterogeneity, and indeed is a poor direct measure of
heterogeneity. (rucker g, schwarzer g, carpenter jr, et al. undue reliance on i(2) in
assessing heterogeneity may mislead. bmc med res methodol 2008;8:79)
3) the authors use stata and the mvmeta module; do they actually mean they used the
network module (which uses mvmeta in the background)?
4) if relevant, how were multiple intervention effects from the same study handled in the
analysis (i.e. was their correlation accounted for)?
5) what assumptions were made about the specification of the between-study variance
matrix components? e.g. were between-study variances made equal and correlations set to
0.5, as is standard?
6) was a random effects meta-analysis used in the network meta-analysis, as in the
pair-wise analyses? was the uncertainty of between-study variance estimates accounted for
when deriving subsequent cis for summary results? e.g. using hartung-knapp
sidik-jonkman approach?
7) what estimation method was used for the network meta-analyses? reml?
8) stata should be stata

9) “we applied a 0.5 zero-cell correction only in the pairwise meta-analysis as a default of
the stata meta command but not in the network-meta-analysis to obtain a more unbiased
estimation.” – i don’t think adding 0.5 in the pair-wise analysis is as appropriate as using the
sweeting correction. (sweeting mj, sutton aj, lambert pc. what to add to nothing? use and
avoidance of continuity corrections in meta-analysis of sparse data. stat med
2004;23(9):1351-75)
moreover, i do not think the 2-stage framework is correct when outcomes are rare, and a
1-stage model is more exact and appropriate. that is, the mvmeta module in stata requires
treatment effect estimates and their variances to be calculated for each study, and these are
then pooled in a meta-analysis. however, when the event rate is low, there is a concern that
such effect estimates are not normally distributed and variances are poorly estimated. this,
a one-stage network meta-analysis that uses the exact binomial likelihood might be
preferred. did the authors consider this, or evaluate if their conclusions are robust to this
issue?
see for example:
1. riley rd, jackson d, salanti g, burke dl, price m, kirkham j, et al. multivariate and
network meta-analysis of multiple outcomes and multiple treatments: rationale, concepts,
and examples. bmj. 2017;358:j3932.
2. salanti g, higgins jp, ades ae, ioannidis jp. evaluation of networks of randomized trials.
stat methods med res. 2008;17(3):279-301.
10) page 12: met-analysis should be meta-analysis
11) “we evaluated the potential inconsistencies… “ – more details are needed on what
criteria they used to confirm consistency or inconsistency. these results should also be
provided in the main text, as this is a fundamental part of a network meta-analysis.
12) it is not clear if the meta-regression described in the methods relates to the network
meta-analysis or the pair-wise meta-analysis.
regardless, meta-regression is very prone to study-level confounding, so i would class these
as an exploratory analysis. in particular, the association of mean prostate volume and
overall treatment effect is at the ecological level – what we really need is the association
between individual prostate volume and individual treatment response.
this could only be ascertained from ipd and within-trial information, and so i strongly
suggest the meta-regression of prostate volume is downplayed.
a nice paper in the bmj on this recently is fisher (fisher dj, carpenter jr, morris tp, et al.
meta-analytical methods to identify who benefits most from treatments: daft, deluded, or
deft approach? bmj 2017;356:j573). also see: hua h, burke dl, crowther mj, et al.
one-stage individual participant data meta-analysis models: estimation of
treatment-covariate interactions must avoid ecological bias by separating out within-trial and
across-trial information. stat med 2017;36(5):772-89. doi: 10.1002/sim.7171
13) multiple time-points are considered. was the correlation across time-points accounted
for? or was a separate network meta-analysis done at each time-point? if the latter, then
were most time-points available in most studies, such that missing time-points is not a big
issue?
14) i find table 2 hard to follow. why are the authors using dichotomised values of prostate
volume here?
15) sometimes in the text the comparator group is difficult to identify

16 we need ranking plots added, and information about mean rank and sucras, to help
summarise the network meta-analysis results in more detail.
17) for the continuous outcomes, we need more details on whether the effect estimates
were appropriately derived from analysis of covariance (i.e. after adjusting for baseline) in
each trial, as this is the best method.[1] if not, then were effect estimates based on change
scores or final value only? and if so, how might this influence the findings?
vickers aj, altman dg. statistics notes: analysing controlled trials with baseline and follow
up measurements. bmj. 2001;323(7321):1123-4.
18) abstract conclusion says: ““the efficacy of vaporization in large prostates seems
questionable” – no results in the abstract relates to this point as far as i can tell? also, see
my comment about the concern of meta-regression of prostate volume above.
19) moreover, the definition of large is arbitrary. “in the large prostate group (mean pv >70
gm), … “ – we need to be looking at prostate volume as a continuous variable within trials
before making strong conclusions
i think this is a sufficient set of comment for the authors to consider going forward and to
inform the bmj’s decision. i hope my comments are ultimately helpful to all parties going
forward, and will enhance the hard work of the authors to this point.
best wishes, prof richard riley


<|EndOfText|>

i thank the authors for their detailed revision and response to my comments. two major
issues remain.
(i) the authors have not done any one-stage analyses that i recommended. they comment
that this is only possible with ipd. however, as they deal with binary outcomes, a
one-stage analysis is also possible with two by tables, obtainable from ipd or study
publications. see stijnen https://www.ncbi.nlm.nih.gov/pubmed/20827667 and simmonds
https://www.ncbi.nlm.nih.gov/pubmed/24823642
it is very straightforward, and crucially avoids the continuity correction issues. the authors
use the burke reference to suggest that the 1-stage and 2-stage will be similar – but
actually the burke paper strongly states that the key situation where they do differ is when
events are rare! the paper says: “for all outcome types where studies are expected to be
small, and in particular, for binary and time-to-event outcomes that are rare (or extremely
common), then a one-stage approach is preferred, as it avoids the use of approximate
normal sampling distributions, known within-study variances, and continuity corrections
that plague the two-stage approach with an inverse variance weighting.”
therefore, i strong recommend that the authors also include a one-stage analysis for
completeness. ultimately the findings in the paper may be open to criticism if this is not
included. such analyses do not need to include the double zero studies, and may even just
be placed in supplementary material, but should be discussed in the main paper.
the authors say that “in order to conduct additional analyses using the ipd, we would
need to propose our new analyses, resubmit a data request, and then re-analyze the data”
– this is concerning, as all that are required are the two by two tables which already are
used in the peto and m-h meta-analyses that are incorporated in the paper. so no new
data are needed than actually is already being used.
ii) the authors also do not address my query about the time of the events adequately (also
made by reviewer 1, comment #3). specifically, why hazard ratios are not reported (or at
least rate ratios), which account for the length of follow-up, in the ipd trials. the authors
main argument is that other articles report odds ratios, and that further analysis would
need more data access agreements etc. but the issue remains: what is the actual
time-point that the summary odds ratios relate to? as the follow-up length differs in all
trials, how do we translate the findings? odds of an outcome event by what time? e.g.
“patients treated with rosiglitazone had a 33% increased risk of a composite event
compared with controls” – this is meaningless without a time point attached to it. a quick
look suggests the follow up length varies from 20 to 260 weeks – which is a huge range.
this remains a major limitation and a waste as the ipd should allow at least rates to be
derived, and should be clearly reported as a limitation in the discussion. with the event
rates being low it is possible that odds ratios and rate ratios may be similar, but this should
be justified more formally, and again the wide range from 20 to 260 weeks suggests that
this may not be sensible. regardless, we need more reassurance about this issue, and
more context in the paper about how the time-scale corresponds to the interpretation of
the results.

minor:
1) the two sentences in the conclusion of the abstract are somewhat contradictory, as the
first stresses the association and the second raises important cautions: “results of this
comprehensive meta-analysis aggregating a multitude of trials and analysed 140 using a
variety of statistical techniques suggest that rosiglitazone is consistently associated with an
increased cardiovascular risk, especially for heart failure events. while increased
myocardial infarction risk was observed across analyses, the magnitudes of risk varied and
were attenuated through aggregation of summary-level data in addition to ipd.” the
words ‘consistently associated’ are perhaps too strong, and i suggest they rather say
something like: “the direction of summary estimates was the same for all outcomes in all
analyses, though the strength of evidence varied according to each outcome and whether
non-ipd trials were included.”
2) the message that ipd and non-ipd are somewhat different is itself a very important
message. this comes across well in the what this study adds: “among trials for which ipd
were available, we identified a greater number of myocardial infarctions and fewer
cardiovascular deaths reported in the ipd as compared to the summary- level data
reported in publications, csrs, and on clinicaltrials.gov, which suggests that ipd may be
necessary to accurately classify all adverse events when performing meta-analyses focused
on safety.” – could a shorter version of this be added to the abstract conclusion too?
3) “heterogeneity between trials was assessed using the i-squared statistics, with values
greater than 50% indicating moderate to substantial statistical heterogeneity.” – as
mentioned in previous report i-squared cannot reveal if heterogeneity is large or not. the
values of tau-squared (or tau – the between-study sd) do that. so please revise and
correct.
4) not clear in the abstract if you assume random-effects in the meta-analyses (i.e.
heterogeneity in effects between studies), which is your default in the results section.
5) the authors refer in places to the ‘lowest categories for risk of bias’ – is this low risk of
bias? it is ambiguous, so please state more clearly.
6) the inclusion of trials with no events at all (in either group) is somewhat controversial. i
know there is mixed guidance on this in the literature
(https://www.ncbi.nlm.nih.gov/pubmed/30887438 recommends inclusion, although
cochrane do not recommend it and personally i would not). i am not suggesting these
results are taken out but, for transparency, i would mention the inclusion of double zero
studies is contentious in the discussion.
in summary, two very important issues remain. whilst recognizing the large undertaking
by the authors, and the hard work clearly done by the study team, the article remains
open to criticism if these are not addressed better.
with best wishes, richard riley


<|EndOfText|>

this is a very good revision and i am happy with their response to my statistical comments.
they do adjust for baseline and have, most importantly, re-worded they interpretation of the
results.
two minor issues remain:
- it is not clear to me why the quartile baseline value of the fao score, rather than the
continuous baseline fao score itself. i doubt this makes much difference, but it seems suboptimal. perhaps the authors can simpy confirm in the paper that, in their continuous outcome
analyses using sas proc mixed, that the use of a continuous baseline score did not change
findings?
- also, the authors say in the methods that: “this approach which used all available
assessments including baseline as outcomes …” – i don’t think baseline was an outcome, but an
adjustment factor (covariate), as later you say: “predictors in the model included baseline faos
quartile as a factor …”. you can’t have the baseline as both an outcome and an adjustment
factor.
- in the discussion the authors say that confounding is unlikely to be a problem and 'baseline
group comparisons essentially demonstrated equivalence on such factors' - yet table 1 shows
there is slight imbalance, with a higher grade 2 injury % in the physio group. i think the
authors should be clearer about this and tone down their writing here.

<|EndOfText|>

the response to my comments is generally well detailed. there are still a few areas for concern, in a generally well
written and clear paper.
1) i still feel the adjustment factors are quite few. for example, for breast cancer there are only about 6 adjustment
factors in the main analyses. therefore residual confounding is a big concern. but also confusingly, the breast cancer
analyses do not adjust for smoking until the sensitivity analyses and yet the cvd analyses do. this seems entirely
unjustified, as the methods says that all known risk factors were included as adjustment factors. am i missing
something? surely this is a serious omission in the main analyses and it does not make sense to include additional
confounders only in the sensitivity analysis. why not include all potential confounders in the main analysis? this needs
to be clarified. also, perhaps i missed it, but where are the breast cancer results when smoking is included? i don’t
think these are presented.
2) why was education imputed using simple imputation and not multiple imputation itself? i do not understand the
rationale for this, unless it is obvious based on the age what the missing education would be? please clarify.
3) under the tables and in the results, can the authors clarify that when they looked at linear and non-linear (u)
shaped trends, that they did this using the actual difference in alcohol intake, and not based on the categorical
classification?
4) table 3 heading says hrs are presented, but that is not true for the first set of rows
5) i do not see a discussion on potential for residual confounding in the abstract or discussion. again, this is a crucial
omission given the limited number of confounders.


<|EndOfText|>

i am happy with the authors response. they now say, in their response to me, that missing data handled using multiple
imputation led to a similar result. however, i have been through the paper and they have not added this important finding to
the paper itself. many critical readers will ask the same question as me: is the finding robust to the use of multiple imputation
rather than the missing indicator method? therefore, my last recommendation is to ensure the authors add a few sentences to
the paper accordingly about the use of multiple imputation as a sensitivity analysis (in the methods), with subsequent results
briefly given at the end of the results.

<|EndOfText|>

this is a very well written article, and an interesting topic. i have reviewed this from a statistical perspective and have
some important concerns, which i hope are addressable by the authors upon revision but need care and additional
analyses.
1) was clustering of children within the same family accounted for? it seems not, and this would not be captured by
the propensity score analysis. children from the same family are likely to be similar to one another, and therefore are
not independent. the cox regression should therefore be re-done, accounting for this (it could though be noted as a
sensitivity analysis in the article) to see if conclusions remain the same.
2) usually, after a propensity score is calculated, the individuals are ‘matched’ (e.g. one in the smoking group is
matched to a non-smoker, based on their score being very close). individuals who cannot be matched are removed. it
seems the authors don’t do this, and rather include everyone and adjust for the propensity score value in categories.
this seems inferior to me than the matching approach, as the latter allows one to display the baseline characteristics
of the two groups and see that (hopefully) the groups are now more balanced. therefore (again as a sensitivity
analysis perhaps), please re-do the analysis with matching (again, accounting for clustering of children in the same
family too). without the matching, one might still have extremes of the propensity score for one group but not the
other, and therefore regression adjustment may still not be fully adjusting for the confounding (as compared to
matching).
3) it’s not clear how the population attributable fraction is derived in the fully adjusted results. please clarify.
4) children were excluded if they were not followed up for 3 years – and the authors say there were no differences
between these and those that did have 3 years follow-up. but it is not clear if these children were excluded from all
analyses, or just from the dental score analysis at 3 years. they should have been included in the survival analysis, as
that can handle drop out before 3 years please clarify.
a related issue is that the exact time of caries is not known (i think). rather, the authors only know if they appeared
by 18 months or 3 years. so i am struggling to understand why the kaplan-meier method was used, as time of event
is not known exactly. please clarify. were some children censored before 18 months? if so, how was the censoring
time calculated? or are the authors assuming censoring took place at time zero or just before time 18 months? what is
the justification? if there is no censoring, then wouldn’t a relative risk at 18 months (and then again at 3 years) be a
better summary than a km estimate (or perhaps they are equivalent) please clarify, and justify why the analysis
chosen and assumptions are appropriate in relation to the data available.

hazard ratios relate to the whole follow-up period, but here i think the authors are looking specifically at risk ratios at
3 years. therefore either the hazard ratios are actually risk ratios at 3 years, or they need to explain why the whole
time period is being accounted for. why are hazard ratios (or relative risks more precisely) not given for 18 months
also? i assume this time-point could include more patients than the 3 years one, so i’d like to see the effects at this
time also (especially as it is closer to the 4 month measurement of smoking, and thus less likely to be affected by
changes in smoking habits over time).
4) kaplan-meier %s are given in the results but these are unadjusted for confounding, thus should be removed or
clearly labelled as unadjusted.
5) the authors should note that the non-significant result for maternal smoking may be due to low power to detect a
small effect (if indeed genuine).
6) abstract says: ‘hazard ratios of second hand smoke …’ this is loose language. it is hazard ratios of dental caries for
children linked with second hand smoke versus those not linked to second hand smoke
7) page 7 tells us that the number of decayed, missing or filled teeth were 0.06+-0.48 …. what does these numbers
actually mean? is it the average number per individual with its standard error? if so, how can the lower interval value
be negative?! <0 decayed teeth? please be explicit!
8) the propensity score analysis adjusted for the propensity score divided into quartiles. this seems again unjustified,
as it loses information and the propensity score should be left on its continuous scale. please see point 3 above for a
better method also.
9) were there any missing values in the adjustment factors used in the propensity score analysis? with such
databases, there are usually missing values for some patients, but i cannot ascertain how this was handled in the
analysis. was multiple imputation used?
10) the authors say there were no differences between those included and excluded from analysis (supp table 1). i
think there are observed differences, but these are generally small. please re-word. similarly when discussing supp
table 3.
in summary, this is an important study but there are a number of important areas for re-analysis and added clarity, to
address my concerns. i hope my comments are helpful to the authors moving forward.

<|EndOfText|>

i thank the authors again for providing a comprehensive response, which i have been through. i assume
they have updated their manuscript accordingly as they stated they have (i haven’t had time to check
this line by line). a few final things:
1) the response to question 3 is interesting, with an updated analysis provided included the updated
follow-up information of participants after the first submission of this article. it is worth adding the
actual results to the paper. i see the authors already mention this updated analysis in the new revision,
so it is merely a case of adding a table (an appendix should be fine for the results).
2) by the way, these new results for 28 days in this table are given as either ne (not evaluable) or
100% (ci 100 to 100). i don’t think a ci of 100 to 100 is sensible to state, and it is unclear why the
results for the hcq group are ne at 28 days. explanation is needed below the table.
3) page 18: please change “the hazard ratio was estimated by the cox model, which is the higher, the
more rapid the virus negative conversion or symptoms alleviation is” to “the hazard ratio was estimated
by the cox model, and hrs greater than 1 indicate the rate of virus negative conversion or symptoms
alleviation is higher in the which group compared to other group’ – and of course please define the
groups appropriately.
4) page 24 – trail should be trial. actually, if you search for trail, there are 6 occurrences in the article.
these should be replaced with trial.
5) supplementary table 1: “negative conversion rate by the specific time” – this should be “probability
of negative conversion by the specific time” – also heading should say difference in probabilities not
difference in rates

6) abstracts: “the median time from randomization to illness onset was 16.6±10.5 days” – i find this
confusing upon re-reading. the participants are already ill upon randomisation, as they have tested
positive, so what illness onset are the authors referring to here?
with best wishes, richard riley


<|EndOfText|>

the response and changes to my previous comments are generally very clear, thank you. i
especially thank the authors for now also providing the absolute risk at 7 days as
requested. the difference in survival probability by 7 days is tiny, 0.93 versus 0.94, which
is very important for the bmj reader to appreciate.
my main remaining comment relates to the missing data. i asked the authors about the
amount of missing data, but the response is not satisfactory. is there missing data or not?
i cannot identify a clear answer to this. they note the database is considered high quality,
and have added the following to the paper: “it has been reported that there is less than
one percent of records that have missing information and less than 0.01 percent of records
that have missing key elements (e.g. demographics, diagnostics).” this is great, but the
use of ‘it has been reported’ suggests the authors are referring to previous work that uses
this database. i want to know, is there missing data in the data they obtained for analysis
in this paper? if so, how much, and how was it handled (e.g. complete case analysis?).
they also say: “in administrative databases, there are essentially no missing data in the
conventional sense (in contrast to cohort studies with ad hoc data collection for example).”

– but i work with administrative (routinely collected) databases regularly, and there is
routinely missing data. for example, smoking status is not recorded for some patients, or
bmi is not available, etc.
lastly, figure 1 does not suggest an interaction between the hr and time. yet this is a key
finding; i.e. the hr gets smaller after the first few days. i can therefore only assume this
graph is produced using the proportion hazards assumption, even though there is strong
evidence that this assumption is not correct. the graph should be corrected accordingly. or
rather, include a second graph with the cumulative incidence estimated based on the
non-proportional hazards approach.
best wishes, richard riley


<|EndOfText|>

i thank the authors for their detailed response and revised manuscript, with is certainly
improved. i do, however, have some further comments that need to be considered in a
further revision.
can there be some consideration of how the findings translate to impact on absolute risks?
for example, by 5 and 10 years after follow-up what is the average additional risk (or
reduction in risk) for each food type? i raise this because significant hazard ratios are

difficult to place into context without translation to the absolute risk scale at key
time-points.
thank you for adding the multiple imputation analysis. the multiple imputation analysis is
much better described in the response to reviewers document, than in the revised paper
which only says: “fifth, we used a markov chain monte carlo (mcmc)-based method to
impute missing data … “ – how many imputations were used and how were results
combined across imputed datasets (e.g. rubin’s rules)?
after imputation, the authors only examine the categorised variables, and not the best
analyses which model the variables on their continuous scale. do the non-linear
relationships change after using imputation to deal with missing data, rather than the
missing indicator method?
“the inverse associations for total whole grains as well as individual whole grain foods were
attenuated but remained statistically significant after adjusting for bmi” – but the result for
popcorn was not statistically significant before adjustment for bmi were they?
“the non-significant p-values for heterogeneity …” – there are only 3 cohorts, so power to
detect genuine heterogeneity between cohorts is very low. worth mentioning this at end of
this sentence for clarity.
“the goodness of fit of fully adjusted model was significantly improved by additionally
adjusting for individual whole grain foods (nhs: p<0.0001; nhsii: p<0.0001; hpfs:
p<0.0001) suggesting potentially heterogeneous associations with t2d risk.” – i think here
the heterogeneity statement refers to the effect being different for different whole grain
foods. make this clear, and just previously the word heterogeneity is used to refer to
between cohort heterogeneity.
“in light of the positive association for popcorn intake, we repeated the likelihood ratio test
by removing popcorn intake variable and found similar results.” – not clear what is meant
by this sentence. that you wanted to ensure the significant test for differences in individual
wheat grain produces was not driven by the popcorn variable? if so, explain this explicitly
for the reader.
paragraph starting “cubic spline modelling suggested …” – i think the authors should make
it clear what adjustment factors were used in the findings being discussed. and similarly in
next paragraph.
in paragraph that starts with “after adjusting for bmi and other lifestyle and dietary risk
factors …”, there is a subsequent sentence that says “the inverse associations for total
whole grains as well as individual whole grain foods were attenuated but remained
statistically significant after adjusting for bmi” – so the paragraph starts by saying bmi was
adjusted for and then say results were the same when bmi was adjusted for. hence
confusing! please revise accordingly.
phrase “a borderline statistically significant p trend” is inappropriate and should be
removed. what is a p trend? the authors should just say there less clear evidence of an
association for popcorn when popcorn types were analysed separately.
p trend is also used in the tables and elsewhere – please remove and give correct wording
(p-value for test of trend?).
“mutually-adjusting for individual whole grain foods produced attenuated but statistically
significant estimates except that the association for wheat germ was attenuated to null” –
this is confusing as attenuated means attenuated toward the null anyway. are the authors

saying that even after adjusting for each other results for each type remained statistically
significant apart from wheat germ? also, is popcorn even significant to begin with?
“however, it is unlikely that the underlying biological mechanisms differ substantially by
race” – i think this is an overly strong statement, as type 2 diabetes is well known to be
associated with race, and so the impact of different food types may also be.
table 1 should also reveal the % with missing values for each variable; currently this is
excluded it seems?
table 2 and other tables, need to make it clear that the ‘p trend’ relates to a test for trend
using the continuous values and not the categorised values in the preceding columns.
also, is this a test for a linear trend?
table headings could be clearer for all tables. e.g. table 2 says “pooled hrs (95%
confidence intervals) of type 2 diabetes for total whole grains consumption …” – i think the
authors mean “pooled hrs (95% confidence intervals) quantifying the adjusted association
of total whole grains consumption and the hazard of type 2 diabetes.”
table 4 – p-values for interaction are based on categorised variables. please make this
clear, and also add note on findings when left on continuous scale.
figure 1 – great to see this added to the main paper. please say in heading that the trend
is the adjusted pooled relationship across the three cohorts, assuming it is common to
all cohorts. same with other figures showing dose response relationships.
i hope this review is helpful, and allows the authors to clarify the article findings further
before publication.
best wishes, richard riley


<|EndOfText|>

the response to my comments is excellent, and i am happy with the revision. however, i must
recommend that the titles for figures 2 and 3 be improved. they must come with a clear warning (in
the title or footnote) that the uncertainty of each ratio is not displayed, due to the majority of studies
not providing this, and that pooling results is not justified due to the heterogeneity and poor reporting.
further, they do not state that each circle represents a study, and how the size of the circle was
calculated.

<|EndOfText|>

i thank the authors for their response to my previous review. the revision is improved,
and the conclusions tempered somewhat as appropriate. most aspects have been
addressed well, but a few queries have sadly been dismissed outright. in particular, i
asked the authors for more detail on the actual analysis methods, as previously they had
simply referred to using a statistical software package. as a bmj statistics editor for over
10 years, i am well aware that transparent and clear reporting is important to the bmj, not
just for a clinical audience (who may indeed apply the results in practice as the authors
note) but also for those appraising the review and its findings (methodologists, reviewers,
those conducting umbrella reviews, and indeed clinicians and health professionals).
therefore, to simply dismiss the need for more transparency of the analysis methods is
rather alarming.

for example, the authors say “we consider it impractical to provide exhaustive detail of the
statistical methods for conducting the network meta-analysis, and alternative ways of
conducting the analyses, due to both space constraints and the fact that these would be of
limited interest to, and potential confusing for, a readership that is composed
predominantly of primary care clinicians looking for up-to-date evidence on which to base
their day-to-day practice. we believe we have already provided adequate information
regarding the type of network meta-analysis and the software used. there are other
resources available elsewhere, which we have referenced, should readers wish to obtain
more detail regarding performance of network meta-analysis.”
in response to this, i would like to emphasise that there are no space constraints. the bmj
publishes the full article on-line. further, the details requested (estimation method and
assumptions made, and whether uncertainty is fully accounted for when producing
confidence intervals) are pivotal for understanding whether the methods used are
appropriate. the authors are not appreciating the importance of this, and the additions
would just take 2-3 sentences, so it is unclear why this is just dismissed. there are many
opportunities to disseminate their key findings in a briefer form (e.g. abstract, press
release, what this study adds, etc), but the full article must adhere to reporting guidelines
and be transparent.
that being said, most of the responses i find adequate, so i don’t want this issue to detract
from the generally well conducted work. but i hope the authors will reconsider their stance
to not addressing my comments for a few more details.
further comments
1) the authors say: “estimates of τ2 of approximately 0.04, 0.16, and 0.36 are considered
to represent a low, moderate, and high degree of heterogeneity, respectively” – why are
these numbers valid? surely this depends on the actual outcome being analysed, the scale
of analysis, and the magnitude of the pooled effect (variability around a or of 1.1 may be
more important than variability around an or of 5 say).
2) i asked for the authors to give study-specific results, not just the pooled results. but
they feel this is an unreasonable request. i see that the authors report that they do
adhere to the prisma extension to network meta-analysis. therefore, i refer them to item
20 in the prisma extension to network meta-analysis that says: “results of individual
studies: for all outcomes considered (benefits or harms), present, for each study: 1)
simple summary data for each intervention group, and 2) effect estimates and confidence
intervals. modified approaches may be needed to deal with information from larger
networks” this is not a large network, so i do not see the issue of at least presenting
study-specific results in the supplementary material for each outcome (or at least the main
outcomes). again, this is to ensure transparency of the evidence.
3) the authors says: “as there were direct comparisons between all of the management
strategies, we were able to perform consistency modelling to check the correlation between
direct and indirect evidence.” – change the word correlation to agreement.
4) the authors say: “there was no evidence of inconsistency after applying the χ2 test of
the q statistic (1.91, p = 0.93).” there is potential confusion here, because the q statistic
is used to examine between-study heterogeneity, and not inconsistency. this (and later
occurrences) must be revised using the correct method for conducting a global test for the
presence of inconsistency. they refer to my paper (ref 35), but this does not recommend
using the q statistic to do this. references that show the correct approach are as follows:
higgins jpt, jackson d, barrett jk, lu g, ades ae, white ir. consistency and inconsistency
in network meta-analysis: concepts and models for multi-arm studies. res synth method.
2012;3:98-110.

white ir, barrett jk, jackson d, higgins jpt. consistency and inconsistency in network
meta-analysis: model estimation using multivariate meta-regression. res synth method.
2012;3: :111-25.
5) “when data were pooled, there was no statistical heterogeneity (τ2 = 0.007)” – rather
say there is very little observed heterogeneity; as tau-squared is > 0 this is some small
heterogeneity. same issue occurs at later points too.
in summary, many responses are entirely appropriate and i thank the authors for that. i
hope the few outstanding issues will be addressed in the next revision.
best wishes,
prof richard riley, bmj statistics editor.


<|EndOfText|>

thank you for the opportunity to review this interesting article. the research presented
represents a huge undertaking, and there is much to be commended here, not least for
getting this much data together to address this question, and by looking at the association
between inactivity and dementia at different lead times. i do feel that there are areas that
could be strengthened and there are a number of queries, as follows
- my main question is where does the 10 year cut-off come from? specifically, why does
looking at activity levels at > 10 years remove the reverse causality issue? why not 9
years, or 11 years, or 5 years? what is the clinical rationale for 10 years being a safe
time-point?
i wonder whether the authors would be better (or at least it would be more complete a
picture) if they report may time-period results, say 0-5, 5-10, 10-15, 15-20 etc to see if
the trend is gradually reduced, rather than just choosing the 10 years value? perhaps they
are limited by the information about activity levels in the available ipd.
regardless, this issue needs to be addressed, justified, or noted clearly as a limitation.
- did the meta-analysis account for the uncertainty in the estimates of heterogeneity,
when deriving confidence intervals? there is strong evidence that approaches such as
hartung-knapp improve coverage of confidence intervals from meta-analysis (including
ipd meta-analysis), and can be implemented as part of the 2nd-stage of the analysis. see
the refs: 1 2
- i-squared is a poor measure of the size of heterogeneity and be misleading (see rucker
et al. 3) – better to focus on the estimate of heterogeneity and even prediction intervals
(though i do see there is no estimated heterogeneity it seems)
- what was the estimation method used to fit the meta-analysis models?
- it is not clear to me how proportional hazards are examined within and across studies
– the comment “we examined whether the hazard ratio for physical inactivity is stable over
the follow-up using flexible parametric proportional-hazards for censored survival data” is
quite vague. the authors say later that the hr does change after a few years, which
support their decision to split at 10 years, but this seem contradictory to use 10 years as
the split and not 2 years. “as shown in figure 2, the associations of physical inactivity with
dementia and alzheimer’s disease varied over time, being strongest during the first few
years of follow-up and then attenuating to the null. this non-proportionality of hazards
(departure from proportionality p <0.0001 for dementia and alzheimer’s disease)
supported our decision to split follow-up period into two; the first 10 years of follow-up and
from year 10 onwards.” in short, the explanation of proportional hazards and use of 10
years cut point needs more justification and expansion.
- “to address regression dilution bias due to measurement error … “ – measurement error
does not always leads to dilution of effects; for example if the error is larger in those with
certain levels of the variable than others.
- how was missing data handled, for example for adjustment variables?

- one notable limitation is the categorical definition of physical activity, which i assume is
due to using existing studies and inconsistent definitions and coding across studies. can
the authors explain this further, and why a continuous scale was not possible / used?
- figure 2: it is not clear if this is for the physical activity measured before 10 years or
after 10 years?
- the abstract might mislead reader to think 9741 studies are used from the ‘data sources’
– so perhaps state that 19 of 9741 studies provided relevant data in this section too.
- dementia definition requires full diagnosis – maybe miss people who are in the early
stages? thus the outcome may be underestimated – could this be a limitation and, if so,
how may it impact upon the findings.
- a key finding noted in the abstract and results is “there was an imprecisely estimated
excess risk of dementia following cardiometbolic disease in physically inactivite versus
physically active individuals” (note inactive and cardiometabolic are mis-spelt by the
authors). the authors are inferring that the hr is different for those with and without
cardiometabolic disease. however, i do not see an interaction test (and indeed estimate of
the difference in) for whether the log hazard ratio is different for those with and without
cardiometabolic disease. the authors refer to figure 4, but cardiometabolic is not
mentioned in this figure.
in summary, this is a very interesting piece of work that has much potential for
publication, but there are some issues that need to be clarified and addressed to help the
bmj make a decision. i sincerely hope my comments help the authors going forward.
best wishes, prof richard riley

1. hartung j, knapp g. a refined method for the meta-analysis of controlled clinical trials
with binary outcome. stat med 2001;20(24):3875-89.
2. cornell je, mulrow cd, localio r, et al. random-effects meta-analysis of inconsistent
effects: a time for change. ann intern med 2014;160(4):267-70.
3. rucker g, schwarzer g, carpenter jr, et al. undue reliance on i(2) in assessing
heterogeneity may mislead. bmc med res methodol 2008;8:79.


<|EndOfText|>

i thank the authors for their further revision. i do think this trial has important value.

it is good that the authors re-ran their analyses, and that the bayesian and frequentist results now
agree more closely for the enterotomies analysis (although a little concerning that the p-value originally
presented to us was 0.99 and now it is 0.49).
the article is much improved by the additional sentences and changes in wording. my only remaining,
minor comment is that the abstract says: “both robotic and laparoscopic ventral hernia repair have
similar 90-day postoperative hospital stays.” – is this a fair conclusion given the width of the ci is 0.37
to 2.19, and so is very wide? better to say there was no evidence of a difference in 90-day hospital
stays. please consider here and elsewhere (eg discussion, what this study adds)
with best wishes, prof richard riley


<|EndOfText|>


i thank the authors for replying to my comments and revising the article. i am pleased to
see the analysis of covariance results added. there are some unresolved issues still.
1) the authors now explain that they used negative binomial regression, and i agree this is
sensible. they use this to derive relative risks, but are they not relative rates? i suggested
they tell us about the difference in log-counts, which when exponentiated is a ratio of rates
(as it is counts per unit time, which here is 90 days i think)
see for example here:
https://stats.idre.ucla.edu/stata/output/negative-binomial-regression/
if the authors want to use the word risk, then they need to say an explicit time-point by
which the risk happens by
2) i asked the authors why the bayesian and frequentist results appear to disagree, and
suggested this may be due to the prior distributions having a strong influence in the
bayesian analysis. the response received is somewhat confusing, and i am not entirely
confident that we have resolved this.
e.g. the authors say “we re-reviewed the analyses with our statisticians. our statisticians
believe that, because the numbers are low and one of the surgeons had no enterotomies,
the generalized linear model (glm) stratified by surgeon may have provided skewed
results. without the stratification, the p-value is 0.498. it does not change the frequentist
result perspective. ” – i am not sure what the authors mean by ‘skewed results’ – biased
results? anyway, i do not see why stratification would cause issues in the frequentist
approach and not the bayesian approach, unless the prior distributions are adding
important information.
stratification is the correct approach, because otherwise the clustering of patients within
surgeons is not accounted for. but the authors might want to put a random effect on the
surgeon, rather than estimating separate terms for each, due to the few evemts
clearly, there is tiny information to be making strong inferences here “there were
two enterotomies with rvhr compared to none with lvhr and the bayesian analysis
demonstrated a 78% probability of rvhr increasing the risk of enterotomy”
therefore we are left with a bayesian analysis that says 78% chance of success, compared
to a frequentist p-value of 0.99 with stratification. these still do not seem to tally. to me, i
would expect a p-value of 0.99 to correspond to a bayesian probability of close to 0.5
(50%).
“this study unexpectedly demonstrated that rvhr was associated with an increased risk of
enterotomy (3% vs 0%)” – a strong conclusion and yet the p-value is 0.996.
also the authors say “we will leave it to the discretion of the journal editors and statistician
which p-value to report.” – this is not appropriate for us to make this decision, as it
passing the buck to people who do not have the actual data and are not doing the actual
modelling. it is not very reassuring.
3) “however, robotic repair had longer operative duration (141 vs 77 minutes;
coefficient=62.89 [95%ci=45.75-80.01]; p=<0.001”
- what does coefficient mean?
4) “robotic repair had clinically important differences in enterotomies (3% vs 0%;
p=0.996)” – this relates to point 2, but clearly this statement seems inappropriate because
of the lack of strong evidence that this is not a chance difference. we need the 95% ci
please. i think the strong statement is coming from the bayesian framework.

5) “there were no differences in conversions to open, number of readmissions, emergency
room visits, wound complications or other complications” – better to say there was no clear
evidence of a difference? as there were observed differences, just that it is not statistically
strong evidence of a genuine difference. same applies to ‘no difference was seen in change
of pain scores between groups at 1-month post-operative follow up.’
6) “a higher percentage of patients in the rvhr arm experienced a major worsening in
aw-qol (28.1% vs 13.6%; rr=2.07 [95%ci=0.98-4.41]; p=0.058) while a higher
percentage of patients in the lvhr group had a major improvement in aw-qol (43.8% vs
52.5%; rr=1.20 [95%ci=0.83-1.74]; p=0.330).” – i would add a sentence to say
‘although, confidence intervals were wide’
7) table 3: we need treatment effect estimates and 95% cis, not just p-values, to
compare the groups
in summary, whilst recognising the improvements in the paper, i think there remain
serious concerns about the interpretation (especially for the enterotomies analysis) that
the bmj and authors need to consider going forward. regardless, i hope my comments are
ultimately useful to the bmj and the authors, who have clearly worked very hard to
undertake this trial.
best wishes, richard riley


<|EndOfText|>

the response to reviewers is very comprehensive, and i am happy with their response and the
sensitivity analyses reported show consistent findings.
however, i am not happy with the actual revision because they do not refer to any of the sensitivity
analyses that we asked them to do, such as looking at continuous trends and multiple imputations and
including adjustment factors individually (rather than as a score).
the sensitivity analyses need a separate section in the methods and the results, and detailed results
can be given in the online material. given that the authors show us, in their response to comments,
that the sensitivity analyses show consistent conclusions to those originally shown, this is an important
message and will improve the robustness of their findings for the bmj reader.
also, tables 2 and 3 do not actually define the numbers within the tables as rrs
page 10: ‘in in’ – delete repeated word
i’m sure the author can address these final issues.
best wishes, richard riley

<|EndOfText|>

thank you for inviting me to review this interesting paper. it is very clear that the
authors have undertaken a painstaking overview of the existing evidence in regards
to diet and prevent of type 2 diabetes. a huge undertaking, and umbrella reviews
are very important to provide a broad summary of an ever expanding field. please
find below my comments and suggestions. i sincerely hope these help the bmj and
the authors going forward.
1) i must admit that i am circumspect of many nutritional epidemiology studies, as
often it is hard to identify exactly the type of food/diet under review and exactly
if/how this is related to outcome risk. i think similar concerns arise in this overview,
as by taking an umbrella overview of all diet studies in this field, the scale is very
broad and it is hard to identify specific implications for what diet is beneficial. for
example, in the abstract the main conclusions relate to food/drink such as sugary
sweetened beverages, processed meat, and red meat. but these are very broad
groups – e.g. what specific sweetened beverages are we talking about? what
specific processed meat? e.g. in their discussion they say: “in accordance, an
unhealthy dietary pattern, high consumption of red meat, especially processed meat
(e.g. bacon, hamburgers or hot dogs), animal protein and heme iron were related to
an increased risk of t2d.” – but is it bacon or a hot dog i should be avoiding?
i find that focussing recommendations on a broad class is difficult to interpret. of
course, this is a consequence of the authors summarising the existing evidence – so
i am not criticising the authors themselves, as they can only summarise what is
reported. but i do worry about the translation of the findings for the bmj reader,
and that the press may pick up on some broad (non-specific) message.
2) another reason for concern is the difficulty in adjusting for confounders, as the
findings are all based on primary studies that were observational. as the focus in the
review is at the broad umbrella review level, i do find it quite detached from the
original primary studies. in particular, what adjustment factors were used in each
primary study? were they adequate? what methods were used to adjust for
confounding in primary studies and were they suitable? is a linear dose response
relationship truly justified? indeed, was this even checked in the original studies, let
alone at this umbrella review stage? these are just some examples of why i find the
review rather detached from the original primary studies, and thus it is hard to
ascertain whether the findings are meaningful.
3) in regards to the adjustment factors, the authors say: “almost all of the primary
studies adjusted at least for age and sex, with the exception of four primary studies
which reported crude estimates.” – surely these 4 studies should be removed?

further, “ 80% of the primary studies conducted a multivariate adjustment (e.g. for
total energy, body mass index, smoking status and physical activity).” – yes, but
were the adjustment factors adequate? it would perhaps have been clearer had
the authors pre-specified a set of adjustment factors that were considered essential
(minimum required), in order to have some credence that the adjusted results were
only prone to small residual confounding.
4) related point: in the discussion it says “it is likely that individuals with unhealthy
dietary behaviours, such as low intake of whole grains and fibre, as well as higher
intake of red and processed meat, have an unhealthier lifestyle per se, such as
higher rates of obesity, smoking and physical inactivity83-85. most of the included
studies adjusted for these factors, and associations persisted” – the word ‘most’ is
not reassuring to me, but moreover the question remains as to whether the
adjustment of these factors (when done) was actually adequate. was a regression
approach used without backwards/forwards selection of adjustment variables? or
perhaps a propensity score analysis was done – but was it done well? etc.
5) multivariate adjustment should say multivariable adjustment
6) i find the data extraction description confusing in the methods. e.g. “if the rr
estimates from primary studies of a dose-response meta-analysis were not reported
in the published meta-analysis, we did not recalculate the meta-analysis, but
extracted the srr from the published meta-analysis. if we could not identify a rr
estimate from a primary study of a high vs. low meta-analysis in the published
meta-analysis or the primary study itself, we excluded that particular primary study
from our meta-analysis.” – please re-write this in clearer language for the bmj
reader to follow.
7) i might be wrong, but it appears to me that the authors are pooling
meta-analysis results. why not actually take the original primary study results, and
pool these in a single meta-analysis? i do not see why pooling the original
meta-analysis results is more helpful. please can they justify this. also, does this not
then make the heterogeneity a between-meta-analysis heterogeneity? rather than a
between-study heterogeneity? if so, this is hard to interpret.
8) it also appears that the meta-analysis results bare eing pooled ignoring the
uncertainty in heterogeneity estimates (within a meta-analysis and across
meta-analyses). this would be easier to address if pooling all the study-specific
results in one go. see references such as cornell et al. and the use of methods such
as the hartung knapp method for widening confidence intervals
cornell je, mulrow cd, localio r, et al. random-effects meta-analysis of inconsistent
effects: a time for change. ann intern med 2014;160(4):267-70.
hartung j, knapp g. a refined method for the meta-analysis of controlled clinical
trials with binary outcome. stat med 2001;20(24):3875-89.
9) heterogeneity should not be measured by i2, and it is wrong to use values of i2
to define low, moderate or high heterogeneity. better to report estimate of the
heterogeneity itself (tau-squared) and, possible, prediction intervals to disseminate
the heterogeneity.
rucker g, schwarzer g, carpenter jr, et al. undue reliance on i(2) in assessing
heterogeneity may mislead. bmc med res methodol 2008;8:79.
10) funnel plot asymmetry does not imply publication bias; a better word is
small=study effects, which indeed may be due to pub bias, but might also be due to
other things.

11) the authors report relative risks. but are the studies really reporting hazard
ratios? and if not, then what are the time-points of interest for diabetes onset, as
the rrs are time-specific measures. this is a critical issue, because i do not see
justification for why relative risks are useful in this context and not hazard ratios. ?
if they are hazard ratios, then really are these constant over time? was this
checked in the original studies? another example, perhaps, of being too detached
from original studies.
12) what does this mean: “for most of the associations, there was no indication for
presence of publication bias according to egger’s test (p≥0.10), with the exception
of chocolate, whole grain, wheat germ, rice, white rice, soy products, legumes, hot
dogs, animal protein, monounsaturated fatty acids, total carbohydrates, total fibre,
vitamin d, total iron in high vs. low meta-analyses, as well as total dairy, low-fat
milk, coffee and cereal fibre from dose-response meta-analyses
(table 1).” – the authors imply no publication bias, and then list many areas where
there may be. i find this confusing.
13) is it justified to mix cohort and case control studies? moreover, it seems that
‘cross-sectional’ studies are also included. but surely we need a design with a
time-to-event outcome, to at least have reassurance that the diet recording was
made at a point before the onset of diabetes. more explanation is needed in these
matters.
14) “the quality of evidence by applying the nutrigrade scoring system, which
comprises different sources of bias (including funding), study design, heterogeneity
between studies, the effect size and its precision.” – i do not see why the effect size
and its precision should be used to define quality. a more precise estimate does not
imply higher quality. indeed a good quality study should be defined independent to
any effect size estimate and any magnitude of precision. yes, bias may impact these
things, but the actual decision about quality should be based on the information
about the factors that cause it.
15) in regards to evaluating quality, i also found it confusing that the overall quality
assessment is made at the meta-analysis level (i.e. each meta-analysis included in
the umbrella review), and not at the study-specific level. if the authors rather pool
the original studies, rather than the meta-analysis results, should the quality
assessment be made at the study-specific level? they could even remove primary
studies that were at high risk of bias, which would otherwise still be included in the
meta-analysis feeding into the umbrella review.
16) is publication bias examined at the study-level or the meta-analysis level. that
is, are multiple primary study-specific estimates plotted on the funnel, or the
multiple meta-analysis results per diet type presented on the funnel plot. again, i
find it hard to ascertain the level of the pooling. i think it is the primary study level.
sorry that i have many concerns/comments. most of these relate to the potential
issues with the primary studies, which perhaps cannot be addressed easily at the
umbrella level, yet do impact upon interpretation and translation of the paper’s
findings for the bmj reader.
best wishes, richard riley


<|EndOfText|>

this is a well-written and well reported article, as expected from the set of authors involved. the authors have clearly
worked hard to address an important topic using the primary care database available. it is good to see that a protocol
was published for this cohort study. i have reviewed this from a statistical perspective, and although standards are
generally good as expected, i have some comments for improvement and areas for clarification to be addressed in any
subsequent revision:
1) when reading the article, my initial impression was that there are a lot of analyses here, for example across
different 3 outcomes, different classes, individual drugs,, and different follow-up times. for the latter it says in the
methods that ‘as sensitivity analyses we repeated the analyses firstly restricted to the first year of follow-up, then
including the entire follow-up period’ and also time since starting treatment is investigated as categories, e.g. first 28
days.
but in the protocol, though 5 years and 28 days are mentioned, i cannot see mention about the 1-year analyses. can
the authors clarify please why they focused on 1-year in the end, if not mentioned in the protocol (perhaps i am
missing something)?
2) in relation to this point, most analyses over the 5 year period are not significant, but there are more significant
results by 1 year. this suggests that the hazard ratio is not proportional over time, but this is not evaluated formally
(statistically) and raises the question about the hrs from years 1 to 2, and 2 to 3 etc. therefore i find the focus on 1year an incomplete picture, and wonder whether the authors could comment in the results about whether the
proportional hazards assumption was appropriate (the methods say it was examined, but we don’t see the details). i
would find it strange that the hrs at 1-year are significant but not at 5-years, if the proportional hazards assumption
is actually ok.
many 1-year results are the main message in the abstract and conclusions, yet they are only given in the
supplementary material in the actual paper. i think they should be brought into the main article tables, and this may
link to a more detailed investigation of the proportional hazards assumption (if the hr is constant over time, or what
the hr is within each year interval upto 5 years).
of fundamental interest: if the sris are associated with benefit for the first year but overall the 5 years there is no
difference, does this mean that the sris are associated with harm in the latter years?
3) further, the authors look at 3 outcomes in the paper ‘arrhythmia, myocardial infarction and stroke or transient
ischaemic attack’. yet, in the protocol there were far more than 3 outcomes listed (see below), and none were
mentioned as primary outcomes. can the authors clarify why they looked at these three outcomes in this paper as a
priority over other outcomes listed below:
•all-cause mortality
•suicide (including open verdicts)
•attempted suicide/self-harm
•sudden death
•overdose/poisoning with an antidepressant
•myocardial infarction
•stroke/transient ischaemic attack (tia)
•cardiac arrhythmia
•epilepsy/seizures
•upper gastrointestinal bleeding
•falls
•fractures
•adverse drug reactions (including bullous eruption)
•motor vehicle crash.
4) the authors adjust for confounding using cox regression, and it is good to see that many confounders are indeed
adjusted for. that being said, i would also have liked to see whether conclusions are robust to the use of propensity
score matching methods. or can they justify in the discussion why this wasn’t considered beneficial over traditional
regression adjustment? perhaps, due to the time-varying nature of the use of antidepressants, this was problematic

5) can the authors clarify in the paper the use of the time-varying antidepressants covariate and its interpretation for
an individual who stopped. if an individual stops antidepressants, then do they then (for subsequent follow-up periods)
move to the non-treatment group? if so, then how does this handle the potential for events to be due to the earlier
use of antidepressants? could it be that the lack of any differences between groups is because some of those who
were on anti-depressants or moving into the non-treatment group, and therefore any genuine difference is being
attenuated?
6) “even for doses of citalopram ≥ 40 mg/day there was no significantly increased risk (adjusted hazard ratio=1.11,
95% ci 0.72 to 1.71).” – though this statement is correct, the confidence interval is 0.72 to 1.71 and is therefore
wide: is there low power? indeed, there are not many events in many analyses. this is worthy of discussion please in
the strengths and limitations section.
7) i am also concerned about missing data: the authors say ‘we included all eligible patients in the database in our
analyses to maximise power’ – but there are no details about how missing data were handled. i notice that under a
table it says ‘5.0% of prescriptions had missing information on dosage.’, so there is some missing data – but how was
it handled? it is also not mentioned in the protocol.
8) the authors used ‘robust standard errors to allow for clustering of patients within practices’ – such methods are
done when the model used is mis-specified (or the correct model is difficult to actually fit), and therefore the ‘robust’
standard errors used to inflate uncertainty accordingly. however, here i do not understand why the clustering within
practices was not accounted for by using, for example, using a stratified cox model or adding a frailty term (with a
random effect on the baseline hazard to allow for separate one for each practice). though this is a minor point, i
would like the article to clarify if alternative approaches to accounting for clustering affected the conclusions.
9) in places, the authors infer a difference between individual drugs, which is often not justified. this is most apparent
in the ‘absolute risks’ section, where they say ‘absolute risks of arrhythmia and myocardial infarction were highest for
lofepramine’ – this is not justified, as the cis for the risks and nnh are very wide and overlap with the other drugs.
this therefore needs to be re-written. please check elsewhere for this issue too.
in summary, this is an important piece of work, and i hope my comments help to improve the article further,
especially in regard to the outcome investigated, the time-points considered and the use of time-varying covariate.

<|EndOfText|>

thank you for the opportunity to review this interesting and well-written manuscript. i have focused on
statistical aspects, and have the following comments
1) why use poisson regression and not a cox model? the former assumes a constant hazard rate over
time, whilst the latter does not and so is more flexible (and more plausible).
2) why were adjustment factors categorised? i find this sub-optimal to including them as linear or,
preferably, by allowing for potential non-linear trends using restricted cubic splines
3) the missing indicator method is not recommended, and imputation should be used. this is a major
issue that needs to be addressed going forward. see for example groenwold rh, white ir, donders ar,
et al. missing covariate data in clinical research: when and when not to use the missing-indicator method
for analysis. cmaj : 2012;184(11):1265-9.
4) those that died before cvd were censored – but this then makes absolute risks and rates inflated to
an artificial population that can never die (and so must at some point develop cvd). so, although hrs
will still be cause-specific, there needs to be much caution about the estimate of absolute risk and rates
when the competing event of death is not modelled. it would be more correct to do a competing risks
analysis, and then derive absolute risks from such an analysis (e.g. using a subdistribution approach).
hrs can still be reported from the suggested cox approach however.
5) incidence rate ratios (irrs) are derived, but why are these a constant over time? this should be
checked, ideally in a cox model as stated. in other words, when fitting the cox model, the proportional
hazards assumption should be checked, as the hrs may not be constant over time.

6) “the increased risks were more pronounced among offspring of diabetic mothers with comorbid cvd
…” – such statements should be justified with an estimate of the difference between groups (i.e. the
interaction between the effect of diabetes and presences of cormobid cvd)
7) in the abstract, there is no mention of any adjustment for confounding
8) following my point 4, if there are confounders to be adjusted for, then how were absolute risks
derived (as these should be conditional on covariate values, or averaged across the distribution of
covariate values)?
9) the list of confounders does not seem particularly comprehensive to me. for example, the gestational
age at delivery of the baby is not included, and nor is smoking habits before pregnancy, or the amount
of smoking (if any) during pregnancy. complications during pregnancy like pre-eclampsia do not seem to
be included, or adverse outcomes like small for gestational age are not included. surely these will also
be important?
i hope these comments are useful to the bmj and the authors going forward.
best wishes, richard riley


<|EndOfText|>


i thank the authors for further addressing my comments, and for the detailed and
considerate response.
only a few minor things remain
1) the authors says increased risks in the manuscript, but technically it should be increased
rates
2) the abstract implies an interaction (subgroup difference) by saying: “the increased risks
were more pronounced among offspring of diabetic mothers with comorbid cvd (hr: 1.73,
95% ci: 1.36 to 2.20) or diabetic complications (hr: 1.60, 95% ci: 1.25 to 2.05).” – yet
the actual difference or interaction is not reported. same issue occurs in the main results
and interaction results should be added to tables 2 and 3
3) what this study adds: “preventing, screening and treating diabetes in women of
childbearing age is important not only for improving health of the women but also for
reducing long-term cvd risks in their offspring” – this is assuming the relationship is causal,
so needs to say as such. causality has not been proved by this study.
4) “our study provides first-line evidence …” – what does the word ‘first-line’ mean? suggest
remove.
5) 0.6962 is the reported p-value for non-proportion hazards – 2 decimal places will suffice
with best wishes, richard riley


<|EndOfText|>

thank you for the opportunity to review this interesting piece of work. the topic is clearly important and
the authors have undertaken a considerable study. i have reviewed this from a statistical perspective,
and have some comments for improvement and clarification
1) the analysis methods (propensity score matching) are seemingly well done, with lots of confounders
adjusted for in table s1, although residual confounding remains a concern. i wonder, how missing data
(e.g. in the confounders) was handled (e.g. multiple imputation?), as i expect there to be missing data
for some covariates for some patients.
2) i did not see that hospital (facility) was adjusted for directly. why was this? the authors may have
covered this by other factors such as region, number of beds, etc anyway, but i would like to know if it
would have made any difference. perhaps this is what is meant by the random-effects model to adjust
for differences in facility and practice patterns?
3) the main the outcome is in-hospital mortality by 7 days. but, is there a competing event of discharge
before 7 days? e.g. it says: “in the itt analysis over 7-day followup, the absolute rate of death per
100 person-days was 1.7 for haloperidol initiators and 1.1 for atypical antipsychotic initiators.” these are
in-hospital deaths; but could there be people who leave before 7 days?
indeed, the authors exclude 1688 with an in-hospital stay of 3 days (fig s1). but what about those who
left between 3 and 7 days? if they are censored at discharge (which it seems was the case), then the
%s relate to an artificial world where people can only ever die in hospital. (%s will be too large). rather,
competing risk methods would be required.1 this will also effect figure 1.
4) i think the authors should examine statistically the proportional hazards assumption, by including an
interaction with time, rather than just plotting the log(log) plot and visually summarising it. this holds
for the 7 day analysis, as well as the 30 day analysis. it seems that the hr is coming down over time,
and this needs better explanation and reporting. for example, what is the change in the hr for each day
increase? this could be plotted, and the hr over time given properly, rather than is done in table 2,
where separate analyses are done for each time-point. i expect this will make the hr at later days, say
25 to 30 days, not significant anymore.

5) more details on how the ps was created are needed. for example, was a logistic regression model
used?
6) figure 2, subgroup comparisons. we need the authors to quantify the actual difference between the
groups (not just present each group separately), and give us a ci and p-value for the difference. in
particular, is the difference between 2 days and 1 days exposure beyond chance? the cis overlap
considerably, so potentially not. same for icu versus medical ward. i also do not like categorisation of
variables, like age at 75 and 85, and cci at 4. this loses power, say to keeping continuous, and
potentially considering non-linear relationships.
7) figure 1 (ignoring the competing risks issue) suggests that there is a statistically significant difference
between the groups, but in absolute terms this is very small. the baseline hazard is low, and the study
is large, and thus we could small p-values, even when the effect (hr = 1.5) is not necessarily important
clinically. would the authors respond to this please?
8) is the sample generalizable? lots of patients were excluded, e.g. authors excluded 17,434 patients
that used an antipsychotic on the first and second day of admission. i find this a considerable omission,
and am struggling to understand the justification. it limits the relevant population to which the results
here apply.
i am pleased to see that the authors note that “residual confounding cannot be completely excluded as a
possible alternative explanation despite careful study design and adjustment for a wide range of
potential confounders.” in summary, i hope my comments help the authors going forward.
best wishes, richard riley


<|EndOfText|>

thank you for asking me to review this interesting paper for potential publication in the bmj
the topic is clearly of huge interest, and the researchers should be congratulated for having
implemented a trial so quickly in this current situation. clearly, in the challenging situation, it may not
be possible to attain as high standards as we would otherwise like. the researchers do discuss this in
their discussion. for example, the open-label, as opposed to double-blind design, may introduce biased
investigator-determined assessments. however this trial may provide some initial evidence of the pros
and cons of hcq, to support further research, even if not definitive to resolve the debate.
yet, having reviewed this from a statistical perspective, i do have some major concerns and crucial
comments for the authors and bmj to consider going forward

1) in particular, i struggle to follow why the paper places strong emphasis on the single significant result
in favour of hcq, which is a secondary outcome (symptoms) and is only significant after adjustment for
another variable in a post-hoc analysis (and reducing the sample size to 28 patients! see comment 4
below). on reading the paper, my strong impression was that there was no clear evidence to support
the use of hcq based on the results presented. furthermore, many confidence intervals (cis) are quite
wide, and so it could be argued that further research is still needed to ascertain the effect of hcq more
precisely. this should be the message, but as it stands, the casual reader (journalist, politician, etc) will
look at the abstract and immediately think that hcq does benefit patients as it reduces symptoms, even
if not improving viral response. i do not agree based on this evidence.
2) going forward, i urge the authors and the bmj to be very careful about any promotion of this
post-hoc analysis. it should be removed. i do not agree with the recommendation that “clinicians might
consider hydroxychloroquine treatment in symptomatic patients with elevated crp and/or lymphopenia
because hydroxychloroquine might prevent disease progression, particularly in patients at higher risk” –
where does this recommendation come from based on the results? there is no evidence that it might
prevent disease progression.
3) it is concerning that the trial was stopped by the idmc after the interim analysis of a secondary
outcome (symptoms) showed “good efficacy” – yet later we see that this is only significant in post-hoc
analysis after adjustment for confounding variables and removing to 28 patients. this again raises
concerns about the quality of this trial, and the potential for prior beliefs about the benefit of hcq to be
influencing the decision to stop the trial and write the article with the focus on symptoms improvement.
surely the hard outcome is the viral response.
4) indeed, i do not follow the rationale for the symptoms analysis adjusting for the ‘confounding effects
of anti-viral agents’. as this is a randomised trial, the sudden adjustment in a post-hoc analysis rings
alarms bells to me, especially when this is the only result that is significant and being promoted. was
this analysis pre-specified? no. why not also adjust for other prognostic variables? why does this
analysis truly adjust for confounding? it raises too many questions to have the credibility that the media
and others will take from it.
upon deeper reading, more alarm bells ring when i see that the significant results are based on a tiny
subset of patients!! see figure 3 – there are just 28 patients included, with just 9 ‘events’. hence why
the 95% ci goes from 1.1 to 70! this selective and highly speculative analysis must be removed.
5) the main outcome is a ‘negative conversion rate’. what does this mean exactly? i struggled in the
whole paper to truly follow this. do the authors mean that the patient becomes covid19 negative?
also, is this truly a dichotomous measure? is it not measured on some continuous scale (viral
response?) – and if so, why is it not analysed on a continuous scale. ““the overall negative conversion
rate was estimated and compared by analyzing time to virus nucleic acid negativity” – how was
negativity defined? isn’t this a continuous measurement?
i see that kaplan-meier curves were used to obtain the conversion proportion over time. does this
mean that some patients were censored (i.e. not followed for 28 days or until conversion)? if so, is such
censoring non-informative? for example, people who left hospital (but were still covid19 positive) at 7
days may be different than people who remained in hospital but only were followed for 7 days until end
of trial. please clarify this.
6) most patients have mild to moderate covid19 symptoms, and no patients in the trial died. therefore,
it seems that this sample does not reflect very well the broader set of patients, in particular those that
have severe symptoms and become very poorly. indeed, the severe patients are those for which the
drug is of most interest. with this in mind, i think the title of the paper should be very clear that this is
in a mild/moderate symptom population – perhaps even a primary care population (which is still very
relevant to the bmj of course!). it is unfortunate that the inclusion criteria is not more explicit in this, as
ultimately a few more severe cases are included. perhaps change the title to: “hydroxychloroquine in

patients with covid-19 expressing mainly mild to moderate symptoms: an openlabel, randomized,
controlled trial”
7) aside from the aforementioned post-hoc analysis of symptom severity, there are many other post-hoc
subgroup analyses which ether need to be undertaken properly or removed. in particular, many use
arbitrary cut-points (e.g. age 45) to dichotomise continuous variables. such categorisation simply loses
power and is biologically implausible. better to examine on the continuous scale, and ideally with
non-linear trends. nevertheless, as the paper was not designed and not power to examine subgroup
effects, i question whether they should be included at all in this piece of research. at best a brief
mention and inclusion in a supplementary material.
8) many analyses do not have all the patients included. this is a major concern. for example, figure 4
has 48 patients included! it seems that many analyses are dropping patients for whom outcomes were
not measured. we really cannot trust the randomisation anymore in such selected subsets of data. this
all points to just focusing on the main (primary ) outcome and not anything else.
9) there are two figure 4s presented, one with 48 patients and one with 38 patients.
10) “the overall 28-day negative conversion rate was not different between soc – better to say they
were similar, as there is an observed difference. check elsewhere for such language too.
11) crp analysis should be the final score adjusted for the baseline score (i.e. not a change score
analysis), for the reasons outlined in vickers et al.
https://www.ncbi.nlm.nih.gov/pmc/articles/pmc1121605/
- same issue also holds for lymphocyte count
12) “which also led to more rapid recovery of lymphopenia, albeit no statistical significance.” – another
example of over-stating findings and this should not be given so much prominence.
13) “adverse events were significantly increased in hcq recipients but no apparently increase of serious
adverse events” – yet i think there were 2 serious in the hcq group and none in the other group. i
would remove the second part of this sentence (after the ‘but’)
14) i would not use the term ‘wonder drug’ in the introduction, as too emotive and may be taken in the
wrong manner.
15) abstract (and what this study adds) needs to make it clear those most people in the trial had mild to
moderate symptoms
16) ‘the family-wise type-i error’ – what does family-wise mean?
17) is there any evidence that hazard ratios may not be a constant over time
18) “the overall rate of symptoms alleviation within 28-day was not different between patients with soc
with (59.9%, 95%ci, 45.0% to 75.3%) and without hcq (66.6%, 95%ci, 39.5% to 90.9%).” – we also
need to have the difference in % presented and the ci for the difference
19) figure 2: focus should be on interaction estimates (differences in effect for subgroups) not (just) the
subgroup results themselves. however, see earlier comment about subgroup analyses. also fig 2 label
says that km curves are presented, but clearly these are hr estimates
in summary, whilst recongising the timely nature of this trial and the difficulty researchers faced when it
was undertaken in challenging circumstances, there are substantial concerns about the quality and
reporting and interpretation of this trial as it stands. if the bmj remains interested, i strongly

recommend it needs major revision, and that is should focus mainly (only) on the primary outcome, that
participants had mild/moderate symptoms, and removing the aforementioned ad-hoc analyses that are
inappropriate and potentially dangerously misleading. even then, we need greater clarity on how the
primary outcome is defined and measured, and how censored observations were handled.
i truly hope this review helps the bmj and the authors going forward.
best wishes, richard riley


<|EndOfText|>

the authors have responded to a wealth of comments from 7 reviewers. this remains an
important article for disseminating the current limitations with ai studies, and to push back
against the (often not justified) hype that we currently experience. it is a large
undertaking. i think the authors need to do more to address some specific points still.
some responses or corrections are too brief, which probably stems from the wealth of
comments receive from reviewers, but also feels somewhat rushed at times. for example,
some replies to reviewers are clear and adequate, but there is not a corresponding revision
to the article.
major comments
• the authors have not addressed our suggestion to include a concise summary table in the
main paper, of included observational studies summarizing their objectives, clinical
context, , etc. they do have bullet points of the study characteristics now in the supp
material but suggest it would be too large as a table, and so retain in supp material. i
recommend that a simpler version could still be included in the main article and encourage
the authors to reconsider this.
• i asked the authors to clarify what exactly they mean by deep learning methods in this
paper. they only added one short sentence to address this comment “the volume of
published research on deep learning in medical imaging, a branch of artificial intelligence

(ai) in which the algorithm learns for itself which features of the image are important for
classification is rapidly growing” – however, this is about as vague as the actual phrase
“deep learning”. what do the authors mean when they say the algorithm learns for itself.
how? what methods does the ai actually implement. there is always some programming
behind the scenes for the ai to start the process of learning and optimising the algorithm –
so what is the approach (method) utilised in the programming? at least give some common
examples. the typical bmj reader, or indeed any reader, needs to know the context far
better here.
• i also asked the authors to provide a box with some examples to illustrate the type of
application and, again, methods that are the focus of this review. however, the authors
have not done this either, simply saying in their response to me that ““in plain language,
this means the algorithm learns for itself the features of an image that are important for
classification rather than being told by humans which features to use” – i hope they can
reconsider this
• i asked the authors to clarify in the abstract the type of studies and outcomes of interest.
they added the following sentence, which i struggle to follow: “there was no limit placed
on the aim or specific outcome measures used in these studies (absolute risk prediction
[probability of disease] or classification [disease or not]).” – i struggle because the authors
say there is no limit placed, but then restrict to risk prediction or classification. so the
focus is on the latter types of studies? if so, i think the authors should just say they
included studies where the aim was to use medical imaging for predicting absolute risk of
existing disease or classification into groups (e.g. disease or non-disease). emphasise the
diagnostic setting again for clarity.
• i asked about why the item within tripod for predictor variables were not considered,
and the authors respond clearly that “it is true that deep learning algorithms can consider
multiple predictors. however, in the cases we assessed, the only predictors (almost
exclusively) were the individual pixels of the image. that is to say the algorithm did not
also receive information on for example the patient age, gender, medical history etc.” –
this information has not been clarified in the article however.
• i asked the authors to better define real-time clinical environment. their response is
clear: “we defined a real-world clinical environment as a situation in which the algorithm
was embedded into an active clinical pathway. for example, instead of an algorithm being
fed thousands of chest x-rays from a database, in a real-world implementation it would
exist within the reporting software used by radiologists and be acting or supporting the
radiologists in real-time.” – again, this information has not been added to the actual
revised article.
• the box of terms is a useful addition at the end of the methods, thank you, although
currently is not a box as such, but a list of points. also, the explanation for bootstrapping is
a bit vague (at least say each sample is the same size as the mode development dataset)
and there is a spelling mistake “… but relies on ransom sampling with replacement” –
change ransom to random.
• in response to reviewer 4, the authors give adequate responses about queries to
appendix 3 and appendix 5 – but again not always has the text been changed or clarified
in the actual revision.
• the authors criticise the lack of prospective non-rct studies – why so critical? why does
data collection need to be prospective? if the aim is diagnosis, then a cross-sectional study
may be appropriate. or if an existing dataset is available, of high quality, then why can’t it
be used to develop and validate a deep learning algorithm may be appropriate.
prospective may be important to inform the reference standard (true disease status) and
impact on patient outcomes, and to compare groups in a trial. so, i think it needs to be
made clear that prospective studies are required to make an unbiased comparison on
predictions or classifications based on a deep learning method or clinical judgement (and
not necessarily to actually develop the model in the first place)
• i think the title should make it clear that the setting is diagnostic
minor comments

• “three quarters of studies stated in their abstract that the ai performance was at least
comparable to (or better than) clinicians” – suggest change to “three quarters of studies
stated in their abstract that the ai performance was comparable to, or better than,
clinicians’ performance”
• conclusions state that the studies ‘demonstrate substantive bias’ – i think we can’t say
whether they are biased (we don’t know the truth), just that they are at high risk of bias
• “the authors found that accuracy of cataract diagnosis and treatment recommendation
were 87% and 71% respectively” – define accuracy here. why not split into sensitivity and
specificity?
i hope these comments are helpful for the authors going forward.
best wishes, richard riley
