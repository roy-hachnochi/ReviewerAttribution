the revision is excellent, and i thanks the authors for answering my comments in detail and revising the
paper accordingly.
i have some further, minor comments
1) why was a 90% ci used when looking at non-inferiority (why not 95%?) – page 16. is this a
standard level used in the non-inferiority field (i am not familiar with this field, but it is important to
clarify here as i don’t see why 95% is not used)
2) “the estimates and 95% confidence intervals at our primary time point of interest were all below our
original sample size estimate (or=0.5)” – do you mean below or above (or estimates are above 0.5)?
please re-write to be clearer
best wishes, richard riley

<|EndOfText|>

the authors have responded very clearly to my comments, and made suitable revision to
the paper. i thank them for their clear responses throughout this process. my only
remaining comment is that throughout the paper the authors discuss results in terms of
risk and lowering risk of t2d. however, i think they mean rates, not risk. please consider
this throughout. the word ‘risk’ should be accompanied by a particular time-point (risk by
what time), whereas rate refers to the whole follow-up.
some examples of this are:

1) “after adjusting for lifestyle and dietary risk factors for diabetes, participants in the
highest quintile of total whole grain consumption had 29% (95% ci: 26% to 33%) lower
risk of t2d comparing with those in the lowest quintile”
- i think the authors mean lower rate and not lower risk
2) “spline regression showed a nonlinear dose-response relationship between total whole
grain intake and t2d risk in that the risk reduction slightly plateaued over two servings/d
(p for curvature < 0.0001)
- again, rate reduction rather than risk reduction?
same applies in many other places too
i thank the authors again.
best wishes, richard riley


<|EndOfText|>


the authors have responded well to the comments, and i still believe this is a very good
research study. but some response do not go far enough in my opinion.
1. the decision to report one of the pre-defined primary outcomes as a secondary
outcome is still troublesome, even though the authors note this in the discussion. why not
simply report 2 primary outcomes? the problem is that the primary outcome moved to
become secondary is actually not statistically significant, whilst the retained primary
outcome is. this looks bad, because, it does appear to be a selective decision. however,
both effect sizes are in the same direction. in many ways, they do back each other up,
and why raise any complaints in this regard? just stick with 2 primary outcomes.
2. i find it incredibly reassuring that the new meta-analysis that i requested shows
consistent findings across centres. i strongly recommend this is given in the appendix at
least, and am surprised why the authors do not see why this add credence to their
findings. this analysis also accounts for clustering of patients within centres, which their
main analysis does not.
3. i asked them to consider adjustment for baseline imbalance (e.g. as a sensitivity
analysis), but they have not done this yet. “as this is a randomized controlled trial, our
proposal is not to adjust the
analysis. if the editor thinks this is important to report, we are more than willing to do
so.” i understand the view that this is an rct, and hence randomisation should expect to
balance groups and so estimates are unbiased; but clearly there is slight imbalance.
hence the question is valid: are conclusions robust. thus i do ask again for this in a
sensitivity analysis. as for the multi-centre analysis above, if findings barely change, then
this add credence to the finding. the focus should not be on the change in p-value here,
but rather any chance in the magnitude of effect (and its clinical relevance).
4. the authors changed their analysis methods from the first submission from a complete
case analysis to one that uses multiple imputation to ensure an itt analysis i think. what
assumptions is this imputation analysis making? what if baseline covariates were
themselves missing, from which the outcome is being computed? how do results compare
to the complete case analysis from before? it would be nice to know in the paper.
also, in the discussion they say: “missing data were balanced between groups averting
any bias in outcome.” – but does the multiple imputation not exceed this statement now?
6) re-write: “of the women assigned to atosiban, cesarean delivery was performed in
60% of the women and 55% in the fenoterol group” – it reads like the fenoterol group is a
subset of those in the atosiban group.
7) at start of discussion it says: “although the difference was not statistically significant, it
is likely to be true, as the number of women in cephalic position after the procedure was
significantly decreased after atosiban.” please change to ‘it is highly plausible’ rather than
likely to be true.
i hope this helps the authors improve the article going forward.
best wishes, professor riley, bmj statistics editor.


<|EndOfText|>

thank you for the opportunity to review this interesting work. ipd meta-analyses are not an easy
undertaking, and it is encouraging to see the authors have obtained and synthesised ipd from over
100,000 patients and 15 studies. this is commendable. the authors conclude that their data indicate
that genetic risk and dietary fat quality are each associated with t2d incidence. but there is no evidence
to support tailoring dietary fat quality recommendations to individual t2d genetic risk (i.e. because there
was no interaction).
i would like to recommend the authors revise their article after addressing the following comments and
concerns.
1) the genetic risk score was created (grs), though i am still not clear about exactly how.
- “the grs was generated by summing the number of risk alleles at each genetic variant weighted by
the respective allelic effect sizes on risk of t2d (log transformed odds ratio estimates) from the largest
published genome-wide association meta-analysis” – this is not clear to me; why not simply adjust for
these genetic factors in the analysis, rather than create an (unvalidated) risk score. at least in
sensitivity analysis, i would also suggest this is investigated.
2) also, i think the weighting approach needs further explanation, so that it could be replicated by
others. perhaps in a supplementary material? this is an ipd meta-analysis, with clustering by study (and
potential heterogeneity in effects across studies). how was heterogeneity across studies accounted for
when creating the score? or perhaps as the weights are derived from external data, this does not
matter?

3) was imputation of missing data done in each study separately (to correctly preserve clustering and
heterogeneity)? i think it was, but just make this more explicit.
4) in the main analysis, grs and fat are assumed to have linear effects, but this may not be the case.
indeed, their interaction may also be non-linear. a multivariate meta-analysis of splines could be used
to investigate this, within the 2-stage ipd meta-analysis framework [1, 2] the authors must consider
non-linear trends in their revision, as otherwise perhaps they are missing non-linear associations.
5) the authors use a fixed effect meta-analysis, which assumed effects are the same across studies.
given the heterogeneity in observational studies, i do not agree with this assumption. they are from
different countries, using different methods of measurement, and follow up periods, etc. at the very
least, we need some assurance that a random effects approach leads to comparable findings
6) i-squared is a very poor measure of heterogeneity; it is misleading.[3] simply estimate and report
the heterogeneity itself (between-study variance).
7) the meta-regression uses dichotomised variables (e.g. mean age > 55 years or < 55 years), which is
arbitrary and loses power. also such analyses are severely prone to across study confounding, so the
analyses have little credibility. i suggest they are removed or severely downplayed. if included, at least
also consider age as continuous.
8) what is the ‘joint meta-analysis method’? a multivariate meta-analysis? please explain and justify.
9) some details on the length of time to obtain and clean ipd was be welcome.
10) the authors do not investigate non-proportional hazards; that is, whether the hazard ratio of
interest is a constant over time. this must be addressed.
11) often the authors use the term relative risk, but do they mean hazard ratio?
12) peter’s test – i thought this was specific to odds ratios, requiring the 2 by 2 tables? perhaps i am
wrong. debray test is specific to hazard ratios.[4] (this is perhaps a minor point)
13) authors say that missing studies will not be due to reporting bias, as they are missing in areas of
high significance. but i disagree, looking at sup fig 3, many of the ‘missing’ studies would fall in the
white non-significant regions. please can they check this again.
14) the interaction analyses are poorly reported, both in the results (just p-values given) and in the
table 3, where p-values are given and just the direction of interaction effect is reported. what are the
summary interaction estimates and cis?
15) “meta-regressions analyses were conducted to examine the impact of continuous mean age and bmi
of each cohort on the interaction between fat” – but why look at mean age, when the authors have the
ipd and so can assess the interaction between individual age and the grs-fat interaction? the
meta-regression is prone to confounding and ecological bias, whereas the within-study information is
less prone to confounding.[5, 6] as mentioned, i am not sure the meta-regression analyses add
important information.
16) the authors excluded participants who reported implausible baseline energy intake (< 500 or >
4,500 kcal/d), and participants who had missing genetic or dietary fat quality data. can the authors
clarify why they were excluded, rather than included as part of the multiple imputation analysis?

17) “with the available sample size, we had 88% statistical power to detect an interaction effect size of
1.04 hr on t2d risk (type 1 error rate set at 0.05)” – do the authors have a reference for this
calculation? power calculations are awkward in this setting, given the clustering and potential for
heterogeneity. it is even more important to report the interaction estimates and cis in table 3, as
otherwise the cis may be wide, and the conclusion about a lack of interaction may actually be due to
low power. indeed, can we also see forest plots of the interaction estimates please.
in summary, there are quite a few points for clarification and/or updating. i sincerely hope the authors
find my comments helpful and that they lead to improvements. i re-emphasise that the bmj should
recognise the considerable achievement by the authors to have obtained this ipd and undertaken the
meta-analysis, and provide a comprehensive and well-written report of their work. i am confident the
authors can address the comments raised, and i look forward to reading the revision.
best wishes, richard riley
reference list
1.
gasparrini a, armstrong b, kenward mg. multivariate meta-analysis for non-linear and other
multi-parameter associations. stat med 2012; 31: 3821-3839.
2.
gasparrini a, armstrong b. multivariate meta-analysis: a method to summarize non-linear
associations. stat med 2011; 30: 2504-2506.
3.
rucker g, schwarzer g, carpenter jr, schumacher m. undue reliance on i(2) in assessing
heterogeneity may mislead. bmc med res methodol 2008; 8: 79.
4.
debray tpa, moons kgm, riley rd. detecting small-study effects and funnel plot asymmetry in
meta-analysis of survival data: a comparison of new and existing tests. res synth methods 2018; 9:
41-50.
5.
berlin ja, santanna j, schmid ch, szczech la, feldman hi. individual patient- versus
group-level data meta-regressions for the investigation of treatment effect modifiers: ecological bias
rears its ugly head. stat med 2002; 21: 371-387.
6.
schmid ch, stark pc, berlin ja, landais p, lau j. meta-regression detected associations
between heterogeneous treatment effects and study-level, but not patient-level, factors. j clin epidemiol
2004; 57: 683-697.


<|EndOfText|>

this is a very good revision and i am happy with their response to my statistical comments.
they do adjust for baseline and have, most importantly, re-worded they interpretation of the
results.
two minor issues remain:
- it is not clear to me why the quartile baseline value of the fao score, rather than the
continuous baseline fao score itself. i doubt this makes much difference, but it seems suboptimal. perhaps the authors can simpy confirm in the paper that, in their continuous outcome
analyses using sas proc mixed, that the use of a continuous baseline score did not change
findings?
- also, the authors say in the methods that: “this approach which used all available
assessments including baseline as outcomes …” – i don’t think baseline was an outcome, but an
adjustment factor (covariate), as later you say: “predictors in the model included baseline faos
quartile as a factor …”. you can’t have the baseline as both an outcome and an adjustment
factor.
- in the discussion the authors say that confounding is unlikely to be a problem and 'baseline
group comparisons essentially demonstrated equivalence on such factors' - yet table 1 shows
there is slight imbalance, with a higher grade 2 injury % in the physio group. i think the
authors should be clearer about this and tone down their writing here.

<|EndOfText|>

the response to reviewers is very comprehensive, and i am happy with their response and the
sensitivity analyses reported show consistent findings.
however, i am not happy with the actual revision because they do not refer to any of the sensitivity
analyses that we asked them to do, such as looking at continuous trends and multiple imputations and
including adjustment factors individually (rather than as a score).
the sensitivity analyses need a separate section in the methods and the results, and detailed results
can be given in the online material. given that the authors show us, in their response to comments,
that the sensitivity analyses show consistent conclusions to those originally shown, this is an important
message and will improve the robustness of their findings for the bmj reader.
also, tables 2 and 3 do not actually define the numbers within the tables as rrs
page 10: ‘in in’ – delete repeated word
i’m sure the author can address these final issues.
best wishes, richard riley

<|EndOfText|>

i thank the authors for their further revision. i do think this trial has important value.

it is good that the authors re-ran their analyses, and that the bayesian and frequentist results now
agree more closely for the enterotomies analysis (although a little concerning that the p-value originally
presented to us was 0.99 and now it is 0.49).
the article is much improved by the additional sentences and changes in wording. my only remaining,
minor comment is that the abstract says: “both robotic and laparoscopic ventral hernia repair have
similar 90-day postoperative hospital stays.” – is this a fair conclusion given the width of the ci is 0.37
to 2.19, and so is very wide? better to say there was no evidence of a difference in 90-day hospital
stays. please consider here and elsewhere (eg discussion, what this study adds)
with best wishes, prof richard riley


<|EndOfText|>

the response to my comments is excellent, and i am happy with the revision. however, i must
recommend that the titles for figures 2 and 3 be improved. they must come with a clear warning (in
the title or footnote) that the uncertainty of each ratio is not displayed, due to the majority of studies
not providing this, and that pooling results is not justified due to the heterogeneity and poor reporting.
further, they do not state that each circle represents a study, and how the size of the circle was
calculated.

<|EndOfText|>

i thank the authors for addressing my comments in detail. i am pleased that the multiple imputation
approach has now been done and shown to not change the conclusions. it would be even better to have
this as the main analysis, but it is at least including in the supplementary material. similarly, the
authors show in their response that a cox modelling approach produces similar results to the poisson
approach. again, it is a shame that these results do not become the main analyses. following this, i
have a major comment
1) in their response, the irr estimates after multiple imputation are slightly closer to 1 than when using
the missing indicator method. also, the irr estimates closer to 1 when using the cox model than when
using the poisson model (e.g. gestational diabetes 1.19 cox compared to 1.25 poisson). this raises the
big question about whether the estimated irr values would be even closer to 1 had both cox and
multiple imputation been used together, as we recommended. this seems to be a major issue
remaining. even if the statistical significance remains in favour of an association, if the estimates move
even closer to 1, then we have more concern that any residual confounding would be able remove the
association altogether.
a related issue is that the associations also come down slightly when adjusting for non-linear effects of
continuous confounders. see table s11, where the effect of gestational diabetes is now 1.20 compared
to 1.25 before. again, we see a picture of more in-depth analyses leading to more attenuated effect
sizes between diabetes and cvd risk.
2) another major comment: those that died before cvd were censored – but this then makes absolute
risks and rates inflated to an artificial population that can never die (and so must at some point develop
cvd). the authors respond to this point by saying that the irr from a fine and gray model (which

accounts for competing risks) is similar to their result. but, this does not address the issue that the
absolute risk estimates produced will be too high, because people who died cannot actually developed
cvd ever, but the authors rather censor such individuals, which assume cvd can still occur after the
censoring point. this is an important issue: the absolute risks must surely be based on the competing
risks analysis approach.unless the number of deaths is so few that is barely has an impact.
minor comments
1) the abstract concludes with a causal statement: “prevention, screening and treatment of diabetes in
women of childbearing age may help reduce cvd risk in the next generation”. it might be more explicit
to say “assuming diabetes in women of childbearing age is causal for increased cvd risk in their
offspring, prevention, screening and treatment of diabetes in such women may help reduce cvd risk in
the next generation”.
2) the cox model results (shown in the response) are not even included in a supplementary material in
the paper; i think this should be done for completeness (with multiple imputation), and a comment
made in the paper. see major comment above.
3) the plot examining a constant irr (hazard ratio) in the response to reviewers should be included in
the supplementary material, and in the main text should be used as a justification of why proportional
hazards was assumed over time. that being said, the lines do appear to be converging slightly – did the
authors actually quantify statistically if there is evidence of an interaction between the irr and time?
4) in their response the authors note that: “we agree with the reviewer that gestational age at delivery,
pre-eclampsia, and small for gestational age are important covariates. previous studies have reported
that maternal diabetes is associated with increased risk of preterm birth, pre-eclampsia, and
macrosomia.23,25,26 therefore, these factors could be consequences of maternal diabetes and may lie
on the pathway from maternal diabetes during pregnancy and early-onset cvd in offspring. as expected,
additional adjustment for maternal pre-eclampsia, preterm birth, and small for gestational age yielded
slightly decreased associations compared to the main analysis (table r4.9-2). therefore, we did not
adjust for these factors in the main analysis, preferring to consider them as potential mediators.”
- this is an interesting discussion. should this rather be considered in the results? currently only
addressed in the response to reviewers.
4) need to explain in the abstract where the absolute risks come from (i.e. averaged across all
individuals and adjustment factors? the word standardized is hard to interpret, and often is used to refer
to dividing by standard deviations )
with best wishes, richard riley


<|EndOfText|>

thank you for the opportunity to review this article submitted to the bmj. it was
a very interesting and well-written paper. generally i am cautious about

nutritional epidemiology studies, due to the big issue of diet changing over time,
missing confounders, and the exact definition of diet. for example, here, what
type of meat do these findings relate to? and how much meat is actually good (or
bad)? i find it hard to translate the findings.
below, there are some further comments to make, which i hope the authors can
use to improve or refine their article further, going forward.
1) the authors do multiple imputation in a sensitivity analysis, rather than the
main analysis; ideally it should be the main analysis, but at least they confirm it
does not change findings.
2) “low meat eaters and fish eaters also appeared to have a higher risk of total
stroke compared with regular meat eaters (p for heterogeneity between all diet
groups=0.03), but the difference was not statistically significant.” – contradiction
as the p-value was small? moreover, i would suggest caution about using the
term statistically significant, and rather focus on the magnitude of effect and
importance of values covered by the 95% ci. so, rather than tests of differences
across groups, focus on the estimated differences and cis.
as an example of how the statements depend on arbitrary statistical significant
see this: “when vegetarians and vegans were assessed separately, vegans had a
borderline lower risk of ihd (0.78, 0.61-1.00) and a higher risk of total stroke
(1.44, 1.01-2.05) compared with regular meat eaters” – the use of the word
borderline in the first but not the second, even though the cis are both basically
ending at 1.00
3) the authors convert hazard ratios to absolute risk at 10 years, though i
wonder whether this should be made conditional on covariate values (because
the hazard ratios are conditional on adjustment factors). so the absolute risk at
10 years is for who? perhaps show a patient with average covariate values?
also, how does this account for competing risks? for example, due to deaths to
cancer? is this a real world interpretation of the change in risk?
4) my main concern is that i’m not comfortable with the dichotomisation of meat
eaters into <50g versus > 50g per day. maybe this is due to the information
about meat eating that was available. ideally, we would want this to be examined
as continuous variable, and non-linear trends examined. but, perhaps
participants did not know the amount of meat they eat each week anyway? we
need clarity on this, as this dichotomisation could be driving the key findings. it is
well-rehearsed in the statistical literature that the dichotomisation of continuous
variables only serves to lose information and is rarely sensible; for example, why
should a meat eater of 49g be different to one with 51g per day? and what if
someone eats 49g one week and then 51g the week after?
5) how may measurement error (in terms of the exact amount of meat eaten per
week) have influenced these findings? bear in mind that the error may be larger
in those that eat more meat, and therefore the error is not consistent across the
range of values.
6) another concern, or at least surprise, is that there is an associated benefit for
ihd but an associated harm for stroke. i would expect this to be in the same
direction, intuitively. therefore, it makes me more worried about residual
confounding.
7) a related point - the findings are not dramatic: e.g. for total stroke, the low
meat group has a hr of 1.17 (1.00 to 1.38), so no overly strong evidence of an

association statistically (would be vulnerable to more confounders, for example)
and not especially large in magnitude anyway. this is reflected by small absolute
numbers that would additionally get a stroke by 10 years if they are meat.
further, would the dichotomisation of meat at 50g and the non-proportional
hazards issue change things?
8) figure 1: important to note this is showing the rate ratio for one group
compared to the regular meat eater groups.
9) when interpreting hrs, i think lower risks of ihd should be changed to lower
rates. (and conversely higher risks to higher rates for stroke). if you want to say
risk then i think you should give the time-point that the risk relates to.
10) “the associations for ihd were partially attenuated after adjustment for
self-reported high blood cholesterol, high blood pressure, and diabetes, and to a
larger extent body mass index.” – this is mentioned in the abstract but without
any results, which is not helpful to the reader. moreover, why did the authors not
adjust for all these confounders to begin with?
11) conclusion of abstract: “low meat eaters, fish eaters and vegetarians had
lower risks …” – be more cautious. low meat eaters, fish eaters and vegetarians
were associated with a lower risk …
12) “we utilised repeated measurements of diet and important time-varying
confounders at follow-up to account for possible changes.” – i did not see this in
your analysis? the follow up information as obtained in 2010, some 17 years
after first recruitment, and so how were time-varying confounders actually
measured in those 17 years? i wonder if this is misleading, as we actually do not
have information (e.g. yearly) on current diet.
this is actually a major concern, because “of low meat eaters at baseline, 48%
were regular meat eaters at follow-up, and 43% were low meat eaters.” –
therefore there is much switch over during the middle period.
in summary, i have a number of concerns for the bmj to consider. these are
typical of many nutritional epidemiology studies we encounter, and they make it
hard to understand the meaning of the findings for the bmj reader. the paper is
well-written and raises interesting questions, but the grouping of meat eaters by
50g, the potential for residual confounding, and the changing of diet over time,
raises important caveats to the study findings. i truly hope my comments help
the authors going forward, as they have clearly worked very hard on their study
and project.
richard riley


<|EndOfText|>

this is clearly a large piece of work in an important clinical area. i have reviewed this from a statistical perspective, and
have some important concerns and suggestions:
1. the abstract and whole paper is heavily focused on statistical significance, and there is no quantification of the actual
treatment effects (with cis) in the abstract or results text to help disseminate clinical importance.
2. outcomes. when defining the outcomes in the methods section, please explain whether these are binary or continuous
in nature. for example, is acr-50 response binary or continuous?
3. for wdae, the denominator is the total exposure time, and therefore i think the authors are looking at the rate of
wdae (i.e. a hazard rate), and thus a rate ratio should be of interest for the meta-analysis (i.e. a hazard ratio).
confusingly, the authors refer to odds ratios for the wdae analyses. i don’t think this is appropriate. can the authors
clarify please why they have analysed this in terms of ors, and not hazard (rate) ratios? that is, the meta-analysis
should be done on the log hazard ratio scale i think, and not the log odds ratio scale. in other words, rather than
modelling the data as binomial, it should be poisson i think.
4. bayesian analyses are fitted, but the ‘significant’ language is used, when this is really a frequentist statistical
argument relating to a p-value and a null hypothesis. i suggest the authors remove the ‘significant’ or ‘not significant’
language throughout, and rather focus on effect sizes and cris, and if they want to talk about strength of evidence, they
can talk in probabilistic language (there was a probability of >95% that …) due to the bayesian nature of analysis.
5. ‘uninformative prior distributions were used for all parameters’ – in my experience, prior distributions are never
uninformative in a bayesian meta-analysis, especially in regard tau-squared (the between-study variance). please see
lambert et al. the authors must tell us explicitly what prior distributions were used, and perform sensitivity analyses to
the choice of priors (especially for tau); can the authors use any of the empirically-based prior distributions of turner or
rhodes?
p. c. lambert, a. j. sutton, p. r. burton, k. r. abrams and d. r. jones, "how vague is vague? a simulation study of the
impact of the use of prior distributions in mcmc using winbugs," statistics in medicine, vol. 24, pp. 2401-2428, 2005,
2005.
k. m. rhodes, r. m. turner and j. p. higgins, "predictive distributions were developed for the extent of heterogeneity in
meta-analyses of continuous outcome data," j. clin. epidemiol., vol. 68, pp. 52-60, jan, 2015.
r. m. turner, j. davey, m. j. clarke, s. g. thompson and j. p. higgins, "predicting the extent of heterogeneity in metaanalysis, using empirical data from the cochrane database of systematic reviews," int. j. epidemiol., vol. 41, pp. 818827, jun, 2012.
6. the bayesian approach is sensible i feel, but the bayesian analysis model is not explained in enough detail. just
saying a bayesian random-effects model was used is not sufficient. for each outcome, what was the model structure?

did the authors model the binomial nature of the study data directly for binary outcomes, or assume logor estimates in
each study were approx. normal distributed? what about for the adverse events outcome, and the continuous outcome?
and what assumptions were made about the heterogeneity for each treatment contrast? was it assumed the same, or
allowed to be different? how was the correlation of multiple treatment effect estimates from the same study accounted
for? what assumptions were made about the between-study correlation of the true treatment effects? even more
crucially, were conclusions sensitive to any of these assumptions?
7. a big assumption for network meta-analysis is consistency. did the models assume consistency (exchangeability of
direct and indirect evidence)? did the authors examine whether there was inconsistency? how vulnerable are the
conclusions to this assumption? perhaps this is what the authors are referring to when discussing ‘node splitting’, but
they don’t mention the word consistency explicitly.
areti angeliki veroniki, haris s vasiliadis, julian pt higgins, and georgia salanti. evaluation of inconsistency in networks
of interventions. int. j. epidemiol. (2013) 42 (1): 332-345 doi:10.1093/ije/dys222
8. was publication bias a potential concern? in other words, was there evidence of small study effects?
9. in the results, it is clear from table 1 that the trials were heterogeneous in the length of follow-up (from 24 to 91
weeks). therefore, why is the or an appropriate summary measure? why are hazard ratios not summarised, which as
far less sensitive to the actual length of follow-up itself. hrs may remain constant over time, but ors change depending
on the time-point. see for example perneger. hence, this must be better justified
perneger tv. estimating the relative hazard by the ratio of logarithms of event-free proportions. contemp clin trials.
2008 sep;29(5):762-6. doi: 10.1016/j.cct.2008.06.002. epub 2008 jun 27.
10. how did the authors translate the pooled smd back onto the sharp vdh scale?
11. table 2 should give us more information, including the pairwise summary result and the estimate of tau-squared
(amount of heterogeneity) in each analysis
12. the treatment effects should be more clearly labelled as the summary treatment effects (or the average treatment
effects) across studies. when labelled as simply treatment effect, it implies that the treatment effect is fixed across
studies, which isn’t the case. perhaps in the methods explain that the meta-analysis results provide the estimate of the
average treatment effects. see:
riley rd, higgins jp, deeks jj. interpretation of random-effects meta-analyses. bmj 2011; 342:d549
13. table 2 uses ‘low’ to indicate poor quality, but elsewhere ‘low’ is used to signal good quality (low risk of bias). please
be consistent.
14. i would like to see a table giving the ranking of treatments, with the probability that each treatment is ‘best’. this is
naturally derived from the bayesian approach. this is better than talking about statistical signficance or whether
treatments are ‘similar’.
15. summary ors should be given to 2 d.p.s consistently
16. i can’t see the individual study results for each outcome – am i missing something?
in summary, this is clearly an immense piece of work to review and summarise the evidence identified from 150 studies.
however, there are critical areas for improvement in the translation and understanding of statistical models, choice of
effect sizes, heterogeneity in follow-up times, sensitivity to model assumptions and presentation of results. i hope my
comments helps the authors moving forward.
with best wishes, richard

<|EndOfText|>

this is a very well written article, and an interesting topic. i have reviewed this from a statistical perspective and have
some important concerns, which i hope are addressable by the authors upon revision but need care and additional
analyses.
1) was clustering of children within the same family accounted for? it seems not, and this would not be captured by
the propensity score analysis. children from the same family are likely to be similar to one another, and therefore are
not independent. the cox regression should therefore be re-done, accounting for this (it could though be noted as a
sensitivity analysis in the article) to see if conclusions remain the same.
2) usually, after a propensity score is calculated, the individuals are ‘matched’ (e.g. one in the smoking group is
matched to a non-smoker, based on their score being very close). individuals who cannot be matched are removed. it
seems the authors don’t do this, and rather include everyone and adjust for the propensity score value in categories.
this seems inferior to me than the matching approach, as the latter allows one to display the baseline characteristics
of the two groups and see that (hopefully) the groups are now more balanced. therefore (again as a sensitivity
analysis perhaps), please re-do the analysis with matching (again, accounting for clustering of children in the same
family too). without the matching, one might still have extremes of the propensity score for one group but not the
other, and therefore regression adjustment may still not be fully adjusting for the confounding (as compared to
matching).
3) it’s not clear how the population attributable fraction is derived in the fully adjusted results. please clarify.
4) children were excluded if they were not followed up for 3 years – and the authors say there were no differences
between these and those that did have 3 years follow-up. but it is not clear if these children were excluded from all
analyses, or just from the dental score analysis at 3 years. they should have been included in the survival analysis, as
that can handle drop out before 3 years please clarify.
a related issue is that the exact time of caries is not known (i think). rather, the authors only know if they appeared
by 18 months or 3 years. so i am struggling to understand why the kaplan-meier method was used, as time of event
is not known exactly. please clarify. were some children censored before 18 months? if so, how was the censoring
time calculated? or are the authors assuming censoring took place at time zero or just before time 18 months? what is
the justification? if there is no censoring, then wouldn’t a relative risk at 18 months (and then again at 3 years) be a
better summary than a km estimate (or perhaps they are equivalent) please clarify, and justify why the analysis
chosen and assumptions are appropriate in relation to the data available.

hazard ratios relate to the whole follow-up period, but here i think the authors are looking specifically at risk ratios at
3 years. therefore either the hazard ratios are actually risk ratios at 3 years, or they need to explain why the whole
time period is being accounted for. why are hazard ratios (or relative risks more precisely) not given for 18 months
also? i assume this time-point could include more patients than the 3 years one, so i’d like to see the effects at this
time also (especially as it is closer to the 4 month measurement of smoking, and thus less likely to be affected by
changes in smoking habits over time).
4) kaplan-meier %s are given in the results but these are unadjusted for confounding, thus should be removed or
clearly labelled as unadjusted.
5) the authors should note that the non-significant result for maternal smoking may be due to low power to detect a
small effect (if indeed genuine).
6) abstract says: ‘hazard ratios of second hand smoke …’ this is loose language. it is hazard ratios of dental caries for
children linked with second hand smoke versus those not linked to second hand smoke
7) page 7 tells us that the number of decayed, missing or filled teeth were 0.06+-0.48 …. what does these numbers
actually mean? is it the average number per individual with its standard error? if so, how can the lower interval value
be negative?! <0 decayed teeth? please be explicit!
8) the propensity score analysis adjusted for the propensity score divided into quartiles. this seems again unjustified,
as it loses information and the propensity score should be left on its continuous scale. please see point 3 above for a
better method also.
9) were there any missing values in the adjustment factors used in the propensity score analysis? with such
databases, there are usually missing values for some patients, but i cannot ascertain how this was handled in the
analysis. was multiple imputation used?
10) the authors say there were no differences between those included and excluded from analysis (supp table 1). i
think there are observed differences, but these are generally small. please re-word. similarly when discussing supp
table 3.
in summary, this is an important study but there are a number of important areas for re-analysis and added clarity, to
address my concerns. i hope my comments are helpful to the authors moving forward.

<|EndOfText|>

this is an important topic and the authors have produced a considerable piece of work. the evidence
retrieved and combined is immense, and they must have worked very hard. however, there are many
concerns and lack of necessary details as it stands, therefore i suggest to the bmj that it is difficult to
ascertain how robust the results are, in its present form. i hope the following comments help the
authors clarify and improve their work going forward:
1. there are no details of study quality assessment (risk of bias) in the methods. this is also only
briefly mentioned as having been done at the beginning of the results, but without details of whether
included trials were low or high risk of bias. supplementary figure 1 shows that this was actually done,
but we need this bringing into the main article and discussing please. this is a critical part of a
systematic review and meta-analysis, and has been over-looked here in favour of presenting the
modelling. we need more details and discussion of the included trials, their qualitative consistency ( in
terms of length of follow-up, populations, etc) and risk of bias.
2) i have some concerns about the analyses presented. most of all, the description of the statistical
approach to network meta-analysis is far too brief and not detailed enough. for example:
- what are the prior distributions being used in the analysis? did the authors use empirically based
priors for the between-study variance, or some ‘vague’ prior? if the latter, then meta-analyses are
often sensitive to the choice of prior, especially for tau-squared (the between-study variance). so are
the results robust to sensitble choices of the prior distributions?
- what type of model is being used in the bayesian analysis (e.g. logistic to model 2 by 2 tables, or
linear regression models with weights to model continuous effects, etc). all we are told is that a
hierarchical model was used.
- what are the assumptions being made? for example, is the between-study variance assumed to be
constant for each treatment contrast? etc
- how is the correlation between multiple effect estimates in the same study accounted for? (withinstudy correlation) how is between-study correlation accounted for?
3) a bayesian approach is used for the main network analyses, and then strangely frequentist
approaches are used for other analyses, such as the pair-wise only analyses. i found this internally

inconsistent, and the discrepancies from the full and pair-wise analyses may be due to the change in
statistical approach as much as anything else. therefore, i encourage the researchers to be consistent.
for example, why is meta-regression done in a frequentist framework using dersimonian and laird,
and not a bayesian analysis? and why are the frequentist results using the dersimonian and laird
method, when this ignores additional uncertainty in the estimate of tau-squared when presenting
confidence intervals? see [1] for better options.
4) it is also strange that the authors often refer to statistical significance of results, when the bayesian
approach rather allows direct probability statements that are more meaningful.
5) in their methods, the authors say that the or was used as the outcome measure. but then a few
lines down, they sat that pooled estimates were quantified as or or md (mean difference). how were
the ors translated back to the md scale?
6) the authors say that ors were used due to the uniform follow up lengths of the trials. but i in table
1 the follow ups seems highly variable to me – e.g angiographic is 24 hrs, 6 months, 7 months, 9
months, 12 months. why should the or be the same at 24 hours as at 12 months? undoubtedly this
will cause heterogeneity in the meta-analysis, and i would rather hazard ratios had been synthesised.
7) we need more on the data extraction phase. how did the authors calculate the ors if they were
missing in study publications? what is trials reported a hr rather than an or?
8) what is the rationale for the trial sequential analysis part? i do not follow this and suggest it is
removed. we simply want a summary of the evidence, not predictions for new trials, don’t we? this
part merely creates confusion and detracts from the main findings.
9) some of the key summary results and cis should be given in the text of the results, to quantify the
comments made
10) i very much like the forest plots and ranking probabilities. can we also have the estimated
heterogeneity on the plots (or given underneath)?
11) i suggest refraining from saying this was a ‘comprehensive’ meta-analysis, as this is very
subjective, unecessary and leaves the authors open to criticism
12) meta-regression is not recommended when there are fewer than 10 studies, yet the authors do
this (e.g. see page 82, there are 5 studies). also, meta-regression analyses are ecological analyses
and prone to confounding. therefore this should be addressed.
13) egger’s test is inappropriate for ors. use peter’s test instead. please see sterne et al. [2]
i don’t like the limit meta-analysis approach, as the asymmetry may be due genuine heterogeneity
here and not publication bias. also, given the number of tests done for asymmetry, it is perhaps not
surprising that a few are found to be significant by chance. thus, please revise this section.
14) finally, i urge the authors to shorten and hone their results section. it is very long, with no
quantitative results given alongside the text, and the text itself is rather long-winded. i suggest adding
results and sub-headings, to help the reader and improve the flow.
ref:
[1] cornell je, mulrow cd, localio r, stack cb, meibohm ar, guallar e, et al. random-effects metaanalysis of inconsistent effects: a time for change. ann intern med. 2014;160:267-70.
[2] sterne jac, a.j. s, j.p.a. i, terrin n, jones dr, lau j, et al. recommendations for examining and
interpreting funnel plot asymmetry in meta-analyses of randomised controlled trials. bmj.
2011;342:d4002.


<|EndOfText|>


the paper is much improved, and i continue to believe it is a well-written and
important article, showing the large number of models available and qualitatively
summarising their performance (and highlighting important omissions). the
paper now includes probast quality assessment and optimism-adjustment
results (i.e. adjusting for overfitting) for each model, and these are very helpful
additions. however, there are still a number of important area for improvement,
in my opinion, and i truly hope the following comments help the authors and the
bmj going forward.
major comments:
1) optimism-adjusted c statistics are now provided, which is a big improvement
and important addition. however, crucially did the developed models apply any
penalisation or shrinkage techniques during model development, or after internal
validation, to reduce overfitting concerns? in other words, was optimism in
calibration assessed in internal validation (e.g. through bootstrapping) and then
adjusted for when producing the final model equations?
the authors say “in conclusion, we considered that the authors examined for
overfitting and adjusted for optimism if they presented a c-statistic after
bootstrapping, cross-validation or non-random split”. however, i do not think this
is a sensible stance. just because a study provides an optimism-adjusted
c-statistic does not mean that they also adjusted the developed model’s
parameter estimates for optimism (e.g. by using a uniform shrinkage factor).
2) the authors still do not provide meta-analysis of any performance statistic.
this is a shame, as a reader would benefit from forest plots and summary results
(including average performance, and also examining heterogeneity in
performance [1]). for example, they might combine the c-statistics obtained for
each model from internal validation (e.g. bootstrapping) and external validation.
indeed table 3 gives the study-specific results of external validation studies, and
so these could be combined. but the authors argue that “we should also consider
that the paper is already too long (~4500) and a proper evidence synthesis
would lengthen the manuscript significantly more” and “to be honest, this could
stand alone as a separate paper in order to be properly analyzed and reported”.
as a compromise, could we see meta-analysis results for just the nine models at
low risk of bias? i think this is needed to make the paper more complete, as
otherwise the results section is quite dry and a long series of qualitative
statements about what (and what was not) available. meta-analysis should be
possible for most models, as the authors say “all these models were externally
validated at least once”. if statistics are not available, then the authors can say
meta-analysis was not possible.
what do the authors think? if they agree, they might even add text to the
methods section that says “at the request of the bmj statistical reviewer, we
considered performing a random effects meta-analysis of (internal and external)
validation statistics for those models considered low risk of bias”. then the
results section could finish with this meta-analysis (or explanation of why it was
not possible). i imagine that only c-statistics could be combined and not
calibration measures (due to lack of information).
3) there is a large emphasis on the need for more external validation studies.
though i agree with this, it should also be recognised that internal validation
results are potentially equally important. for example, if the model is developed
is a sample that represents the intended population of application, then a suitable
internal validation (i.e. bootstrapping to produce optimism adjusted results) is as
good as an external validation in my opinion, for that same population. further,

internal validation using a large sample size is better than a small external
validation. so the authors must make it clear that both internal and external
validation is needed going forward.
4) it is interesting that the authors use probast to classify the risk of bias of
each model. however, probast is actually a tool for examining the risk of bias of
a study (not a model). so, for example, if a model is validated in multiple
studies, then there are multiple risk of bias assessments (one for each study),
and not one for the model. this tension needs to be addressed. when they
authors say that a model has low risk of bias, is this based on a summary of the
risk of bias across multiple studies for that model? the tension or lack of clarity
perhaps arises because the authors refer to the risk of bias of single model
development studies for each model, and this they use model and study
interchangeable (as there is one of each)? i am not sure.
5) methods section does not say what measures of predictive performance were
sought in the data extraction section. i think this is crucial, i.e. to name the
measures of calibration and discrimination that were desired.
6) the conclusion section of the discussion should also mention that appropriate
internal validation is needed, in order to obtain more appropriate performance
statistics for the population represented by the development sample.
7) how were continuous predictors handled in the model’s developed? is there
any need for recommendations to improve this, for example by using non-linear
trends (and not using dichomisation and cut-points[2])
8) table provides all the external validation results. but calibration is not
mentioned? were truly no calibration results presented? calibration slope, o/e,
calibration-in-the-large, calibration plots?
9) table 3: what do the * refer to in column 1? also, why are the first 2 rows
different? they give 2 c-statistics, but nothing else in the tow differs. may also be
true of other rows.
minor
1) abstract says “to map and assess prognostic models for patients with chronic
obstructive pulmonary disease” – for extra clarity suggest change to “to map and
assess prognostic models for outcome prediction in patients with chronic
obstructive pulmonary disease” (otherwise might appear you look at models for
prediction of copd onset). similarly, i think the title might benefit from adding
this clarity about predicting outcomes in those with copd.
2) “our systematic search yielded 236 articles, describing 409 prognostic models
… “ – do you mean describing the development of 409 models?
3) “there are more than 400 prognostic models for copd patients in an
ambulatory, hospital or emergency department setting, with the majority
focusing on mortality” – again change to “there are more than 400 prognostic
models for outcome prediction in copd patients in an ambulatory, hospital or
emergency department setting, with the majority focusing on mortality”
- please clarify his throughout the article (i.e. that it is outcome prediction in
those with copd)
4) “internal validation provides a more accurate estimate of model performance
in new subjects” – agree but only if done well! (i.e. using bootstrapping or

cross-validation, and not simply giving apparent performance) - need to state
this.
5) “missing data often lead to biased estimates if not imputed, because it can
distort the performance of a prediction model if the missingness of values is
related to other known characteristics” – should this sentence be prefaced with a
statement that the handling of missing data was unclear or sub-optimal?
otherwise it jumps out of the blue.
6) did any studies evaluate the net benefit of the model?[3]
7) table 4 last column should emphasise that this is from internal validation
8) table 6 nope provides the model equations. however, the ones in the table do
not tell us how to move from the scores for each predictor to the actual predicted
outcome risk. also, underneath it says “for the extended ado index, a model
presentation was not available. the prognostic model by bertens et al 2013 was
only available as a regression equation” – remove the word only from this
sentence, as providing the regression equation is a good thing (indeed better
than the score systems in the table)
with best wishes, richard riley
1. riley rd, ensor j, snell ki, debray tp, altman dg, moons kg, et al. external
validation of clinical prediction models using big datasets from e-health records or
ipd meta-analysis: opportunities and challenges. bmj. 2016;353:i3140.
2. riley rd, van der windt d, croft p, moons kgm, editors. chapter 4: ten
principles to strengthen prognosis research. in: prognosis research in
healthcare: concepts, methods and impact. oxford, uk: oxford university press;
2019.
3. vickers aj, van calster b, steyerberg ew. net benefit approaches to the
evaluation of prediction models, molecular markers, and diagnostic tests. bmj.
2016;352:i6.


<|EndOfText|>

this is an excellent revision. i only have a few further comments:
hazard ratios relate to the whole follow-up period, and yet in various
places (especially the abstract) the authors infer it is time-point specific
(‘… with mesh has a higher chance of having a re-intervention (hr 1.47,
1.21 to 1.79) at one year...). in other words, it does not relate to just
one year, but also at all times up to one year. please address this
throughout by changing the language (e.g. could say: throughout the
first year, patients with mesh had an increased risk ….). if the authors
want to report a comparison at a particular time-point, then the different
in survival % at one year between the matched groups is more pertinent.
i suggest this is also added, as derived from figure 2b.
abstract should detail the matching factors used in the propensity score
approach (i.e. detail the confounders adjusted for)
my biggest issue: the authors do not discuss any other potential
confounders that may still be affecting their results, in their discussion.
the hr is attenuated from the unadjusted analysis to the propensity
score analysis – could additional confounders attenuate it further? this
needs serious discussion and thought.
best wishes, richard

<|EndOfText|>

i start by saying that the pdf of the submitted article (which i used for my review below initially)
does not seem to match the track changes version of the submission (labelled v2 in the
supplementary material). that is, the pdf version does not contain most (perhaps any) of the
changes highlighted in yellow. i’m not sure what has gone on there, but i spent 2 hours going
through the pdf version and only at the end, when looking through the supplementary material,
did i identify this discrepancy. i therefore do not know which version is right for me to be
reviewing! however, i have checked whether the track changes version addresses my concerns,
and it does not. therefore, i think my comments below still stand.
following the response to comments, it is interesting to see that the odds of immediate death (on
day of birth) are larger at weekends than during the week. but this information is only given in
the response to comments, and i can’t see it detailed in the actual article! this is one of a few
examples of where the authors have only gone part of the way to addressing my comments. no
propensity score analysis was done; it’s still not clear how missing data was handled (not multiple
imputation it seems); and clustering multiple babies from the same woman is still not accounted
for. as mentioned, the reporting of the methods is still not adequate – for example, even though
they respond to the comments, they have not always revised the paper in the same way (e.g.
details about missing data and clustering are still missing in the paper) therefore, i am not
convinced the article is robust enough for the bmj as it stands. i’m sorry to say that. i suggest
further revisions and clarification are still needed:
1) the authors say that missing data was handled by the regression analysis, but it is not clear
how. they say in their response:
“missing data is not a significant problem in hes data due to the coding process and rules. for
some variables, there are “unknown” responses which are accounted for the regression model.
the extent of “unknown” values is given in table 2. as described above, coding practice is not
influenced the day of admission/birth so missing values are not likely to bias the results”
was ‘missing’ given its own category? would the results be different if a multiple imputation
approach was used? we need to know for robustness. i searched the article for ‘missing’ and
can’t see anything.
2) the authors still do not do a propensity score analysis, which is a real shame as i still feel the
paper would be far stronger as a result. they say: ‘we did not undertake propensity score
analysis but did validate the key results by using alternative methodological approaches (e.g.
difference case-mix adjustment and use of hierarchical models). such analysis would not address
the limitation of residual confounding bias.’
i agree that a propensity score model won’t solve the issue of potential residual confounding, but
when matched propensity score analysis is done, the adjustment for confounding is less prone to

influence of extreme individuals (i.e. those can’t be matched) in one of the groups than the
regression models used. i think i would, at the very least, note the potential to consider
propensity score analyses in the future for this type of study, and to explain why you did not do
this.
3) i do not understand why an analysis that accounts for clustering of multiple babies for the
same woman does not converge. the authors infer this is due to such woman only being 2.9% of
the total number. however, this still represents 36,679 babies, which is enormous. is it because
there are so few events in this set of babies? we need an explanation, and full details in the
paper that you tried to do this but it did not work, as otherwise the paper could be severely
criticised.
4) please present the unadjusted odds ratios in the table; it is only one additional column and it is
not clear, as it stands, whether the ors presented are unadjusted or adjusted anyway.
5) the immediate deaths analysis is important and should be reported in the paper, surely? or
am i missing it somewhere? otherwise, if death occurs 3 days later, then how do we know it was
due to being born on a saturday or something that happened on the monday? the immediate
death is therefore more pertinent.
6) the abstract conclusion infers causality – there is no mention of the potential for residual
confounding. i am disappointed by this, as in their response the authors clearly say that they
have noted the limitations of their work more clearly now; yet this is not evident here.
7) what this study adds says that the paper gives the ‘most comprehensive evaluation …’ – but
with no accounting for clustering, no propensity score analysis, no multiple imputation of missing
values, etc i can’t agree with this statement. please tone down.
8) the abstract says that 750 and 450 perinatal and maternal deaths extra occur at the weekend:
but over what period? one year? each weekend? please be absolutely clear, and give confidence
intervals for these pivotal numbers. and be clear what the population size these numbers relate
to. 750 extra deaths out of how many total deaths, in a population of size of …
9) i am still unclear, and i think the methods are still unclear in the paper, how events were
defined in relation to the day of the week. so if a woman was admitted on a saturday but had the
event on a tuesday, is this classed as a week day event? i think it is, but it needs to be explicitly
stated.
10) further, another important issue that i think is unresolved. please clarify how the days prior
to the event are accounted for (if at all). for example, could the authors count for each individual
how many weekend days they were exposed to? then, they could look at including the number of
weekend days as a covariate in a model, and (after adjustment for other things) ascertain
whether there is an association between the number of weekend days of exposure and the rate of
event.
this is interesting for two reasons:
(i) it is another way to examine if there is an indication that weekend care is having a negative
impact on the whole (not just on the day of delivery itself).
(ii) it may be that, after you adjust for the number of days exposed to weekend care, that the
effect of the actual day of delivery disappears. or it could be that both day of delivery and the
number of exposure days are important, which would be revealing.
11) the analysis described as follows it not clear enough to me still: ‘using regression analysis on
just the reference days (tuesdays), probabilities of in-hospital perinatal death and puerperal
infections (maternal) were derived. by matching these probabilities for each tuesday admission,
based on the mother’s or neonate’s characteristics, indirectly standardised estimates for the
outcomes as if those non-tuesday cases had had similar rates as their tuesday counterparts were
calculated.’
- how was matching done? how close did the probability have to be in order to be matched? was
the matching random?
- for deriving the predictions, what covariates were included? how were missing values dealt
with?
- indirectly standardises estimates … of what? odds ratios? proportions? and how were they
derived?
12) “the most common day for giving birth is a thursday (15%...)” – can we have a ci please for
this estimate, otherwise this is just an observation. the ci should be very narrow given the large
sample size.
13) “much of the difference is explained by number of elective …’ – please refer to the figure
where this is apparent
14) in their responses, the authors make the important statement that immediate death is also
higher in those born at weekends. but
15) the discussion says that the estimates of weekend effect are likely to be an underestimate – i
don’t see the justification for this at all. residual confounding is one reason why it may be an

overestimate.
16) throughout the article, the authors use ‘rate’ when they really mean % or proportion. a rate
is the number of events per period of time. here though they look at the % with an event by one
time-point (e.g. by 3 days).
i'm sorry to be so negative, but these are all genuinely important concerns and areas of worry for
me. i sincerely hope these comments help improve the robustness and interpretation of the
article moving forward.

<|EndOfText|>

i thank the authors again for their revision and clear response. i think we are very close
now, but i would suggest one further revision to address the points below. i thank the
authors in advance for addressing these, and i am confident they will do a good job.
1) in the abstract, need to make it clear whether the summary hazard ratios are adjusted
for confounding factors (other risk factors)
2) in the data extraction of the methods section, please define what is meant by ‘maximally
adjusted’ – the authors refer to the reported hr that adjusted for the most confounders, but
the reader might think that this rather refers to hrs that adjusted for a pre-defined set of
confounders that would the authors considered to be the maximum.
3) explain that by ‘crude estimate’ the authors mean unadjusted estimates
4) stata should be stata
5) in the assessment of heterogeneity section of the methods, start by saying “in each
meta-analysis, heterogeneity was evaluated by using …”

6) methods says: “a p-value of <0.10 indicates the presence of publication bias” – rather
replace this with “a p-value of <0.10 was taken as statistical evidence of the the presence of
small study effects (potential publication bias)”
7) results: “figure 1 shows the shrs and their corresponding …” – please make it clear here
and indeed everywhere that the shrs are summary adjusted hazard ratios
8) in the results, the authors often refer to statistical significance. i would prefer them to
say that there was no strong evidence of association, and also refer to whether the width of
the 95% ci is wide. in other words, don’t infer that there is no association when actually it
may be a low power issue. so, just be cautious about using statistical significance to guide
statements about no associations.
9) no results are embedded within the results text – the tables are referred to, but i think
providing summary results in brackets for a few key variables would make the reading
clearer and flow better.
10) most important point: i think the quality of evidence should come early on in the results.
then, when the authors provide their shrs, they can make an accompanying statement
about the quality of evidence behind it?
11) the previous two points are already done superbly well in the abstract, e.g. “the quality
of evidence was rated high for an inverse association with incidence of t2d for an increased
intake of whole grains (shr for an increment of 30 g/d (95%-ci): 0.87 (0.82-0.93)) and …”
– i would suggest the authors write similarly in the actual results.
at the moment the results are quite disjointed, and hard to piece together the shrs with the
quality angle. starting with quality and the focusing the start of the shrs to be on those
with high quality evidence would really help.
i hope this helps the authors and the bmj going forward
best wishes, richard riley


<|EndOfText|>

i thank the authors for responding in detail to my comments. their rebuttal and revision appear
sensible, and i am satisfied that the statistical issues i raised have been addressed or resolved. there
remain a large number of analyses and secondary reports as the basis for the paper. i am still slightly
concerned about the issue of low power, due to wide cis and secondary nature. on the other hand, had
the authors actually found a few strong results, i am not convinced how reliable this would have been
given the extensive number of analyses. but as the authors argue, the original trial program was
complex and other reviewers seem to appreciate the further detail. i leave the bmj clinical editors to
decide whether this adds clinical value over and above the previous publications. i only have a few more
comments
abstract conclusion: this reads better, but the authors now say “a bivalirudin monotherapy strategy, as
compared with heparin with or without gpi, does not significantly reduce mace or nace in patients with
steacs nor with nsteacs.” – i find the word ‘significantly’ rather vague, and i ask the authors to be
more specific. is this clinical or statistical significance? given their rebuttal, i think this is based on
statistical significance, but as i mentioned before this could be due to low power? would it be better to
say ‘ there is no strong evidence from this trial that a bivalirudin monotherapy strategy … reduces
mace or nace …’? i just would avoid the use of significant, and focus on strength of evidence.
similarly, in the discussion the authors say that ‘a regimen of bivalirudin monotherapy, as
compared with unfractionated heparin with provisional gpi, is not associated with …’ – i think that
better language would be that there is no strong evidence from this trial that ….
please mention that the mantel-cox method produces unadjusted rate ratios.
the authors show me in their rebuttal that adjusting for centre does not make much difference. can
they also state this in the results or methods please?
page 13 penultimate paragraph, we are missing a closing bracket.
table 1: footnote still discusses the method to obtain p-values but none are shown
table 2: footnote mentions the use of continuity corrections, yet this was not explained in the methods.
please address this.
table 2: footnote says that fisher’s test was used to calculate p-values, but this method is for 2 by 2
tables, not for comparing rates. or have i misunderstood? again, there is no mention in the methods.
‘event rates at 30 day’ – the authors rather mean ‘event rates over the first 30 days’, the word ‘at’ is
inappropriate i feel.
i hope these further comments help improve the paper going forward.

<|EndOfText|>

i thank the authors for their detailed response. on reading their
response and the revision, some further statistical issues and
clarifications have arisen for them to address going forward, as
follows:
abstract: the design section needs to mention how the cohort study
was set-up or obtained (e.g. prospective, database study, etc),

years of follow-up, representativeness, and that propensity score
matching was done, and briefly how.
abstract and main results: to translate to absolute risk, in addition
to giving the absolute rates, could we also have the probability of
survival at 7 days for each group, and their difference? this would
help the reader know the probability of death is very low.
abstract: give the total number that died.
methods: i cannot see the full list of baseline characteristics given
that were used in the ps approach. it just says: “the ps is the
probability of receiving haloperidol as opposed to one of the three
atypical antipsychotics estimated by logistic regression, given the
baseline characteristics described above.” – but the latter need to
given explicitly, as i can’t find them ‘above’. do you mean to refer
to table s1? i think adjustment factors are crucial, so should be in
the main paper, not the supplementary material.
methods: “we assessed potential confounders for the planned
analyses using information from hospital admission to the day
before antipsychotic initiation.” – how were these assessed?
statistically? clinical judgement? etc
results: i do not follow the comment that “the ps distributions
were largely overlapping with a c-statistic of 0.65” – i think you
need to explain to the reader that the c-statistic is for the logistic
regression model that produces the ps values. it is not a measure
of separation of the ps values for those treated and untreated (i
think? indeed, if it was, this would be cause for concern, because a
c of 0.65 suggests considerable distinction between groups). so this
sentence should be split; explain the ps model and its c-statistic.
then says that ps values were largely overlapping (perhaps a figure
in the sup material to show the 2 distributions for the groups?)
results: in response to my previous comment about the need to
check for non-proportional hazards, it is an important new finding
that there is strong evidence of non-proportional hazards. “the
time by exposure interaction coefficient was statistically significant
(p=0.008), suggesting that the proportional hazards assumption is
not met in the model”. however, the implications of this are only
briefly mentioned in the results and reference given to the
supplementary material. i think more details are needed in the main
text, with results shown for the hr in day 0, 3, 5, 7. 10 as in s4.
indeed, what is rather striking, and worthy of discussion, is that the
hr is close to 1 by day 5, and then becomes < 1 by day 7 and 10,
with almost a significant result in the opposite direction at day 10.
if this is genuine, what are the implications?
i think the issue of non-ph should be given more prominence in the
paper, e.g. what this study adds and the abstract.

subgroup analyses: it is not clear why age, cci, exposure have
been dichotomised. this is not a sensible analysis strategy, and – if
the covariates are of genuine interest – a more powerful and
meaningful analysis would be to consider them on their continuous
scale (at the very least, in addition to the analyses done). this
comment was also made in my earlier review but not addressed as
far as i can tell.
results: “comparison of hrs between patients who were in the icu
(hr=1.11, 0.68-1.81) on the index date and those who were on the
medical ward (hr=2.01, 1.44-2.82) suggests that there might be
effect heterogeneity (p=0.04).” was icu / medical ward part of the
ps matching criteria?
discussion: “the potential adverse effect of haloperidol appeared to
be the strongest during the first few days following initiation,
suggesting an acute harmful effect of haloperidol” – i would tone
down the implied causal language here to something like: “the
association of haloperidol with increased rate of death appeared to
be the strongest during the first few days following initiation, which
if the association was causal, would suggest an acute harmful effect
of haloperidol”
in response to my comment about missing data, the authors say
there is not any missing data in ‘the strict sense.’ so, really no
missing data in any of the variables used in the ps matching model?
i find that hard to believe based on my personal experience, but if
so then this should be added, i.e. that there was complete data for
all individuals. however, in one subgroup analysis they say: “it
should be acknowledged, however, that there is an increased
likelihood of residual confounding in this analysis due to potentially
incomplete baseline information.” is this just the subgroup analysis
where there is missing data then?
i hope these additional comments can be addressed by the authors,
and help improve the article further.
best wishes, richard riley


<|EndOfText|>

the authors have clearly worked hard to align their paper with our suggestions.
this makes the paper a long read, especially in the methods section, and we now
have most of the original analyses supplemented with the analyses that we
suggested. for example, the multiple imputation follows the description of single
imputation analyses, and the investigation of non-linear associations now follows
the standard linear assumption. so this is much to process for the reader. that
being said, the complexity is a fair reflection on how the findings were derived
and the order the methods were operationalised. i have a few main comments
remaining, which i’m sure the authors can address, and some minor ones.
main comments
•
results ‘main associations …’ section. when the authors say ‘in model 1
…’ – i think it would help to remind the reader what this model is (specifically
what adjustment factors were included). also, there is still no mention of

whether the proportional hazards assumption was sensible for the analyses
reported.
•
and, the linearity assumption is mentioned for the first analysis, but not
for the others. the spline plot is a very nice addition (i thank the authors for
doing this on the log scale now), and shows that the linearity assumption appears
sensible for this one outcome, but what about other outcomes of chd and
cerebrovascular disease?
•
the abstract focuses on hazard ratios. i think it would be beneficial to
also indicate how they translate to change in absolute risk by a particular
time-point. the hazard ratio appears small, and the impact on absolute risk may
be very small. this is an important point to convey to the reader. the results and
discussion may also benefit from this
•
in the discussion, can the authors discuss whether further research
might rather analyse the total amount of ultra-processed food eaten, rather than
as a % of the total diet? i am not sure if this would change the findings or be
relevant to the field, but to me, the absolute amount may be more important (or
also important) than %.
minor comments
•
in the what this study adds: “an absolute increment of 10 in the
percentage of ultra-processed foods in the diet was associated with a >10%
significant increase in the risks of overall cardiovascular, coronary heart, and
cerebrovascular diseases.” i would remove the word significant in this sentence.
•
“absolute cvd risks were 253 for 100000 person years in the whole
population: more specifically, age and sex-corrected absolute cvd risks were 242
for 100000 person years …” – i think cvd risks should be changed to cvd rates
in this sentence.
•
“more specifically, ultra-processed beverages (p=0.004) were associated
with increased overall cvd, ultra-processed fats and sauces (p=0.04) and meats
(0.05) were associated with increased coronary heart diseases, and
ultra-processed beverages (p=0.01), sugary products (p=0.05) and salty snacks
(p=0.04) were associated with increased cerebrovascular diseases” – i think it
would be beneficial for the p-values to be replaced or supplemented with hrs and
cis here.
•
“in contrast, none of these food groups were associated with cvd risk in
their non-ultra-processed form …” – i think the authors are better to say that
there was no strong evidence for an association, rather than there is definitely
not an association.
•
table 2: i still do not find the sex-specific quartiles easy to follow. i
would have thought there were 4 for males and 4 for females, but they seem to
be lumped together? please can the authors clarify how these groupings are
created and analysed in their article?
•
“the association between ultra-processed food and overall cardiovascular
risk was also investigated separately in different strata of the population:
men/women, younger adults (<45y)/older adults (≥45y), participants with a high
lipid intake (>median)/those with a lower one, participants with a bmi<25
kg/m2/those with a bmi≥25 kg/m2, …” – as mentioned before, these groupings
are rather arbitrary, and a better approach would have been to look at the
interaction between the continuous covariate and the effect of ultra-processed
food (rather than creating groups and looking in them separately). but, at this
stage, i do not think it makes sense to ask the authors to do anymore work on
this specific point.
best wishes, richard riley


<|EndOfText|>

i thank the authors again for providing a comprehensive response, which i have been through. i assume
they have updated their manuscript accordingly as they stated they have (i haven’t had time to check
this line by line). a few final things:
1) the response to question 3 is interesting, with an updated analysis provided included the updated
follow-up information of participants after the first submission of this article. it is worth adding the
actual results to the paper. i see the authors already mention this updated analysis in the new revision,
so it is merely a case of adding a table (an appendix should be fine for the results).
2) by the way, these new results for 28 days in this table are given as either ne (not evaluable) or
100% (ci 100 to 100). i don’t think a ci of 100 to 100 is sensible to state, and it is unclear why the
results for the hcq group are ne at 28 days. explanation is needed below the table.
3) page 18: please change “the hazard ratio was estimated by the cox model, which is the higher, the
more rapid the virus negative conversion or symptoms alleviation is” to “the hazard ratio was estimated
by the cox model, and hrs greater than 1 indicate the rate of virus negative conversion or symptoms
alleviation is higher in the which group compared to other group’ – and of course please define the
groups appropriately.
4) page 24 – trail should be trial. actually, if you search for trail, there are 6 occurrences in the article.
these should be replaced with trial.
5) supplementary table 1: “negative conversion rate by the specific time” – this should be “probability
of negative conversion by the specific time” – also heading should say difference in probabilities not
difference in rates

6) abstracts: “the median time from randomization to illness onset was 16.6±10.5 days” – i find this
confusing upon re-reading. the participants are already ill upon randomisation, as they have tested
positive, so what illness onset are the authors referring to here?
with best wishes, richard riley


<|EndOfText|>

the revision is superb and the response to my comments very clear. i thank the
authors for this. i only have two further minor comments. best wishes, richard
riley
- harrel’s e statistic should be harrell’s e statistic i assume? also, can the
authors very briefly explain to the reader what the e statistic is, as i actually had
never heard of it.
- i-squared is a poor measure of heterogeneity; rather report the estimated
between-study standard deviation (sd), tau, should be reported.

1. rucker g, schwarzer g, carpenter jr, schumacher m. undue reliance on i(2)
in assessing heterogeneity may mislead. bmc med res methodol. 2008;8:79.


<|EndOfText|>

this is an extensive revision, and the authors have clearly responded very well to
the many comments they received. i only have minor points remaining.
1) “all pairwise analyses were conducted using the ‘meta’ package (version
4.9-4) (21) in rstudio 1.0.143” – please be clearer on the actual estimation
method used to fit the random effects model as there are many within the
software package. i think the dl method is used for estimation (as noted in the
text a bit further down in regards to the estimate of tau-squared) but should be
explicit.
same applies to the use of mvmeta and network modules in stata to fit the
network models. what was used? reml estimation? that is the default.
2) did the network analysis account for uncertainty in variance estimates (note:
this is the default in network and mvmeta, unless the nounc option is removed).
please clarify, as confidence intervals may be too narrow otherwise, as
mentioned in my previous notes in the editors comments). see
1. hartung j, knapp g. a refined method for the meta-analysis of controlled
clinical trials with binary outcome. stat med 2001;20(24):3875-89.
2. jackson d, riley rd. a refined method for multivariate meta-analysis and
meta-regression. stat med 2014;33(33):541-54.
3) “statistical heterogeneity within each pairwise comparison was estimated
using the i2 statistic” – as mentioned i2 is not a measure of heterogeneity
directly, which you recognise in your response, but it still reads like you are
saying this. rather say in the text tht it is the proportion of the total variance
that is due to between-study heterogeneity
4) in the results when giving the ors in brackets from the various
meta-analyses, i suggest saying ‘summary or = ‘ to emphasise that this is the
summary result. as noted, there is heterogeneity, so just saying or = … is a bit
misleading. i think the word summary helps to clarify that this is a meta-analysis
result.
5) results: “no other significant differences between active treatments were
found” – significant in what sense? statistically or clinically? if the former,
suggest you rather say: there was not statistical enough evidence to suggest any
other difference between active treatments.
6) “the treatment protocols with the highest probabilities of being the most
efficacious in terms of response were bitemporal ect (36.9%) and priming tms
(18.5%), while low-frequency left rtms (30.3%) and continuous tbs (29.6)
were least efficacious”
- recommend here that additionally you also say which ones had the highest
mean rank, and which ones had the highest probability of being ranked last.
focusing just on the one with the highest probability of being ranked first may be
an incomplete picture. i recognise the full results are in the sup material, but
here it is important to say also in the results.
7) “fitting the design-by-treatment interaction model provided no evidence for
significant …” – again statistically significant?
8) “however, there was moderate uncertainty in these estimates, and we cannot
exclude the possibility that the actual number of inconsistent loops is higher than

those reported.” – the last bit is too vague. reported by who? by the authors
above, or something else?
9) “when trials with high overall risk of bias were excluded, …” – did they resolve
any of the inconsistency concerns too?
10) “there was considerable variation in the precision of treatment effect
estimates” – do you mean from the individual studies, or from the
meta-analyses? if the former, why does that matter? if the latter, then explain
more why this is important. i think because it implies some treatments have
more evidence available than others?
11) figure 3 – tell the reader these are summary meta-analysis results. are
these from the pairwise ma or the network ma? same for fig 5.
12) table 1: “effect sizes represent relative odds ratios …” – do the authors just
mean odds ratios? they are not dividing 2 ors as far as i can tell, so not relative
ors. also the outcome should be clearly labelled for the left (pink) and top
(blue), for the reader.
13) lastly i see that the number needed to treat and number needed to harm
have been added to the results of the paper, at the suggestion of a previous
reviewer. i understand the desire to translate to clinical decision making, but i
worry about these numbers, because they are based on the summary
meta-analysis result. as we know, there is some heterogeneity (in both the
baseline risk and the treatment effect across studies), and therefore the nnt and
nnh are also hugely variable. if the authors want to include this, then they need
to make it clear in the methods section how these are calculated and what
hypothetical population they relate to. the heterogeneity makes the
interpretation of nnt even more problematic than it is even when there is a
single trial, as discussed here:
https://discourse.datamethods.org/t/problems-with-nnt/195
perhaps, in hindsight, the authors should remove these values or place them in
the supp material.
with best wishes, prof richard riley


<|EndOfText|>

this is a well-written and well reported article, as expected from the set of authors involved. the authors have clearly
worked hard to address an important topic using the primary care database available. it is good to see that a protocol
was published for this cohort study. i have reviewed this from a statistical perspective, and although standards are
generally good as expected, i have some comments for improvement and areas for clarification to be addressed in any
subsequent revision:
1) when reading the article, my initial impression was that there are a lot of analyses here, for example across
different 3 outcomes, different classes, individual drugs,, and different follow-up times. for the latter it says in the
methods that ‘as sensitivity analyses we repeated the analyses firstly restricted to the first year of follow-up, then
including the entire follow-up period’ and also time since starting treatment is investigated as categories, e.g. first 28
days.
but in the protocol, though 5 years and 28 days are mentioned, i cannot see mention about the 1-year analyses. can
the authors clarify please why they focused on 1-year in the end, if not mentioned in the protocol (perhaps i am
missing something)?
2) in relation to this point, most analyses over the 5 year period are not significant, but there are more significant
results by 1 year. this suggests that the hazard ratio is not proportional over time, but this is not evaluated formally
(statistically) and raises the question about the hrs from years 1 to 2, and 2 to 3 etc. therefore i find the focus on 1year an incomplete picture, and wonder whether the authors could comment in the results about whether the
proportional hazards assumption was appropriate (the methods say it was examined, but we don’t see the details). i
would find it strange that the hrs at 1-year are significant but not at 5-years, if the proportional hazards assumption
is actually ok.
many 1-year results are the main message in the abstract and conclusions, yet they are only given in the
supplementary material in the actual paper. i think they should be brought into the main article tables, and this may
link to a more detailed investigation of the proportional hazards assumption (if the hr is constant over time, or what
the hr is within each year interval upto 5 years).
of fundamental interest: if the sris are associated with benefit for the first year but overall the 5 years there is no
difference, does this mean that the sris are associated with harm in the latter years?
3) further, the authors look at 3 outcomes in the paper ‘arrhythmia, myocardial infarction and stroke or transient
ischaemic attack’. yet, in the protocol there were far more than 3 outcomes listed (see below), and none were
mentioned as primary outcomes. can the authors clarify why they looked at these three outcomes in this paper as a
priority over other outcomes listed below:
•all-cause mortality
•suicide (including open verdicts)
•attempted suicide/self-harm
•sudden death
•overdose/poisoning with an antidepressant
•myocardial infarction
•stroke/transient ischaemic attack (tia)
•cardiac arrhythmia
•epilepsy/seizures
•upper gastrointestinal bleeding
•falls
•fractures
•adverse drug reactions (including bullous eruption)
•motor vehicle crash.
4) the authors adjust for confounding using cox regression, and it is good to see that many confounders are indeed
adjusted for. that being said, i would also have liked to see whether conclusions are robust to the use of propensity
score matching methods. or can they justify in the discussion why this wasn’t considered beneficial over traditional
regression adjustment? perhaps, due to the time-varying nature of the use of antidepressants, this was problematic

5) can the authors clarify in the paper the use of the time-varying antidepressants covariate and its interpretation for
an individual who stopped. if an individual stops antidepressants, then do they then (for subsequent follow-up periods)
move to the non-treatment group? if so, then how does this handle the potential for events to be due to the earlier
use of antidepressants? could it be that the lack of any differences between groups is because some of those who
were on anti-depressants or moving into the non-treatment group, and therefore any genuine difference is being
attenuated?
6) “even for doses of citalopram ≥ 40 mg/day there was no significantly increased risk (adjusted hazard ratio=1.11,
95% ci 0.72 to 1.71).” – though this statement is correct, the confidence interval is 0.72 to 1.71 and is therefore
wide: is there low power? indeed, there are not many events in many analyses. this is worthy of discussion please in
the strengths and limitations section.
7) i am also concerned about missing data: the authors say ‘we included all eligible patients in the database in our
analyses to maximise power’ – but there are no details about how missing data were handled. i notice that under a
table it says ‘5.0% of prescriptions had missing information on dosage.’, so there is some missing data – but how was
it handled? it is also not mentioned in the protocol.
8) the authors used ‘robust standard errors to allow for clustering of patients within practices’ – such methods are
done when the model used is mis-specified (or the correct model is difficult to actually fit), and therefore the ‘robust’
standard errors used to inflate uncertainty accordingly. however, here i do not understand why the clustering within
practices was not accounted for by using, for example, using a stratified cox model or adding a frailty term (with a
random effect on the baseline hazard to allow for separate one for each practice). though this is a minor point, i
would like the article to clarify if alternative approaches to accounting for clustering affected the conclusions.
9) in places, the authors infer a difference between individual drugs, which is often not justified. this is most apparent
in the ‘absolute risks’ section, where they say ‘absolute risks of arrhythmia and myocardial infarction were highest for
lofepramine’ – this is not justified, as the cis for the risks and nnh are very wide and overlap with the other drugs.
this therefore needs to be re-written. please check elsewhere for this issue too.
in summary, this is an important piece of work, and i hope my comments help to improve the article further,
especially in regard to the outcome investigated, the time-points considered and the use of time-varying covariate.

<|EndOfText|>

i have looked at this from a statistical perspective and have some comments.
1) i understand the reasons why the authors do not perform meta-analysis, due to the heterogeneity in
time-points, analysis results presented, inappropriate tests in primary studies, etc. however, though this
overall decision is perhaps commendable at first glance (and is probably correct!), it is rather ‘broad brush’
as perhaps there were subgroups of studies that were combinable (e.g. outcome measured at same timepoint, analysis methods appropriate and well reported)? further, even when time-points differ, hazard
ratios are combinable as we typically assumed hrs are constant over time.

2) thus, far more details are needed in regard how data extraction was performed regarding the effects of
interest. for example, when hazard ratios are of interest but poorly reported, there is a multitude of
methods to indirectly obtain them from other information (even simply an exact p-value and the numbers
of patients/events in each group) see ref [1]. therefore, i would like reassurance that the authors have
done their utmost to get the right data, before we can be sure that a lack of meta-analysis is entirely
sensible.
3) a related point is that the effect of interest is not clearly defined in the text. are we looking at
continuous outcomes, or binary outcomes? are we looking at mean change, ors, hrs, or …? even though
results are not being combined, we need clarity in the text as to the measures of effect being reported and
extracted. i suggest a section, near the start of the results, that explains what exactly was extractable (in
terms of effect estimates) for each outcomes, the scale(s) of this effect, and the why meta-analysis was
not done.
4) i imagine that the authors are ultimately right to not pool, but as mentioned above we need greater
justification. but the issues of heterogeneity are very common in meta-analyses of lab based research, for
example see all the quotes about the difficulties of doing meta-analysis in the ref [2] paper, so i
sympathise and we need to address the issue of heterogeneity in these types of studies, and make sure
research studies are combinable in the future. prospectively planned ipd meta-analyses would really help
us in this regard [2][3-5]
5) table 2 – why just report the statistically significant findings? i suggest the authors may simply be
promoting selective reporting or chance findings by doing this, and are not being comprehensive in the
review summary. at least, can the full results for each study be given in a supplementary material? this
relates to the point 6 below, where i ask for all the results in the plots to be quantitatively summarised,
where possible, in terms the estimate and ci of the ratio being presented.
6) the authors present the results on a graph (fig 2 and 3) by first creating a ratio of the response in the
vaccine group compared to the non-vaccine group. these are not combinable, as they highlight in the
methods, but allow us to get a feel for the direction of results. i think we need the authors to give far more
details next to the results of each study in each plot, or in a separate companion table. in particular, what
is the ratio of (means, odds, medians, etc) and what is the ci for the ratio (if derivable) for each bubble?
otherwise the reader may try to pool without due thought. in particular, uncertainty of the evidence is
being completed ignored, in terms of the ci around each point. so these figures need to come with a clear
warning that they are just for illustrative purposes, and the limitations of them must be emphasised!
7) figs 2 and 3 also need to be improved in terms of the size of the bubbles. some bubbles are so big that
they encompass other bubbles on lines above them. e.g. page 19, plot a, the bubbles on rows 2 to 4 are
encompassed by one huge bubble, which makes it hard to follow. but i appreciate the authors are
struggling to present the messy data.
i hope my comments help improve the article going forward.
[1] parmar mk, torri v, stewart l. extracting summary statistics to perform meta-analyses of the
published literature for survival endpoints. stat med. 1998;17:2815-34.
[2] riley rd, hayden ja, steyerberg ew, moons kg, abrams k, kyzas pa, et al. prognosis research
strategy (progress) 2: prognostic factor research. plos med. 2013;10:e1001380.
[3] riley rd, lambert pc, abo-zaid g. meta-analysis of individual participant data: rationale, conduct, and
reporting. bmj. 2010;340:c221.
[4] altman dg, riley rd. an evidence-based approach to prognostic markers. nature clinical practice
oncology. 2005;2:466-72.
[5] riley rd, abrams kr, sutton aj, lambert pc, jones dr, heney d, et al. reporting of prognostic
markers: current problems and development of guidelines for evidence-based practice in the future. br j
cancer. 2003;88:1191-8.

<|EndOfText|>

thank you for the opportunity to review this interesting and well-written manuscript. i
have focused on statistical aspects, and have the following comments
1) at the start of the results, there needs to be more discussion on the included
populations of the trials and whether they are broadly similar across them. this is crucial to
the understanding of whether these trials and their populations are in some sense
exchangeable, such that the consistency assumption is likely to hold (in advance of data
analysis). i.e. that it is sensible to combine these trials in a network meta-analysis. when i
look at the table of study characteristics i see trials from different countries, different
settings (eg primary and secondary care), different age range (some all ages, other upto
just 45)and different lengths of follow-up (eg 12 and 18 months), which do raise concerns
about the distribution of potential effect modifiers being different for studies that give
direct and indirect evidence.
2) the authors say in the results section that: “direct evidence was therefore available for
nine of the 10 possible comparisons. one rct was a cluster-randomised trial. we therefore
used the cluster size and the intra-cluster correlation coefficient to reduce the size of the
trial to its “effective sample size” before we carried out any data pooling” –the authors
should explain this approach in the methods section.
3) in addition to the heat plot, it would useful to have a statistical measure or test of
whether there is any evidence of inconsistency across the network ma overall. i do not
find the heat plot easy to conclude whether there is inconsistency or not. the consistency
assumption can also be examined across the whole network using design-by-treatment

interaction models, which allow an overall significance test for inconsistency. (of course this
may have low power; see introduction by riley rd, jackson d, salanti g, et al. multivariate
and network meta-analysis of multiple outcomes and multiple treatments: rationale,
concepts, and examples. bmj 2017;358:j3932)
4) the authors say the “the p-score is the probability of each treatment being ranked as
best in the network analysis” – this is not correct, or at least it not quite exact. readers
may interpret this as the probability of being ranked first, when actually the p-score has
the same interpretation as the sucra, which is the area under a line plot of the cumulative
probability over ranks (from most effective to least effective) and is just the mean rank
scaled to be between 0 and 1. i think the mean rank is more interpretable. could this also
be provided? also the sucra graphs are needed, and a graph giving the probability of each
rank (see the aforementioned riley et al. paper for examples)
5) “we performed a network meta-analysis using the frequentist model, with the statistical
package “netmeta”” – we need more details than just referring to a stats package, as there
are many options therein. in particular, what estimation method was used (reml), were
random treatment effects assumed, was the heterogeneity assumed the same for each
treatment effect, what was used in the pooling (rrs or the raw numbers of events/total),
etc? also whether a two-stage or one-stage meta-analysis modelling approach was used,
and whether the confidence intervals accounted for uncertainty in the estimate of
tau-squared, for example using the hartung-knapp method?
https://www.ncbi.nlm.nih.gov/pubmed/30067315
6) i-squared is a poor direct measure of heterogeneity, and we should rather be seeing the
estimate(s) of tau-squared, the actual between-study variance estimate(s). (rucker g,
schwarzer g, carpenter jr, et al. undue reliance on i(2) in assessing heterogeneity may
mislead. bmc med res methodol 2008;8:79)
7) how were multiple (i.e. 2 or more) treatment effects from the same study handled in
the analysis (as they are correlated due to the common control group)?
8) in the results, there is a major emphasis on the p-score, but we do not get told the
magnitude of treatment effects and their cis. this needs to be clearer in the text as well as
the tables.
9) for the outcome of being asymptomatic (see figure 2) we have wide cis that include
the null, and so we might even want further evidence to clarify that there is a benefit for
any of these treatments, before ranking them. whereas there is clearer evidence that
test-treat has significantly fewer endoscopies than other approaches, it less clear for other
outcomes. actually the authors already hint at this when they say “but none of the
strategies was significantly less effective than “test and treat”, or more effective than each
other, on direct or indirect comparison” – so is the following conclusion actually justified:
- ““test and treat” is likely to be the most effective first-line strategy for the management
of uninvestigated dyspepsia in primary care”
cis might be even wider following a bayesian approach or when using the hartng-knapp
correction
10) the last follow-up time is the key time-point, but as mentioned this varies across trials.
this will lead to heterogeneity in the rrs and make it harder to interpret. this should be
discussed. i assume hazard ratios were not available?
11) table 3 and 4– i am not sure the words ‘league table’ are correct or needed – simply
say summary treatment effect estimates from the network meta-analysis. as mentioned,
we need all the treatment effects for all pairs.

12) usually we are shown the direct evidence results too for each treatment effect; a forest
plot for each would be welcome, to reveal study specific results. indeed, i cannot see study
specific results in this article at all.
13) none of the studies were at low risk of bias. thus, in addition to the summary results
from the nma having wide cis that include the null, this perhaps gives extra credence for
not making strong conclusions about the best treatments from these network
meta-analysis results.
14) the abstract does not explain what the p-score is, and i think readers will confuse it
with a p-value. suggest that the authors define it or, better still, rather give the mean
rank. as mentioned, the abstract should also discuss the uncertainty of the ranking, i.e.
that comparisons of key treatments have wide cis
15) the authors say “we used a rr of remaining symptomatic at the final point of
follow-up” – but earlier say “we extracted all endpoints at a minimum of 12 months, even
for rcts providing effectiveness data at other time points. we did this to ensure as much
homogeneity as possible between individual trial results, and to avoid overestimating the
effectiveness of one management strategy relative to another”
- thus taking the final point of follow-up does not seem sensible. why not choose
time-points that were similar to each? eg do a separate meta-analysis at 12 months, and
then at 2 years etc? this is the best way to improve homogeneity, rather than taking the
final follow-up times.
i thank the authors for considering my comments and i hope they are useful going
forward.
best wishes, prof richard riley


<|EndOfText|>

this is an interesting and well conducted trial. here we see secondary (i think, though it could be clearer) analyses of a
previously conducted and published trial. i have reviewed this from a statistical perspective and have some comments and
suggestions as follows:
is this a negative trial? i’m not as sure as the authors are. all of the effect sizes are in the direction that favours bivalirudin,
but are not statistically significant. cis are wide (e.g. 0.70 to 1.16, 0.67 to 1.05, etc). therefore, how can the authors strongly
state: “a bivalirudin monotherapy strategy, as compared with heparin with or without gpi, does not reduce mace or nace in
patients”. i suggest the authors revisit this conclusion. is it not a case that further research is needed should be the message?
i assume the strong statement is based on non-significant p-values (cis containing 1). perhaps low power is causing the nonsignificance of the findings? or are the authors inferring that no clinically relevant values are in the cis? please clarify. a rr of
0.70 for example would seem to be quite beneficial to me (though i’m not a clinician). in short, the authors should be clear
and justified) about whether there conclusion is based on statistical significance, or clinical magnitude of effects, or indeed
both.
at end of the introduction it says ‘here we present the primary results …’ – what does primary mean here? does this mean
primary outcomes of the trial? i think these are not primary analyses of the trial? thus i would be clearer at the end of the
introduction that this is a secondary analysis of a trial but for pres-specified outcomes / groupings. indeed, the statistical
analysis section should make clearer that the trial was powered on a different main analysis (i think?). this issue relates to the
point above about potential low power.
i would also like more motivation in the intro as to why these secondary but pre-specified analyses were not already reported
in the main trial publication.
i do not think it is necessary to present p-values in table 1, as the study is not aimed (powered) to detect differences at
baseline.
in terms of the analyses, what is the mantel-cox method? no reference is given, and i was expecting a cox regression analysis
to have been used to estimate rate ratios.
was the assumption of proportional hazards evaluated for each analysis? that is, were any treatment effects time-dependent
(non-constant hazard ratio)? please discuss in the manuscript.
why did the main analysis not adjust for centre? would conclusions change if the analysis was stratified by centre? these
centres are across different countries, therefore there is the potential for differences in baseline risk, and thus clustering by
centre might be important. further, is there any evidence of heterogeneity in treatment effect across centres (or at least
across countries)? these points could be considered in a sensitivity analysis, with results shown in a supplementary material
perhaps
there are lots of analyses being done in terms of subgroups and interactions and tests for trend (see methods for list and table
2, though i think more analyses have been done than shown in table 2. with tests for trends and interactions). therefore, even
i struggle to believe anything beyond chance to be honest. this weakens the robustness of the paper, in my opinion, and
reflects perhaps the secondary nature of the work compared to the original trial. perhaps i am wrong, and the authors could
respond to this, but this is my instinct from reading.

for example, the authors have a paragraph about a significant interaction effect. they say: “rates of myocardial infarction
were similar between bivalirudin and heparin in acs patients
with and without st elevation (rr 1.24, 95% ci 0.88 to 1.76 and rr 0.95, 95% ci 0.78 to
1.16 respectively, p for interaction=0.19). notably, event rates at 30 day differed markedly
between the two population: 3.3% in patients with steacs and 15% in those with nsteacs.
randomised treatment effect on stroke were directionally opposite in patients with and
without st elevation (p for interaction=0.0052) with bivalirudin associated with lower risk of
stroke as compared to heparin in steacs patients (rr 0.35, 95% ci 0.13 to 0.97, p=0.035)
and higher risk of stroke in nsteacs patients (rr 4.02, 95% ci 0.85 to 18.95, p=0.057).”
- the focus on 30 days is worrying. is this because it was significant? the overall effect (across all times) is noted as not
significant in the first sentence, but then the authors focus on 30 days. also, what is an ‘event rate’ at 30 days. do the authors
mean event risk by 30 days?
the start of the methods intrdocues 3 matrix trials, but then it is not clearly specified which trial is used going forward (just
the matrix anti-thrombin one?).
some results are slipped into the discussion i think. in the discussion it say: “in secondary analyses we observed that
bivalirudin was associated with a reduction in all-cause mortality due to lower cv mortality, a difference that in this analysis
was similar in patients with steacs and with nsteacs (p for interaction=0.72) while in the updated meta-analysis was limited
to patients with steacs (with a p value for difference for steacs vs nsteacs of 0.07)”. i can’t see the meta-analysis
mentioned earlier.
some of the points in the discussion are debatable. “while we did not observe significant differences in the rate of acute stent
thrombosis between randomised strategies in acs patients with and without st elevation, the present analysis indicates that
the risk of acute st with bivalirudin is probably limited to patients with steacs” – why? what results support this.
in summary, in the context of an important and well conducted trial, i do have some concerns about the potential low power
for these additional analyses, and the over-emphasis (at times) of either non-significant findings being conclusively negative,
or subgroup effects being important (beyond chance). therefore i currently struggle to identify firm findings that will be useful
to the bmj readers. further, some analysis issues remain to be addressed / clarified. i hope the authors can take my
comments as constructive and allow them to revise the paper going forward.


<|EndOfText|>

i thank the authors for considering my comments in detail and revising the article accordingly. i am
generally happy that the analysis methods are now clear, and the limitations / robustness of findings
more clearly detailed. they have clearly done a lot more work to address my views, and i think (and i
hope they agree) that the article is now more transparent, and i hope this improves the translation of
findings for the bmj audience. with that in mind, i have a few final comments to improve clarity further
(without the need for additional analyses):
(1) the language needs to be refined throughout the paper to clarify that the key groups are those
babies delivered or women admitted at weekend versus those delivered or admitted during the
week.. at the moment it sometimes still sounds like the key groups are those who had an outcome at the
weekend itself or during the week (regardless of whether the delivery/admission was in the weekend or
not). here are some examples
abstract: ‘… was significantly worse at weekends’ – i think the authors must be more explicit and say ‘…
was significantly worse for babies delivered at weekend and mothers admitted at weekend’
similar elsewhere: e.g. ‘…the perinatal mortality rate was 7.3 per 1,000 at weekends’; i think the
authors mean ‘…the perinatal mortality rate was 7.3 per 1,000 babies born at the weekend’
and ‘the results would suggest approximately 7750 perinatal deaths and 4750 maternal infections per
year above what might be expected at weekends if performance was consistent across days of the week’
should be: ‘the results would suggest approximately 7750 perinatal deaths and 4750 maternal infections
per year above what might be expected for deliveries/admissions at weekends if performance was
consistent with baby/mother outcomes of deliveries/admissions across days of the week’
please address everywhere in the article for clarity and to ensure correct interpretation
2) for added clarity, the abstract should say that: groups (e.g. weekday versus weekend) were defined
by day of admission (for maternal indicators) or birth (for neonatal records) rather than by day of
complication.
3) methods: i do not follow this sentence ‘where indicators cover national performance across the
measures was calculated, disaggregated by day of delivery’
4) page 11 says ‘we estimated there are some 7750 perinatal deaths (95% ci 720 to 830) per year’ and
later 470 maternal infections above what would be expected if rates were as they are on tuesday. but
when? weekend? all other days? is this the total additional death/infections that relate to the weekend,
or the whole wed-mon period? it’s not clear, though the abstract is more specific in saying that it relates
to weekends.
5) page 12: ‘even through’ should be ‘even though’?
i hope this improves things further before acceptance

<|EndOfText|>

this is clearly an important topic. the statistical analyses appear well done, with clustering and
heterogeneity appropriately modelled for example. however, i have some suggestions for improvement:
1) i found this a tough read, and (as a non-specialist) i found it hard to identify the question and
distinguish the definition of ‘at risk’ from the outcome. for example, the language appears inconsistent
(predictors, indicators, patients at risk could all be used to describe the patients of interest, but it took a
long time to realise indicators meant the outcome) such that the start-point and end-point (outcomes)
were not always clear. the ‘indicator’ language is especially problematic, to me.
eg at the start of the stats methods, it says ‘ for each indicator, a binary outcome variable categorised for
each patient at risk as triggering an indicator or not’
so for each indicator, the definition of the outcome is the indicator present or not – but upon a first read, i
thought the indicator was a coding for those identified at risk, and then the outcome was an actual
prescribing error in this subset. indicator is commonly used in the bmj or other journals to indicate those at
higher risk, so this is incredibly confusing and needs to be addressed.
2) key statistical measures like ‘icc’ and ‘reliability’ (based on the ‘prophecy formula’) are hard to translate
to clinical meaning. i would urge the authors to either explain these better, or focus primarily on the
summary %s and the distribution.
3) in relation to this point, the authors focus in tables on the summary %s from the analyses. these are
the summary %s across all the practices. whilst this itself is important, i think of more importance is the
variability (heterogeneity) of the %s across the practices. this is what the authors are alluding to by
focusing on the icc, but it would be far better to actually quantify the range of the %s across practices by
a 95% prediction interval for the potential true % in a new practice. this would reveal, with better clinical
meaning, the distribution of true %s. at the moment, the distribution is summarised by the range of
observed %s. but this is inappropriate as it is too wide, as it accounts for sampling error (chance) due to
a restricted sample size in each practice. therefore, a 95% prediction (akin to that in meta-analysis, see
ref [1] below) would be more intuitive, for the true %s. this can be added to table 2
[1] riley rd, higgins jp, deeks jj. interpretation of random effects meta-analyses. bmj. 2011;342:d549.
4) the design of the study also warrants clarification please. i think it is cross-sectional of sorts, but then
looks back upto 6 months (?) to ascertain prescribing practices. but what time-point is chosen for each
individual to be ‘at risk’. for example, are all times looked at, and what if the patients could be included at
multiple time-points (e.g. at risk at time 1, 5 and 6 – were the same patients then included at multiple
time-points to see if they had the outcome? and if so, how was the correlation of individuals accounted for?
in other words, the sampling frame needs to be much clearer to me. similarly, if individuals were are risk
for 2 or more prescribing errors, then how was the correlation accounted for?
or is the outcome just yes or no for each patient, with ‘yes’ meaning 1 or more errors?
e.g. confusingly the authors say ‘the proportion of patients triggering each indicator was calculated
individually’ – how can the proportion of all patients be calculated individually? do the authors mean, of the
total patients in each practice, they first identified those at risk and then, of these, which were exposed to
a prescribing error (indicator)?
are the ‘at risk’ group all patients in all practices?
5) the analyses seem well done, for example with clustering of patients within practices accounted for, and
the heterogeneity in effects (and prevalences) accounted for across practices. given the authors, i
expected this to be good. but better description of the variability across practices, in terms of the
prediction interval of the true %, is needed.
i hope my comments helps improve the article further.

<|EndOfText|>

i thank the authors for their clear response, which i am generally happy with. i will take as read the
rationale for >2.5% being clinically important, as i am not best judge in this regard. it is good to see
95% confidence intervals additionally included. minor comments remain:
1) abstract: “non-inferiority with a margin of 2.5% absolute difference in binomial proportions of 5-year
local recurrence rates, and long-term survival outcomes.” – i find the use of a % to define an absolute
difference confusing. it is a difference of 0.025, or a difference of 0.025*p, where p is the risk in the
control group? i think the former, but as it stands it is ambiguous.
2) the same issue occurs throughout the paper. e.g “it is well established that, a local recurrence
difference of less than 10% at 5 years does not worsen breast cancer survival”. does this mean a risk
difference of 0.1. or does it mean a risk difference of 0.1*p? please go through the whole paper and
clarify this language.
3) the authors do not always revise the paper to address the comments raised. that is, the comments
in their reply are satisfactory, but the revision does not always reflect this. for example, there is no
mention of the examination of non-proportional hazards in the revision, but it is detailed in the
response. also, in response to the sample size queries, they authors say “the interim analysis confirmed
the safety of targitiort, but the follow up was relatively short. therefore, the independent data
monitoring committee recommended that we should continue recruitment so as to increase the length of
follow up for the next planned analysis. in this way, the patients (50% of those eligible) would still have
access to targitiort within the randomised trial.” however, this information is not in the revision.
also, i asked them to display a forest plot of the country-specific results; this is provided in their
response (and is very useful for the reader) but not presented in the revision (nb the forest plot does
not label the names of the countries, which is needed also convert to hr scale).
in summary, i thank the authors again for their response and for dealing with these final comments.
best wishes, richard riley


<|EndOfText|>

i am happy with the majority of this revision, and i thank the reviewers for replying so clearly.
however, i would like them to address one final point regarding missing data. the authors now clarify
that they used the missing indicator method to deal with missing data. even though missing data is a
small % for each covariate, it still represents a large number of people (they say: 1126 women had
missing alcohol use (gm/day), 833 women had missing bmi, 417 women had missing physical activity
(mets/wk), 818 had missing postmenopausal hormones use,141 had missing oral contraceptive use,
and 161 had missing smoking status at baseline in 1989. family history of mi,
aspirin/nsaid/acetaminophen use, history of hypertension , elevated cholesterol or diabetes had no
missing values). unfortunately, the missing indicator method is a very poor approach for dealing with
missing data in observational studies. please see [1]. they say in this article that: ‘in nonrandomized
studies, the factor or test under study is often related to variables with missing values, in which case
the missing-indicator method typically results in biased estimates’ and ‘although the missing-indicator
method was originally proposed for missing confounder data in etiologic research, its use should be
limited to randomized trials only’.
therefore i must recommend that the authors rather use multiple imputation. i would be very surprised
if this changed the conclusions, though confidence intervals may be wider. nevertheless, it is important
– for the robustness of the article and to ensure the quality for the bmj reader – that the authors rather
consider multiple imputation for dealing with missing covariate data. this is readily available in

statistical software, and i envisage (hope) that it is not a difficult undertaking, especially given that i
have no further concerns apart from this.
i hope the authors find this constructive. best wishes, richard riley
[1] groenwold rh, white ir, donders ar, carpenter jr, altman dg, moons kg. missing covariate data
in clinical research: when and when not to use the missing-indicator method for analysis. cmaj :
canadian medical association journal = journal de l'association medicale canadienne. 2012;184:12659.

<|EndOfText|>

thank you for the opportunity to review this interesting paper, on clearly an important topic.
i have been through this from a statistical perspective, and have a number of comments and
suggestions for improvement, as follows.
1) the authors should use prisma-nma, not prisma
(http://www.prisma-statement.org/extensions/networkmetaanalysis.aspx)
2) i-squared is not a test of heterogeneity, and indeed is a poor direct measure of
heterogeneity. (rucker g, schwarzer g, carpenter jr, et al. undue reliance on i(2) in
assessing heterogeneity may mislead. bmc med res methodol 2008;8:79)
3) the authors use stata and the mvmeta module; do they actually mean they used the
network module (which uses mvmeta in the background)?
4) if relevant, how were multiple intervention effects from the same study handled in the
analysis (i.e. was their correlation accounted for)?
5) what assumptions were made about the specification of the between-study variance
matrix components? e.g. were between-study variances made equal and correlations set to
0.5, as is standard?
6) was a random effects meta-analysis used in the network meta-analysis, as in the
pair-wise analyses? was the uncertainty of between-study variance estimates accounted for
when deriving subsequent cis for summary results? e.g. using hartung-knapp
sidik-jonkman approach?
7) what estimation method was used for the network meta-analyses? reml?
8) stata should be stata

9) “we applied a 0.5 zero-cell correction only in the pairwise meta-analysis as a default of
the stata meta command but not in the network-meta-analysis to obtain a more unbiased
estimation.” – i don’t think adding 0.5 in the pair-wise analysis is as appropriate as using the
sweeting correction. (sweeting mj, sutton aj, lambert pc. what to add to nothing? use and
avoidance of continuity corrections in meta-analysis of sparse data. stat med
2004;23(9):1351-75)
moreover, i do not think the 2-stage framework is correct when outcomes are rare, and a
1-stage model is more exact and appropriate. that is, the mvmeta module in stata requires
treatment effect estimates and their variances to be calculated for each study, and these are
then pooled in a meta-analysis. however, when the event rate is low, there is a concern that
such effect estimates are not normally distributed and variances are poorly estimated. this,
a one-stage network meta-analysis that uses the exact binomial likelihood might be
preferred. did the authors consider this, or evaluate if their conclusions are robust to this
issue?
see for example:
1. riley rd, jackson d, salanti g, burke dl, price m, kirkham j, et al. multivariate and
network meta-analysis of multiple outcomes and multiple treatments: rationale, concepts,
and examples. bmj. 2017;358:j3932.
2. salanti g, higgins jp, ades ae, ioannidis jp. evaluation of networks of randomized trials.
stat methods med res. 2008;17(3):279-301.
10) page 12: met-analysis should be meta-analysis
11) “we evaluated the potential inconsistencies… “ – more details are needed on what
criteria they used to confirm consistency or inconsistency. these results should also be
provided in the main text, as this is a fundamental part of a network meta-analysis.
12) it is not clear if the meta-regression described in the methods relates to the network
meta-analysis or the pair-wise meta-analysis.
regardless, meta-regression is very prone to study-level confounding, so i would class these
as an exploratory analysis. in particular, the association of mean prostate volume and
overall treatment effect is at the ecological level – what we really need is the association
between individual prostate volume and individual treatment response.
this could only be ascertained from ipd and within-trial information, and so i strongly
suggest the meta-regression of prostate volume is downplayed.
a nice paper in the bmj on this recently is fisher (fisher dj, carpenter jr, morris tp, et al.
meta-analytical methods to identify who benefits most from treatments: daft, deluded, or
deft approach? bmj 2017;356:j573). also see: hua h, burke dl, crowther mj, et al.
one-stage individual participant data meta-analysis models: estimation of
treatment-covariate interactions must avoid ecological bias by separating out within-trial and
across-trial information. stat med 2017;36(5):772-89. doi: 10.1002/sim.7171
13) multiple time-points are considered. was the correlation across time-points accounted
for? or was a separate network meta-analysis done at each time-point? if the latter, then
were most time-points available in most studies, such that missing time-points is not a big
issue?
14) i find table 2 hard to follow. why are the authors using dichotomised values of prostate
volume here?
15) sometimes in the text the comparator group is difficult to identify

16 we need ranking plots added, and information about mean rank and sucras, to help
summarise the network meta-analysis results in more detail.
17) for the continuous outcomes, we need more details on whether the effect estimates
were appropriately derived from analysis of covariance (i.e. after adjusting for baseline) in
each trial, as this is the best method.[1] if not, then were effect estimates based on change
scores or final value only? and if so, how might this influence the findings?
vickers aj, altman dg. statistics notes: analysing controlled trials with baseline and follow
up measurements. bmj. 2001;323(7321):1123-4.
18) abstract conclusion says: ““the efficacy of vaporization in large prostates seems
questionable” – no results in the abstract relates to this point as far as i can tell? also, see
my comment about the concern of meta-regression of prostate volume above.
19) moreover, the definition of large is arbitrary. “in the large prostate group (mean pv >70
gm), … “ – we need to be looking at prostate volume as a continuous variable within trials
before making strong conclusions
i think this is a sufficient set of comment for the authors to consider going forward and to
inform the bmj’s decision. i hope my comments are ultimately helpful to all parties going
forward, and will enhance the hard work of the authors to this point.
best wishes, prof richard riley


<|EndOfText|>

i thank the authors for their detailed revision and response to my comments. two major
issues remain.
(i) the authors have not done any one-stage analyses that i recommended. they comment
that this is only possible with ipd. however, as they deal with binary outcomes, a
one-stage analysis is also possible with two by tables, obtainable from ipd or study
publications. see stijnen https://www.ncbi.nlm.nih.gov/pubmed/20827667 and simmonds
https://www.ncbi.nlm.nih.gov/pubmed/24823642
it is very straightforward, and crucially avoids the continuity correction issues. the authors
use the burke reference to suggest that the 1-stage and 2-stage will be similar – but
actually the burke paper strongly states that the key situation where they do differ is when
events are rare! the paper says: “for all outcome types where studies are expected to be
small, and in particular, for binary and time-to-event outcomes that are rare (or extremely
common), then a one-stage approach is preferred, as it avoids the use of approximate
normal sampling distributions, known within-study variances, and continuity corrections
that plague the two-stage approach with an inverse variance weighting.”
therefore, i strong recommend that the authors also include a one-stage analysis for
completeness. ultimately the findings in the paper may be open to criticism if this is not
included. such analyses do not need to include the double zero studies, and may even just
be placed in supplementary material, but should be discussed in the main paper.
the authors say that “in order to conduct additional analyses using the ipd, we would
need to propose our new analyses, resubmit a data request, and then re-analyze the data”
– this is concerning, as all that are required are the two by two tables which already are
used in the peto and m-h meta-analyses that are incorporated in the paper. so no new
data are needed than actually is already being used.
ii) the authors also do not address my query about the time of the events adequately (also
made by reviewer 1, comment #3). specifically, why hazard ratios are not reported (or at
least rate ratios), which account for the length of follow-up, in the ipd trials. the authors
main argument is that other articles report odds ratios, and that further analysis would
need more data access agreements etc. but the issue remains: what is the actual
time-point that the summary odds ratios relate to? as the follow-up length differs in all
trials, how do we translate the findings? odds of an outcome event by what time? e.g.
“patients treated with rosiglitazone had a 33% increased risk of a composite event
compared with controls” – this is meaningless without a time point attached to it. a quick
look suggests the follow up length varies from 20 to 260 weeks – which is a huge range.
this remains a major limitation and a waste as the ipd should allow at least rates to be
derived, and should be clearly reported as a limitation in the discussion. with the event
rates being low it is possible that odds ratios and rate ratios may be similar, but this should
be justified more formally, and again the wide range from 20 to 260 weeks suggests that
this may not be sensible. regardless, we need more reassurance about this issue, and
more context in the paper about how the time-scale corresponds to the interpretation of
the results.

minor:
1) the two sentences in the conclusion of the abstract are somewhat contradictory, as the
first stresses the association and the second raises important cautions: “results of this
comprehensive meta-analysis aggregating a multitude of trials and analysed 140 using a
variety of statistical techniques suggest that rosiglitazone is consistently associated with an
increased cardiovascular risk, especially for heart failure events. while increased
myocardial infarction risk was observed across analyses, the magnitudes of risk varied and
were attenuated through aggregation of summary-level data in addition to ipd.” the
words ‘consistently associated’ are perhaps too strong, and i suggest they rather say
something like: “the direction of summary estimates was the same for all outcomes in all
analyses, though the strength of evidence varied according to each outcome and whether
non-ipd trials were included.”
2) the message that ipd and non-ipd are somewhat different is itself a very important
message. this comes across well in the what this study adds: “among trials for which ipd
were available, we identified a greater number of myocardial infarctions and fewer
cardiovascular deaths reported in the ipd as compared to the summary- level data
reported in publications, csrs, and on clinicaltrials.gov, which suggests that ipd may be
necessary to accurately classify all adverse events when performing meta-analyses focused
on safety.” – could a shorter version of this be added to the abstract conclusion too?
3) “heterogeneity between trials was assessed using the i-squared statistics, with values
greater than 50% indicating moderate to substantial statistical heterogeneity.” – as
mentioned in previous report i-squared cannot reveal if heterogeneity is large or not. the
values of tau-squared (or tau – the between-study sd) do that. so please revise and
correct.
4) not clear in the abstract if you assume random-effects in the meta-analyses (i.e.
heterogeneity in effects between studies), which is your default in the results section.
5) the authors refer in places to the ‘lowest categories for risk of bias’ – is this low risk of
bias? it is ambiguous, so please state more clearly.
6) the inclusion of trials with no events at all (in either group) is somewhat controversial. i
know there is mixed guidance on this in the literature
(https://www.ncbi.nlm.nih.gov/pubmed/30887438 recommends inclusion, although
cochrane do not recommend it and personally i would not). i am not suggesting these
results are taken out but, for transparency, i would mention the inclusion of double zero
studies is contentious in the discussion.
in summary, two very important issues remain. whilst recognizing the large undertaking
by the authors, and the hard work clearly done by the study team, the article remains
open to criticism if these are not addressed better.
with best wishes, richard riley


<|EndOfText|>

the paper is very well written and the analyses well described. the authors have done a very good revision from their
earlier version, and in particular responded well to the truly superb review by efthimiou. i have some further suggestions
for improvement, though these are relatively minor except for comments about ors (see below) which need detailed
thought/justification. the analysis methods appear entirely appropriate and in keeping with the current literature. it is
an immense piece of work and very well reported, with excellent appendices.
major
1) ors are provided, and i think used to translate back to the absolute risk scale (assuming a particular baseline risk).
the authors are therefore assuming that ors are close approximation to rrs. but the event %s appear quite high. i
therefore worry that this is not appropriate. please can the authors justify this, or revise accordingly.
2) why are ors relevant here, when the studies have different follow up lengths? surely hazard ratios would be
preferred. indeed, the authors often refer to rates, but simply give %s, which are not rates but risks. i worry they are
confusing the two here. they say “the primary efficacy outcome was prevention of advanced metachronous neoplasia,
within 3-5 years of the index colonoscopy” – therefore, it could be that ors for 3 years are very different to ors for 5
years? if follow-up length was different for different agents, then this might aversely bias the comparisons? can the
authors address this please, either as a limitation, or to justify why ors were used and what time-point they exactly
relate to.
we certainly need to know the time in each study when the or was measured.
3) a related point, when the absolute risks are presented: what time-point is this the risk of an outcome by?
4) ors require all patients to be followed until the time-point of analysis. therefore, how were patients handled who
were censored before the time-point? i don't know whether censoring was an issue or not, but with 3-5 year follow up it
is possible.
minor
1) sucra is an unusual term for the bmj reader, and the abstract would benefit from either explaining its meaning in
more detail, or removing it.
2) introduction says: “network meta-analyses combine direct and indirect evidence to
establish comparative efficacy and safety across a network of rcts of all agents used in a particular condition” – the
authors should note the assumptions at this point also. we gain more by assuming more.
3) the authors put most of their an analysis description in the supplementary appendix. i strongly suggest this is moved
to the main paper. that is, integrate the ‘emethods’ stats analysis section to the methods of the main paper. there are
many assumptions and explanations therein that are important, and deserve to be in the paper. the bmj has no word
limit.
4) figure 1 – why are the number of studies with direct evidence not simply added to the lines?
i hope my comments help the authors improve the article going forward.
richard riley, bmj statistics editor.

<|EndOfText|>

i thank the authors for their detailed response. on reading their
response and the revision, some further statistical issues and
clarifications have arisen for them to address going forward, as
follows:
abstract: the design section needs to mention how the cohort study
was set-up or obtained (e.g. prospective, database study, etc),

years of follow-up, representativeness, and that propensity score
matching was done, and briefly how.
abstract and main results: to translate to absolute risk, in addition
to giving the absolute rates, could we also have the probability of
survival at 7 days for each group, and their difference? this would
help the reader know the probability of death is very low.
abstract: give the total number that died.
methods: i cannot see the full list of baseline characteristics given
that were used in the ps approach. it just says: “the ps is the
probability of receiving haloperidol as opposed to one of the three
atypical antipsychotics estimated by logistic regression, given the
baseline characteristics described above.” – but the latter need to
given explicitly, as i can’t find them ‘above’. do you mean to refer
to table s1? i think adjustment factors are crucial, so should be in
the main paper, not the supplementary material.
methods: “we assessed potential confounders for the planned
analyses using information from hospital admission to the day
before antipsychotic initiation.” – how were these assessed?
statistically? clinical judgement? etc
results: i do not follow the comment that “the ps distributions
were largely overlapping with a c-statistic of 0.65” – i think you
need to explain to the reader that the c-statistic is for the logistic
regression model that produces the ps values. it is not a measure
of separation of the ps values for those treated and untreated (i
think? indeed, if it was, this would be cause for concern, because a
c of 0.65 suggests considerable distinction between groups). so this
sentence should be split; explain the ps model and its c-statistic.
then says that ps values were largely overlapping (perhaps a figure
in the sup material to show the 2 distributions for the groups?)
results: in response to my previous comment about the need to
check for non-proportional hazards, it is an important new finding
that there is strong evidence of non-proportional hazards. “the
time by exposure interaction coefficient was statistically significant
(p=0.008), suggesting that the proportional hazards assumption is
not met in the model”. however, the implications of this are only
briefly mentioned in the results and reference given to the
supplementary material. i think more details are needed in the main
text, with results shown for the hr in day 0, 3, 5, 7. 10 as in s4.
indeed, what is rather striking, and worthy of discussion, is that the
hr is close to 1 by day 5, and then becomes < 1 by day 7 and 10,
with almost a significant result in the opposite direction at day 10.
if this is genuine, what are the implications?
i think the issue of non-ph should be given more prominence in the
paper, e.g. what this study adds and the abstract.

subgroup analyses: it is not clear why age, cci, exposure have
been dichotomised. this is not a sensible analysis strategy, and – if
the covariates are of genuine interest – a more powerful and
meaningful analysis would be to consider them on their continuous
scale (at the very least, in addition to the analyses done). this
comment was also made in my earlier review but not addressed as
far as i can tell.
results: “comparison of hrs between patients who were in the icu
(hr=1.11, 0.68-1.81) on the index date and those who were on the
medical ward (hr=2.01, 1.44-2.82) suggests that there might be
effect heterogeneity (p=0.04).” was icu / medical ward part of the
ps matching criteria?
discussion: “the potential adverse effect of haloperidol appeared to
be the strongest during the first few days following initiation,
suggesting an acute harmful effect of haloperidol” – i would tone
down the implied causal language here to something like: “the
association of haloperidol with increased rate of death appeared to
be the strongest during the first few days following initiation, which
if the association was causal, would suggest an acute harmful effect
of haloperidol”
in response to my comment about missing data, the authors say
there is not any missing data in ‘the strict sense.’ so, really no
missing data in any of the variables used in the ps matching model?
i find that hard to believe based on my personal experience, but if
so then this should be added, i.e. that there was complete data for
all individuals. however, in one subgroup analysis they say: “it
should be acknowledged, however, that there is an increased
likelihood of residual confounding in this analysis due to potentially
incomplete baseline information.” is this just the subgroup analysis
where there is missing data then?
i hope these additional comments can be addressed by the authors,
and help improve the article further.
best wishes, richard riley


<|EndOfText|>

i am happy with the authors response. they now say, in their response to me, that missing data handled using multiple
imputation led to a similar result. however, i have been through the paper and they have not added this important finding to
the paper itself. many critical readers will ask the same question as me: is the finding robust to the use of multiple imputation
rather than the missing indicator method? therefore, my last recommendation is to ensure the authors add a few sentences to
the paper accordingly about the use of multiple imputation as a sensitivity analysis (in the methods), with subsequent results
briefly given at the end of the results.

<|EndOfText|>

the response to my comments is generally well detailed. there are still a few areas for concern, in a generally well
written and clear paper.
1) i still feel the adjustment factors are quite few. for example, for breast cancer there are only about 6 adjustment
factors in the main analyses. therefore residual confounding is a big concern. but also confusingly, the breast cancer
analyses do not adjust for smoking until the sensitivity analyses and yet the cvd analyses do. this seems entirely
unjustified, as the methods says that all known risk factors were included as adjustment factors. am i missing
something? surely this is a serious omission in the main analyses and it does not make sense to include additional
confounders only in the sensitivity analysis. why not include all potential confounders in the main analysis? this needs
to be clarified. also, perhaps i missed it, but where are the breast cancer results when smoking is included? i don’t
think these are presented.
2) why was education imputed using simple imputation and not multiple imputation itself? i do not understand the
rationale for this, unless it is obvious based on the age what the missing education would be? please clarify.
3) under the tables and in the results, can the authors clarify that when they looked at linear and non-linear (u)
shaped trends, that they did this using the actual difference in alcohol intake, and not based on the categorical
classification?
4) table 3 heading says hrs are presented, but that is not true for the first set of rows
5) i do not see a discussion on potential for residual confounding in the abstract or discussion. again, this is a crucial
omission given the limited number of confounders.


<|EndOfText|>

response 1: i am satisfied with this response. it is much clearer for me now what the model is
doing, and i agree that it allows patients with no values beyond baseline to still be included. i
had wrongly thought, from the description given, that baseline had also been adjusted for as a
continuous covariate as well as an outcome response. that would be wrong. but i can see that
your use of the word ‘factor’ relates to just an indicator variable, so that you can see the mean
baseline scores and contrast them with follow up scores. this is acceptable, and i am happy
now.
response 2: again, fine, thank you.
i have no further comments, and i thank the authors for their consideration.
professor riley, bmj statistics editor

<|EndOfText|>

the response and changes to my previous comments are generally very clear, thank you. i
especially thank the authors for now also providing the absolute risk at 7 days as
requested. the difference in survival probability by 7 days is tiny, 0.93 versus 0.94, which
is very important for the bmj reader to appreciate.
my main remaining comment relates to the missing data. i asked the authors about the
amount of missing data, but the response is not satisfactory. is there missing data or not?
i cannot identify a clear answer to this. they note the database is considered high quality,
and have added the following to the paper: “it has been reported that there is less than
one percent of records that have missing information and less than 0.01 percent of records
that have missing key elements (e.g. demographics, diagnostics).” this is great, but the
use of ‘it has been reported’ suggests the authors are referring to previous work that uses
this database. i want to know, is there missing data in the data they obtained for analysis
in this paper? if so, how much, and how was it handled (e.g. complete case analysis?).
they also say: “in administrative databases, there are essentially no missing data in the
conventional sense (in contrast to cohort studies with ad hoc data collection for example).”

– but i work with administrative (routinely collected) databases regularly, and there is
routinely missing data. for example, smoking status is not recorded for some patients, or
bmi is not available, etc.
lastly, figure 1 does not suggest an interaction between the hr and time. yet this is a key
finding; i.e. the hr gets smaller after the first few days. i can therefore only assume this
graph is produced using the proportion hazards assumption, even though there is strong
evidence that this assumption is not correct. the graph should be corrected accordingly. or
rather, include a second graph with the cumulative incidence estimated based on the
non-proportional hazards approach.
best wishes, richard riley
