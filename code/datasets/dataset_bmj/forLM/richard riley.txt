a guide to systematic review and meta-analysis of prognostic factor studies prognostic factors are associated with the risk of future health outcomes in individuals with a particular health condition or some clinical start point (eg, a particular diagnosis). research to identify genuine prognostic factors is important because these factors can help improve risk stratification, treatment, and lifestyle decisions, and the design of randomised trials. although thousands of prognostic factor studies are published each year, often they are of variable quality and the findings are inconsistent. systematic reviews and meta-analyses are therefore needed that summarise the evidence about the prognostic value of particular factors. in this article, the key steps involved in this review process are described. systematic reviews and meta-analyses are common in the medical literature, routinely appearing in specialist and general medical journals, and forming the cornerstone of cochrane. the majority of systematic reviews focus on summarising the benefit of one or more therapeutic interventions for a particular condition. however, they are also important for summarising other evidence, such as the accuracy of screening and diagnostic <UNK> the causal association of risk factors for disease onset, and the prognostic ability of bespoke factors and biomarkers. prognostic evidence arises from prognosis studies, which aim to examine and predict future outcomes (such as death, disease progression, side effects or medical complications like pre-eclampsia) in people with a particular health condition or start point (such as those developing a certain disease, undergoing surgery, or women who are pregnant). the progress (prognosis research strategy) framework defines four types of prognosis research objectives: (a) to summarise overall prognosis (eg, overall risk or rate) of health outcomes for groups with a particular health <UNK> (b) to identify prognostic factors associated with changes in health <UNK> (c) to develop, validate, and examine the impact of prognostic models for individualised prediction of such <UNK> and (d) to identify predictors of an individual’s response to <UNK> each objective requires specific methods and tools for conducting a systematic review and meta-analysis. two recent articles provided a guide to undertaking reviews and meta-analysis of prognostic (prediction) <UNK> in this article, we focus on prognostic factors. a prognostic factor is any variable that is associated with the risk of a subsequent health outcome among people with a particular health condition. different values or categories of a prognostic factor are associated with a better or worse prognosis of future health outcomes. for example, in many cancers, tumour grade at the time of histological examination is a prognostic factor because it is associated with time to disease recurrence or death. each grade represents a group of patients with a different prognosis, and the risk or rate (hazard) of the outcome increases with higher grades. many routinely collected patient characteristics are prognostic, such as sex, age, body mass index, smoking status, blood pressure, comorbidities, and symptoms. many researched prognostic factors are biomarkers, which include a diverse range of blood, urine, imaging, electrophysiological, and physiological variables. prognostic factors have many potential uses, including aiding treatment and lifestyle decisions, improving individual risk prediction, providing novel targets for new treatment, and enhancing the design and analysis of randomised <UNK> this motivates so-called “prognostic factor research” to identify genuine prognostic factors (sometimes also called “predictor finding <UNK> although thousands of such studies are published each year, often they are of variable quality and have inconsistent findings. systematic reviews and meta-analyses are therefore urgently needed to summarise the evidence about the prognostic value of particular <UNK> in this article, we provide a step-by-step guide on conducting such reviews. our aim is to help readers, healthcare providers, and researchers understand the key principles, methods, and challenges of reviews of prognostic factor studies. summary points primary studies to identify prognostic factors are abundant, but often findings are inconsistent and quality is variable. systematic reviews and meta-analyses are urgently needed to identify, evaluate, and summarise prognostic factor studies and their findings. a clear review question should be defined using the picots system (population, index prognostic factor, comparator prognostic factors, outcome, timing, setting), and a transparent search undertaken for eligible articles. broad search strings may be required, leading to a large number of articles to screen. a data extraction phase is needed to obtain the relevant information from each study. a modification of charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) can be used for prognostic factors (charms-pf). the quips tool (quality in prognostic factor studies) can be used to examine each study’s risk of bias. unfortunately, many primary studies may have a high risk of bias because of poor design standards, conduct, and analysis. applicability of a study should also be checked. if appropriate, meta-analysis can be used to combine prognostic effect estimates (such as hazard ratios, risk ratios, or odds ratios) across studies to produce an overall summary of a factor’s prognostic effect. between-study heterogeneity should be expected and accounted for. ideally separate meta-analyses should be performed for unadjusted and adjusted prognostic effect estimates; adjusted estimates are important to examine a factor’s independent prognostic value over and above (that is, after adjustment for) other prognostic factors. separate meta-analyses may also be required for each method of measurement (for factors and outcomes), each approach to handling continuous factors, and each type of estimate (such as hazard ratios or odds ratios). publication bias is a major threat to the validity of meta-analyses of prognostic factor studies based on published evidence, and may cause small-study effects (asymmetry on a funnel plot). remark (reporting recommendations for tumour marker prognostic studies) and prisma (preferred reporting items for systematic reviews and meta-analyses) can be used to guide the reporting of the systematic review and meta-analysis of prognostic factor studies; the degree of confidence in the summary results from the review may be examined by use of adapted forms of grade (grades of recommendation, assessment, development, and evaluation) for interventions and diagnostic test accuracy studies. availability of individual participant data from primary prognostic factor studies may alleviate many of the challenges. step <UNK> defining the review question the first step is to define the review question. a review of prognostic factor studies falls within the second objective of the progress <UNK> because it aims to summarise the prognostic value of a particular factor (or each of multiple factors) for relevant health outcomes and time points in people with a specific health condition (eg, disease). some reviews are broad; for example, riley and colleagues aimed to identify any prognostic factor for overall and disease free survival in children with neuroblastoma or ewing’s <UNK> other reviews have a narrower focus; for example, hemingway and colleagues aimed to summarise the evidence on whether c reactive protein (crp) is a prognostic factor for fatal and non-fatal events in patients with stable coronary <UNK> this crp review is used as an example throughout this article. charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling studies) provides guidance for formulating a review question (table <UNK> in the article by moons and <UNK> although charms was <UNK> and <UNK> for reviews of prediction model studies, it can also be used to define and frame the question for reviews of prognostic factor studies. <UNK> and subsequent <UNK> propose a modification of the traditional pico system (population, index intervention, comparison, and outcome) used in systematic reviews of therapeutic intervention studies. the modification is called picots, because it also considers timing and setting (box <UNK> in the context of prognostic factor reviews, the “p” of population and the “o” of outcome remain largely the same as in the original pico system, but the “i” refers to index prognostic factors and the “c” refers to other prognostic factors that can be considered as comparators in some way. for example, the aim may be to compare the prognostic ability of a certain index factor with one or more other (that is, comparator) prognostic factors; or to investigate the adjusted prognostic value of a particular index factor over and above (adjusted for) other (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, which is not generally recommended, then no comparator factor is being considered. the “t” denotes timing and refers to two concepts of time. firstly, at what time point the prognostic factors under review are to be measured or assessed (that is, the time point at which prognosis information is required); and secondly, over what time period the outcomes are predicted by these factors. the “s” of setting refers to the setting or context in which the index prognostic factors are to be used because the prognostic ability of a factor may change across healthcare settings. table <UNK> charms-pf checklist of key items to be extracted from primary studies of prognostic factors, based on additions and modifications of the original charms checklist for primary studies of prediction <UNK> view popupview inline box <UNK> six items (picots) defining the question for systematic reviews of prognostic factor studies, based on charms (checklist for critical appraisal and data extraction for systematic reviews of prediction modelling <UNK> and applied to a review of the adjusted prognostic value of c reactive protein <UNK> population: define the target population for which prognostic factors under review are to be used. for example, crp review: patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of acute coronary syndrome at least two weeks before prognostic factor (crp) measurement. index prognostic factor: define the factors for which prognostic value is under review. for example, crp review: crp was the single biomarker reviewed for its prognostic value. comparator prognostic factors: comparator prognostic factors can be considered in a review in various ways. for example, the aim could be to compare the prognostic ability of a certain index factor with two or more other (that is, comparator) prognostic factors; or to review the adjusted prognostic value of a particular index factor—that is, over and above (adjusted for, independent of) other existing (that is, comparator) prognostic factors. if the only aim is to summarise the unadjusted prognostic effect of a particular index factor, then no comparator factor is being considered. for example, crp review: the focus was on the adjusted prognostic value of crp—that is, its prognostic effect after adjusting for existing (comparator) prognostic factors. in particular, adjustment for the following conventional prognostic factors was of interest: age, sex, smoking status, obesity, diabetes, and one or more lipid variables (from total cholesterol, low density lipoprotein cholesterol, high density lipoprotein cholesterol, triglycerides) and inflammatory markers (fibrinogen, interleukin <UNK> white cell count). outcome: define the outcomes for which the prognostic ability of the factor(s) under review are of interest. for example, crp review: outcome events were defined as coronary (coronary death, sudden cardiac death, acute non-fatal myocardial infarction, primary percutaneous coronary intervention, unplanned emergency admissions with unstable angina), cardiovascular (when coronary events were reported in combination with heart failure, stroke, or peripheral arterial disease), and all cause mortality. timing: define firstly at what time points the prognostic factors (index and comparators) are to be used (that is, the time point of prognostication), and secondly over what time period the outcomes are predicted by these factors. for example, crp review: the crp measurement had to be done at least two weeks after diagnosis and all follow-up information on the outcomes (all time periods) was extracted from the studies. setting: define the intended setting and role of the prognostic factors under review. for example, crp review: crp measurement was studied in primary and secondary care to provide prognostic information about patients diagnosed with coronary heart disease; this information may be useful for healthcare professionals treating and managing such patients. return to text an important component of reviews of prognostic factors is whether unadjusted or adjusted estimates of the index prognostic factors will be summarised, or both. we recommend that reviewers primarily focus on adjusted prognostic factor estimates because they reveal whether a certain index factor contributes independently to the prediction of the outcome over and above (that is, after adjustment for) other prognostic factors. in particular, for each clinical scenario there are often so-called “established” or “conventional” prognostic factors that are always measured. therefore, for prognostic factors under review, it is important to understand whether they contribute additional (sometimes called “independent”) prognostic information to the routinely measured ones. this means that reviewers need adjusted (and not unadjusted or crude) prognostic effect estimates to be estimated and reported in primary prognostic factor studies. such adjusted prognostic estimates are typically derived from a multivariable regression model containing the established prognostic factors plus each index prognostic factor of interest. for example, consider a logistic regression of a binary outcome including three adjustment factors <UNK> <UNK> and <UNK> and one new index prognostic factor <UNK> which is expressed as: <UNK> = <UNK> here, “p” is the probability of the outcome. after estimation of all the unknown parameters (that is, α, <UNK> <UNK> <UNK> <UNK> of key interest is the estimated <UNK> this parameter provides the adjusted prognostic effect of the index prognostic factor and reveals its independent contribution to the prediction of the outcome over and above the prognostic effects of the other (established comparator) factors <UNK> <UNK> and <UNK> combined. the need to focus on adjusted prognostic effects is no different from (systematic reviews of) aetiological studies, in which the focus is on estimating the association of a certain causal risk factor after adjustment for other risk factors. in such causal research, these factors are usually referred to as “confounders” rather than as “other prognostic factors,” which is the term typically used for prognosis research. the crude (unadjusted) prognostic effect of some index factors may completely disappear after adjustment and is therefore rather uninformative, especially because prognostication in healthcare is rarely based on a single prognostic factor but rather on the information from multiple prognostic <UNK> this article focuses on systematic reviews to summarise prognostic factor effect estimates. some primary studies may also evaluate an index factor’s added value in terms of improvement in risk classification and clinical use (eg, measures such as net reclassification improvement and net benefit), and change in prediction model performance (eg, by calculating the change in the concordance index, also known as the c statistic or area under the receiver operating characteristics (roc) <UNK> however, this is beyond the scope of this article, and we refer the reader to other relevant <UNK> application to crp review crp is widely studied for its prognostic value in patients with coronary disease. however, there is uncertainty whether crp is useful because us and european clinical practice guidelines recommend measurement but clinical practice varies widely. this uncertainty motivated the systematic review by hemingway and <UNK> with the corresponding picots system presented in box <UNK> no studies were excluded on the basis of methodological standards, sample size, duration of follow-up, publication year, or language of publication. step <UNK> searching for and selection of eligible studies the next step is to identify primary studies that are eligible for review; studies that address the review question defined in step <UNK> following the picots framework. unfortunately, it is more difficult to identify prognostic factor studies than randomised trials of interventions. prognosis studies do not tend to be indexed (“tagged”) because a taxonomy of prognosis research is not widely recognised. moreover, compared with studies of interventions, there is much more variation in the design of prognostic factor studies (eg, data from cohort studies, randomised trials, routine care registries, and case-control studies can all be used), patient inclusion criteria, prognostic factor and outcome measurement, follow-up time, methods of statistical analysis, and adjustment of (and number of) other prognostic factors (covariates). between-study heterogeneity is therefore the rule rather than the exception in prognostic factor research. it is essential that systematic reviews of prognostic factor studies define the study inclusion and exclusion criteria based on the picots structure (step <UNK> because this determines the study search and selection strategy. typically, broad search and selection filters are required that combine terms related to prognosis research (such as prognostic, predict, predictor, factor, independent) with domain or disease specific terms (such as the name of prognostic factors and the targeted disease or patient <UNK> a broad search comes at the (often considerable) expense of retrieving many irrelevant records. geersing and <UNK> validated various existing search strategies for prognosis studies and suggested a generic filter for identifying studies of prognostic <UNK> which extended the work of ingui, haynes, and <UNK> when tested in a single review of prognostic factors, this generic filter had a number needed to read of <UNK> to identify one relevant article, emphasising the difficulty in targeting prognostic factor articles. the number needed to read could be considerably reduced when specific factors or populations are added to the filter. even then, care is needed to be inclusive because multiple terms are often used for the same meaning; for example, biomarker mycn is also referred to as n-myc and nmyc, among other <UNK> once the search is complete, each potentially relevant study must be screened for its applicability to the review question. because of the heterogeneity in prognostic factor studies, during this study selection phase more deviations from the defined picots (in step <UNK> are possible (far greater than what is typically encountered during the selection of randomised intervention studies). the applicability of this primary study selection should firstly be based on title and abstract screening, followed by full text screening, both ideally done by two researchers independently. any discrepancies should be resolved through discussion, potentially with a third reviewer. to check if any relevant articles have been missed, it is helpful to share the list of identified articles with researchers in the field to examine the reference lists of these articles and to perform a citation search. application to crp review hemingway and colleagues included any prospective observational study that reported risk of subsequent events among patients with stable coronary disease in relation to measured crp <UNK> eligible studies had to include patients with stable coronary disease, defined as clinically diagnosed angina pectoris or angiographic disease, or a history of previous acute coronary syndrome at least <UNK> weeks before crp measurement. hemingway and colleagues searched medline between <UNK> and <UNK> november <UNK> and embase between <UNK> and <UNK> december <UNK> using a search string containing terms for coronary disease, prognostic studies, and crp. the search identified <UNK> articles, of which <UNK> fulfilled the inclusion criteria. if specific terms for crp had not been included in the search string, then the total number of identified articles would have far exceeded <UNK> step <UNK> data extraction the next step is to extract key information from each selected study. data extraction provides the necessary data from each study, which enables reviewers to examine their (eventual) applicability to the review question and their risk of bias (see step <UNK> this step also provides the information required for subsequent qualitative and quantitative (meta-analysis) synthesis of the evidence across studies. the charms checklist gives explicit guidance (table <UNK> in the article by moons and <UNK> about which key items across <UNK> domains should be extracted from primary studies of prediction models, and for what reason (that is, to provide general information about the primary study, to guide risk of bias assessment, or to assess applicability of the primary study to the review question). based on our experience of conducting systematic reviews of prognostic factor studies, we modified the original charms checklist for prediction model studies to make it suitable for data extraction in reviews of prognostic factors (here referred to as charms-pf; table <UNK> this basically means that three domains typically addressing multivariable prediction modelling aspects were combined to one overall analysis domain, while other domain names and key items were slightly reworded or extended. reasons for extraction of each key item are similar to charms for prediction models. because we developed the original charms checklist, a wider consensus of the charms-pf content was not considered necessary. table <UNK> quips tool (quality in prognostic factor studies), which can be used to classify risk of bias of prognostic factor studies view popupview inline reviewers should extract fundamental information from the primary prognostic factor studies, such as the dates, setting, study design, definitions of start points, outcomes, follow-up length, and prognostic factors; reviewers will often find large heterogeneity in this information across studies. the extracted information can be summarised in tables of study characteristics. in addition, more specific information is needed to properly assess applicability and risk of bias (see step <UNK> such as methods used to measure prognostic factors and outcomes, handling missing data, attrition (loss to follow-up), and whether estimated associations of the prognostic factors under review were adjusted for other prognostic factors. this information also enhances the potential for meta-analysis and the presentation and interpretation of subsequent summary results (see steps <UNK> to enable meta-analysis of prognostic factor studies, the key elements to extract are estimates, and corresponding standard errors or confidence intervals, of the prognostic effect for each factor of interest; for example, the estimated risk ratio or odds ratio (for binary outcomes), hazard ratio (for time-to-event outcomes), or mean difference (for continuous outcomes). as most prognostic factor studies consider time-to-event outcomes (including censored observations and different follow-up lengths for patients), hazard ratios are often the most suitable effect measure. a concern is that hazard ratios may not be constant over time, and therefore any evaluations of non-proportional hazards (that is, non-constant hazard ratios for the prognostic factors of interest) should also be extracted; however, such information is rarely reported in sufficient detail. unfortunately, many prognostic factor studies do not adequately report estimated prognostic effect measures or their precision. for this reason, methods are available to restore the missing information upon data extraction. in particular, parmar and <UNK> and tierney and <UNK> describe how to obtain unadjusted hazard ratio estimates (and their variances) when they are not reported directly. for example, under assumptions, the number of outcomes (events) and an available p value (eg, from a log rank test or cox regression) can be used to indirectly estimate the unadjusted hazard ratio between two groups defined by a particular factor (eg, “high” versus “normal” levels). perneger and <UNK> report how to derive unadjusted hazard ratios from survival proportions, and pérez and colleagues suggest using a simulation <UNK> even with such indirect estimation methods, not all results can be obtained. for example, in a systematic review of <UNK> studies investigating prognostic factors in <UNK> the methods of parmar and colleagues were used to obtain <UNK> hazard ratio estimates and their confidence intervals; but this represented only <UNK> of the potential evidence. although indirect estimation methods help retrieve unadjusted prognostic factor effect estimates, they often have limited value for obtaining adjusted effect estimates. furthermore, even when multiple studies provide the adjusted prognostic effect of a particular factor, the set of adjustment factors will usually differ across studies, which complicates the interpretation of subsequent meta-analysis results. we recommend that reviewers predefine the core set of prognostic factors for the outcome of interest (eg, age, sex, smoking status, disease stage) that represents the desired “minimal” set of adjustment factors. an agreed process among health professionals and researchers in the field could be required to define this set. for example, a list of established prognostic factors could be identified that are routinely used within current prognostication of the clinical population of interest. it may also be necessary to standardise the extracted estimates to ensure they all relate to the same scale and direction in each study. in particular, the direction of the prognostic effect will need standardising if one study compares the hazard rate in a factor’s “high” versus “normal” group, whereas another study compares the hazard rate in the factor’s “normal” versus “high” group. when the outcome is defined differently across studies, approaches to convert effect measures on different outcome scales could be <UNK> also, to deal with different cutpoint levels for values of a particular <UNK> the prognostic effects of “high” versus “normal” could be converted to prognostic effects relating to a <UNK> unit increase in the factor. this requires assumptions about the underlying distribution of the factor. such an approach was used by hemingway and <UNK> of concern, however, is that the actual distribution of a prognostic factor may be unknown (or even vary across studies). finally, it is also possible to derive standardised effect estimates by standardising the corresponding regression <UNK> application to crp review hemingway and colleagues extracted background information such as year of study start, number of included patients, mean age, baseline coronary morbidity (eg, proportion with stable angina), average levels of biomarker at baseline, method of crp measurement, follow-up duration, and number and type of events. basic information was often missing. for example, nearly a fifth of studies did not report the method of measurement, and only a quarter gave the number of patients included in the analyses and reasons for dropout. prognostic effect estimates for crp were extracted in terms of the reported risk ratio, odds ratio, or hazard ratio (labelled generally as “risk ratio” in this article), and <UNK> confidence intervals. these effect estimates were then converted to a standardised scale comparing the highest third with the lowest third of the (log transformed) crp distribution. if available, separate prognostic effect estimates were extracted for different degrees of adjustment for other prognostic factors. step <UNK> evaluating applicability and risk of bias of primary studies once eligible studies are identified and data are extracted, an important next step is to assess the applicability and risk of bias (quality) of each study in the review. as for steps <UNK> and <UNK> ideally this is done by two reviewers, independently, with any discrepancies resolved. applicability refers to the extent to which a selected study (in step <UNK> matches the review question in terms of the population, timing, prognostic factors, and outcomes (endpoints) of interest. just because a study is eligible for inclusion does not mean it is free from applicability concerns. some aspects of a study may be applicable (eg, correct condition at start point, with prognostic factors of interest evaluated) but not others (eg, incorrect population or setting, inappropriate outcome definition, different follow-up time, lack of adjustment for conventional prognostic factors). applicability is typically first assessed during title and abstract screening, and then during this step, so that it is based on full text screening and determined by picots (step <UNK> and inclusion and exclusion criteria of studies (step <UNK> risk of bias refers to the extent to which flaws in the study design or analysis methods could lead to bias in estimates of the prognostic factor effects. unfortunately, based on growing empirical evidence from systematic reviews examining methodology quality, many primary studies will be at high risk of <UNK> for prognostic factor studies, hayden and colleagues developed the quips checklist (quality in prognostic factor studies) for examining risk of bias across six <UNK> study participation, study attrition, prognostic factor measurement, outcome measurement, adjustment for other prognostic factors, and statistical analysis and reporting. table <UNK> shows the signalling items within these domains to help guide reviewers in making low, unclear, or high risk of bias classifications. additional guidance may be found in general tools examining the quality of observational <UNK> and the remark guideline (reporting recommendations for tumour marker prognostic studies) for reporting of primary prognostic factor <UNK> we recommend that users first operationalise criteria to assess the signalling items and domains for the specific review question. for example, with the study participation and attrition domains, this includes defining a priori the most important characteristics that could indicate a systematic bias in study recruitment (study participation domain) and loss to follow-up (study attrition domain). defining these characteristics ahead of time will facilitate assessment and consensus related to the importance of potential differences that could influence the observed association between the index prognostic factors and outcomes of interest. definitions of sufficiently valid and reliable measurement of the index prognostic factors and outcomes should also be specified at the protocol stage. similarly, the core set of other (adjustment) prognostic factors that are deemed necessary for the primary studies to have adjusted for, should be predefined to facilitate judgment related to risk of bias in domain <UNK> overall assessment of the six risk of bias domains is undertaken by considering the risk of bias information from the signalling items for each domain, rated as low, moderate, and high risk of bias. occasionally, item information needed to assess the bias domains is not available in the study report. when this occurs, other publications that may have used the same dataset (which often occurs in prognostic studies based on large existing cohorts) should be consulted and study authors should be contacted for additional information. an informed judgment about the potential risk of bias for each bias domain should be made independently by two reviewers, and discussed to reach consensus. each of the six domains needs to be rated and reported separately because this will inform readers, flag improvements needed for subsequent primary studies, and facilitate future meta-epidemiological research. we recommend defining studies with an overall “low risk of bias” as those studies where all, or the most important domains (as determined a priori), are rated as having low (or low to moderate) risk of bias. application to crp review hemingway and colleagues assessed the quality of included studies by the quality of their reporting on <UNK> items derived from the remark <UNK> the median number of study quality items reported was seven of a possible <UNK> and standards did not change between <UNK> and <UNK> only two studies referred to a study protocol, with none referring to a statistical analysis plan. hemingway and colleagues noted that this “makes it difficult to know what the specific research objectives were at the start of cohort recruitment, at the time of crp measurement, or at the onset of the statistical <UNK> only two studies reported the time elapsed between first lifetime presentation with coronary disease and assessment of crp and this raised applicability concerns. step <UNK> meta-analysis meta-analysis of prognostic factor studies aims to summarise the (adjusted) prognostic effect of each factor of interest. in addition to missing estimates, challenges for the meta-analyst include (a) having different types of prognostic effect measures (eg, odds ratios and hazard ratios), which are not necessarily <UNK> (b) estimates without standard errors, which is a problem because meta-analysis methods typically weight each study by (a function of) their standard error; (c) estimates relating to various time points of the outcome occurrence or measurement; (d) different methods of measurement for prognostic factors and outcomes; (e) various sets of adjustment factors; and (f) different approaches to handling continuous prognostic factors (eg, categorisation, linear, non-linear trends), including the choice of cutpoint value when dichotomising continuous values into “low” and “normal” groups. many of these issues lead to substantial heterogeneity and if a meta-analysis is performed, summary results cannot be directly interpreted. generally, meta-analysis results will be most interpretable, and therefore useful, when a separate meta-analysis is undertaken for groups of “similar” prognostic effect measures. in particular, we suggest considering a meta-analysis for: hazard ratios, odds ratios, and risk ratios separately unadjusted and adjusted associations separately prognostic factor effects at distinct cutpoints (or groups of similar cutpoints) separately prognostic factor effects corresponding to a linear trend (association) separately prognostic factor effects corresponding to non-linear trends separately each method of measurement (for factors and outcomes) separately. ideally a meta-analysis of adjusted results should ensure that all included estimates are adjusted for the same set of other prognostic factors. this situation is unlikely and so a compromise could be to ensure that all adjusted estimates in the same meta-analysis have adjusted for at least a (predefined) minimum set of adjustment factors (that is, a core set of established prognostic factors). even when adhering to this guidance, unexplained heterogeneity is likely to remain because of other reasons (eg, differences in length of follow-up or in treatments received during follow-up). therefore, if a meta-analysis is performed, a random effects approach is essential to allow for unexplained heterogeneity across studies (box <UNK> as previously described in the <UNK> this approach provides a summary estimate of the average prognostic effect of the index factor and the variability in effect across studies. also potentially useful are meta-analysis methods to estimate the trend (eg, linear effect) of a prognostic factor that has been grouped into three or more categories within studies (with each category compared with the reference category). these methods generally model the estimated prognostic effect sizes in each category as a function of “exposure” level (eg, midpoint or median prognostic factor value in the category) and account for within-study correlation and between-study <UNK> to apply these methods, some additional knowledge of the factor’s underlying distribution is usually needed to help define the “exposure” level because the chosen value can have an impact on the <UNK> box <UNK> explanation of a random effects meta-analysis of prognostic factor effect estimates the true prognostic effect of a factor is likely to vary from study to study; therefore assuming a common (fixed) prognostic effect is not sensible. if yi and var(yi) denote the prognostic effect estimate (eg, ln(hazard ratio), ln(odds ratio), ln(risk ratio), or mean difference) and its variance in study i, then a general random effects meta-analysis model can be specified as: yi <UNK> most researchers use either restricted maximum likelihood or the approach of dersimonian and laird to estimate this <UNK> but other options are available, including a bayesian <UNK> of key interest is the estimate of μ, which reveals the summary (average) prognostic effect of the index prognostic factor of interest. the standard deviation of this prognostic factor effect across studies is denoted by τ, and non-zero values suggest there is between-study heterogeneity. confidence intervals for µ should ideally account for uncertainty in estimated variances (in particular <UNK> and we have found the approach of hartung-knapp to be robust for this purpose in most <UNK> when synthesising prognostic effects on the log scale, the summary results and confidence intervals require back transformation (using the exponential function) to the original scale. return to text advanced multivariate meta-analysis methods are also available to handle multiple <UNK> multiple methods of <UNK> or different adjustment factors in prognostic factor <UNK> an introduction to multivariate meta-analysis has been published in the <UNK> application to crp review hemingway and <UNK> applied a random effects meta-analysis to combine <UNK> adjusted prognostic effect estimates for crp from studies that adjusted for at least one of six conventional risk factors (age, sex, smoking status, diabetes, obesity, and lipids). the summary meta-analysis result was a risk ratio of <UNK> <UNK> confidence interval <UNK> to <UNK> which gives the average prognostic effect of crp (for those in the top v bottom third of crp distribution), and suggests larger crp values are associated with higher risk. although there was substantial between-study heterogeneity, nearly all estimates were in the same direction (that is, risk ratio <UNK> when restricting meta-analysis to just the <UNK> studies that adjusted for at least all six conventional prognostic factors, the summary risk ratio decreased to <UNK> <UNK> confidence interval <UNK> to <UNK> and the between-study heterogeneity reduced. using the study specific estimates given by hemingway and colleagues, we updated this meta-analysis (fig <UNK> obtaining the same summary result but a wider confidence interval <UNK> to <UNK> through the hartung-knapp <UNK> fig <UNK> fig <UNK> forest plot showing the study specific estimates and meta-analysis summary result of the adjusted prognostic effect (risk ratio) of c reactive protein taken from the review of hemingway and <UNK> all studies were adjusted for a core set of existing prognostic factors (age, sex, smoking status, diabetes, obesity, and lipids), plus up to <UNK> other prognostic factors. meta-analysis results shown are based on a random effects meta-analysis model with dersimonian and laird estimation of the between-study variances. the summary result is identical to hemingway and <UNK> but the confidence interval is wider because we used the hartung-knapp approach to account for uncertainty in variance <UNK> “risk ratio” is used, the estimates actually correspond to a mixture of risk ratios, odds ratios, and hazard ratios download figure open in new tab download powerpoint step <UNK> quantifying and examining heterogeneity for all meta-analyses, when there is large heterogeneity across included studies, it may be better not to synthesise the study results, but rather display the variability in estimates on a forest plot without showing an overall pooled estimate. when a meta-analysis is performed in the face of heterogeneity, it is important to quantify and report the magnitude of heterogeneity itself; for example, through the estimate of (the between-study <UNK> or an approximate <UNK> prediction interval indicating the potential true prognostic effect of a factor in a new <UNK> subgroup analyses and meta-regression can be used to examine or explore the causes of heterogeneity. a subgroup analysis performs a separate meta-analysis for categories defined by a particular characteristic, such as those with a low risk of bias, those with a follow-up of less than one year or of at least one year, or those set in countries in europe. a better approach is meta-regression, which extends the meta-analysis equation shown in box <UNK> by including study level <UNK> and allows a formal comparison of meta-analysis results across groups defined by covariates (eg, low risk of bias studies v studies at higher risk of bias). unfortunately, subgroup analyses and meta-regression are often problematic. there will often be few studies per subgroup and low power to detect genuine causes of heterogeneity. furthermore, study level confounding will be rife so that it is difficult to disentangle the associations for one covariate from another. for example, studies with a low risk of bias may also have a different length of follow-up or a particular cutpoint level compared with studies at higher risk of bias. application to crp review hemingway and colleagues reported that meta-regression identified four study level covariates that explained some between-study heterogeneity in the prognostic effect of crp: definition of comparison group, number of adjustment factors, the (log) number of events, and the proportion of patients with stable coronary disease (reflecting study <UNK> studies originally reporting unequal crp groups had stronger effects than those reporting crp on a continuous scale. for each additional adjustment factor, the summary risk ratio decreased by <UNK> the summary risk ratio was smaller among studies with more than the median number of outcome events, and smaller among studies confined to stable coronary disease. there was no evidence that the crp effect differed according to the number of quality items reported by a study, or by the type of prognostic effect measure provided (that is, risk ratio, odds ratio, or hazard ratio). step <UNK> examining small-study effects the term “small-study effects” refers to when there is a systematic difference in prognostic effect estimates for small studies and large <UNK> a particular concern is when small studies (especially those that are exploratory because these often evaluate many potential prognostic factors with relatively few outcome events) show larger prognostic effects than larger studies. this difference may be due to chance or heterogeneity, but a major threat here is publication bias and selective reporting, which are endemic in prognosis <UNK> such reporting biases lead to smaller studies, with (statistically) significant or larger prognostic factor effect estimates being more likely to be published or reported in sufficient detail, and thus included in a meta-analysis, than smaller studies with non-significant or smaller prognostic effect estimates. this bias is a potential concern for unadjusted and adjusted prognostic effects. a primary study usually estimates an unadjusted prognostic effect for each of multiple prognostic factors, but study authors may only report effects that are statistically significant. in addition, adjusted results are often only reported for prognostic factors that retain statistical significance in univariable and multivariable analysis. a consequence is that meta-analysis results will be biased, with larger summary prognostic effects than in reality, and potentially some factors being deemed to have clinical value when actually they do not. the evidence for small-study effects is usually considered on a funnel plot, which shows the study estimates (x axis) against their precision (y axis). a funnel plot is usually recommended if there are <UNK> or more <UNK> the plot should ideally show a symmetric, funnel like shape, with results from larger studies at the centre of the funnel and smaller studies spanning out in both directions equally. asymmetry will arise if there are small-study effects, with a greater proportion of smaller studies in one particular direction. statistical tests for asymmetry in risk, odds and hazard ratios can be used, such as peter’s and debray’s <UNK> contour enhanced funnel plots also show the statistical significance of individual studies, and “missing” studies are perhaps more likely to fall within regions of non-significance if publication bias was the cause of small-study effects. an example is shown in figure <UNK> fig <UNK> fig <UNK> evidence of funnel plot asymmetry (small-study effects) in the c reactive protein meta-analysis shown in figure <UNK> the smaller studies (with higher standard errors) have risk ratio (rr) estimates mainly to the right of the larger studies, and therefore give the largest prognostic effect estimates. a concern is that this is due to publication bias, with “missing” studies potentially falling to the left side of the larger studies and in the lighter shaded regions denoting non-significant rr estimates download figure open in new tab download powerpoint as mentioned, small-study effects may also arise due to heterogeneity. therefore, it is difficult to disentangle publication bias from heterogeneity in a single review. for example, if smaller studies used an analysis with fewer adjustment factors, then this may cause larger prognostic factor effects in such studies, rather than it being caused by publication bias. a multivariate meta-analysis could reduce the impact of small-study effects by “borrowing strength” from related <UNK> a related concern is that smaller prognostic factor studies are generally at higher risk of bias than larger studies. smaller studies tend to be more exploratory in nature and typically based on a convenient sample, often examining many (sometimes hundreds of) potential prognostic factors, with relatively few outcome events. this design leads to spurious (due to chance) and potentially biased (due to poor estimation <UNK> prognostic effect estimates, which are more prone to selective reporting. in contrast, larger studies are often confirmatory studies focusing on one or a few prognostic factors, and are more likely to adopt a protocol driven and prospective approach, with clearer reporting regardless of their <UNK> therefore, larger studies are less likely to identify spurious prognostic factor effect estimates. it is helpful to examine small-study effects (potential publication bias) when restricting analysis to the subset of studies at low risk of bias. if this approach resolves previous issues of small-study effects in the full meta-analysis, then it gives even more credence to focus conclusions and recommendations on the meta-analysis results based only on the higher quality studies. application to crp review figure <UNK> shows a funnel plot of the study estimates from the crp meta-analysis shown in figure <UNK> there is clear asymmetry, which shows the strong potential for publication bias. there was an insufficient number of studies considered at low risk of bias to evaluate small-study effects in a subset of higher quality studies. step <UNK> reporting and interpretation of results as with all research studies, clear and complete reporting is essential for reviews of prognostic factor studies. most of the reporting guidelines of prisma (preferred reporting items for systemic reviews and meta-analyses) and moose (meta-analysis of observational studies in epidemiology) will be <UNK> and should be complemented by <UNK> which was aimed at primary prognostic factor studies. more specific guidance for reporting systematic reviews of prognostic factor studies is under development. interpretation and translation of summary meta-analysis results is an important final step. the guidance in the previous steps is the essential input for this step. discussion is necessary on whether and how the prognostic factors identified may be useful in practice (that is, translation of results to clinical practice), and what further research is necessary. ideally impact studies (eg, randomised trials that compare groups which do and do not use a prognostic factor to inform clinical practice) are needed before strong recommendations for clinical practice are made; however, these studies are rare and outside the scope of the review framework outlined in this article. to interpret the certainty (confidence) of the summary results of a review of intervention effectiveness, grade (grades of recommendation, assessment, development, and evaluation) was developed. this approach assesses the overall quality of and certainty in evidence for the summary estimates of the intervention effects by addressing five domains: risk of bias, inconsistency, imprecision, indirectness, and publication bias. the grade domains can be assessed using the information obtained by the tools and methods described in the above steps. however, it is not known whether these domains, developed for reviews of interventions, are equally applicable to assessing the certainty of summary results of systematic reviews of prognostic factor studies. compared with reviews of intervention studies, allowing for heterogeneity (the inconsistency domain) might be more acceptable in reviews of prognostic factor studies because of the inevitable heterogeneity caused by study differences in methods of measurement, adjustment factors, and statistical analysis methods, among others. furthermore, the threat of selective reporting or publication bias in reviews of prognostic factor studies may be more severe than in reviews of intervention studies because of the problems of exploratory studies, poor reporting, and biased analysis methods. there is limited empirical evidence for using the existing domains to grade the certainty of summary estimates of prognostic factor studies, although a first attempt has been <UNK> in addition, an assessment has been performed on grading the certainty of evidence of summary estimates of overall prognosis <UNK> reviewers need to be especially cautious when comparing the adjusted prognostic value of multiple index factors, for example, to conclude whether the summary adjusted hazard ratio for prognostic factor a is larger than that for factor b. usually different sets of studies will be available for each index factor, and so the comparison will be indirect and potentially biased. moreover, the studies evaluating factor a may often have used different sets of adjustment factors (other prognostic factors) than those evaluating factor b. it will be rare to find studies on different index factors that used exactly the same set of adjustment factors. we therefore recommend reviewers restrict comparisons (of the adjusted prognostic value) of two or more index factors to those studies that at least used a similar, minimally required set of adjustment <UNK> even then, due to different scales and distributions of each factor (eg, continuous or binary), a simple comparison of the prognostic effect sizes (eg, hazard ratio for factor a v hazard ratio for factor b) may not be straightforward. application to crp review the meta-analysis results suggest crp is a prognostic factor for the risk of death and non-fatal cardiovascular events, even when only including the largest studies that adjusted for all six conventional prognostic factors. in their discussion, hemingway and colleagues downgraded the meta-analysis findings because of a strong concern about the quality and reliability of the underlying <UNK> the absence of prespecified protocols, poor and potentially biased reporting, and strong potential for publication bias prevented the authors from making firm conclusions about whether crp has prognostic value after adjustment for established prognostic factors. they state that the concerns “explicitly challenge the statement for healthcare professionals made by the centers for disease control that measuring crp is both ‘useful’ and ‘independent’ as a marker of <UNK> summary in this article, we described the key steps and methods for conducting a systematic review and meta-analysis of prognostic factor studies. current reviews are often limited by the quality and heterogeneity of primary <UNK> we expect the prevalence of such reviews to grow rapidly, especially as cochrane has recently embarked on prognosis reviews (see also the cochrane prognosis methods group website <UNK> our guidance will help researchers to write grant applications for reviews of prognostic factor studies, and to develop protocols and conduct such reviews. protocols of prognostic factor reviews should be published ideally at the same time as the review is registered, for example within prospero, the international prospective register of systematic reviews (www.crd.york.ac.uk/prospero/), or the cochrane <UNK> our guidance will also allow readers and healthcare providers to better judge reports of prognostic factor reviews. finally, we note that some of the limitations described (eg, use of different cutpoint values across studies) could be alleviated if the individual participant data were obtained from primary prognostic factor <UNK> rather than being extracted from study publications; although, this may not solve all problems (eg, quality of original study, availability of different adjustment <UNK> further discussion on individual participant data meta-analysis of prognostic factor studies is given <UNK>

<|EndOfText|>

calculating the sample size required for developing a clinical prediction model summary points patients and healthcare professionals require clinical prediction models to accurately guide healthcare decisions larger sample sizes lead to the development of more robust models data should be of sufficient quality and representative of the target population and settings of application it is better to use all available data for model development (ie, avoid data splitting), with resampling methods (such as bootstrapping) used for internal validation when developing prediction models for binary or time-to-event outcomes, a well known rule of thumb for the required sample size is to ensure at least <UNK> events for each predictor parameter the actual required sample size is, however, context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the model we propose to use such information to tailor sample size requirements to the specific setting of interest, with the aim of minimising the potential for model overfitting while targeting precise estimates of key parameters our proposal can be implemented in a four step procedure and is applicable for continuous, binary, or time-to-event outcomes the pmsampsize package in stata or r allows researchers to implement the procedure clinical prediction models are needed to inform diagnosis and prognosis in <UNK> well known examples include the wells <UNK> <UNK> and the nottingham prognostic <UNK> such models allow health professionals to predict an individual’s outcome value, or to predict an individual’s risk of an outcome being present (diagnostic prediction model) or developed in the future (prognostic prediction model). most prediction models are developed using a regression model, such as linear regression for continuous outcomes (eg, pain score), logistic regression for binary outcomes (eg, presence or absence of pre-eclampsia), or proportional hazards regression models for time-to-event data (eg, recurrence of venous <UNK> an equation is then produced that can be used to predict an individual’s outcome value or outcome risk conditional on his or her values of multiple predictors, which might include basic characteristics such as age, weight, family history, and comorbidities; biological measurements such as blood pressure and biomarkers; and imaging or other test results. supplementary material <UNK> shows examples of regression equations. developing a prediction model requires a development dataset, which contains data from a sample of individuals from the target population, containing their observed predictor values (available at the intended moment of <UNK> and observed outcome. the sample size of the development dataset must be large enough to develop a prediction model equation that is reliable when applied to new individuals in the target population. what constitutes an adequately large sample size for model development is, however, <UNK> with various blanket “rules of thumb” proposed and <UNK> this has created confusion about how to perform sample size calculations for studies aiming to develop a prediction model. in this article we provide practical guidance for calculating the sample size required for the development of clinical prediction models, which builds on our recent methodology <UNK> we suggest that current minimum sample size rules of thumb are too simplistic and outline a more scientific approach that tailors sample size requirements to the specific setting of interest. we illustrate our proposal for continuous, binary, and time-to-event outcomes and conclude with some extensions. moving beyond the <UNK> events per variable rule of thumb in a development dataset, the effective sample size for a continuous outcome is determined by the total number of study participants. for binary outcomes, the effective sample size is often considered about equal to the minimum of the number of events (those with the outcome) and non-events (those without the outcome); time-to-event outcomes are often considered roughly equal to the total number of <UNK> when developing prediction models for binary or time-to-event outcomes, an established rule of thumb for the required sample size is to ensure at least <UNK> events for each predictor parameter (ie, each β term in the regression equation) being considered for inclusion in the prediction model <UNK> this is widely referred to as needing at least <UNK> events per variable <UNK> epv). the word “variable” is, however, misleading as some predictors actually require multiple β terms in the model equation—for example, two β terms are needed for a categorical predictor with three categories (eg, tumour grades i, ii, and iii), and two or more β terms are needed to model any non-linear effects of a continuous predictor, such as age or blood pressure. the inclusion of interactions between two or more predictors also increases the number of model parameters. hence, as prediction models usually have more parameters than actual predictors, it is preferable to refer to events per candidate predictor parameter (epp). the word candidate is important, as the amount of model overfitting is dictated by the total number of predictor parameters considered, not just those included in the final model equation. the rule of at least <UNK> epp has been widely advocated perhaps as a result of its simplicity, and it is regularly used to justify sample sizes within published articles, grant applications, and protocols for new model development studies, including by ourselves previously. the most prominent work advocating the rule came from simulation studies conducted in the <UNK> although this work actually focused more on the bias and precision of predictor effect estimates than on the accuracy of risk predictions from a developed model. the adequacy of the <UNK> epp rule has often been debated. although the rule provides a useful starting point, counter suggestions include either lowering the epp to below <UNK> or increasing it to <UNK> <UNK> or even <UNK> these inconsistent recommendations reflect that the required epp is actually context specific and depends not only on the number of events relative to the number of candidate predictor parameters but also on the total number of participants, the outcome proportion (incidence) in the study population, and the expected predictive performance of the <UNK> this finding is unsurprising as sample size considerations for other study designs, such as randomised trials of interventions, are all context dependent and tailored to the setting and research question. rules of thumb have also been advocated in the continuous outcome setting, such as two participants per <UNK> but these share the same concerns as for <UNK> <UNK> sample size calculation to ensure precise predictions and minimise overfitting recent work by van smeden et <UNK> and riley et <UNK> describe how to calculate the required sample size for prediction model development, conditional on the user specifying the overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit <UNK> these authors’ approaches can be implemented in a four step procedure. each step leads to a sample size calculation, and ultimately the largest sample size identified is the one required. we describe these four steps, and, to aid general readers, provide the more technical details of each step in the figures. step <UNK> what sample size will produce a precise estimate of the overall outcome risk or mean outcome value? fundamentally, the sample size must allow the prediction model’s intercept to be precisely estimated, to ensure that the developed model can accurately predict the mean outcome value (for continuous outcomes) or overall outcome proportion (for binary or time-to-event outcomes). a simple way to do this is to calculate the sample size needed to precisely estimate (within a small margin of error) the intercept in a model when no predictors are included (the null <UNK> <UNK> shows the calculation for binary and time-to-event outcomes, and we generally recommend aiming for a margin of error of <UNK> in the overall outcome proportion estimate. for example, with a binary outcome that occurs in half of individuals, a sample size of at least <UNK> people is needed to target a confidence interval of <UNK> to <UNK> for the overall outcome proportion, and thus an error of at most <UNK> around the true value of <UNK> to achieve the same margin of error with outcome proportions of <UNK> and <UNK> at least <UNK> and <UNK> participants, respectively, are required. fig <UNK> fig <UNK> calculation of sample size required for precise estimation of the overall outcome probability in the target population download figure open in new tab download powerpoint for time-to-event outcomes, a key time point needs to be identified, along with the anticipated outcome event rate. for example, with an anticipated event rate of <UNK> per <UNK> person years of the entire follow-up, the sample size must include a total of <UNK> person years of follow-up to ensure an expected margin of error of <UNK> in the estimate of a <UNK> year outcome probability of <UNK> such that the expected confidence interval is <UNK> to <UNK> for continuous outcomes, the anticipated mean and variance of outcome values must be prespecified, alongside the anticipated percentage of variation explained by the prediction model (see supplementary material <UNK> for <UNK> step <UNK> what sample size will produce predicted values that have a small mean error across all individuals? in addition to predicting the average outcome value precisely (see step <UNK> the sample size for model development should also aim for precise predictions across the spectrum of predicted values. for binary outcomes, van smeden et al use simulation across a wide range of scenarios to evaluate how the error of predicted outcome probabilities from a developed model depends on various characteristics of the development dataset sampled from a target <UNK> they found that the number of candidate predictor parameters, total sample size, and outcome proportion were the three main drivers of a model’s mean predictive accuracy. this led to a sample size formula (fig <UNK> to help ensure that new prediction models will, on average, have a small prediction error in the estimated outcome probabilities in the target population (as measured by the mean absolute prediction error, mape). the calculation requires the number of candidate predictor parameters and the anticipated outcome proportion in the target population to be prespecified. for example, with <UNK> candidate predictor parameters and an outcome proportion of <UNK> a sample size of at least <UNK> participants and <UNK> epp is required to target a mean absolute error of <UNK> between observed and true outcome probabilities (see fig <UNK> for calculation). the calculation is available as an interactive tool (https://mvansmeden.shinyapps.io/beyondepv/) and applicable to situations with <UNK> or fewer candidate predictors. ongoing work aims to extend to larger numbers of candidate predictors and also to time-to-event outcomes. fig <UNK> fig <UNK> sample size required to help ensure a developed prediction model of a binary outcome will have a small mean absolute error in predicted probabilities when applied in other targeted individuals download figure open in new tab download powerpoint for continuous outcomes, accurate predictions across the spectrum of predicted values require the standard deviation of the residuals to be precisely <UNK> supplementary material <UNK> shows that to target a less than <UNK> multiplicative error in the estimated residual standard deviation, the required sample size is simply <UNK> where p is the number of predictor parameters considered. step <UNK> what sample size will produce a small required shrinkage of predictor effects? our third recommended step is to identify the sample size required to minimise the problem of <UNK> overfitting is when a developed model’s predictions are more extreme than they ought to be for individuals in a new dataset from the same target population. for example, an overfitted prediction model for a binary outcome will give a predicted outcome probability too close to <UNK> for individuals with a higher than the average outcome probability and too close to <UNK> for individuals with a lower than the average outcome probability. overfitting notably occurs when the sample size is too small. in particular, when the number of candidate predictor parameters is large relative to the number of participants in total (for continuous outcomes) or to the number of participants with the outcome event (for binary or time-to-event outcomes). a consequence of overfitting is that a developed model’s apparent predictive performance (as observed in the development dataset itself) will be optimistic (ie, too high), and its actual predictive performance in new data from the same target population will be lower (ie, worse). shrinkage (also known as penalisation or regularisation) methods deal with the problem of overfitting by reducing the variability in the developed model’s predictions such that extreme predictions (eg, predicted probabilities close to <UNK> or <UNK> are pulled back toward the overall <UNK> however, there is no guarantee that shrinkage will fully overcome the problem of overfitting when developing a prediction model. this is because the shrinkage or penalty factors (which dictate the magnitude of shrinkage required) are also estimated from the development dataset and, especially when the sample size is small, are often imprecise and so fail to tackle the magnitude of overfitting correctly in a particular <UNK> furthermore, a negative correlation tends to occur between the estimated shrinkage required and the apparent performance of a model. if the apparent model performance is excellent simply by chance, the required shrinkage is typically estimated too <UNK> thus, ironically, in those situations when overfitting is of most concern (and thus shrinkage is most urgently needed), the prediction model developer has insufficient assurance in selecting the proper amount of shrinkage to cancel the impact of overfitting. riley et al therefore suggest identifying the sample size and number of candidate predictors that correspond to a small amount of desired shrinkage <UNK> during model <UNK> the sample size calculation (fig <UNK> requires the researcher to prespecify the number of candidate predictor parameters and, for binary or time-to-event outcomes, the anticipated outcome proportion or rate, respectively, in the target population. in addition, a (conservative) value for the anticipated model performance is required, as defined by the cox-snell r squared statistic <UNK> the anticipated value of <UNK> is important because it reflects the signal:noise ratio, which has an impact on the estimation of multiple parameters and the potential for overfitting. when the signal:noise ratio is anticipated to be high (eg, <UNK> is close to <UNK> for a prediction model with a continuous outcome), true patterns are easier to detect and so overfitting is less of a concern, such that more predictor parameters can be estimated. however, when the signal:noise ratio is low (ie, <UNK> is anticipated to be close to <UNK> true patterns are harder to identify and there is more potential for overfitting, such that fewer predictor parameters can be estimated reliably. fig <UNK> fig <UNK> how to calculate the sample size needed to target a small magnitude of required shrinkage of predictor effects (to minimise potential model overfitting) for binary or time-to-event outcomes download figure open in new tab download powerpoint in the continuous outcome setting, <UNK> is simply the coefficient of determination <UNK> which quantifies the proportion of the variance of outcome values that is explained by the prediction model and thus is between <UNK> and <UNK> for example, when developing a prediction model for a continuous outcome with up to <UNK> predictor parameters and an anticipated <UNK> of <UNK> a sample size of <UNK> participants is required to ensure the expected shrinkage is <UNK> (see supplementary material <UNK> for full calculation). this corresponds to about seven participants for each predictor parameter considered. the <UNK> statistic generalises to non-continuous outcomes and allows sample size calculations to minimise the expected shrinkage when developing a prediction model for binary and time-to-event outcomes (fig <UNK> for example, when developing a new logistic regression model with up to <UNK> candidate predictor parameters and an anticipated <UNK> of at least <UNK> a sample size of <UNK> participants is required to ensure the expected shrinkage is <UNK> (see fig <UNK> for full calculation). if the target setting has an outcome proportion of <UNK> this corresponds to an epp of <UNK> the required sample size and epp are sensitive to the choice of <UNK> with lower anticipated values of <UNK> leading to higher required sample sizes. therefore, a conservative choice of <UNK> is recommended (fig <UNK> fig <UNK> fig <UNK> how to decide on the model’s anticipated <UNK> in advance of data collection download figure open in new tab download powerpoint as in sample size calculations for randomised trials evaluating intervention effects, external evidence and expert opinion are required to inform the values that need specifying in the sample size calculator. figure <UNK> provides guidance for specifying <UNK> importantly, unlike for continuous outcomes when <UNK> is bounded between <UNK> and <UNK> the <UNK> is bounded between <UNK> and <UNK> for binary and time-to-event outcomes. the <UNK> denotes the maximum possible value of <UNK> which is dictated by the overall outcome proportion or rate in the development dataset and is often much less than <UNK> supplementary material <UNK> shows the calculation of <UNK> for logistic regression models with outcome proportions of <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> the corresponding <UNK> values are <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> respectively. thus the anticipated <UNK> might be small, even for a model with potentially good performance. step <UNK> what sample size will produce a small optimism in apparent model fit? the sample size should also ensure a small difference in the developed models apparent and optimism adjusted values of <UNK> (ie, <UNK> as this is a fundamental overall measure of model <UNK> the apparent <UNK> value is simply the model’s observed performance in the same data as used to develop the model, whereas the optimism adjusted <UNK> value is a more realistic (approximately unbiased) estimate of the model’s fit in the target population. the sample size calculations are shown in supplementary material <UNK> for continuous outcomes and in figure <UNK> for binary and time-to-event outcomes. as before, they require the user to specify the anticipated <UNK> and the <UNK> as described in figure <UNK> for example, when developing a logistic regression model with an anticipated <UNK> of <UNK> and in a setting with an outcome proportion of <UNK> (such that the <UNK> is <UNK> <UNK> participants are required to ensure the expected optimism in the apparent <UNK> is just <UNK> (see figure <UNK> for calculation). fig <UNK> fig <UNK> how to calculate the sample size needed to target a small optimism in model fit (to minimise potential model overfitting) for binary and time-to-event outcomes download figure open in new tab download powerpoint recommendations and software box <UNK> summarises our recommended steps for calculating the minimum sample size required for prediction model development. this involves four calculations for binary outcomes <UNK> to <UNK> three for time-to-event outcomes <UNK> to <UNK> and four for continuous outcomes <UNK> to <UNK> to implement the calculations, we have written the pmsampsize package for stata and r. the software calculates the sample size needed to meet all the criteria listed in box <UNK> (except <UNK> which is available at https://mvansmeden.shinyapps.io/beyondepv/), conditional on the user inputting values of required parameters such as the number of candidate predictors, the anticipated outcome proportion in the target population, and the anticipated <UNK> the calculations are especially helpful when prospective data collection (eg, new cohort study) are required before model development; however, they are also relevant when existing data are available to guide the number of predictors that can be considered. box <UNK> recommendations for calculating the sample size needed when developing a clinical prediction model for continuous, binary, and time-to-event outcomes to increase the potential for developing a robust prediction model, the sample size should be at least large enough to minimise model overfitting and to target sufficiently precise model predictions binary outcomes for binary outcomes, ensure the sample size is enough to: estimate the overall outcome proportion with sufficient precision (use equation in figure <UNK> <UNK> target a small mean absolute prediction error (use equation in figure <UNK> if number of predictor parameters is <UNK> <UNK> target a shrinkage factor of <UNK> (use equation in figure <UNK> <UNK> target small optimism of <UNK> in the apparent <UNK> (use equation in figure <UNK> <UNK> time-to-event outcomes for time-to-event outcomes, ensure the sample size is enough to: estimate the overall outcome proportion with sufficient precision at one or more key time-points in follow-up (use equation in figure <UNK> <UNK> target a shrinkage factor of <UNK> (use equation in figure <UNK> <UNK> target small optimism of <UNK> in the apparent <UNK> (use equation in figure <UNK> <UNK> continuous outcomes for continuous outcomes, ensure the sample size is enough to: estimate the model intercept precisely (see supplementary material <UNK> <UNK> estimate the model residual variance with sufficient precision (see supplementary material <UNK> <UNK> target a shrinkage factor of <UNK> (use equation in figure <UNK> <UNK> target small optimism of <UNK> in the apparent <UNK> (use equation in figure <UNK> <UNK> these approaches require researchers to specify the anticipated overall outcome risk or mean outcome value in the target population, the number of candidate predictor parameters, and the anticipated model performance in terms of overall model fit <UNK> when the choice of values is uncertain, we generally recommend being conservative and so taking those values (eg, smallest <UNK> that give larger sample sizes when an existing dataset is already available (such that sample size is already defined), the calculations can be used to identify if the sample size is sufficient to estimate the overall outcome risk or the mean outcome value, and how many predictor parameters can be considered before overfitting becomes a concern applied examples we now illustrate the recommendations in box <UNK> by using three examples. example <UNK> binary outcome north et al developed a model predicting pre-eclampsia in pregnant women based on clinical predictors measured at <UNK> weeks’ <UNK> including vaginal bleeding, age, previous miscarriage, family history, smoking, and alcohol consumption. the model included <UNK> predictor parameters and had a c statistic of <UNK> emerging research aims to improve this and other pre-eclampsia prediction models by including additional predictors (eg, biomarkers and ultrasound measurements). as the outcome is binary, the sample size calculation for a new prediction model needs to examine criteria <UNK> to <UNK> in box <UNK> this requires us to input the overall proportion of women who will develop pre-eclampsia <UNK> and the number of candidate predictor parameters (assumed to be <UNK> for illustration). for an outcome proportion of <UNK> the <UNK> value is <UNK> (see supplementary material <UNK> if we assume, conservatively, that the new model will explain <UNK> of the variability, the anticipated <UNK> value is <UNK> now we can check criteria <UNK> <UNK> and <UNK> by typing in stata: pmsampsize, type(b) <UNK> <UNK> <UNK> this indicates that at least <UNK> women are required, corresponding to <UNK> events and an epp of <UNK> this is driven by criterion <UNK> to ensure the expected shrinkage required is just <UNK> (to minimise the potential overfitting). to check criterion <UNK> in box <UNK> we can apply the formula in figure <UNK> this suggests that <UNK> women are needed to target a mean absolute error in predicted probabilities of <UNK> this is much lower than the <UNK> women needed to meet criterion <UNK> if recruiting <UNK> women is impractical (eg, because of time, cost, or practical constraints for data collection), the sample size required can be reduced by identifying a smaller number of candidate predictors (eg, based on existing evidence from systematic <UNK> for example, with <UNK> rather than <UNK> candidate predictors, the required sample size to meet all four criteria is at least <UNK> women and <UNK> events (still <UNK> epp). example <UNK> time-to-event outcome many prognostic models are available for the risk of a recurrent venous thromboembolism (vte) after cessation of treatment for a first <UNK> for example, the model of ensor et al included predictors of age, sex, site of first clot, d-dimer level, and the lag time from cessation of treatment until measurement of d-dimer (often around <UNK> <UNK> the model’s c statistic was <UNK> and the adjusted <UNK> was <UNK> (corresponding to <UNK> of the total variation). emerging research aims to extend such models by including additional predictors. the sample size required for a new model must at least meet criteria <UNK> to <UNK> this requires us to input a key time point for prediction of vte recurrence risk (eg, two years), alongside the number of candidate predictor parameters <UNK> the anticipated mean follow-up <UNK> years), and outcome event rate <UNK> or <UNK> vte recurrences for every <UNK> person years of follow-up), and the conservative value of <UNK> <UNK> with all chosen values based on ensor et <UNK> now criteria <UNK> to <UNK> can be checked, for example by typing in stata: pmsampsize, type(s) <UNK> <UNK> <UNK> <UNK> meanfup <UNK> this indicates that at least <UNK> participants are required, corresponding to <UNK> events and an epp of <UNK> this is considerably more than <UNK> epp, and is driven by a desired shrinkage factor (criterion <UNK> of only <UNK> to minimise overfitting based on just <UNK> of variation explained by the model. if the number of candidate predictor parameters is lowered to <UNK> the required sample size is reduced to <UNK> (still an epp of <UNK> example <UNK> continuous outcome hudda et al developed a prediction model for fat free mass in children and adolescents aged <UNK> to <UNK> years, including <UNK> predictor parameters based on height, weight, age, sex, and <UNK> the model is needed to provide an estimate of an individual’s current fat mass (=weight minus predicted fat free mass). on external validation, the model had an <UNK> of <UNK> let us assume that the model will need updating (eg, in <UNK> years owing to changes in the population behaviour and environment), and that an additional <UNK> predictor parameters (and thus a total of <UNK> parameters) will need to be considered in the model development. the sample size for a model development dataset must at least meet the four criteria of <UNK> to <UNK> in box <UNK> this requires us to specify the anticipated <UNK> <UNK> number of candidate predictor parameters <UNK> and mean <UNK> kg) and standard deviation <UNK> kg) of fat free mass in the target population (taken from hudda et <UNK> for example, in stata, after installation of pmsampsize (type: ssc install pmsampsize), we can type: pmsampsize, type(c) <UNK> <UNK> <UNK> sd <UNK> this returns that at least <UNK> participants are required, and so <UNK> participants for each predictor parameter. the sample size of <UNK> is driven by the number needed to precisely estimate the model standard deviation (criterion <UNK> as only <UNK> participants are needed to minimise overfitting (criteria <UNK> and <UNK> extensions and further topics ensuring accurate predictions in key subgroups alongside the criteria outlined in box <UNK> a more stringent task is to ensure model predictions are accurate in key subgroups defined by particular values or categories of included <UNK> one way to tackle this is to ensure predictor effects in the model equation are precisely estimated, at least for key subgroups of <UNK> for binary and time-to-event outcomes, the precision of a predictor’s effect depends on its magnitude, the variance of the predictor’s values, the predictor’s correlation with other predictors in the model, the sample size, and the outcome proportion or rate in the <UNK> for continuous outcomes, it depends on the sample size, the residual variance, the correlation of the predictor with other included predictors, and the variance of the predictor’s <UNK> note that for important categorical predictors large sample sizes might be needed to avoid separation issues (ie, where no events or non-events occur in some <UNK> and potential bias from sparse <UNK> sample size considerations when using an existing dataset our proposed sample size calculations (ie, based on the criteria in box <UNK> are still useful in situations when an existing dataset is already available, with a specific number of participants and predictors. firstly, the calculations might identify that the dataset is too small (for example, if the overall outcome risk cannot be estimated precisely) and so the collection of further data is <UNK> secondly, the calculations might help identify how many predictors can be considered before overfitting becomes a concern. the shrinkage estimate obtained from fitting the full model (including all predictors) can be used to gauge whether the number of predictors could be reduced through data reduction techniques such as principal components <UNK> this process should be done blind to the estimated predictor effects in the full model, as otherwise decisions about predictor inclusion will be influenced by a “quick look” at the results (which increases the overfitting). sample size requirements when using variable selection further research on sample size requirements with variable selection is required, especially for the use of more modern penalisation methods such as the lasso (least absolute shrinkage and selection operator) or elastic <UNK> such methods allow shrinkage and variable selection to operate simultaneously, and they even allow the consideration of more predictor parameters than number of participants or outcome events (ie, in high dimensional settings). however, there is no guarantee such models solve the problem of overfitting in the dataset at hand. as mentioned, they require penalty and shrinkage factors to be estimated using the development dataset, and such estimates will often be hugely imprecise. also, the subset of included predictors might be highly <UNK> that is, if the prediction model development was repeated on a different sample of the same size, a different subset of predictors might be selected and important predictors missed (especially if sample size is small). in healthcare the final set of predictors is a crucial consideration, owing to their cost, time, burden (eg, blood test, invasiveness), and measurement requirements. larger sample sizes might be needed when using machine learning approaches to develop risk prediction models an alternative to regression based prediction models are those based on machine learning methods, such as random forests and neural networks (of which “deep learning” methods are a special <UNK> when the focus is on individualised outcome risk prediction, it has been shown that extremely large datasets might be needed for machine learning techniques. for binary outcomes, machine learning techniques could need more than <UNK> times as many events for each predictor to achieve a small amount of overfitting compared with classic modelling techniques such as logistic regression, and might show instability and a high optimism even with more than <UNK> <UNK> a major cause of this problem is that the number of predictor (“feature”) parameters considered by machine learning approaches will usually far exceed that for regression, even when the same set of predictors is considered, particularly because they routinely examine multiple interaction terms and categorise continuous predictors. therefore, machine learning methods are not immune to sample size requirements, and actually might need truly “big data” to ensure their developed models have small overfitting, and for their potential advantages (eg, dealing with highly non-linear relations and complex interactions) to reach fruition. the size of most medical research datasets is better suited to using regression (including penalisation and shrinkage <UNK> especially as regression also leads to a transparent model equation that facilitates implementation, validation, and graphical displays. sample size for model updating when an existing prediction model is updated, the existing model equation is revised using a new dataset. the required sample size for this dataset depends on how the model is to be updated and whether additional predictors are to be included. in our worked examples, we assumed that all parameters in the existing model will be re-estimated using the model updating dataset. in that situation, the researcher can still follow the guidance in box <UNK> for calculating the required sample size, with the total predictor parameters the same as in the original model plus those new parameters required for any additional predictors. sometimes, however, only a subset of the existing model’s parameters is to be <UNK> in particular, to deal with calibration-in-the-large, researchers might only want to revise the model intercept (or baseline survival), while constraining the other parameter estimates to be the same as those in the existing model. in this case the required sample size only needs to be large enough to estimate the mean outcome value or outcome risk precisely (ie, to meet criteria <UNK> <UNK> or <UNK> in box <UNK> even if researchers also want to update the existing predictor effects, they might decide to constrain their updated values to be equal to the original values multiplied by a constant. then, the sample size only needs to be large enough to estimate one predictor parameter (ie, the constant) for the existing predictors, plus any new parameters the researchers decide to add. such model updating techniques therefore reduce the sample size needed (to meet the criteria in box <UNK> compared with when every predictor parameter is re-estimated without constraint. conclusion patients and healthcare professionals require clinical prediction models to accurately guide healthcare <UNK> larger sample sizes lead to more robust models being developed, and our guidance in box <UNK> outlines how to calculate the minimum sample size required. clearly, the more data for model development the better; so if larger sample sizes are achievable than our guidance suggests, use it! of course, any data collected should be of sufficient quality and representative of the target population and settings of <UNK> after data collection, careful model building is required using appropriate <UNK> in particular, we do not recommend data splitting (eg, into model training and testing samples), as this is inefficient and it is better to use all the data for model development, with resampling methods (such as bootstrapping) used for internal <UNK> sometimes external information might be used to supplement the development dataset <UNK> lastly, sample size requirements when externally validating an existing prediction model require a different approach, as discussed elsewhere.

<|EndOfText|>

the value of preseason screening for injury prediction: the development and internal validation of a multivariable prognostic model to predict indirect muscle injury risk in elite football (soccer) players abstract background in elite football (soccer), periodic health examination (phe) could provide prognostic factors to predict injury risk. objective to develop and internally validate a prognostic model to predict individualised indirect (non-contact) muscle injury (imi) risk during a season in elite footballers, only using phe-derived candidate prognostic factors. methods routinely collected preseason phe and injury data were used from <UNK> players over <UNK> seasons <UNK> july <UNK> to <UNK> may <UNK> ten candidate prognostic factors <UNK> parameters) were included in model development. multiple imputation was used to handle missing values. the outcome was any time-loss, index indirect muscle injury (i-imi) affecting the lower extremity. a full logistic regression model was fitted, and a parsimonious model developed using backward-selection to remove factors that exceeded a threshold that was equivalent to akaike’s information criterion (alpha <UNK> predictive performance was assessed through calibration, discrimination and decision-curve analysis, averaged across all imputed datasets. the model was internally validated using bootstrapping and adjusted for overfitting. results during <UNK> participant-seasons, <UNK> i-imis were recorded. the parsimonious model included only age and frequency of previous imis; apparent calibration was perfect, but discrimination was modest (c-index = <UNK> <UNK> confidence interval (ci) = <UNK> to <UNK> with clinical utility evident between risk thresholds of <UNK> after validation and overfitting adjustment, performance deteriorated (c-index = <UNK> <UNK> ci = <UNK> to <UNK> calibration-in-the-large = − <UNK> <UNK> ci = − <UNK> to <UNK> calibration slope = <UNK> <UNK> ci = <UNK> to <UNK> conclusion the selected phe data were insufficient prognostic factors from which to develop a useful model for predicting imi risk in elite footballers. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field. trial registration <UNK> key points factors measured through preseason screening generally have weak prognostic strength for future indirect muscle injuries, and further research is needed to identify novel, robust prognostic factors. because of sample size restrictions and until the evidence base improves, it is likely that any further attempts at creating a prognostic model at individual club level would also suffer from poor performance. the value of using preseason screening data to make injury predictions or to select bespoke injury prevention strategies remains to be demonstrated, so screening should only be considered as useful for detection of salient pathology or for rehabilitation/performance monitoring purposes at this time. background in elite football (soccer), indirect (non-contact) muscle injuries (imis) predominantly affect the lower extremities and account for <UNK> to <UNK> of all injuries that result in time lost to training or competition <UNK> reduced player availability negatively impacts upon medical <UNK> and financial resources <UNK> <UNK> and has implications for team performance <UNK> therefore, injury prevention strategies are important to professional teams <UNK> periodic health examination (phe), or screening, is a key component of injury prevention practice in elite sport <UNK> specifically, in elite football, phe is used by <UNK> of teams and consists of medical, musculoskeletal, functional and performance tests that are typically evaluated during preseason and in-season periods <UNK> phe has a rehabilitation and performance monitoring function <UNK> and is also used to detect musculoskeletal or medical conditions that may be dangerous or performance limiting <UNK> another perceived role of phe is to recognise and manage factors that may increase, or predict, an athlete’s future injury risk <UNK> although this function is currently unsubstantiated <UNK> phe-derived variables associated with particular injury outcomes (such as imis) are called prognostic factors <UNK> which can be used to identify risk differences between players within a team <UNK> single prognostic factors are unlikely to satisfactorily predict an individual’s injury risk if used independently <UNK> however, several factors could be combined in a multivariable prognostic prediction model to offer more accurate personalised risk estimates for the occurrence of a future event or injury <UNK> <UNK> such models could be used to identify high-risk individuals who may require an intervention that is designed to reduce risk <UNK> thus assisting decisions in clinical practice <UNK> despite the potential benefits of using prognostic models for injury risk prediction, we are unaware of any that have been developed using phe data in elite football <UNK> therefore, the aim of this study was to develop and internally validate a prognostic model to predict individualised imi risk during a season in elite footballers, using a set of candidate prognostic factors derived from preseason phe data. methods the methods have been described in a published protocol <UNK> so will only be briefly outlined. this study has been registered on clinicaltrials.gov (identifier: <UNK> and is reported according to the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) statement <UNK> <UNK> data sources this study was a retrospective cohort design. eligible participants were identified from a population of male elite footballers, aged <UNK> years old at manchester united football club. a dataset was created using routinely collected injury and preseason phe data over <UNK> seasons <UNK> july <UNK> to <UNK> may <UNK> for each season, which started on <UNK> july, participants completed a mandatory phe during week <UNK> and were followed up to the final first team game of the season. if eligible participants were injured at the time of phe, a risk assessment was completed by medical staff. only tests that were appropriate and safe for the participant’s condition were completed; examiners were not blinded to injury status. participants and eligibility criteria during any season, participants were eligible if they <UNK> were not a goalkeeper and <UNK> participated in phe for the relevant season. participants were excluded if they were not contracted to the club for the forthcoming season at the time of phe. ethics and data use informed consent was not required as data were captured from the mandatory phe completed through the participants’ employment. the data usage was approved by the club and university of manchester research ethics service. outcome the outcome was any time-loss, index imi (i-imi) of the lower extremity. that is, any i-imi sustained by a participant during matches or training, which affected lower abdominal, hip, thigh, calf or foot muscle groups and prohibited future football participation <UNK> i-imis were graded by a club doctor or physiotherapist according to the validated munich consensus statement for the classification of muscle injuries in sport <UNK> <UNK> during routine assessments undertaken within <UNK> h of injury. these healthcare professionals were not blinded to phe data. sample size we allowed a maximum of one candidate prognostic factor parameter per <UNK> i-imis, which at the time of protocol development, was the main recommendation to minimise overfitting (additional file <UNK> <UNK> <UNK> the whole dataset was used for model development and internal validation, which agrees with methodological recommendations <UNK> candidate prognostic factors the available dataset contained <UNK> candidate factors <UNK> because of the sample size considerations, before any analysis, the set of candidate factors was reduced. initially, an audit was conducted to quantify missing values and to determine the measurement reliability of the eligible candidate factors <UNK> any candidate factors which had greater than <UNK> missing data or where reliability was classed as fair to poor (intraclass correlation coefficient < <UNK> were excluded <UNK> (additional file <UNK> of the remaining <UNK> eligible factors, previous evidence of prognostic value <UNK> and clinical reasoning were used to select candidate prognostic factors suitable for inclusion <UNK> this process left a final set of <UNK> candidate factors, represented by <UNK> model parameters (table <UNK> the <UNK> factors that were not included in model development are also listed in additional file <UNK> and will be utilised in a related, forthcoming exploratory study which aims to examine their association with indirect muscle injuries in elite football players. table <UNK> set of candidate prognostic factors (with corresponding number of parameters) for model development full size table statistical analysis data handling—outcome measures each participant-season was treated as independent. participants who sustained an i-imi were no longer considered at risk for that season and were included for further analysis at the start of the next season if still eligible. any upper limb imi, trunk imi or non-imi injuries were ignored, and participants were still considered at risk. eligible participants who were loaned to another club throughout that season, but had not sustained an i-imi prior to the loan, were still considered at risk. i-imis that occurred whilst on loan were included for analysis, as above. permanently transferred participants (who had not sustained an i-imi prior to leaving) were recorded as not having an i-imi during the relevant season and exited the cohort at the season end. data handling—missing data missing values were assumed to be missing at random <UNK> the continuous parameters generally demonstrated non-normal distributions, so were transformed using normal scores <UNK> to approximate normality before imputation, and back-transformed following imputation <UNK> multivariate normal multiple imputation was performed, using a model that included all candidates and i-imi outcomes. fifty imputed datasets were created in stata <UNK> (statacorp llc, texas, usa) and analysed using the mim module. prognostic model development continuous parameters were retained on their original scales, and their effects assumed linear <UNK> a full multivariable logistic regression model was constructed, which contained all <UNK> parameters. parameter estimates were combined across imputed datasets using rubin’s rules <UNK> to develop a parsimonious model that would be easier to utilise in practice, backward variable selection was performed using estimates pooled across the imputed datasets at each stage of the selection procedure to successively remove non-significant factors with p values > <UNK> this threshold was selected to approximate equivalence with akaike’s information criterion <UNK> <UNK> multiple parameters representing the same candidate factor were tested together so that the whole factor was either retained or removed. candidate interactions were not examined, and no terms were forced into the model. all analyses were conducted in stata <UNK> assessment of model performance the full and parsimonious models were used to predict i-imi risk over a season, for every participant-season in all imputed datasets. for all performance measures, each model’s apparent performance was assessed in each imputed dataset and then averaged across all imputed datasets using rubin’s rules <UNK> discrimination determines a model’s ability to differentiate between participants who have experienced an outcome compared to those who have not <UNK> quantified using the concordance index (c-index). this is equivalent to the area under the receiver operating characteristic (roc) curve for logistic regression, where <UNK> demonstrates perfect discrimination, whilst <UNK> indicates that discrimination is no better than chance <UNK> calibration determines the agreement between the model’s predicted outcome risks and those observed <UNK> evaluated using an apparent calibration plot in each imputed dataset. all predicted risks were divided into ten groups defined by tenths of predicted risk. the mean predicted risks for the groups were plotted against the observed group outcome proportions with corresponding <UNK> confidence intervals (cis). a loess smoothing algorithm showed calibration across the range of predicted values <UNK> for grouped and smoothed data points, perfect predictions lie on the <UNK> line (i.e. a slope of <UNK> the systematic (mean) error in model predictions was quantified using calibration-in-the-large (citl), which has an ideal value of <UNK> <UNK> <UNK> and the expected/observed (e/o) statistic, which is the ratio of the mean predicted risk against the mean observed risk (ideal value of <UNK> <UNK> <UNK> the degree of over or underfitting was determined using the calibration slope, where a value of <UNK> equals perfect calibration on average across the entire range of predicted risks <UNK> nagelkerke’s <UNK> was also calculated, which quantifies the overall model fit, with a range of <UNK> (no variation explained) to <UNK> (all variation explained) <UNK> assessment of clinical utility decision-curve analysis was used to assess the parsimonious model’s apparent clinical usefulness in terms of net benefit (nb) if used to allocate possible preventative interventions. this assumed that the model’s predicted risks were classed as positive (i.e. may require a preventative intervention) if greater than a chosen risk threshold, and negative otherwise. nb is then the difference between the proportion of true positives and false positives, where both were weighted by the odds of the chosen risk threshold and also divided by the sample size <UNK> positive nb values suggest the model is beneficial compared to treating none, which has no benefit to the team but with no negative cost and efficiency implications. the maximum possible nb value is the proportion with the outcome in the dataset. the model’s nb was also compared to the nb of delivering an intervention to all individuals. this is considered a treat-all strategy, offering maximum benefit to the team, but with maximum negative cost and efficiency implications <UNK> a model has potential clinical value if it demonstrates higher nb than the default strategies over the range of risk thresholds which could be considered as high risk in practice <UNK> internal validation and adjustment for overfitting to examine overfitting, the parsimonious model was internally validated using <UNK> bootstrap samples, drawn from the original dataset with replacement. in each sample, the complete model-building procedure (including multiple imputation, backward variable selection and performance assessment) was conducted as described earlier. the difference in apparent performance (of a bootstrap model in its bootstrap sample) and test performance (of the bootstrap model in the original dataset) was averaged across all samples. this generated optimism estimates for the calibration slope, citl and c-index statistics. these were subtracted from the original apparent calibration slope, citl and c-index statistics to obtain final optimism-adjusted performance estimates. the nagelkerke <UNK> was adjusted using a relative reduction equivalent to the relative reduction in the calibration slope. to produce a final model adjusted for overfitting, the regression coefficients produced in the parsimonious model were multiplied by the optimism-adjusted calibration slope (also termed a uniform shrinkage factor), to adjust (or shrink) for overfitting <UNK> finally, the citl (also termed model intercept) was then re-estimated to give the final model, suitable for evaluation in other populations or datasets. complete case and sensitivity analyses to determine the effect of multiple imputation and player transfer assumptions on model stability, the model development process was repeated: <UNK> as a complete case analysis and <UNK> as sensitivity analyses which excluded all participant-seasons where participants had not experienced an i-imi up to the point of loan or transfer, which were performed as both multiple imputation and complete case analyses. results participants during the five seasons, <UNK> participants were included, contributing <UNK> participant-seasons and <UNK> imis in the primary analyses (fig. <UNK> three players were classified as injured when they took part in phe (which affected three participant-seasons). this meant they were unavailable for full training or to play matches at that time. however, these players had commenced football specific, field-based rehabilitation around this time, so also had similar exposure to training activities as the uninjured players. as such, these players were included in the cohort because it was reasonable to assume that they could also be considered at risk of an i-imi event even during their rehabilitation activities. fig. <UNK> <UNK> participant flow chart. key: n = participants; i-imi = index indirect muscle injury full size image table <UNK> describes the frequency of included participant-seasons, and the frequency and proportion of recorded i-imi outcomes across all five seasons. for the sensitivity analyses (excluding loans and transfers), <UNK> independent participant-seasons with <UNK> imis were included; <UNK> participants were transferred on loan, whilst <UNK> participants were permanently transferred during a season, which excluded <UNK> participant-seasons in total (fig. <UNK> table <UNK> also describes the frequency of excluded participant-seasons where players were transferred either permanently or on loan, across the <UNK> seasons. table <UNK> frequency of included participant-seasons, i-imi outcomes and participant-seasons affected by transfers, per season (primary analysis) full size table table <UNK> shows anthropometric and all prognostic factor characteristics for participants included in the primary analyses. these were similar to those included in the sensitivity analyses (additional file <UNK> table <UNK> characteristics of included participants in the primary analysis full size table missing data and multiple imputation all i-imi, age and previous muscle injury data were complete (table <UNK> for all other candidates, missing data ranged from <UNK> (for hip internal and external rotation difference) to <UNK> for countermovement jump (cmj) power (table <UNK> the distribution of imputed values approximated observed values (additional file <UNK> confirming their plausibility. model development table <UNK> shows the parameter estimates for the full model and parsimonious model after variable selection (averaged across imputations). table <UNK> results of the full and parsimonious multivariable logistic regression models, with prediction formulae full size table for both models, only age and frequency of previous imis had a statistically significant (but modest) association with increased i-imi risk (p < <UNK> no clear evidence for an association was observed for any other candidate factor. model performance assessment and clinical utility table <UNK> shows the apparent performance measures for the full and parsimonious models, all of which were similar. figure <UNK> shows the apparent calibration of the parsimonious model in the dataset used to develop the model (i.e. before adjustment for overfitting). these were identical across all imputed datasets because the retained prognostic factors contained no missing values. the parsimonious model had perfect apparent overall citl and calibration slope by definition, but calibration was more variable around the <UNK> line between the expected risk ranges of <UNK> to <UNK> discrimination was similarly modest for the full (c-index = <UNK> <UNK> ci = <UNK> to <UNK> and parsimonious models (c-index = <UNK> <UNK> ci = <UNK> the apparent overall model fit was low for both models, indicated by nagelkerke <UNK> values of <UNK> for the full model and <UNK> for the parsimonious model. fig. <UNK> <UNK> apparent calibration of the parsimonious model (before adjustment for overfitting). key: e:o = expected to observed ratio; ci = confidence interval; i-imi = index indirect muscle injury full size image figure <UNK> displays the decision-curve analysis. the nb of the parsimonious model was comparable to the treat-all strategy at risk thresholds up to <UNK> marginally greater between <UNK> and <UNK> and exceeded the nb of either default strategies between <UNK> and <UNK> fig. <UNK> <UNK> decision curve analysis for the parsimonious model (before adjustment for overfitting) full size image internal validation and adjustment for overfitting table <UNK> shows the optimism-adjusted performance statistics for the parsimonious model, with full internal validation results shown in additional file <UNK> after adjustment for optimism, the overall model fit and the model’s discrimination performance deteriorated (nagelkerke <UNK> = <UNK> c-index = <UNK> <UNK> ci = <UNK> to <UNK> furthermore, bootstrapping suggested the model would be severely overfitted in new data (calibration slope = <UNK> <UNK> ci = <UNK> to <UNK> so a shrinkage factor of <UNK> was applied to the parsimonious parameter estimates, and the model intercept re-estimated to produce our final model (table <UNK> complete case and sensitivity analyses the full and parsimonious models were robust to complete case analyses and excluding loans and transfers, with comparable apparent performance estimates. for the full models, the c-index range was <UNK> to <UNK> and nagelkerke <UNK> range was <UNK> to <UNK> whilst for the parsimonious models, the c-index range was <UNK> to <UNK> and nagelkerke <UNK> range was <UNK> to <UNK> (additional files <UNK> <UNK> <UNK> <UNK> and <UNK> the same prognostic factors were selected in all parsimonious models. the degree of estimated overfitting observed in the complete case and sensitivity analyses was comparable to that observed in the main analysis (calibration slope range = <UNK> to <UNK> (additional files <UNK> <UNK> <UNK> <UNK> and <UNK> discussion we have developed and internally validated a multivariable prognostic model to predict individualised i-imi risk during a season in elite footballers, using routinely, prospectively collected preseason phe and injury data that was available at manchester united football club. this is the only study that we know of that has developed a prognostic model for this purpose, so the results cannot be compared to previous work. we included both a full model which did not include variable selection and a parsimonious model, which included a subset of variables that were statistically significant. the full model was included because overfitting is likely to increase when variable inclusion decisions are based upon p values. in addition, the use of p value thresholds for variable selection is somewhat arbitrary. however, the overfitting that could have arisen in the parsimonious model after using p values in this way was accounted for during the bootstrapping process, which replicated the variable selection strategy based on p values in each bootstrap sample. the performance of the full and parsimonious models was similar, which means that utilising all candidate factors offered very little advantage over using two for making predictions. indeed, variable selection eliminated <UNK> candidate prognostic factors that had no clear evidence for an association with i-imis. our findings confirm previous suggestions that phe tests designed to measure modifiable physical and performance characteristics typically offer poor predictive value <UNK> this may be because unless particularly strong associations are observed between a phe test and injury outcome, the overlap in scores between individuals who sustain a future injury and those who do not results in poor discrimination <UNK> additionally, after measurement at a single timepoint (i.e. preseason), it is likely that the prognostic value of these modifiable factors may vary over time <UNK> due to training exposure, environmental adaptations and the occurrence of injuries <UNK> the variable selection process resulted in a model which included only age and the frequency of previous imis within the last <UNK> years, which are simple to measure and routinely available in practice. our findings were similar to the modest association previously observed between age and hamstring imis in elite players <UNK> however, whilst a positive previous hamstring imi history has a confirmed association with future hamstring imis <UNK> we found that for lower extremity i-imis, cumulative imi frequency was preferred to the time proximity of any previous imi as a multivariable prognostic factor. nevertheless, the weak prognostic strength of these factors explains the parsimonious model’s poor discrimination and low potential for clinical utility. our study is the first to utilise decision-curve analysis to examine the clinical usefulness of a model for identifying players at high risk of imis and who may benefit from preventative interventions such as training load management, strength and conditioning or physiotherapy programmes. our parsimonious model demonstrated no clinical value at risk thresholds of less than <UNK> because its nb was comparable to that of providing all players with an intervention. indeed, the only clinically useful thresholds that would indicate a high-risk player would be <UNK> where the model’s nb was greater than giving all players an intervention. however, because of the high baseline imi risk in our population (approximately <UNK> of participant-seasons affected), the burden of imis <UNK> and the minimal costs <UNK> versus the potential benefits of such preventative interventions in an elite club setting, these thresholds are likely to be too high to be acceptable in practice. accordingly, it would be inappropriate to allocate or withhold interventions based upon our model’s predictions. because of severe overfitting our parsimonious model was optimistic, which means that if used with new players, prediction performance is likely to be worse <UNK> although our model was adjusted to account for overfitting and hence improve its calibration performance in new datasets, given the limitations in performance and clinical value, we cannot recommend that it is validated externally or used in clinical practice. this study has some limitations. we acknowledge that the development of our model does not formally take account of the use of existing injury prevention strategies, including those informed by phe, and their potential effects on the outcome. rather, we predicted i-imis under typical training and match exposure and under routine medical care. in addition, it should be noted that injury risk predictions at an elite level football club may not generalise to other types of football clubs or sporting institutions, where ongoing injury prevention strategies may not be comparable in terms of application and equipment. we measured candidate factors at one timepoint each season and assumed that participant-seasons were independent. whilst statistically complex, future studies may improve predictive performance and external validity by harnessing longitudinal measurements and incorporating between-season correlations. we did not perform a competing risks analysis to account for players not being exposed to training and match play due to injuries other than i-imis. that is, our approach predicted the risk of i-imis in the follow up of players, allowing other injury types to occur and therefore possibly limiting the opportunity for i-imis during any rehabilitation period. the competing risk of the occurrence of non-imis was therefore not explicitly modelled and players remained in the risk set after a non-imi had occurred. we also merged all lower extremity i-imis rather than using specific muscle group outcomes. although less clinically meaningful, this was necessary to maximise statistical power. nevertheless, our limited sample size prohibited examination of complex non-linear associations and only permitted a small number of candidates to be considered. a lack of known prognostic factors <UNK> meant that selection was mainly guided by data quality control processes and clinical reasoning, so it is possible that important factors were not included. risk prediction improves when multiple factors with strong prognostic value are used <UNK> therefore, future research should aim to identify novel prognostic factors, so that these can be used to develop models with greater potential clinical benefit. this may also allow updating of our model to improve its performance and clinical utility <UNK> until the evidence base improves, and because of sample size limitations, it is likely that any further attempts to create a prognostic model at individual club level would suffer similar issues. importantly, this means that for any team, the value of using preseason phe data to make individualised predictions or to select bespoke injury prevention strategies remains to be demonstrated. however, the pooling of individual participant data from several participating clubs may increase sample sizes sufficiently to allow further model development studies <UNK> where a greater number of candidate factors could be utilised. conclusion using phe and injury data available preseason, we have developed and internally validated a prognostic model to predict i-imi risk in players at an elite club, using current methodological best practice. the paucity of known prognostic factors and data requirements for model building severely limited the model’s performance and clinical utility, so it cannot be recommended for external validation or use in practice. further research should prioritise identifying novel prognostic factors to improve future risk prediction models in this field.

<|EndOfText|>

temporal recalibration for improving prognostic model development and risk predictions in settings where survival is improving over time abstract background prognostic models are typically developed in studies covering long time periods. however, if more recent years have seen improvements in survival, then using the full dataset may lead to out-of-date survival predictions. period analysis addresses this by developing the model in a subset of the data from a recent time window, but results in a reduction of sample size. methods we propose a new approach, called temporal recalibration, to combine the advantages of period analysis and full cohort analysis. this approach develops a model in the entire dataset and then recalibrates the baseline survival using a period analysis sample. the approaches are demonstrated utilizing a prognostic model in colon cancer built using both cox proportional hazards and flexible parametric survival models with data from <UNK> from the surveillance, epidemiology, and end results (seer) program database. comparison of model predictions with observed survival estimates were made for new patients subsequently diagnosed in <UNK> and followed-up until <UNK> results period analysis and temporal recalibration provided more up-to-date survival predictions that more closely matched observed survival in subsequent data than the standard full cohort models. in addition, temporal recalibration provided more precise estimates of predictor effects. conclusion prognostic models are typically developed using a full cohort analysis that can result in out-of-date long-term survival estimates when survival has improved in recent years. temporal recalibration is a simple method to address this, which can be used when developing and updating prognostic models to ensure survival predictions are more closely calibrated with the observed survival of individuals diagnosed subsequently. prognostic models, temporal recalibration, period analysis, up-to-date survival predictions, flexible parametric survival models, cox proportional hazards models topic: patient prognosisdatasets issue section: original article key messages if survival has been improving over time, standard full cohort models can under-estimate survival. period analysis uses a more recent subset of data to produce survival estimates which are more up-to-date, however it reduces the sample size and number of events used in the analysis. temporal recalibration combines the sample size advantages associated with full cohort analysis with the up-to-date estimates produced with period analysis. temporal recalibration can be used at the model development stage or to update existing prognostic models when new data becomes available. introduction for individuals diagnosed with a particular disease or health condition, prognostic models can provide outcome predictions and aid treatment <UNK> in this article, we focus on the outcome of time-until-death from colon cancer and survival predictions, however the approach can be generalized. prognostic models contain multiple predictors and are typically developed using a regression format such as logistic, cox or a parametric survival model. it is often of interest to provide survival predictions at different time points, such as <UNK> <UNK> and <UNK> years after diagnosis. for <UNK> predictions, it is necessary to have a model development dataset that includes individuals who were diagnosed at least <UNK> years ago, such that the analysis has sufficient follow-up length. however, this can lead to out-of-date (miscalibrated) survival predictions for recently diagnosed individuals if there have been improvements in survival over calendar time: e.g. in recent years treatment may have improved survival compared with <UNK> or <UNK> years earlier. improvements in survival for colorectal cancer have been reported in a number of different <UNK> with the development of online tools and apps, survival estimates from prognostic models have become more accessible. some models such as predict, a prognostic model for breast <UNK> and qcancer, a prognostic model for colorectal <UNK> are freely available online for both clinicians and the public. the survival estimates produced from these, and many other webtools, are from a standard full cohort analysis approach. such models may produce survival predictions that under-estimate the true survival probability of recently diagnosed patients (and conversely over-estimate the actual risk of adverse outcomes). period analysis has been used in population-based cancer studies to obtain up-to-date estimates of <UNK> and in this article we explore its use in the development and updating of prognostic models. period analysis defines a recent time window and only the risk-time and events that fall within this window contribute to the estimates of the hazard rates and predictor <UNK> this method is not commonly used for prognostic models, however keogh et <UNK> produced survival predictions for cystic fibrosis patients using period analysis. a disadvantage with period analysis is that it results in a reduction of sample size for model development. this could be particularly problematic in small datasets, when there are rare predictor patterns or rare events, and may lead to a low number of events per predictor parameter, which increases the potential for model <UNK> in this article we introduce a new approach, called temporal recalibration, that combines the use of full cohort analysis, period analysis and recalibration methods. specifically it aims to maximize the use of data toward model development, with the full dataset used to model predictor effects and the baseline survival recalibrated in a recent time window to produce more up-to-date survival predictions for new individuals. we illustrate and compare these methods using an example of colon cancer from the surveillance, epidemiology, and end results (seer) program <UNK> methods cox proportional hazards models and post-estimation of the baseline cox proportional hazards (ph) models are frequently used to develop prognostic <UNK> the model is of the form: <UNK> with h(t;xi) the hazard function, <UNK> the baseline hazard function and βxi the prognostic <UNK> the cumulative hazard function h(t;xi) must be approximated to calculate survival predictions as it is not directly modelled. this can be achieved post-estimation using a non-parametric approach, or by a smoother using fractional polynomials or <UNK> in this article, restricted cubic splines are used to create a smooth approximation of the log cumulative baseline hazard post-estimation. the same knot locations as the flexible parametric survival models (fpms) (see supplementary file <UNK> available as supplementary data at ije online) were used to ensure a fair comparison. the baseline survival curve was approximated by <UNK> and survival predictions for individuals with different values of the prognostic index by <UNK> it is possible to extend these models to include time-dependent predictor effects (i.e. non-proportional hazards). period <UNK> (see the period analysis section) can be performed using delayed entry techniques. flexible parametric survival models although cox models are widely used for prognostic modelling, fpms have several advantages. fpms directly model the log baseline cumulative hazard function which allows for smooth survival curves to be produced during model development, without the need for post-estimation <UNK> it remains straightforward to include time-dependent predictor <UNK> and incorporate delayed entry. fpms use restricted cubic splines to directly model the baseline <UNK> (see supplementary file <UNK> available as supplementary data at ije online). a prognostic model can be written in the following form where <UNK> is the restricted cubic spline function and βxi is the prognostic <UNK> <UNK> period analysis period analysis, in the context of population-based cancer data, was developed by brenner and <UNK> only individuals who contribute follow-up time during the period window are included in the analysis to estimate predictor effects and baseline survival (see table <UNK> this reduces the sample size since people who experienced the event before the window (e.g. participant b, see figure <UNK> are excluded. only the events that occur within the window are considered in the analysis and therefore the choice of window width is a balance between ensuring up-to-date survival estimates and having sufficient events (and events per predictor parameter). the width of the window could be determined by meeting the criteria defined by riley et <UNK> further details and a sensitivity analysis of using different window widths are included in supplementary file <UNK> available as supplementary data at ije online. figure <UNK> contribution of follow-up time from four hypothetical participants (diagnosed <UNK> january) to a <UNK> period window of <UNK> open in new tabdownload slide contribution of follow-up time from four hypothetical participants (diagnosed <UNK> january) to a <UNK> period window of <UNK> table <UNK> summary of the data used for the estimation of the baseline and predictor effects for each method method baseline predictor effects full cohort full full temporal recalibration recent full period analysis recent recent open in new tab delayed entry techniques are used to left truncate the follow-up time of people diagnosed before the window so that the short-term hazard rates are only estimated from those diagnosed within or shortly before the period window (e.g. participant d, see figure <UNK> this method has been shown to produce more up-to-date survival estimates than full cohort analysis in population-based cancer settings for many types of cancer in different <UNK> and is used routinely within international cancer survival <UNK> temporal recalibration a key disadvantage with period analysis is the reduction in sample size and number of events for model estimation. to address this, we propose temporal recalibration, which combines the sample size advantages associated with the full cohort analysis with the up-to-date predictions from period analysis. the process of fitting a temporal recalibration model is as follows. (i) fit a survival model using the full cohort dataset to estimate the predictor effects using all individuals. (ii) recalibrate the model by re-estimating the baseline using the subset of individuals from a period analysis sample, while holding the predictor effect estimates from step (i) fixed. recalibrating the baseline in a recent period analysis sample allows for improvements in survival to be captured and leads to more up-to-date predictions. under proportional hazards the model can be written in the following form for fpms: <UNK> where <UNK> is the updated spline function for the log cumulative baseline hazard function estimated in the recent period data, <UNK> are the knot locations from the full cohort model and offset(pii) is the prognostic index estimated from the full cohort model as an offset term. fixing the predictor effects with constraints when fitting in the period analysis sample would offer an equivalent approach. for a cox ph model it can be written as: <UNK> where hnew(t;xi) and <UNK> are the hazard and baseline hazard functions respectively, estimated on the recent time window, and offset(pii) is the prognostic index estimated from the full cohort model as an offset term. as with period analysis, the choice of the window width is a bias-variance trade-off (see supplementary file <UNK> available as supplementary data at ije online). the width of the window could possibly be reduced compared with a standard period analysis approach as it is only necessary to have a sufficient number of events to estimate the baseline (and not the predictor effects). in temporal recalibration we explicitly assume the predictor effects are the same as they were in the full cohort model (see table <UNK> assessing the performance of predictions marginal survival (i.e. average across all individuals) can be calculated both within-sample (i.e. in the same dataset used to develop the model) and out-of-sample (i.e. in new individuals) by calculating every individual’s predicted survival over time, and then averaging the survival <UNK> <UNK> out-of-sample marginal survival predictions can be compared with the observed survival (kaplan–meier estimates) to determine the calibration of a model’s survival predictions for a new group of individuals. studying the marginal survival only assesses how well the model performs on average (sometimes referred to as <UNK> whereas calibration plots can be used to determine the model’s performance in different risk groups at particular time points. in this article the risk groups were defined by dividing the prognostic index from the full cohort models into <UNK> equally sized risk groups. the e/o statistic quantifies calibration-in-the-large by comparing predicted or expected (e) outcome risk to the observed (o) risk through <UNK> exp <UNK> e is calculated from the marginal survival prediction from the model [sexp(t)] and o is from the observed kaplan–meier curve [sobs(t)]. a value of <UNK> indicates <UNK> harrell’s c-index can be used to assess the concordance of survival predictions from proportional hazards models. a value of <UNK> indicates perfect <UNK> we now compare full cohort, temporal recalibration and period analysis approaches using an illustrative example of colon cancer. example data we used the public-access seer database from the <UNK> the seer program covers <UNK> of the us population and collects population-based data on all reported cases of cancer within the cancer registries included in the seer <UNK> the analysis was restricted to adults who were aged <UNK> years at the time of their diagnosis of colon cancer <UNK> codes <UNK> if there were any duplicates of the patient id, only the first record was retained. patients with an unknown survival time (recorded to the nearest month) or incomplete dates for their diagnosis or death were also excluded. data from <UNK> were available for this analysis. as the aim was to identify which model gave better long-term survival predictions in new data, the data were split at <UNK> for illustration purposes. data from <UNK> were used to develop the models and a <UNK> year period window from <UNK> january <UNK> to <UNK> december <UNK> was used to fit the temporal recalibration and period analysis models. the data from <UNK> were then used to validate the models. baseline characteristics for the development dataset can be found in table <UNK> table <UNK> baseline characteristics of the <UNK> <UNK> participants in the development dataset once participants with missing predictor values were removed. mean (sd) is presented for continuous variables and n (%) for categorical variables variable mean (sd) or n (%) age <UNK> <UNK> sex male <UNK> <UNK> <UNK> female <UNK> <UNK> <UNK> race white <UNK> <UNK> <UNK> black <UNK> <UNK> stage at diagnosis stage <UNK> <UNK> <UNK> <UNK> stage <UNK> <UNK> <UNK> <UNK> stage <UNK> <UNK> <UNK> grade of tumour at diagnosis grade <UNK> <UNK> <UNK> grade <UNK> <UNK> <UNK> <UNK> grade <UNK> <UNK> <UNK> grade <UNK> <UNK> <UNK> open in new tab models cause-specific cox and fpms were fitted, meaning that deaths due to causes other than colon cancer were censored. age at diagnosis, stage at diagnosis (localized, regional, distant), grade of the tumour (i–iv), sex and race (restricted to white and black patients only) were included as predictors. age was modelled using restricted cubic splines with three degrees of freedom, and stage, grade, sex and race were modelled categorically. all predictors were forced to be included (i.e. there was no variable selection). for the fpms, five degrees of freedom were used to model the log baseline cumulative hazard and, to simplify the process of recalibration, the baseline splines were not orthogonalized. example code used to fit these models is provided in supplementary file <UNK> available as supplementary data at ije online. in this illustrative example, any participants with missing predictor values were excluded in order to more easily compare the approaches, though in practice multiple imputation is usually preferable. age at diagnosis was <UNK> to provide more stability in the extremes by adding an additional constraint forcing the splines to be constant for the top and bottom <UNK> of the age <UNK> in further analyses, the ph assumption was relaxed using time-dependent predictor effects for age and stage. to compare the model predictions from these three approaches, the marginal predicted survival for the <UNK> patients diagnosed in <UNK> was calculated using each model and compared with the observed kaplan–meier estimates. this was further assessed through calibration plots at <UNK> years after diagnosis. all analyses were performed using stata version <UNK> fpms were fitted using the user-written package <UNK> and harrell’s c-index was calculated for these models using the user-written package <UNK> results in terms of predictor effect estimates, the log hazard ratios and standard errors were very similar regardless of whether cox models or fpms were used (table <UNK> the log hazard ratios were fairly similar for full cohort and period analysis, however the standard errors from the period analysis approaches were around twice as large due to the reduction in sample size. overfitting was minimial due to the large number of events relative to the number of predictor parameters, highlighted by a uniform shrinkage <UNK> for the full cohort model of <UNK> table <UNK> comparison of the sample size, number of events, log hazard ratios (hr) and standard errors (s.e.) of the log hazard ratios for the categorical predictors in each model flexible parametric survival model cox proportional hazards model full cohort period analysis full cohort period analysis sample size <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> number of events <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> predictor effects: log hr (s.e. of log hr) female <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> black <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> stage <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> stage <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> grade <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> grade <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> grade <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> open in new tab similar marginal survival predictions were produced regardless of whether cox or fpms were used when predicting for patients diagnosed in <UNK> (figure <UNK> the marginal survival predictions for temporal recalibration and period analysis were very similar and consistently provided more well-calibrated estimates than the standard full cohort model. the survival probability is under-estimated for all risk groups in the full cohort analysis models, and in <UNK> of the <UNK> groups the predictions are the furthest from the reference line. however, using temporal recalibration, all the predicted survival estimates increase and agree more closely with the kaplan–meier estimates. although the marginal survival predictions from the temporal recalibration and period analysis models are very similar, small differences in predicted survival can be seen for the highest risk groups. including time-dependent effects for age and stage in the fpm improves the calibration in the third highest risk group, however there is very little difference in the marginal survival estimates, see supplementary file <UNK> available as supplementary data at ije online. figure <UNK> external validation of the models to assess the calibration of survival predictions for new patients (diagnosed in <UNK> with follow-up data until <UNK> top: comparison of marginal observed (kaplan–meier) and predicted survival from each model. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. bottom: <UNK> calibration plots comparing the observed and predicted cancer-specific survival probabilities from each model. open in new tabdownload slide external validation of the models to assess the calibration of survival predictions for new patients (diagnosed in <UNK> with follow-up data until <UNK> top: comparison of marginal observed (kaplan–meier) and predicted survival from each model. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. bottom: <UNK> calibration plots comparing the observed and predicted cancer-specific survival probabilities from each model. a comparison of the model performance in terms of calibration and concordance of survival predictions is displayed in table <UNK> calibration improves by <UNK> by performing temporal recalibration which is large at the population level and improves the net benefit of the <UNK> in other scenarios, the difference may be greater if there have been more substantial changes in baseline survival over calendar time. as the predictor effects for the temporal recalibration models are constrained to be the same as those from the full cohort model, harrell’s c-index will always be the same for these models. in this example, the predictor effects for the period analysis models were also very similar and therefore harrell’s c-index is the same to three decimal places. table <UNK> comparison of model performance in the validation dataset. the difference in observed and predicted marginal survival at <UNK> years after diagnosis <UNK> – <UNK> the ratio of expected to observed risk at <UNK> years after diagnosis (e/o) and harrell’s c-index model <UNK> – <UNK> <UNK> harrell’s c-index full cohort: fpm <UNK> <UNK> <UNK> full cohort: cox <UNK> <UNK> <UNK> temporal recalibration: fpm <UNK> <UNK> <UNK> temporal recalibration: cox <UNK> <UNK> <UNK> period analysis: fpm <UNK> <UNK> <UNK> period analysis: cox <UNK> <UNK> <UNK> a <UNK> kaplan–meier estimate at <UNK> years after diagnosis; <UNK> <UNK> year marginal survival prediction from the model. open in new tab updating prognostic models temporal recalibration can also be used to produce up-to-date survival estimates when new data become available by simply re-estimating the baseline without the need for repeating the model-building process or re-estimating the predictor effects. this is akin to previous work by riley et <UNK> schuetz et <UNK> and <UNK> that show how recalibrating the baseline hazard in new (local) settings can be important. to illustrate this, prognostic models were fitted using fpms with data from <UNK> and data from <UNK> was used to update these models. as stage was only available from <UNK> onwards, only age, sex, race and grade were included as predictors, and for simplicity phs was assumed. table <UNK> defines the models <UNK> that were compared in this analysis. table <UNK> comparison of the data used to estimate the predictor effects and baseline of each flexible parametric survival model model description data for predictor effects data for baseline <UNK> original full cohort model <UNK> <UNK> <UNK> full cohort model with all available data <UNK> <UNK> <UNK> full cohort model with most recent data <UNK> <UNK> <UNK> temporal recalibration of <UNK> <UNK> period window <UNK> <UNK> temporal recalibration of <UNK> <UNK> period window <UNK> <UNK> period analysis period window <UNK> period window <UNK> open in new tab to illustrate the difference in survival predictions for these models, <UNK> survival was estimated for patients diagnosed in <UNK> and compared with the kaplan–meier estimates for these patients, see figure <UNK> figure <UNK> comparison of marginal observed (kaplan–meier curve) and predicted survival from the original and updated models. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. open in new tabdownload slide comparison of marginal observed (kaplan–meier curve) and predicted survival from the original and updated models. note: the predictions from the temporal recalibration and period analysis models overlay almost exactly. using the original model <UNK> resulted in a difference between the observed and predicted survival of <UNK> which was reduced to <UNK> by using a longer timespan <UNK> and <UNK> by using a more recent cohort <UNK> temporal recalibration models <UNK> and <UNK> and the period analysis model <UNK> produced the closest estimates which differed by <UNK> performing temporal recalibration improves the calibration of the full cohort models by at least <UNK> and a larger improvement of <UNK> is observed when recalibrating the original full cohort model. despite different models being temporally recalibrated, the predictions overlaid exactly. this demonstrates that temporal recalibration is appropriate in this example since the predictor effects do not greatly change over time and therefore it is only necessary to re-estimate the baseline. discussion often there are large underlying improvements in survival over the follow-up available in a model development dataset, which presents a challenge for subsequently making predictions for newly diagnosed patients. we have shown that survival predictions from prognostic models developed using a standard full cohort approach underestimate survival of recently diagnosed patients. however, more up-to-date, and thus accurate, survival predictions can be produced by developing prognostic models using temporal recalibration, where the baseline hazard is recalibrated in a subset of most recent data. this idea is similar to the approach of period analysis, but has the additional benefit of more precisely estimating predictor effects as it uses all the data to estimate the prognostic index. unlike period analysis, it is possible to directly apply temporal recalibration to a range of existing prognostic models (i.e. cox ph models, fpms with time-dependent effects) to update the survival predictions without the need of repeating the model-building process or re-estimating predictor effects. no additional data are required, only a period analysis sample of the most recent data is needed to re-estimate the baseline and produce more up-to-date predictions which better reflect the survival of those currently being diagnosed. we have also shown the importance of regularly updating prognostic models when new data become available and how this can easily be achieved using temporal recalibration. we have used seer public use data for colon cancer patients, with a range of predictors in order to illustrate the approach. for cancer sites and settings with smaller improvements over calendar time, the predicted survival estimates from a standard and temporally recalibrated approach would differ less. however, the approach would still be valid in this case. in this example we only showed complete case analysis, however, temporal recalibration could be performed on imputed datasets and the survival predictions from the models could be combined using rubin’s <UNK> example code for fitting these models is included in supplementary file <UNK> available as supplementary data at ije online. temporal recalibration assumes that the predictor effects are the same in the recent data as in the full cohort and therefore do not change as a function of diagnosis date. this is in contrast to period analysis which updates both the baseline and the prognostic index. therefore, the parameter estimates from the full cohort and period analysis models can be informally compared to verify that this assumption is plausible. further, careful consideration should be given to the consistency of predictor’s values over time, but this is an issue generally and not specific to the approach we outline here. temporal recalibration is a similar concept to model <UNK> in which the calibration of predictions from a previously developed prognostic model are externally validated using new data obtained from a more recent time point. in that setting, if the model consistently under or over predicts survival, it is recalibrated; typically predictor effects are kept fixed (i.e. as originally estimated), but the baseline is updated. the difference with temporal recalibration is that the period analysis sample (used for the recalibration) is not a separate dataset and has already been included in the full cohort model to estimate predictor effects. an alternative to temporal recalibration and period analysis would be to model the year of diagnosis directly and then predict survival using the most recent year included in the model. this approach would make developing and updating existing prognostic models more challenging as it would require the year of diagnosis to be modelled appropriately, which may include time-dependent effects and non-linear terms. this method would also rely more heavily on extrapolation of effects when producing long-term survival predictions for the most recent calendar year. however, with temporal recalibration the long-term hazards are estimated directly from those included in the period window. with both temporal recalibration and modelling the year of diagnosis it may be necessary to consider interactions between predictor effects and year of <UNK> many existing prognostic models use the standard full cohort approach. we have illustrated that using temporal recalibration could update these survival predictions and be a more accurate reflection of the prognosis of patients who are currently being diagnosed.

<|EndOfText|>

methods and reporting of systematic reviews of comparative accuracy were deficient: a methodological survey and proposed guidance abstract objective the objective of this study was to examine methodological and reporting characteristics of systematic reviews and meta-analyses which compare diagnostic test accuracy (dta) of multiple index tests, identify good practice, and develop guidance for better reporting. results of <UNK> reviews, <UNK> <UNK> reviews restricted study selection and test comparisons to comparative accuracy studies while the remaining <UNK> <UNK> reviews included any study type. fifty-three reviews <UNK> statistically compared test accuracy with only <UNK> <UNK> of these using recommended methods. reporting of several items—in particular the role of the index tests, test comparison strategy, and limitations of indirect comparisons (i.e., comparisons involving any study type)—was deficient in many reviews. five reviews with exemplary methods and reporting were identified. conclusion reporting quality of reviews which evaluate and compare multiple tests is poor. the guidance developed, complemented with the exemplars, can assist review authors in producing better quality comparative reviews. previous article in issuenext article in issue keywords comparative accuracydiagnostic accuracytest accuracymeta-analysissystematic reviewtest comparison what is new? key findings • methods known to have methodological flaws are frequently used in reviews which evaluate and compare the accuracy of multiple tests. reporting quality is variable but often poor. • test comparisons based on studies that have not directly compared the index tests are common in reviews but review authors fail to appreciate the potential for bias due to confounding. what this adds to what was known? • guidance developed to promote better conduct and reporting of test comparisons in diagnostic accuracy reviews and to facilitate their appraisal. exemplars also provided to assist review authors. what is the implication and what should change now? • to avoid misleading conclusions and recommendations, the methodological rigor and reporting of comparative reviews should be improved. • researchers and funders should recognize the merit of designing studies for obtaining reliable evidence about the relative accuracy of competing diagnostic tests. <UNK> introduction medical tests are essential in guiding patient management decisions. ideally, tests should only be recommended for routine clinical use based on evidence of their clinical performance (diagnostic accuracy) and clinical impact (benefits and harms) derived from relevant, high-quality primary studies, and systematic reviews. systematic reviews and meta-analyses of diagnostic test accuracy (dta) generally assess the performance of one index test at a time, thus providing a limited view of the test options available for a given condition and no information about the performance of alternatives. however, comparative reviews which compare the accuracy of two or more index tests are potentially more useful to clinicians and policy-makers for guiding decision-making about optimal test selection. because test evaluation is often limited to the assessment of test accuracy with limited or no regulatory requirement to demonstrate clinical impact <UNK> it is vital that in the rapidly expanding evidence base, comparative accuracy reviews are conducted appropriately and well reported to avoid misleading conclusions and recommendations. several reporting checklists have been developed to improve the transparency and reproducibility of medical research, including the preferred reporting items for systematic reviews and meta-analyses (prisma) checklist <UNK> and prisma-dta, the extension for dta reviews <UNK> comparative accuracy reviews and meta-analyses are more challenging to perform than those of a single test; high-quality reporting will enable assessment of the credibility of analysis methods and findings. therefore, our aim was to summarize the methodological and reporting characteristics of comparative accuracy reviews, provide examples of good practice, and develop guidance for improving the reporting of test comparisons in future dta reviews. <UNK> methods <UNK> terminology to avoid confusion due to lack of standard terminology for types of test accuracy studies and systematic reviews, we describe here our choice of terminology. in appendix box <UNK> we provide a summary and other relevant definitions. unlike randomized controlled trials (rcts) of interventions, which have a control arm, most test accuracy studies do not compare the index test with alternative index tests <UNK> we used the term “noncomparative” to describe a primary study that evaluated a single index test or only one of the index tests being evaluated in a review, and “comparative” to describe a study that made a head-to-head comparison by comparing the accuracy of at least two index tests in the same study population. a comparative study may either randomize patients to receive only one of the index tests (randomized design), or apply all the index tests to each patient (paired or within-subject design) <UNK> with both designs, patients also receive the reference standard. for brevity, we will often refer to the index test simply as test. we defined a comparative accuracy review as a review that met at least one of the following four criteria: <UNK> clear objective to compare the accuracy of at least two tests; <UNK> selected only comparative studies; <UNK> performed statistical analyses comparing the accuracy of all or a pair of tests; or <UNK> performed a direct (head-to-head) comparison of two tests. reviews that assessed multiple tests but did not meet any of the four criteria were termed a multiple test review. such reviews assess each test individually without making formal comparisons between tests and often include a large number of tests such as signs and symptoms from clinical examination. we included this category of reviews to be comprehensive and to avoid excluding reviews in the absence of established terminology. the two main approaches for test comparisons in a dta review are direct and indirect (between-study uncontrolled) comparisons (appendix fig. <UNK> in a direct comparison, only studies that have evaluated all the index tests are included in the comparison, whereas an indirect comparison includes all eligible studies that have evaluated at least one of the index tests. <UNK> data sources we used an existing collection of <UNK> systematic reviews published up to october <UNK> the reviews were originally identified for an earlier empirical study using a previously described search strategy <UNK> the reviews were identified by searching the database of abstracts of reviews of effects (dare) for reviews with a structured abstract and the cochrane database of systematic reviews (cdsr issue <UNK> <UNK> reviews undergo quality appraisal before inclusion in dare and so we expect reviews in dare to be of higher quality than would be expected in the wider literature. we did not update the search because dare is no longer being updated and we judged it unlikely that more recent reviews from the general literature would be of better methodological quality given the findings of recent empiric studies of dta reviews <UNK> early publications <UNK> and <UNK> of dta reviews followed methodology for intervention reviews and key advances in methodology for dta reviews were published between <UNK> and <UNK> <UNK> for these reasons, and to make allowance for dissemination of methods, reviews for the current study were limited to a <UNK> period from january <UNK> to october <UNK> <UNK> eligibility criteria all test accuracy reviews that evaluated at least two tests and included a meta-analysis were eligible. we excluded reviews where full-text papers were unavailable, had insufficient data to determine study type (comparative or noncomparative), or where different tests were analyzed together as a single test without separate meta-analysis results for each test. <UNK> review selection and data extraction using a revised screening form from a previous empiric study, one assessor (y.t. or c.p.) assessed review eligibility by screening the abstract, followed by full-text examination. when eligibility was unclear, the inclusion decision was made following discussion with a member of the author team (j.d.). we scrutinized full-text articles and their supplementary files. data extraction was undertaken by one assessor (y.t.). to verify the data, a random subset of half of the included reviews was generated using the surveyselect procedure in sas software, version <UNK> (sas institute, cary, nc, usa). data were extracted from these reviews by a second assessor. any disagreements were discussed by the two assessors and agreement was achieved without having to involve a third person. we focused on methodological and reporting characteristics likely to differ between reviews of a single test and comparative reviews. we extracted data on general, methodological, and reporting characteristics. these included data on target condition, tests evaluated, study design, and the analytical methods used for comparing tests and investigating differences between studies. <UNK> development of test comparison reporting guidance to identify a set of criteria, we used the list of methodological and reporting characteristics that we devised and the prisma-dta checklist, combined with theoretical reasoning based on published methodological recommendations <UNK> <UNK> <UNK> and the cochrane handbook for systematic reviews of diagnostic test accuracy <UNK> the criteria were selected to emphasize their importance for test comparisons when completing the prisma-dta checklist for a comparative review. <UNK> data analysis we computed descriptive statistics for categorical variables as frequencies and percentages. continuous variables were summarized using the median, range, and interquartile range. using the criteria and definition specified in section <UNK> we categorized reviews into comparative and multiple tests reviews. we subdivided comparative reviews into comparative reviews with and without a statistical comparison because one of the key aspects that we examined was synthesis methods. thus we summarized and presented our findings within three review categories. all data analyses were performed using stata se version <UNK> (stata-corp, college station, tx, usa). <UNK> results the flow of reviews through the screening and selection process is shown in fig. <UNK> of the <UNK> reviews in the collection, <UNK> reviews met the inclusion criteria. download : download high-res image <UNK> : download full-size image fig. <UNK> flow of reviews through the selection process. *the <UNK> comparative accuracy reviews met at least one of the following four criteria: <UNK> clear objective to compare the accuracy of at least two tests; <UNK> selected only comparative studies; <UNK> performed statistical analyses comparing the accuracy of all or at least a pair of tests; or <UNK> performed a direct (head-to-head) comparison of two tests. <UNK> general characteristics there were <UNK> comparative reviews and <UNK> multiple test reviews. of the <UNK> comparative reviews, <UNK> <UNK> formally compared test accuracy. characteristics of the <UNK> reviews are summarized in table <UNK> the reviews were published in <UNK> different journals, with the majority <UNK> <UNK> in specialist medical journals. the reviews covered a broad array of target conditions and test types, with neoplasms <UNK> and imaging tests <UNK> being the most frequently assessed target condition and test type. the median (interquartile range) number of comparative and noncomparative studies included per review were <UNK> <UNK> to <UNK> and <UNK> <UNK> to <UNK> respectively. table <UNK> descriptive characteristics of <UNK> reviews of comparative accuracy and multiple tests characteristic comparative reviews multiple test reviews total statistical test performed to compare accuracy yes no or uncleara number of reviews <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> year of publication <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> type of publication cochrane review <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> general medical journal <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> specialist medical journal <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> technology assessment report <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> number of tests evaluated <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> clinical topic (according to <UNK> version: <UNK> circulatory system <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> digestive system <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> infectious and parasitic diseases <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> injury, poisoning, and certain other consequences of external causes <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> mental, behavioral, or neurodevelopmental disorders <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> musculoskeletal system and connective tissue <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> neoplasms <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> other <UNK> codesc <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> type of tests evaluated biopsy <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> clinical and physical examination <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> device <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> imaging <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> laboratory <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> rdt or poct <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> self-administered questionnaire <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> combinations of any of the aboved <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> clinical purpose of the tests diagnostic <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> monitoring <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> prognostic/prediction <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> response to treatment <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> screening <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> staging <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> number of test accuracy studies in reviews median (range) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> interquartile range <UNK> <UNK> <UNK> <UNK> number of comparative studies median (range) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> interquartile range <UNK> <UNK> <UNK> <UNK> number of noncomparative studies median (range) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> interquartile range <UNK> <UNK> <UNK> <UNK> abbreviations: <UNK> international classification of diseases, eleventh revision; rdt, rapid diagnostic test; poct, point of care test. numbers in parentheses are column percentages unless otherwise stated. percentages may not add up to <UNK> because of rounding. a in <UNK> reviews, it was unclear whether a statistical comparison of test accuracy was done. b includes only studies published up to october <UNK> c includes <UNK> <UNK> codes that had fewer than <UNK> reviews across the <UNK> groups. d tests evaluated in a review were not of the same type. <UNK> statistical characteristics <UNK> use of comparative studies and test comparison strategies sixteen <UNK> reviews restricted study selection and test comparisons to comparative studies, whereas the remaining <UNK> <UNK> reviews included any study type (table <UNK> in <UNK> reviews <UNK> both direct and indirect comparisons were performed with the direct comparisons performed as secondary analyses using pairs of tests for which data were available. direct comparisons were not performed in <UNK> <UNK> reviews even though comparative studies were available in <UNK> of the reviews and qualitative or quantitative syntheses would have been possible. table <UNK> strategies and methods for test comparisons characteristic comparative reviews multiple test reviews total statistical analyses to compare test accuracy yes no or unclear number of reviewsa <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> study type comparative only <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> any study type <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> test comparison strategy direct comparison only <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> indirect comparison only—comparative studies available <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> indirect comparison only—no comparative studies available <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> both direct and indirect comparison <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> none <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> method used for test comparisonb meta-regression—hierarchical model <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> meta-regression—sroc regression <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> meta-regression—ancova <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> meta-regression—logistic regression <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> univariate pooling of difference in sensitivity and specificity or dors <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> naïve (comparison of pooled estimates from separate meta-analyses) <UNK> <UNK> z-test <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> paired t-test <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> unpaired t-test <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> chi-squared test <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> comparison of q* statistic and their sesc <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> overlapping confidence intervals <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> narrative <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> none <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> unclear <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> relative measures used to summarize differences in test accuracy <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> multiple thresholds included <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> if multiple thresholds included, were they accounted for in the comparative meta-analysis (meta-analysis at each threshold or fitted appropriate model) yes <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> unclear <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> abbreviations: ancova, analysis of covariance; dor, diagnostic odds ratio; se, standard error; sroc, summary receiver operating characteristic. numbers in parentheses are column percentages unless otherwise stated. percentages may not add up to <UNK> because of rounding. a numbers in parentheses are row percentages. b these methods either involve a comparative meta-analysis or follow-on from a meta-analysis of each test individually. c moses et al. <UNK> proposed the q* statistic as an alternative to the area under the curve. q* is the point on the sroc curve where sensitivity is equal to specificity, that is, the intersection of the summary curve and the line of symmetry. <UNK> methods for comparative meta-analysis and informal comparisons we classified methods used in the <UNK> comparative reviews that statistically compared test accuracy into three main groups: <UNK> naïve comparison <UNK> <UNK> which refers to a comparison where a statistical test, for example, a z-test, was used to compare summary estimates from separate meta-analysis of one test with summary estimates from the meta-analysis of another test; <UNK> univariate pooling of differences in sensitivity and specificity, or pooling of differences in the diagnostic odds ratio <UNK> <UNK> and <UNK> meta-regression by adding test type as a covariate to a meta-analytic model <UNK> <UNK> for the remaining <UNK> <UNK> reviews, the method used was unclear. relative measures were used to summarize differences in accuracy in <UNK> of the <UNK> <UNK> reviews. for the remaining <UNK> comparative reviews that did not formally compare tests (i.e., through statistical quantification of the difference in accuracy, either via a p-value or estimate of the difference), three <UNK> determined the statistical significance of differences in test accuracy based on whether or not confidence intervals overlapped, nine <UNK> narratively compared tests, <UNK> <UNK> did not perform a comparison and three <UNK> were unclear. <UNK> investigations of heterogeneity investigations of heterogeneity were performed for individual tests in <UNK> <UNK> reviews, of which <UNK> <UNK> used meta-regression, <UNK> <UNK> used subgroup analyses, and <UNK> <UNK> used both methods (table <UNK> among the <UNK> comparative reviews with a statistical comparison, <UNK> <UNK> investigated heterogeneity. five <UNK> of the <UNK> reviews assessed the effect of potential confounders on relative accuracy using subgroup analyses (four reviews) or bayesian bivariate meta-regression (one review). table <UNK> investigations of heterogeneity in comparative and multiple test reviews characteristic comparative reviews multiple test reviews total statistical analyses to compare test accuracy yes no or unclear number of reviewsa <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> formal investigation performed yes—meta-regression and subgroup analyses <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> yes—meta-regression <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> yes—subgroup analyses <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no—limited data <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no—only tested for heterogeneity <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no—nothing reported <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> unclear <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> if yes above, was effect on relative accuracy also investigated? yes <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> planned but no data <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> unclear <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> numbers in parentheses are column percentages unless otherwise stated. percentages may not add up to <UNK> because of rounding. a numbers in parentheses are row percentages. <UNK> presentation and reporting thirteen reviews <UNK> used a reporting guideline (table <UNK> five reviews used prisma; four used quorum (quality of reporting of meta-analyses), the precursor to prisma; one used both quorum and prisma; one used both stard (standards for the reporting of diagnostic accuracy), and moose (meta-analysis of observational studies in epidemiology); and the remaining two stated they followed recommendations of the cochrane dta working group. table <UNK> reporting and presentation characteristics of the reviews characteristic comparative reviews multiple test reviews total statistical analyses to compare test accuracy yes no or unclear number of reviewsa <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> reporting guideline used <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> clear comparative objective stated <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> role of the tests add-on <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> replacement <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> triage <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> any two of the above <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> unclear <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> flow diagram presented yes—included number of studies per test <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> yes—excluded number of studies per test <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> comparative studies identified yes <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no comparative studies in review <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> study characteristics presented <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> test comparison strategy yesb <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> nob <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no—included only comparative studies <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> method used for test comparisonc yes <UNK> <UNK> na na <UNK> <UNK> unclear <UNK> <UNK> na na <UNK> <UNK> <UNK> × <UNK> data for each study <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> individual study estimates of test accuracy <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> forest plot(s) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> sroc plot sroc plot comparing summary points or curves for <UNK> or more tests <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> separate sroc plot per test <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no sroc plot <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> limitations of indirect comparison acknowledged yes <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> no but only comparative studies included <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> abbreviations: na, not applicable; sroc, summary receiver operating characteristic. numbers in parentheses are column percentages unless otherwise stated. percentages may not add up to <UNK> because of rounding. a numbers in parentheses are row percentages. b these reviews included both comparative and noncomparative studies. c these methods either involve a comparative meta-analysis or follow-on from a meta-analysis of each test individually. <UNK> summary of reporting quality and exemplars based on recommendations in the cochrane handbook <UNK> five comparative reviews <UNK> <UNK> <UNK> <UNK> <UNK> were judged exemplary in terms of clarity of objectives and reporting of test comparison methods. a brief summary of the reviews is given in appendix table <UNK> fig. <UNK> summarizes results for <UNK> reporting characteristics (derived from table <UNK> for each of the <UNK> reviews. the figure clearly shows that the reporting of several items—in particular the role of the index tests, test comparison strategy and limitations of indirect comparisons—was deficient in many reviews. further details are provided in sections <UNK> review objectives and clinical pathway, <UNK> study identification and characteristics, <UNK> strategy for comparing test accuracy, <UNK> graphical presentation of test comparisons, <UNK> limitations of indirect comparisons. <UNK> review objectives and clinical pathway a comparative objective was explicitly stated in <UNK> <UNK> reviews (table <UNK> it was possible to deduce the role of the tests in <UNK> <UNK> reviews as add on, triage, and/or replacement for an existing test. for <UNK> of the <UNK> <UNK> reviews, the role was explicitly stated while we used implicit information in the background and discussion sections to make judgments for the remaining <UNK> <UNK> reviews. <UNK> study identification and characteristics a flow diagram illustrating the selection of studies was not presented in <UNK> <UNK> reviews (table <UNK> in <UNK> <UNK> reviews, a flow diagram was presented without the number of studies per test, whereas <UNK> <UNK> reviews presented a comprehensive flow diagram with the number of studies per test. of these <UNK> reviews, the flow diagrams in five reviews <UNK> <UNK> <UNK> <UNK> were notable examples. these flow diagrams clearly showed the number of studies included in the analysis of each test, and also indicated the number of comparative studies available. of the <UNK> reviews that had at least one comparative study, <UNK> <UNK> reviews did not identify the comparative studies. most of the reviews <UNK> reported study characteristics; however, the detail reported varied. <UNK> strategy for comparing test accuracy seventy-three comparative reviews included both comparative and noncomparative studies and <UNK> <UNK> of these reviews stated their strategy for comparing tests, that is, direct and/or indirect comparisons (table <UNK> of the <UNK> reviews, <UNK> <UNK> formally compared test accuracy. <UNK> graphical presentation of test comparisons an sroc plot showing results for two or more tests was presented in <UNK> <UNK> reviews, <UNK> <UNK> reviews showed each test on a separate sroc plot, and the remaining <UNK> <UNK> reviews did not present an sroc plot (table <UNK> two multiple test reviews and seven comparative reviews without a formal test comparison presented an sroc plot showing a test comparison. <UNK> limitations of indirect comparisons twenty-one <UNK> reviews restricted inclusion to comparative studies (table <UNK> of the remaining <UNK> reviews that included any study type, <UNK> <UNK> acknowledged the limitations of indirect comparisons. furthermore, <UNK> of these <UNK> reviews recommended that future primary studies should directly compare the performance of tests within the same patient population. <UNK> discussion <UNK> principal findings the findings of our methodological survey showed considerable variation in methods and reporting. despite the importance of clear review objectives, they were often poorly reported and the role of the tests was ambiguous in many reviews. comparative studies ensure validity by comparing like with like, thus avoiding confounding but only <UNK> reviews <UNK> restricted study selection to comparative studies. this may be due to scarcity of comparative studies <UNK> it is worth noting that only two tests were evaluated in most <UNK> of the <UNK> reviews that restricted inclusion to comparative studies. the strategy adopted for test comparisons (direct comparisons and/or indirect comparisons) was not specified in many reviews. furthermore, the strategies that were specified varied considerably, reflecting a lack of understanding of the best methods for comparative accuracy meta-analysis. the validity of indirect comparisons largely depends on assumptions about study characteristics but reviews did not always report study characteristics. to pool data for a direct or indirect comparison, the hierarchical methods recommended for comparative meta-analysis were not often used, with many reviews using methods known to have methodological flaws that can lead to invalid statistical inference <UNK> <UNK> <UNK> there are several potential sources of bias and variation in test accuracy studies <UNK> <UNK> <UNK> and investigations of heterogeneity were commonly performed. however, the analyses were often performed separately for each test rather than examining the effect jointly on all tests in a comparison. understandably, the latter is rarely possible because of limited data. as empirical findings have shown that results of indirect comparisons are not always consistent with those or direct comparisons <UNK> and adjusting for potential confounders in an indirect comparison will be uncommon, review findings should be carefully interpreted in the context of the quality and the strength of the evidence. nevertheless, reviews seldom acknowledged the limitations of indirect comparisons. <UNK> strengths and limitations to our knowledge, a comprehensive overview of reviews of comparative accuracy across different target conditions and types of tests has not been undertaken. we thoroughly examined a large sample of reviews published in a wide range of journals. our classification of reviews was inclusive to enable a broad perspective of the literature and the generalizability of our findings. in addition to documenting review characteristics, we highlighted examples of good practice that review authors can use as exemplars. we also expanded relevant prisma-dta items for reporting test comparisons in a dta review. our study has limitations. first, the most recent review in our cohort of reviews was published in october <UNK> because the prisma-dta checklist was published in january <UNK> we did not update the collection as there had been no prior developments in reporting to suggest more recently published reviews would be better reported than older reviews. dare is based on extensive searches of a wide array of databases and also includes gray literature. given that for a review to be included in dare, it must meet certain quality criteria, the quality of the literature may be even poorer than we have shown. this view is supported by a study of <UNK> dta reviews published between october <UNK> and january <UNK> which found that the reviews were not fully informative when assessed against the prisma-dta and prisma-dta for abstracts reporting guidelines <UNK> furthermore, we examined the use of six comparative meta-analysis methods that have been published since <UNK> by checking their citations in scopus <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> only one of the methods <UNK> had been cited in a dta review published in <UNK> we also conducted a search of medline (ovid) on july <UNK> <UNK> to identify dta reviews published in <UNK> (appendix <UNK> of <UNK> records retrieved, <UNK> reviews met the inclusion criteria. the findings summarized in appendix <UNK> show that test comparison methods and reporting remain suboptimal. thus, our collection of reviews in this study reflects current practice. second, the assessment of the role of the tests was sometimes subjective and relied on the judgment of the assessor. therefore, we only considered whether the item was reported or not, without assessing the quality of the description provided. we also discussed any uncertainty in a judgment before making a final decision. <UNK> comparison with other studies previous research focused on systematic reviews of a single test or overview of any review type without detailed assessment of comparative reviews <UNK> specific clinical area <UNK> or specific methodological issue <UNK> <UNK> <UNK> mallett et al. <UNK> and cruciani et al. <UNK> concluded that conduct and reporting of dta reviews in cancer and infectious diseases was poor. in an overview of dta reviews published between <UNK> and <UNK> <UNK> of reviews that evaluated multiple tests reported statistical comparative analyses <UNK> similarly, <UNK> of our reviews reported such analyses. <UNK> guidance and implications for research and practice in box <UNK> we provide reporting guidance for test comparisons to augment the prisma-dta checklist and facilitate improvements in the reporting quality of comparative reviews. the guidance can also be used by peer reviewers and journal editors to appraise comparative dta reviews. the challenges of a dta review and the added complexity of test comparisons necessitate clear and complete reporting because of their increasing role in health technology assessment and clinical guideline development. space constraints in journals are not an excuse for poor reporting because many journals publish online supplementary files. we noted that <UNK> <UNK> reviews used supplementary files to provide additional data and information. tutorial guides should be developed to assist review authors in navigating and understanding the complexity of dta review methods. the cochrane screening and diagnostic tests methods group have already made contributions by providing freely available distance learning materials and tutorials on their website. box <UNK> guidance for reporting test comparisons in systematic reviews of diagnostic accuracy item description (prisma-dta items)a rationale and explanation <UNK> role of tests in diagnostic pathway <UNK> <UNK> test evaluation requires a clear objective and definition of the intended use and role of a test within the context of a clinical pathway for a specific population with the target condition. the intended role of a test guides formulation of the review question and provides a framework for assessing test accuracy, including the choice of a comparator(s) and selection of studies. the role of a test is therefore important for understanding the context in which the tests will be used and the interpretation of the meta-analytic findings. the existing diagnostic pathway and the current or proposed role of the index test(s) in the pathway should be described. a new test may replace an existing one (replacement), be used before the existing test (triage) or after the existing test (add-on) <UNK> <UNK> test comparison strategy <UNK> comparative studies are ideal but they are scarce <UNK> an indirect between-study (uncontrolled) test comparison uses a different set of studies for each test and so does not ensure like-with-like comparisons; the difference in accuracy is prone to confounding because of differences in patient groups and study methods. although direct comparisons based on only comparative studies are likely to ensure an unbiased comparison and enhance validity, such analyses may not always be feasible because of limited availability of comparative studies. conversely, an indirect comparison uses all eligible studies that have evaluated at least one of the tests of interest thus maximizing use of the available data (see appendix fig. <UNK> if study selection is not limited to comparative studies and comparative studies are available, a direct comparison should be considered in addition to an indirect comparison. the direct comparison may be narrative or quantitative depending on the availability of comparative studies. <UNK> meta-analytic methods <UNK> hierarchical models which account for between-study correlation in sensitivity and specificity while also allowing for variability within and between studies are recommended for meta-analysis of test accuracy studies <UNK> the two main hierarchical models are the bivariate and the hierarchical summary receiver operating characteristic (hsroc) models which focus on the estimation of summary points (summary sensitivities and specificities) and sroc curves, respectively (see appendix fig. <UNK> <UNK> for the summary point of a test to have a clinically meaningful interpretation, the analysis should be based on data at a given threshold. for the estimation of an sroc curve, data from all studies, regardless of threshold, can be included. as such, test comparisons may be based on a comparison of summary points and/or sroc curves. for the estimation of an sroc curve using the hsroc model, one threshold per study is selected for inclusion in the analysis. if multiple cutoffs were considered, the description of methods should include how the cutoffs were selected and handled in the analyses. methods have been proposed which allow inclusion of data from multiple thresholds for each study but the methods are yet to be applied to test comparisons. <UNK> identification of included studies for each test <UNK> review complexity increases with increasing number of tests, target conditions, uses and/or target populations within a single review. therefore, distinguishing between the different groups of studies that contribute to different analyses in the review enhances clarity. the prisma flow diagram can be extended to show the number of included studies for each test or group of tests if inclusion is not limited to comparative studies. the detail shown—individual tests or groups of tests, settings and populations—will depend on the volume of information and the ability of the review team to neatly summarize the information. if such a comprehensive flow diagram is not feasible, the studies contributing to the assessment of each test can be clearly identified in the manuscript in some other way. the source of the evidence should be declared by stating types of included studies. studies contributing direct evidence should also be clearly identified in the review. <UNK> study characteristics <UNK> relevant characteristics for each included study should be provided. this may be summarized in a table and should include elements of study design if eligibility was not restricted to specific design features. heterogeneity is often observed in test accuracy reviews and differences between tests may be confounded by differences in study characteristics. confounders can potentially be adjusted for in indirect test comparisons, though this is likely to be unachievable due to small number of studies and/or incomplete information on confounders. the effect of factors that may explain variation in test performance is typically assessed separately for each test. <UNK> study estimates of test performance and graphical summaries e.g., forest plot and/or sroc plot <UNK> it is desirable to report <UNK> × <UNK> data (number of true positives, false positives, false negatives, and true negatives) and summary statistics of test performance from each included study. this may be done graphically (e.g., forest plots) or in tables. such summaries of the data will inform the reader about the degree to which study-specific estimates deviate from the overall summaries, as well as the size and precision of each study. it is plausible that study results for one test may be more consistent or precise than those of another test in an indirect comparison. in addition to forest plots, reviews may include sroc plots such as those shown in appendix figures <UNK> and <UNK> an sroc plot of sensitivity against specificity displays the results of the included studies as points in roc space. the plot can also show meta-analytic summaries such as sroc curves (panel b in appendix fig. <UNK> or summary points (summary sensitivities and specificities) with corresponding confidence and/or prediction regions to illustrate uncertainty and heterogeneity, respectively (panel a in appendix fig. <UNK> ideally, results from a test comparison should be shown on a single sroc plot instead of showing the results for each test on a separate sroc plot. furthermore, for pairwise direct comparisons, the pair of points representing the results of the two tests from each study can be identified on the plot by adding a connecting line between the points such as in the plot shown in panel b of appendix fig. <UNK> <UNK> limitations of the evidence from indirect comparisons <UNK> this is only applicable for reviews that include indirect comparisons. be clear about the quality and strength of the evidence when interpreting the results, including limitations of including noncomparative studies in a test comparison. the results of indirect comparisons should be carefully interpreted taking into account the possibility that differences in test performance may be confounded by clinical and/or methodological factors. this is essential because it is seldom feasible to assess the effect of potential confounders on relative accuracy. a related to the prisma-dta item(s) indicated in parentheses. because long-term rcts of test-plus-treatment strategies which evaluate the benefits of a new test relative to current best practice are not always feasible <UNK> and are rare <UNK> comparative accuracy reviews are an important surrogate for guiding test selection and decision-making. however, given the preponderance of indirect comparisons and paucity of comparative studies, there is a need to educate trialists, clinical investigators, funders, and ethics committees about the merit of comparative studies for obtaining reliable evidence about the relative performance of competing diagnostic tests. <UNK> conclusions comparative accuracy reviews can inform decisions about test selection but suboptimal conduct and reporting will compromise their validity and relevance. complete and unambiguous reporting is therefore needed to enhance their use and minimize research waste. we advocate using the guidance we have provided as an adjunct to the prisma-dta checklist to promote better conduct and reporting of test comparisons in dta reviews.

<|EndOfText|>

a study protocol for the development and internal validation of a multivariable prognostic model to determine lower extremity muscle injury risk in elite football (soccer) players, with further exploration of prognostic factors abstract background: indirect muscle injuries (imis) are a considerable burden to elite football (soccer) teams, and prevention of these injuries offers many benefits. preseason medical, musculoskeletal and performance screening (termed periodic health examination (phe)) can be used to help determine players at risk of injuries such as imis, where identification of phe-derived prognostic factors (pf) may inform imi prevention strategies. furthermore, using several pfs in combination within a multivariable prognostic model may allow individualised imi risk estimation and specific targeting of prevention strategies, based upon an individual’s pf profile. no such models have been developed in elite football and the current imi prognostic factor evidence is limited. this study aims to <UNK> develop and internally validate a prognostic model for individualised imi risk prediction within a season in elite footballers, using the extent of the prognostic evidence and clinical reasoning; and <UNK> explore potential phe-derived pfs associated with imi outcomes in elite footballers, using available phe data from a professional team. methods: this is a protocol for a retrospective cohort study. phe and injury data were routinely collected over <UNK> seasons <UNK> july <UNK> to <UNK> may <UNK> from a population of elite male players aged <UNK> years old. of <UNK> candidate pfs, <UNK> were excluded. twelve variables (derived from <UNK> pfs) will be included in model development that were identified from a systematic review, missing data assessment, measurement reliability evaluation and clinical reasoning. a full multivariable logistic regression model will be fitted, to ensure adjustment before backward elimination. the performance and internal validation of the model will be assessed. the remaining <UNK> candidate pfs are eligible for further exploration, using univariable logistic regression to obtain unadjusted risk estimates. exploratory pfs will also be incorporated into multivariable logistic regression models to determine risk estimates whilst adjusting for age, height and body weight. background indirect muscle injuries (imis) are the most common injury type in elite football (soccer), predominantly affecting lower extremity muscle groups <UNK> <UNK> such injuries occur in the absence of direct impact-related trauma (during sprinting for example) <UNK> <UNK> and are subclassified into functional disorders without macroscopic structural tissue muscle damage, or structural injuries with clear evidence of muscle disruption <UNK> <UNK> imis are problematic for elite teams in terms of both incidence and severity <UNK> accounting for <UNK> to <UNK> of all injuries that result in time lost to both training and competition <UNK> <UNK> with the mean and median absence duration reported as <UNK> <UNK> and <UNK> days respectively <UNK> player availability is crucial to team prosperity, with vast commercial and financial rewards on offer to successful teams and players <UNK> <UNK> conversely, player absences through injury negatively affect team performance <UNK> <UNK> increase demand on medical services and carry a significant financial burden. as an illustration, for each first team player missing through injury, the daily cost to a participating team in the uefa champions league is approximately <UNK> to <UNK> <UNK> <UNK> periodic health examination (phe) is used by <UNK> of elite teams and typically consists of medical examination, musculoskeletal assessment, functional movement evaluation and performance tests, conducted during preseason and in-season periods <UNK> phe is considered important because its intended purposes are to: <UNK> allow regular health monitoring for underlying but asymptomatic pathology <UNK> <UNK> establish baseline measures for setting rehabilitation or training targets <UNK> and <UNK> identify individuals who are susceptible to common or severe injury types (such as imis) <UNK> for the latter function, phe cannot detect causes of injury, but can highlight factors that may be associated with an injury outcome (prognostic factors) and therefore help explain differences in injury risk across individuals within the team <UNK> several prognostic factors could also be used in combination within a multivariable prognostic model to predict an individual’s absolute injury risk <UNK> <UNK> importantly, both prognostic models and prognostic factors (pfs) can be used to inform management approaches designed to modify an individual’s absolute risk <UNK> despite the potential benefits of prognostic models for shaping injury prevention strategies aimed at clinically important injuries such as imis, none have been developed in elite football <UNK> in addition, there are significant methodological limitations in the evidence base relating to phe-derived pfs <UNK> therefore, this study will consist of two primary objectives: <UNK> to develop and internally validate a prognostic model for individualised imi risk prediction during a season in elite footballers, using a small number of phederived candidate pfs selected from a previous systematic review <UNK> and clinical reasoning; and <UNK> to explore potential pfs associated with imi outcomes during a season in this elite cohort, using available phe data from a professional team. methods study design this study will be of retrospective cohort design, using a population of elite male football players aged <UNK> years old who were employed on a full-time basis at an english premier league club. the first objective will be conducted in accordance with existing guidelines for model development and internal validation <UNK> <UNK> and reported in accordance with the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) statement <UNK> <UNK> the second objective will be conducted in accordance with existing guidelines <UNK> and reported in accordance with the reporting recommendations for marker prognostic studies <UNK> <UNK> data sources this study will use routinely collected data that was obtained over five seasons (from <UNK> july <UNK> to <UNK> may <UNK> data collected from the musculoskeletal and performance test components of the club’s phe will be used to identify candidate pfs. injury outcome data will also be used to establish the available number of imi outcomes. preseason phe data collection each new season commenced from july <UNK> available players completed a mandatory phe on one of <UNK> days during the first week of the season. typically, the musculoskeletal and performance components of the phe included the following: <UNK> anthropometric measurements; <UNK> medical history review (i.e. previous injury history); <UNK> musculoskeletal examination tests; <UNK> functional hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> movement and balance tests; and <UNK> strength and power tests. detailed descriptions of all tests are provided in additional file <UNK> the phe test order was self-selected by each player. a standardised warm up was not implemented, although players could undertake their own warm up procedures if they wished. each component of the phe test battery was standardised according to a written protocol and conducted by physiotherapists, sports scientists or club medical doctors. to avoid inter-tester variability, the same examiners performed the same test every season and throughout the <UNK> data collection period, no examiner attrition occurred. if a participant was injured at the time of phe, a risk assessment was completed by medical staff. in such instances, participants only completed tests that were deemed appropriate and safe for the participant’s condition; examiners were therefore not blinded to the injury status of participants. participant follow-up and injury data collection participants were followed up to the last day of each competitive domestic season (defined as the date of the last first team game of the season) irrespective of whether they had completed the phe procedure or not. participants completed their routine training and match programmes throughout. for every player in the squad, any injuries that occurred during the season were assessed and electronically documented within <UNK> h by a club medical doctor or physiotherapist in accordance with the consensus statement on injury definitions and data collection procedures in studies of football injuries <UNK> musculoskeletal assessments were dependent on the clinical presentation, although typically consisted of observation, effusion, range of movement, muscle length and resisted muscle tests, palpation and special diagnostic manual tests. radiological imaging was used to assist diagnosis as required. ultrasound scans were performed by the club medical doctor using a toshiba aplio <UNK> or <UNK> machine (toshiba corporation, tokyo, japan). magnetic resonance imaging (mri) was performed as appropriate, using a canon vantage titan <UNK> t scanner (canon medical systems, otowara, japan) according to sequences determined by the club medical doctor. images were evaluated by a club medical doctor and an independent musculoskeletal radiologist. the medical professionals were not blinded to phe data at the time of diagnosis. these data were not routinely used to inform diagnoses, but instead used to identify functional rehabilitation targets and for benchmarking purposes. following injury, players completed a rehabilitation programme as directed by club medical staff to enable them to return to training and match participation. participants and eligibility criteria eligible participants were identified from a review of the phe database entries during the dates stated above. during any season, participants were eligible for inclusion into the analysis if they: <UNK> had an outfield position (i.e. not a goalkeeper); and <UNK> participated in phe testing for the relevant season. participants were excluded from the analysis for any season if they were a triallist player or not contracted to the club at the time of phe. ethics and data use because all data were captured from the mandatory phe procedure completed through the participants’ employment, informed consent was not required. the anonymity and rights of all participants were protected. the football club granted permission to use these data, and the use of the data for this study was approved by the research ethics service at the university of manchester. this study has been registered on clinicaltrials.gov, with registered number as <UNK> data extraction all phe records from eligible participants were extracted and placed into a separate database. using the club’s electronic medical records system, a further database was generated of all recorded injuries for each season and a manual review of each eligible participant’s medical record was undertaken to ensure accuracy. each injury was categorised according to the following: <UNK> contact or non-contact mechanism of injury; <UNK> injured side; <UNK> affected body area; <UNK> injury type, i.e. imi/ligament/tendon/cartilage/contusion or laceration/bone/ concussion/other musculoskeletal injury; and <UNK> muscle group and diagnostic classification if recorded as an imi. this process allowed an in-house audit of injury incidence and absolute risk evaluation for each injury type for the squad overall and for those who underwent phe. all imis were then extracted and merged with the phe database of included participants, for each season in which they remained eligible. outcome measures for this study, the primary outcome measure will be the occurrence of an initial (index) lower extremity imi sustained by a participant during a season. only time-loss injuries will be included; that is, any index lower extremity imi that occurred during match play or training that resulted in the player being unable to take full part in future match play or training <UNK> an imi was confirmed during the injury assessment procedure outlined above and graded by the club medical doctor or physiotherapist according to the munich consensus statement for the classification of muscle injuries in sport <UNK> this diagnostic classification system was the primary method hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> of muscle injury classification used by the club and has been validated previously <UNK> each participant-season will be treated as independent. if an index lower extremity imi occurred, the participant’s outcome for the season will be determined and that participant will no longer be considered at risk beyond the time of imi occurrence. in these circumstances, participants will be included for further analysis at the start of the consecutive season, providing they remain eligible. if participants sustained any upper limb imi, trunk imi or non-imi injury type, these will be ignored and the participant will still be considered at risk of a lower extremity index imi. eligible participants who were loaned out or transferred to another club throughout that season, but had not sustained an index imi prior to the loan or transfer, will still be considered in the risk set. participants who sustained an index imi whilst on loan will be included for analysis, as outlined above. any participants who were permanently transferred during a season (but had not sustained an index imi prior to the transfer) will be recorded as not having an imi event during the relevant season, and they will exit the cohort at this point. a sensitivity analysis may be conducted to evaluate the effect of player loans or transfers on the results. sample size to maximise statistical power, we have elected to use all data from the <UNK> period. this approach agrees with methodological recommendations that data splitting should be avoided, and all available data should be used for model validation <UNK> the extracted injury data were audited in parallel with the development of this protocol to determine the number of available index imi events in the dataset. this was essential to allow calculation of the maximum number of candidate pfs that could be included in model development in order to limit the effects of statistical overfitting <UNK> the number of candidate pfs for inclusion in model development will be restricted to a minimum of <UNK> events per variable (epv), which is recommended to reduce overfitting and optimism during the development of a logistic regression model <UNK> note that ‘variable’ here means a parameter included (or considered for inclusion) in the model that corresponds to one of the pfs. following the audit, the number of independent participant-seasons that will be included for analysis is <UNK> with <UNK> index imi events recorded during the <UNK> season period. therefore, we have chosen to restrict the number of parameters (variables) for inclusion in model development to <UNK> which corresponds to having <UNK> epv and thus above the minimum recommendation of <UNK> we also checked if this met the criteria to minimise overfitting recently proposed by riley et al. <UNK> assuming the model will have a modest nagelkerke r-squared of <UNK> then with an outcome proportion of <UNK> our <UNK> candidate pf variables correspond to targeting an approximate shrinkage factor of <UNK> and thus a relatively small amount of overfitting <UNK> <UNK> we deemed this a suitable compromise between increasing the number of pf parameters and minimising the overfitting. candidate prognostic factors the extracted phe data were audited as per current methodological recommendations <UNK> to establish data quality and quantify missing values. this process was also conducted in parallel with the development of this protocol, to assist selection of candidate pfs to be included in either model development or exploration a priori and to inform strategies for handling missing data in the final analysis. a complete list of all <UNK> candidate pfs extracted from the phe dataset is presented in table <UNK> with quantitative analysis of missing values for each pf. missing data as presented in table <UNK> all medical history and age factors were complete <UNK> factors). of the <UNK> remaining candidate pfs, the proportion of missingness ranged from <UNK> (for height and weight) to <UNK> (for body fat). eleven of these had > <UNK> missing observations (which included body fat, toe touch in standing, sacroiliac kinematic function, all y balance test and upper body peak power variables). for these factors, the large degree of missingness was because of procedural changes in the phe process, which meant that these tests were not conducted across all seasons. for candidate pfs with < <UNK> missing observations, all tests were conducted consistently across all <UNK> seasons. for these factors, the sample characteristics of cases with complete pf data were compared to incomplete cases which had at least one missing observation (table <UNK> for complete cases, the mean values of all characteristics were less than incomplete cases, with the largest differences observed in age <UNK> and <UNK> years, respectively) and weight <UNK> and <UNK> kg, respectively). therefore, a complete case only analysis was not appropriate and we will rather assume that the mechanism of missingness can be considered as missing at random (mar), where the distribution of missing values is related to values of observed variables <UNK> to allow imputation and so inclusion of individuals with missing data. hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> table <UNK> list of candidate prognostic factors, methods and units of measurement, frequency of complete and incomplete observations and proportion of missing observations type of prognostic factor candidate prognostic factor measurement method measurement unit data type complete obs missing obs percent missing (%) anthropometric age birthdate years cont. <UNK> <UNK> <UNK> height standing height cm cont. <UNK> <UNK> <UNK> weight digital scales kg cont. <UNK> <UNK> <UNK> bmi height/weight <UNK> cont. <UNK> <UNK> <UNK> body fat skin callipers % cont. <UNK> <UNK> <UNK> medical history frequency of previous imis within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous imi within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous foot or ankle injuries within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous foot or ankle injury within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous hip or groin injuries within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous hip or groin injury within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous knee injuries within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous knee injury within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous shoulder injuries within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous shoulder injury within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous lumbar spine injuries within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous lumbar spine injury within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous iliopsoas imis within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous iliopsoas imi within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous adductor imis within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous adductor imi within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous hamstring imis within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous hamstring imi within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous quadriceps imis within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous quadriceps imi within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> frequency of previous calf imis within <UNK> years prior to phe medical records freq. dis./ cont. <UNK> <UNK> <UNK> most recent previous calf imi within <UNK> years prior to phe medical records never, < <UNK> months, <UNK> months, > <UNK> months cat. <UNK> <UNK> <UNK> musculoskeletal prom r hip joint internal rotation digital inclinometer degrees cont. <UNK> <UNK> <UNK> prom l hip joint internal rotation digital inclinometer degrees cont. <UNK> <UNK> <UNK> prom r hip joint external rotation digital inclinometer degrees cont. <UNK> <UNK> <UNK> prom l hip joint external rotation digital inclinometer degrees cont. <UNK> <UNK> <UNK> r hip flexor muscle length digital inclinometer—thomas test degrees cont. <UNK> <UNK> <UNK> hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> model development and internal validation we have chosen to conduct the model development before the pf exploration because of the restrictions on the number of pfs permitted to limit potential overfitting of the model. because only <UNK> pf variables will be used in model building, we have defined these candidate pfs a priori (table <UNK> three candidate pfs have known importance based on the results of our previous systematic review so were selected for inclusion <UNK> all other pfs listed in table <UNK> were eligible unless there were > <UNK> missing observations or if reliability (where applicable) was classed as fair to poor (icc < <UNK> <UNK> in these cases, the relevant candidate pfs were excluded (table <UNK> this was to ensure that only the highest quality data will be used in the analysis, with pfs that would generally be available and routinely measured. co-linearity amongst factors within a logistic regression model can cause inaccuracies in standard error and table <UNK> list of candidate prognostic factors, methods and units of measurement, frequency of complete and incomplete observations and proportion of missing observations (continued) type of prognostic factor candidate prognostic factor measurement method measurement unit data type complete obs missing obs percent missing (%) l hip flexor muscle length digital inclinometer—thomas test degrees cont. <UNK> <UNK> <UNK> r hamstring muscle length /neural mobility digital inclinometer—slr degrees cont. <UNK> <UNK> <UNK> l hamstring muscle length /neural mobility digital inclinometer—slr degrees cont. <UNK> <UNK> <UNK> r quadriceps muscle length goniometer—ely’s test degrees cont. <UNK> <UNK> <UNK> l quadriceps muscle length goniometer—ely’s test degrees cont. <UNK> <UNK> <UNK> r calf muscle length digital inclinometer—wbl degrees cont. <UNK> <UNK> <UNK> l calf muscle length digital inclinometer—wbl degrees cont. <UNK> <UNK> <UNK> toe touch in standing fingertip-floor distance cm cont. <UNK> <UNK> <UNK> sacroiliac joint kinematic function gillet’s test subjective kinematic function cat. <UNK> <UNK> <UNK> functional movement/ balance y balance test—r anterior translation y balance test cm cont. <UNK> <UNK> <UNK> y balance test—l anterior translation y balance test cm cont. <UNK> <UNK> <UNK> y balance test—r posteromedial translation y balance test cm cont. <UNK> <UNK> <UNK> y balance test—l posteromedial translation y balance test cm cont. <UNK> <UNK> <UNK> y balance test—r posterolateral translation y balance test cm cont. <UNK> <UNK> <UNK> y balance test—l posterolateral translation y balance test cm cont. <UNK> <UNK> <UNK> r relative tibial angles sls measured with imu degrees cont. * * * l relative tibial angles sls measured with imu degrees cont. * * * strength/power r upper body peak power horizontal press <UNK> cont. <UNK> <UNK> <UNK> l upper body peak power horizontal press <UNK> cont. <UNK> <UNK> <UNK> r maximal loaded leg extension power double leg press <UNK> cont. <UNK> <UNK> <UNK> l maximal loaded leg extension power double leg press <UNK> cont. <UNK> <UNK> <UNK> r maximal loaded leg extension velocity double leg press m <UNK> cont. <UNK> <UNK> <UNK> l maximal loaded leg extension velocity double leg press m <UNK> cont. <UNK> <UNK> <UNK> r maximal loaded leg extension force double leg press <UNK> cont. <UNK> <UNK> <UNK> l maximal loaded leg extension force double leg press <UNK> cont. <UNK> <UNK> <UNK> cmj height cmj cm cont. <UNK> <UNK> <UNK> cmj force per kilogram of body mass cmj n/kg cont. <UNK> <UNK> <UNK> cmj peak power cmj w cont. <UNK> <UNK> <UNK> freq. frequency, obs observations, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom passive range of movement, slr straight leg raise, sls single leg squat, imu inertial measurement units, bmi body mass index, <UNK> kilograms/body height (metres) squared, w watts (note: <UNK> has a scaling factor to normalise power to body mass), n newtons (note: <UNK> has a scaling factor to normalise force to body mass), cm centimetres, kg kilograms, cont. continuous, dis./cont. discrete treated as continuous, cat categorical, r right, l left, m <UNK> metres/second, “–” not applicable/not available, “*” missing data analysis not completed—test evaluated through a reliability/agreement study published as a related part of this project and excluded from further analysis based on the results hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> confidence interval estimates <UNK> so a scatterplot matrix was used to informally assess between-factor correlations for eligible pfs. if pfs were highly correlated, one of the pfs was dropped or new composite pfs were generated and replaced the original factors (highlighted in tables <UNK> <UNK> and <UNK> typically, this occurred where measurements examined both right and left limbs separately; composite factor variables were therefore created for both between-limb measurement differences and the mean of the measurements for both limbs. of the remaining eligible pfs, <UNK> further candidate factor variables were selected for inclusion, through use of clinical reasoning to identify those with a biologically plausible association with imi development. the final set of <UNK> pf variables is shown in table <UNK> prognostic factor exploration candidate pfs that were that were not selected for use in model development (but not excluded) will be eligible for further exploratory analysis (table <UNK> this will allow identification of other potentially useful associations which may assist future analyses or updating of the model created under the first objective of this investigation. statistical analysis model development and internal validation multivariable logistic regression will be used for the analysis as this is an appropriate method where outcomes are binary <UNK> and independent variables (pfs) are continuous, categorical or a combination <UNK> initially, we will fit a full multivariable model containing all <UNK> candidate pf variables to ensure a fully adjusted model prior to the potential elimination of unimportant candidate factors <UNK> backward elimination will then be used to successively remove non-significant factors with p values of greater than <UNK> this threshold was set to approximate equivalence with akaike’s information criterion <UNK> using backward elimination in this way may deliver a more parsimonious model which is therefore easier to implement in clinical practice than a full model. where possible, we will retain continuous candidate pfs in their continuous form to avoid statistical power loss <UNK> because the missing data mechanism is considered as missing at random (mar), multiple imputation (mi) will be implemented, using <UNK> imputations. we have chosen to utilise mi because it avoids excluding participants from the analysis, is an effective method of handling missing prognostic factor information and can be used to account for uncertainty in missing data <UNK> the apparent performance of the developed model will be summarised in the development datasets (averaged over imputation datasets), via calibration and discrimination. model calibration determines performance in terms of the agreement between predicted outcome risks and those actually observed <UNK> graphical plots are useful to assess calibration <UNK> so will be produced and utilised in the analysis. we will calculate calibration-in-the-large (citl, ideal value of <UNK> which quantifies the systematic error in model predictions (overall agreement). a related measure is e/o (ideal value of <UNK> which gives the ratio of the mean of the predicted (expected (e)) risks against the mean of the observed risks (o) <UNK> <UNK> a calibration slope will also be calculated, where a value of <UNK> equals perfect calibration <UNK> models demonstrate perfect calibration within development data, but in new data, the slope may be < <UNK> due to overfitting in the model development dataset (see below for how this will be handled) <UNK> discrimination performance is a measure of a model’s ability to separate participants who have experienced an outcome compared to those who have not, quantified using the c (concordance) statistic (equivalent to the area under the roc curve) <UNK> this index measure will be calculated for the development model, where <UNK> demonstrates perfect discrimination, whilst <UNK> indicates that discrimination is no better than by chance alone. to quantify the degree of optimism due to overfitting, our model will be internally validated using bootstrap re-sampling. this will be conducted as previously outlined <UNK> <UNK> the prognostic factor variable selection procedure and model construction will be repeated for <UNK> bootstrap samples. for each sample, the difference in bootstrap apparent performance (of the bootstrap model in the bootstrap data) and test performance (of the bootstrap model in the original dataset) will be averaged across the <UNK> samples, to obtain a single estimate table <UNK> characteristics of cases with complete candidate prognostic factor data, and cases with at least one missing observation for any candidate prognostic factor in the phe dataset with < <UNK> missing values complete cases <UNK> person-seasons) incomplete cases <UNK> person-seasons) sample characteristic number of obs mean (sd) number of obs mean (sd) age (years) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> height (cm) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> weight (kg) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> bmi <UNK> ) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> pf prognostic factor, obs observations, sd standard deviation, cm centimetres, kg kilograms, <UNK> kilograms divided by body height (metres) squared hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> of optimism for each performance statistic. then, to calculate optimism-adjusted estimates of performance for our new model, the estimates of optimism will be subtracted from the original apparent estimates of performance. the optimism-adjusted calibration slope will provide a uniform shrinkage factor, which will be applied to all prognostic factor effects in the developed model to adjust (shrink) for overfitting. the intercept of the model will then be re-estimated accordingly. this will then form our final model. prognostic factor exploration all remaining candidate factors that are eligible for exploration (table <UNK> will undergo univariable logistic regression analyses to determine unadjusted associations with imis. candidate pfs will also be incorporated into multivariable logistic regression models to determine odds ratios after adjustment for age, height and body weight. note that because age was included as a candidate in the original model and will also be used for adjustment purposes in the exploratory multivariable models, the total number of candidate pfs eligible for exploration is <UNK> exploration of non-linear associations between candidate factors and index imi outcomes will also be evaluated using a fractional polynomial approach <UNK> discussion although previous studies in elite football have investigated the association between factors obtained during phe and imis using multivariable models, none have developed, validated or evaluated the performance a prognostic model for injury prediction purposes <UNK> whilst it is possible to develop a prognostic model from phe data <UNK> our investigation will offer valuable insights into the practical aspects of this process and the clinical usefulness of a model when applied to an individual football club. our findings may also outline how these principles may be used in future at other clubs or sports, or on larger datasets which could be derived from several collaborating clubs. table <UNK> restricted set of candidate prognostic factors for model development and validation selection method candidate prognostic factor composite pf measurement unit number of parameters in model measurement method data type reliability (if applicable) systematic review age no years <UNK> date of birth continuous - frequency of previous imis within <UNK> years prior to phe no freq. <UNK> medical records discrete (treated as continuous) - most recent previous imi within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months <UNK> medical records categorical - clinical reasoning/ data quality cmj peak power no watts <UNK> cmj using force plates continuous test-retest icc = <UNK> <UNK> prom hip joint internal rotation difference* yes degrees <UNK> digital inclinometer continuous intra-rater icc = <UNK> <UNK> prom hip joint external rotation difference* yes degrees <UNK> digital inclinometer continuous intra-rater icc = <UNK> <UNK> hip flexor muscle length difference* yes degrees <UNK> digital inclinometer - thomas test continuous inter-rater icc = <UNK> <UNK> hamstring muscle length /neural mobility difference* yes degrees <UNK> digital inclinometer - slr continuous intra-rater icc = <UNK> <UNK> interrater icc = <UNK> <UNK> calf muscle length difference* yes degrees <UNK> digital inclinometer - wbl continuous inter-rater icc = <UNK> <UNK> <UNK> intra-rater = icc <UNK> <UNK> bmi yes <UNK> <UNK> composite height (cm) and weight (kg) continuous – pf prognostic factor, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom passive range of movement, icc intraclass correlation coefficient, slr straight leg raise, bmi body mass index, kg kilos, freq. frequency, <UNK> kilograms/body height (metres) squared. "*" denotes between limb differences hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> despite the availability of high-quality phe and injury data, the relatively small number of outcomes in this dataset is problematic and will permit only a limited selection of candidate prognostic factors for use in model development. utilising more than one prognostic factor variable for every <UNK> injury outcomes may cause significant issues with model overfitting, where spurious observed relationships occur because of regression value distortion <UNK> this leads to an overestimation of predictive performance (optimism) which is especially evident in small datasets <UNK> to limit the effects of overfitting, only <UNK> pfs (resulting in <UNK> variables) will be permitted and use of data reduction methods have been required to select appropriate candidate factors for inclusion. pfs for clinical injury outcomes are either intrinsic (person specific) or extrinsic (environment specific) <UNK> and can be modifiable or non-modifiable <UNK> only the non-modifiable factors of increasing age and history of previous muscle injury have been shown to have modest table <UNK> candidate prognostic factors excluded from both model development and prognostic factor exploration type of prognostic factor candidate prognostic factor composite pf measurement unit measurement method data type reason for elimination anthropometric body fat no percentage skin callipers continuous missing data > <UNK> musculoskeletal quadriceps muscle length difference* yes degrees goniometer - ely's test continuous intra-rater icc = <UNK> <UNK> inter-rater icc = <UNK> <UNK> mean quadriceps muscle length** yes degrees goniometer - ely's test continuous intra-rater icc = <UNK> <UNK> inter-rater icc = <UNK> <UNK> toe touch in standing no centimetres fingertip to floor distance continuous missing data > <UNK> sacroiliac joint kinematic function no subjective score gillet’s test categorical missing data > <UNK> functional movement/ balance y balance test—anterior translation difference* yes centimetres y balance test continuous missing data > <UNK> y balance test—mean anterior translation** yes centimetres y balance test continuous missing data > <UNK> y balance test—posteromedial translation difference* yes centimetres y balance test continuous missing data > <UNK> y balance test—mean posteromedial translation** yes centimetres y balance test continuous missing data > <UNK> y balance test—posterolateral translation difference* yes centimetres y balance test continuous missing data > <UNK> y balance test—mean posterolateral translation** yes centimetres y balance test continuous missing data > <UNK> r relative tibial angles no degrees sls measured with dorsavi viperform imu continuous within-session iccs = <UNK> between-session iccs = <UNK> <UNK> l relative tibial angles no degrees sls measured with dorsavi viperform imu continuous within-session iccs = <UNK> between-session iccs = <UNK> <UNK> strength/power upper body peak power difference* yes normalised watts per kilo <UNK> double horizontal press using a keiser chest press air <UNK> machine continuous missing data > <UNK> mean upper body peak power** yes normalised watts per kilo <UNK> double horizontal press using a keiser chest press air <UNK> machine continuous missing data > <UNK> icc intraclass correlation coefficient, sls single leg squat, w watts, (note that <UNK> has a scaling factor to normalise power to body mass), kg kilos, imu inertial measurement units, r right, l left. “*” denotes between limb differences and “**” denotes the mean of the measurements for both limbs hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> table <UNK> candidate prognostic factors—exploration type of prognostic factor candidate prognostic factor composite pf measurement unit measurement method data type reliability (if applicable/available) anthropometric height no centimetres standing height measure continuous – weight no kilograms digital scales continuous – medical history frequency of previous foot or ankle injuries within <UNK> years prior to phe no freq. medical records continuous – most recent previous foot or ankle injury within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous hip or groin injuries within <UNK> years prior to phe no freq. medical records continuous – most recent previous hip or groin injury within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous knee injuries within <UNK> years prior to phe no freq. medical records continuous – most recent previous knee injury within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous shoulder injuries within <UNK> years prior to phe no freq. medical records continuous – most recent previous shoulder injury within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous lumbar spine injuries within <UNK> years prior to phe no freq. medical records continuous – most recent previous lumbar spine injury within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous iliopsoas imis within <UNK> years prior to phe no freq. medical records continuous – most recent previous iliopsoas imi within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous adductor imis within <UNK> years prior to phe no freq. medical records continuous – most recent previous adductor imi within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous hamstring imis within <UNK> years prior to phe no freq. medical records continuous – most recent previous hamstring imi within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous quadriceps imis within <UNK> years prior to phe no freq. medical records continuous – most recent previous quadriceps imi within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – frequency of previous calf imis within <UNK> years prior to phe no freq. medical records continuous – most recent previous calf imi within <UNK> years prior to phe no < <UNK> months, <UNK> months, > <UNK> months medical records categorical – musculoskeletal mean prom hip joint internal rotation** yes degrees digital inclinometer continuous intra-rater icc = <UNK> <UNK> mean prom hip joint external rotation** yes degrees digital inclinometer continuous intra-rater icc = <UNK> <UNK> mean hip flexor muscle length** yes degrees digital inclinometer - thomas test continuous inter-rater icc = <UNK> <UNK> mean hamstring muscle length/neural mobility** yes degrees digital inclinometer - slr continuous intra-rater icc = <UNK> <UNK> inter-rater icc = <UNK> <UNK> mean calf muscle length** yes degrees digital inclinometer - wbl continuous inter-rater icc = <UNK> <UNK> <UNK> intra-rater icc = <UNK> <UNK> strength/power maximal loaded leg extension power difference* yes normalised watts per kilo <UNK> double leg press test using a keiser air <UNK> machine continuous test-retest icc = <UNK> <UNK> hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> prognostic value for hamstring muscle injuries in elite footballers <UNK> so will be included in model development. however, their non-modifiable nature means that they have limited use in terms of informing injury prevention strategies. to enhance the clinical applicability of the model, other potentially relevant and modifiable factors have been selected for inclusion. the methodological shortcomings in the literature mean that only three candidate prognostic factors could be selected for model development from our previous systematic review <UNK> subsequently, candidate pf selection for our model has been largely based upon the evaluation of collinearity, measurement reliability and clinical reasoning, which means that it is possible that some important factors have not been considered. it is also possible that some potentially useful factors have been excluded on the basis of having <UNK> of missing values. as such, only modest performance of this initial model is expected. it is acknowledged that the proposed prognostic model will assume that participants are independent for each season and utilise the binary outcome of at least one imi in a season, rather than evaluating time to individual imi events. this means that we will not account for within-person correlations from season to season. although this is not fully representative of the real world, because this is a novel area and we are restricted to a relatively small dataset, we have elected to perform the analyses in a more simplistic manner in the first instance. further, more complex analyses may be conducted in the future. to assess the generalisability of a prognostic model, it should be externally validated using data from another location <UNK> <UNK> such as a dataset from another comparable elite level football team. because there is likely to be considerable between-team heterogeneity in phe processes <UNK> candidate prognostic factors within our model may not translate externally at this time. there are no immediate plans to externally validate this model. however, depending on the outcome of the model development and exploratory objectives, it may be possible to conduct a future prospective temporal validation study within the same football club, or external validation study in different population. if feasible, such investigations will require a separate associated protocol. the current evidence relating to pfs for injury in football is frequently flawed due to issues with the reliability of data measurement, adjustment, dichotomisation and potential diagnostic misclassification, so there is a need for further studies that address these issues <UNK> further hypothesis-free exploratory studies that investigate many factors (including those that are not necessarily biologically plausible) may assist with identification of new factors that may help inform management decisions and monitoring purposes <UNK> furthermore, these types of studies are helpful because new pfs may be used to update a developed model to improve performance <UNK> we have therefore outlined an exploratory objective to investigate the association between imis and other factors from the current dataset, using a validated diagnostic outcome classification system and recommended statistical approaches, ensuring that where possible, analysis of continuous data remains on the continuous scale to explore linear and non-linear associations. we anticipate that this investigation will provide a comprehensive evaluation of what is currently possible in terms of using phe data to predict imis at an elite football club, by adhering to transparent reporting procedures and current best practice for model table <UNK> candidate prognostic factors—exploration (continued) type of prognostic factor candidate prognostic factor composite pf measurement unit measurement method data type reliability (if applicable/available) mean of maximal loaded leg extension power** yes normalised watts per kilo <UNK> double leg press test using a keiser air <UNK> machine continuous test-retest icc = <UNK> <UNK> loaded maximal leg extension velocity difference* yes peak velocity (m <UNK> ) double leg press test using a keiser air <UNK> machine continuous test-retest icc = <UNK> <UNK> mean of maximal loaded leg extension velocity** yes peak velocity (m <UNK> ) double leg press test using a keiser air <UNK> machine continuous test-retest icc = <UNK> <UNK> loaded maximal leg extension force difference* yes normalised peak force <UNK> ) double leg press test using a keiser air <UNK> machine continuous test-retest icc = <UNK> <UNK> mean of maximal loaded leg extension force** yes normalised peak force <UNK> double leg press test using a keiser air <UNK> machine continuous test-retest icc = <UNK> <UNK> cmj force per kilogram of body mass no force per kg (n/kg) cmj using force plates continuous – cmj height no centimetres cmj using force plates continuous test-retest icc = <UNK> <UNK> pf prognostic factor, phe periodic health examination, imi indirect muscle injury, wbl weight bearing lunge, cmj countermovement jump, prom passive range of movement, icc intraclass correlation coefficient, slr straight leg raise, kg kilos, w watts, (note that <UNK> has a scaling factor to normalise power to body mass), n newtons, (note that <UNK> has a scaling factor to normalise force to body mass), m <UNK> metres/second, "-" not applicable/not available. “*” denotes between limb differences and “**” denotes the mean of the measurements for both limbs hughes et al. diagnostic and prognostic research <UNK> <UNK> page <UNK> of <UNK> development, validation and exploration of potential pfs. we hope this study will also identify further research priorities for this novel and potentially valuable area of sports/football medicine research.

<|EndOfText|>

individual participant data meta-analysis of continuous outcomes: a comparison of approaches for specifying and estimating one-stage models one-stage individual participant data meta-analysis models should account for within-trial clustering, but it is currently debated how to do this. for continuous outcomes modeled using a linear regression framework, two competing approaches are a stratified intercept or a random intercept. the stratified approach involves estimating a separate intercept term for each trial, whereas the random intercept approach assumes that trial intercepts are drawn from a normal distribution. here, through an extensive simulation study for continuous outcomes, we evaluate the impact of using the stratified and random intercept approaches on statistical properties of the summary treatment effect estimate. further aims are to compare (i) competing estimation options for the one-stage models, including maximum likelihood and restricted maximum likelihood, and (ii) competing options for deriving confidence intervals (ci) for the summary treatment effect, including the standard normal-based <UNK> ci, and more conservative approaches of kenward-roger and satterthwaite, which inflate cis to account for uncertainty in variance estimates. the findings reveal that, for an individual participant data meta-analysis of randomized trials with a <UNK> treatment:control allocation ratio and heterogeneity in the treatment effect, (i) bias and coverage of the summary treatment effect estimate are very similar when using stratified or random intercept models with restricted maximum likelihood, and thus either approach could be taken in practice, (ii) cis are generally best derived using either a kenward-roger or satterthwaite correction, although occasionally overly conservative, and (iii) if maximum likelihood is required, a random intercept performs better than a stratified intercept model. an illustrative example is provided. keywords continuous outcomes, estimation, individual participant data, ipd, meta-analysis <UNK> introduction individual participant data (ipd) meta-analysis involves obtaining and then synthesizing raw individual-level data from multiple related studies, to produce summary results that inform clinical decision <UNK> the ipd approach is increasingly popular and has many potential advantages over a traditional meta-analysis of published aggregate data, such as increased power to detect treatment-covariate interactions and avoiding reliance on published <UNK> statistical methods to perform an ipd meta-analysis involve either a one-stage or two-stage <UNK> generally, these approaches give very similar meta-analysis results, especially when they use the same modeling assumptions and/or estimation <UNK> however, the one-stage approach has become increasingly popular over the past <UNK> it conveniently allows all studies to be analyzed simultaneously and avoids the assumption of normally distributed study effect estimates with known variances that is usually made in the second stage of the two-stage approach. it also allows greater flexibility of parameter specification over the two-stage <UNK> when conducting a one-stage ipd meta-analysis, it is important to account for clustering of participants within studies, to correctly condition an individual's response to the study they are in. ignoring clustering and analyzing ipd as if coming from a single study can result in misleading conclusions. for example, abo-zaid et <UNK> showed that family history of thrombophilia was statistically significant as a diagnostic marker of deep vein thrombosis when clustering was accounted for (odds ratio = <UNK> <UNK> confidence interval (ci): <UNK> <UNK> p value = <UNK> but not when clustering was ignored (odds ratio = <UNK> <UNK> ci: <UNK> <UNK> p value = <UNK> while it is well established that clustering should be accounted for, it is debatable exactly how this should be done. in particular, there are two competing approaches to account for clustering in a one-stage model: a stratified intercept or a random intercept. the stratified approach involves a separate intercept term being estimated for each study; thus, if there are <UNK> studies, <UNK> intercept terms would be estimated (one for each study). in the random intercept approach, the intercepts are assumed to be drawn from some distribution, typically normal with an underlying mean value and variance. the advantage of the stratified intercept approach is that it makes no assumptions about the distribution of intercepts across studies. in contrast, the advantage of the random intercept approach is that it requires fewer parameters to be estimated. in this article, we evaluate through an extensive simulation study the impact of using either the stratified or random intercept approach on the statistical properties of the summary treatment effect estimate (for example, in terms of bias, precision, mean square error (mse), and coverage). this is considered in the context of randomized trials with a continuous outcome and a <UNK> treatment:control allocation ratio, assuming either common or random treatment effects across trials. two further aims are to (i) compare competing estimation options for the one-stage models, including maximum likelihood (ml) and restricted maximum likelihood (reml) and (ii) compare competing options for deriving confidence intervals for the summary treatment effect, including the standard normal-based <UNK> ci, and (for reml, but not ml estimation) the kenward-roger <UNK> and <UNK> corrections that inflate confidence intervals to account for uncertainty in variance estimates. this paper is structured as follows. in section <UNK> we introduce the two competing one-stage ipd meta-analysis models of interest that account for clustering, as well as the competing estimation and ci derivation options. in section <UNK> we outline how the simulation study was conducted and present the results, and in section <UNK> we provide a real example to illustrate the methods considered. finally, in section <UNK> we conclude with a discussion of the key findings and limitations and offer a recommendation for those conducting one-stage ipd meta-analysis of randomized trials with <UNK> treatment:control allocation ratio and with a continuous outcome. <UNK> introducing different model specification and estimation options consider that ipd have been obtained from i = <UNK> to k related randomized trials, each investigating a treatment effect based on a continuous outcome y (say, blood pressure); that is, the mean difference in outcome value between a treatment and a control group. suppose that there are ni participants in trial i. let yfi j be the end-of-trial (f used to denote final) continuous outcome value, for participant j in trial i, and ybi j (b to denote baseline) be the pre-treatment outcome value. let treati j take the value <UNK> or <UNK> for participants in the treatment or control group, respectively. given such ipd, there are several ways in which researchers can use a one-stage meta-analysis to model the summary treatment effect across trials. we focus initially on presenting one-stage analysis of covariance (ancova) mixed models, which either use a stratified intercept or a random intercept to account for clustering of participants within trials. we also assume a random treatment effect since heterogeneity is usually expected. <UNK> model <UNK> stratified intercept with the following approach, a stratified intercept is used to account for within-trial clustering. y𝐹 𝑖𝑗 = 𝛽i + 𝜆i ( y𝐵𝑖𝑗 − y𝐵𝑖) + (𝜃 + ui) treat𝑖𝑗 + e𝑖𝑗 <UNK> ui ∼ <UNK> <UNK> ) e𝑖𝑗 ∼ n ( <UNK> <UNK> i ) here, 𝛽i denotes the intercept term for trial i (expected final outcome value for participants in the control group in trial i who have the mean baseline outcome value), and the distinct intercept for each trial is used to account for within trial clustering. the term 𝜆i denotes a trial-specific adjustment term for the baseline outcome value (here, centered at the mean for each trial (y𝐵𝑖) to aid interpretation of the trial-specific intercepts). for example, when there are k = <UNK> trials, there would be <UNK> 𝛽i terms and <UNK> 𝜆i terms. of main interest is an estimate of the model parameter 𝜃, as this denotes the summary (average) treatment effect. the random effect, ui, indicates that the true treatment effects in each trial are assumed to arise from a distribution of true effects with mean 𝜃 and between-trial variance <UNK> this assumption could be constrained if considered appropriate, with a common (fixed) treatment effect (ie, constrain <UNK> = <UNK> lastly, <UNK> i denotes a distinct residual variance per trial. the flexibility of the one-stage ipd approach allows us to make further modifications by considering, for example, a common baseline adjustment term (ie, 𝜆i= 𝜆) across trials, or common residual variances (ie, <UNK> i = <UNK> if <UNK> however, this should be justified (eg, based on computational reasons or estimation problems), and sensitivity analysis to the choice of assumptions is often sensible. <UNK> model <UNK> random intercept when there are a large number of trials to be synthesized, a stratified intercept approach to clustering can be computationally intensive (as equation <UNK> requires estimation of <UNK> k + <UNK> <UNK> an alternative approach for dealing with clustering, which is preferred by some <UNK> is to use a random intercept term. y𝐹 𝑖𝑗 = (𝛽 + <UNK> + 𝜆i ( y𝐵𝑖𝑗 − y𝐵𝑖) + (𝜃 + <UNK> treat𝑖𝑗 + e𝑖𝑗 <UNK> <UNK> ∼ n ( <UNK> <UNK> 𝛽 ) <UNK> ∼ <UNK> <UNK> ) e𝑖𝑗 ∼ n ( <UNK> <UNK> i ) parameters are as in equation <UNK> except that within-trial clustering has now been accounted for by a random (instead of stratified) intercept term, with <UNK> 𝛽 denoting the between trial variance in the intercept about the mean intercept (𝛽). equation <UNK> assumes independence of the two random effects (ie, a covariance of zero), but their correlation could be accounted for assuming a bivariate random effect distribution; indeed, this might be of special interest when evaluating the relationship across trials of mean baseline in the control group and true treatment <UNK> compared to equation <UNK> the number of parameters to be estimated has been reduced, with only 𝛽 and 𝜏𝛽 for the intercept, instead of k separate terms. therefore, fewer estimation problems might be anticipated than in equation <UNK> on the downside, equation <UNK> makes a strong and potentially unnecessary assumption that control group means are drawn from a normal distribution with a common mean and variance. furthermore, the estimation of an additional random effect term might increase computational intensity. <UNK> options for estimation and ci derivation the parameters in models <UNK> and <UNK> are typically estimated using either a ml or reml approach. ml is known to produce downwardly biased estimates of between trial variance when there are few <UNK> whereas reml addresses the downward bias and is thus generally <UNK> in addition to competing options for model parameter estimation, there are also competing options to subsequently derive <UNK> − <UNK> cis for the true summary treatment effect (𝜃). standard cis are based on large-sample inference and assume 𝜃 ̂ is approximately normally distributed: 𝜃 ̂± <UNK> <UNK> √ var(𝜃 ̂), <UNK> where 𝜃 ̂is the estimate of 𝜃, var(𝜃 ̂) is its variance, and <UNK> <UNK> is the upper <UNK> 𝛼 <UNK> quantile of the standard normal distribution. this standard approach may produce cis that are too narrow, as var(𝜃 ̂) does not account for the uncertainty in the estimate of the between trial variation of 𝜃 ̂. <UNK> to address this, more conservative options are available based on small-sample inference, which define the uncertainty around 𝜃 ̂ using approximations based on a t-distribution, such as the <UNK> and <UNK> corrections, which are also known as denominator-degrees-of-freedom adjustments. the kr corrected <UNK> − <UNK> ci is given by 𝜃 ̂± <UNK> <UNK> √ varkr(𝜃 ̂), <UNK> where 𝜃 ̂ is as before, but now a bias-adjusted (inflated) variance (varkr(𝜃 ̂)) is used, and <UNK> <UNK> (the upper <UNK> <UNK> quantile of the t-distribution with an adjusted degrees of freedom, 𝜐) instead of <UNK> <UNK> . for a single parameter of interest (as in our case), the satterthwaite corrected <UNK> − <UNK> ci is given by 𝜃 ̂± <UNK> <UNK> √ var(𝜃 ̂), <UNK> where <UNK> <UNK> is as in the kr correction, but the original (unadjusted) variance of 𝜃 ̂is used. note that, while the denominator degrees of freedom calculated from the kr and satterthwaite corrections are the same for single hypothesis tests, the kr correction uses a bias-adjusted variance; therefore, cis derived using equations <UNK> and <UNK> will potentially differ, with the one using the kr correction (equation <UNK> leading to slightly wider <UNK> although schaalje et <UNK> recommend kr over satterthwaite in special cases when the sampling distribution of the test statistic is known, there remains debate over the best method, and a lack of literature in this area in regard to ipd meta-analysis for estimation of a parameter of interest. <UNK> simulation study we now perform a simulation study to examine the statistical performance of the summary treatment effect estimate (𝜃 ̂) from a one-stage ipd meta-analysis across a range of scenarios. our aim is to assess the different model specifications, parameter estimation methods and ci derivation options described in section <UNK> that is, we compare the following: stratified or random intercept specifications; ml or reml estimation options; and, for reml estimation, <UNK> cis based on asymptotic formula (equation <UNK> or with either kr or satterthwaite corrections (equations <UNK> and <UNK> respectively)). <UNK> methods provided is a step-by-step guide to our simulation study. for simplicity, and to considerably speed up the many simulations, we removed the baseline adjustment term in models <UNK> and <UNK> such that it does not exist in any of the data generating mechanisms or models fitted in our simulations. in other words, we generate data without baseline imbalances and thus analyze the data according to a final score ipd meta-analysis model, which is appropriate in this situation. <UNK> for similar reasons of simplicity and computational complexity, we assumed a common residual variance across trials (both in data generation and models fitted). extension to different residual variances is considered in our discussion (section <UNK> to inform the true parameter values for the simulation, we used a previous ipd meta-analysis of treatment for lower blood pressure <UNK> all analyses were conducted using stata <UNK> (stata corporation, tx, <UNK> <UNK> scenario <UNK> (base case) the simulation process is now explained, in the context of an initial base case scenario with ipd from <UNK> trials and a relatively simple data generating mechanism. extensions to other more complex scenarios are described afterwards. step <UNK> data generating mechanism for one ipd meta-analysis of <UNK> trials consider that an ipd meta-analysis of i = <UNK> to k related trials is of interest, with the goal to summarize a treatment effect on a continuous outcome. to generate such data for the base case of this simulation study, we started by setting the number of trials, k, to <UNK> we set a fixed number of participants, n = <UNK> in each trial, and assumed a fixed randomization of <UNK> in each trial; that is, on average, <UNK> of participants within any given trial are allocated to a treatment group, and the remaining <UNK> to a control group. this gave us a triali (trial <UNK> indicator) and treatij (treatment group <UNK> indicator) value for each of <UNK> participants in each of <UNK> trials. next, based on the previous <UNK> we set the true parameter values for this simulation to be as follows: 𝜃 = <UNK> (summary treatment effect; negative value favors treatment group), <UNK> = <UNK> (between trial variation in the treatment effect), 𝛽 = <UNK> (mean blood pressure response in control group), <UNK> 𝛽 = <UNK> (between trial variation in the intercept), and <UNK> = <UNK> (residual variance). we then used these parameter values to generate further terms, beginning with using <UNK> to generate an error term ei j, for the jth participant from the ith trial e𝑖𝑗 ∼ <UNK> <UNK> ). <UNK> then, we generated the trial level values for the random parts of the intercept and treatment effect terms, <UNK> and <UNK> respectively, <UNK> ∼ n ( <UNK> <UNK> 𝛽 ) <UNK> <UNK> ∼ <UNK> <UNK> ). finally, with all the parameters defined (𝛽, <UNK> 𝜃, <UNK> treati j, and ei j), we generated the end-of-trial continuous outcome value yfi j, under the random intercept model <UNK> (with no baseline adjustment term and assuming a common residual variance) y𝐹 𝑖𝑗 = (𝛽 + <UNK> + (𝜃 + <UNK> treat𝑖𝑗 + e𝑖𝑗. <UNK> this gave one complete ipd meta-analysis dataset of <UNK> total participants, containing <UNK> participants in each of <UNK> trials, consisting of the following data for each individual: a trial indicator (triali), a treatment group indicator (treati j), and an end-of-trial continuous outcome value (yfi j). step <UNK> model fit and replication using the generated data, we fitted a stratified intercept model <UNK> and a random intercept model <UNK> (without the baseline adjustment term and assuming a common residual variance) separately to this simulated ipd, under all the combinations of estimation and ci derivation methods outlined in section <UNK> figure <UNK> provides a flow diagram summarizing the possible combinations. each time a model was fitted (under a particular combination of estimation and ci derivation methods), we stored the following: the summary treatment effect estimate, 𝜃 ̂; its corresponding <UNK> ci; a binary indicator variable for coverage of 𝜃 ̂ (ie, the value <UNK> if the <UNK> ci of 𝜃 ̂ contained the true 𝜃, and <UNK> otherwise); estimates of any variance parameters; model run time (from start of model fit to end of post estimation); and model convergence <UNK> for convergence within <UNK> iterations/nonconvergence, respectively). standard ci stratified intercept model ml reml standard ci kr ci satterthwaite ci intercept option estimation method ci method standard ci random intercept model ml reml standard ci kr ci satterthwaite ci figure <UNK> flow diagram of possible combinations of intercept option, estimation, and ci methods. ci, confidence interval; kr, kenward-roger correction; ml, maximum likelihood estimation; reml, restricted maximum likelihood for each model (stratified or random intercept) fitted to the data, this enabled us to obtain two estimates of 𝜃 (one each for the models fitted using ml and reml estimation, respectively) and four <UNK> cis for 𝜃 ̂ (one for ml estimation with a standard ci derivation, and then one each for reml estimation with the standard, kr-corrected, and satterthwaite-corrected ci derivations). step <UNK> simulation replications steps <UNK> and <UNK> were repeated until <UNK> ipd meta-analysis datasets had been generated using the true parameter values and procedure as outlined thus far, followed by application of the various intercept option, estimation, and ci methods to each of the <UNK> replicated datasets (note: <UNK> simulations were chosen to give a monte carlo error of <UNK> on a coverage of <UNK> step <UNK> summarizing performance using the results obtained after step <UNK> the statistical properties of 𝜃 ̂ under the different model specification and estimation options were assessed by summarizing the <UNK> results obtained using the following metrics: mean percentage (%) bias, empirical standard error (se), mse, coverage (separately for each ci method), convergence, and mean run time (separately for each ci method). additionally, we considered the median percentage bias in the heterogeneity <UNK> of the true treatment effects also. definitions of these performance measures are provided in web appendix a. <UNK> extended set of <UNK> scenarios changing number of trials, participants, between-trial distributions, and data generating mechanisms the base case scenario defined in section <UNK> was extended to further settings, leading to an extensive range of <UNK> scenarios in total (see table <UNK> which we now summarize. we varied the number of trials (scenarios <UNK> and <UNK> so that k = <UNK> <UNK> and <UNK> were considered, which cover the typical sizes of ipd meta-analyses in our experience. we also considered trials with differing sample sizes within an ipd, so that ni (number of participants within trial i) was drawn from a uniform distribution, ni∼u(a, b). fixing a = <UNK> b = <UNK> (scenario <UNK> allowed for mixed sample sizes, and having <UNK> trials with a = <UNK> b = <UNK> and <UNK> with a = <UNK> b = <UNK> within an ipd (scenario <UNK> tested the effect of a mix of small and large sample sizes only. lastly, fixing a = <UNK> b = <UNK> tested the effect of having only small trials (scenario <UNK> we also tested the combined effect of varying the number of trials and number of participants per trial simultaneously (scenarios <UNK> <UNK> <UNK> <UNK> and we tested the effects of adjusting the magnitude of the intercept or treatment effect heterogeneity (scenarios <UNK> <UNK> <UNK> <UNK> scenarios <UNK> to <UNK> replicate the first <UNK> scenarios where possible, for modifications to the base case data generating mechanism. first, to test the robustness of the normality of the intercept assumption in the random intercept model, we altered the final step of the data generating mechanism in equation <UNK> so that the final outcome was calculated by y𝐹 𝑖𝑗 = 𝛽i + (𝜃 + <UNK> + e𝑖𝑗 <UNK> 𝛽i ∼ (beta <UNK> <UNK> × <UNK> <UNK> ∼ <UNK> <UNK> ) e𝑖𝑗 ∼ <UNK> <UNK> ). therefore, the intercept term 𝛽i was now derived from a beta distribution with shape parameters of <UNK> and <UNK> which represent a negatively skewed distribution that was then scaled by <UNK> to give sensible values for systolic blood pressure (the outcome upon which the hypothetical data is based). an example density plot of this beta distribution for modeling the intercept term is shown in web figure <UNK> secondly, we also considered a data generating mechanism with a common (fixed) treatment effect (ie, <UNK> = <UNK> here, the fitted stratified and random intercept models were also modified to have a common treatment effect. <UNK> results simulation results are shown in tables <UNK> and <UNK> covering most of the scenarios under the normal and beta distribution intercept data generating mechanisms, across all options for specifying and estimating the intercept. these tables show the mean percentage bias of the summary treatment effect estimate (𝜃 ̂) (table <UNK> and the median percentage bias in its heterogeneity <UNK> (table <UNK> figure <UNK> graphically depicts the percentage coverage of the summary treatment effect estimate (𝜃 ̂). . table <UNK> summary of the different simulation scenarios* scenario data generation details modification from base case scenario base case (i) number of trials, k = <UNK> - (ii) number of participants in trial i, ni = <UNK> (fixed across all trials) (iii) fixed treatment exposure of <UNK> (iv) 𝜃 = <UNK> (summary treatment effect; negative value favors treatment group) (v) <UNK> = <UNK> (between trial variation in 𝜃) (vi) 𝛽 = <UNK> (mean response in control group) (vii) 𝜏𝜷 <UNK> = <UNK> (between trial variation in 𝛽) (viii) <UNK> = <UNK> (residual variance) <UNK> same as base case, except changed (i) k = <UNK> <UNK> same as base case, except changed (i) k = <UNK> <UNK> same as base case, except changed (ii) n ∼i <UNK> <UNK> <UNK> same as base case, except changed (ii) n ∼i <UNK> <UNK> for trials <UNK> to <UNK> n ∼i <UNK> <UNK> for trials <UNK> to <UNK> <UNK> same as base case, except changed (i) and (ii) k = <UNK> and n ∼i <UNK> <UNK> <UNK> same as base case, except changed (i) and (ii) k = <UNK> and n ∼i <UNK> <UNK> <UNK> same as base case, except changed (i) and (ii) n ∼i <UNK> <UNK> for trials <UNK> and <UNK> n ∼i <UNK> <UNK> for trials <UNK> to <UNK> <UNK> same as base case, except changed (i) and (ii) n ∼i <UNK> <UNK> for trials <UNK> to <UNK> n ∼i <UNK> <UNK> for trials <UNK> to <UNK> <UNK> same as base case, except changed (ii) n ∼i <UNK> <UNK> <UNK> same as base case, except changed (vii) halving 𝜏𝛽 <UNK> to <UNK> <UNK> same as base case, except changed (vii) doubling 𝜏𝛽 <UNK> to <UNK> <UNK> same as base case, except changed (v) halving <UNK> to <UNK> <UNK> same as base case, except changed (v) doubling <UNK> to <UNK> *each scenario was repeated under the following data generating mechanisms: <UNK> random treatment effect with a normally distributed intercept, <UNK> random treatment effect with a <UNK> <UNK> distribution for the intercept (except scenarios <UNK> and <UNK> and <UNK> common treatment effect with a normally distributed intercept (except scenarios <UNK> and <UNK> abbreviations: k = number of trials, ni = number of participants in trial i, 𝜃 = summary treatment effect, <UNK> = between trial variation in summary treatment effect, 𝛽 = mean response in control group, 𝜏𝛽 <UNK> = between trial variation in mean response in control group, <UNK> = residual variance, u (a, b) = uniform distribution over the interval (a, b). table <UNK> mean percentage bias of the summary treatment effect estimate (𝜃 ̂) under different scenarios, for the random treatment effect with normal and beta distributions for the intercept data generating mechanisms. results shown separately for stratified <UNK> and random <UNK> intercept models, under each of the different estimation options considered mean percentage bias of 𝜽̂ intercept normal distribution beta distribution generating mechanism method for stratified intercept random intercept stratified intercept random intercept modeling intercept estimation ml reml ml reml ml reml ml reml scenario* base case <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> n/a n/a n/a n/a <UNK> <UNK> <UNK> <UNK> <UNK> n/a n/a n/a n/a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> * see table <UNK> for full data generation details relating to each scenario. true value for 𝜃 is <UNK> n/a = not applicable, since there is no 𝜏𝛽 <UNK> to vary when a beta distribution is used for the intercept data generating mechanism. options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation. we focus on the results when assuming a random treatment effect. further results assuming a common treatment effect data generating mechanism and for additional performance measures (percentage convergence of models, numerical percentage coverage of the summary treatment effect estimate, average run time of simulations, and empirical se and mse of the summary treatment effect estimate) are shown in the supplementary material (web appendices b and c, respectively). in the following, we summarize the key findings. <UNK> convergence of models under a random treatment effect data generating mechanism, the proportion of models that converged was consistently high, with a minimum convergence of <UNK> across all situations (web table c.i). note that all other performance measures to follow are estimated conditional on model convergence. <UNK> bias of summary treatment effect estimate generally, there were negligible differences in mean percentage bias of 𝜃 ̂ between ml and reml estimation options for either model (stratified or random intercept), under any given scenario and data generating mechanism (table <UNK> and web table b.i). nor were there any important differences in the mean percentage bias of 𝜃 ̂ between the stratified model and random intercept model. furthermore, mean bias was close to zero in all situations and only reached a maximum absolute percentage of <UNK> <UNK> bias of estimated between-trial variance of treatment effects for either model (stratified or random intercept), under any given scenario and data generating mechanism, using ml always produced more downwardly biased estimates than reml (table <UNK> as <UNK> for example, for the base table <UNK> median percentage bias of the between-trial variance of treatment effects <UNK> under different scenarios for the random treatment effect with normal and beta distributions for the intercept data generating mechanisms. results shown separately for stratified and random intercept models, under each of the estimation options considered median percentage bias of ̂𝝉𝟐 intercept normal distribution beta distribution generating mechanism method for stratified intercept random intercept stratified intercept random intercept modeling intercept estimation ml reml ml reml ml reml ml reml scenario* base case <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> n/a n/a n/a n/a <UNK> <UNK> <UNK> <UNK> <UNK> n/a n/a n/a n/a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> * see table <UNK> for full data generation details relating to each scenario. true value for <UNK> is <UNK> except scenarios <UNK> and <UNK> where <UNK> is equal to <UNK> and <UNK> respectively. n/a = not applicable, since there is no 𝜏𝛽 <UNK> to vary when a beta distribution is used for the intercept data generating mechanism. options: ml, maximum likelihood estimation; reml, restricted maximum likelihood estimation. case scenario with the random intercept model, under the normal intercept data generating mechanism, the median percentage bias using reml estimation was <UNK> compared to <UNK> using ml estimation. the bias was worse when using a stratified intercept model (due to the extra number of parameters to estimate), as ml estimation often produced a downward median bias of <UNK> when using reml estimation, there were generally only small differences between random and stratified intercept models in terms of bias of the between-trial variance of treatment effects; however, while better than ml, downward bias was not removed entirely with reml. furthermore, the overall size of the bias was typically greater in the beta distribution intercept case than in the normal distribution intercept case, regardless of which model was used. <UNK> empirical se and mse of summary treatment effect estimate there were negligible differences in empirical se or mse of 𝜃 ̂ between the two models (stratified or random intercept), under any given scenario and data generating mechanism (web tables c.viii to c.x). <UNK> coverage of summary treatment effect estimate there were marked differences observed in the coverage of 𝜃 ̂ across the different estimation approaches (ml or reml) and ci derivations (standard, kr, or satterthwaite), as now explained. (i) under a normal distribution intercept generating mechanism we consider first the normal distribution intercept generating mechanism (figure <UNK> and web table c.ii). across both models and all scenarios, ml with standard ci (ml + standard) derivation always exhibited under-coverage compared to the other options (reml+standard, reml+kr, reml+satterthwaite). for example, for scenario <UNK> using the stratified intercept model, the percentage coverage using ml + standard was <UNK> compared to <UNK> <UNK> and <UNK> using figure <UNK> percentage coverage of the summary treatment effect estimate (𝜃 ̂) under different scenarios for the random treatment effect with normal (figure <UNK> and beta distributions (figure <UNK> for the intercept data generating mechanisms, for stratified (left) and random (right) intercept models, under each of the estimation and ci derivation options considered. options: ml, maximum likelihood estimation with standard confidence interval (ci) derivation; reml, restricted maximum likelihood estimation with standard ci derivation; reml+kr, reml estimation with kenward-roger ci derivation; reml+satt, reml estimation with satterthwaite ci derivation [colour figure can be viewed at wileyonlinelibrary.com] reml+standard, reml+kr and reml+satterthwaite, respectively. the random intercept model always performed better with respect to coverage under ml than the stratified intercept model under ml, likely due to the reduction in the number of parameters that needed estimation. for example, when considering only small trials (scenario <UNK> percentage coverage improved from <UNK> to <UNK> (close to the nominal <UNK> level), when comparing the stratified to a random intercept model with ml estimation. using reml substantially improved on the coverage obtained from ml and removed any important differences between the stratified and random intercept models. however, for either model (stratified or random intercept), reml+standard still had important under-coverage in some scenarios. for example, in scenario <UNK> a percentage coverage of <UNK> and <UNK> was observed, under a stratified and random intercept model, respectively. the reml+kr approach generally improved on the coverage compared to reml+standard, again with no important differences observed between the stratified and random intercept models. percentage coverage ranged from <UNK> to <UNK> using reml+kr, while the percentage coverage ranged from <UNK> to <UNK> using reml+standard. the improvement gained by using reml+kr was especially important for scenarios that involved at least <UNK> trials and a large variation in sample sizes <UNK> <UNK> <UNK> <UNK> for example, for scenario <UNK> (five small and five large sample sized trials, with average sample size <UNK> and <UNK> in the small and large trials, respectively), percentage coverage from the stratified intercept model was <UNK> using reml+standard, but <UNK> using reml+kr. using reml+satterthwaite gave very similar results to reml+kr. occasionally, there was some over-coverage using reml+kr or reml+satterthwaite, particularly when using a low number of trials (k = <UNK> for example, coverage was close to <UNK> (regardless of which model was used), in a setting of k = <UNK> trials with an equal number of participants per trial (scenario <UNK> ni = <UNK> and in a setting of k = <UNK> trials with some small-sized and some large-sized trials (scenario <UNK> <UNK> small trials where <UNK> <UNK> and <UNK> large trials where <UNK> <UNK> (ii) under a beta distribution intercept generating mechanism for the beta distribution intercept generating mechanism (figure <UNK> and web table c.iii), using reml+standard again gave better coverage than using ml, and using reml+kr or reml+satterthwaite generally further improved upon this coverage (ie, moved it closer to <UNK> especially with scenarios concerning at least <UNK> trials that had a large variation in sample sizes. as before, under ml estimation, the random intercept model showed better estimates of between-trial variance and improved coverage (closer to <UNK> than the stratified intercept model. however, differences between the two models were generally small for estimation under reml (with or without a <UNK> ci correction). <UNK> common treatment effect data generating mechanism results based on a common (fixed) treatment effect data generating mechanism are shown in web appendix b. all fitted models assumed a common treatment effect and converged every time (ie, <UNK> convergence), and there was negligible difference in mean percentage bias of 𝜃 ̂ between ml and reml estimation options for either model (stratified or random intercept), or between either model (web table <UNK> the percentage coverage results were stable across all comparisons, ranging from <UNK> to <UNK> with negligible differences between the various models and estimation options (web figure <UNK> <UNK> key findings a summary of the key findings from this simulation study for settings with between-trial heterogeneity in the treatment effect is given in figure <UNK> figure <UNK> key simulation findings and recommendation for estimating a summary treatment effect based on a one-stage individual participant data (ipd) meta-analysis of randomized trials with a <UNK> treatment:control allocation ratio and a continuous outcome, with between-study heterogeneity in the treatment effect. ci, confidence interval; ml, maximum likelihood estimation; mse, mean square error; reml, restricted maximum likelihood; se, standard error <UNK> illustration of methods and key findings in a real example the international weight management in pregnancy (i-wip) collaborative group dataset includes ipd from <UNK> trials <UNK> women), collected for a health technology assessment report in <UNK> the authors investigated the association between diet and lifestyle interventions to prevent weight gain in pregnancy and several other primary outcomes. here, we present ipd meta-analysis results using the i-wip dataset, for illustration purposes only, to demonstrate the key findings from our simulation study. we include only trials that collected follow-up outcome values for weight in pregnancy and apply a one-stage ipd model for this continuous outcome, with model assumptions in line with our simulation study analysis. however, while we did not generate any baseline imbalances in our simulation data, baseline weight imbalance was present in some trials from the i-wip data. to remove this imbalance, we apply a baseline adjustment in our models, as is <UNK> table <UNK> shows ipd meta-analysis results for a random sample of <UNK> and <UNK> trials that investigated exercise interventions and for <UNK> trials (using all <UNK> exercise trials, plus <UNK> additional trials that investigated mixed interventions). these results are in agreement with the key findings observed in section <UNK> and summarized in figure <UNK> firstly, the magnitude of summary treatment effect estimate was similar throughout, irrespective of model used or estimation method. secondly, with ml estimation, the stratified intercept model gave narrower <UNK> cis and smaller estimates of the between trial variance than the random intercept model, especially with k = <UNK> trials. thirdly, using reml overcame this discrepancy, with now very similar results between the random and stratified intercept models; in addition, the <UNK> cis were wider when using reml, due to the larger estimates of the between trial variance. fourthly, applying a kr or satterthwaite correction in addition to reml further widened the <UNK> cis. finally, although <UNK> cis were slightly wider when using a kr correction instead of satterthwaite, results were generally similar from these two corrections, especially with k = <UNK> trials. <UNK> discussion <UNK> key findings in summary, we have conducted an extensive simulation study to examine the estimation of a summary treatment effect using a one-stage ipd meta-analysis model for a continuous outcome. specifically, we examined different options for specifying the trial-specific intercepts and compared different options for parameter estimation and ci derivation. fourteen different scenarios were tested (varying the number of trials, number of participants per trial, and heterogeneity of parameters), for each of three different data generating mechanisms (encompassing a common and random treatment effect with a normally distributed intercept, as well as a beta distributed intercept and random treatment effect). all scenarios assumed a <UNK> treatment:control allocation ratio, and a data generating mechanism that was based on a random intercept model; hence, our conclusions are restricted to this context. our key findings, for settings with heterogeneity in treatment effect, were illustrated using a real example, and these are summarized in figure <UNK> firstly, the results suggest that, as long as the same estimation method is used, there are no important differences between the stratified and random intercept models in terms of bias, empirical se or mse for the summary treatment effect estimate. indeed, the mean bias in 𝜃 ̂ was close to zero throughout, which is perhaps expected given the statistical theory underpinning linear mixed models. furthermore, when using reml (with or without a ci correction), there were generally no important differences in coverage performance between the stratified and random intercept models. interestingly, the random intercept model (which assumes normality of the intercept) performed well even when the trial intercepts were drawn from a highly asymmetric beta distribution. kahan and <UNK> also found that misspecifying the random intercept distribution of random effects models did not impact treatment effect results. secondly, the kr and satterthwaite corrections generally performed similarly in terms of improving the coverage and were especially effective for scenarios involving at least <UNK> trials with a mix of small and large sample sizes, but also considerably increased mean run time in these instances (see web tables c.v to c.vii). the satterthwaite correction always had a similar or quicker average run time than kr (sometimes by more than eight times). one could surmise from the similarity in coverage performance that the main impact of both corrections is in the use of a t-distribution to derive cis and that the kr adjusted variance of the summary estimate has relatively less impact. thirdly, when using ml estimation, the random intercept model always showed better or comparable coverage to the stratified intercept model (closer to <UNK> this is likely due to the random intercept model having a reduced number of . table <UNK> results from baseline weight adjusted individual participant data meta-analysis of i-wip data: summary treatment effect estimate ( ̂ 𝜃) with <UNK> confidence interval and between-trial variance of treatment effects estimate <UNK> from meta-analysis with different numbers of trials (k = <UNK> <UNK> or <UNK> and assuming a random treatment effect and a common residual variance throughout ̂ 𝜽 <UNK> ci); ̂𝝉𝟐 method for modeling stratified random intercept intercept intercept estimation ml reml reml+kr reml+satt ml reml reml+kr reml+satt number of trials <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> ci = confidence interval options: ml, maximum likelihood estimation with standard ci derivation; reml, restricted maximum likelihood estimation with standard ci derivation; reml+kr, reml estimation with kenward-roger ci derivation; reml+satt, reml estimation with satterthwaite ci derivation. parameters, and thus improved ml estimation of the between-trial variance. a similar finding was also recently shown by jackson et <UNK> for one-stage meta-analysis models for a binary outcome. nevertheless, even the random intercept model produced downwardly biased estimates of the between-trial variance using ml and low coverage. using reml is therefore important, to improve on this coverage. indeed, coverage is more consistently near <UNK> when using reml with either a kr or satterthwaite correction. however, on some occasions (particularly, when there are a low number of trials), the kr and satterthwaite corrections lead to over-coverage. this is similar to the hartung-knapp sidik-jonkman correction to <UNK> cis following a two-stage <UNK> which generally gives a more suitable coverage than a standard <UNK> ci, although on occasion is overly <UNK> if there is genuinely no heterogeneity in treatment effect across trials, however, our findings suggest that there are generally no differences in mean bias, empirical se, mse, or coverage for the treatment effect between the stratified and random intercept models, for any estimation method, ci derivation approach, and under any simulation scenario. however, in our experience, situations of completely homogeneous treatment effects are unlikely. unreported simulations following recent work by morris et <UNK> and jackson et <UNK> which considered an alternative coding for the binary treatment group variable <UNK> for treatment/control groups, respectively), we also tested this treatment group coding for our reml estimation simulation results but found only small differences in performance results compared to the <UNK> coding. hence, we did not present the results here. we also tested using stratified (instead of common) residual variances for both the data generating mechanisms and models fitted. again, no difference in performance of the summary treatment effect estimate was observed, suggesting that the ipd model may be robust to the (mis) specification of the residual variances. morris et <UNK> also found that assuming common or distinct residual variances, in a common treatment effect ipd meta-analysis setting, has very little impact on the precision of the summary effect when the number of patients per trial is over <UNK> in general, from a point of principle, we recommend a separate residual variance for each trial, but in situations where this has convergence problems, a common residual variance would seem apt. further research of this issue would be welcome. <UNK> recommendation for researchers conducting a one-stage ipd meta-analysis of randomized trials with a <UNK> treatment:control allocation ratio and a continuous outcome and aiming to estimate a summary treatment effect that is heterogeneous across trials, we recommend that either a stratified or random intercept model is used, and estimated using reml, ideally followed by a <UNK> ci derived using either the kr or satterthwaite approaches. in our simulations, this approach gave close to zero mean bias in the summary treatment effect estimate and coverage generally close to <UNK> except in a few situations where there was over-coverage (particularly, when there were a low number of trials). using reml with a kr correction for linear mixed models based on continuous outcomes has already been proposed by some <UNK> while literature advocating the merits of the satterthwaite correction is less common. however, in our simulations, we found that the satterthwaite correction generally obtains similar results to the kr approach, hence making for an excellent alternative. <UNK> limitations and further research throughout this simulation study, we focused solely on synthesizing trials containing a <UNK> treatment:control allocation ratio; hence, an important limitation is that our conclusions may not hold under settings involving other treatment allocation ratios. in addition, we have focused solely on ipd of continuous outcomes, hence another important limitation is that our conclusions are not necessarily generalizable to other popular outcome types in the meta-analytical field, such as binary and time-to-event outcomes. binary outcomes, for example, are more complex to deal with than continuous outcomes, as a logistic mixed effects model is nonlinear, and hence, the corresponding maximum likelihood function has no closed form. jackson et <UNK> recently investigated the use of ml estimation and found that a stratified intercept model leads to substantial downward bias in between-trial variance estimates and under-coverage of cis for the summary result, which increases as the number of trials (and thus parameters) increases. interestingly, the issue was resolved when using a + <UNK> coding for the treatment variable, rather than a <UNK> coding, or when placing random effects on the trial <UNK> <UNK> investigated logistic mixed models by either retaining the nonlinearity of the model and making an approximation for the likelihood function or linearly approximating the model to give the likelihood function a closed form (pseudo-likelihood approach). the latter option was shown to be favorable (under the specific conditions of the study), by use of a residual penalized quasi-likelihood with a kr correction. while our simulation study did consider an extensive range of scenarios—we varied the number of trials, number of participants per trial, and heterogeneity of parameters—we recognize that our conclusions were based on a final score model that did not adjust for baseline outcome value. often, the ancova model should be used, as in our applied example, because there will be baseline imbalances in practice. however, as baseline values did not vary across individuals in our simulation study, using a final score analysis model rather than ancova was appropriate. when using one-stage ancova ipd models, an additional issue is using stratified adjustment terms or placing a random effect on the adjustment term. based on our study findings, we expect that, with reml, either approach should be suitable. we also assumed independence of the two random effects (ie, a covariance of zero) when assuming random intercept and random treatment effects, both in the data generating mechanism and when fitting the corresponding model. their correlation could be taken into account if deemed <UNK> however, we did not consider this alternative assumption in our simulation study, largely due to the added complexity and difficulty in estimating the correlation parameter in practice with few trials. importantly, it is perhaps likely that the effect of treatment could be correlated with the control group outcome, and therefore, the most appropriate assumption needs further consideration. another limitation is that we did not consider prediction intervals. these allow us to make predictive inferences of the potential treatment effect in a single setting of <UNK> some researchers argue that prediction intervals offer a more appropriate summary of trial findings than cis of the average <UNK> however, partlett and <UNK> showed in a two-stage ipd meta-analysis setting that there was considerable under-coverage of prediction intervals in some situations. for example, under-coverage was observed in settings involving a low heterogeneity or with varied trial sample sizes and was not improved upon by increasing the number of trials or using ci corrections such as kr. hence, we did not consider it useful to consider prediction intervals in our study. finally, we could have considered a bayesian approach to our simulation study, which is an alternative to frequentist methods, and a natural way to account for all parameter uncertainty, to make predictions and to derive (joint) probabilistic statements regarding parameters of interest. however, we deemed this extension to be beyond the scope of this paper. yet, if a bayesian approach is to be used in practice, bayesians still need to choose between random or stratified intercept one-stage ipd models, which is something that our work can help with. <UNK> conclusions in an ipd meta-analysis of trials with a <UNK> treatment:control allocation ratio and a continuous outcome, aiming to estimate a summary treatment effect that is heterogeneous across trials, our findings suggest that researchers use either a stratified or random intercept model with reml estimation and ideally derive <UNK> cis using either the kr or satterthwaite approach. further work is needed to improve upon coverage in a few situations where the kr and satterthwaite intervals are overly conservative. such situations include when there are a low number of trials; these are also situations where corrections to cis in a two-stage ipd meta-analysis are overly <UNK>

<|EndOfText|>

minimum sample size for developing a multivariable prediction model: part ii - binary and time-to-event outcomes when designing a study to develop a new prediction model with binary or time-to-event outcomes, researchers should ensure their sample size is adequate in terms of the number of participants (n) and outcome events (e) relative to the number of predictor parameters (p) considered for inclusion. we propose that the minimum values of n and e (and subsequently the minimum number of events per predictor parameter, epp) should be calculated to meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global shrinkage factor of <UNK> (ii) small absolute difference of ≤ <UNK> in the model's apparent and adjusted nagelkerke's <UNK> and (iii) precise estimation of the overall risk in the population. criteria (i) and (ii) aim to reduce overfitting conditional on a chosen p, and require prespecification of the model's anticipated cox-snell <UNK> which we show can be obtained from previous studies. the values of n and e that meet all three criteria provides the minimum sample size required for model development. upon application of our approach, a new diagnostic model for chagas disease requires an epp of at least <UNK> and a new prognostic model for recurrent venous thromboembolism requires an epp of at least <UNK> this reinforces why rules of thumb (eg, <UNK> epp) should be avoided. researchers might additionally ensure the sample size gives precise estimates of key predictor effects; this is especially important when key categorical predictors have few events in some categories, as this may substantially increase the numbers required. keywords binary and time-to-event outcomes, logistic and cox regression, multivariable prediction model, pseudo r-squared, sample size, shrinkage <UNK> introduction statistical models for risk prediction are needed to inform clinical diagnosis and prognosis in <UNK> for example, they may be used to predict an individual's risk of having an undiagnosed disease or condition (“diagnostic prediction model”), or to predict an individual's risk of experiencing a specific event in the future (“prognostic prediction model”). they are typically developed using a multivariable regression framework, such as logistic or cox (proportional hazards) regression, which provides an equation to estimate an individual's risk based on their values of multiple predictors (such as age and smoking, or biomarkers and genetic information). well-known examples are the wells score for predicting the presence of a pulmonary <UNK> ; the framingham risk score and <UNK> which estimate the <UNK> risk of developing cardiovascular disease (cvd); and the nottingham prognostic index, which predicts the <UNK> survival probability of a woman with newly diagnosed breast <UNK> researchers planning or designing a study to develop a new multivariable prediction model must consider sample size requirements for their development data set. our related paper considered this issue for prediction models of a continuous outcome using linear <UNK> here, we focus on binary and time-to-event outcomes, such as the risk of already having a pulmonary embolism, or the risk of developing cvd in the next <UNK> years. in this situation, the effective sample size is often considered to be the number of outcome events (eg, the number with existing pulmonary embolism, or the number diagnosed with cvd during follow-up). in particular, a well-used “rule of thumb” for sample size is to ensure at least <UNK> events per candidate predictor <UNK> where “candidate” indicates a predictor in the development data set that is considered, before any variable selection, for inclusion in the final model. note that, if a predictor is categorical with three of more categories, or continuous and modelled as a nonlinear trend, then including the predictor will require two or more parameters being included in the model. therefore, we refer to events per predictor parameter (epp) here, rather than events per variable. the <UNK> epp rule has generated much debate. some authors claim that the epp can sometimes be lowered below <UNK> in contrast, harrell generally recommends at least <UNK> <UNK> and others identify situations where at least <UNK> epp or up to <UNK> epp are <UNK> however, a concern is that any blanket rule of thumb is too simplistic, and that the number of participants required will depend on many intricate aspects, including the magnitude of predictor effects, the overall outcome risk, the distribution of predictors, and the number of events for each category of categorical <UNK> for example, courvoisier et <UNK> concluded that “there is no single rule based on epp that would guarantee an accurate estimation of logistic regression parameters.” a new sample size approach is needed to address this. in this article, we propose the sample size (n) and number of events (e) in the model development data set must, at the very least, meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global shrinkage factor of <UNK> (ii) small absolute difference of ≤ <UNK> in the model's apparent and adjusted nagelkerke's <UNK> and (iii) precise estimation of the overall risk or rate in the population (or similarly, precise estimation of the model intercept when predictors are mean centred). the values of n and e (and subsequently epp) that meet all three criteria provide the minimum values required for model development. criteria (i) and (ii) aim to reduce the potential for a developed model to be overfitted to the development data set at hand. overfitting leads to model predictions that are more extreme than they ought to be when applied to new individuals, and most notably occurs when the number of candidate predictors is large relative to the number of outcome events. a consequence is that a developed model's apparent predictive performance (as observed in the development data set itself) will be optimistic, and its performance in new data will usually be lower. therefore, it is good practise to reduce the potential for overfitting when developing a prediction <UNK> which criteria (i) and (ii) aim to achieve. in addition, criterion (iii) aims to ensure that the overall risk (eg, by a key time point for prediction) is estimated precisely, as fundamentally, before tailoring predictions to individuals, a model must be able to reliably predict the overall or mean risk in the target population. the article is structured as follows. section <UNK> introduces our proposed criterion (i), for which key concepts of a global shrinkage factor and the cox-snell <UNK> are <UNK> the latter needs to prespecified to utilise our sample size formula, and so in section <UNK> we suggest how realistic values of the cox-snell <UNK> can be obtained in advance of any data collection, eg, by using published information from an existing model in the same field, including values of the c statistic or alternative <UNK> measures. extension to criteria (ii) and (iii) is then made in section <UNK> section <UNK> then provides two examples, which demonstrate our sample size approach for diagnostic and prognostic models. section <UNK> raises a potential additional criteria to consider: ensuring precise estimates of key predictor effects, to help ensure precise predictions across the entire spectrum of predicted risk. section <UNK> concludes with discussion. <UNK> sample size required to minimise overfitting of predictor effects to adjust for overfitting during model development (and thereby improve the model's predictive performance in new individuals), statistical methods for penalisation of predictor effect estimates are available, where regression coefficients are shrunk toward zero from their usual estimated value (eg, from standard maximum likelihood <UNK> van houwelingen notes that “ … shrinkage works on the average but may fail in the particular unique problem on which the statistician is <UNK> therefore, it is important to minimise the potential for overfitting during model development, and this criterion forms the basis of our first sample size calculation. our approach is motivated by the concept of a global shrinkage factor (a measure of overfitting), and so we begin by introducing this, before then deriving a sample size formula. <UNK> concept of a global shrinkage for logistic and cox regression the concept of shrinkage (penalisation) was outlined in our accompanying <UNK> and is explained in detail <UNK> here, we focus on using a global shrinkage factor (s), sometimes referred to as a uniform shrinkage factor. consider a logistic regression model has been fitted using standard maximum likelihood estimation (ie, traditional and unpenalised estimation). subsequently, s can be estimated (eg, using <UNK> or via a closed-form solution; see section <UNK> and applied to the estimated predictor effects, so that the revised model is 𝑙𝑛 ( pi <UNK> − pi ) = 𝛼∗ + s ( 𝛽̂ <UNK> + 𝛽̂ <UNK> + 𝛽̂ <UNK> +···) . <UNK> here, pi is the outcome probability for the ith individual, the 𝛽̂ terms denote the original predictor effect estimates (ln odds ratios) from maximum likelihood, and 𝛼* is the intercept that has been re-estimated (after shrinkage of predictor effects) to ensure perfect calibration-in-the-large, such that, the overall predicted risk still agrees with the overall observed risk in the development data set (for details on how to do this, we refer to the works of <UNK> and <UNK> ). similarly, after fitting a proportional hazards (cox) regression model using standard maximum likelihood, the model can be revised using hi(t) = <UNK> ∗ exp ( s ( 𝛽̂ <UNK> + 𝛽̂ <UNK> + 𝛽̂ <UNK> +···)) , <UNK> where hi(t) is the hazard rate of the outcome over time (t) for the ith individual and ho(t) * is the baseline hazard function re-estimated (after shrinkage of predictor effects) to ensure the predicted and observed outcome rates agree for the development data set as whole. compared to the original (nonpenalised) models, the revised models <UNK> and <UNK> will shrink predicted probabilities away from zero and one, toward the overall mean outcome probability in the development data set. example of a global shrinkage factor van diepen et al developed a prognostic model for <UNK> mortality risk in patients with diabetes starting <UNK> they use a logistic regression framework, with backwards selection to choose predictors in a dataset of <UNK> patients with <UNK> deaths by <UNK> year, and the estimated model is shown in table <UNK> to examine overfitting, the authors use bootstrapping to estimate a global shrinkage factor of <UNK> indicating that the original model was slightly overfitted to the data. therefore, a revised prediction model was produced by multiplying the original 𝛽̂ coefficients (ln odds ratios) from the original logistic regression model by a global shrinkage factor of s = <UNK> table <UNK> example of global shrinkage applied to a prognostic model for <UNK> mortality risk in patients with diabetes starting <UNK> developed (unpenalised) model final (penalised) model adjusted for overfitting intercept 𝜶 𝜶 ̂ * <UNK> <UNK> predictor 𝜷 ̂ s𝜷 ̂ = <UNK> ̂ age (years) <UNK> <UNK> smoking <UNK> <UNK> macrovascular complications <UNK> <UNK> duration of diabetes mellitus (years) <UNK> <UNK> karnofsky scale <UNK> <UNK> haemoglobin level (g/dl) <UNK> <UNK> albumin level (g/l) <UNK> <UNK> <UNK> expressing sample size in terms of a global shrinkage factor bootstrapping is an excellent way to calculate the shrinkage factor postestimation, but (as it is a resampling method) is not useful for us in advance of data collection. an alternative approach to calculating a global shrinkage factor is to use the closed form “heuristic” shrinkage factor of van houwelingen and le <UNK> defined by svh = <UNK> − p lr , <UNK> where p is the total number of predictor parameters for the full set of candidate predictors (ie, all those considered for inclusion in the model) and lr is the likelihood ratio (chi-squared) statistic for the fitted model defined as lr = <UNK> (ln lnull − ln lmodel) , <UNK> where ln lnull is the log-likelihood of a model with no predictors (eg, intercept-only logistic regression model), and ln lmodel is the log-likelihood of the final model. in our related paper on linear regression, we used the copas shrinkage estimate that is similar to equation <UNK> but with p replaced by p + <UNK> in our experience, svh performs better for generalised linear models than the copas estimate, with svh further from <UNK> and closer to the corresponding estimate obtained from bootstrapping. copas also notes that, unlike for linear regression, a formal justification for replacing p by p + <UNK> in equation <UNK> has not been proved for logistic <UNK> hence, we use equation <UNK> as our shrinkage estimate (ie, our measure of overfitting) for logistic and cox regression models, which now motivates our sample size approach to meet criterion (i). first, let us re-express the right-hand side of equation <UNK> in terms of sample size (n), number of candidate predictor parameters (p), and the cox-snell generalised <UNK> <UNK> the latter is also known as the maximum likelihood <UNK> the likelihood ratio <UNK> or magee's <UNK> <UNK> and it provides a generalisation (eg, to logistic and cox regression models) of the well-known proportion of variance explained for linear regression models. let us use <UNK> cs_app to denote the apparent (“app”) estimate of a prediction model's cox-snell (“cs”) <UNK> performance as obtained from the model development data set. it can be shown (eg, see the works of <UNK> or hendry and <UNK> that the lr statistic can be expressed in terms of the sample size (n) and <UNK> cs_app as follows: lr = −n ln ( <UNK> − <UNK> cs_app) . <UNK> this leads to the cox-snell generalised definition of the apparent <UNK> expressed in terms of the lr value for any regression model, including logistic and cox regression <UNK> cs_app = <UNK> − exp (−lr n ) . <UNK> applying equation <UNK> within equation <UNK> the van houwelingen and le cessie shrinkage factor becomes svh = <UNK> + p n ln ( <UNK> − <UNK> cs_app) . <UNK> <UNK> criterion (i): calculating sample size to ensure a shrinkage factor ≥ <UNK> equation <UNK> provides a closed-form solution for the expected shrinkage conditional on n, p, and <UNK> cs_app. therefore, if we could specify a realistic value for <UNK> cs_app in advance of our study starting, we could identify values of n and p that correspond to a desired shrinkage factor (eg, <UNK> thus informing the required sample size. however, a major problem is that <UNK> cs_app is a postestimation measure of model fit, whereas for a sample size calculation, this needs to be specified in advance of collecting the data when designing a new study. furthermore, due to overfitting in the model development data set, the observed <UNK> cs_app is generally an upwardly biased (optimistic) estimate of the cox-snell <UNK> as it is estimated in the same data used to develop the model. thus, in new data, the actual cox-snell <UNK> peformance is likely to be lower. therefore, we need to re-express svh in terms of <UNK> cs_adj, an adjusted (approximately unbiased) estimate of the model's expected <UNK> cs performance in new individuals from the same population. in other words, <UNK> cs_adj is a modification of <UNK> cs_app to adjust for optimism (caused by overfitting) in the model development data set. for generalised linear models such as logistic regression, mittlboeck and heinzl suggest that <UNK> cs_adj can be obtained <UNK> <UNK> cs_adj = <UNK> cs_app <UNK> as the expected value of this <UNK> cs_adj corresponds to the underlying population <UNK> by rearranging equation <UNK> we can express <UNK> cs_app in terms of <UNK> cs_adj <UNK> cs_app = <UNK> cs_adj svh . <UNK> applying equation <UNK> within equation <UNK> we can now express svh in terms of <UNK> cs_adj, rather than <UNK> cs_app svh = <UNK> + p n ln ( <UNK> − <UNK> cs_adj svh ). <UNK> finally, a simple rearrangement of equation <UNK> leads to a closed-form solution for the required sample size to develop a prediction model conditional on p, svh and <UNK> cs_adj n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ). <UNK> for example, for developing a new logistic regression model based on up to <UNK> candidate predictor parameters with an anticipated <UNK> cs_adj of at least <UNK> then to target an expected shrinkage of <UNK> we need a sample size of n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> and thus <UNK> individuals. <UNK> translating the calculated sample size to the number of events and epp it may be surprising that the overall outcome proportion (or overall outcome rate) is not directly included in the right-hand side of the sample size equation <UNK> especially because the total number of events, e, (which depends on the outcome proportion or rate) is often considered the effective sample size for binary and time-to-event <UNK> however, the outcome proportion (rate) is indirectly accounted for in the sample size calculation via the chosen <UNK> cs_adj, as the maximum value of <UNK> cs_adj for the intended population of the model depends on the overall outcome proportion (rate) for that population. as the outcome proportion decreases, the maximum value of <UNK> cs decreases. this is explained further in section <UNK> therefore, after n is derived from the sample size equation <UNK> e can be obtained by combining the calculated n with the outcome proportion (rate) for the intended population. similarly, epp can be obtained. for example for binary outcomes, e = n𝜙 and epp = n𝜙/p, where 𝜙 is the overall outcome proportion in the target population (ie, the overall prevalence for diagnostic models, or the overall cumulative incidence by a key time point for prognostic models). in our aforementioned hypothetical example, where <UNK> subjects were needed based on an <UNK> cs_adj of <UNK> and svh of <UNK> then if the intended setting has 𝜙 of <UNK> (ie, overall outcome risk is <UNK> the required e = <UNK> × <UNK> = <UNK> with <UNK> predictor parameters, the required epp = <UNK> × <UNK> = <UNK> however, if the intended setting has 𝜙 of <UNK> then e = <UNK> and epp = <UNK> the big change in epp is because, although the chosen value of <UNK> cs_adj is fixed at <UNK> the maximum value of <UNK> cs is much higher for the setting with the higher outcome proportion. we can explain this further using nagelkerke's “proportion of total variance <UNK> which is calculated as <UNK> cs_adj∕ <UNK> cs). if two models have the same <UNK> cs_adj (say at <UNK> as in the aforementioned examples), then nagelkerke's measure of predictive performance will be lower for the model whose setting has a higher outcome proportion, as the <UNK> cs) is larger in that setting. models with lower performance have larger overfitting <UNK> and therefore require larger epp to minimise overfitting than models with high performance. hence, explaining why epp was larger when 𝜙 was <UNK> compared with <UNK> in the aforementioned example. this highlights that a blanket rule of thumb (such as at least <UNK> epp) is unlikely to be sensible to meet criterion (i), as the actual epp depends on the setting/population of interest (which dictates the overall outcome proportion or rate) and expected model performance. <UNK> how to prespecify r𝟐 cs_adj based on previous information our sample size proposal in equation <UNK> requires researchers to provide a value for the model's <UNK> cs_adj, that is, to prespecify the anticipated cox-snell <UNK> value if the model was applied to new individuals. how should this be done? we recommend using <UNK> cs_adj values from previous prediction model studies for the same (or similar) population, considering the same (or similar) outcomes and time points of interest. for example, the researcher could consult systematic reviews of existing models and their performance, which are also increasingly <UNK> or registries that record the prediction models available in a particular <UNK> often, a new prediction model is developed specifically to update or improve upon the performance of an existing model, by using additional predictors. then, the existing model's <UNK> cs_adj could be used as a lower bound for the new model's anticipated <UNK> cs_adj. in this situation, if the apparent cox-snell estimate, <UNK> cs_app, is available in an article describing the development of the existing model, then its <UNK> cs_adj can be derived using equation <UNK> as long as the study's n and p can also be obtained. in addition, as in van diepen et al's example (table <UNK> a global shrinkage factor may be reported directly for an existing model development study, and if so, <UNK> cs_adj can be derived from a simple rearrangement of equation <UNK> again as long as the study's n and p are also available. note that, if <UNK> cs_app is available from an external validation study of an existing model, there is no need for adjustment (ie, <UNK> cs_app = <UNK> cs_adj), as the validation dataset provides a direct estimate of the model's performance in new individuals (free from overfitting concerns as there is no model development therein). other options to obtain <UNK> cs_adj from the existing literature are now described. for guidance on choosing an <UNK> cs_adj value in the absence of any prior information, please see our discussion. <UNK> using the lr statistic to derive the cox-snell r𝟐 adj if the <UNK> cs_app or <UNK> cs_adj is not available in the publication of an existing model, the lr value may be reported, which would allow <UNK> cs_app to be derived using equation <UNK> then svh for the model derived using equation <UNK> (assuming the model's n and p are also provided), and finally <UNK> cs_adj using equation <UNK> sometimes the log-likelihood of the final model (lnlmodel) is reported, but not the lr value itself. in this situation, the researcher should calculate ln lnull based on other information in the article, and then calculate lr using equation <UNK> thus allowing <UNK> cs_app and <UNK> cs_adj to be derived using equations <UNK> and <UNK> respectively. for example, in a logistic regression model, the loglnull value can be calculated using ln lnull = e ln (e n ) + (n − e)ln ( <UNK> − e n ) , <UNK> where e is the total number of outcome events. of course, this assumes e and n are actually available in the article. similarly, for an exponential survival model (equivalent to a poisson model with ln (survival time) as an offset), the ln lnull can be calculated using ln lnull = e ln(𝜆) + 𝜆t = e ln (e t ) + e <UNK> as long as 𝜆 (the constant hazard rate), e (the total number of events), and t (the total time at risk, eg, total person-years) are available in the article. note that, for survival models, packages such as sas and stata usually add a constant to the reported log-likelihood to ensure it remains the same value regardless of the time scale used. for example, stata adds the sum of the ln (survival times) for the noncensored individuals to the reported ln lmodel and ln lnull, and so this constant must be either consistently used or consistently removed in each of ln lmodel and ln lnull when deriving the lr value. <UNK> using other <UNK> statistics to derive r𝟐 cs_adj sometimes other <UNK> statistics are reported for logistic and survival models, rather than the cox-snell version specified in equation <UNK> in particular, because <UNK> cs_app has a maximum value less than <UNK> nagelkerke's <UNK> is sometimes <UNK> which divides <UNK> cs_app by the maximum value defined by <UNK> − exp <UNK> ln lnull n ) , as follows: <UNK> nagelkerke_app = <UNK> cs_app max ( <UNK> cs_app) = <UNK> cs_app <UNK> − exp <UNK> ln lnull n ). <UNK> recall that ln lnull is derivable from other information, eg, using equations <UNK> or <UNK> for logistic and exponential (poisson) models, respectively. when nagelkerke's <UNK> ln lnull, and n are available, the <UNK> cs_app can be calculated by rearranging equation <UNK> to give <UNK> cs_app = <UNK> nagelkerke_app ( <UNK> − exp <UNK> ln lnull n )) , <UNK> and then <UNK> cs_adj calculated via equation <UNK> another measure sometimes reported is mcfadden's <UNK> <UNK> <UNK> mcfadden_app = <UNK> − ln lmodel ln lnull . <UNK> as ln lnull is often obtainable (see previous equation), when <UNK> mcfadden_app is reported, we can rearrange equation <UNK> to obtain ln lmodel, and subsequently derive the lr statistic using equation <UNK> the cox-snell <UNK> cs_app from equation <UNK> svh from equation <UNK> (assuming the model's n and p are also provided), and finally <UNK> cs_adj via equation <UNK> for proportional hazards survival models, o'quigley et al suggested to modify <UNK> cs_app by replacing n with the number of events (e) <UNK> <UNK> óquigley_app = <UNK> − exp (−lr e ) . <UNK> therefore, if <UNK> óquigley_app and e were reported, the lr value could be found using lr = −e ln ( <UNK> − <UNK> óquigley_app) , <UNK> and subsequently, <UNK> cs_app can be obtained using equation <UNK> svh using equation <UNK> and finally <UNK> cs_adj using equation <UNK> another measure increasingly being reported for survival models is royston's measure of explained <UNK> which is given by <UNK> royston_app = <UNK> óquigley_app <UNK> óquigley_app + <UNK> <UNK> ) <UNK> − <UNK> óquigley_app). <UNK> when <UNK> royston_app is reported it can be used to obtain <UNK> óquigley_app by rearranging equation <UNK> as <UNK> óquigley_app = <UNK> <UNK> <UNK> royston_app ( <UNK> − <UNK> <UNK> ) <UNK> royston_app − <UNK> . <UNK> this subsequently allows lr, <UNK> cs_app, svh and then <UNK> cs_adj to be derived as explained previously. a similar measure to <UNK> royston is royston and sauerbrei's <UNK> d, <UNK> which can be derived from their proposed d statistic (the ln(hazard ratio) comparing two groups defined by the median value of the model's risk score in the population of application) <UNK> d_app = 𝜋 <UNK> <UNK> <UNK> <UNK> + 𝜋 <UNK> <UNK> . <UNK> in examples shown by <UNK> <UNK> royston_app and <UNK> d_app are reasonably similar, and thus, we tentatively suggest <UNK> d_app as a proxy for <UNK> royston_app when only <UNK> d_app (or d) is reported; though, we recognise that further research is needed on the link between <UNK> d_app and <UNK> royston. table <UNK> predicted values of the d statistic and <UNK> d from equation <UNK> for selected values of the c statistic (values taken from table <UNK> in the work of jinks et <UNK> c dr𝟐 d cdr𝟐 d <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> using values of the c statistic to derive r𝟐 cs_adj jinks et al also proposed the following equation, based on empirical evidence, for predicting royston's d (and thus subsequently <UNK> d_app) when only the c statistic is reported for a survival <UNK> d = <UNK> − <UNK> + <UNK> − <UNK> <UNK> . <UNK> table <UNK> provides values of d (and corresponding values of <UNK> d_app from equation <UNK> predicted from equation <UNK> for selected values of the c statistic, as taken from the work of jinks et <UNK> thus, if only the c statistic is reported, we can use equation <UNK> to predict royston's d statistic and calculate <UNK> d_app (using equation <UNK> as a proxy to <UNK> royston_app, and then <UNK> óquigley_app, lr, <UNK> cs_app and finally <UNK> cs_adj computed sequentially using the equations given previously. further evaluation of the performance of jinks' formula is required, eg, using simulation and across settings with different cumulative outcome incidences. indeed, based on figure <UNK> in the work of jinks et <UNK> the potential error in the predictions of d appears to increase as c increases, and is about +/− <UNK> when c is <UNK> nevertheless, equation <UNK> serves as a good starting point and works well in our applied example (see section <UNK> further research is also needed to ascertain how to predict <UNK> cs from other measures, such as somer's d statistic. <UNK> the anticipated value of r𝟐 cs_adj may be small it is important to emphasise that the cox-snell, <UNK> cs, values for logistic and survival models are usually much lower than for linear regression models, with values often less than <UNK> a key reason is that (unlike for linear regression) the <UNK> cs_app has a maximum value less than <UNK> defined by max( <UNK> cs_app) = <UNK> − <UNK> ln lnull n ) . <UNK> this is because ln lnull is itself bounded for binary and time-to-event outcomes (see equations <UNK> and <UNK> for example, for a logistic regression model with an outcome proportion of <UNK> using equation <UNK> and an arbitrary sample size of <UNK> we have ln lnull = e ln (e n ) + (n − e)ln ( <UNK> − e n ) = <UNK> ln ( <UNK> <UNK> ) + <UNK> − <UNK> ( <UNK> − <UNK> <UNK> ) = <UNK> and therefore, using equation <UNK> max( <UNK> cs_app) = <UNK> − <UNK> ln lnull n ) = <UNK> − <UNK> <UNK> ) = <UNK> however, for an outcome proportion of <UNK> the <UNK> cs_app) is <UNK> and for an outcome proportion of <UNK> the <UNK> cs_app) is <UNK> therefore, especially in situations where the outcome proportion is low, researchers should anticipate a model with a (seemingly) low <UNK> cs_app value, and subsequently a low <UNK> cs_adj value. low values of <UNK> cs_app or <UNK> cs_adj do not necessarily indicate poor model performance. consider the following three examples. first, poppe et al used a cox regression to develop a model (“predict-cvd”) to predict the risk of future cvd events within two years in patients with atherosclerotic <UNK> and directly report an <UNK> cs_app of <UNK> however, the corresponding c statistic is <UNK> which shows discriminatory magnitude typical of many prognostic models used in practice. second, hippisley-cox and coupland use the qresearch database to produce three models (qdiabetes) that estimates the risk of future diabetes in a general <UNK> in their validation of their “model a,” there were <UNK> <UNK> incident cases of diabetes recorded in <UNK> <UNK> <UNK> women <UNK> cases per <UNK> person-years) during follow-up, and the reported <UNK> royston_app was <UNK> using the approach described previously to convert <UNK> royston to lr, this leads to a <UNK> cs_app of <UNK> however, the corresponding d statistic of <UNK> and c statistic of <UNK> are large. third, in a risk prediction model for venous thromboembolism (vte) in women during the first <UNK> weeks after <UNK> <UNK> cs_app was <UNK> due to the extremely low event risk <UNK> per <UNK> <UNK> deliveries), but the model still had important discriminatory ability as the corresponding c statistic was <UNK> <UNK> additional sample size criteria criterion (i) focuses on shrinkage of predictor effects, which is a multiplicative measure of overfitting (ie, on the relative scale). harrell suggests to also evaluate overfitting on the absolute scale and to check key model parameters are estimated <UNK> we now address this with two further criteria. <UNK> criterion (ii): ensuring a small absolute difference in the apparent and adjusted r𝟐 nagelkerke our second criterion for minimum sample size is to ensure a small absolute difference (𝛿) between the model's apparent and adjusted proportion of variance explained. we suggest using nagelkerke's <UNK> for this purpose as, unlike the cox-snell <UNK> value, it can range between <UNK> and <UNK> and so a small difference (say ≤ <UNK> can be ubiquitously defined. based on equation <UNK> the difference in the apparent and adjusted nagelkerke's <UNK> can be defined as <UNK> nagelkerke_app − <UNK> nagelkerke_adj = <UNK> cs_app max ( <UNK> cs_app) − <UNK> cs_adj max ( <UNK> cs_app) = <UNK> cs_adj svh − <UNK> cs_adj max ( <UNK> cs_app) = <UNK> cs_adj <UNK> − s𝑉 𝐻 ) svh max ( <UNK> cs_app), <UNK> where <UNK> cs_app) = <UNK> − exp <UNK> ln lnull n ) , as shown in equation <UNK> therefore, to meet sample size criterion (ii) and ensure the difference is less than a small value (say, 𝛿), we require <UNK> csadj <UNK> − svh) svh max ( <UNK> csapp) ≤ 𝛿. <UNK> we generally recommend 𝛿 is ≤ <UNK> such that the optimism is nagelkerke's percentage of variation explained is ≤ <UNK> rearranging equation <UNK> we find that <UNK> − svh) svh ≤ 𝛿 max ( <UNK> csapp) <UNK> csadj , and therefore, svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp). <UNK> equation <UNK> allows the researcher to calculate the required svh to satisfy criterion (ii), conditional on prespecifying the model's anticipated <UNK> cs_adj (as they did for criterion (i)) and also the value of <UNK> csapp ) as outlined for equation <UNK> then, sample size equation <UNK> can be used to derive the sample size needed to satisfy criterion (ii). this is only necessary when the calculated value of svh from equation <UNK> is larger than that chosen for criterion (i), as then the sample size required to meet criterion (ii) will be larger than that for criterion (i). for example, consider the development of a logistic regression model with anticipated <UNK> cs_adj of at least <UNK> and in a setting with the outcome proportion of <UNK> such that the <UNK> cs_app) is <UNK> then, to ensure 𝛿 is ≤ <UNK> we require svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp) = <UNK> <UNK> + <UNK> × <UNK> = <UNK> therefore, svh must be at least <UNK> to meet criterion (ii). as this is lower than the recommended value of at least <UNK> to meet criterion (i), no further work is required. however, had the anticipated <UNK> cs_adj been <UNK> then svh ≥ <UNK> <UNK> + <UNK> × <UNK> = <UNK> as this is higher than <UNK> we would need to reapply sample size equation <UNK> using <UNK> rather than <UNK> to obtain a sample size that meets both criteria (i) and (ii). <UNK> criterion (iii): ensure precise estimate of overall risk (model intercept) for logistic and time-to-event models, it is fundamental that the available sample size can precisely estimate the overall risk in the population by key time-points of interest. one way to examine this is to calculate the margin of error in outcome proportion estimates (𝜙̂) for a null model (ie, no predictors included). for example, for a binary outcome, an approximate <UNK> confidence interval for the overall outcome proportion is 𝜙̂ ± <UNK> <UNK> − 𝜙̂) n . therefore, the absolute margin of error (𝛿) is <UNK> n , which leads to n = <UNK> 𝛿 <UNK> <UNK> − 𝜙̂) . <UNK> this is largest when the outcome proportion is <UNK> we require <UNK> individuals to ensure a margin of error ≤ <UNK> when the true value is <UNK> however, we recommend a more stringent margin of error ≤ <UNK> which, when the outcome proportion is <UNK> requires n = <UNK> <UNK> <UNK> <UNK> − <UNK> = <UNK> and thus, <UNK> participants (and hence, about <UNK> events) are required. if the outcome proportion is <UNK> then we require <UNK> subjects to ensure a margin of error ≤ <UNK> whilst an outcome proportion of <UNK> requires <UNK> subjects. these sample sizes aim to ensure precise estimation of the overall risk in the population of interest. strictly speaking, we are more interested in precise estimation of the mean risk in an actual model including multiple predictors. if we centre predictors at their mean value, then the model's intercept is the logit risk for an individual with mean predictor values. the corresponding risk for this individual will often be very similar (though not identical) to the mean risk in the overall population. furthermore, the variance of the estimated risk for this individual will be approximately <UNK> n .* *as obtained by inversing the information matrix <UNK> and replacing individual variances defined by <UNK> with a constant variance defined by <UNK> − 𝜙̂). thus, it follows that equation <UNK> is also a good approximation to the sample size required to precisely estimate the mean risk in a model containing predictors centred at their mean. for time-to-event data, we could consider the precision of the estimated cumulative incidence (outcome risk) at a key time point of interest. a simple (and therefore practical) approach is to assume an exponential survival model, for which the estimated cumulative incidence function is f(t) = <UNK> − exp(−𝜆̂ t), where 𝜆̂ is the estimated rate (number of events per person-year). an approximate <UNK> confidence interval for the estimated f(t) is <UNK> ( − ( 𝜆̂ ± <UNK> t ) t ) , where t is the total person-years of follow-up. therefore, to ensure a small absolute margin of error, such that the lower and upper bounds of the confidence interval are ≤ 𝛿 (eg, <UNK> of the true value, we must ensure both the following are satisfied: − exp ( − ( 𝜆̂ + <UNK> 𝜆̂ t ) t ) + exp(−𝜆̂ t) ≤ 𝛿 − exp(−𝜆̂ t) + exp ( − ( 𝜆̂ − <UNK> 𝜆̂ t ) t ) ≤ 𝛿. <UNK> for example, for a constant event rate of <UNK> <UNK> events per <UNK> person-years), then by <UNK> years, the outcome risk is <UNK> = <UNK> − exp <UNK> × <UNK> = <UNK> then, <UNK> person-years of follow-up (and thus <UNK> × <UNK> ≈ <UNK> events) are needed to provide a confidence interval, which has a maximum absolute error of <UNK> from the true value. that is, <UNK> − exp ( − ( 𝜆̂ ± <UNK> 𝜆̂ t ) t ) = <UNK> − exp ( − ( <UNK> ± <UNK> <UNK> <UNK> <UNK> = <UNK> to <UNK> thus, equation <UNK> is satisfied, as both the lower and upper bounds are ≤ <UNK> of the true value of <UNK> more generally, to avoid assuming simple survival distributions like the exponential, harrell suggests using the dvoretzky-kiefer-wolfowitz inequality to estimate the probability of a chosen margin of error anywhere in the estimated cumulative incidence <UNK> <UNK> worked examples to summarise our sample size approach for researchers, we provide a step-by-step guide in figure <UNK> the sample size (and corresponding number of events and epp) that meets criteria (i) to (iii) provides the minimum sample size required for model development. we now present two worked examples to illustrate our approach. <UNK> a diagnostic prediction model for chronic chagas disease our first example considers the minimum sample size required for developing a diagnostic model for predicting a binary outcome (disease: yes or no). brasil et al developed a logistic regression model containing <UNK> predictor parameters for predicting the risk of having chronic chagas disease in patients with suspected chagas <UNK> upon external validation in a cohort of <UNK> participants containing <UNK> with chagas disease, the model had an estimated c statistic of <UNK> and an <UNK> nagelkerke_app of <UNK> consider that a researcher wants to update this model and improve the predictive performance. our sample size approach can be applied as follows. <UNK> steps <UNK> and <UNK> identifying values for p, r𝟐 cs_adj, and max(r𝟐 cs_app) assume that the researcher has identified (eg, based on recent studies) <UNK> additional predictor parameters that they wish to add to the original model. thus, in total, the number of predictor parameters, p, is <UNK> the next step is to identify a sensible value for the anticipated cox-snell <UNK> adj. to achieve this, we can convert the <UNK> nagelkerke_app value for brasil's existing model into a <UNK> cs_app value. assume the disease prevalence is <UNK> as in the brasil validation study, and use equation <UNK> to calculate the log-likelihood for the null model in brasil's validation study ln lnull = e ln (e n ) + (n − e)ln ( <UNK> − e n ) = <UNK> ln ( <UNK> <UNK> ) + <UNK> − <UNK> ( <UNK> − <UNK> <UNK> ) = <UNK> figure <UNK> summary of the steps involved in calculating the minimum sample size required for developing a multivariable prediction model for binary or time-to-event outcomes hence, the <UNK> csapp ) = <UNK> − exp <UNK> ln lnull n ) = <UNK> − exp <UNK> <UNK> ) = <UNK> now, we can use equation <UNK> to obtain <UNK> cs_app = <UNK> nagelkerke_app ( max ( <UNK> csapp)) = <UNK> × <UNK> = <UNK> this apparent cox-snell value of <UNK> can be directly used as an estimate of the model's <UNK> cs_adj, as it was obtained in a different data set to that used for model development. therefore no adjustment is needed, because <UNK> cs_app= <UNK> cs_adj here. <UNK> step <UNK> criterion (i) - ensuring a global shrinkage factor of <UNK> let us assume <UNK> is a lower bound for the <UNK> cs_adj of our new model. we now use equation <UNK> to estimate the sample size required to ensure an expected shrinkage factor (svh = <UNK> conditional on a number of predictor parameters (p= <UNK> n = p (svh − <UNK> ( <UNK> − <UNK> csadj svh ) = <UNK> <UNK> − <UNK> ( <UNK> − <UNK> <UNK> ) = <UNK> thus, <UNK> participants are required to meet criterion (i). <UNK> step <UNK> criterion (ii) - ensuring a small absolute difference in the apparent and adjusted r𝟐 nagelkerke to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of <UNK> or less in the apparent and adjusted <UNK> nagelkerke. using equation <UNK> we obtain svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp) = <UNK> <UNK> + <UNK> × <UNK> = <UNK> this is more stringent than the <UNK> assumed for criterion (i). therefore, we need to reapply equation <UNK> to estimate the sample size required conditional on svh = <UNK> (rather than <UNK> n = p (svh − <UNK> ( <UNK> − <UNK> csadj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> therefore, <UNK> subjects are required to meet criterion (ii), exceeding the <UNK> subjects required for criterion (i). <UNK> step <UNK> criterion (iii) - ensure precise estimate of overall risk (model intercept) assuming the prevalence of chagas disease is <UNK> (as observed from the brasil validation study), then to ensure we estimate this with a margin of error ≤ <UNK> we require (using equation <UNK> n = <UNK> <UNK> <UNK> <UNK> <UNK> − <UNK> = <UNK> and thus <UNK> subjects. this is far fewer than the sample size required to meet criteria (i) and (ii). <UNK> step <UNK> minimum sample size that ensures all criteria are met the largest sample size required was <UNK> subjects to meet criterion (ii), and so this provides the minimum sample size required for developing our new model. it corresponds to <UNK> × <UNK> = <UNK> events, and an epp of <UNK> = <UNK> which is considerably lower than the “epp of at least <UNK> rule of thumb. <UNK> a prognostic model to predict a recurrence of vte our second example considers the sample size required to develop a prognostic model with a time-to-event outcome. ensor et al developed a prognostic time-to-event model for the risk of a recurrent vte following cessation of therapy for a first <UNK> the sample size was <UNK> participants, with a median follow-up of <UNK> months, a total of <UNK> person-years of follow-up, and <UNK> <UNK> of) individuals had a vte recurrence by end of <UNK> the model included predictors of age, gender, site of first clot, d-dimer level, and the lag time from cessation of therapy until measurement of d-dimer (often around <UNK> days). these predictors corresponded to six parameters in the model, which was developed using the flexible parametric survival modelling framework of royston and <UNK> and royston and <UNK> although ensor's model performed well on average, the model's predicted risks did not calibrate well with the observed risks in some <UNK> therefore, new research is needed to update and extend this model, eg, by including additional predictors. we now identify suitable sample sizes to inform such research. <UNK> steps <UNK> and <UNK> identifying values for p, r𝟐 cs_adj and max(r𝟐 cs_app) assume that there are <UNK> potential predictor parameters for inclusion in the new model, and thus, p = <UNK> we next need to identify suitable values for <UNK> cs_adj and <UNK> cs_app). calculating max(r𝟐 cs_app) for the ensor model, <UNK> cs_app was not reported but we should expect it to be quite small because the maximum value of <UNK> cs_app is low. for example, assuming (for simplicity) an exponential survival model was fitted to the ensor data, then using equation <UNK> we have ln lnull = e ln (e t ) + e = <UNK> <UNK> + <UNK> = <UNK> and therefore, using equation <UNK> max ( <UNK> cs_app) = <UNK> − exp <UNK> ln lnull n ) = <UNK> − exp <UNK> × <UNK> <UNK> ) = <UNK> thus, <UNK> cs_app) is considerably less than <UNK> obtaining a sensible value for r𝟐 cs_adj from the study authors as <UNK> cs_app was not reported for the ensor model, we need to obtain it. we contacted the original authors who told us their model's <UNK> cs_app was <UNK> in the development data set. thus, let us use this value to derive <UNK> adj from equation <UNK> based on ensor's sample size of <UNK> and six predictor parameters, we obtain <UNK> cs_adj = <UNK> cs_app = ⎛ ⎜ ⎜ ⎜ ⎝ <UNK> + p n ln ( <UNK> − <UNK> cs_app) ⎞ ⎟ ⎟ ⎟ ⎠ <UNK> cs_app = ( <UNK> + <UNK> <UNK> ln <UNK> − <UNK> ) <UNK> = <UNK> hence, when developing a new model in this field, we could assume <UNK> is a lower bound for the expected <UNK> cs_adj of the new model. this corresponds to nagelkerke's proportion variation explained of <UNK> <UNK> cs_app) ≈ <UNK> = <UNK> (or <UNK> calculating a sensible value for r𝟐 cs_adj from other reported information for illustration, we also consider how <UNK> cs_app could have been estimated indirectly from other available information. the model's reported c statistic was <UNK> and so we can use equation <UNK> to predict the corresponding d statistic d = <UNK> (c − <UNK> + <UNK> − <UNK> <UNK> = <UNK> <UNK> − <UNK> + <UNK> − <UNK> <UNK> = <UNK> the corresponding <UNK> d_app can be derived from equation <UNK> <UNK> d_app = 𝜋 <UNK> <UNK> <UNK> <UNK> + 𝜋 <UNK> <UNK> = 𝜋 <UNK> <UNK> <UNK> <UNK> + 𝜋 <UNK> <UNK> = <UNK> taking <UNK> d_app as a proxy for <UNK> royston_app, we can then use equation <UNK> to obtain <UNK> óquigley_app = <UNK> <UNK> <UNK> royston_app ( <UNK> − <UNK> <UNK> ) <UNK> royston_app − <UNK> = <UNK> <UNK> <UNK> ( <UNK> − <UNK> <UNK> ) <UNK> − <UNK> = <UNK> next, we can use <UNK> óquigley_app and the number of reported events (e = <UNK> to derive the lr statistic from equation <UNK> lr = −e ln ( <UNK> − <UNK> ó quigleyapp) = <UNK> ln <UNK> − <UNK> = <UNK> using equation <UNK> this corresponds to <UNK> cs_app = <UNK> − exp (−lr n ) = <UNK> − exp <UNK> <UNK> ) = <UNK> thus, based on using the reported c statistic, an indirect estimate of the <UNK> cs_app is <UNK> for the ensor model. this is reassuringly close to the estimate of <UNK> provided directly by the study authors. <UNK> step <UNK> criterion (i) - ensuring a global shrinkage factor of <UNK> equation <UNK> can now be applied to derive the required sample size to meet criterion (i). using an <UNK> cs_adj of <UNK> for a model with <UNK> predictor parameters and a targeted expected shrinkage of <UNK> the sample size required is n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> and thus <UNK> participants. <UNK> step <UNK> criterion (ii) - ensuring a small absolute difference in the apparent and adjusted r𝟐 nagelkerke to meet criterion (ii), we first need to calculate the shrinkage factor required to ensure a small difference of <UNK> or less in the apparent and adjusted <UNK> nagelkerke. recall, assuming an exponential model for simplicity, we calculated that the <UNK> csapp ) = <UNK> then, using equation <UNK> we obtain svh ≥ <UNK> csadj <UNK> csadj + 𝛿 max ( <UNK> csapp) = <UNK> <UNK> + <UNK> × <UNK> = <UNK> this is less stringent than the <UNK> assumed for criterion (i), and so no further sample size calculation is required to meet criterion (ii). <UNK> step <UNK> criterion (iii) - ensure precise estimate of overall risk assuming a simple exponential model, we can check the width of the confidence interval for the overall risk at a particular time point based on the sample size identified, using the approach outlined in section <UNK> ensor et <UNK> reported an overall vte recurrence rate of <UNK> = <UNK> with an average follow-up of <UNK> years. therefore, assuming 𝜆 is <UNK> in our new study, and that a predicted risk at <UNK> years is of key interest, an exponential survival model would give the cumulative incidence of <UNK> exp <UNK> × <UNK> based on the calculated sample size of <UNK> participants from criterion (i), and thus an estimated <UNK> = <UNK> person-years of follow-up, the <UNK> confidence interval would be <UNK> − exp ( − ( 𝜆̂ ± <UNK> 𝜆̂ t ) t ) = <UNK> − exp ( − ( <UNK> ± <UNK> <UNK> ) <UNK> ) = <UNK> to <UNK> this is reassuringly narrow, and satisfies equation <UNK> as both the lower and upper bounds are well within an error of <UNK> of the true value of <UNK> <UNK> step <UNK> minimum sample size that ensures all criteria are met the largest sample size required was <UNK> participants to meet criterion (i), which therefore provides the minimum sample size required for developing our new model. this assumes the new cohort will have a similar follow-up, censoring rate, and event rate to that reported by ensor et al, where the mean follow-up per person was <UNK> years, <UNK> of individuals had a vte recurrence by end of follow-up, and the event rate was <UNK> then, the required <UNK> participants corresponds to about <UNK> × <UNK> = <UNK> person-years of follow-up, and <UNK> × <UNK> ≈ <UNK> outcome events, and thus an epp of <UNK> ≈ <UNK> this is over twice the “epp of at least <UNK> rule of thumb. figure <UNK> shows that an epp of <UNK> only ensures a shrinkage factor of <UNK> which would reflect relatively large overfitting. <UNK> what if the sample size is not achievable? if a researcher was restricted in their total sample size, for example, by the time and cost of a new cohort study, then a sample size of <UNK> may not be practical. in this situation, we do not recommend reducing sample size by decreasing sc below <UNK> (as this would reflect larger overfitting) or by assuming a larger <UNK> cs_adj value (as this is anticonservative for criterion (i)). rather, to ensure an svh of <UNK> (ie, an expected shrinkage of <UNK> the researcher should lower p by reducing the number of candidate predictors. for example, predictors could be prioritised based on previous evidence (eg, systematic reviews). after data collection, unsupervised learning techniques such as principal component analysis may be useful, which are blinded to the outcome data. figure <UNK> shows how changing p changes the required sample size to meet criterion (i). for example, if a researcher was restricted to a sample size of about <UNK> participants, then they would need to reduce p to <UNK> to ensure an expected shrinkage of <UNK> this is because, for an svh of <UNK> and <UNK> cs_adj of <UNK> the sample size required is n = p (svh − <UNK> ln ( <UNK> − <UNK> cs_adj svh ) = <UNK> <UNK> − <UNK> ln ( <UNK> − <UNK> <UNK> ) = <UNK> figure <UNK> events per predictor parameter required to achieve various expected shrinkage (svh) values for a new prediction model of venous thromboembolism recurrence risk with an assumed <UNK> cs_adj of <UNK> [colour figure can be viewed at wileyonlinelibrary.com] figure <UNK> sample size required (based on equation <UNK> for a particular number of predictor parameters (p) to achieve a particular value of expected shrinkage (svh), for a new prediction model of venous thromboembolism recurrence risk with an assumed <UNK> cs_adj of <UNK> [colour figure can be viewed at wileyonlinelibrary.com] and so now close to <UNK> figure <UNK> also shows how larger values of svh require larger sample sizes; in particular, the increase in sample size required is substantial when moving from svh of <UNK> to <UNK> values of svh < <UNK> lead to lower sample sizes, but come at the cost of larger expected overfitting, and so are not recommended. therefore, targeting a value of svh of <UNK> would seem a pragmatic choice. <UNK> potential additional criterion: precise estimates of predictor effects ideally, predictions should also be precise across the entire spectrum of predicted values, not just at the mean. this is challenging to achieve, but is helped by ensuring the sample size will give precise estimates of the effects of key <UNK> hence, this may form a further criterion for researchers to check (ie, in addition to criteria (i) to (iii)). briefly, for a particular predictor of a binary or time-to-event outcome, the sample size required to precisely estimate its association with the outcome (ie, an odds ratio or hazard ratio) depends on the assumed magnitude of this effect, the variability of the predictor's values across subjects, the predictor's correlation with other predictors in the model, and the overall outcome proportion in the <UNK> ideally, we want to ensure a sample size that gives a precise confidence interval around the predictor's effect <UNK> however, this is taxing, as closed-form solutions for the variance of adjusted log odds ratio or hazard ratios, from logistic and cox regression, respectively, are nontrivial. one solution is to use simulation-based <UNK> however, perhaps a more practical option is to utilise readily available power-based sample size calculations that calculate the sample size required to detect (based on statistical significance) a predictor's effect for a chosen type i error level (eg, <UNK> and <UNK> as such sample size calculations are likely to be less stringent than those based on confidence interval width (especially for predictors with large effect sizes), we might use a high power, say of <UNK> in the calculation. checking sample size for predictor effects will be laborious with many predictors, and so it may be practical to focus on the subset of key predictors with smallest variance of their values, as these predictors will have the least precision. in particular, when there are important categorical predictors but with few subjects and/or outcome events in some categories, substantially larger sample sizes may be needed to avoid separation issues (ie, no event or nonevents in some <UNK> in addition, any predictors whose effect is small (and thus harder to detect), but still important, may warrant special attention. for example, returning to the vte prediction model from section <UNK> a key predictor in the original model by ensor et al was <UNK> with an adjusted log hazard ratio of <UNK> although this is close to zero, as age is on a continuous scale, the impact of age on outcome risk is potentially large; for example, it corresponds to an adjusted hazard ratio of <UNK> comparing two individuals aged <UNK> years apart. based on the results presented by ensor et <UNK> the standard deviation of age was <UNK> and the overall outcome occurrence by end of follow-up was <UNK> based on these values, and assuming other included predictors explain <UNK> of the variation in age, then the sample size approach of hsieh and <UNK> suggests <UNK> subjects are required to have <UNK> power to detect a prognostic effect for age. this is larger than the <UNK> subjects required to meet criterion (i), and so, to be extra stringent beyond criteria (i) to (iii), the researcher might raise the recommended sample size to <UNK> subjects, if possible. <UNK> discussion sample size calculations for prediction models of binary and time-to-event outcomes are typically based on blanket rules of thumb, such as at least <UNK> epp, which generates much debate and <UNK> in this article, building on our related work for linear <UNK> we have proposed an alternative approach that identifies the sample size, events and epp required to meet three key criteria, which minimise overfitting whilst ensuring precise estimates of overall outcome risk. criterion (i) aims to ensure the optimism of predictor effect estimates is small, as defined by a global shrinkage factor of ≥ <UNK> this idea extends the work of harrell who suggests that, after a model is developed, if the shrinkage estimate “falls below <UNK> for example, we may be concerned with the lack of calibration the model may experience on new <UNK> our premise is the same, except we focused on calculating the expected shrinkage before data collection, to inform sample size calculations for a new study. criterion (ii) extends this idea to ensure the optimism is small on the <UNK> nagelkerke scale, such that there is a difference of ≤ <UNK> in the apparent and adjusted percentage of variation explained by the model. lastly, criterion (iii) ensures the sample size will precisely estimate the overall outcome risk, which is fundamental. by utilising the model's anticipated cox-snell <UNK> the sample size calculations are essentially tailored to the model and setting at hand, because the cox-snell <UNK> reflects many factors including the outcome proportion (ie, outcome prevalence or cumulative incidence) and the overall fit (performance) of the model. it therefore better reflects the trait of a particular model and setting at hand rather than a blanket epp <UNK> in our examples, the sample sizes required often differed considerably from an epp of <UNK> reinforcing the idea that this rule is too <UNK> indeed, the required epp was much higher <UNK> in our second example than our first <UNK> illustrating the problem with a blanket epp rule trying to cover all <UNK> section <UNK> also showed how to obtain a realistic value for cox-snell <UNK> based on previous models to make our proposal more achievable in practice. if no previous prediction model exists for the outcome and setting of interest, then information might be used from studies in a related setting or using a different but similar outcome definition or time points to those intended for the new model. information can also be borrowed from predictor finding studies (eg, studies aiming to estimate the prognostic effect of a particular predictor adjusted for other <UNK> typically, these studies apply multivariable modelling, and although mainly focused on predictor effect estimates, they often report the c statistic and <UNK> values. further research is needed to help researchers when there are no existing studies or information to identify a sensible value of the expected cox-snell <UNK> medical diagnosis and prediction of health-related outcomes are, generally speaking, low signal-to-noise ratio situations. it is not uncommon in these situations to see <UNK> nagelkerke values in the <UNK> to <UNK> range. therefore, in the absence of any other information, we suggest that sample sizes be derived assuming the value of <UNK> cs_adj corresponds to an <UNK> nagelkerke of <UNK> (ie, <UNK> csadj <UNK> csadj) = <UNK> an exception is when predictors include “direct” (mechanistic) measurements, such as including the baseline version of the binary or ordinal outcome (eg, including smoking status at baseline when predicting smoking status at <UNK> year), or direct measures of the processes involved (eg, including physiologic function of patients in intensive care when predicting risk of death within <UNK> hours). then, in this special situation, an <UNK> nagelkerke = <UNK> may be a more appropriate default choice. the rule of having an epp of at least <UNK> stems from limited simulation studies examining the bias and precision of predictor effects in the prediction <UNK> jinks et <UNK> alternatively developed sample size formulae for a time-to-event prediction model based on the d <UNK> they suggest to predefine the d statistic that would be expected, and then, based on a desired significance or confidence interval width, their formulae provide the number of events required to achieve this. however, their method does not account for the number of candidate predictors and does not consider the potential for overfitting when developing a model. our sample size calculations address this, and are meant to be used before any data collection. in situations where a development data set is already available, containing a specific number of participants and predictors, our criteria could be used to identify whether a reduction in the number of predictors is needed before starting model development. indeed, harrell already illustrated this concept by using the shrinkage estimate from the full model (including all predictors) to gauge whether the number of predictors should be reduced via data reduction <UNK> ideally, this should be done blind to the estimated predictor effects (ie, just calculate the shrinkage factor for the full model, but do not observe the predictor effect estimates and associated p-values), as otherwise decisions about predictor inclusion are influenced by a “quick look” at the effect estimates from the full model results. similarly, when planning to use a predictor selection method (such as backwards selection) during model development, researchers should define p as the total number of parameters due to all predictors considered (screened), and not just the subset that are included in the final <UNK> as harrell <UNK> the value of p should be honest. section <UNK> also highlighted the potential additional requirement to ensure precise estimates of key predictor effects. in particular, special attention may be given to those predictors with strong predictive value (and thus most influential to the predicted outcome risk), especially if the variance in their values is small, or when events or nonevents in some categories of the predictor are rare, as this leads to larger sample sizes. for example, van smeden et al highlighted that “separation” between events and nonevents is an important consideration toward the required sample size, which occurs when a single predictor (or a linear combination of multiple predictors) perfectly separates all events from all nonevents, and thus causes estimation <UNK> this may lead to substantially larger epp to resolve the issue (eg, so that all categories of a predictor have both events and nonevents). for such reasons, we labelled our criteria (i) to (iii) proposal as the “minimum” sample size required. further research should identify how our sample size criteria relates to that of the work of van smeden et al, who focused on sample size in regards to the mean squared error in predictions from the <UNK> specifically, they use simulation to evaluate the characteristics that influence the mean squared prediction error of a logistic model, and identify that the outcome proportion and number of predictors are <UNK> in addition to total sample size. this leads to a sample size equation to minimise root mean-squared prediction error in a new model development study. harrell also suggested using simulation to inform sample size, and illustrates this for a logistic regression model with a single <UNK> for example, one could simulate a very large dataset from an assumed prediction model, and quantify the mean square (prediction) error and mean absolute (prediction) error of a model developed from this data set. then, repeat this process each time removing an individual at random, until a sample size is identified below which the mean squared (prediction) error is unacceptable. in summary, we have proposed criteria for identifying the minimum sample size required when developing a prediction model for binary or time-to-event outcomes. we hope this, and our related <UNK> encourages researchers to move away from rules of thumb, and to rather focus on attaining sample sizes that minimise overfitting and ensure precise estimates of overall risk within the model and setting of interest. we are currently writing software modules to implement the approach.

<|EndOfText|>

individual participant data meta-analysis to examine interactions between treatment effect and participant-level covariates: statistical recommendations for conduct and planning precision medicine research often searches for treatment-covariate interactions, which refers to when a treatment effect (eg, measured as a mean difference, odds ratio, hazard ratio) changes across values of a participant-level covariate (eg, age, gender, biomarker). single trials do not usually have sufficient power to detect genuine treatment-covariate interactions, which motivate the sharing of individual participant data (ipd) from multiple trials for meta-analysis. here, we provide statistical recommendations for conducting and planning an ipd meta-analysis of randomized trials to examine treatment-covariate interactions. for conduct, two-stage and one-stage statistical models are described, and we recommend: (i) interactions should be estimated directly, and not by calculating differences in meta-analysis results for subgroups; (ii) interaction estimates should be based solely on within-study information; (iii) continuous covariates and outcomes should be analyzed on their continuous scale; (iv) nonlinear relationships should be examined for continuous covariates, using a multivariate meta-analysis of the trend (eg, using restricted cubic spline functions); and (v) translation of interactions into clinical practice is nontrivial, requiring individualized treatment effect prediction. for planning, we describe first why the decision to initiate an ipd meta-analysis project should not be based on between-study heterogeneity in the overall treatment effect; and second, how to calculate the power of a potential ipd meta-analysis project in advance of ipd collection, conditional on characteristics (eg, number of participants, standard deviation of covariates) of the trials (potentially) promising their ipd. real ipd meta-analysis projects are used for illustration throughout. keywords effect modifier, individual participant data (ipd), meta-analysis, subgroup effect, treatment-covariate interaction this is an open access article under the terms of the creative commons attribution license, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. <UNK> introduction precision medicine and stratified healthcare tailors treatment decisions to individuals based on their particular <UNK> the goal is to optimize treatment decisions and reduce unnecessary costs for each individual, by selecting treatments most likely to benefit (or least likely to harm) them based on their participant-level covariate values. for example, trastuzumab is only given to the subgroup of breast cancer patients who are human epidermal growth factor receptor <UNK> <UNK> positive, as it is known to lock on to the <UNK> protein, block the receptor, and stop the cells from dividing and <UNK> it is therefore considered unnecessary for those who are <UNK> negative. in this situation, there is a so-called treatment-covariate interaction, such that the treatment effect (measured on a scale such as a mean difference, risk ratio, odds ratio, or hazard ratio) changes according to an individual's <UNK> status. participant-level covariates that interact with treatment effect are also known as effect modifiers, moderators, predictors of treatment effect and, mainly in the cancer literature, predictive markers. though some treatment-covariate interactions, such as <UNK> are suspected in advance due to strong biological rationale, others are only identified following secondary investigations of existing data. single randomized trials are typically powered on the overall treatment effect (ie, the treatment effect averaged across all individuals), and so do not usually have sufficient power to detect differences in treatment effect between individuals. powering a single trial to detect a genuine treatment-covariate interaction will typically require at least four times the sample size needed to test the overall treatment <UNK> thus the funding of such trials is expensive and often infeasible. when individual participant data (ipd) from multiple randomized trials are available, meta-analysis provides the opportunity to increase power to detect true treatment-covariate <UNK> for this reason, many ipd meta-analyses of randomized trials are initiated specifically to examine one or more potential treatment-covariate interactions, often with the aim to reduce or explain observed between-study heterogeneity in a previous meta-analysis of (published) aggregate data. for pharmaceutical companies, a driver may be to rescue a treatment that previously showed no overall benefit on all patients combined, but which may still benefit a select group of individuals. over the last decade we have been involved in a number of ipd meta-analysis projects aiming to examine treatment-covariate interactions at the participant-level, and learnt important lessons and pitfalls from a statistical point of view. in this tutorial article, we share our experience to help other researchers in this situation, building on previous work in both ipd meta-analysis and single study <UNK> our main goal is to provide statistical recommendations for the meta-analysis part of the project; in particular, how to estimate treatment-covariate interactions using either a one-stage or two-stage ipd meta-analysis framework. a further aim is to highlight two statistical topics that are relevant to consider before embarking on an ipd meta-analysis project to examine treatment-covariate interactions. the outline of the article is as follows. section <UNK> describes two motivating examples used for illustration throughout the whole article. section <UNK> describes a framework for two-stage and one-stage ipd meta-analysis models for estimating treatment-covariate interactions, and section <UNK> highlights five recommendations in this context: (i) calculating differences in meta-analysis results for subgroups is misleading; (ii) interaction estimates should be based solely on within-study information; (iii) continuous covariates (and outcomes) should be analyzed on their continuous scale; (iv) nonlinear relationships should be examined for continuous covariates, using a multivariate meta-analysis of the trend (eg, using restricted cubic spline functions); and (v) translation of interactions into clinical practice requires individualized treatment effect prediction. section <UNK> then describes two statistical issues in guiding decisions to initiate an ipd meta-analysis project to examine interactions. first, how the decision to initiate such ipd meta-analysis projects should not be based on the amount of between-study heterogeneity in the overall treatment effect. second, how to calculate, in advance of ipd collection, the power of a potential ipd meta-analysis conditional on the characteristics of those trials promising their ipd. section <UNK> concludes with discussion. <UNK> motivating examples we now introduce two examples, which will be used to illustrate some of the key issues in later sections. <UNK> an ipd meta-analysis examining the effect of antihypertensive treatment wang et <UNK> performed an ipd meta-analysis of trials in participants with hypertension to investigate to what extent lowering of systolic blood pressure (sbp) contributed to the prevention of cardiovascular events. they selected randomized trials that tested active antihypertensive drugs against control. ipd was sought from trials in the individual data analysis of antihypertensive intervention trials (indana) dataset or at the studies coordinating centre in leuven (belgium). ten trials (with a parallel group design) were ultimately included, and these provided ipd for a total of <UNK> <UNK> patients. key outcomes of interest by end of follow-up were sbp, cardiovascular disease (cvd), and all-cause mortality. an important focus is on whether the effect of antihypertensive treatment on these outcomes is modified by (ie, interacts with) participant-level covariates such as gender and baseline blood pressure. <UNK> an ipd meta-analysis examining the effect of interventions to reduce gestational weight gain in pregnancy the international weight management in pregnancy (i-wip) collaborative group obtained ipd from <UNK> trials <UNK> <UNK> women), to investigate whether diet and lifestyle interventions improve outcomes during pregnancy. a primary outcome was whether interventions reduced gestational weight gain, for which <UNK> studies and a total of <UNK> women were available. weight was recorded as a continuous outcome (in kg), available at baseline (ie, confirmation of pregnancy) and follow-up (ie, last available weight recorded before delivery). other outcomes include pre-eclampsia and stillbirth. a key aim was to examine if the intervention effect on these maternal and fetal outcomes is modified by (ie, interacts with) participant-level covariates such as the mother's age and body mass index (bmi) at baseline. <UNK> two- stage and one- stage ipd meta-analysis models for estimating treatment-covariate interactions in this section, we introduce the framework of two-stage and one-stage models for conducting an ipd meta-analysis to examine treatment-covariate interactions based on within-study information. <UNK> the two-stage approach a two-stage ipd meta-analysis for summarizing treatment-covariate interactions is a straightforward application of a traditional meta-analysis framework. in the first stage the treatment-covariate interactions are estimated using the ipd from each trial separately; then in the second stage these interaction estimates are pooled using a traditional (eg, inverse-variance weighted) meta-analysis model. consider an ipd meta-analysis of multiple randomized trials, each comparing the effect of a particular treatment relative to a control using a simple parallel group design. let i denote trial (i = <UNK> to s), ni denote the number of participants in the ith trial, j denote participant (j = <UNK> to ni), and xij denote allocation to either the treatment (xij <UNK> or control (xij <UNK> group for the jth participant in the ith trial. let zij be a key participant-level covariate of interest (eg, the sex of participant j in trial i), observed for all participants in each trial. then, the two-stage approach can be detailed as follows. <UNK> first stage the analysis to apply in the first stage depends on the outcome type. for example, assume the aim is to evaluate the treatment effect on a continuous outcome (yij), such as sbp, then the first stage of the ipd meta-analysis should apply a linear regression in each trial separately, adjusting for the baseline sbp <UNK> and the covariate (zij), whilst including the treatment (xij) and the treatment-covariate interaction (xijzij): y𝑖𝑗 = 𝛼i + <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗 + e𝑖𝑗 e𝑖𝑗 ∼ <UNK> <UNK> i ). <UNK> the key parameter in model <UNK> is the treatment-covariate interaction (𝛾wi), which represents the change in the treatment effect (ie, the change in the mean difference in outcome value for treatment compared to control) for a <UNK> increase in zij after adjusting for the prognostic effects of zij and <UNK> other terms are 𝛼i (the intercept, that is, the expected outcome value for participants in the control group with zero values of zij and <UNK> <UNK> and <UNK> (the expected change in the outcome value for a <UNK> increase in zij and <UNK> respectively) and <UNK> (the treatment effect for those with a zij of zero after adjusting for baseline). the same residual variance <UNK> i ) is assumed for the treatment and control groups, but this can be relaxed if considered <UNK> if a binary outcome was rather of interest (ie, yij <UNK> or <UNK> then alternatively a binomial regression could be fitted in each trial separately, such as a logistic regression, y𝑖𝑗 ∼ bernoulli(p𝑖𝑗) ln ( p𝑖𝑗 <UNK> − p𝑖𝑗 ) = 𝛼i + <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗, <UNK> where pij is the probability of an outcome event (ie, yij = <UNK> for the jth participant in the ith trial conditional on their covariate values. the parameters are defined similar to those for model <UNK> expect now we model the log-odds of the outcome event ( ie, ln ( p𝑖𝑗 <UNK> )) such that the treatment effect is measured by a log odds ratio. hence, the treatment-covariate interaction (𝛾wi) represents the change in the log odds ratio for a <UNK> increase in zij after adjusting for the prognostic effects of zij and <UNK> if a time-to-event outcome was of interest, then a cox proportional hazards regression model could be fitted in each trial separately, such as 𝜆𝑖𝑗(t) = <UNK> <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗), <UNK> where 𝜆ij(t) is the hazard rate of the outcome for the jth participant in the ith trial conditional on their covariate values, whilst <UNK> is the baseline hazard (ie, the hazard rate for a participant in the control group with zero values of zij and <UNK> now the treatment effect is measured by a log hazard ratio, such that the treatment-covariate interaction (𝛾wi) represents the change in the log hazard ratio for a <UNK> increase in zij after adjusting for the prognostic effects of zij and <UNK> extension of models <UNK> to <UNK> to adjust for further baseline covariates (ie, in addition to zij and <UNK> is recommended when there are other known prognostic <UNK> ideally, the researcher should predefine a core set of strong prognostic factors to be adjusted for in each trial, particularly focusing on those routinely recorded in the trials available for ipd meta-analysis. for example, in many disease fields, age and stage of disease are key prognostic factors and routinely recorded at baseline, such that they can be adjusted for in the analysis. other more complex trial designs (eg, cluster trials, or multicenter or multiarm trials) would also require the models to be modified, as would allowing for nonproportional hazards in model <UNK> estimation of models <UNK> to <UNK> in each study, for example using (restricted) maximum likelihood estimation, produces a treatment-covariate interaction estimate, ̂𝛾𝑊𝑖, and its variance, var(̂𝛾𝑊𝑖). the “w” is used to emphasize that the interaction, 𝛾wi, in each study is based solely on within-study information; that is, it is only based on differences in the treatment effect across participant-level covariate values observed within each study. this is an important point (as it avoids using across-study information and potentially introducing aggregation bias), which we return to in the next section. each 𝛾wi indicates for trial i the change in treatment effect for a one-unit increase in the participant-level covariate zij. for a continuous covariate, this assumes the effect of the interaction is linear (although extension to nonlinear trends is important, as described later in the article). <UNK> second stage in the second stage of a two-stage ipd meta-analysis, we simply combine the ̂𝛾𝑊𝑖 values across trials in a traditional meta-analysis model, such as a common-effect (sometimes known as a fixed-effect) model, ̂𝛾𝑊𝑖 ∼ n(𝛾w , var(̂𝛾𝑊𝑖)), <UNK> or a random-effects model: ̂𝛾𝑊𝑖 ∼ n(𝛾𝑊𝑖, var(̂𝛾𝑊𝑖)) 𝛾𝑊𝑖 ∼ n(𝛾w , <UNK> ). <UNK> after estimation of the chosen meta-analysis model (eg, using restricted maximum likelihood estimation), the estimate of 𝛾w summarizes the difference in the treatment effect for two individuals who differ in zij by one-unit. based on the linear, logistic, and cox models used in the first-stage, 𝛾w represents a difference in mean difference for a continuous outcome, a difference in log odds ratios for a binary outcome (ie, exp(̂𝛾w ) gives a ratio of odds ratios), and a difference in log hazard ratios for a time-to-event outcome (ie, exp(̂𝛾w ) is a ratio of hazard ratios). note that random-effects meta-analysis model <UNK> allows for between-study heterogeneity in the true treatment -covariate interaction. it may arise due to differences across studies in, for example, the dose of the treatment, the length of follow-up, the way the covariate has been measured, and the magnitude of any interaction. it may also be due to case-mix differences in the study populations, for example leading to between-study differences in the distribution of within-study confounders and even the covariate itself. for example, if a treatment-covariate interaction is nonlinear, and the covariate distribution is narrow in some studies and wide in others, then this will induce between-study heterogeneity in the treatment-covariate interaction, unless the nonlinear association is modeled directly (see later). the magnitude and impact of heterogeneity can be summarized by providing estimates of <UNK> and using <UNK> prediction <UNK> to account for uncertainty in the estimate of <UNK> when deriving <UNK> confidence intervals for 𝛾w , we recommend the hartung-knapp sidik-jonkman (hksj) <UNK> or alternatively using a bayesian framework. if some studies do not agree to share their ipd but do provide (either directly or in a publication) the required treatment-covariate interaction estimate and its corresponding standard error (se), these can be incorporated in the second stage. that is, the ̂𝛾𝑊𝑖 derived directly from ipd trials are combined with the ̂𝛾𝑊𝑖 extracted (or provided) from non-ipd trials. <UNK> the one-stage approach a one-stage ipd meta-analysis can also be used to summarize treatment-covariate interactions. this approach analyzes the ipd from all trials in a single step using a general or generalized linear (mixed) model <UNK> or a survival (frailty) <UNK> this allows a more exact likelihood specification than that used in the second stage of the two-stage approach, and thus avoids assuming study-specific treatment-covariate interaction estimates are normally distributed with known <UNK> hence one-stage models may be most advantageous when the studies in the ipd meta-analysis have small numbers of participants and/or <UNK> inclusion of a treatment-covariate interaction term in a one-stage model is not as straightforward as it may seem. depending on the model specification, the apparently simple inclusion of a global treatment-covariate interaction term may allow across-study information to contribute toward the summary interaction estimate, in combination with within-study <UNK> this may lead to aggregation bias (also known as ecological bias), which refers to when the information across studies distorts the interaction estimate compared to when using only within-study information. let zi represent the study-specific mean of the covariate zij, and let us assume there is potential between-study heterogeneity in the treatment-covariate interaction. then, to disentangle within-study and across-study information in a one-stage model, there are two key options: (i) center the covariate zij about its study-specific mean, zi and add an additional term which allows the covariate means (zi) to explain between-study heterogeneity in the overall treatment <UNK> and/or. (ii) stratify by trial other parameters outside the interaction term, including the parameter representing the treatment effect (ie, the treatment effect at the covariate's reference value, typically zij = <UNK> or if the covariate is centered z𝑖𝑗 = zi). implementing either approach should give very similar summary estimates of the treatment-covariate interaction, 𝛾w , from a one-stage ipd meta-analysis. approach (i) leads to a one-stage model for continuous, binary, and time-to-event outcomes of the following <UNK> y𝑖𝑗 = 𝛼i + <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi) + 𝜀𝑖𝑗 <UNK> ∼ n(𝜑 + 𝛾azi, <UNK> <UNK> ) 𝛾𝑊𝑖 ∼ n(𝛾w , <UNK> <UNK> ), <UNK> y𝑖𝑗 ∼ bernoulli(p𝑖𝑗) logit(p𝑖𝑗) = 𝛼i + <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi) <UNK> ∼ n(𝜑 + 𝛾azi, <UNK> <UNK> ) 𝛾𝑊𝑖 ∼ n(𝛾w , <UNK> <UNK> ), <UNK> 𝜆𝑖𝑗(t) = <UNK> <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi)) <UNK> ∼ n(𝜑 + 𝛾azi, <UNK> <UNK> ) 𝛾𝑊𝑖 ∼ n(𝛾w , <UNK> <UNK> ). <UNK> parameters in models <UNK> to <UNK> are similar to those defined for models <UNK> to <UNK> in section <UNK> in addition, the study-specific treatment-covariate interactions (𝛾wi) are assumed normally distributed with a mean 𝛾w and between-study variance <UNK> <UNK> , as for random-effects model <UNK> in the second stage of the two-stage approach. of key interest is an estimate of 𝛾w , to summarize the expected change in the treatment effect (eg, log hazard ratio in model <UNK> for each one unit increase in zij. as previously, “w” indicates that the interaction will be based solely on within-study information. models <UNK> to <UNK> also include a meta-regression component (𝜑 + 𝛾azi), where 𝜑 denotes the summary treatment effect for participants with zero values of zij and <UNK> in trials with zi equal to zero; and 𝛾a is the change in the summary treatment effect for each <UNK> increase in zi. the models also allow for residual between-study variance <UNK> <UNK> ) in the summary treatment effect (ie, that not explained by zi). inclusion of the 𝛾azi term, together with the centering of zij within the interaction term, disentangles 𝛾w and 𝛾a such that they are uncorrelated with each other, and thus ̂𝛾w will be based solely on within-trial <UNK> note that if the 𝛾azi term is not included in model <UNK> then the interaction term will represent some weighted average of ̂𝛾w and the magnitude of aggregation bias (̂𝛾e = ̂𝛾a − ̂𝛾w ). other parameters (ie, intercepts, baseline hazards, prognostic effects of zij and <UNK> are stratified by trial (ie, estimated separately for each trial). approach (ii) leads to one-stage models for continuous, binary and time-to-event outcomes of the following format: y𝑖𝑗 = 𝛼i + <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗 + 𝜀𝑖𝑗 𝛾𝑊𝑖 ∼ n(𝛾w , <UNK> ), <UNK> y𝑖𝑗 ∼ bernoulli(p𝑖𝑗) logit(p𝑖𝑗) = 𝛼i + <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗 𝛾𝑊𝑖 ∼ n(𝛾w , <UNK> ), <UNK> 𝜆𝑖𝑗(t) = <UNK> <UNK> + <UNK> + <UNK> + 𝛾𝑊𝑖x𝑖𝑗z𝑖𝑗) 𝛾𝑊𝑖 ∼ n(𝛾w , <UNK> ). <UNK> in models <UNK> to <UNK> each of the “nuisance” parameters (ie, the 𝛼i, <UNK> <UNK> <UNK> and <UNK> parameters which are not of primary interest) are stratified by trial, and random effects are placed only on the within-study interaction. this stratification of all nuisance parameters by trial, in particular <UNK> representing the treatment effect at the covariate's reference value, ensures that 𝛾w only contains within-trial information. it also closely reflects the two-stage approach, where all nuisance parameters are naturally stratified by trial as they are estimated in each trial separately in the first stage. as mentioned for the two-stage approach, all these one-stage models <UNK> to <UNK> need to be modified for other trial designs (eg, cluster trials) and when adjusting for further prognostic factors (ie, in addition to zij and <UNK> the latter is a sensible strategy (see discussion in section <UNK> but again requires each prognostic factor's effect to be stratified by study. this will increase the number of parameters and so, for estimation reasons, centering each included prognostic factor by figure <UNK> a two-stage ipd meta-analysis of treatment-sex interactions, summarizing the difference in the effect of antihypertensive treatment for males compared to females. note: the interactions refer to the difference between males and females in the treatment effect (ie, their difference in the mean difference in final systolic blood pressure for treatment vs control, after adjusting for baseline), with negative values indicating that the treatment effect is better in females than males [colour figure can be viewed at wileyonlinelibrary.com] its study-specific mean is a sensible default approach (especially when unrestricted maximum likelihood estimation is used to fit the one-stage model). <UNK> applied example: is the effect of antihypertensive treatment different for males and females? the two-stage and one-stage approaches are now illustrated using the ipd from the <UNK> randomized trials examining the effect of antihypertensive treatment on sbp, as introduced in section <UNK> the question is whether the treatment effect is different for males compared to females; that is, whether there is a treatment-sex interaction. two-stage approach in the first stage, restricted maximum likelihood (reml) estimation was used to fit model <UNK> with final sbp as the response, and sex (males = <UNK> females = <UNK> as the covariate, zij, of interest. figure <UNK> provides a forest plot of the treatment-sex interaction estimates, ̂𝛾𝑊𝑖, plotted as circles. circles are recommended by fisher et al to help distinguish a forest plot of interaction estimates from a standard forest plot of treatment effect <UNK> for which squares are typically used. in the second stage, reml estimation was used to fit random-effects meta-analysis model <UNK> and this gave a summary interaction estimate of ̂𝛾w <UNK> <UNK> ci: <UNK> to <UNK> hence there is no evidence of an important difference in treatment effect for males compared to females; the summary interaction estimate is close to zero and clinically unimportant, the confidence interval overlaps zero, and there is also between-study heterogeneity. using the hksj approach, the <UNK> confidence is slightly wider <UNK> to <UNK> one-stage approach for the one-stage approach, we used reml estimation to fit models <UNK> and <UNK> this gave the same summary interaction estimate of ̂𝛾w <UNK> <UNK> ci: <UNK> to <UNK> practically identical to that from the two-stage approach when reml and a wald-based confidence interval are used. when we rather used the satterthwaite approach to derive confidence intervals following estimation of the one-stage model (as recommended for continuous <UNK> the obtained <UNK> confidence interval for 𝛾w was slightly wider <UNK> to <UNK> and practically identical to that derived using the hksj method in the two-stage approach (as they both utilize the t-distribution rather than the standard normal distribution). thus, one-stage and two-stage ipd meta-analysis results for the treatment-sex interaction are almost identical. this agrees with burke et al who describe in detail that when the same assumptions and the same estimation approaches are applied, one-stage and two-stage approaches will closely agree unless most studies are <UNK> in conclusion, this example does not support there being an important treatment-sex interaction. <UNK> statistical modeling recommendations when conducting an ipd meta-analysis to examine treatment-covariate interactions some published ipd meta-analysis projects that examine treatment-covariate interactions use inappropriate statistical approaches that ignore or incorrectly modify the models described in section <UNK> we now make five recommendations to address the key concerns. <UNK> do not make inferences about interactions using the summary treatment effect derived in each subgroup separately when the participant-level covariate is categorical, it may be tempting to perform a one-stage or two-stage ipd meta-analysis of the overall treatment effect in each category (subgroup) separately. for example, an ipd meta-analysis might be conducted for males and females separately, to obtain the summary treatment effect for males and females. however, fisher et al and belias et al show that it is dangerous to use the subsequent results to make inferences about whether an interaction <UNK> in particular, a common mistake is to conclude a treatment-covariate interaction exists if the summary treatment effect estimate is statistically significant in one subgroup but not the other. in this situation the actual treatment-covariate interaction (difference between subgroups based on within-study information) may not be statistically significant. altman and bland consider this <UNK> it is even flawed to compare the summary treatment effects for each subgroup, for example via a statistical test or by calculating their difference. although this is apparently simple, it amalgamates within-study and across-study information, and so should be avoided. the larger the differences in mean covariate values across studies, the larger the potential contribution of the across-study information. one way to understand this is to consider an extreme situation, where a treatment-sex interaction is of interest, but some trials contain only males. such trials cannot contribute any within-study information about the interaction between treatment effect and sex at the individual level, as there are no females. however, the trial would still contribute toward the subgroup result for males, and thus subsequently toward the difference between meta-analysis results for male and female subgroups. this issue is avoided by meta-analyzing the treatment-covariate interaction estimates observed within trials, by using the two-stage and one-stage approaches outlined in section <UNK> application to the hypertension example recall that a two-stage ipd meta-analysis using only within-study information gave a summary treatment-sex interaction estimate of ̂𝛾w <UNK> <UNK> ci: <UNK> to <UNK> when we do a separate ipd meta-analysis for males and females, we obtain summary treatment effect estimates of <UNK> <UNK> ci: <UNK> to <UNK> and <UNK> <UNK> ci: <UNK> to <UNK> respectively. the estimated difference in these subgroup results is <UNK> which is an amalgamation of within-trial and across-trial information, and about twice the size of the summary interaction estimate of <UNK> based solely on within-study information. the across-trial information arises because the proportion of males varies considerably across trials, from about <UNK> to <UNK> <UNK> separate within-trial and across-trial information in one-stage models one-stage ipd meta-analysis models are appealing to statisticians, as they eloquently synthesize all the ipd in a single statistical model as described in section <UNK> however, compared to the two-stage approach, it is easier to make modeling errors when estimating 𝛾wa. one-stage models <UNK> to <UNK> show how to separate within-study and across-study information, and ensure that 𝛾wa is estimated based solely on within-study information. as such, they are labeled as “deft” by fisher et <UNK> however, many researchers wrongly fit models <UNK> to <UNK> without the 𝛾azi term, and with 𝛾𝑊𝑖x𝑖𝑗(z𝑖𝑗 − zi) replaced by 𝛾waixijzij. for example, a flawed one-stage linear regression model for a continuous outcome is: y𝑖𝑗 = 𝛼i + <UNK> + <UNK> + <UNK> + 𝛾𝑊𝐴𝑖x𝑖𝑗z𝑖𝑗 + e𝑖𝑗 𝛾𝑊𝐴𝑖 ∼ n(𝛾𝑊𝐴, <UNK> 𝛾𝑊𝐴 ) <UNK> ∼ <UNK> <UNK> <UNK> ) e𝑖𝑗 ∼ <UNK> <UNK> i ). <UNK> here, the interaction 𝛾wa is an amalgamation of within-study information and across-study information; essentially a weighted average of 𝛾w and 𝛾a. <UNK> the estimate of 𝛾wa from model <UNK> will be more precise than the estimate of 𝛾w from figure <UNK> does age interact with the effect of antihypertensive treatment on systolic blood pressure? findings from an ipd meta-analysis of <UNK> trials, showing difference in results based on across-study (solid line) and within-study (dashed lines) information. across-study relationship (from meta-regression of trial's treatment effect estimates vs mean age) denoted by gradient of solid line. participant-level relationship using within-study information (ie, treatment-sex interaction within each trial) denoted by gradient of dashed lines and the summary gradient (̂γw) is <UNK> <UNK> ci: <UNK> to <UNK> each block represents one trial, and the block size is proportional to the size of the trial. <UNK> <UNK> <UNK> <UNK> trial treatment effect estimate (mm hg) <UNK> <UNK> <UNK> <UNK> <UNK> mean age in the trial model <UNK> but at the expense of the across-study information distorting the estimate and interpretation compared to 𝛾w . this motivates fisher et al to call it a “deluded” <UNK> application to the hypertension example returning to the hypertension example, let us consider whether there is a treatment-age interaction whilst assuming that the effect of age is linear. reml estimation of one-stage model <UNK> gives ̂𝛾w <UNK> <UNK> ci: <UNK> to <UNK> and thus no clear evidence of a treatment-age interaction. when rather fitting model <UNK> we obtain ̂𝛾𝑊𝐴= <UNK> <UNK> ci: <UNK> to <UNK> which is almost twice the size of that based solely on within-study information and now strongly suggests a treatment-age interaction. the difference is because ̂𝛾𝑊𝐴 is strongly influenced by the across-study information, which is suggesting a different magnitude of effect than suggested by the within-study information in half of the trials. this is illustrated in figure <UNK> which also suggests that there may be a nonlinear interaction with age, as the direction of the interactions appears to be different in trials with younger and older ages. extension to nonlinear trends is considered later. <UNK> do not dichotomize continuous covariates or outcomes categorization, and in particular dichotomization, of continuous covariates (eg, such as age, blood pressure, and most biomarkers) using cut-points is best avoided when examining their interaction with treatment <UNK> the usual argument for categorization is to aid clinical interpretation and maintain simplicity. however it can rarely, if ever, be justified that an individual whose value is just below the cut-point is completely different from an individual whose value is just above it. moreover, categorization reduces the power to detect genuine covariates that interact with treatment effect. ensor et al show that, in the aforementioned ipd meta-analysis to examine exercise interventions to reduce gestational weight gain in pregnancy, the loss of information by dichotomizing bmi (rather than keeping bmi as continuous) is equivalent to throwing away about one-third of the ipd <UNK> categorization of continuous outcomes should also be avoided, as it can mislead researchers into thinking there are differential responses to treatment. in particular, classifying individuals as either responders or nonresponders, based on an arbitrary cut-point value for a continuous outcome (eg, <UNK> mmhg at follow-up), will lead to those just above the threshold being classed differently to those just below it, which is nonsense. such dichotomization also leads to misclassification when there is measurement <UNK> application to the pregnancy example in the pregnancy example an outcome of interest was caesarean section and, across all participants in <UNK> trials, the intervention reduces the odds of caesarean section by about <UNK> (summary or = <UNK> <UNK> ci: <UNK> to <UNK> it is of interest whether the mother's age interacts with this intervention effect. if we arbitrarily dichotomize age at <UNK> years, so that it becomes a binary variable (ie, <UNK> if age <UNK> and <UNK> if age <UNK> then a two-stage ipd meta-analysis (model <UNK> followed by model <UNK> shows no clear evidence of an interaction between age and treatment effect (summary ratio of ors = <UNK> <UNK> ci: <UNK> to <UNK> however, if age is kept as a continuous variable, then there is stronger evidence of a treatment-age interaction. the intervention becomes less effective at reducing the odds of caesarean section as a women's age increases. for every <UNK> increase in age the odds ratio increases by about <UNK> (summary ratio of ors = <UNK> <UNK> ci: <UNK> to <UNK> when analyzed on the risk (rather than odds scale), using a binomial regression with a log-link, this translates to about a <UNK> increase in the risk ratio for every <UNK> increase <UNK> ci: <UNK> to <UNK> <UNK> allow for potential nonlinear relationships when modeling interactions with continuous covariates in the previous example we assumed a linear trend for the interaction of treatment and a continuous covariate. however, sometimes the interaction may be nonlinear, as emphasized by royston and <UNK> and considered in detail by kasenda et <UNK> this implies the change in treatment effect for every <UNK> increase in the covariate may vary across the distribution of the covariate. therefore, nonlinear interactions should be evaluated when the interaction of a continuous covariate and treatment effect is of interest. we now illustrate this with an example, and explain how it can be implemented using a multivariate meta-analysis of spline functions. application to the hypertension example wang et al consider whether there is an interaction between age and the effect of hypertensive treatment, and identify a nonlinear relationship, with older patients between <UNK> and <UNK> years old seeming to benefit more than younger patients and those older than <UNK> their original analysis categorized age, but we now update their analysis to rather display the change in treatment effect as a smooth, nonlinear function of age (figure <UNK> with the reference group being the treatment effect for a <UNK> year old. a j-shaped relationship is visible; in particular, there is strong evidence that younger individuals have the smallest treatment effect. for example, compared to an individual aged <UNK> years, an individual aged <UNK> years has about a <UNK> mmhg greater reduction in sbp due to the treatment (compared to control). in very old ages the treatment effect also appears to reduce slightly, but there is large uncertainty (wide confidence intervals). interestingly, if we just include an interaction with age (ie, assume a linear interaction term) then there is no evidence of a treatment-age interaction. the change in treatment effect for each year increase of age is <UNK> <UNK> ci: <UNK> to <UNK> the linear assumption hides the more j-shaped interaction revealed by the nonlinear modeling approach. conducting a two-stage multivariate ipd meta-analysis of restricted cubic splines figure <UNK> was obtained by conducting a two-stage ipd meta-analysis, where in the first stage a restricted cubic spline function was estimated in each study separately and in the second stage a multivariate meta-analysis was fitted. splines are a flexible way of modeling smooth nonlinear <UNK> briefly, a restricted cubic spline is obtained by fitting a series of cubic functions and forcing them to join (and be smoothed) at certain points (called internal knots), whilst constraining the function to be linear in the tails (ie, before the first internal knot and after the last internal knot). the magnitude and shape of the curve are defined by multiple parameters depending on the number of knots chosen; as explained in figure <UNK> the knot locations are forced to be the same in every study, to ensure the study-specific curves can be synthesized meaningfully. rather than using a reference group whose covariate value is <UNK> it helps to center the covariate at a meaningful value (eg, <UNK> years was chosen as the reference group in figure <UNK> which is the same in every study. in our example, three internal knots (at the same locations in every study) were chosen for the restricted cubic function representing the association between age and final sbp in the control group. hence, in the first stage three parameters defining the spline function are estimated in each study (an intercept and two slope terms), plus the interaction of this spline function with the treatment effect. the latter provides the within-study treatment-covariate interaction defined by <UNK> and <UNK> (see figure <UNK> which represent the change (difference) in treatment effect across covariate values, relative to the chosen reference group of an individual aged <UNK> years. figure <UNK> summary of the nonlinear interaction between age and effect of hypertension treatment on final sbp value. figure created by fitting an analysis of covariance model in each study separately, with the interaction between age and treatment modeled via a restricted cubic spline function (with knot positions of <UNK> <UNK> and <UNK> and then the study-specific parameter estimates (relating to the interaction) pooled in a multivariate random-effects meta-analysis figure <UNK> introduction to modeling nonlinear relationships in a single study using restricted cubic splines; for further details we recommend the reader refer to <UNK> figure <UNK> (a): overview of the first stage of a two-stage multivariate ipd meta-analysis to summarize a nonlinear treatment-covariate interaction using a restricted cubic spline. the steps are described in relation to the hypertension example of figure <UNK> which examined a treatment-age interaction. figure <UNK> (b): overview of the second stage of a two-stage multivariate ipd meta-analysis to summarize a nonlinear treatment-covariate interaction using a restricted cubic spline. the steps are described in relation to the hypertension example of figure <UNK> which examined a treatment-age interaction in the second stage, the study estimates of the difference in slope parameters of the spline function (ie, <UNK> and <UNK> can be meta-analyzed using a multivariate random-effects <UNK> this allows the synthesis of multiple parameter estimates whilst accounting for their within-study and between-study correlation, and produces a summary estimate for each parameter, from which the summary spline (nonlinear) function can be derived. it can handle missing parameter estimates (ie, for missing parts of the spline function in studies with a narrow distribution of covariate values) as described in figure <UNK> this summary curve describes the association between values of the covariate and the change in treatment effect, relative to the reference group (age <UNK> years in the example). this can then be plotted graphically to aid interpretation. the study-specific estimated curves from the first stage (or study-specific empirical bayes curves obtained postestimation from the second stage) might also be presented, as shown by gasparrini et <UNK> a summary of the two-stage multivariate ipd meta-analysis of restricted spline functions is given in figure <UNK> a similar approach is a multivariate ipd meta-analysis of a polynomial <UNK> in the first stage the differences (ie, treatment minus control) of the parameters of a chosen polynomial function are estimated in each study (eg, differences in a quadratic shape, such as <UNK> + <UNK> 𝑖𝑗) and the parameter estimates <UNK> and <UNK> are jointly synthesized to produce a summary <UNK> in particular, fractional polynomial functions provide a flexible set of power transformations to describe a potentially nonlinear <UNK> in order for the parameters of the function to be combinable across studies in a multivariate meta-analysis, the same powers of the fractional polynomial function must be specified in each study (ie, the shape of the nonlinear association is fixed across <UNK> in contrast, as long as the same number and location of knots is used in each study, a restricted cubic spline function is more flexible (ie, the shape of the nonlinear association can vary across studies) and it can handle different covariate distributions across studies by strategic placement of knots. sauerbrei and royston show that it is possible to combine different fractional polynomial power transformations across studies, if the predicted values of the function are pooled (rather than the parameters defining the <UNK> with the pooling of predictions done at each value of the covariate separately. a one-stage ipd meta-analysis model can also be used to examine nonlinear treatment-covariate interactions, but difficulties can arise. first, the extra parameters required to model nonlinear functions, and potentially multiple random effects, may cause estimation and convergence problems. second, when centering covariates by their study-specific means (ie, to avoid aggregation bias), by extending models <UNK> to <UNK> the interpretation of the spline function becomes problematic. unless all studies have the same mean covariate value, the change in treatment effect for a <UNK> increase in a covariate from its mean will have a different interpretation in each study; this will make the summary spline function uninterpretable. therefore, it is preferable to examine nonlinear trends by extending one-stage models <UNK> to <UNK> stratifying by trial parameters outside the interaction term, to remove aggregation bias. <UNK> to personalize decision-making, individualized predictions of treatment effect are required for clinical practice, translation of identified treatment-covariate interactions is required for the individual patient, in order to tailor their treatment decisions. this is nontrivial. a fundamental error is to assume that if a treatment-covariate interaction exists then the treatment is effective in some individuals but not in others. actually, even when the magnitude of treatment effect varies across individuals, the direction of effect may consistently suggest the treatment is beneficial for everyone. to better guide decision-making, a predicted treatment effect is required for each individual. this requires a robust prediction model equation, developed and validated using methodology principles for clinical prediction <UNK> in particular, to reduce overfitting (too extreme predictions) the model development might require penalization and shrinkage <UNK> to reduce the variability of predictions in new datasets, thereby reducing the mean-square error of the predictions. a step further is to predict an individual's absolute outcome value or risk conditional on their prognostic factors and expected treatment effect. these and related issues are discussed in detail <UNK> crucially, aggregation bias should also be avoided when making predictions, and we now illustrate this with an example. application to the hypertension example consider the example within figure <UNK> that suggests a potential nonlinear interaction between age and the effect of antihypertensive treatment, with younger patients seemingly having less benefit (in terms of reduction in sbp) than older individuals. the smooth curve reflects the treatment-age interaction; it reveals the summary estimate of the difference in treatment effect for an individual with a particular age compared to an individual aged <UNK> years. however, it does not tell us the predicted (expected) treatment effect for an individual with a particular age. to obtain this, we performed a multivariate ipd random-effects meta-analysis, where we estimate and then meta-analyze the two slopes <UNK> and <UNK> of the spline function (as defined in figure <UNK> and also the reference treatment group <UNK> the study-specific treatment effect for individuals aged <UNK> years, our reference group). then, the summary estimates (𝛽 <UNK> <UNK> <UNK> obtained were used to define the prediction model for an individual's treatment effect conditional on their age: predicted treatment effect for jth individual = 𝛽 <UNK> + <UNK> − <UNK> + <UNK> − <UNK> = <UNK> × <UNK> − <UNK> + <UNK> × <UNK> − <UNK> <UNK> here, <UNK> is the predicted treatment effect for an individual aged <UNK> years, and <UNK> and <UNK> represent, respectively, the individual's values of the first and second spline transformation of their age. for example, for an individual aged <UNK> their corresponding <UNK> is <UNK> and <UNK> is <UNK> and thus their predicted treatment effect is <UNK> × <UNK> + <UNK> × <UNK> = <UNK> overfitting is potentially of limited concern for prediction equation <UNK> as it was derived using over <UNK> <UNK> participants with only two parameters (excluding the intercept) estimated. the predicted treatment effect across the age range is summarized in figure <UNK> across all ages the predicted treatment effect is at least <UNK> mmhg. hence, on average, all patients are predicted to benefit by a clinically useful amount, even though the magnitude of effect varies due to the treatment-age interaction. a concern is that a multivariate meta-analysis of 𝛽 <UNK> <UNK> and <UNK> accounts for the correlation amongst these parameters, both within and between studies. yet again this allows the potential for aggregation bias to influence the summary interaction terms, <UNK> and <UNK> as their correlation with 𝛽 <UNK> allows the borrowing of across-trial information. though it is perhaps sensible for 𝛽 <UNK> (as it is our reference treatment effect averaged across studies), we should want our summary interactions terms to be based solely on within-trial information, as previously argued. to address this, we replace the summary <UNK> and <UNK> estimates in our prediction equation <UNK> with <UNK> and <UNK> their respective estimates from a multivariate meta-analysis ignoring any correlation with 𝛽 <UNK> terms then, the modified prediction equation is: predicted treatment effect for jth individual = 𝛽 <UNK> + <UNK> − <UNK> + <UNK> − <UNK> = <UNK> × <UNK> − <UNK> + <UNK> × <UNK> − <UNK> <UNK> this allows us to produce predicted treatment effects conditional on age, summarized across all studies and removing aggregation bias. using equation <UNK> for an individual aged <UNK> years, their predicted treatment effect is, <UNK> × <UNK> + <UNK> × <UNK> = <UNK> mmhg figure <UNK> the predicted effect of antihypertensive treatment on sbp conditional on an individual's age, based on a multivariate meta-analysis either with or without aggregation bias in the summary treatment-age interactions which is larger than the <UNK> predicted from the previous equation. indeed, the predicted curve across the entire age range is noticeably shallower after removing aggregation bias (figure <UNK> because the aggregation bias (arising from incorporating across trial information) makes interaction estimates larger than when based solely on within-trial information. note that the predictions shown here are averaged across studies. if possible, incorporation of study-level covariates (eg, country) that explain between-study heterogeneity in parameter estimates might help tailor predictions further. <UNK> to ipd or not to ipd? statistical recommendations when planning an ipd meta-analysis project to examine treatment-covariate interactions statistical considerations are also important when deciding whether to initiate an ipd meta-analysis project (ie, before ipd collection), and two key issues are now discussed in the context of treatment-covariate interactions. <UNK> the decision to initiate an ipd meta-analysis project to examine treatment-covariate interactions should not be based on the amount of between-study heterogeneity in the overall treatment effect without ipd, most meta-analyses of randomized trials will summarize the treatment effect based on all trial participants (ie, without consideration of treatment-covariate interactions). if such a meta-analysis does not find evidence of between-study heterogeneity in the treatment effect, it may be tempting to conclude that the treatment effect is the same for all individuals. however, this logic is flawed: the absence of heterogeneity in the overall effect (across all participants) does not necessarily imply an interaction does not exist. first, if there is a genuine treatment-covariate interaction, but the distribution of the covariate is very similar across studies, then (assuming no other patient-level or study-level effect modifiers differ across studies), the overall treatment effect will be the same in each study (ie, there will be no heterogeneity). second, even if the distribution of the covariate does change across studies, the overall treatment effect may still be homogenous; for example this may arise due to chance, or because the covariate has a nonlinear (eg, u-shaped) interaction with treatment (so that the overall treatment effect may still be the same in two studies with very different mean covariate values), or even due to multiple effect modifiers acting in combination and different directions. conversely, if there is heterogeneity in the overall treatment effect this does not imply that an interaction exists at the participant-level. heterogeneity can arise due to changes in study-level characteristics such as dose, follow-up length, and setting, even when there is no interaction at the participant-level. hence, the decision to investigate treatment-covariate interactions should not be driven by the presence of between-study heterogeneity in treatment effect, and should be rather motivated by the supposed (biological) mechanism of treatment response. application to the hypertension example as an illustration of why heterogeneity is a poor indicator of whether treatment-covariate interactions exist, consider a two-stage ipd meta-analysis examining the effect of antihypertensive treatment on rate of cvd, for which the summary hazard ratio was <UNK> <UNK> ci: <UNK> to <UNK> with no observed heterogeneity <UNK> = <UNK> however, there is some suggestion figure <UNK> evidence of a potential nonlinear interaction between baseline sbp and the effect of hypertension treatment on the rate of cvd, even though there was no between-study heterogeneity in the overall treatment effect of a nonlinear interaction between baseline sbp and the treatment effect on cvd, with the treatment effect gradually reducing as the baseline sbp moves from about <UNK> mmhg toward <UNK> mmhg (figure <UNK> as obtained using a two-stage multivariate meta-analysis of restricted cubic splines as described in figure <UNK> <UNK> calculate the power to identify a treatment-covariate interaction prior to collection of ipd ipd meta-analysis projects are more likely to be funded if they have sufficient power to answer the research question at hand. before ipd collection, power calculations for ipd meta-analysis projects can be made conditional on the number of trials promising their ipd, using known trial characteristics such as the number of participants and standard deviation (sd) of covariate values. closed-form solutions are difficult to obtain, especially for noncontinuous outcomes unless approximations are <UNK> a pragmatic starting point is to consider whether the total sample size (and overall outcome proportion or rate for binary and time-to-event outcomes) of the promised ipd has adequate power if naively considered to come from a single study. then standard statistical software for estimating the power of an interaction in a single randomized trial could be used. to mirror the ipd meta-analysis setting more exactly, a simulation-based approach has been <UNK> in particular, ensor et al suggest generating ipd containing the same number of studies (and same number of participants and events therein) as promising their <UNK> with covariate values simulated based on assumed true treatment effects and treatment-covariate interaction effect size. then a two-stage ipd meta-analysis is applied to this simulated dataset, and the estimated treatment-covariate interaction and its confidence interval are stored. this is repeated m times (ideally thousands). based on a traditional frequentist paradigm, power can then be estimated by calculating the proportion of times the summary estimate was statistically significant (eg, as defined by the associated <UNK> confidence interval excluding the null value). this process might also be repeated assuming different values for the size of the assumed treatment-covariate interaction, and different values for the assumed magnitude of heterogeneity in the interaction across studies (starting from zero). for an ipd meta-analysis of s randomized trials with continuous outcomes, closed-form solutions are obtainable for the power to evaluate a treatment-covariate <UNK> simmonds and higgins provide the following analytic solution for the maximum likelihood estimate of a treatment-covariate interaction for a continuous outcome in a single randomized trial with two parallel <UNK> ̂𝛾𝑊𝑖 = <UNK> ∑ni <UNK> <UNK> 𝑖𝑗 ( ∑ j∈ti y𝑖𝑗z′ 𝑖𝑗 − ∑ j∈ci y𝑖𝑗z′ 𝑖𝑗) . here, ti denotes the treatment group and ci the control group in study i, and z′ 𝑖𝑗 denotes that each zij is centered about the study-specific mean zij value (ie, z′ 𝑖𝑗 = z𝑖𝑗 − zi). simmonds and higgins use this to derive subsequent power <UNK> assuming common residual variances across trials. if we extend their work by allowing for different residual variances in each trial <UNK> i ), the variance (var) of the interaction estimate in a particular study i is: var(̂𝛾𝑊𝑖) = var ( <UNK> ∑ni <UNK> <UNK> 𝑖𝑗 ( ∑ j∈ti y𝑖𝑗z′ 𝑖𝑗 − ∑ j∈ci y𝑖𝑗z′ 𝑖𝑗)) = <UNK> (∑ni <UNK> <UNK> 𝑖𝑗 <UNK> ( ∑ j∈ti <UNK> 𝑖𝑗 var(y𝑖𝑗) + ∑ j∈ci <UNK> 𝑖𝑗 var(y𝑖𝑗) ) = <UNK> i (∑ni <UNK> <UNK> 𝑖𝑗 <UNK> ( ∑ j∈ti <UNK> 𝑖𝑗 + ∑ j∈ci <UNK> 𝑖𝑗 ) . <UNK> let us assume an equal number of participants in the treatment and control groups, and that the variance of the covariate <UNK> zi ) is the same in each treatment group ( ie, <UNK> tizi = <UNK> cizi = <UNK> zi ) . then ∑ni <UNK> <UNK> 𝑖𝑗 = <UNK> zi , and equation <UNK> simplifies to: var(̂𝛾𝑊𝑖) = <UNK> i (∑ni <UNK> <UNK> 𝑖𝑗 <UNK> ( ∑ni <UNK> <UNK> 𝑖𝑗 ) = <UNK> i ∑ni <UNK> <UNK> 𝑖𝑗 = <UNK> i <UNK> zi . <UNK> using the solution for var(̂𝛾𝑊𝑖) from either equation <UNK> or <UNK> we can derive a closed-form solution for the variance of the summary interaction estimate from the second stage of a two-stage ipd meta-analysis. assuming common-effect model <UNK> var(̂𝛾w ) is simply the sum of the inverse of the variances from each study, var(̂𝛾w ) = ( ∑ s <UNK> <UNK> var(̂𝛾𝑊𝑖) <UNK> , <UNK> and the subsequent power to estimate a treatment-covariate interaction of size 𝛾w using the ipd meta-analysis is approximately, power = prob ( ̂𝛾w √var(̂𝛾w ) > <UNK> + prob ( ̂𝛾w √var(̂𝛾w ) < <UNK> . <UNK> for example, assuming a common interaction and that trials have equally sized groups, we can use equations <UNK> and <UNK> to give, power = prob ( ̂𝛾w √var(̂𝛾w ) > <UNK> + prob ( ̂𝛾w √var(̂𝛾w ) < <UNK> = φ ⎛ ⎜ ⎜ ⎝ <UNK> + 𝛾w √√√√∑ s <UNK> <UNK> zi <UNK> i ⎞ ⎟ ⎟ ⎠ + φ ⎛ ⎜ ⎜ ⎝ <UNK> − 𝛾w √√√√∑ s <UNK> <UNK> zi <UNK> i ⎞ ⎟ ⎟ ⎠ , <UNK> where φ(x) is the probability of sampling a value < x from the standard normal distribution. allowing for different sized groups in a trial (ie, using equation <UNK> rather than <UNK> the power calculation becomes, power = φ ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ <UNK> + 𝛾w √√√√√√√ ∑ s <UNK> ⎛ ⎜ ⎜ ⎜ ⎝ (∑ni <UNK> <UNK> 𝑖𝑗 <UNK> <UNK> i (∑ j∈ti <UNK> 𝑖𝑗 + ∑ j∈ci <UNK> 𝑖𝑗 ) ⎞ ⎟ ⎟ ⎟ ⎠ ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ + φ ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ <UNK> − 𝛾w √√√√√√√ ∑ s <UNK> ⎛ ⎜ ⎜ ⎜ ⎝ (∑ni <UNK> <UNK> 𝑖𝑗 <UNK> <UNK> i (∑ j∈ti <UNK> 𝑖𝑗 + ∑ j∈ci <UNK> 𝑖𝑗 ) ⎞ ⎟ ⎟ ⎟ ⎠ ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ , <UNK> and as before we could replace ∑ni <UNK> <UNK> 𝑖𝑗 with <UNK> zi , and similarly ∑ j∈ti <UNK> 𝑖𝑗 and ∑ j∈ci <UNK> 𝑖𝑗 could be replaced with nti <UNK> tizi and nci <UNK> cizi , respectively. equations <UNK> and <UNK> require the user to specify a minimally important value for 𝛾w , and also the values of ni, <UNK> i , and <UNK> zi based on published study information (eg, from the baseline characteristics table for a trial) or provided directly by study authors. the residual variance <UNK> i ) might be unavailable, and so approximated by the variance of outcome values. extension can be made to allow for potential heterogeneity in the treatment-covariate interaction, but the amount of heterogeneity is difficult to know in advance. thus we suggest to focus on the power in an ideal situation where there is no between-study heterogeneity in the size of the interaction. <UNK> application to the pregnancy example in the ipd meta-analysis conducted by the i-wip collaboration introduced in section <UNK> a primary objective was to examine a potential interaction between baseline bmi and intervention effect on gestational weight gain. the prior hypothesis was that those with high baseline bmi may benefit most from weight management interventions. no formal power calculation was performed in advance of ipd collection but ultimately ipd from <UNK> trials. for illustration, here we reconstruct the power calculation for this project assuming it was known that ipd could be obtained from these <UNK> trials. the values of ni, <UNK> i , and <UNK> zi were extracted from the publications of the <UNK> trials, and are shown by ensor et <UNK> occasionally <UNK> i and <UNK> zi were missing and for these we used a weighted average of values from other studies. these values were input into equation <UNK> to derive var(̂𝛾𝑊𝑖) for each trial, which then were used within equation <UNK> to give var(̂𝛾w ) = <UNK> a minimally important interaction size of <UNK> was suggested by clinical experts, such that the reduction in weight is at least <UNK> kg larger for a <UNK> increase in bmi. thus inputting 𝛾w = <UNK> and var(̂𝛾w ) = <UNK> into equation <UNK> we obtain: power = φ ( <UNK> − <UNK> <UNK> <UNK> + φ ( <UNK> + <UNK> <UNK> <UNK> = <UNK> + <UNK> = <UNK> + <UNK> thus the estimated power is <UNK> ensor et al estimate a comparable <UNK> power using a simulation-based power <UNK> but our closed-form solution is obtained in a much quicker time-frame. had this power been known at the time of the i-wip grant application, it would have given further merit to undertaking and funding the ipd meta-analysis project. although, even if power was deemed low, there are often many other potential benefits of undertaking an ipd meta-analysis project (eg, obtaining additional follow-up, standardizing inclusion criteria, and so on). <UNK> discussion there are multiple reasons for pursuing the collection and synthesis of ipd, <UNK> but personalized medicine is driving many ipd meta-analyses to search for treatment-covariate interactions. to guide such ipd meta-analysis projects, this article has outlined key statistical methods for their conduct and planning, whilst drawing attention to issues and pitfalls. we focused on interactions in randomized trials, but key points also apply to modeling interactions in ipd meta-analyses of study types, such as those evaluating the accuracy of diagnostic tests or the association of prognostic factors to subsequent <UNK> the actual identification, validation, and successful implementation of treatment-covariate interaction is <UNK> a sensible starting point—for funders, researchers, health professionals, and patients—is that the relative treatment effect is similar for all individuals, unless there is strong justification otherwise (eg, from previous findings, and especially biological <UNK> we highlighted many issues and pitfalls that if ignored may produce misleading conclusions, and we suspect many will affect the <UNK> ipd meta-analyses identified by schuit et al as providing a significant interaction <UNK> our work focuses on statistical models for estimating an interaction. however if we are willing to make clinical decisions (and even withhold treatment) based on an individual's predicted treatment effect, then there should be a firm understanding of the causal mechanism and pathway, and the cost-effectiveness of the approach. sun et al suggest criteria to assess the credibility of a treatment-covariate interaction (which they call a subgroup <UNK> including an item about the need for evidence to be based on within-study rather than across-study information. a key role is to disentangle different sources of <UNK> and senn recommends we need more <UNK> <UNK> to repeatedly test multiple treatments in the same person, including the same treatment multiple times. for further consideration of whether interactions are likely to be genuine, we refer to two excellent tutorials which discuss intricate issues such as dose-response relationships, measurement error, adjusting for confounders, dealing with multiple covariates, and multiplicative vs additive <UNK> scale of the analysis is an important issue. for example, a treatment-covariate interaction may occur on the odds ratio scale but not the risk ratio scale, when the covariate is also a prognostic factor, as superbly illustrated by shrier and <UNK> conversely the risk ratio scale may also be problematic in some particular situations as, unlike the odds ratio, its value may be <UNK> nonlinear trends may also exist on one scale but not another. therefore consideration of multiple scales may be important, as illustrated in the example of section <UNK> allowing for nonproportional hazards may also be important for similar reasons. more research is needed to help address the issue of multiple testing and exploratory research when investigating multiple interactions in ipd meta-analysis settings, extending recommendations for a single <UNK> mistry et al propose a tree-based recursive partitioning algorithm (called “ipd-sides”) to identify subgroup effects in an ipd meta-analysis when there are many covariates of interest (ie, in an exploratory <UNK> a limitation of their proposal is that it requires the use of cut-points to dichotomize continuous covariates, rather than leaving them as continuous, and—as far as we can tell—amalgamated within-study and across-study information. extension of their work to address these issues would be welcome. treatment-covariate interactions correspond to changes across individuals in the treatment effect as measured on the relative scale (eg, risk ratio, odds ratio, hazard ratio). however, another approach to personalizing or stratifying the use of treatments is to consider their impact on absolute risks. those people with the highest absolute risk will derive the largest absolute benefit from a treatment (eg, greatest reduction in probability of the outcome) when the treatment effect expressed as a risk ratio is the same for all patients. therefore, even in the absence of treatment-covariate interactions, treatment decisions might be tailored conditional on absolute outcome <UNK> lastly, we note that those considering ipd meta-analysis projects should also consider other practicalities aside from the analysis, such as obtaining, cleaning and harmonizing the <UNK>

<|EndOfText|>

one-stage individual participant data meta-analysis models for continuous and binary outcomes: comparison of treatment coding options and estimation methods a one-stage individual participant data (ipd) meta-analysis synthesizes ipd from multiple studies using a general or generalized linear mixed model. this produces summary results (eg, about treatment effect) in a single step, whilst accounting for clustering of participants within studies (via a stratified study intercept, or random study intercepts) and between-study heterogeneity (via random treatment effects). we use simulation to evaluate the performance of restricted maximum likelihood (reml) and maximum likelihood (ml) estimation of one-stage ipd meta-analysis models for synthesizing randomized trials with continuous or binary outcomes. three key findings are identified. first, for ml or reml estimation of stratified intercept or random intercepts models, a t-distribution based approach generally improves coverage of confidence intervals for the summary treatment effect, compared with a z-based approach. second, when using ml estimation of a one-stage model with a stratified intercept, the treatment variable should be coded using “study-specific centering” (ie, <UNK> minus the study-specific proportion of participants in the treatment group), as this reduces the bias in the between-study variance estimate (compared with <UNK> and other coding options). third, reml estimation reduces downward bias in between-study variance estimates compared with ml estimation, and does not depend on the treatment variable coding; for binary outcomes, this requires reml estimation of the pseudo-likelihood, although this may not be stable in some situations (eg, when data are sparse). two applied examples are used to illustrate the findings. keywords estimation methods, individual participant data, ipd, maximum likelihood., meta-analysis, treatment coding <UNK> introduction an individual participant data (ipd) meta-analysis synthesizes the raw individual-level data from multiple related studies to produce summary results, for example, about the effect of a <UNK> a common approach to ipd meta-analysis is a two-stage framework, where the first step analyses the ipd from each study separately to produce aggregate data (such as a treatment effect estimate and its se), which are then synthesized in the second step using a traditional meta-analysis, such as a random effects model to account for between-study heterogeneity in the (treatment) effect of interest. an alternative approach to ipd meta-analysis is a one-stage framework, in which all studies are analyzed simultaneously using a hierarchical model, such as a generalized linear mixed model or a frailty survival model, to produce summary results in a single step. the one-stage approach has been increasingly used in the past <UNK> with a one-stage ipd meta-analysis model, it is essential to account for clustering of participants within studies to avoid misleading <UNK> in particular, in a generalized linear mixed model framework two options to account for this clustering are (i) by using a stratified intercept term in the analysis, which involves estimating a separate intercept for each study; or (ii) by assuming random study intercepts, whereby study intercepts are assumed to be drawn from a distribution (typically a normal distribution). we recently showed through simulation that, when applying a one-stage ipd meta-analysis of randomized controlled trials (rcts) with a <UNK> treatment:control allocation ratio and a continuous outcome, the meta-analyst can choose either a stratified intercept or random intercepts model when restricted maximum likelihood (reml) is used for <UNK> that is, the statistical properties of the estimate of summary treatment effect, the <UNK> confidence interval for the summary treatment effect, and the estimate of between-study variance of treatment effects are all very similar regardless of whether a stratified intercept or random intercepts model is used. however, when using maximum likelihood (ml) estimation, there was less downward bias in the estimate of between-study variance of the treatment effect when using random intercepts rather than a stratified intercept, due to fewer parameters being <UNK> consequently, for ml estimation, the coverage of <UNK> confidence intervals for the summary treatment effect was better (ie, closer to <UNK> when random intercepts rather than a stratified intercept was used. a recommendation to use random study intercepts, rather than a stratified study intercept, for one-stage ipd meta-analysis models that require ml estimation (eg, for binary outcomes) may be disconcerting to some readers. in particular, the use of random intercepts is often considered inappropriate on philosophical grounds, because it allows across-trial information to inform the control group results, which may compromise randomization within each trial and bias the summary treatment effects. there is the potential for bias in situations when the allocation ratio is associated with the overall mean outcome (risk). in such situations the introduced bias will often be <UNK> but may be substantial in extreme situations. for example, white et <UNK> show that when using extreme hypothetical binary outcome data in a network meta-analysis setting, there can be large potential bias in the summary treatment effect when using a random intercept; the summary treatment effect was an odds ratio of <UNK> when the truth was <UNK> another issue is that it is usually recommended to allow the random effects on the intercept and treatment effect to be correlated; however, this might then allow the baseline risk to contribute toward the summary treatment effect estimates. as an extreme example, the model could incorporate randomized trials alongside observational studies that only provide information about the control (untreated group); the latter will then contribute (via the correlation) toward the summary treatment effect. in this article, we aim to build on previous <UNK> and to improve ml estimation of the stratified intercept model so that it is at least comparable to that of the random intercepts model. specifically, we focus on an ipd meta-analysis of randomized trials, and evaluate whether the coding of the treatment variable is important toward ml estimation properties. our previous simulations focused on situations where the treatment:control allocation ratio was <UNK> in each study in the meta-analysis, and found a <UNK> coding for the treatment variable (instead of <UNK> substantially improved ml estimation <UNK> subsequently, we realized that a <UNK> coding is the same as using the <UNK> coding minus the proportion in the treatment group (ie, using <UNK> - <UNK> when the treatment:control allocation ratio is <UNK> this raises the question about whether ipd meta-analysts should always use a <UNK> coding of treatment in their one-stage models, or whether the choice should be context specific, especially when the actual allocation ratio is not <UNK> therefore, in this article our primary aims to assess ml estimation performance of the stratified intercept model when using four treatment coding options: • the traditional <UNK> coding • the <UNK> coding recommended by jackson et <UNK> • a coding of <UNK> minus the average proportion of participants in the treatment group in all trials (ie, an “overall centering” approach) • a coding of <UNK> minus the proportion of participants in the treatment group in that trial (ie, a “study-specific centering” approach) we evaluate which coding approach gives the smallest bias in the estimates of the summary treatment effect and between-study variance of treatment effects. two additional objectives are also considered: (a) whether the coverage of <UNK> confidence intervals for the summary treatment effect are improved by using a t-distribution rather the standard z-based (wald) approach, and (b) whether reml estimation of the pseudo likelihood leads to better performance than ml estimation of the exact likelihood for one-stage ipd meta-analysis models of binary outcomes. the structure of this article is as follows. in section <UNK> we introduce one-stage ipd meta-analysis models with a stratified intercept, for both continuous and binary outcomes. in section <UNK> we evaluate the four treatment coding options in an extensive simulation study, for both continuous and binary outcomes. section <UNK> provides real examples and section <UNK> concludes with discussion. <UNK> one-stage model specifications and treatment coding options in this section we introduce one-stage ipd meta-analysis models for continuous and binary outcomes with either a stratified intercept or random intercepts model, and then define the various treatment coding options. <UNK> continuous outcomes consider that ipd have been obtained from i = <UNK> to k rcts, each of which has a parallel-group design investigating whether a treatment is effective (vs a control or existing treatment) at improving a continuous outcome. the treatment effect then relates to the mean difference (at some follow-up time) in the continuous outcome value between the treatment and control groups. suppose that there are ni participants in trial i, and that yfij represents the end-of-trial final (f) continuous outcome value for participant j in trial i. let the treatment group variable be denoted by xij, with coding options (such <UNK> for treatment/control groups) discussed further in section <UNK> in this situation, a one-stage ipd meta-analysis with a stratified study intercept (ie, a separate intercept per study to account for within-study clustering of individuals) and assuming between-study heterogeneity of the treatment effect, can be written as follows: y𝐹 𝑖𝑗 = 𝛼i + (𝜃 + ui)x𝑖𝑗 + e𝑖𝑗 ui ∼ <UNK> <UNK> ) e𝑖𝑗 ∼ <UNK> <UNK> i ). <UNK> here the outcome value (yfij) is assumed normally distributed in each study conditional on the included covariates (here just xij, but additional covariates could also be included such as the baseline value of the continuous <UNK> there are k distinct intercept terms (𝛼i) and the main parameter of interest is 𝜃, which denotes the summary (average) treatment effect from the included studies. the true treatment effect in each study is assumed drawn from a normal distribution with mean 𝜃 and between-trial variance <UNK> and <UNK> i denotes the study-specific residual variance which is assumed normally distributed (this could also be stratified by treatment group, but we do not consider this here). the choice of coding of xij (eg, <UNK> or <UNK> for treatment/control groups) does not alter the interpretation of 𝜃, our key parameter of interest; however, it does change interpretation of the 𝛼i and may have implications on estimation of <UNK> as discussed by jackson et <UNK> we evaluate this later in our simulations. alternatively, a random intercepts model could be specified. for example, allowing for between-study correlation between the random effects of the intercept and the treatment effect, the model can be written as follows: y𝐹 𝑖𝑗 = (𝛼 + <UNK> + <UNK> + e𝑖𝑗 ( <UNK> <UNK> ) ∼ n <UNK> 𝛼 <UNK> <UNK> <UNK> ) e𝑖𝑗 ∼ <UNK> <UNK> i ) <UNK> the parameter terms are as defined for model <UNK> except now the study-specific intercepts are also assumed drawn from a normal distribution, with mean of 𝛼 and between trial variance of <UNK> 𝛼 , and the two random effects <UNK> and <UNK> are allowed to be correlated through the covariance term <UNK> allowing for correlation imposes a between-study relationship of control group mean response and treatment effect, which might be viewed as controversial (see discussion). to avoid this, <UNK> might be set to zero, but then the coding of treatment is potentially crucial (see <UNK> in a frequentist framework, models <UNK> and <UNK> are typically fitted using reml estimation. following estimation, a <UNK> confidence interval for 𝜃 is conventionally derived using a (wald) z-based method (𝜃 ̂ ± <UNK> × s.e.(𝜃 ̂))), but other options include the satterthwaite and kenward-roger approaches, which replace <UNK> with the critical value of a t-distribution with a particular denominator degrees of <UNK> in this article, we also consider using 𝜃 ̂ ± <UNK> × s.e.(𝜃 ̂)), where k is the number of studies in the ipd meta-analysis. for brevity, we refer to this as the t-based confidence interval approach. <UNK> binary outcomes now let us consider a binary outcome (eg, dead or alive <UNK> month after surgery), such that yij is <UNK> for individuals with an event and <UNK> for those without an event. we use a logit-link function, such that our one-stage models have a logistic regression modeling framework (as suggested by simmonds and <UNK> and the treatment effect is measured by a log odds ratio. again let the treatment group variable be denoted by xij, with coding options (such <UNK> for treatment/control groups) discussed further in section <UNK> in this situation, the stratified intercept model can be written as, y𝑖𝑗 ∼ bernoulli(𝜋𝑖𝑗) logit(𝜋𝑖𝑗) = 𝛼i + (𝜃 + ui)x𝑖𝑗 ui ∼ <UNK> <UNK> ), <UNK> where 𝜋ij is the event probability for individual j in study i. there are k distinct intercept terms, 𝛼i, and the model parameter 𝜃, denotes the summary (average) treatment effect (log odds ratio). the true treatment effects are again assumed drawn from a normal distribution with mean 𝜃, and between-trial variance <UNK> as in models <UNK> and <UNK> adjustment for baseline covariates is also possible. alternatively specifying a random intercepts model, and allowing for between-study correlation of control group risk and treatment effect, we <UNK> y𝑖𝑗 ∼ bernoulli(𝜋𝑖𝑗) logit(𝜋𝑖𝑗)=(𝛼 + <UNK> + <UNK> ( <UNK> <UNK> ) ∼ n <UNK> 𝛼 <UNK> <UNK> <UNK> ) . <UNK> the parameter terms are as defined for model <UNK> except now the study-specific intercepts are also assumed drawn from a normal distribution, with mean of 𝛼 and between trial variance of <UNK> 𝛼 , and the two random effects <UNK> and <UNK> are allowed to be correlated through the covariance term <UNK> as discussed for model <UNK> setting <UNK> to zero assumes no between-study correlation, but then treatment coding is more important (see below). models <UNK> and <UNK> are typically fitted using ml estimation, via a numerical integration approach such as (adaptive) gaussian quadrature. unfortunately, there is no natural extension from ml to reml estimation for the exact likelihood defined by a glmm of a binary, ordinal or count outcome, as the model residuals cannot be estimated separately from the main parameters. thus ml estimation is generally the default frequentist estimation choice for ipd meta-analyses of noncontinuous outcomes, for which downward bias in between-study variance estimates and low coverage of confidence intervals is a strong concern, especially with <UNK> or fewer studies in the ipd meta-analysis. however, wolfinger and o'connell suggest using a pseudo-likelihood approximation of the exact <UNK> where the outcome response variable is transformed to an approximately linear scale. this allows reml to be used for glmms of noncontinuous outcomes, but at the expense of an approximate likelihood. this may be an acceptable trade-off in some situations, to improve between-study variance estimates and confidence interval coverage. we will investigate this in section <UNK> <UNK> coding of treatment when a treatment variable is entered into a regression model as a covariate, it is typical practice for researchers to code the variable as <UNK> for treatment/control. however, a coding of <UNK> has also been used by others, such as morris et <UNK> tudur-smith et <UNK> and turner et <UNK> for random intercepts models <UNK> and <UNK> which allow for between-study correlation in control group risk and treatment effect, the choice of treatment coding should be unimportant, because one can show mathematically a one-to-one correspondence of the model parameters with one coding and the model parameters with another coding, so that the maximized likelihood is the <UNK> however, if the between-study correlation is set to zero, then turner et al suggest a <UNK> coding is crucial; in particular, for model <UNK> this ensures the variance of the log-odds in control group patients is modeled as equal to that in intervention <UNK> as otherwise with a <UNK> treatment/control coding the variation for the intervention group is modeled as greater than or equal to the variance for the control group. for stratified intercept models <UNK> and <UNK> jackson et <UNK> suggest a coding of <UNK> improves ml estimation. however, they mainly evaluated situations where the treatment:control allocations were <UNK> in each trial. therefore, in the following section we address unequal treatment:control allocations, and examine whether the following alternative treatment coding options improve ml estimation even further: • overall centering: a coding of <UNK> minus the average (unweighted across trials) of the proportion of participants in the treatment group in each trial. for example, if there are <UNK> studies within an ipd meta-analysis, with five of those studies having <UNK> of participants in the treatment group and the other five having <UNK> in the treatment group, then the unweighted average proportion treated per trial is <UNK> in this situation, the treatment coding is <UNK> - <UNK> in all trials, and thus individuals in the treatment group are coded as <UNK> and those in the control group are coded as <UNK> • study-specific centering: a coding of <UNK> minus the study-specific proportion of participants in the treatment group. for example, for individuals in a trial where <UNK> of participants are in the treatment group, then an individual in the treatment group will be coded as <UNK> = <UNK> and an individual in the control group would be coded as <UNK> = <UNK> however, if another trial had <UNK> in the treatment group, then individuals in the treatment and control groups would be coded as <UNK> and <UNK> respectively. <UNK> simulation study for continuous and binary outcomes we now use a simulation study to compare the performance of ipd meta-analysis models with stratified intercept or random intercepts, first for continuous outcomes and then for binary outcomes. we have three research questions: <UNK> does an “overall centering” or “study-specific centering” coding of the treatment variable improve ml estimation of the between-study variance of the treatment effect, over and above the <UNK> coding proposed by jackson et al, for the stratified intercept models <UNK> and <UNK> in situations where one or more trials have an unequal treatment:control allocation ratio? <UNK> does a t-based approach to confidence interval derivation improve coverage compared with a standard z-based (wald) approach, for both stratified intercept and random intercepts models? <UNK> does reml estimation of the pseudo-likelihood perform better than ml estimation of the exact likelihood for binary outcome models <UNK> and <UNK> <UNK> continuous outcome simulation study in our first simulation, we extend the simulation study of legha et al for one-stage ipd meta-analysis models of continuous outcomes to the situation when there are varying treatment:control allocation ratios. <UNK> methods full details of the simulation methods are provided in supplementary material <UNK> briefly, we simulated ipd according to model <UNK> with a <UNK> treatment/control coding and assuming no correlation of the pair of random effects (ie, <UNK> = <UNK> for simplicity to avoid a relationship between control group response and treatment effect. a range of different simulation scenarios were considered (see supplementary material <UNK> each involving varying treatment:control allocation ratios (randomly drawn from a <UNK> distribution), and varying the number of studies, number of participants, and magnitude of between-study variance of treatment effects. one thousand ipd meta-analysis datasets were generated for each scenario. to each we fitted the random intercepts model <UNK> used to generate the data; that is, model <UNK> using a <UNK> treatment coding option, whilst forcing <UNK> to be <UNK> (its correct value) to avoid estimation issues that often arise when estimating between-study <UNK> then we also fitted the stratified intercept model <UNK> for each of the four treatment coding options. both ml and reml estimation were examined, alongside z-based and sattherwaite confidence intervals. although reml is the preferred estimation method for one-stage ipd meta-analyses of continuous outcomes, we also considered ml estimation to inform subsequent extension to one-stage ipd meta-analyses of binary outcomes, for which ml estimation is usually the default (see section <UNK> performance was summarized by the bias in the summary treatment effect estimate (𝜃 ̂), the coverage of <UNK> confidence intervals for the summary treatment effect, and the bias in the between-study variance <UNK> of the treatment effects. for the latter, we calculated percentage difference between the estimated and true between-study variance of the treatment effect (ie, <UNK> × <UNK> − <UNK> and report the mean and median of these percentages across the <UNK> simulations. distributions of <UNK> were extremely skewed across each set of <UNK> results, and so presentation of median values helps indicate estimation problems in addition to the more formally correct mean bias. <UNK> results when using ml estimation the summary treatment effect estimates were approximately unbiased for all scenarios, modeling approaches, and treatment coding options. however, in most scenarios there was considerable downward bias in the estimated between trial variance <UNK> of the treatment effects (see figure <UNK> and also see supplementary material tables <UNK> (b), and (c)). the bias was largest when using the stratified intercept model <UNK> with a <UNK> treatment/control coding for the treatment variable, and the bias was least when using the “study-specific centering” coding. for example, under the stratified intercept model and setting <UNK> (which involves five trials where the number of participants per trial was <UNK> <UNK> the median downward bias of <UNK> was <UNK> with <UNK> treatment/control coding, which improved to <UNK> <UNK> and <UNK> with <UNK> coding, an “overall centering” coding, and a “study-specific centering” coding, respectively. coverage of <UNK> confidence intervals for the summary treatment effect was also closest to <UNK> when using the “study-specific centering” approach. for example, in scenario <UNK> the stratified intercept model had a z-based confidence interval coverage of <UNK> <UNK> and <UNK> for the <UNK> <UNK> and “study-specific centering” codings, respectively (table <UNK> crucially, “study-specific centering” of the treatment variable not only improves ml estimation of stratified intercept model <UNK> but also makes it comparable (in terms of bias and coverage) to ml estimation of the data generating model <UNK> (ie, the random intercepts model with <UNK> coding). however, despite having the best performance, both approaches still have downward bias in <UNK> and gave z-based confidence interval coverage <UNK> for most scenarios. this appears worst in situations where the allocation ratio was most unbalanced (eg, see results in table <UNK> where treatment prevalence was <UNK> in all studies). <UNK> results when using reml estimation reml estimation also gives approximately unbiased summary treatment effect estimates for all scenarios, modeling approaches, and treatment coding options. it also reduces the downward bias in the ml estimate of <UNK> for all modelling options (although the downward bias was not removed entirely). furthermore, unlike for ml estimation, the choice of treatment coding becomes irrelevant when fitting the stratified intercept model <UNK> using reml estimation. the median (or mean) bias in <UNK> was generally very similar regardless of the coding used (see supplementary material table <UNK> as were the summary treatment effect estimates and their confidence intervals. therefore, when using reml estimation of random, <UNK> stratified, <UNK> stratified, <UNK> stratified, average centred coding: stratified, study-specific centred coding: random, <UNK> stratified, <UNK> stratified, <UNK> stratified, average centred coding: stratified, study-specific centred coding: random, <UNK> stratified, <UNK> stratified, <UNK> stratified, average centred coding: stratified, study-specific centred coding: random, <UNK> stratified, <UNK> stratified, <UNK> stratified, average centred coding: stratified, study-specific centred coding: random, <UNK> stratified, <UNK> stratified, <UNK> stratified, average centred coding: stratified, study-specific centred coding: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> base case <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> median % bias in <UNK> figure <UNK> the median percentage bias of the between-trial variance of treatment effects <UNK> for ml estimation of the stratified intercept model <UNK> and random intercepts model <UNK> for the continuous outcome simulation scenarios** described in table <UNK> allowing for unequal treatment:control allocation ratios in each trial in the ipd meta-analysis from <UNK> to <UNK> circular points denote the estimated percentage bias, and a horizontal line is drawn from each estimate to the ideal value of <UNK> ipd, individual participant data; ml, maximum likelihood. *the random intercepts model refers to the data generating model, which was model <UNK> but with treatment coded as <UNK> and the between-study correlation assumed zero. **scenarios are labeled “base case,” <UNK> <UNK> and so on. for explanation of each scenario setting, see table <UNK> briefly, the base case was k <UNK> ni <UNK> 𝜃 = <UNK> <UNK> <UNK> then the other scenarios made changes of: <UNK> k <UNK> <UNK> k <UNK> <UNK> ni ∼ <UNK> <UNK> k <UNK> ni ∼ <UNK> for trials <UNK> to <UNK> ni ∼ <UNK> for trials <UNK> to <UNK> <UNK> k <UNK> and ni ∼ <UNK> <UNK> k <UNK> and ni ∼ <UNK> <UNK> ni ∼ <UNK> for trials <UNK> and <UNK> ni ∼ <UNK> for trials <UNK> to <UNK> <UNK> k <UNK> ni ∼ <UNK> for trials <UNK> to <UNK> ni ∼ <UNK> for trials <UNK> to <UNK> <UNK> ni ∼ <UNK> <UNK> halving variance of intercept; <UNK> doubling variance of intercept; <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> the stratified intercept model <UNK> “study-specific centering” of the treatment variable does not improve performance over traditional <UNK> coding, and the coding is irrelevant. results from the stratified intercept model were also comparable to reml estimation of the random intercepts model <UNK> with <UNK> coding. coverage of z-based <UNK> confidence intervals for the summary treatment effect were generally too low, but improved close to <UNK> when using the satterthwaite approach (as also shown by legha et <UNK> <UNK> binary outcome simulation study in our second simulation study we extend the simulations of jackson et al for ipd meta-analysis of binary <UNK> to evaluate the ml estimation performance of random intercepts model <UNK> with a <UNK> coding and the stratified intercept table <UNK> simulation study scenarios of jackson et <UNK> that were extended in this article, by allowing for unequal treatment:control allocation ratios in each trial in the ipd meta-analysis data generation scenario, as labeled by jackson et al k <UNK> number of participants in the treatment group (n) number of participants in the control groupa baseline log-odds of the event in the control group (loc) <UNK> <UNK> <UNK> n ∼ <UNK> <UNK> n loc <UNK> <UNK> <UNK> <UNK> <UNK> n ∼ <UNK> <UNK> n loc <UNK> <UNK> <UNK> <UNK> <UNK> n ∼ <UNK> <UNK> n loc <UNK> <UNK> <UNK> <UNK> <UNK> n ∼ <UNK> <UNK> n loc <UNK> <UNK> <UNK> <UNK> <UNK> n ∼ <UNK> <UNK> n loc <UNK> <UNK> <UNK> <UNK> <UNK> n ∼ <UNK> <UNK> n loc <UNK> <UNK> note: all scenarios were performed with 𝜃 <UNK> and 𝜃 =log <UNK> for further details see section <UNK> in jackson et al. abbreviation: ipd, individual participant data. ashows the number used in the control group of the jackson et al simulation. however, in our simulations we changed the number in the control group of a trial to ensure the treatment group prevalence was one of the following (chosen at random): <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> or <UNK> model <UNK> for each of the four treatment coding options. a variety of simulation settings are considered, allowing for unequal treatment:control allocation ratios in each trial within the ipd meta-analysis. we examine the estimate of the between-study variance <UNK> of the treatment effect, the estimate of the summary treatment effect (𝜃 ̂), and the coverage of the <UNK> confidence interval for the summary treatment effect. <UNK> methods we chose the simulation scenarios from jackson et <UNK> that most closely correspond to those used for our continuous outcome simulations; that is, jackson et al scenario settings <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> where the mean risk in the control group was <UNK> these are summarized in table <UNK> and cover a range of settings that vary in terms of the number of trials, number of participants per trial, and heterogeneity of parameters. we now consider these scenarios when we vary the treatment:control allocation ratio in the simulated trials. the omitted jackson et al scenarios mainly corresponded to either zero or extremely large between-study heterogeneity, or a different logit data generating mechanism for the control group. for each scenario, <UNK> simulated ipd meta-analysis datasets were created from model <UNK> with a <UNK> treatment/control coding, but assuming no correlation of the pair of random effects (ie, <UNK> = <UNK> for simplicity to avoid a relationship between baseline risk and treatment effect. within each scenario we allowed for potential unequal treatment:control allocation ratios in each trial, by randomly drawing from a <UNK> distribution. each scenario was undertaken assuming the summary odds ratio of <UNK> for the treatment effect (ie, 𝜃 = ln <UNK> <UNK> and also assuming a summary odds ratio of <UNK> (ie, 𝜃 = ln <UNK> of note, the chosen magnitude of heterogeneity in setting <UNK> corresponded to a mean <UNK> of about <UNK> in the meta-analysis datasets simulated. to each simulated ipd meta-analysis dataset, ml estimation was used to fit the random intercepts model used to generate the data (ie, model <UNK> with a <UNK> coding option and forcing <UNK> = <UNK> to avoid estimation issues that often arise when estimating between-study <UNK> and the stratified intercept model <UNK> for each of the four treatment coding options. we used the <UNK> r package (version <UNK> with ml estimation undertaken using adaptive gaussian quadrature, with seven quadrature points. across all <UNK> results obtained for each scenario, the median percentage bias of the between-study variance was calculated, as well as the mean bias of the summary treatment effect, and the coverage of <UNK> confidence intervals for the summary treatment effect. the latter was derived as the proportion (across the <UNK> results) of <UNK> confidence intervals that contained the true summary effect. confidence intervals were calculated using the standard z-based method (𝜃 ̂ ± <UNK> × s.e.(𝜃 ̂))) and also by the t-based approach that replaces <UNK> with the critical value of a t-distribution with <UNK> degrees of freedom (ie, use 𝜃 ̂ ± <UNK> × s.e.(𝜃 <UNK> given the <UNK> simulations in each scenario, if coverage is truly <UNK> then we would expect to observe an estimated coverage between <UNK> and <UNK> finally, we repeated our simulations using reml estimation of the pseudo likelihood. given that all treatment coding options gave similar performance for reml estimation of continuous outcomes (section <UNK> we only considered the <UNK> treatment variable coding when fitting the stratified intercept and random intercepts models. reml estimation was undertaken using mlwin, via the runmlwin package within <UNK> we obtained parameter estimates using reml estimation of the first-order marginal quasi-likelihood linearization of the likelihood <UNK> option). this linearization approach is noted in the runmlwin help file as being the most stable and fastest to converge, and so was deemed sensible for our large simulation study. in practice, more accurate (though potentially less stable) estimation options such as second-order penalized quasi-likelihood linearization <UNK> option) could be used, with the estimates from the first-order approach used as initial values (see applied examples in section <UNK> for further discussion on this). <UNK> results when using ml estimation of the exact likelihood and standard z-based confidence intervals simulation results for the mean bias of the summary treatment effect estimate (𝜃 ̂) across all scenarios for a true treatment effect of 𝜃 <UNK> and of 𝜃 <UNK> are shown in supplementary tables <UNK> and <UNK> respectively. the bias of 𝜃 ̂was generally negligible in all cases. figures <UNK> and <UNK> show the median percentage bias in the between-study variance of the treatment effect <UNK> for the 𝜃 <UNK> and 𝜃 = ln <UNK> scenarios, respectively (and also supplementary tables <UNK> and <UNK> show the mean percentage bias). intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> median % bias for <UNK> figure <UNK> the median percentage bias of the between-trial variance of treatment effects <UNK> ml estimation (exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model <UNK> and random intercepts model <UNK> for simulation scenarios** where 𝜃 <UNK> and allowing for random treatment prevalences <UNK> for each study within the ipd meta-analysis. circular points denote the estimated percentage bias, and a horizontal line is drawn from each estimate to the ideal value of <UNK> ipd, individual participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model, which was model <UNK> with treatment coded as <UNK> and the between-study correlation assumed zero. **see table <UNK> for full details of the scenario corresponding to the number shown. true value for <UNK> is <UNK> except in setting <UNK> where <UNK> equals <UNK> all settings also allow the treatment prevalence for a particular study within a meta-analysis to vary, whereby this is selected from <UNK> and then rounded to the nearest <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> median % bias for <UNK> figure <UNK> the median percentage bias of the between-trial variance of treatment effects <UNK> ml estimation (exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model <UNK> and random intercepts model <UNK> for simulation scenarios** where 𝜃 = ln <UNK> and allowing for random treatment prevalences <UNK> for each study within the ipd meta-analysis. circular points denote the estimated percentage bias, and a horizontal line is drawn from each estimate to the ideal value of <UNK> ipd, individual participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model, which was model <UNK> with treatment coded as <UNK> and the between-study correlation assumed zero. **see table <UNK> for full details of the scenario corresponding to the number shown. true value for <UNK> is <UNK> except in setting <UNK> where <UNK> equals <UNK> all settings also allow the treatment prevalence for a particular study within a meta-analysis to vary, whereby this is selected from <UNK> and then rounded to the nearest <UNK> decimal place as observed for continuous outcomes, the results show that using the stratified intercept model <UNK> with a <UNK> treatment/control coding gives the most downwardly biased estimates of the between trial variance of treatment effect. often the median downward bias was <UNK> and mean downward bias typically between <UNK> and <UNK> this downward bias was generally reduced when using either a <UNK> treatment coding or the “overall centering” treatment coding, and by a similar amount. however, the downward bias was smallest when using a “study-specific centering” coding. for example, under setting <UNK> (involving <UNK> trials), the median downward bias of between study variance estimates from stratified intercept model <UNK> was <UNK> with a <UNK> treatment/control coding; this was slightly reduced to <UNK> and <UNK> with <UNK> coding or “overall centering” coding, but considerably reduced to <UNK> when using the “study-specific centering” coding. the mean downward bias shows a similar pattern; this was <UNK> <UNK> <UNK> and <UNK> when using the <UNK> <UNK> “overall centering,” and “study-specific centering,” respectively (table <UNK> for stratified intercept model <UNK> the reduction in downward median and mean bias of <UNK> when using a “study-specific centering” coding of treatment also improves upon the z-based coverage of <UNK> confidence intervals for the summary treatment effect, as shown in figures <UNK> and <UNK> (and also supplementary tables <UNK> and <UNK> for the 𝜃 <UNK> and 𝜃 = ln <UNK> scenarios, respectively. for example, in setting three of figure <UNK> the coverage of z-based confidence intervals from the <UNK> coding is <UNK> but this improves to <UNK> when using the “study-specific centering” coding. furthermore, the z-based intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> z-based t-based ci coverage for θ figure <UNK> coverage (proportion) of z-based and t-based <UNK> confidence intervals for the summary treatment effect for ml estimation (exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model <UNK> and random intercepts model <UNK> for simulation scenarios** where 𝜃 <UNK> and allowing for random treatment prevalences <UNK> for each study within the ipd meta-analysis. circular points denote the estimated coverage, and a horizontal line is drawn from each estimate to the ideal value of <UNK> ipd, individual participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model, which was model <UNK> with treatment coded as <UNK> and the between-study correlation assumed zero. ** see table <UNK> for full details of the scenario corresponding to the number shown. true value for <UNK> is <UNK> except in setting <UNK> where <UNK> equals <UNK> all settings also allow the treatment prevalence for a particular study within a meta-analysis to vary, whereby this is selected from <UNK> and then rounded to the nearest <UNK> coverage is always best (ie, closest to <UNK> when using the “study-specific centering” coding, and also worst when using the <UNK> coding; the coverage of the <UNK> and “overall centering” coding are similar and fall in between the z-based coverage when using the <UNK> coding and “study-specific centering” coding. crucially, “study-specific centering” of the treatment variable not only improves ml estimation of stratified intercept model <UNK> but also makes it comparable to ml estimation of the random intercepts model <UNK> with <UNK> coding (ie, bias and coverage are very similar). however, despite having the best performance, both approaches still have (often considerable) downward bias in <UNK> and subsequently z-based coverage is less than <UNK> for most scenarios. the coverage appears closest to <UNK> in the scenario <UNK> setting (see figure <UNK> this can be explained by the studies being smaller in this setting, and so the within-study variances dominate the total variability (ie, <UNK> is small), and so any downward bias in the between-study variance is less impactful. <UNK> results when using ml estimation and t-based confidence intervals for all treatment coding options, and for both stratified intercept and random intercepts models, coverage of <UNK> confidence intervals for the summary treatment effect was generally improved (ie, moved closer to <UNK> by using the intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> intercept estimation treatment coding stratified ml <UNK> stratified ml <UNK> stratified ml avg. centering stratified ml study-specific centering random ml <UNK> stratified reml <UNK> random reml <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> z-based t-based ci coverage for θ figure <UNK> coverage (proportion) of z-based and t-based <UNK> confidence intervals for the summary treatment effect for ml estimation (exact likelihood) and reml estimation (pseudo likelihood) of the stratified intercept model <UNK> and random intercepts model <UNK> for simulation scenarios** where 𝜃 = ln <UNK> and allowing for random treatment prevalences <UNK> for each study within the ipd meta-analysis. circular points denote the estimated coverage, and a horizontal line is drawn from each estimate to the ideal value of <UNK> ipd, individual participant data; ml, maximum likelihood; reml, restricted maximum likelihood. *the random intercepts model refers to the data generating model, which was model <UNK> with treatment coded as <UNK> and the between-study correlation assumed zero. **see table <UNK> for full details of the scenario corresponding to the number shown. true value for <UNK> is <UNK> except in setting <UNK> where <UNK> equals <UNK> all settings also allow the treatment prevalence for a particular study within a meta-analysis to vary, such that it is selected from <UNK> <UNK> and then rounded to the nearest <UNK> t-based approach to deriving confidence intervals based on the t-distribution with <UNK> degrees of freedom (see tables <UNK> and <UNK> for example, in setting <UNK> when using stratified intercept model <UNK> with “study-specific centering” of the treatment variable, the coverage was <UNK> and <UNK> for z-based and t-based confidence intervals, respectively. only in setting <UNK> where the number of studies was only <UNK> is the t-based approach a concern as the coverage is close to <UNK> and so too high; although arguably this is still preferable to the under-coverage from the corresponding z-based approach. <UNK> results when using reml estimation of the pseudo likelihood when using reml estimation of the pseudo-likelihood with a <UNK> treatment coding, results are shown in table <UNK> for all scenarios. stratified intercept model <UNK> and random intercepts model <UNK> have similar performance in all scenarios, and there is negligible or very small bias in the summary treatment effect estimates. compared with ml estimation of the exact likelihood, reml estimation of the pseudo likelihood improved the between-study variance estimate <UNK> for both stratified intercept and random intercepts models. for example, in setting <UNK> and 𝜃 = ln <UNK> when using stratified model <UNK> the median (mean) bias in <UNK> was <UNK> <UNK> using reml box <UNK> a summary of the key findings based on our simulation study results and applied examples • for ml estimation of a one-stage ipd meta-analysis model with a stratified intercept, a “study-specific centering” coding of the treatment variable reduces downward bias of between-study variances and improves coverage of <UNK> confidence intervals for the summary (treatment) effect, as compared with other treatment coding options such as <UNK> for treatment/control. supplementary material <UNK> also shows this mathematically for a simple case where all studies in the ipd meta-analysis are of the same size. • reml is better than ml estimation for continuous outcomes. for binary outcomes, the simulations do not suggest an important difference in terms of bias and coverage of confidence intervals for the summary treatment effect when using reml estimation of the pseudo likelihood compared with ml estimation of the exact likelihood. however, in most scenarios reml reduces the downward bias in the between-study variance estimates, which may be important when the focus is on predictive inferences (eg, the predicted treatment effect in a new <UNK> thus both ml (exact likelihood) and reml (pseudo likelihood) estimation may be important to consider for binary outcomes. • for either ml or reml estimation, coverage of <UNK> confidence intervals for the summary treatment is generally too low when using a z-based approach. improvements are generally made for reml estimation of continuous outcomes by using satterthwaite or kenward-roger approaches, and for ml or pseudo reml estimation of binary outcomes by using <UNK> × s.e.(̂θ)) where k is the number of studies in the meta-analysis. • for continuous outcomes, reml estimation is recommended over ml estimation for either stratified intercept or random intercepts models (see work of legha et <UNK> as it improves estimates of between-study variances (though some downward bias may remain), whilst having negligible bias in the summary treatment effect estimate and does not depend on the treatment coding chosen. • for binary outcomes, when fitting a stratified intercept model both ml estimation of the exact likelihood (with “study-specific centering” treatment coding) and reml estimation of the pseudo likelihood (with <UNK> treatment coding) give negligible bias in the summary treatment effect estimate, and their coverage of <UNK> confidence intervals is close to <UNK> when using the t-based approach (unless the number of studies is less than <UNK> in addition, reml estimation of the pseudo likelihood often has less downward bias of between-study variance estimates than ml estimation. • for binary outcomes, reml estimation of the pseudo likelihood may be unstable in sparse data situations, such as when most studies in the ipd meta-analysis are small (in terms of participants or events). • the decision to use random study intercepts, rather than a stratified study intercept, depends on whether the researcher is willing to borrow information about control group risk across studies and/or assume a between-study relationship of control risk and treatment effect. estimation (with a <UNK> treatment/control coding) compared with <UNK> <UNK> when using ml estimation (with a “study-specific centering coding” for treatment). the coverage of confidence intervals from reml estimation were closest to <UNK> when using the t-based approach; for example, in setting <UNK> with 𝜃 = ln <UNK> the coverage from z-based and t-based confidence intervals was <UNK> and <UNK> respectively. indeed, t-based coverage was consistently good in all settings, generally between <UNK> and <UNK> and comparable to that when using ml estimation with “study-specific centering” and the t-based approach. <UNK> summary of our key findings a summary of key findings from the simulation study is shown in box <UNK> <UNK> illustration of key findings in applied examples we now illustrate the key findings in applied examples, which focus on binary outcomes. example <UNK> has data similar to that used in the simulation studies, whilst example <UNK> considers more sparse data. table <UNK> summary of the ipd from seven trials examining the effect of hormone replacement therapy on the incidence of heart disease, as reported by simmonds and <UNK> number of women number of cardiovascular disease events study control treatment control treatment <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> abbreviation: ipd, individual participant data. table <UNK> results from ml estimation of models <UNK> and <UNK> when fitted to the ipd summarized in table <UNK> model treatment coding summary treatment effect, 𝜽̂ <UNK> ci: z-based <UNK> ci: t-based between-study (co)variances stratified intercept model <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> “study-specific centering” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> random intercepts model <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> <UNK> = <UNK> <UNK> 𝛼 = <UNK> abbreviations: ipd, individual participant data; ml, maximum likelihood. <UNK> example <UNK> hormone replacement therapy and incidence of heart disease simmonds et al combined ipd from seven trials examining the effect of hormone replacement therapy compared with control on the incidence of heart <UNK> the binary outcome data are sparse (table <UNK> such that the number of events is few in all studies due to the outcome being rare, and some groups have zero events. in this situation, a traditional two-stage ipd meta-analysis (ie, estimating the treatment effect and its variance in each study separately, and then pooling the results in an inverse-variance weighted meta-analysis) is problematic. the treatment effect cannot be estimated in every study unless a continuity correction is applied in those studies with a zero event; further, the assumption in the second stage that study-specific treatment effect estimates are normally distributed with known variances may be inappropriate. a one-stage approach avoids these issues by analyzing the ipd in a single step, for example, using either stratified intercept model <UNK> or random intercepts model <UNK> and the results are shown in table <UNK> <UNK> stratified intercept model results one of the studies in the ipd meta-analysis (study <UNK> had a treatment:control allocation ratio of about <UNK> whereas other studies have close to a <UNK> allocation ratio. our simulation results showed that in this situation ml estimation of model <UNK> is improved by using a “study-specific centering” of the treatment variable, as this reduces downward bias in between-study variance estimates compared with a traditional <UNK> coding. the ml estimates in table <UNK> reflect this, as <UNK> = <UNK> when using a <UNK> coding and <UNK> = <UNK> when using “study-specific centering” coding. this led to a noticeably different summary treatment effect of 𝜃 ̂ = <UNK> (odds ratio of <UNK> when using “study-specific centering” compared with 𝜃 ̂ = <UNK> (odds ratio of <UNK> when using <UNK> coding. confidence intervals were also much wider. another key finding of the simulation study was that z-based (wald) confidence intervals are generally too narrow, and t-based confidence intervals are more appropriate. in our example, t-based confidence intervals were also considerably wider. for example, when using the “study-specific centering” coding for ml estimation of model <UNK> the confidence interval for the summary odds ratio was <UNK> to <UNK> when using t-based, compared with <UNK> to <UNK> when using the z-based approach. although our simulations suggest reml estimation of the pseudo likelihood for model <UNK> performs well, the scenarios did not cover sparse data akin to that in table <UNK> indeed, when applying reml estimation to this example, parameter estimates were unstable; there were large differences in parameter estimates from first and second-order linearization of the likelihood, and even when changing the coding of treatment (which should not occur for reml; see section <UNK> therefore, the ml estimates in table <UNK> based on the exact likelihood are more reliable for this example. of note, these ml estimates were very different to those obtained from a traditional two-stage ipd meta-analysis with continuity corrections of <UNK> added to deal with zero cells. the latter gave a summary odds ratio of <UNK> and <UNK> = <UNK> from reml estimation, which are much lower than the ml estimates from the more exact one-stage model using “study-specific centering.” <UNK> comparison of results for stratified intercept and random intercepts models unlike in the simulation study, the ml estimation results for random intercepts model <UNK> with <UNK> coding were not comparable to those from stratified intercept model <UNK> with “study-specific centering” coding (table <UNK> the reason is that the simulation did not allow borrowing of information between baseline (control group) risk and treatment effect when generating the ipd. however, in the applied example model <UNK> estimated a strong negative correlation of <UNK> between the pair of random effects, which had a strong influence on the results. furthermore, in our simulation study we knew that a normal distribution on the baseline risk was appropriate (as we simulated the ipd from this assumption). however, in this real example we did not know if such an assumption is correct, and it may even compromise randomization in each trial, especially given the sparse events in the included trials. the approach of stratified intercept model <UNK> avoids making any assumptions about the between-study distribution of baseline risk, or the between-study relationship between baseline risk and treatment effect. <UNK> example <UNK> diet and lifestyle interventions and health outcomes during pregnancy in our second example, we used ipd from <UNK> randomized trials <UNK> <UNK> women) evaluating the effect of diet and lifestyle interventions compared with control (usual care) on health outcomes during <UNK> we focused on a subset of <UNK> trials that recorded the binary outcome of large for gestational age (yes/no). eight studies had approximately <UNK> treatment:control allocation, and the other two had a <UNK> allocation ratio. the outcome risk in the control group varied, but was about <UNK> on average, similar to that used in our simulation studies. although the number of participants was reasonably large in most studies, often there are fewer than <UNK> events in each group (table <UNK> which again raised doubt as to the suitability of a traditional two-stage approach. ml and reml estimates for one-stage model <UNK> are shown in table <UNK> the findings again mirror those of the simulation study. when using ml estimation for model <UNK> the estimated between-study variance was much larger when using “study-specific centering” <UNK> = <UNK> rather than <UNK> coding <UNK> = <UNK> of the treatment variable, and this led to wider confidence intervals for the summary treatment effect. reml estimation of the pseudo likelihood was quite stable, with more similar estimates for first- and second-order linearizations of the likelihood. the reml estimates of between-study variances were larger than the ml estimates (table <UNK> and this widened confidence intervals for the summary treatment effect. results for model <UNK> were again somewhat different to model <UNK> for the same reasons described in the previous example. for all models, the widest confidence intervals arose when using the t-based rather than z-based approach. <UNK> discussion our simulation study and applied examples identify key findings for estimation of one-stage ipd meta-analysis models, which are summarized in box <UNK> there are three major implications. first, for ml or reml estimation of stratified intercept or random intercepts models, z-based (wald) confidence intervals for the summary treatment effect are generally too narrow; performance is generally improved by using the satterthwaite (or kenward-roger) approaches for continuous <UNK> or a t-based approach with <UNK> degrees of freedom for binary outcomes. second, when using ml table <UNK> summary of the ipd from <UNK> trials examining the effect of diet and lifestyle interventions on large for gestational age number of women number of babies large for gestational age study control treatment control treatment <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> abbreviation: ipd, individual participant data. table <UNK> results from maximum likelihood (ml) and restricted maximum likelihood (reml) estimation of models <UNK> and <UNK> when fitted to the individual participant data summarized in table <UNK> model estimation method treatment coding summary treatment effect, 𝜽̂ <UNK> ci:z-based <UNK> ci:t-based between-study (co)variances stratified intercept model <UNK> ml exact <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> ml exact “study-specific centering” <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> reml pseudo <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> random intercepts model <UNK> ml exact <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> <UNK> = <UNK> <UNK> 𝛼 = <UNK> reml pseudo <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> = <UNK> <UNK> = <UNK> <UNK> 𝛼 = <UNK> note: ml estimates-based numerical quadrature of the exact likelihood (with seven quadrature points), and reml estimates based on a second-order penalized quasi-likelihood linearization with the estimates from the first-order marginal quasi-likelihood linearization used as initial values. estimation of a one-stage model with a stratified intercept, a “study-specific centering” coding of the treatment variable should be chosen, as this reduces the bias in the between-study variance estimate (compared with <UNK> and other coding options). third, reml estimation reduces downward bias in between-study variance estimates compared with ml estimation, and does not depend on the treatment coding chosen, thus should be used where possible; for ipd meta-analyses of binary outcomes, this requires reml estimation of the pseudo-likelihood, although it may be unstable when data are sparse. <UNK> reml vs ml estimation our simulations of continuous outcomes show that reml is better than ml estimation to improve variance estimates, which will not be a surprise to most readers. in particular, reml reduces the bias in <UNK> by adjusting for the total number of parameters being <UNK> however, reml is not an option when using numerical integration of the exact likelihood for a one-stage ipd meta-analysis of a binary outcome, and therefore ml estimation is the most common method used. in most software packages the default estimation method to fit generalized linear mixed models is ml estimation via a numerical integration method such as quadrature. therefore, our findings about the importance of “study-specific centering” coding are most relevant for one-stage ipd meta-analyses of binary outcomes, or other generalized linear mixed models or frailty models that apply ml estimation. indeed, improving ml estimation by centering covariates has a reml essence to it, as both approaches aim to disentangle (ie, make uncorrelated) the estimation of main parameters of interest from other nuisance parameters. given that considerable downward bias in between-study variance estimates often occurs using ml estimation (even after “study-specific” treatment coding; see figures <UNK> to <UNK> reml estimation of the pseudo likelihood is appealing for binary <UNK> our simulations do not suggest an important difference between reml and ml in terms of bias of the summary treatment effect estimate and coverage of t-based confidence intervals for the summary treatment effect. however, in most scenarios reml estimation did reduce the median downward bias in the between-study variance estimates, and therefore we suggest it is the default. however, caution is advised if the data are sparse, such that most studies are small and have few or even zero events. our simulations did not cover scenarios with sparse data, but previous work suggests that reml estimation of pseudo likelihood is not accurate in such situations and ml estimation of the exact likelihood is <UNK> indeed, reml estimates may be unstable in such situations. instability is evident when first-order and second-order linearizations of the likelihood lead to very different parameter estimates, or when reparametizations that should not affect reml (such as centering of covariates) do still change parameter estimates importantly. in our applied example using the trials in table <UNK> the data were sparse, and these stability problems were evident when using reml estimation, and so the ml estimates with “study-specific centering” were deemed more reliable. a bayesian approach could also be considered in such situations, which would retain the exact likelihood during parameter estimation and could be combined with empirically based prior distributions for the between-study <UNK> our findings warrant further evaluation in a wider variety of settings than those considered in our simulation, but concur with related <UNK> including piepho et <UNK> who evaluate frequentist network meta-analysis of binary outcomes. they too show that reml estimation of the pseudo-likelihood, and also the use of the h-likelihood, reduce bias in between-study variance estimates and give satisfactory coverage rates, especially when the kenward-roger approach is used to derive confidence intervals. they also consider improving ml estimation by various reparameterizations of the exact likelihood that aim to mimic the reml approach for linear mixed models. these reparameterizations also reduce the downward bias in ml estimates, but coverage of summary treatment effects is often too low. our “study-specific centering of covariates” approach showed suitable coverage when using the t-based approach to confidence intervals, and is potentially simpler to implement in existing software. however, formal comparison of our proposal with those of piepho et al is needed. thomas et al also compare the performance of one-stage ipd meta-analyses for binary <UNK> but do not find a “meaningful difference” between results from reml of the pseudo likelihood and ml of the exact likelihood. however, the authors only considered <UNK> treatment:control allocations and implemented a <UNK> treatment variable coding, and thus essentially adapted “study-specific centering” in their setting, which ensures ml estimation performs well. still, their bias in between-study variances were generally lower using reml, akin to our findings. coverage of confidence intervals was generally much lower than <UNK> but were based on a z-based approach. comparisons to the traditional two-stage ipd meta-analysis approach are also needed, for which reml is often recommended in the second <UNK> although it assumes normality of the between-study variance of treatment effects, reml is quite robust to deviations from this <UNK> we suspect that situations where reml estimation of the pseudo-likelihood performs well for a one-stage analysis of binary outcomes, a two-stage approach using reml will also perform well. thomas et <UNK> recommend one-stage rather than two-stage analyses when data in the ipd meta-analysis are sparse. we agree with langan et <UNK> that, especially in ipd meta-analyses of few studies, any heterogeneity variance estimate “should not be used as a reliable gauge for the extent of heterogeneity in a meta-analysis”. <UNK> implications of our findings our findings have important consequences, as they allow researchers to apply one-stage ipd meta-analysis models with a stratified intercept, rather than random intercepts, when using either reml or ml estimation. this is important, as the use of random intercepts makes distributional assumptions and potentially compromises within-trial randomization, but this is avoided using a stratified intercept. previously, we recommended random intercepts for one-stage ipd meta-analysis models fitted using ml <UNK> as this approach had better estimates of between-study variances due to reducing the number of parameters. however, our simulations show that the “study-specific centering” makes the stratified intercept model comparable to the random intercepts model (when no borrowing of information across studies is allowed in control group risk). such comparable performance is despite the data generating mechanism actually being based on the random intercepts model, and so the simulation set-up might be considered more favorable toward the random intercepts model. a potential limitation of stratified intercept models is that they may fail to converge when the number of events are rare, and in particular when some studies have a zero event in the control group (although this was not an issue for our first applied example). in that situation, assuming random study intercepts may help, because study-specific intercepts are not estimated directly, and studies rather contribute toward the estimation of the between-study distribution of intercepts (with the caveat of sharing information about control group risk across trials and thus potentially compromising randomization within trials). if between-study correlation is included in such random intercepts models (ie, model <UNK> is used), then the coding of treatment should not matter. however, if between-study correlation is assumed zero, then a <UNK> coding is recommended. an alternative method is the hypergeometric-normal approach of stijnen et <UNK> (referred to as model <UNK> in jackson et <UNK> which conditions out the study-specific intercepts (thus avoiding their estimation); in the scenarios of the jackson et al simulation study, this approach performs well and comparable to using a stratified intercept with “study-specific centering”. <UNK> extensions although we focused on randomized trials, our findings also apply more generally. in particular, any one-stage ipd meta-analysis model fitted using ml estimation should include covariates (treatments, prognostic factors, adjustment factors, and so on) centered by their study-specific means; for example, when synthesizing ipd from observational studies to evaluate risk or prognostic factors for binary or survival <UNK> the included factors and any adjustment covariates should be coded with “study-specific centering”. indeed, exposure prevalence (eg, the proportion of individuals classed as biomarker positive) is likely to be more varied across included covariates in observational studies (than treatment prevalence in randomized trials), and thus “study-specific centering” will be even more important. we focused on parallel-group trials, for which our “study-specific centering” approach centers by the proportion in the treatment group; equivalently, we could center around the proportion in the control group. we considered binary variables, but “study-specific centering” should also be used for continuous variables where they are centered by the mean value. indeed, it generalizes to any covariate: we simply center at the covariate's mean value in each study. for example, for an ordinal covariate with possible values of <UNK> <UNK> <UNK> and <UNK> the “study-specific centering” coding is the original value minus the mean value for all individuals in the same study. in our simulations, we assumed a uniform distribution or fixed treatment prevalences across studies. similarly we generated control groups risks assuming a normal distribution, and did not allow any correlation between baseline risk and treatment effect. other approaches could be considered for data generation in further work. we also only consider one treatment and one control group per study. our simulations did not allow the between-study correlation to be estimated when fitting the random intercepts models <UNK> or <UNK> as we forced the correlation to be zero as it was in the data generating model. further research might also consider how the random intercepts models perform when the correlation is freely estimated, although related simulations have shown that between-study correlations are difficult to estimate reliably, and often estimated values are <UNK> or <UNK> <UNK> conclusions we recommend one-stage ipd meta-analysis models for continuous or binary outcomes use a stratified intercept. when using ml estimation to fit such models, researchers should use a “study-specific centering” coding of included variables. this will improve estimation of between-study variances and give more appropriate coverage of <UNK> confidence intervals for the summary (treatment) effects of interest. for continuous outcomes, reml estimation is recommended and then the coding should not be important. for binary outcomes, reml estimation of the pseudo likelihood will often improve upon ml estimation of the exact likelihood, although it may be unstable when data are sparse. for either ml or reml estimation, confidence intervals should be derived using an approach based on the t-distribution.