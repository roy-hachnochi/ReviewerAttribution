Overall:
The paper does a fairly good job of organizing a number of difficulties of conducting
double-blind RCTs, but the range of issues addressed ultimately supporting a number of
different conclusions. Four threads that seem to run though the paper would seem to be
from strongest to weakest (1) double blind RCTs do not give us the information we actually
want; (2) RCTs would give us the information we want, but are impossible to conduct; (3)
RCTs would be preferable, but raise ethical concerns the motivate against their use; and (4)
RCTs would be preferable, but make the trial difficult and we have prudential reasons to
prefer pragmatic trials (i.e. the higher reliability of the data is not worth the cost to obtain
it. It would help if the authors gave more explicit consideration to which claim they actually
wish to make and emphasize the information accordingly, or at least were clearer about
what kind of barrier each difficulty presents. With some moderate revision, I think the
paper would make a nice contribution to the methodological discourse, especially in light of
modern technological developments making preservation of blinding increasing difficult.
Specific comments
Page 6:
The discussion of “canceling out” the placebo effect is somewhat imprecise. Blinding
doesn’t necessarily stop people from believing they are receiving treatment in a trial and
thus experiencing an expectancy effect. Optimally, what blinding does is ensure that
expectancy effects are not unequally distributed between the experimental and control

groups. Similarly, as Kirsch has discussed (e.g. Kirsch 2000) the idea that removing the
placebo effect yields the “true effect” assumes that the effects are additive.
Kirsch, I. (2000, April). Are drug and placebo effects in depression additive?. In Clinical
Trials in Mood Disorders: The Use of Placebo… Past, Present, and Future., Sep, 1999,
Washington, DC, US; Aspects of this work were presented at the aforementioned
conference.. Elsevier Science.

Page 8:
The authors write:
“Key reasons given by patients for not wanting to enroll in these trials were that they
wanted a named medication or wanted to know what was in the tablets. This suggests that
achieving blinding and using a non-active comparator, such as a placebo, discourages
people from joining a trial.”
This seems to run two separate issues together. Concerns about knowing what was in the
tablet a patient was taking pertains to blinding (i.e. open administration of a placebo would
satisfy this concern). Concerns about wanting a named medication (if I understand
correctly) pertains to placebo use (i.e. the concern would be satisfied by blind allocation into
two active competitors).
Page 9:
If they are not familiar with it, the issue of blinding in the context of social media platforms
has received a detailed and recent treatment in:
Tempini, N., & Teira, D. (2019). Is the genie out of the bottle? Digital platforms and the
future of clinical trials. Economy and Society, 1-30.
Page 10:
I have argued that the problem of whether blind breaking trades on symptom improvement
or side-effects can be addressed and corrected for statistically. Is this issue as intractable
issue or just blurred given current practice.
Holman, Bennett. "Why most sugar pills are not placebos." Philosophy of Science 82, no. 5
(2015): 1330-1343.
Page 11:
Presumably the authors mean: “in blinded trials where a fixed dose…”
I’m unclear whether the authors are suggesting that the problem of dose adjustment
presents a practical difficulty given the way the trials are typically conducted, or present a
problem that is inherently intractable. The authors write that the use of placebos make
dose adjustments more difficult. Could they please be more explicit about why this cannot
be addressed methodologically. Is it because adjustments typically need some sort of
feedback that would unblind the researchers? Why can such adjustments not be
sequestered to preserve the blind?
I think the authors need to be clear here about what they are claiming. If there are nocebo
effects, then it seems that active comparators might yield equal or greater nocebo effects
(relative to an inactive control). As it stands, I don’t see how sham therapy adds risks to
the patient over and above what would be incurred by any proper control group. If a
control group experiences nocebo effects with an inert comparator, what evidence is there
that they would experience fewer/less severe side-effects with an active control. If there is
not evidence of this, then the inert comparator does not seem to expose the control group
to increased risk.

Page 12:
The authors rely Wartolowska et al., to support the claim that: “Placebo controls can also
induce nocebo effects in invasive surgical procedures with a systematic review of surgical
trials finding that adverse events were likely to be associated with the placebo in 9 of the 53
trials. Two trials within the review demonstrated harms directly related to the placebo.”
But the implication that placebos come serious risk seems stand at odds with the author’s
Wartolowska et al.’s conclusion that “The risks of adverse effects associated with the
placebo are small.” Do the author’s agree with this assessment and their point is that they
are small but not non-existent? If so this needs to be made clear. Otherwise, please
explain why you come to a different assessment.
I think the second question: “Are the potential harms to participants excessive?” needs to
be reworded so that a negative answer is disqualifying.
Page 14:
The Acne cream example could be much better at illustrating the author’s point. As this is a
hypothetical example, it should be clearer how results from and open administration active
competitor trial gives more useful information than would a double-blind test with a
placebo. For example, the difficulty in creating a placebo does not seem relevant to the
point. Why are patient biases/brand attachment/etc. crucially important to capture? The
example should be constructed to make this point more clearly. Why shouldn’t someone
read that box and say, this is exactly why we need blinded trial so that decisions can be
guided by what is effective, not which company has the best marketing.
Page 15:
Therapist example. Again, it is not clear to me what the moral is. Suppose that knowing
that a “therapist” is experienced does lead to better outcomes, but there is no beneficial
effect over and above that. Why wouldn’t we want to (1) establish that fact via blind
randomized RCT and then assuming we find no difference between novice and experienced
therapist (other than expectancy effects) seek to assure patients that the therapist they are
seeing has the necessary experience? Again, from the example it is not clear (to me) why
we would be better off with the pragmatic trial in the case described. The author’s claim
that the biases “are real” but just because they exist doesn’t mean that we have to take
them as a given feature. If there are negative biases against novices, why isn’t the proper
course of action to establish this and engage in steps to mitigate the bias rather than cater
to and/or exacerbate the bias.
Airway clearance example. Again, it is not clear to me if the authors believe that we would
be better off if we could conduct or a traditional RCT, but it is just difficult or that the trial
would actually be inferior to a pragmatic trial. For example, they note that patients who are
experienced with the therapy would be unblinded. This is not a problem with conducting
the trial in principle, because naivety could be made an inclusion criterion. Difficulty with
extrapolation seem like issues with whether the results from the trial provide what is
needed. It seems to me to have each example serve a single illustrative purpose and to
make that purpose clear rather than identify multiple issues within a single example.
