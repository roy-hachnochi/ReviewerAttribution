Strengths
This is a well-written paper on an important topic. The concept of assessing to what extent the
literature is generalizable to real-world contexts is important. The idea of assessing trustworthiness
provides an interesting variation to the systematic review.
Main issues
1. The abstract does not provide sufficient detail on the methods for critical appraisal,
trustworthiness assessments, or data extraction.
2. When reading methods from the paper it appears that the search strategy uses the term
reproducibility in searching. Reliability and agreement are commonly terms in this type of work.
Rarely, and potentially inappropriately, agreement between raters is classified as validity
particularly if one is seen to be a higher rank of authority than the other. Therefore, it is not clear
that this search strategy would have retrieved all the relevant papers on this topic from the way it
is written in the methods. When I look at the actual search strategy it does appear that
consideration to this issue was accommodated in the search strategy, but the author should clarify
this for the readers.
3. There is little description of how the data was extracted in the methods section of the abstract.
4. I think there needs to be some discussion about number seven in the criteria since having more
raters often means that fewer participants are required - if authors use a sample size calculation
to drive their research design. That is there is a balance in power achieved all the reliability studies
based on the number of raters and the number of participants. The authors imply that having more
raters is more typical of what happens in clinical contexts and therefore, enhances the
trustworthiness of the data. However, if the study was driven by sample size requirements then
increasing the number of raters often decreases the number of participants required. Since in most
reliability studies raters contribute little to measurement error and subjects contribute a lot, it
would be my personal opinion that having more participants more important to the overall
trustworthiness. I realize that you have points for both sample size and number of raters; but
these seem to suggest that more is better. Although this is technically true many high quality
studies would have been driven by sample size requirements not the arbitrary criteria set out in
your index. I understand you are trying to achieve here for discussion to make the distinction
between quality of the paper and the construct that you are trying to achieve with your ratings. I
guess I have concerns about the arbitrary benchmark for the number of raters that would allow
you to trust that the results could be generalized to the pool of raters actually doing those types of
assessment as I think that would vary based on the type of assessment and the interrater
reliability established for that assessment process. I can think of examples where a relatively
narrow group of professionals makes the decision and others where it is more diverse.
5. I understand that you were talking about trustworthiness which is related to the generalizability
of the findings to real-world practice however I would have expected in a systematic review that
you would have commented on the quality of the articles and included a quality appraisal. Surely,
quality of the study should also be a factor in trustworthiness. There are several options for
evaluating quality of clinical measurement issues. For example, the methodological quality of the
included studies could have been evaluated using the Quality Appraisal for Reliability Studies
(QAREL) checklist.
6. The idea of assessing trustworthiness is an interesting idea and has value. In your discussion
please distinguish it from generalizability-which is a term that meant most people would be familiar
with. I think it is also important to acknowledge that some of your criteria are quite arbitrary. I
would be concerned that people reading the paper with think that unless they had more than 16
raters their work would not be considered trustworthy. While it may not have been your intention
to set benchmarks for research design we know that appraisal tools often do become benchmarks
for future research design. Depending on the nature of the study the idea of including 16 raters
may be neither feasible nor appropriate. Further, if you have five raters that are assumed to be
randomly picked from the pool of raters and have established that high interrater reliability is
present-than the idea that your study does not hold a high level of trustworthiness without 16
raters seems contrary to the standards by which we typically evaluate reliability studies. It is
always true that we are generalizing the findings from raters within a study to a larger pool of
raters. I am wondering why the assessment tool would not have the evaluators determine whether
an appropriate range and number of raters have been included rather than setting arbitrary
benchmarks.
7. It would be helpful if you could provide a more quantitative description in some of the areas. For
example, when you talk about outcomes in your results section you list the types of outcome
measures that are included, but it might be more helpful if you would give some kind of indication
of the relative inclusion of different outcomes.
Minor Editorial Issues
Line- please do not start sentences with numbers- such as in abstract