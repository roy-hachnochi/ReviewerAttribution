This article shows a thorough evaluation of the performance of five text mining tools for mutation extraction. The authors perform two types of evaluation: one to test the ability of the tools in identifying specific mentions of genetic variation in the manually-annotated Variome corpus and the other, in the context of database curation, to compare results against the data in COSMIC and InSiGHT databases. The authors analyzed the tools both individually and combined. The main findings include: The tools have complementary coverage of genetic variants mentioned in the literature (e.g. some tools covered only single point mutations whereas others covered deletions and insertions as well), therefore the combination of tools proves to be more effective. The external evaluation on the database corpora is important to assess the utility of the tools for biocuration. The main results derive from the location of the data in the article. The majority is located in the Supplementary materials (for COSMIC) and also in Tables (for InSiGHT). The authors also demonstrated that the tools, even when they were developed for medline abstracts, can be applied to other text. The article is well written, provides sufficient background information, is scientifically sound, and both the title and abstract reflect the main research results and conclusions of this work. However, there is still room for improvement. Here are some suggestions: The article talks about mutations referring more narrowly to natural gene variants (at least that would be the case for the databases mentioned above). However, at least one tool (MutationFinder) seems to consider mutation in a broader sense. The fact that the corpus was based on PMIDs in PDB suggests that these are most likely to be mutations introduced by mutagenesis (e.g. I checked a couple of examples of the corpus used by MF and, in fact, some are mutagenesis experiments) rather than genetic variants. This may also affect the performance of the tool if most of the examples that it was exposed to were of these kind. I would suggest including some comment on this in the article. The description of TmVar in the background section should include the type of variant it is able to extract. This description is present in all other systems and should be described in the paragraph where they are referred to in the paper. Both exact and partial matches are used for the calculation of performance. Partial match is simply defined as annotated entities may have any overlap with the entities in the reference set . Have you analyzed the range of percent overlap? Tables 8, 9, 14 and 15 need some clarification. I understand that the point the authors want to convey is about the coverage not only of the tools, but of the data in the various sections of the article (main text, tables, supplementary material, abstract vs full length).The value therefore, that is presented in the column is like a fractional recall, as it is compared to the total number of variants in the reference set. The last row is probably the only line presenting the total Recall measure. The table is missing a row before the last one with Dataset:ALL, Tool: each tool. This row and the previous one should be highlighted as these are the ones showing the total recall. I would also add a column with the number of articles and annotations in each of the data sets (medline, pmc, pdf, etc), and the performance at each of these levels, i.e. calculate the recall on a section (Medline) based on the total annotation present in such section. I would change the title of Recall for the one showing fractional recall (calculated against Total). Maybe use a term such as coverage fraction or coverage recall? (Except for the Data set all). 