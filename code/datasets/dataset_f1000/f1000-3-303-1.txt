This version of the paper is much improved, and in general, I agree with the authors response comments. I still have some concerns with two issues, and reading the paper made me think of one more point of interest, but in general, I am satisfied to see this paper move forward as accepted. The first issue I have is in the description of the scenarios, where the first says that in a somewhat typical analysis, the probability of a wrong output is 100%. This says to me that there is something wrong with the calculations, as I dont think typical outputs are 100% likely to be incorrect. My second issue is the discussion of workflows in the final paragraph. While there are indeed a set of computations that are described as workflows, particularly data analysis applications, there is also a large set that are not workflows, particularly simulation applications, that are much better described as monolithic programs (though using subroutines, methods, objects, etc. internally). I dont think the workflow discussion is needed in this paper, or that it really helps make the key points. Finally, as an additional point, it would be interesting to compare this paper with work done in fault tolerance (aka resilience) where errors in hardware lead to different types of errors in the application, ranging from hangs and crashes to detectable errors to insignificant or significant undetectable errors. From my own work, http://www.slideshare.net/danielskatz/aft-dsk-reefinalreviewmay2001 provides an example of this, though there are likely many peer-reviewed published works that could also be used for a source of a comparison.