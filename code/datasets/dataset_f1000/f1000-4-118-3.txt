The analysis presented in this paper is of considerable interest and should be indexed, However, I think there are many confounding factors within the ChEMBL data that the authors have not addressed sufficiently. I will pick up some of these below. Polypharmacology usually implies the affects mediated via the multiple targets are therapeutically . Is this the authors implication also? Otherwise the term implicitly extends to toxicity and side effects. Figure 1 should include the distribution that underlies the other three, namely papers per-year. While the databases used were different, a published tracking of compound output from papers showed much less increase over 20 years than in figure 1 (PMID:24204758) although the target growth pattern was similar. Have the authors checked that ChEMBL did not pick up new journal coverage from 2008 that would spike the increases? I would like more detail on how the filtration methodology in the paper is used to extract and score (a flow chart would help). Let me pose a hypothetical case of two compounds. The first ranks target A at an IC50 of 20nM and target B at 30 nM. The second compound is 1nM and 500 nM for the same two targets. Do the two cases get the same promiscuity score? (It would be confounding if they did.) What happens when compound-target-assay values are identical for different publication years (not uncommon in ChEMBL) - Do you score only the first year ? Im confused by use of release date (as for a database) surely publication date is meant? For fig.3 I suggest the dominant explanation for apparently constant promiscuity is simply (i.e. researchers typically do not re-test compounds published by others). As we know re-testing leading to the publication of new results (promiscuous or not) will be largely dependent as to whether structures become reference compounds, are advanced into development, or become drugs. So could the relationship be plotted to provide insight into this? There are other confounding trends that could be tested for, for example targets-per-paper (i.e. cross-screening over the years might correlate with apparent promiscuity ) and orthologue vs paralogoue cross screening (i.e. if the average human:rodent ratio changes over time for the low confidence set). Why not select kinase inhibitors as a control subset? We would expect these to exhibit highest promiscuity and they would thus be an important methodological cross-check. In terms of other obvious hypothesis checks why not split by LogP (as might increase promiscuity) and Mw (as might decrease it) ? While appreciating the academic imperative I do wish this team could have merged some of their previous papers that appear to address essentially the same theme. For example, comparing drug (ref.18) vs non-drug promiscuity in the same standardised study is better (easier to review even :) than splitting the result sets. 