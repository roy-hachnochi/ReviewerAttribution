This article is about two tools implemented within UberResearch, a small company operating as part of the Digital Science portfolio. The first is a tool that allows researchers to add funding awards to their ORCID record (the UberWizard for ORCID); the second is a tool for funders ("Dimensions") that performs analytics on funding decisions using research classification systems. A machine-learning method has been uses to tag documents (grant awards) with various categories of the research classification systems. The two come together as grants claimed by an individual researcher can be tagged and therefore categorized on an individual basis as well as for funders. I wish the authors could have been clearer on this from the outset. I think this context would help orient readers better. Much of the abstract focuses on the details of funders being able to automatically update a researchers ORCID with funding information and an abstract discussion on automatic tagging of research classification systems. The writing and narrative style could be improved - some sentences extremely long e.g. p3. "Creating an ORCID D (sic) isnt difficult .... funded research." However the major comment I have on the article is that it describes outcomes without describing how they were achieved, and furthermore there is no way for readers to evaluate Dimensions as a product. More specifically: Did UberResearch need to be a member of ORCID to provide the grant linking service? How has the "global grant database" been generated - from where? What is its scope? What protocols and standards are used to make the transactions between ORCID and the UberWizard? Are there any other tools available? What methods of machine learning were used, how were they validated? The given info is: "The approach used to derive the model using machine learning routines will be discussed in a separate paper once the evaluation of several of the classification systems has been concluded." I would be happier if this were published already. As a reviewer or a reader, I cant access Dimensions so I have no means to evaluate it. The article seems to be aimed primarily at funders, as is the Dimensions product - what is in it for the researcher, who must be the primary readership of F1000Research? I have personally used the UberWizard for ORCID and it worked fine; I also saw a brief preview of Dimensions a few months ago, and was impressed. I see this article is an "Opinion" article - but nevertheless it is tough to evaluate when the methods and outcomes are not publicly available. I would agree that ORCIDs are important for effective research assessment, but it would be a more powerful position if there were some means to evaluate Dimensions directly. Would a product review be a more appropriate route? 