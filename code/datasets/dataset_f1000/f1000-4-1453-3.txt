This manuscript addresses the analysis of scientific articles published in PDF form, and introduces a software tool that essentially wraps a number of other tools to build a single tool that integrates a number of features. There is no novel methodological contribution in the tool itself that I could determine; the value is primarily in integrating the other tools into a user-facing tool. However, the value of a user-facing tool (as opposed to an automated tool that proceeds without human input) is not made clear or evaluated (are users happy to do the work expected of them?). As a very high level point on motivation, many scientific articles -- particularly in the open access literature which form the majority of the studied papers -- are already available as raw XML and it does not make sense to me to try to parse the PDF to infer structure (in a way that first requires user interaction, and second may introduce errors) that can be read directly from the publisher-produced XML. The PubMed Central repository is a repository of full-text, available online as structured HTML, and the open access collection which is downloadable in its entirety uses XML ( https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ ). So in my opinion several of the key objectives of this tool are not needed for a large and growing proportion of the scientific literature. The argument could be made that older articles and articles from certain publishers are not available as "raw" XML, but the author neither make this argument nor specifically test this question. In addition, this XML resource provides a fantastic opportunity to automatically test (some aspects of) the performance of the authors tool. This is not done. The authors should consider the contribution of their tool in the context of such resources. The Introduction focuses on PubMed, but PubMed does not claim to search over the full-text literature (it is limited to abstracts by design), and PubMed Central is not mentioned here. There is one more tool that I am aware of -- and I suspect there are others -- that the authors do not mention; the Layout-Aware PDF tool 1 . In general, I'd be interested to understand more deeply how the authors' tool differs from the various tools they mention, and why they selected the tools they did for their final system. The Results presented by the authors are not framed in any well-defined evaluation framework. While they mention "best overall performance" no indication of either specific quantitative or qualitative criteria used to assess the tool have been provided. What were the criteria that were used? Were there guidelines for how to "score" systems on various criteria? What is presented is largely ad hoc qualitative observations. On what basis was performance of the tool ranked? Comments on presentation: It is poor practice to cite papers that are purely studied as artifacts -- a citation implies that you are referencing scientific content which you are not. Please remove citations to the papers that were used to test the system; they should be listed (preferably with DOIs) in Table 2, with a paper ID, and the paper IDs should be referenced in the article. There are a number of phrases in the manuscript that are awkward. "marginalization" does not mean "to find margins". Queries in PubMed are not "NLP quer[ies]" but rather user queries processed by an IR system (NB: arguably, IR systems don't even use NLP). The authors talk about "orthodox NLP approaches"; I have no idea what makes an NLP approach "orthodox". I suspect the authors mean "pipeline" rather than "product line". What is an "inherently multiplexed approach"? The long list of databases in the Conclusions doesn't add much to the manuscript; perhaps the example of the DrumPID should be pulled into a discussion section, and elaborated to clearly demonstrate the practical application and value of the tool. 