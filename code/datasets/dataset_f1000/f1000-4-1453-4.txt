The authors present a system for extracting the main text, the images, and the text within those (labels, etc), from scientific papers in PDF format. The system is implemented in an interactive desktop application for Windows and tested in 10 papers. Extracting the large amounts of scientific and medical information stored in an unstructured way is an important challenge. Any effort in that direction, such as that presented in this work, is of potential interest. My main concern with this work is that most of the features described are already available in existing software, even those described as “new”. For example, tools like “pdftotext” can parse multi-column PDFs, “pdfimages” extract the images within a PDF file, “OCRFeeder” detects the image elements and extract the texts, tables, etc, … On the contrary, some potentially newer features of MSL, such as recognizing the article parts (Abstract, References, … -page 3-) mentioned in Methods are not described later (Results). Problems of other tools mentioned in the Introduction, such as “removing irrelevant graphics” are not solved by MSL either. I think the comparison with other tools should be presented in terms of what these fail to detect while MSL does not, and vice versa. E.g. For a particular article “pdftotext” was not able to recognize the columns while MSL does, etc. In summary, better putting this system into the context of existing ones. The system is implemented as an interactive tool intended for analyzing a single article at a time, even presenting some of the final results (e.g. text extracted from images) back as PDF. For a single article (or a small number) human inspection will be better than any automated system. I guess the real potential of this system is in the automated parsing of large collections of articles. In my opinion the authors should focus the manuscript more on that. 