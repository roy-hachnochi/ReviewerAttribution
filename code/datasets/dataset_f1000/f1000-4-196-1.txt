Brief Overview: This study is situated within a national research evaluation context in Italy (i.e., designed for the selection of the best candidates for the ranks of associate and full professor) and focuses on two distinct rating exercises, one for journals and one for individual research articles published in five fields of the Social Sciences and Humanities. The authors take advantage of a fortuitous data set, and use an ordered probit-model to compare the score given by expert peer reviewers to 11,500 research articles (i.e., Excellent-A; Good-B; Fair-C; Limited-D) and the rating of the journal (classified as A or non-A) in which the individual articles were published. For all papers, a series of additional variables are taken into account, including a) language of the article, b) the scientific area of the author, c) the authors age, academic status and gender, and d) the inclusion of an international co-author. In terms of referees, an allowance is made for the possibility that he/she was international. The purpose of this controlled experiment was to test for the robustness of expert-based journal ratings by determining the probability of a paper receiving a high independent review score, where the journal in which it was published also received a high independent score. Assessment: I tend to agree with the first reviewer in that more background information is needed regarding the criteria behind the original two rating exercises, although I basically find that for a very short article, the statistical aspects of the methodology are quite thorough. I have only one comment about the rating exercises. The article indicates that different panels or groups of individual experts were chosen: some were assigned to provide the journal classifications (but we do not know how many), and a separate group of others (i.e., one non-panel, plus a consensus panel) were asked to rate individual articles from the specific fields (also how many?). Neither group were said to have exchanged information; thus acted independently of one another. The peer reviewers of the individual articles were instructed to evaluate articles only on the basis of their merit regardless of journal and of language of publication (p. 3). Here, I am curious specifically about how much information pertaining to the journal (e.g., header; footer; abstract; citation style; volume number) had been removed prior to the article evaluation/rating procedure? Were the reviewers in this experiment truly blind to the journals influence? In certain areas of the Humanities, where there are notably fewer A-class journals, perhaps the format of the article instantly gave away the type of journal in which it was published. This could mean that when an article had been rated as being , the reviewer was actually making a simultaneous judgement on the journal as well. I would therefore like to assume that all of the articles peer-reviewed in this experiment were non-formatted pre-prints (?); otherwise, this could be a factor which is contributing to the mutually related ratings.