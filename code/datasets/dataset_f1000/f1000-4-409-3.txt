This is an interesting topic, and one that is ripe for study. As the authors point out, there is a growing amount of activity in and funding for various kinds of science communication (they point to the phenomenon of Science 2.0, but they may also like to think about this relates to scholarship on Mode 2 knowledge production - e.g. see Etzkowitz Leydesdorff (2000) ). There is definitely lots going on, a mass of different terms for related activities, and a degree of slippage between different terminologies. As the authors themselves point out, though, there are some problems with their methodology, and I also feel that the analysis and discussion would benefit from a fuller engagement with the literature. Ill take the latter point first. To start with, I think the authors need to more clearly make a case for their focus on the terms that their analysis is based on (thats rather clumsily put - but I hope my point will become clear). Their starting point, unreferenced, is that "Science communication is often analogously and interchangeably referred to as science outreach, public engagement, widening participation, and/or knowledge exchange". Yes - but many other terms are also used, including dialogue, PUS, scientific literacy, tech transfer, third stream activity, and co-enquiry. Why were these terms selected (and indeed why not science communication itself)? This sense of arbitrariness is heightened by a literature analysis that bounces around in a somewhat haphazard way. The authors ignore, for instance, work that has attempted to define some of these terms (Burns et al. (2003), whom they cite at the very start of the paper, offer an extended discussion of terms they see as similarly related to science communication, including scientific literacy and PUS; the work of the NCCPE - http://www.publicengagement.ac.uk - is also a valuable resource for seeing how different forms of science communication have been framed). By interpreting the that the science communication literature often refers to (e.g. in the Rowe Frewer article they cite) as being about widening participation initiatives they also make a bridge to an area of university outreach and external relations that is infrequently discussed in the main body of science communication research (wrongly so, I think - but thats another discussion); rather, the term tends to be used in the tradition of deliberative theory and public participation in science research and policy (a randomly chosen example would be Bogner (2012) ). All this is to say that things like public engagement and widening participation actually have rather different histories, and different communities around them - so it doesnt surprise me, for instance, that some respondents hadnt heard of widening participation initiatives. These are likely to be organised quite different within university systems. I was also surprised that the authors didnt spend more time considering other empirical work on practitioner definitions of these terms or of science communication generally. Some of my work has treated this (not that its necessarily essential reading - but see, e.g., Davies (2013) , but Jason Chilvers, John C Besley, and Kevin Burchell, among others, have also published analyses of how scientists and other practitioners of public communication tend to define and understand it (e.g. Chilvers (2008) ; also Besley (2010) ). So - in short, I think both the framing of the study, and the analysis and discussion, would benefit from a more thorough engagement with the qualitative literature that has built up around the meaning and practice of science communication. I also have some comments about the methodology and analysis - largely to do with the need for more explanation as to what the former was, and why it was chosen. For instance, the authors note that the survey was "advertised via email, social media accounts, and the mailing list"; earlier, they say that the study aimed to understand opinion on definitions of the four terms "in UK HEIs". What, exactly, is the target participant group? Everyone working in UK HEIs? A certain subset of this population, those who are interested in science communication? The HEIs themselves, as institutions (i.e. as organisations with particular brands)? How was the sampling strategy (the distribution of the survey) designed to reach the desired population? In terms of the survey advertisement via "email and social media accounts": email to whom, and why, and which social media accounts (and why - but you get the picture...)? The PSCI-COMM list is distributed to a large group of those interested or working in or researching science communication, in the UK but also internationally. Is the target population therefore science communicators? (In practice almost all respondents had participated in science communication in some way, so perhaps so. But this needs to be clear.) [Apologies - Ive just re-read your comment in the conclusion that respondents only being active science communicators is a weakness of the study. But in that case, if youre interested in everyone working in UK HEIs from chancellors to cleaners, you need to justify why you used a survey methodology, and why you thought your sampling strategy would reach everyone.] I would also like to know more about how the analysis, and particularly the qualitative analysis, was carried out. Using word clouds is a rather basic means of analysis, as it tells us only how often a word is cited, not the context in which it is used or the meaning that is attached to it; discussion of the qualitative responses is therefore important. Were these coded in some way? How were themes robustly identified? The data evidently did not reach saturation (to use a term from grounded theory), as the authors emphasise the diversity in the definitions given. Do you think you needed a larger sample size, or is there so much interpretative flexibility in these terms that saturation would never be reached? The authors do note, in the conclusion, that the sample size is a weakness of the study, and I would agree with this. As far as I can tell you were also not able to identify the role or situation of your respondents, only whether they had participated in science communication activities in the past. This, to me, also weakens the results significantly - or at least represents a lost opportunity. There are big differences between different kinds of communities within universities and as participants in communication (e.g. scientists, outreach officers, admin staff, PR teams, tech transfer offices...). It seems unlikely that these communities would have homogeneous definitions of the terms in question - or even have heard of them all (as your findings suggest). I want to close - and I do apologise for the lengthy review - with a broader point. The issues that you touch upon raise some fascinating questions. I would love to know more, for instance, about the relations between individuals and teams in universities working on widening participation and on public engagement-type projects, and I would be very curious to know if academic staff give different kinds of definitions of these terms to those who organise science communication on a professional basis. In this regard I would encourage you to look beyond finding the , or even commonly used, definitions of particular terms. Alan Irwin has talked about third order studies of science communication, which explore how different terms are mobilised by different groups, and the kinds of effects that this has (see Irwin (2008) In: Bucchi M and Trench B (eds), Handbook of Public Communication of Science and Technology , London and New York: Routledge ). Your study suggests interesting ways to explore how very different meanings can be applied to the same terms - it would be great to hear more, in the future, about how these different meanings are made to matter in particular contexts (such as, to go back to the very beginning, moves towards Mode 2 and entrepreneurial universities). I wish you all the best with this future research.