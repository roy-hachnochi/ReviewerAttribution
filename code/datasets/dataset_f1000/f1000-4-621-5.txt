I appreciate the authors attempt to write a short tutorial on NHST. Many people dont know how to use it, so attempts to educate people are always worthwhile. However, I dont think the current article reaches its aim. For one, I think it might be practically impossible to explain a lot in such an ultra short paper - every section would require more than 2 pages to explain, and there are many sections. Furthermore, there are some excellent overviews, which, although more extensive, are also much clearer (e.g., Nickerson, 2000 ). Finally, I found many statements to be unclear, and perhaps even incorrect (noted below). Because there is nothing worse than creating more confusion on such a topic, I have extremely high standards before I think such a short primer should be indexed. I note some examples of unclear or incorrect statements below. Im sorry I cant make a more positive recommendation. investigate if an effect is likely ambiguous statement. I think you mean, whether the observed DATA is probable, assuming there is no effect? The Fisher (1959) reference is not correct Fischer developed his method much earlier. This p-value thus reflects the conditional probability of achieving the observed outcome or larger, p(Obs|H0) please add assuming the null-hypothesis is true. explain this notation for novices. Following Fisher, the smaller the p-value, the greater the likelihood that the null hypothesis is false. This is wrong, and any statement about this needs to be much more precise. I would suggest direct quotes. there is something in the data that deserves further investigation unclear sentence. The reason for this unclear what refers to. not the probability of the null hypothesis of being true, p(H0) second of can be removed? Any interpretation of the p-value in relation to the effect under study (strength, reliability, probability) is indeed wrong, since the p-value is conditioned on H0 - incorrect. A big problem is that it depends on the sample size, and that the probability of a theory depends on the prior. If there is no effect, we should replicate the absence of effect with a probability equal to 1-p. I dont understand this, but I think it is incorrect. The total probability of false positives can also be obtained by aggregating results (Ioannidis, 2005). Unclear, and probably incorrect. By failing to reject, we simply continue to assume that H0 is true, which implies that one cannot, from a nonsignificant result, argue against a theory according to which theory? From a NP perspective, you can ACT as if the theory is false. (Lakens Evers, 2014) we are not the original source, which should be cited instead. Typically, if a CI includes 0, we cannot reject H0. - when would this not be the case? This assumes a CI of 1-alpha. If a critical null region is specified rather than a single point estimate, for instance [-2 +2] and the CI is included within the critical null region, then H0 can be accepted. you mean practically, or formally? Im pretty sure only the former. The section on The (correct) use of NHST seems to conclude only Bayesian statistics should be used. I dont really agree. we can always argue that effect size, power, etc. must be reported. which power? Post-hoc power? Surely not? Other types are unknown. So what do you mean? The recommendation on what to report remains vague, and it is unclear why what should be reported.