Thank you for the opportunity to review this paper. While it tackles an important subject, we have serious reservations about its approach and conclusions, and do not approve it. The fact that this is to be published in an section does not absolve the writers from clearly expressing what text is actually opinion, what text is fact, and what facts are used to support their opinion. We would urge more scholarship in how the information is presented. The discussion within each category, what information is available, and how to get it appears very superficial. There is no information provided as to when the checks in Google Scholar were made, or how often they were made, or if the authors checked the to verify applicability to the topic at hand. The numbers are not static and can change in time. In Category 9, we get 1540 hits for and data manipulation they get over 6000. We get 3590 hits for and data manipulation and they get about 1200. They refer to Scientific Peer Review by Lutz Bornmann and say that it is widely recognized because Google Scholar lists 120 citations for this article. The authors not indicate whether the citations are used in a positive or negative light, or even in the same concept as these authors think. As of 10/17/15, the first 6 citations were by Bornmann himself. The authors make the statement: For the purposes of future research the concept of funding-induced bias is analyzed in the context of various practices in science where bias can occur. There is little to no provided, other than Google Scholar hits, which is meaningless for analysis as it has to many hidden variables to allow adequate evaluation, followed by suggestions as to how information may be obtained. The suggestions do not provide specific means to obtain information. The has no information to justify the 15 categories chosen. They are not clearly defined as to meaning, context, and criteria. They supply no studies to support that any of these numbered items actually existed in form to affect any funding process. They may, but some type of evidence should be presented to provide justification for the category to exist and to be considered a discrete concept. List number 11 is : Asserting conjecture as fact. It can be in a researchers, as well as their funding organizations, interest to exaggerate their results, especially when these results support an agency policy or paradigm, One way to do this is to assert as an established fact what is merely a conjecture. The authors then list Google Scholar hits for the word There is no evidence offered for any associations with titles. There is no evidence offered that the hits for the word were even associated with science. Google Scholar draws from a wide variety of indexing databases, and blogs, and various other items with internet access. The authors offer no evidence for statistically significant associations between the words they use for and any science-based studies. Some examples of statements meriting citation support (there are others): The concept of finding-induced bias is one that specifically occurs in the discussion of and research into some of the fifteen bias research types that we have identified, but not in all of them. It tends to occur where specific funding is the issue. What studies have been done that verify that this even occurs, that the fifteen bias types are indeed areas where bias is possible, and that the tendency to occur with specific funding issues is factual and not conjecture? For example, the US Federal basic research budget for the NIH is larger than the combined budget for all other forms of basic research. From where and when was this information obtained? Under the section Quantification analysis method issues, the authors write What is the suggested best method of quantification? In particular is it subjective or objective, that is is human judgment and classification of the data involved, or just simple counting of clearly defined instances. Qualitative studies (based in subjectively-derived data sets) would be the description of the former, and quantitative studies (based on analytical analysis of discrete data) would be the latter. A typical definition for general science can be found via internet sources: qualitative research research dealing with phenomena that are difficult or impossible to quantify mathematically, such as beliefs, meanings, attributes, and symbols; it may involve content analysis . quantitative research research involving formal, objective information about the world, with mathematical quantification; it can be used to describe test relationships and to examine cause and effect relationships. (Retrieved on 10/19/15 from http://medical-dictionary.thefreedictionary.com/Quantitative+research ) The use of the term in subsequent discussion seems to indicate , not . List number 12 is False confidence in tentative findings. Google Scholar reports about 2500 articles using the exact term false confidence in the 2010-2014 time period. However, this term occurs just 5 times in article titles, suggesting that the concept per se is not a focal point for research. This statement has several weaknesses in reasoning. 1. false confidence is a term that is also used to convey , and thus the scholar reports may not be conceptually representative of the term as used by the authors. 2. Requiring the exact word usage to convey interest is misleading, as it may be called something different in researchers vernacular. 3. Noting how many times the words are used in a title that is indexed with Google Scholar is misleading. The concept may be included under a greater concept, about which there may be numerous research studies published under a different database (Medline, Scopus, etc) 4. Using the term in any search function related to research can be misleading as quantitative studies typically use confidence intervals in their statistical analysis. These are specifically defined mathematical concepts. The authors write that "A meta-analysis refers to studies that purport to summarize a number of research studies that are all related to the same research question. That is the wrong description of , which is not a . Meta-analyses take all the data from the studies they include, and perform analyses as if they are one huge data pool. Statistical results obtained are therefore not , as the creation of a large data pool may allow differences in means, trends and associations. Quite commonly, differences in effects are found because the size of the dataset will reduce bias found in smaller samples. "Definition of META-ANALYSIS: a quantitative statistical analysis of several separate but similar experiments or studies in order to test the pooled data for statistical significance. Retrieved on 10/19/15 from http://www.merriam-webster.com/dictionary/meta-analysis . The authors write that It is not clear that quantification can play a major role in this sort of bias research. For example, if a meta-analysis is found to be ignoring scientific papers reporting negative results, how many such papers there are may not be the issue. This may be more a matter of the strength of evidence, not a matter of counting up the sides. As discussed previously, it is in these meta-analyses that quantification can and will play a major role. (See definition as given above). If papers are without sound rationale, that is indeed an issue, as the sample set is incomplete. However, if papers are and not included for issues of poor methodology, missing values, differences in measuring instruments, or inclusion of confounders not defined, then the exclusion decision can be considered reasonable. A good meta-analysis should consider all the available research, and provide sound reasoning for what studies are or are not included. Under item number 10, the authors write, A researcher or their funding organization may balk at sharing data with known critics or skeptics, because of the negative effect it may lead to. There are, however, some well known (sic) cases of scientists refusing to share policy relevant data. In the US one of the most prominent is the so-called Six Cities study regarding the long term health effects of airborne fine particulates. The term is connotatively misleading, as indicated in the first three articles (the third was actually from a book chapter) found in the Google search the authors said they run. The issue as described was in participant privacy, which might be considered a HIPPA issue in some instances. The participants had been promised privacy (confidentiality) and the researchers were (by what was in the articles) merely holding to their promise: The year was 1997, and Dockery had arrived in Washington to tell Congress that because it had promised study participants confidentiality, Harvard couldnt share the raw data from its federally funded Six Cities study Retrieved on 10/19/15 from http://www.hsph.harvard.edu/news/magazine/f12-six-cities-environmental-health-air-pollution/ . "The authors of both studies have resisted demands to open up their data to public scrutiny. In the case of the Harvard study, for instance, they cite the need to keep the identities and health status of some 8,000 study subjects in six communities, including Watertown, Mass., confidential. They contend that, even if names and addresses are removed, it would be possible for someone to determine the identities of many subjects based on their age, hometown, and date of death. The controversy poses a test for government officials and scientific researchers, who increasingly are being asked to balance the health care privacy rights of individuals against demands for data from outside researchers, the public, and, politically motivated critics. Retrieved on 10/19/15 from https://www.bostonglobe.com/news/nation/2013/09/06/landmark-harvard-study-health-effects-air-pollution-target-house-gop-subpoena/2K0jhfbJsZcfXqcQHc4jzL/story.html . When Harvard researchers published the Six Cites Study suggesting that fine particulate pollution led to an unexpectedly high mortality rate, particulate-emitting industries were understandably concerned. 7 A number of affected industries requested the original data supporting the study, but the Harvard researchers refused, because they were concerned that even the redacted data could be used to identify original study participants who had been assured of confidentiality. 8 (The citation numbers go to references within the book.) Retrieved on 10/19/15 from https://books.google.com/books?id=Ah6-__otORACf=false . The authors write that There appears to be very little scientific research on potential funding-induced bias in the construction or use of scientific models. and This appears to be a major gap in policy related research Web of Science gave back 320 hits for research fundingand and modeling." 