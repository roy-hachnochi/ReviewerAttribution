The article provides an introduction to automated software testing, its application to computational biology, and model validation as a form of testing, with examples taken from the OpenWorm project. The article is clearly written, and will be a helpful resource for computational biologists. The article could be improved by a deeper discussion of some of the more difficult issues in the automation of model validation: what criteria to apply when transforming a numerical measure of closeness into a pass/fail? how to support the use of different criteria by different scientists, who might weigh the relative importance of particular validations very differently? how to handle contradictory experimental results? I would also be interested to read a discussion of possible improvements to continuous integration dashboards in the context of continuous validation, e.g. tracking the evolution of numerical validation results across model versions. 