This manuscript seeks to compare two large pharmacogenomics datasets (several hundred cancer cell lines screened against 15 common drugs) and evaluate their level of agreement via (1) the drug sensitivity values and (2) gene expression profiles of the cell lines. Broadly speaking, the value of these profiles is the discovery of (gene) biomarkers that could predict response of cells to the drugs. Previous efforts, including the authors' previous attempts, have had trouble with reproducibility. The authors have previously given harsh critiques regarding the reproducibility of the two datasets. This manuscript is very important, and it has the potential to dissect sources of both agreement and disagreement that can be amplified or minimized in the future respectively. The reviewer also has little doubt that there is in fact disagreement between these two datasets and, moreover, it is significant enough as to interfere with the discovery of biomarkers. The reviewer also agrees with the authors that this is important to point out and understand, and the "call to arms" in the Discussion (the best written part of the manuscript) should certainly be listened to. However, because this manuscript is very important , the cornerstones of the comparative analysis must be correct. The Supplemental Methods are near impossible to decipher and are littered with undefined terms, confusion in mathematical notation, poor equation formatting non-intuitive statements that do not assist the reader with understanding the numerous design choices (from throwing out poor quality data, to model fitting, to different measures of consistency). The Results section is not sufficient methodical to follow the argument of what does or does not represent ***statistically significant*** disagreement. Almost every paragraph until the conclusion presented serious challenges to this referee. They are included below. This is an important effort and the authors should return with an improved manuscript. Many of the co-authors are skilled mathematicians and they are strongly recommended to revisit every line of this manuscript to ensure correctness and to present with craftmanship. This is especially true in the Supplementary Methods that actually provide the "meat" of the methodology: this I believe must have been an oversight with this submission. This manuscript needs to be published and I believe there are important lessons to be learnt here but there has to be a more focused, tighter argument to establish where there is disagreement and hypotheses as to why (in the Discussion) and what can be done about it. But the main issue here is that the basics of the paper are not solid, or at least they cannot be evaluated. The authors should be commended for the effort to be reproducible (in the sense they give the list of R packages used and their code) but that is only one aspect. The mathematics and statistics requires clarity and correctness. Terms such as "metric" should be used properly, and novel equations that are derived (e.g their "E" parameter) must be done so in a careful correct manner, with attempts made to justify these parameters (e.g \epsilon, \rho, 2*\epsilon, the $E$ parameter from the modified fit etc. I would be happy to view a revised version of the manuscript and I hope that my comments aid in this important project. pg5: What is “Dataset 1”? The link here doesn’t lead anywhere that I can tell. Figure 3. I’m not really sure what the value is in plotting the density functions for the mismatched and matched cell lines. First, wouldn’t one density function suffice with a threshold I guess? Second, do you really need it at all? In the Methods “Cell line identity….”, it is stated that 66 samples fell below threshold with a reference to the Supplementary methods. However I don’t see anything in the supplementary methods that discusses this. Moreover in the text, it seems that you threw 8 cases away. This is confusing. pg 5. I’m not sure what you mean by “remain stable or decrease monotonically”? Do you just mean “monotonically non-increasing”? Please see comment regarding “Filtering of drug dose-response curves” form Supp Methods below. I think this really needs to be reworked, and I have to trust you guys here that you are doing the right thing. “as exemplified…” depicted? In Figure 4, is it possible to relate this back to the choice of \rho, \epsilon and 2 \cdot \epsilon from the Supp Methods, or perhaps integration a version of this figure (but annotated) into the Supp Methods. In panel A, the grey area is a bit non-intuitive no? I would say that post 0.03, it’s looking pretty good, and it’s not measured in GDSC after that. However the first points are off. I don’t know what \epsilon or \rho are so it’s hard to relate what is depicted in Figure 4 back to your model. When I look through Supplemental Figure 1 (all the excluded comparisons), it seems like your criterion for excluding a comparison boils down cases where at least one of the curves has high variance, and the cutoff 2\epsilon I think is a constant independent of the distribution of points for either curve. I don’t see in your equations how you encode that the sequence is monotonically non-increasing, or how “order” along the left to right sweep is incorporated. Supp Methods Filtering of drug dose-response curves i +1^{st} - (i+1)^{st} It seems like there is a formatting problem here. In my pdf there is something like I”A squiggle after “in some large fraction ??? of the cases (1).” I guess that’s supposed to be \rho right? I’m not sure I understand this sentence “Our quality control …” Are you saying that \Delta_{i, i+1} \epsilon in some fraction \phi of the cases? equation #2 below: I also have never seen set notation such as \{ \Delta_{i,i+1} | \Delta_{i,i+1} \epsilon \}. Are you trying to say “given all the \Deltas that are less than \epsilon? So the vertical | means cardinality here right? But then the denominator has the cardinality of a value, or do you mean absolute value? What is \rho? It’s undefined. “Unfortunately …” The english is a bit rough - could be rephrased in terms of specificity and sensitive, I guess. I don’t understand the significance of the sentence “Consider, for instance, …” But in the main text, you say that it remains stable or decreases monotonically. Here it increases monotonically for many successive points, so this violates your model, no? I think that this subsection wants simply to spell out mathematically the thresholds and also provide some rationale for the parameters. I think the text doesn’t really do a good job of establishing this rationale and needs work. Perhaps define the parameters precisely and then phrase the exposition in standard terms e.g. specific and sensitivity for different \epislon, etc. Your equation (2) is inconsistent. In the text you specify \Sigma_{\forall i,j} \Delta_{i,j} but below your criterion seems to change to (the correct) $i j$. It is also sufficient to write \Sigma{ i j } \Delta_{i,j} and avoid the double summation. You should probably define D_i in the text and not make the reader deduce it from the figure below The comments here w.r.t. the Supplementary Methods also apply to the associated subsection of the Methods. The mathematical correctness of some comments needs attention. For example, it is not quite correct to say that the “curve fitting would have yielded erroneous results”. The curve fitting is just that, curve fitting. It’s not really an error. Then in the Methods, you claim to use this equation but this is inconsistent with the discussion in the Supplemental Methods (where you have an equation with this undefined parameter $E$). The least-squares method using a three-parameter sigmoid model. I understand the intuition for this, but when I look through Supplementary File 2, I think there are a lot cases where this is perhaps not the correct pattern to assume (e.g. straight lines). Moreover, there are some very strange fits, for example, AZD6244:G−361 AZD6244:SK−MM−2 PLX4720:MDA−MB−175−VII In some cases the curve is always above all of the observations. Perhaps this is because the measurements of viability 100%? In your model you have removed the $E_0$ from Barretina et al. for different reasons. Supp Methods - Fitting of drug dose-response curves I’m a bit lost with your choice of notation here. For example, you define y as an equation, not a function, and then write y(0) which I assume is supposed to be something like y(x). Ok but then you have y=0 and y = y(0) = 1. I’m not sure this is correct mathematically. (I think you mean to say that y(x ) = … *where* y(0) =1. “viability is reduced to half … concentration of the drug”… so the “Top”, E_\infinity … I find this a bit wordy. “The dose response equation now becomes …” So I deduce that E is the new parameter?!?! Where has E_\infinity gone?!?! (But then down below E is constrained to be in [0,1] and seems to related to the fitness of neoplastic cells. I’m not sure I understand this.) There is no derivation of this formula whatsoever. In fact, I don’t see how this could be correct any longer. Couldn’t this be expressed as mixture of two cell types, and y would be then a sort of weighted average? I really don’t see how this was derived. This is a very central part of your paper (since the manuscript is measuring agreement) and therefore it needs to be bulletproof. Please define “extant drug”. Also HS is allowed to vary apparently but I don’t see where it is then optimized in your analysis later on. This is confusing. Consistency of Drug Sensitivity Data Is the ABC method standard? Are there citations for this? You should probably define properly what you mean by the “insertion of the concentration range”. Elsewhere it seems that you are referring to this as “common concentration range” e.g. SFig 2 More generally, isn’t this a sort of (non-statistical) version of the Kolmogorov-Smirnoff test? Figure 5A: Actually there are many such cases in your Supplementary Figures. Doesn’t this just mean that the range of concentrations are not sufficient in both datasets? pg 7 “We then computed the median …” I don’t understand what your distance metric is here. If I understand correctly, you could the ABC for each pair of drugs in the GDSC dataset. From that I can imagine a distance matrix D where D_ij i the ABC between drugs i and j. But you said you take the median ABC? median over what? cell lines? repeats? Whatever the case, are you sure it’s a distance metric?! Is it really true that distances derived from ABCs are metrics? I think this should be shown in the Supplemental Methods. Also as a minor comment, the caption in Supp Figure 3 says that you are using the mean ABC value but elsewhere it says median. p8. I am not sure what the significance of Supplementary Figure 3 is: are any of these clusters significant? Why are two drugs coloured red in panel A? On page 8 it is claimed that the samples split by hospital (MGH vs WTSI) but I don’t see how this is represented in Supplemental Figure 3. pg 8. I have a very hard time estimating the significance of a statement like “…3 out of 15 common drugs clustered tightly”. I am not sure what tight means here. When I look at Supp Figure 3 there has been no effort to annotate the clusters with their reproducibility e.g. pvclust or measure their significance in some other way. When I look at the figure I think there appears to be a lot of co-clustering of the drugs, at least given that the median ABC across a diverse collection of cell lines might not be such a great “distance” measure. pg 9. What do you mean by “highly targeted therapies”? pg 9. paragraph “Although the ABC values …” I think the ABC is interesting but it takes a very prominent role in your paper when there are other standard techniques already like AUC and IC_50. Perhaps the manuscript should have comments about why have chosen this approach that is not standard. Also I think you would need to make precise what the differences are between how GDSC and CCLE computed the AUC and IC_50 that are different than how PharmacoGX does. This is a very central concept in your comparison so it would have to have a very solid definition and analysis. Supplemental Figure 4 suggests in a round-about way that the only difference in in the number of cell lines (figure caption). This is a bit confusing. Again, I am not sure what “Dataset 2” refers to. Perhaps the manuscript would benefit from the addition of some interpretation as to what you believe Supp Figure 4 means. I don’t understand the definition of your three classes of drugs (no effect (AUC 0.2); narrow effect AUC \leq 0.13 or broad affect AUC 0.13). I don’t see how this definition clearly delineates between “no effect” and “narrow effect”. The bottom paragraph of the first column is one sentence that spans 8.5 lines. It references 2 main figures of the paper and 5 supplementary figures. To be honest, this is very frustrating. I have gone through the Supplemental Methods very closely and I don’t see anywhere where the authors have distinguished between “recomputed AUC” and “AUC computed based on the common concentration range”. Then “IC_50 (figure figure) values” ?? I’m not sure what to interpret re: Figure 7 for example. To me it looks like there is excellent agreement except for perhaps the first row. Only paclitaxel fits into this “cyotoxic drug’ category but for the life of me, I don’t see where this is defined. The authors just defined three types of drugs (no effect, narrow effect and broad effect) but that’s not what they are using here. I don’t understand this. To me it simply seems to be that at low AUCs there is high variance in the last 3 distributions of the first line (17-AAG PD-0325901 and AZD6244), but actually they look like they pretty well agree at higher AUCs. I’m not sure what that means “and calculated the consistency of drug sensitivity data between studies using all common cases and only those that the data suggested were sensitive in at least on study. Maybe a table would help, especially if each of these different objects were properly defined in the Methods/Supp Methods. “Given that no single metric can capture all forms of consistency, …” So you add three more. I don’t see the point here. Why these three? and how is something like pearson \rho applied here. What is the vector? I would guess that Supp Figures should show the distribution of correlations for all three distributions so that we can look the different moments of these distributions (e.g. skew). In Figure 8, there is a use of a * but how where these p-values estimated? Are these empirical estimations of the p-value?! I am totally confused here. You say in Figure 8 that panel A is “full data”. But panel B is “sensitive cell lines”. Where is this defined? the parentheses beside this in the figure caption? But why did you introduce this “broad, narrow, no effect” definitions only to redefine something else here? I’m not sure I understand Supplemental Figure 11. Is this just all probe groups for the Affy arrays, or how were features chosen? What is an “RNA-seq expression value”? how is this formed? rpm? Most importantly, I just don’t know what the message is here, and if there is any statistics to support that statement. I’m not sure I understand Figure 9 or what the take home message should be. I have a hard time understanding the labels along the x-axis in these figure. I just don’t really know statistically how one can conclude that gene expression is more “consistent” than the drug sensitivity values. There could be a million things going on in those arrays. There are so many more datapoint and you have literally a hundred thousand probes that probably don’t have an IQR 1.5 on those arrays that “pump up” the correlation values, I would guess. What does this analysis mean? 