 Review for: Sonification of hyperspectral fluorescence microscopy datasets Summary: This paper by Mysore, et al., describes a framework and ImageJ plugin for constructing an audio signal from, or sonifying, 2-D multichannel fluorescence microscopy images. There are many positive aspects to this work. There is a pressing need for more effective approaches for human interaction with large, high-dimensional datasets. The use of audio information to enhance multi-channel image data is a key aspect of effective visualization that has not been well investigated. This is an important research topic. The manuscript is clearly written. The software is free and open source, with a broadly applicable ImageJ plugin provided. These are all commendable aspects of the manuscript. There are however, a few shortcomings in both the approach and the manuscript that should be addressed in the next revision. Most significant, the approach proposed here seems to suggest that sonification is an alternative to visualization, rather than an enhancement to visualization. It seems clear that sound will greatly enhance visualization, but the manuscript states in multiple places that sound is superior to visualization. The result is the implication that sonification is an alternative rather than an enhancement to visualization. This is a surprising and seemingly unsupported notion. This point needs clarification. Specific Comments: The authors repeatedly refer to the discriminative limitations of the human visual system compared to auditory processing. For example, “While the human vision system is limited for natively comprehending rich hyperspectral data, the human ear and auditory system are more optimally suited for this task 1 ”. This statement is untrue, and it is not supported by this reference that makes no relative measure in discriminative ability between human audio vs. visual capacity: Related comment: “whereas the eye has only three channels of spectral sensitivity, the ear can distinguish about 1400 pitches throughout its range” is another misleading statement about human visual capacity. The visual system is generally superior to the auditory system at subtle discrimination tasks. The human eye is remarkably sensitive to color variations. If visual data has only three channels, then audio has at best two (stereo) and in the present approach just one channel. One key aspect of visual vs. auditory capabilities is the color map (or tonal map) used to render image (sound) data…. Please comment on the established literature quantifying relative bandwidth and discriminatory capabilities of the human visual vs. audio perception. ...leading to another related point: “False color images of datasets with three or more spectral dimensions can visually appear to be the same in areas where the data are vastly different, because the transcoding operation used to render the raw data visible to human eyes involves a data loss”. Color mapped rendering is only lossy when the number of colors used exceeds the size of the color map. How many unique intensity bins come out of the microscope? Often, 8 bit per pixel images are sufficient quantization, and at this level it is straightforward to design a colormap that not only preserves intensity differences, but that enhances them in a perceptual manner. Even for 16 bit color images it can be possible to design effective color mappings. see e.g 2 The introduction refers to spectral unmixing as a bottleneck in multi-channel image analysis, but does not clarify why or how audio should be used as part of the solution to the spectral unmixing question. Put another way, how is the mouse-driven sonification as described in the present paper used for spectral unmixing? What is the source for Figure 1? Figure 2 seems to be sourced from references 1 , 3 – is this correct? Or was this somehow measured as part of the present work? Dimensionality reduction is only a concern if meaningful information is lost. Ultimately, all biological image analysis involves dimensionality reduction – from the raw image dataset to a pie chart or distribution or some such representation. The manuscript states that “By allowing researchers to interact directly with the spectral composition of their datasets, we can potentially elucidate salient characteristics of the data that otherwise would have gone unnoticed.” Do you have an example of this for a real-world dataset? The example datasets (Figures 9-11) all seem to show data that is not clearly visually discriminated, but this seems a function of poor visualization rather than inherent visual limitation (see more detailed comments on color mapping above). Are there differences that can be detected via sonification that would not be detectable by a human using effective visualization, or by a computer using statistical pattern recognition techniques? How about feature-driven sonification rather than pixel driven? The manuscript alludes to this in stating that “since FIJI is used for many spectral fluorescence microscopy analysis routines, it opens up the possibility of integrating and interacting with these steps as needed by the end user.” This seems a much more logical and promising approach – why isn’t it explored in more detail here? Does the method work with 3-D multi-channel images? If so, please explain how. If not, then this seems an important limitation that should be explicitly acknowledged. The system is driven by clicking and dragging the cursor across the image. This seems a cumbersome method, and a good illustration of the lack of spatial information in the audio signal. This also seems to introduce a new bottleneck into the analysis pipeline, particularly as the image resolutions continue to increase, e.g. to 4k resolutions and beyond. Consider this in light of the 450x450 pixel sample images. Can you comment on the scalability of the approach? The sound design methodology fails to account for a perceptually “pleasing” sound scheme. The present tonal model becomes quite annoying after a short amount of interaction with the image data – is there a way to capture a less abrasive representation? The manuscript states that “Another issue related to context we observed was that, similar to the case with color lookup tables, there is no “one size fits all” auditory display solution”. There are in fact a number of generally effective color mapping approaches. Specifically, there exists a significant amount of research on quantitative color spaces (e.g. HSV, perceptual spaces as mentioned above, etc.). Are there equivalent theoretical bases for audio? If not, why not? If so, please reference. Figure 6 seems misleading. Clearly a color map that can more effectively combine the channels could be formulated, particularly given the interactive environment that is enabled by sliding a mouse across the image for an audio comparison. Figure 8 also seems contrived – clearly it is possible to construct a multi-channel image that naively combines to any monotone. At the same time, it would be straightforward to e.g. learn a color space that maximizes separation among the image channels just from the given data. What would Figure 10 look like with an HSV color map? Just because the fluorophores are colored green and red does not mean that is the best color choice for representing them in a visualization context. Following the questions of naively designed color maps being easily defeated, are there equivalent limitations in your audio mapping that would allow images that are visually different to sound identical? Could you construct such an example? What about other senses in addition to sound? E.g. touch has been explored in data visualization previously 4 – would such approaches add value in combination with your method? References 1. Kollmeier B, Brand T, Meyer B: Perception of Speech and Sound. Springer Handbook of Speech Processing, Springer Science and Business Media . 2008; 65 . 2. Schanda J: Colorimetry: Understanding the CIE System. John Wiley and Sons . 2007. 3. Olson HF: Music, Physics and Engineering. Dover Publications . 1967. 4. Koerner M, Wait E, Winter M, Bjornsson C, et al.: Multisensory interface for 5D stem cell image volumes. Conf Proc IEEE Eng Med Biol Soc . 2014; 2014 : 1178-81 PubMed Abstract | Publisher Full Text Competing Interests: No competing interests were disclosed. I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above. Close READ LESS CITE CITE HOW TO CITE THIS REPORT Cohen AR. Reviewer Report For: Sonification of hyperspectral fluorescence microscopy datasets [version 1; peer review: 1 approved, 1 approved with reservations] . F1000Research 2016, 5 :2572 ( https://doi.org/10.5256/f1000research.9937.r17182 ) The direct URL for this report is: https://f1000research.com/articles/5-2572/v1#referee-response-17182 NOTE: it is important to ensure the information in square brackets after the title is included in all citations of this article. COPY CITATION DETAILS Report a concern Respond or Comment COMMENT ON THIS REPORT Views 0 Cite How to cite this report: Maddox PS. Reviewer Report For: Sonification of hyperspectral fluorescence microscopy datasets [version 1; peer review: 1 approved, 1 approved with reservations] . F1000Research 2016, 5 :2572 ( https://doi.org/10.5256/f1000research.9937.r17919 ) The direct URL for this report is: https://f1000research.com/articles/5-2572/v1#referee-response-17919 NOTE: it is important to ensure the information in square brackets after the title is included in this citation. Close Copy Citation Details Reviewer Report 23 Nov 2016 Paul S. Maddox , Department of Biology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA Approved VIEWS 0 https://doi.org/10.5256/f1000research.9937.r17919 In the manuscript titled “Sonification of hyperspectral fluorescence microscopy datasets”, Mysore, Velten and Eliceiri present the development of an “eye-to-ear” conversion plugin for Fiji. Modern imaging places a premium on extracting as much information as possible from images and much ... Continue reading READ ALL 