 Title and abstract The title is appropriate and discusses the content of the paper in one sentence. The abstract starts generally, drills down into the methods concisely and discusses the contribution to the literature which this manuscript and software project appropriately. Article content Powell-Smyth and Goldacre report on a piece of work which will make a substantial contribution to the clinical trials enterprise. They have developed an open source web application which automatically takes data from the US based ClinicalTrials.gov registry and searches for results (either summary results on ClinicalTrials.gov or an abstract on PubMed). The software then ranks study sponsors by the proportion of trials which have reported results. This approach is novel in its approach to on-line availability of data. This means that the dataset is easily searchable through a web based application. Automated systems have been explored in the past (e.g. Huse r et al 2013), as have manual searches (Tompson 2016), and the results of the automated system presented appear consistent with these. I have reviewed the online web based software, this is simple to use and demonstrates the ability of the approach to hold institutions which sponsor research to account, by summarising their contribution of results to the clinical trial literature. The central contribution is an automated system for determining if a trial registered on ClinicalTrials.gov has published summary records on clinicaltrials.gov, or has an abstract indexed on PubMed. The work hinges on whether their automated system can in fact do this. The authors make a persuasive case that they are able to find summary results and abstracts where these have been published. They provide what they have said they can do in the on-line Jupyter notebook. Additionally, the open source code in the Github repository is straightforward to read, and supports their case. Finally, I downloaded the full dataset and explored it, and in the cases which I looked at their spreadsheet had correctly identified completed trials and the accompanying Pubmed abstract. Therefore, although there may be a few trials which have been misclassified, I think that the methods used appear very robust. Additionally, if trials have been misclassified, the authors give suggestions of how to adjust this through changes to the journal entries on Pubmed, or through summary results on Pubmed. In the discussion the strengths and limitations of their automated approach are carefully elaborated upon. The key strength is that a large proportion of the clinical trials landscape is included in their study. The limitation is of course that automated analysis may incorrectly label some trials as unreported when in fact they are, but my assessment of their raw data is that this must be infrequent as I have not been able (in and admittedly unscientific sample obtained by scrolling through the raw data, and looking at trials which I am familiar with if I see them) to identify such a case. Conclusions The authors state that they present this work to aim to improve the clinical trials landscape in terms of the ‘information architecture’ of missing results. I believe that we should take this work at face value as a genuine, innovative approach which is trying to improve the problem of non-reporting, by giving transparency of reporting at the study sponsor level. It is reported carefully. The data presented back up the case for a clear need for improved trial reporting. Data This study is an exemplar of how to publish reproducible research. The data and code and extensive documentation are available and free to download and explore. My only suggestion is to have a second repository in case GitHub disappears. 