In this paper the authors discuss three general concepts to improve the construction of a risk prediction model, namely data pre-processing, ensemble modeling and recalibration. The idea is to take advantage of the Prostate Cancer DREAM challenge to show them applied in a case study example. I really like the idea, because in the literature there is a clear need for papers which can provide the practitioners with useful guidelines and suggestions (see, e.g., Sauerbrei et al. , 2014 1 ). I was therefore very positive about this paper, especially after the first part, in which the authors show in a clear and effective way the importance and possible implementations of data pre-processing. Unfortunately, in the following the standard drops quite abruptly, and the other two concepts, ensemble modeling and recalibration, are treated in a rather superficial and sometimes misleading way. Main concerns The actualization of the ensemble modeling concept, that in the paper is called "model averaging'', is discussed in a single paragraph, basically giving to the reader no details about its implementation. This is particularly unsatisfying in light of its good results in improving the predictions both in terms of iAUC (discrimination) and RMSE (calibration). Actually, there is a further sentence in the Result section which says something about the implementation, but it is again very cryptic. All in all, this part left me with the feeling that a reader cannot learn much on ensemble modeling from this paper. My strongest concerns, however, are related to the recalibration part. I agree that recalibration is important to be able to profitably use a model derived on a dataset to predict occurrences on a different dataset. Nevertheless, I am not sure that the proposed solutions are sensible. In the first approach ("high-risk and low-risk recalibration''), two further Cox models are fitted on modifications of the dataset in which the effective sample size is reduced by considering all observations either smaller or larger than a threshold as censored. I am not sure that this procedure makes sense and, in any case, I cannot see how it can help under the proportional hazards assumption. Either this assumption is valid, and this methods is useless, or it is not valid, and the Cox model should not be used in the first place. I am not sure, this procedure may help in checking the validity of the proportional hazards assumption, but not for recalibration. I was therefore not surprised to see, in Figure 7, that there is no substantial improvement in using this method (assuming that the proportional hazards assumption holds). As a further note, I think that there is some confusion with the terms "average patient", "high-risk or low-risk patient" in the text. I was also pretty surprised to see van Houwelingen (2000)'s paper 2 associated with the authors' third approach (``validation by calibration''). Despite the name used, the two methods do not have much in common. The main idea of van Houwelingen (2000)'s procedure is to use data from the target population to calibrate the Cox model in order to have better predictions in the new dataset. There is no artificial split in training and validation sets, as it seems to be suggested by the authors, but two separate datasets. And, in contrast to what is stated in this paper, this division cannot be avoided, otherwise it makes no sense to apply the method. In the current problem, the authors should have used the ``random subsets of data from the fourth validation trial'' in their steps 2 and 3. Moreover, these two steps should have been suitably modified to work for the Cox model as in formula 3 of van Houwelingen (2000). In the context of survival analysis, indeed, the ``validation by calibration'' cannot be performed through a linear model. I have not be totally able to understood the idea behind the algorithm proposed in the paper, but it seems to me that the authors are simply replicating their previous procedure "quantile recalibration'', just working directly on the time axis instead of through the survival curve. Further comments I find quite dangerous to write "After trying several machine learning and statistical models, the combined Cox proportional hazards and lasso model was chosen as it performed optimally on the validation sets''. A statistical model (or machine learning procedure) should clearly not be chosen in this way, but after rational considerations, especially when the method relies on a pretty strong assumption like the Cox model. I somehow understand that here the goal is prediction and there may not be necessary to focus on the procedure to obtain the results, but such a sentence encourage the practitioners to try whatever and pick the method which, maybe by chance, seems good on the particular case. Even here, the danger of having a model which works well only for the specific subgroups of the target population and not for the whole one (a sort of "overfitting'' in a broader sense) is high. Somehow related, it is important to note that having no-overlapping Kaplan-Meier curves is a necessary but not sufficient condition for assuming the validity of the proportional hazards assumption. I would be curious to see how much of the improvements related to pre-processing is due to the data cleaning and how much is due to the presence of new variables. In particular, whether the new variables created with the first method (PCA) are useful at all. In contrast to the second method, in which external information is included from literature research, the first method is a simple transformation of existing data, and I do not see how it can improve the prediction. The original covariates, indeed, are kept in the model, and it seems to me that there is only two times the same information. This may even be counter-productive, as lasso is well know to have problems in dealing with correlated variables. Minor concerns When mentioning methods to deal with ties in the Cox model, references should be provided and the choice implemented in the paper (if relevant) specified. Something about the selection of the lasso tuning parameter "s'' should be included, at least by describing the procedure implemented in the paper. The definition of discrimination reported in the paper is specific for binary classification problems. It would be better to (also) have a specific formulation for the survival analysis context (see, e.g., De Bin et al. , 2014 3 ). Tables 4 and 5 do not provide any information in addition to that provided by Figures 7 and 8. They should be removed. "The second criterion of calibration focused on accuracy in terms of how \dots'' -- ``The second criterion focused on calibration, i.e., how \dots''. The sentence "For extension to prediction of survival up until fixed time periods that accommodate censored observations'' is not clear to me, it may need to be rephrased. Variables are sometimes called "covariates'', sometimes ''covariables'', the notation should be consistent throughout the paper. "survive curve'' -- ``survival curve''. References 1. Sauerbrei W, Abrahamowicz M, Altman DG, le Cessie S, et al.: STRengthening analytical thinking for observational studies: the STRATOS initiative. Stat Med . 2014; 33 (30): 5413-32 PubMed Abstract | Publisher Full Text 2. van Houwelingen H: Validation, calibration, revision and combination of prognostic survival models. Statistics in Medicine . 2000; 19 (24): 3401-3415 3.0.CO;2-2">Publisher Full Text 3. De Bin R, Herold T, Boulesteix AL: Added predictive value of omics data: specific issues related to validation illustrated by two case studies. BMC Med Res Methodol . 2014; 14 : 117 PubMed Abstract | Publisher Full Text Competing Interests: No competing interests were disclosed. I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above. Close READ LESS CITE CITE HOW TO CITE THIS REPORT de Bin R. Reviewer Report For: Three general concepts to improve risk prediction: good data, wisdom of the crowd, recalibration [version 1; peer review: 2 approved with reservations] . F1000Research 2016, 5 :2671 ( https://doi.org/10.5256/f1000research.9340.r18283 ) The direct URL for this report is: https://f1000research.com/articles/5-2671/v1#referee-response-18283 NOTE: it is important to ensure the information in square brackets after the title is included in all citations of this article. COPY CITATION DETAILS Report a concern Respond or Comment COMMENT ON THIS REPORT Views 0 Cite How to cite this report: PÃ¶lsterl S. Reviewer Report For: Three general concepts to improve risk prediction: good data, wisdom of the crowd, recalibration [version 1; peer review: 2 approved with reservations] . F1000Research 2016, 5 :2671 ( https://doi.org/10.5256/f1000research.9340.r17698 ) The direct URL for this report is: https://f1000research.com/articles/5-2671/v1#referee-response-17698 NOTE: it is important to ensure the information in square brackets after the title is included in this citation. Close Copy Citation Details Reviewer Report 28 Nov 2016 Sebastian Plsterl , Institute for Cancer Research, London, UK Approved with Reservations VIEWS 0 https://doi.org/10.5256/f1000research.9340.r17698 Kondofersky and co-authors discuss three important aspects when developing risk prediction models: good data, model averaging, and recalibration. In their paper, they evaluate the added value of each of these concepts in the context of the Prostate Cancer DREAM challenge. ... Continue reading READ ALL 