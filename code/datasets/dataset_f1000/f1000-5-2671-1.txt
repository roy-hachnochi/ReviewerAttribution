Kondofersky and co-authors discuss three important aspects when developing risk prediction models: good data, model averaging, and recalibration. In their paper, they evaluate the added value of each of these concepts in the context of the Prostate Cancer DREAM challenge. Their results demonstrate that data pre-processing and deriving additional variables resulted in the largest improvement, followed by combining predictions from a diverse set of models, whereas the benefit of recalibration was modest. The paper addresses many questions researchers are facing with when building a risk prediction model from data collected from distinct clinical trials. They elegantly overcome the problem of trial-specific variables by employing an ensemble of models, where each model is specific to a particular trial (or a subset of multiple trials), which allows to leverage the full set of variables collected for a particular trial despite there being only a small set of variables that is common to all trials. Overall, the paper is well written, easy to follow, and features an extensive set of experiments. Major concerns: Preprocessing : Some aspects of preprocessing and missing value imputation are not well explained and should be addressed in a revision of the paper. It is not clear whether “high-cost data cleaning and preprocessing” was used as an alternative to the “low-cost minimal adaptation approach” or in addition to it. In the paragraph on the low-cost approach, the authors say that they “excluded variables with more than 10% missing values in either the training or test set”, but later in the paragraph on the high-cost method, they state “we identified incomplete (e. g. more than 70% missing values in either the training data or the test data)”. If the high-cost approach follows the low-cost approach there should not be any variable with 70% missing values any more. The paragraph on preprocessing on page 6 mentions that highly correlated variables were removed, but is unclear whether this step occurred before or after missing value imputation using MICE. If redundant variables were removed before imputation, this could significantly increase the error of imputed values, because in contrast to model building, highly correlated variables can be very helpful for the purpose of imputation. In the description of the low-cost approach, they authors mention that missing values were imputed by the mean. How were categorical variables, which have no mean, imputed? Assuming the low-cost approach was the baseline method denoted as “standard data” in figure 7, it is difficult to judge whether the improvement due the high-cost approach was due to a less biased imputation approach (MICE instead of mean) or the addition of additional variables. It would be very interesting to see what the improvement would be if the same imputation method was used and only additional derived variables would have been added. Since the standard data was augmented by several derived variables, an important question is to determine which of the derived variables had the largest impact, or if it was their combined effect that resulted in the observed improvement. Model averaging : Based on the results of figures 7 and 8, model averaging resulted in a considerable improvement in risk prediction, however not much technical details were provided. The authors should provide a more elaborate description or refer to a previously published work describing the details. In particular, the authors mention that each of the 7 combinations of trials in figure 2 was imputed independently using MICE and subsequently used to construct a penalized Cox model. In this setting, two levels of averaging are necessary, first across the multiple imputed data sets, and second across the combinations of trials. The former is usually achieved by applying Rubin’s rule and the latter can be achieved using several methods ranging from simple averaging to learning an additional meta-model on top of the individual models’ risk scores. A description of the approach selected by the authors would be highly appreciated. Recalibration : Kondofersky and co-authors describe an additional ensemble approach were they combine a low-risk and high-risk model with an average model. It seems that the authors assumed that the proportional hazards assumption does not hold for a subset of patients and tried to address this problem by model averaging. Although this is an interesting idea, the authors unfortunately did not provide an explanation – besides their empirical evaluation – why this approach might be useful. In fact, results in figure 7 show that this approach has little benefit. It would be very interesting to investigate whether this was due to the proposed recalibration approach itself or the choice of evaluation measure, which failed to measure the improvement. The authors state the using the iAUC score to measure the benefit of their methods is flawed, because it is invariant to monotone transformations. Maybe more insight could be obtained if the iAUC score is substituted for the time-dependent Brier score, which does not suffer from this problem. Little details on the construction of the low- and high-risk models has been provided. The authors write that they “modified the target variable DEATH […] such that it only counted events that happened prior to 14 months” regarding the high-risk model and that they “only considered events that occurred after 18 months” regarding the low-risk model. Does this mean that some patients were excluded entirely to construct the low- and high-risk models, which would lead to right and left truncation, respectively? A Cox model trained on the truncated data would thus be biased if truncation is ignored. The cut-offs for the low-risk and high-risk group is based on 25-percentile and median, respectively. Are the risk groups defined based on the training data, or are new risk groups constructed each time when predicting previously unseen data? In addition, are the same risk groups used for all 7 models in the ensemble? Finally, the authors sate that “modifications only altered the ranks of patients within the defined ranges”. Limiting recalibration by a hard threshold means that risk scores at the limits of the respective intervals are only allowed to move into one direction, whereas risk scores located in the middle have much more freedom to move around. It would be interesting to investigate whether a soft threshold that pulls the recalibrated risk scores towards the original threshold without establishing hard boundaries would be beneficial. Minor concerns: Preliminaries (Paragraph 1): The hazard ratio should be written as exp(β j ), not exp(β), because β has been used as a vector before. It should be mentioned that the traditional Kaplan-Meier estimator is only applicable to right-censored data, not any type of censored data. To check the proportional assumption it is not sufficient that estimated Kaplan-Meier do not intersect, they should be parallel to each other. In addition, other methods such as goodness-of-fit tests can be employed as well. Preliminaries (Paragraph 2): For the sake of completeness it would be good to reference methods used for accounting for ties in event times. Methods (First concept: good data) It is not explicitly stated which trials comprised the training data, although this information can be inferred from table 1. Methods (Table 1): The table should list exact p-values, especially for the range 0.001 to 0.05 Methods (Preprocessing) Were only redundant numerical features identified and removed (using Pearson correlation) or was a similar approach applied to categorical data too? If yes, which measure was used to assess whether they are redundant? Methods (High-risk and low-risk recalibration) “we calculated the average between the initial model and the low-risk model”: meaning averaging the model’s predictions as for the high-risk model? Methods (Quantile recalibration) How was weight of the lasso penalty determined? Results (Impact of wisdom of the crowd) Which trials were used to obtain the “standard approach”? ASCENT-2, MAINSAIL and VENICE, or only a subset of these? Grammar: Introduction (Paragraph 3): “either for statistical reasons or grounds content” Methods (Cleaning of core table): “In a first data cleaning” → add word “step” at the end Methods (Second concept wisdom of the crowd): “the readable book by Surowiecki”: no need to emphasise that books can be read “the opening of challenges to mass numbers of competitive team” → massive numbers Methods (Quantile recalibration): “survive curve for patient i” → survival curve 