In “Communicating behavioral genetics: Charting the limits of the genetic interpolation effect” the authors report research from an experimental trial run online which explores how exposure to genetic information about human behaviors affects beliefs about the genetic (versus environmental) determinants of social behavior. This topic is an important area for research given the often widespread misunderstanding of genetics and behavior, which is both perpetuated through genotype in the media and also through science education. Furthermore, genetic determinism has been shown to be implicated in a variety of social prejudices by social psychologists. Consequently, the null findings reported in this research could be significant to many science communication scholars, science education researchers, or social psychologists interested in public understanding of genetics. To make that contribution more clear, however, the author needs to attend to some theoretical and methodological issues in this present draft of the manuscript. Those issues are: A clearer and stronger theoretical and/or empirical rationale for testing a softer treatment to chart the limits of the genetic interpolation effect. Clearly defined confirmatory and exploratory hypothesis tests which are then mapped onto a post-hoc power analysis to demonstrate that the trial was well-powered, and thus, that the null findings are significant. More care needs to be taken in interpreting the null findings. At present, the authors provide a partial explanation for the null findings with the idea of straightlining. However, by their own account this explanation might only capture 6-10% of the null effect.Thus, a more convincing explanation for the null effect is needed to guide future research. In sum, the significance of a null-experimental finding should be judged on its ability to guide future research. Null findings, when well powered, should help researchers decide which paths NOT to go down. And, they should provide some theoretical reason for why such paths lead nowhere. At present, the manuscript does not accomplish this task. But, with major revisions it might be able to make this contribution. Thus, I recommend that the study be approved with reservations. If all of the issues outlined below in my critique of the piece can be addressed by the author, then I think this piece will make a significant contribution to the public understanding of genetics. Theoretical issues: I would have liked to see a more nuanced discussion about the possible mechanisms which link the reading of behavioral genetics research to genetic attributions of complex human traits. For example, if the genetic interpolation effect is actually based on anchoring and adjustment, then one would expect that individual differences in people’s prior conceptions about the genetic basis of human traits might moderate the activity of the genetic interpolation effect. For example, the author hypothesizes that “Some people may react to behavior genetics by thinking to themselves that "if genetics is strong enough to have a significant influence on this complex social trait, then its general influence on other social traits must be stronger than I had imagined," thus causing them to infer greater genetic influence for other complex characteristics not mentioned in the news article content.” But, what if people already strongly believe that social traits are caused by genes. In this situation, a ceiling effect might exist, thus preventing the statistical detection of the genetic interpolation effect. Under a ceiling effect model, we would expect a Treatment x Prior genetic belief interaction, where those who do not attribute social traits to genes before reading are affected by the experiment and those who do are not impacted by treatment. Conversely, it could also be the case that those who are generally averse to the idea that genes cause human social traits are inoculated against any impact that reading about behavioral genetics might have on genetic interpolation. Thus, the Treatment x Prior genetic belief interaction on the genetic interpolation effect could be the opposite of what I have previously stated. The point is that prior knowledge and beliefs could have washed out the treatment effects and produced the null findings of this experiment. And, there is a large body of reading comprehension research which shows that the meaning people construct from readings varies with their prior knowledge. Hence, to interpret these null findings some discussion of the impact of prior knowledge is needed both in the theoretical motivation for the study and during the discussion. Indeed, the authors allude to this need when they discuss an unexpected finding in their results. They state: “Noticeably, the results also present an unanticipated finding: compared to Group 1, Group 2 and Group 3 show lower average genetic attribution for natural hair style. The genetic interpolation hypothesis offers no explanation for this phenomenon, but something else may be at play here.” Perhaps the explanation is that individual differences in people’s prior beliefs caused the null effect? The authors partly explain this finding with straightliners who varied significantly in proportion by condition. However, only 6-10% of straightliners could be inferred from their responses in each condition. So, the above issue – the issue of prior knowledge and beliefs affecting how people constructed meaning from the texts – might help to explain the other part. I would encourage the authors to analyze and report any findings they have (or have not reported) which explore how responses in the baseline data help to resolve the mechanism for the null results. Methodological issues: Because the researchers apparently did not measure prior knowledge of genetics or genetic beliefs prior to reading, then this could also be discussed as a methodological limitation of the present experiment. The theoretical and empirical rationale for testing a shorter treatment given in the introduction is not convincing nor is it compelling. For example, the authors state, “The purpose of this study is threefold. First, the main goal is to test if the persuasion effect (H1) and the genetic interpolation effect (H2) also emerge following exposure to a softer treatment. Addressing this issue is necessary to better capture what kind of message can trigger the genetic interpolation effect”. If this study is telling us anything about the world, then it should tell us what it is actually modeling in the world through the experimental design. For example, is the shorter reading on behavioral genetics more akin to what science students might read in a textbook, and thus, worth investigating? Or, is it the kind of message that people might come across on PubMed? In short, why is the shorter reading important to investigate? There are four conditions and 2080 participants. Given that the main finding in this study is a null finding, some care and effort needs to be taken to explain the a priori power analysis and post-hoc power analysis for the trial. For example, if the experiment is underpowered, then the significance of the null finding is diminished if not destroyed. This issue is exacerbated by the multiple comparisons in the study. With four groups in the study there are three comparisons that can be made with the control group. Then, there are three different measures. So, in sum total there are at least nine-different statistical tests that could be run to test hypotheses. The authors should outline which of these tests are the confirmatory tests and which are the exploratory tests. Alpha values, and hence p-values, should be adjusted to account for Type-I error on the confirmatory tests. And, then, based on the actual effect sizes, and the adjusted alpha criterion, the authors should report what their post-hoc power was on each test. If the reach 80% power and their finding is still null, then such a null finding, in my mind, is worthy of indexing. However, if the authors were underpowered, then the contribution of these findings to the research community is less clear. If we take the authors at their word, then with 2080 people in their panel and a response rate of 62.6%, there are about 1300 people enrolled in the experiment. If we divide that number in half (n = 650), that is the amount of people present in any single comparison with the control. Then, we can divide 0.05/3 to account for the three different variables per treatment-control contrast. If we assume there is no baseline data which would improve the precision of the estimate by increasing the R2 in the models, then any single treatment-control contrast could detect an effect size of d = 0.25 or greater. If we, instead, adjust alpha to account for all possible treatment control contrasts on all variables (i.e. 9 tests or 0.05/9 = 0.0055), then effects of d = 0.28 or greater should be detectable. So, the question is, how big were the effects. Unfortunately, the standard deviation for the control group is not reported, nor are the pairwise effect sizes. If the authors provided that information, then the readers would have more confidence that the null effects are significant and merit indexing.