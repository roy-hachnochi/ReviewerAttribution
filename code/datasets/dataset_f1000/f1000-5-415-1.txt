This is a very well written paper discussing the important and timely topic of reproducibility. As an opinion piece it is factually accurate, but despite promising introduction to the topic, it provides very little in terms of practical solutions. Listed proposal lack detail (How is the replication going to be defined in the new metric? Who decides if experiment A replicates experiment B? Why would commercial publishers bother implementing and maintaining such metric?) and poor understanding of the complex system of incentives in the academic world (Why the NIH does not have a stronger position on replicability and data sharing? Why there aren’t enough good quality standards in life sciences? Why PIs chase high impact factor journals instead of replicating other people work?). Unless the second part of the paper undergoes a major revision and provides practical, detailed and feasible solutions I doubt this publication will have a non-negligible influence on the reproducibility crisis. Other comments: You say “When applying for positions in academia, publications are king with quantity being above quality (...)” - all I keep hearing is that it takes one Nature paper to get a tenure. Do you have any evidence of the “more papers is better than good papers in terms of landing a job” claim? You also mention the it is hard to assess how many null results are not published. However, in the context of meta-analysis there are techniques to assess publication bias from the expected shape of the effect size distribution. Isn’t there any piece of meta-research assessing what percentage of experiments should by chance yield a null result? It would be definitely worth looking for. 