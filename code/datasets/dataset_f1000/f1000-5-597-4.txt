This is potentially an important negative result in QSAR, however I think some revision is necessary because some aspects are unclear. The title “Understanding covariate shift…” is a little weak. One could say “Failure of covariant shift to improve model performance…” It needs to be explicitly pointed out in the introduction that in most QSAR one builds a model then is able to predict arbitrary compounds. On the other hand, to use covariant shift, one must know which molecules one is predicting before one can generate the model. One can regard “lazy learning” as an extreme version of covariant shift: neighbors of the test set molecules are given weights of 1.0 and all other molecules are given weights of 0. I need a little more explanation in words of how the weighting is done for training set compounds. Since we are using substructure descriptors here, I am finding it hard to visualize. For example, are we just using distance to the nearest test set example, or are we looking at overlap of the training set descriptors with the distribution of test set descriptors? Practically no explanation is given as to what QSAR methods are being used. I know what K-NN is and I presume LR is linear regression. Why weren’t popular methods like random forest, SVM, or PLS tried? The color key in Figure 1 does not seem to match what is in the text. In any case, perhaps a better way of looking at would be the enclosed figure .