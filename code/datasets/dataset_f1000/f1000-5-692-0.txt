The authors present a data note which aims at describing a data set by giving details on how data was collected and processed and which software or protocols were used, but which will not provide an analysis of the data, results, or conclusions. The authors met these F1000Research requirements and, accordingly, describe the setup of the survey, give details on the sampling methods and ways of disseminating the survey request, and briefly introduce the distribution of responses. They also discuss the population, sample size and response rate as well as the completeness of responses and observed and biases in the data. Information on the post-hoc data processing (i.e., anonymization, cleaning, and harmonization of data) is also given. The data note finishes with a quantitative description of the data, how it is stored (i.e. openly on zenodo, as required by F1000Research), and how it can be accessed. Overall, the description of the data and the data processing is sound, seems to be reasonable, and as far as I can assess meet the standards of studies of that kind. The data generation is also suitable for investigations of usage of tools and the data set will serve the understanding of scholarly communication in the digital era in general and on social media in particular. Moreover, the data set cannot only answer if researchers use particular tools but also for what purposes or in what steps of the research cycle respectively. However, to get a more complete view on how the data has exactly been processed and collected, as well as to enhance repeatability of the study and to aid interpretation of results in later research making use of this data set I recommend adding information to following questions (which mostly refer to initial premises set by the authors of the survey and which have to been known in order to comprehend the processing steps that have been taken): Activities were selected from a database developed by the authors. How did you create the database – how did you find the entries? Could tool providers register themselves? How complete is it? Described activities in the survey were chosen for their overall importance/ the most well-known tools were selected as answers: How do you define “overall importance” and “most well-known”? How did you determine this selection? Can you provide evidence (even if this is a data note)? Have you taken into account disciplinary peculiarities? Was it possible to choose more than one tool as answer in the survey (adding up to answer numbers 100%)? Was it possible for participants to answer the survey more than once? Have you detected any bot-like behavior? Six obvious spam answers have been removed from the data set: can you give examples on what was considered spam? 