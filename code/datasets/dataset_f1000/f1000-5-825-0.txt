The paper highlights the curious lack of rigorous standards for what constitutes ‘agreement’, ‘consistency’ between genomic studies, or more generally, the fundamental issues of ‘validation’ and ‘reproducibility’, etc. The problem is even more serious of results based on high-throughput omics data as the potential for false positive is substantial. The persistent lack of consensus or standards may partly indicate that these issues are not so straightforward. The main problem is that when we say we ‘validate’ a result, this can be done at different strengths. For example, consider the commonly performed method in statistical analyses, the so-called ‘cross-validation’, where we split our total sample into training and validation sets. If the split is done randomly, then we have only a ‘soft validation’, since it applies to the same sample (or same lab, same population, same measurement method, etc) so the ‘validation’ is internal and corresponds to statistical significance only. In contrast a scientist may wish for something stronger, for an external validation, for example, for the ‘biological truth’ to apply other populations; thus, one study may be performed in a European population, but the external validation is done in an Asian population. The latter is a stronger validation than the random-split validation, giving a more compelling and general biological story. What is relevant here is that both validations are commonly done in practice, and both are valid, but they carry different levels of information. I think what matters in practice is that the implication of the validation should always be clear (or clarified), so that the user of the information can judge its relevance. The key point of Safikhani et al is that their 2013 validation study of the genomic predictors of drug-sensitivity was more stringent than the 2015 validation studies by the GDSC and CCLE investigators. This is clearly highlighted in Figure 1, where the latter used the same molecular data, so the ‘validation’ is only of the pharmacological data and perhaps (not clear to me) the method of analyses. Which level of validation is more relevant here? Let us imagine how the results (eg the genomic predictors) are to be used in patients. The molecular data are likely to be generated and analyzed in a diversity of labs, so the genomic predictors should really be robust to the actual heterogeneity in the molecular data. The results (the genomic predictors) may not survive such stringent requirements, but that is what we need to know. So, overall, I agree with Safikhani et al that a more stringent validation allowing for variability in both molecular and pharmacological data is more relevant in this context of drug prediction. (However, reading Haibe-Kains et al , there seemed to be an emphasis that the failure of agreement was due to the high variability in the pharmacological data. So it is possible that the later studies by the GDSC-CCLE investigators responded to this concern only.) Regarding specific issues in the paper: I do not consider the use of most recent data as a key issue. I agree that the choice of IC 50 in GDSC vs AUC in CCLE is puzzling and only raises a question mark regarding the results. Arbitrary cutoffs in defining what constitutes an ‘agreement’ are unnecessary if authors can refrain from using judgmental words like ‘high consistency’ etc., especially when used as a summary statement across distinct drugs. It would be better to just report the actual performance for each drug or for each cancer type, since it is still not clear how these statistics would translate in terms of clinical cost-benefit balance. 