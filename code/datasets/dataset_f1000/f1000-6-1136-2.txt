The report describes the two winning ligand-based virtual screening methods of the 2014 Teach-Discover-Treat exercise. Both workflows and the supporting data are available in their entirety online (behind a request for the user's email address) and are adequately described in the manuscript. The report is of general interest to the community and a useful resource to practitioners in ligand-based drug discovery. I have a few minor suggestions for strengthening the manuscript. Page 4. A few sentences describing "heterogeneous classifier fusion" would be appreciated. Page 5. Descriptors. I would be interested in knowing the number of bits (i.e. unique ECFP/FCFP fragments) required to represent the full dataset (that is, the number of features in the input, which I suspect is actually larger than the number of examples?). Page 5. Task 2. The weights for the two models found by the linear regression would be interesting to report (is one model favored more heavily than the other?). Table 4. This would be a bit more informative if variance was reported as well. Figure 2. It isn't clear to me exactly what this is reporting. Is this the distribution of all possible pairs between the two sets? Please clarify. Table 5. My understanding is that the Rank (1000) numbers are essentially meaningless as the compounds were (accidentally) randomly selected. Can the corrected top 1000 ranks be provided as well (or instead) and clearly labeled as such (realizing that not all compounds will have such a rank). It's also hard to get a sense of enrichment from these numbers since only 114 compounds were tested but the ranks have a much larger span. For example, the workflow 2 active compounds have poor ranks (500), but this is misleading since there were no highly ranked (novel) compounds tested. I would really appreciate some visualization of enrichment relative to ranking (e.g. ROC curve) for 114/81 compounds tested for workflow 1. 