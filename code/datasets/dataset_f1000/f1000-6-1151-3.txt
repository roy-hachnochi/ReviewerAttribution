This manuscript is a herculean effort and enjoyable read. I learned lots, (and I think the paper will be a good resource for anybody interested in the field of peer review) which for me is usually a good sign of a paper’s worth. The authors report on many aspects of peer review and devote considerable attention to some challenges in the field and the enormous innovation the field is witnessing. I think the paper can be improved: 1. It is missing a Methods section. It was unclear to me whether the authors conducted a systematic review or whether they used a snowballing technique (starting with seed articles) to identify the content discussed in the paper? Did the authors search electronic databases (and if so which ones?) What were their search strategies and/or did they rely on their own file drawers? Are all the peer review innovations/systems/approaches identified by the authors discussed or did they only discuss some (i.e., was a filter applied?)? With a focus on reproducibility I think the authors need to document their methods. 2. I think the authors missed an important opportunity to discuss more deeply the need for evidence with all the current and emerging peer review systems (the authors reference Rennie 2016 1 in their conclusions. I think the evidence argument needs to be made more strongly in the body of the paper). I do not think the paper is strong enough regarding the large swaths of the peer review processes (current and innovations) for which there is no evidence 2 and it is difficult to gain access to peer reviews to better understand their processes and effectiveness – open the black box of peer review 3 . 3. There is limited data to inform us about several of the current peer review systems and innovations. In clinical medicine new drugs do not simply enter the market. They need undergo a rigorous series of evaluations, typically randomized trials prior to approval. Shouldn’t we expect something similar for peer review in the marketplace? It seems to me that any peer review process/innovation in development or released should have an evaluation (experimental, whenever possible) component integrated into it. Without evaluation we will miss the opportunity to generate data as to the effectiveness of the different peer review systems and processes. Research is central to getting a better understanding of peer review. It might useful for the authors to mention the existence of some groups/outlets committed to such research – PEERE (http://www.peere.org/) and the International Congress on Peer Review and Scientific Publication (http://www.peerreviewcongress.org/index.html). There is also a new journal committed to publishing peer review research ( https://researchintegrityjournal.biomedcentral.com/ ). 4. In section 1.3 of the paper the authors could add (or replace) Jefferson 2002 4 with Bruce 5 . The Bruce paper is also important for two additional reasons not adequately discussed in the paper: how to measure peer review and optimal designs for assessing the effects of peer review. Concerning measurement of peer review, there is accumulating evidence that there is little agreement as to how best to measure it. Unlike clinical medicine where there is a growing recognition of the need for core outcome set assessments (http://www.comet-initiative.org/) across all studies within specific content areas (e.g., atopic eczema/dermatitis clinical trials) we have not yet developed such an approach for peer review. Without a core outcome set for measuring peer review it will continue to be difficult to know what components of peer review researchers are trying to measure. Similarly, without a core outcome set it will be difficult to aggregate estimates of peer review across studies (i.e., to do meaningful systematic reviews on peer review). Concerning the second point – there is little agreement as to an optimal design to evaluate the effectiveness of peer review. This is a critical issue to remedy in any effort to assess the effectiveness of peer review. 5. The paper assumes (at least that’s how I’ve interpreted it – the paper is silent on this issue) that peer reviewers are all similarly proficient in peer reviewing. There is little training for peer reviewers (new efforts by some organizations such as Publons Academy are trying to remedy this). I started my peer-reviewing career without any training, as did many of my colleagues. If we do not train peer reviewers to a minimum globally accepted standard we will fail to make peer review better. 6. Peer review does not function in a vacuum. The larger ecosystem includes other players’ most notably scientific editors. There is little discussion in the paper about this relationship and its potential (dys)function 6 . 7. In section 2.2.1 you could also add at least one journal has an annual prize for peer reviewing (Journal of Clinical Epidemiology – JCE Reviewer Award: http://www.jclinepi.com/) 8. In the competing interests section of the paper it indicates that the first author works at ScienceOpen although the affiliation given in the paper is Imperial College London. Is this a joint appointment? Clarification is needed. A similar clarification is required for TRH. 