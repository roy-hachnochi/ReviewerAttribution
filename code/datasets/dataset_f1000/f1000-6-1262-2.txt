First of all, I would like to congratulate the authors for making the dataset available, which should allow interested scientists to explore the data and enrich the research they conduct with more information that can eventually lead to helpful new discoveries. It is remarkable that more than one processing stream was used (FSL and AFNI for functional, and volumetric and surface-based for structural), and further, three different inference approaches were considered, all of which are a great bonus in terms of comparisons among methods. I have very few concerns about the current version of the manuscript (v1, dated 28/July/2017): Page 2, 1st column, 2nd paragraph of the Introduction: "giving researchers the freedom to fit many different models that incorporate different denoising schemes": as stated, it may suggest that it would be adequate to simply run multiple models, without attention to excess error due to multiple testing. Perhaps a different wording such as "giving researchers the freedom to choose their own denoising schemes" could still accommodate what the authors may have wished to state. I cannot find information about the subjects. Who were them? Where was the data collected? With what scanner and sequences? Who approved the protocol? Presumably this information is on reference #1 but it cannot hurt to have that information here. It would be good if a few more details on what exactly FMRIPREP does could be given, without having to rely completely on external links that may no longer be available in the future. Even more important considering that results did change with after minor version changes. Page 2, 1st column, 4th paragraph of the Methods: As written, a reader may think that the input images to FreeSurfer were those non-linearly aligned to the MNI space, which surely was not the case. But if that was, then the FS analysis would have to re-done as the warps affect thickness and area measurements. Page 3, 2nd column: Regarding the masks, one would have thought that using the mask from FEAT/FLAME could have been a good shortcut instead of creating a new one for randomise. Why wasn't that done? Page 3, Validation section: The strategy using the mask contours to investigate between-subjects registration is surely not a good one for not showing how the overlap between structures. A hyperslab across subjects would have been more informative. Moreover, the contours shown in Figure 1 are a bit concerning for suggesting somewhat suboptimal registration. Still in the Validation section: what exactly is being validated here? It doesn't seem to show that the dataset would be valid or not valid in any particular aspect. Consider investigating some specific validation parameters over different aspects (e.g., registration, bias correction, surface reconstruction, the tasks eliciting expected response, etc), or remove this section altogether, as it can be misleading for suggesting that the dataset is "valid" somehow. Page 6: The description of the files is extremely helpful. I note that one of the files listed has extension .h5. Is this HDF5? If yes, please state so. I believe this format was used for the lack of another option, but in fact, this is a great format that probably should in the future be an option for most imaging data we use (both surface-based and volume-based). Of the FreeSurfer surfaces, the white is the most important one, not pial or midthickness. The pial is computed after the white already exists, and its exactness depends on the white. The midthickness does not match any particular tissue border, and if one measures surface area from it, that area will depend on thickness, which would make it a poor phenotype. It would be great if the white surface files could be provided. Page 7: I find it concerning that information and resources about this dataset are scattered over the internet: There is the current paper (PDF) and its Supplementary Material on F1000, then there are results stored in NeuroVault, source code on Github and Zenodo, and finally, a Docker container on DockerHub. Could not a copy of all these pieces be on a single place that can be simply downloaded and maintained on the long term, e.g., in DataDryad? How can the readers be sure that all these links will be alive in 10 or 20 years? 