This opinion piece is a detailed explanation of what the authors believe to be some of the best tools for promoting one's research online. The authors explore many aspects of online engagement, and make a solid case for why the average researcher should care to promote her own work. They are careful to address caveats in their argument (e.g. how disciplinarity or region might affect an author's ability to effectively promote his work on, say, Facebook). They cover a great deal of ground and share many relevant tools that would be valuable to the average researcher. In general, the piece could make a stronger argument for how these tools help one to achieve true impact (rather than just drawing attention to one's work--the Olympicene example is a good illustration of the difference here--or being a prolific author--which is what one is arguably tracking when adding papers to ORCID or Google Scholar). More evidence beyond the authors' own experiences would be helpful to see. Positioning arguments in relation to existing resources (e.g. the Fast Track Impact Handbook and the Impactstory 30 Day Impact Challenge) can help readers understand the unique importance of this paper's recommendations to their own work. The metaphor of "new alchemy" might also be explained in greater depth, and more connections made from it to the specific recommendations, limitations, etc contained in the article. The Conclusion points to the need for an integrated suite of tools, to avoid duplication of effort on the part of researchers, but I'd argue that Kudos gets one part of the way there--it's a one-stop-shop for promoting one's work across a number of social media sites. The "organizational impact" section could be cut, as most of the paper focuses upon how individual researchers can manage their own outreach/engagement. It's possible that the piece could be strengthened overall by choosing one area to focus upon (e.g. data sharing OR online networking), rather than trying to cover so much ground. There are points throughout the paper where the authors' opinions are presented as fact. I suggest clarifying. There are also several points where the authors link to blog posts as opposed to peer reviewed research that exists on a topic (e.g. the CostalPathogens post ). The paper would be strengthened by a thorough literature review on the topics presented here and supporting arguments being made on the basis of those published papers, in addition to blog and Twitter posts. I'll speak specifically to the altmetrics-related content from here, given my area of expertise. Disclaimer: I'm employed by Altmetric. The authors describe altmetrics as including citations, views, etc alongside other types of data, but many would argue that altmetrics are distinct from citations and usage statistics, and that cites and usage statistics are better described as article-level metrics. To that end, a brief explanation of why article-level metrics are more useful than journal-level metrics like the journal impact factor is probably appropriate here (assuming that the intended reader--a beginner--is not likely to have heard such arguments before). The tweet from Bilder that is used as the basis of claiming certain coverage for altmetrics across the research literature is actually just the coverage provided by one service (Crossref Event Data). While it's still useful as an illustration, that should be clarified in the text. In the sentence where "altmetric algorithms" are described, it would be helpful to explain what you mean by "produces increasingly relevant results". It would also be helpful to readers to hear more about at least one recommended altmetrics tool that they can use to track attention to their work (e.g. Impactstory profiles or the Altmetric bookmarklet), and how the promotion strategies you recommend affect various types of altmetrics (more examples like the Kudos/NTU study would be good here). Finally, the authors should take care to distinguish Altmetric (the company) from altmetrics (the larger data type that's provided by many companies, including Impactstory and Plum Analytics in addition to Altmetric). Specifically, avoid referring to "altmetric scores" (too easily confused with the proprietary Altmetric Attention Score) or to the company Altmetric as "Altmetrics". 