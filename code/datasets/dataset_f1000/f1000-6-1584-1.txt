The authors propose to incorporate findings from mathematical modeling studies into the development of WHO guidelines and other processes related to evaluating and developing public health policies. They argue that evidence from modeling studies should be included in the Grading of Recommendations Assessment, Development and Evaluation (GRADE) process, and make specific recommendations to that effect. The authors argue that model credibility is more appropriate than risk of bias for evaluating strength of evidence generated by modeling studies. The paper is based on discussions and findings from a meeting of modeling experts in Geneva in 2016; the authors were also participants in the meeting. The paper lays out a structured argument for incorporating modeling studies into the evidence base, particularly for formulating WHO recommendations related to treatment of HIV. The authors start by providing a review of how models are used in various fields, with suggestions about how they can inform guideline development. They address the question of what constitutes a modeling study. A comprehensive accounting of published literature on assessment of models is provided. Finally, they give recommendations for how models can be evaluated using the GRADE approach, with specific conclusions about important issues such as when modeling studies should be used as part of the evidence base and how their credibility should be assessed. Given the sweeping variety of models used in published studies about HIV treatments, policies and interventions, the authors are to be applauded for putting forward a framework for having this conversation. It will promote broader understanding of how models work and how they can be most optimally used for informing treatment guidelines. At the crux of their argument is the claim that evidence generated from models should be judged in terms of model credibility rather than on risk of bias. This argument raises several important issues. First, what constitutes a mathematical model? If a model is to be evaluated on its credibility, we need a definition to work from. Second, what kinds of output from models should be considered evidence, and how should the quality of that evidence be judged and ultimately weighed against or combined with evidence generated from randomized trials and observational cohort data? What is a mathematical model? According to the authors, mathematical modeling is “a mathematical framework representing variables and their interrelationships to describe observed phenomena or predict future events.” On its face this is surely true, but for the purpose of understanding whether and how models should add to the evidence base, it’s too broad. This definition covers a vast assortment of mathematical models, ranging from validated descriptions of natural phenomena (where the mathematical relationships are known and directly observable) to representations of progression from HIV infection to death (comprising known and unknown mathematical components, many of which cannot be directly observed). Consider three examples for illustration: The mathematical representation of radioactive decay is known and can be written down explicitly. The model enables accurate and replicable predictions of future observations. The mathematical model for absorption of a specific drug is typically not known, but empirical studies have shown that it is possible to approximate the systematic variation using nonlinear equations. These models incorporate known information about physiology and properties of a specific drug, but are necessarily oversimplified representations of drug absorption because there are unobservable characteristics of individuals that affect absorption. The models can be used to make reliable predictions on average, but require unexplained variation to be reflected in terms of prediction intervals. Now consider a model of the population dynamics of HIV infection and disease progression. This process also follows a mathematical model, but the model itself is highly complex. Unlike radioactive decay or rate of drug absorption, the mathematical representations of several components of the underlying processes are essentially unknown. Moreover, much of the data needed to inform the models are either unobserved (e.g. timing of HIV infection) or only sparsely observed (e.g. individual-level viral load). All of these are mathematical models, but definitions must distinguish between them. Otherwise there is an implied equivalence that lends more credibility than is deserved to models that are heavily reliant on unverified assumptions about the mathematical structure underlying the dynamic system being modeled. A more systematic classification of model types would therefore be helpful. While the authors' definition of mathematical model is overly broad, the definition of statistical model, used to contrast with mathematical models, is too narrow. A statistical model is used to characterize sources of variation in observed data. It is based on a probabilistic representation of the data generating mechanism, which is itself a mathematical model. Theory and methods of statistical inference provides a rigorous and transparent set of techniques for parameter estimation, prediction of future outcomes, extrapolation (e.g. for causal inference), and uncertainty quantification. The last of these, uncertainty quantification, is a critical and frequently missing component of predictions based on mathematical models. For the purposes of generating evidence for WHO recommendations, the main difference between a mathematical model and a statistical model is that mathematical models tend to have broader scope and incorporate higher dimensions of complexity, but rely more heavily on assumptions about underlying mathematical structure than on individual-level data. Statistical models tend to have less mathematical complexity and more narrow scope, and are typically fitted to a single (possibly large) set of observed individual-level data drawn from the target population(s) of interest. A mathematical structure underlies both statistical and mathematical models, and both can be used for prediction of future outcomes and for causal policy comparisons. Should models be judged on 'risk of bias'? The authors propose that evidence generated from mathematical models should be weighted more heavily toward model credibility than risk of bias. Many mathematical models are over-parameterized relative to the amount of data used to fit them; hence multiple configurations of parameter values can be lead to very similar predictions. Mathematical models are typically calibrated to observed population-level data (e.g. annual HIV incidence rate for the target population), but the formal rules for doing this seem to vary across application. For many consumers of model-based outputs, this is a significant methodologic concern that goes directly to the question of credibility. If multiple model configurations can generate similar predictions, which configuration is the most credible one? It seems reasonable that model-based outcomes such as 10-year predictions of HIV incidence need to be evaluated on their own terms. If coupled with a formal process for back-checking or recalibrating existing models this would surely add value, and would possibly strengthen model identifiability (i.e., provide evidence in favor of one set of model parameters over another). A more general justification for incorporating risk of bias into model evaluation can be found in Coveney et al 1 (page 4), who provide a general rubric for assessing quality of scientific evidence in the age of big data, emphasizing ‘acceptance of the theory based on concordance between the predictions and the measurements.’ Model calibrations at the time of model fitting partially fulfill this objective, but post-hoc evaluation of model predictions must play an important role in establishing credibility. The process of combining and comparing models is highly innovative and likely to have a positive impact on whether the results will be well received. This kind of cooperation and collaboration, exemplified recently by the Modelling Consortium, is perhaps unique to the mathematical modeling community. Evidence generated by these kinds of activities can form an important part of the evidence base. Summary The authors have provided a thorough case for including results from mathematical modeling into the formal evidence base used for making health recommendations, especially as they relate to HIV. The paper is based on findings from a recent conference and a comprehensive survey of extant literature. The main critiques are that the definition of mathematical model is far too broad, and that bias (or risk of bias) needs to be incorporated into the evaluation criteria. Formal methods for uncertainty quantification are critical as well. Mathematical models are prevalent and influential in the HIV literature; hence a discussion about whether and how to place their findings in the broader evidence base is needed and welcome. This paper provides a necessary starting point. 