The other two reviewers did a good job, such that I am going to be more philosophical about my review by focusing on two questions: Is this the best format for this kind of information, and am I (or the other two reviewers) the right person to review it? Other than a brief introduction and conclusion, the manuscript is mostly a detailed recipe of a specific pathway to assess correlations between evolutionary rates and some crystal structural features (when such information is available). This might be useful to a range of users such as undergraduates and beginning graduate students for whom it would serve as a gateway to thinking about these kinds of questions. The described pipeline is not a substantive modification to existing methods or an innovative application to to new scientific questions, but rather is a tool designed to facilitate performance of experiments in a particular way. I would hope that more advanced students would quickly want to modify the experiments and tools that make up this pipeline, asking questions about the assumptions that the included tools make, and asking questions about alternative methods of analysis. Should one use only a maximum likelihood “best tree”, or should one consider a bootstrap or Bayesian posterior distribution? How does model choice interact with inference of site-specific rates? What assumptions were made in the alignment, and do those assumptions and errors in the alignment interact with the phylogenetic and rates inferences? In many ways, rather than a fixed and published document, the author’s pipeline might be better implemented in the context of an interactive and perhaps continuously modified lesson plan to teach students about the underlying issues, with more direction and encouragement to substitute in different methods and models. This leads to the question of whether I, or other professors, are the right people to review this. I trust Wilke and his laboratory members to have tested and made this pipeline function, and I don’t have time in the context of reviewing a paper to install and implement and test their scripts. But what they and I and other professors or advanced students are going to be bad at anyway is figuring out how hard it is and how it can go wrong in the hands of beginning students. So my suggestion is that it is beginning students who should be reviewing this pipeline, trying to implement it and trying to break it (or breaking it without trying). That would seem to me to be a more meaningful review. As an aside, publishing answers to simplistic bubble questions seems inane to me, especially without the choice to opt out, and that is the context in which they were filled. 