The article by Anna Delprato describes an interesting way to engage students in an online research program in the neurosciences. This reviewer is interested in the program and the potential impacts and uses of the program for K-12 students. This reviewer, however, has several concerns that need to be addressed before the paper is indexed. The authors stipulate in the abstract that the students in the program were "capable of comprehending ...such a project". The authors cannot make that claim with the evidence presented in the manuscript. Self-reported surveys were the only form of data presented, and this type of survey does not show student learning or comprehension. The reader also does not know if this comprehension is applied to the ability to perform research, the content learned, or learning how to communicate the information (or a combination of all three). This is a significant concern and must be addressed before publication. There were only six references cited for the entire manuscript. There are many papers in the literature that discuss programs similar in nature to this. It would behoove the authors to include more background research to support their claims. The authors need to include more information about the project itself (i.e. a discussion of the modules used for teaching, the mentoring, etc.). The reader is left wondering if the students are doing actually self-initiated projects, or if they are being led by the modules. It was helpful to read about the databases that the students use, but the reader does not gain any knowledge about the actual e-internship program through which the students progress (or are those the modules? It is unclear.). In other words, how were they mentored? Is the mentoring effective for student growth? How are the modules designed? Is there any research to show that the module design is effective? Do they present and defend their research in a scientific forum (even online)? It would also help to put the Project Design section first (before the database discussion). The evaluation of the program needs to be more robust. Self-reported surveys are a good starting point, but they are only formative assessments. You need to include some summative assessments for the program and more qualitative assessments to determine the efficacy of the program for student learning. Also, it sounds as though the students are spending much less time on the project as opposed to an onsite laboratory model, so the reader is left wondering if the program is really effective. Also, do you have demographics of the participating students? There are a couple of grammatical errors (mostly words that need to be omitted and comma usage errors). In summary, this manuscript presents an interesting program, but the authors need to address the issues of a clear evaluation plan to determine the efficacy of the program, and provide enough discussion of the program so that readers understand exactly what the students gain.