This manuscript pertains to a study in how applicants for faculty positions at Karolinska Institute (KI) are assessed. Applicants submit proposal information which is first passed through a Phase 1 cut-off whereby the applicants have to have published a requisite number of publications above a threshold impact factor to qualify for further review (roughly a third). Phase 2 is then an external review by a panel of 6 reviewers (3 male/ 3 female) outside of KI. The proposals were scored in two dimensions, once based on the merits of the applicants track records and training (Merits) and once on the quality of the research proposed (Project Plan). Scores from these two dimensions were added together to create the ranking. Demographic data was gleaned from applicant CVs, as was their publication list and any affiliated KI mentors/collaborators, etc. The bibliometric data from KI collaborators was gathered with the help of the University Library at KI. External reviewer Merit scores were plotted against the author’s productivity scores for different subsets of data (gender and ethnicity). Also Merit scores were separated into Quartiles (Q1-4), and proportionality across different variables was observed. Scores on Project plan were not analyzed much in this work, but were found to be well correlated with Merit scores for both men and women. Multivariate analysis was conducted as well as principal component analysis (PCA) to see if there is clustering. The applicants find substantial differences in distribution across Merit Score Quartiles between male/female applicants, as a disproportionate # of men appeared in Q1. Gender proportions were more balanced in other quartiles. Regression analysis revealed a significant relationship between Merit score and productivity scores for men, but not for women or for non-europeans. Step wise regression revealed productivity scores, Project Plan scores and the presence of grants all had significant relationships to Merit scores for men, but only Project Plan scores were important for women. Applicants in Q1 also were more likely to be connected to male collaborators than women. PCA suggested having children was somewhat associated with lower Merit scores (Q4) and citation levels and 1 st author publications were somewhat associated with Q1 Merit scores. The goals of this research and the statistical analysis are straightforward, and there are some clear observations of not only disproportionate representation in grading but also review panels differentially emphasizing criteria across gender and ethnicity, specifically with bibliometric productivity and presence of grants. These results are disturbing as they suggest that biases are contributing to the observed disproportionate scoring. However, there are some issues that may need some clarification, consideration: Firstly, in Supplementary Fig 1, the proportion of women granted in 2011 and 2012 seem to be worse than 2014; as 2014 has a triage of sorts based on bibliometric productivity, does this mean that the current system (2014) is less biased than previous years? It seems as the proportions of women who applied vs granted for 2014 are pretty comparable, despite disproportionate representation in Q1 (Merit score). What are the reasons for this? Do Project scores compensate for biased Merit scores to push these applicants into the funding range? Looks like 38% of the total granted were women, which means about 4 out of the 10 granted were women. If only 2 women were granted from Q1 (merit score), but apparently 4 women were funded, 2 must have come from Q2-4, yet there were 12 other males in Q1. So either some males in Q1 merit score did not do well in their project scores, or the granting is not in strict order of rank? It would be interesting to know how the Project scores affected the ranking. Perhaps this could be addressed in the text. Also, in 2014, because of the triage, the review panel only evaluates a subset of already excellent applicants (based on bibliometrics). But peer review is known to be poor at discriminating between highly qualified applicants 1 . This should probably be referenced and discussed in the text, as reviewer biases may be more prevalent in this situation. Secondly, in Fig 2, while male Merit scores correlated to productivity measurements, females scores did not. Yet, the authors mention that “The PCA analysis demonstrated an inverse correlation between having children and scores received on merits, as a family often slows down the production speed, resulting in fewer publications, shorter postdoc visits abroad and a higher academic age, resulting in less funding and more time spent on getting alternative funding, as commissioned research on short time contracts.” If female scores are not derived by the reviewers from their productivity, why would having children, and its effects on productivity, matter for reviewer’s scores? In fact, based on the regression, the authors state that “There were no significant effects of having children…on the score outcome.” So it’s a bit confusing what is happening here. Also, the authors mention women may be more affected by having children, “since women traditionally are more involved in family life.” Do the data show that having children and gender correlated in this sample? Thirdly, it is clear there are differences in how reviewers evaluate applicants of different gender. The authors may mention work by Carole lee on commensuration bias in the text, which I believe predicts this kind of behaviour 2 . Out of curiosity, do the authors have any information about the reviewer discussions that could shed light to how the panel weighed criteria relative to applicant demographics? Also, some research has come out suggesting there is more variation across reviewers than across proposals 3 . Do the authors have any information on how individual reviewer scores varied? Were some panelists more biased than others? Did this vary at all by reviewer gender? This may be beyond the scope of this study, but it might be appropriate to mention that there may be different sources of the bias, at the panel level vs individual level. A few more minor points: For the linear regressions, only p-values were reported in what the authors refer to as trend test. Could the authors include the correlation coefficient as well, as it seems there is a lot of spread in the data. Also, for Fig. 2c, the data for European men still have a good deal of variability that seems independent of actual productivity. Could the authors comment on potential sources for this variability? In the text, it is said that “information on children was found in the CV or from time deducted from research due to parental leave;” is this information always reported on a CV? It was mentioned the authors imputed missing data; did this include data on children? Citations are time and field dependent; were they normalized for this productivity measurement for the applicants? If not, it may be difficult to compare. It seems, though, that citations were normalized for the KI collaborators/mentors. It’s unclear why different bibliometric approaches were used for applicants vs collaborators. Also, h-index is sensitive to age, was there an attempt to account for this confounder? 