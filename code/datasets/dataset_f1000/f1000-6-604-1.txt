Freedman and colleagues present a narrative review on current efforts underway to improve reproducibility in preclinical biomedical research. They begin by summarizing the extent of the problem and noting that quality checkpoints are either used in disparate points of the research cycle or used only sparingly. They identify key sources of irreproducibility as poor study design and analysis, inadequately authenticated reagents and reference materials, inadequately documented laboratory protocols, and inadequate reporting and review. They describe the important roles of many stakeholders, including funders, researchers, research institutions, industry, foundations, professional societies, and the public. The authors proceed to describe many efforts already under way including new funder requirements, journal guidelines, enhanced training opportunities, programs to enhance standards development and authentication checks, protocol repositories, improved reporting platforms, open access policies (including open access publishing, greater use of preprint servers, data and code sharing), data standards, and post-publication review. They conclude with a “path forward” that they call the “Reproducibility2020 Action Plan” that includes specific recommendations for funders, researchers, institutions, journals, industry, foundations, and the public. Thoughts and comments: The paper is interesting, well-written, and well-documented. I appreciated the many web links that take the reader directly to interesting sites. The authors suggest that the current crisis begins with the Amgen findings (Reference 2). While that was a defining moment, I wonder whether it’s also worth mentioning that contemporary discussion about false research findings dates back at least as far back as Ioannidis 2005 ( https://doi.org/10.1371/journal.pmed.0020124) . Ioannidis there suggests that exploratory research was highly vulnerable because of small sample sizes, overly flexible designs, and biased designs (e.g. with lack of randomization and proper masking). Table 1: I commend the authors for noting that “the chance of an irreproducible finding is much higher than the commonly noted 5% threshold.” This is widely under-appreciated, even by well-trained scientists. The authors might consider spelling out that prospective, properly done sample size calculations are critical to overcoming this problem. The “elephant in the room” is that sample sizes will have to increase substantially, meaning that with constrained funds researchers will be forced to conduct fewer experiments. But as some have noted ( Cressey D, Nature, April 15, 2015 ), that may be good for the enterprise – it would be better to do fewer properly powered experiments than to do too many woefully underpowered experiments. Table 1 and elsewhere: Should there be a “Consumer Reports” for antibodies, cell lines, and other resources? Or maybe I’m missing it, and you’re saying that’s happening. Such a “Consumer Reports” would allow for large-scale surveys in which researchers can report problems with purchased materials. Table 1: Another potential solution to study design and analysis is mandatory sharing of statistical code (e.g. in SAS, R, or Stata). This is already common practice in some fields (e.g. economics). Table 2: Another consequence for the public is lack of faith in science. They hear scientists promising the moon, and then nothing happens. Table 2: There is an ethical problem subjecting animals and people to inadequately designed or documented experiments that were doomed to be irreproducible from the beginning. Table 2 or elsewhere: NAS just released a report on research integrity in which notes a continuum between frank misconduct (fabrication, falsification, and plagiarism) and “practices detrimental to research.” The authors might want to consider the comments of the report ( https://www.nap.edu/catalog/21896/fostering-integrity-in-research) . There have been some recent successes in improved rigor, such as in preclinical stroke research. (For example, see http://circres.ahajournals.org/content/early/2017/04/04/CIRCRESAHA.117.310628) . The authors note that “stroke research has uniquely improved.” Page 6 – the link didn’t take me directly to “Statcheck software,” though I did eventually find it. Protocols – many leading clinical journals require authors to submit full clinical trial protocols along with the manuscripts. Table 3 Should it be the responsibility of funders to provide statistical consultation to applicants? Should it be the responsibility of funders to pay for open access and transparency tools? Should funders include dedicated reviews on methodological issues for those applications deemed meritorious by content? 