The authors developed a new mobile application named RNAtor to assist in the designing of RNA-seq experiments. It provides users with the number of reads that is required for optimal detection of differentially expressed genes at a given fold-change threshold based on simulations and real data. This is a useful tool for NGS users. However there are some errors and suggestions that need to be fixed. Major: In general, some reads in RNA-seq analysis are removed due to their low sequencing quality or cannot be mapped probably due to library contamination of rRNA or other species’ RNA. Does this software consider the effect? If not, it could underestimate the number of required reads. How does RNAtor calculate the number of reads required for DEG detection? The authors claim that it is calculated based on the number of DEGs at its peak in both real and simulated data but the exact algorithm is not shown. Specifically, how were the peak values calculated and how were the peak values of real and simulated data merged? Minor: Which does the term ‘replicates’ mean in this manuscript, technical replicate or biological replicate? Implementation: “… workflow followed by differential expression analysis using five tools:…” I think four instead of five is correct because Kallisto does not use outputs of Tophat-Cufflinks pipeline (Supplementary Figure 1). Figure 2: How many DEGs were included in total in the simulated datasets? Sensitivity (%) is preferable to the number of DEGs in this case. The number of replicate is a discrete value but the slope in this figure is smooth. What do the numbers in fold change (1.2 0.83-5, 0.2) mean? Figure 3: Why were the result of 5x and 4x merged? What does each line indicate? # reads = 0 means nothing thus should be removed. 