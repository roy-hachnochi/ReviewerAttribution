First let me apologise for a stupid slip in my first review. I referred to sampling without replacement and the authors quite rightly corrected me and pointed out that they sampled with replacement. That is, in fact, what I meant to say. Second, let me acknowledge that the authors have gone some way to answering my criticisms. However, I am not completely satisfied, so before changing my overall judgement of the paper, I am going to explain the problem again. Consider an alternative to sampling with replacement that will produce almost the same results they did. This is to compare a super trial in which the values for every patient in the TEXT ME trial are copied a huge number of times to create a million versions of each patient. Thus every patient in the trial has 999,999 identical virtual siblings. Where the TEXT ME trial had 710 patients we now have 710 million patients. Let us call this trial MegaText. If all these patients were real, then in the huge population of MegaText the effect seen would be the true effect. Now we can actually sample without replacement from MegaText and the result will be almost identical to sampling with replacement from the TEXT ME trial. The only slight difference is that in sampling without replacement from MegaText once a patient has been chosen there is a very slightly reduced chance of the patient being chosen again but since there are so many copies, this hardly matters. Therefore, sampling from the TEXT ME trial is almost identical to sampling from the MegaText trial and it thus follows that we are sampling from a population in which there is a genuine treatment effect and there is no uncertainty about this. Thus the right decision is known to be to implement the program. Of course, and their simulation shows this, if the trial is small enough, you will sometimes choose the wrong treatment. If the purpose of the trial is pragmatic in the sense of Schwartz and Lellouch(Schwartz, D. Lellouch, J., 1967), then it is this Type III error that has to be guarded against. However, this raises the second issue. The decision to always choose what appears to be the better of two treatments being compared, however weak the evidence, is logical if no further evidence can be obtained. However, it is not necessarily logical if more information can be obtained at a modest cost. To see this, consider the case where a new treatment N is being compared to a standard treatment S and high values are good, as would be the case if we are measuring utility. The Bayesian posterior distribution for difference in effects ( N-S ) is mainly in the positive area: it is more probable than not, taking all things into account that it is better to use N . However, there is a non-negligible probability that actually S is better. If information can be obtained at low cost it may be worth doing so just to exclude the possibility that S is after all better. Thus, I have some unease that the combination of starting with a proven treatment and showing that if one had carried out a smaller trial one would often come to an apparently good choice of treatment if one had to be made is quite as relevant to the practical problem that choosing a sample size is meant to solve. This is not to say that common approaches to doing this are good: far from it. They ignore the dimension of cost and this cannot be rationale. Thus, if what the authors wish to say is: “just because a trial has not found a significant result it does not follow that it cannot be used to decide to implement a new treatment if no further information will be forthcoming” there are some circumstances under which I could agree. If they wish to imply that standards should generally be less stringent than they currently are, I do not think this sort of investigation is particularly relevant. References 1. Schwartz D, Lellouch J: Explanatory and pragmatic attitudes in therapeutical trials. J Chronic Dis . 1967; 20 (8): 637-48 PubMed Abstract Competing Interests: As far as I am aware I have no competing interests. I maintain a general statement of interests here: http://www.senns.demon.co.uk/Declaration_Interest.htm Reviewer Expertise: I am a medical statistician with many years experience in dealing with problems associated with drug development and regulation. I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above. Close READ LESS CITE CITE HOW TO CITE THIS REPORT Senn S. Reviewer Report For: Smaller clinical trials for decision making; a case study to show p-values are costly [version 2; peer review: 1 approved, 1 approved with reservations] . F1000Research 2018, 7 :1176 ( https://doi.org/10.5256/f1000research.17883.r38819 ) The direct URL for this report is: https://f1000research.com/articles/7-1176/v2#referee-response-38819 NOTE: it is important to ensure the information in square brackets after the title is included in all citations of this article. COPY CITATION DETAILS Report a concern Author Response 31 Oct 2019 Nicholas Graves , Duke NUS, Singapore, Singapore 31 Oct 2019 Author Response We thank the reviewer for the additional comments and try to respond to the key issue that arose in the first review. The reviewer says “ simulating only from the case where ... Continue reading We thank the reviewer for the additional comments and try to respond to the key issue that arose in the first review. The reviewer says “ simulating only from the case where the intervention is beneficial is not adequate ”, but we simulate because we are not certain the intervention is beneficial. Hence, we simulate from the observed data in order to see whether the intervention is beneficial on a meaningful scale. In general, these simulations use multiple outcomes, some of which may have a positive mean (e.g., improvement in blood pressure and the associated health benefits/ health utility) and some have a negative mean (e.g., increase in costs). Hence, it is a composite estimate that is more complex that just one mean being positive or not. We used this approach for another important question about whether a hand hygiene campaign should be funded and showed the conclusions varied for different states and territories of Australia, see Table 3 of this paper… https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148190 In South Australian, Tasmania Western Australia the positive difference in the main outcome was not large enough to justify a decision to continue the campaign. But in Queensland, ACT and New South Wales we concluded the opposite. We are not simply reliant on a single positive mean, which should happen for around 50% of studies where the intervention has no effect and hence is quite easy to achieve. Instead, we are looking at whether the observed difference is meaningful using the observed variation in the sample. It is possible to use scenario analysis to simulate results under more pessimistic scenarios, such as a null treatment effect but reduction in costs, and these can be informative for decision makers. We thank the reviewer for the additional comments and try to respond to the key issue that arose in the first review. The reviewer says “ simulating only from the case where the intervention is beneficial is not adequate ”, but we simulate because we are not certain the intervention is beneficial. Hence, we simulate from the observed data in order to see whether the intervention is beneficial on a meaningful scale. In general, these simulations use multiple outcomes, some of which may have a positive mean (e.g., improvement in blood pressure and the associated health benefits/ health utility) and some have a negative mean (e.g., increase in costs). Hence, it is a composite estimate that is more complex that just one mean being positive or not. We used this approach for another important question about whether a hand hygiene campaign should be funded and showed the conclusions varied for different states and territories of Australia, see Table 3 of this paper… https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148190 In South Australian, Tasmania Western Australia the positive difference in the main outcome was not large enough to justify a decision to continue the campaign. But in Queensland, ACT and New South Wales we concluded the opposite. We are not simply reliant on a single positive mean, which should happen for around 50% of studies where the intervention has no effect and hence is quite easy to achieve. Instead, we are looking at whether the observed difference is meaningful using the observed variation in the sample. It is possible to use scenario analysis to simulate results under more pessimistic scenarios, such as a null treatment effect but reduction in costs, and these can be informative for decision makers. Competing Interests: None Close Report a concern Respond or Comment COMMENTS ON THIS REPORT Author Response 31 Oct 2019 Nicholas Graves , Duke NUS, Singapore, Singapore 31 Oct 2019 Author Response We thank the reviewer for the additional comments and try to respond to the key issue that arose in the first review. The reviewer says “ simulating only from the case where ... Continue reading We thank the reviewer for the additional comments and try to respond to the key issue that arose in the first review. The reviewer says “ simulating only from the case where the intervention is beneficial is not adequate ”, but we simulate because we are not certain the intervention is beneficial. Hence, we simulate from the observed data in order to see whether the intervention is beneficial on a meaningful scale. In general, these simulations use multiple outcomes, some of which may have a positive mean (e.g., improvement in blood pressure and the associated health benefits/ health utility) and some have a negative mean (e.g., increase in costs). Hence, it is a composite estimate that is more complex that just one mean being positive or not. We used this approach for another important question about whether a hand hygiene campaign should be funded and showed the conclusions varied for different states and territories of Australia, see Table 3 of this paper… https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148190 In South Australian, Tasmania Western Australia the positive difference in the main outcome was not large enough to justify a decision to continue the campaign. But in Queensland, ACT and New South Wales we concluded the opposite. We are not simply reliant on a single positive mean, which should happen for around 50% of studies where the intervention has no effect and hence is quite easy to achieve. Instead, we are looking at whether the observed difference is meaningful using the observed variation in the sample. It is possible to use scenario analysis to simulate results under more pessimistic scenarios, such as a null treatment effect but reduction in costs, and these can be informative for decision makers. We thank the reviewer for the additional comments and try to respond to the key issue that arose in the first review. The reviewer says “ simulating only from the case where the intervention is beneficial is not adequate ”, but we simulate because we are not certain the intervention is beneficial. Hence, we simulate from the observed data in order to see whether the intervention is beneficial on a meaningful scale. In general, these simulations use multiple outcomes, some of which may have a positive mean (e.g., improvement in blood pressure and the associated health benefits/ health utility) and some have a negative mean (e.g., increase in costs). Hence, it is a composite estimate that is more complex that just one mean being positive or not. We used this approach for another important question about whether a hand hygiene campaign should be funded and showed the conclusions varied for different states and territories of Australia, see Table 3 of this paper… https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148190 In South Australian, Tasmania Western Australia the positive difference in the main outcome was not large enough to justify a decision to continue the campaign. But in Queensland, ACT and New South Wales we concluded the opposite. We are not simply reliant on a single positive mean, which should happen for around 50% of studies where the intervention has no effect and hence is quite easy to achieve. Instead, we are looking at whether the observed difference is meaningful using the observed variation in the sample. It is possible to use scenario analysis to simulate results under more pessimistic scenarios, such as a null treatment effect but reduction in costs, and these can be informative for decision makers. Competing Interests: None Close Report a concern COMMENT ON THIS REPORT Version 1 VERSION 1 PUBLISHED 02 Aug 2018 Views 0 Cite How to cite this report: Senn S. Reviewer Report For: Smaller clinical trials for decision making; a case study to show p-values are costly [version 2; peer review: 1 approved, 1 approved with reservations] . F1000Research 2018, 7 :1176 ( https://doi.org/10.5256/f1000research.16927.r37678 ) The direct URL for this report is: https://f1000research.com/articles/7-1176/v1#referee-response-37678 NOTE: it is important to ensure the information in square brackets after the title is included in this citation. Close Copy Citation Details Reviewer Report 07 Sep 2018 Stephen Senn , Competence Center for Methodology and Statistics, Luxembourg Institute Of Health, Strassen, Luxembourg; School of Health and Related Research, University of Sheffield, Sheffield, UK Approved with Reservations VIEWS 0 https://doi.org/10.5256/f1000research.16927.r37678 The authors propose that when a clinical trial is sought to inform practical decision-making, conventional standards of 'proof' may be too stringent and in consequence resources may be wasted. They illustrate this by simulating from a particular clinical trial, the ... Continue reading READ ALL 