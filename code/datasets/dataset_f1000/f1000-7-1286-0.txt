The paper presents a recursive algorithm to find modules in biological networks. The authors evaluate the modules using a DREAM dataset. The algorithm is promising and the paper is well written. The results are comparable and sometimes better than the best performance of the DREAM Challenge. I think some changes could improve the paper: 1. In the definition of the algorithm, the intuition between the concept of modularity could be included, and maybe a high level definition of the chosen modularity index. 2. “By default, in the Louvain algorithm, the initial partition assigns each node to a module that contains only the node itself. This creates a lot of variability in the results” Do you mean variability from one run to another? 3. “For each configuration, we performed 10 runs of our algorithm.” Here you are evaluating each run and showing all 10 results. I wonder what would happen if one combined modules from different runs. To create an ‘ensemble’ module extractor. One could just pool together the modules found. This could be done over the 10 runs with the same K, but also maybe over runs with different k? Or even changing the modularity criterion and combining the results. And then selecting the most frequently found modules... 4. For figure 3: is there information on how many significant modules are actually known in the networks? One could think of adding another line that would show the number of significant modules known as an upper bound for the performance. 5. “If we combine the best result for each network, we obtain a theoretical total of 81 significant modules, close to double our final score and 35% better than the best-performing solution in the challenge”. How is this combination made? 