Cusco et al. evaluate methods for long read sequencing and classification of marker genes from microbial communities, both for mock communities of known microbial composition and complex communities from dog skin from two anatomical locations, chin and back. They find that long read sequencing of 16S and the rrn operon is sufficiently accurate to classify microbes and that rrn is more sensitive at the species level. This work demonstrates a valuable option for species identification from microbial samples using long reads, overcoming the current high error rate through covering a larger region. The work also highlights some of the issues that can arise from multiplexing amplicons of differing lengths when using Nanopore sequencing. Overall the paper is well written and detailed, however there are a few details I feel could be addressed: 16S length reads in rrn barcodes: Do you expect this to be entirely from barcode misassignment or were these shorter fragments produced during PCR? You state that the loss of rrn amplicons during the length trimming step was probably due to over-representation of 16S amplicon on the flow cell, and most of the reads lost were roughly 16S amplicon sized - are you suggesting that there are large numbers of 16S reads that are assigned to rrn barcodes after 2 rounds of demultiplexing? Are these shorter reads actually whole 16S amplicons or fragments of rrn ? Expected sensitivity given read count : The authors state that failure to detect the less abundant species from the mock community in the rrn dataset was "probably" due to their being fewer reads. As the proportions of the species in the mock samples are known, theoretically what total number of reads would be necessary to detect the less abundant species? Given the number of rrn reads obtained, did the authors detect as many species as they would expect to detect and what is the minimum total number of reads they would need to be likely to detect the lowest abundance species? Differences in classification methods : Differences in classifications between the mock community database/rrn database and the NCBI database may be attributable to differences in the tools, with minimap2 being used for the mock and rrn databases and WIMP (based on centrifuge) being used for the NCBI database. My understanding is that the authors are mainly interested in classifications from different databases rather than differences between methods. While the authors do not directly compare the classification results between these different methods in text, some of the figures appear to imply that these results are directly comparable (e.g. Figure 3a). It would be useful if either all three databases were used with a single method (for example, using centrifuge with all three databases) or if these were at least more obviously separated or marked as coming from different classification methods in the figures. Classification rates against NCBI: The authors should further discuss ways to improve the classification rates, will the biggest improvements come from reduced error rate, better classification tools, improving species representation in databases? The authors conclude that in the future we should aim to improve accuracy, but one of the main results here is that sequencing the full 16S/ rrn overcomes the problem of the current error rate - perhaps highlight benefits such as improved barcode assignment and emphasise that while this works well classification against a large database would likely improve with increased accuracy. The authors also conclude that rrn offers higher resolution at species level, however I suspect that currently more species have 16S sequences in databases than rrn . Additionally, I have a few minor corrections mainly around small grammatical errors and figure/table modifications: Page 5: Paragraph beginning "To assign taxonomy...", change "to strategies" to "two strategies". Also I would change the last sentence on the page to say "some of the reads excluded were the expected length of the 16S rRNA gene rather than the rrn operon". Figure 1 should also be labelling Albacore as the basecaller. Page 6: change "would allow us determining" to "would allow us to determine". Page 11, column 2, line 2: change "associated to" to "associated with". Figure 3a would benefit from separating the reference bar from the other bars or adding this bar to the other two plots (currently it is grouped with Mock database, but it is also relevant to the rrn database and the NCBI database). Figure 4 text is quite difficult to read. Table 2: the title of the final column isn't clear. Is this the % of reads that pass the quality filters before chimera detection? Could another column be added showing number of reads that pass this filter? Figure 5: there are several different colours of 0 in this heat map? In the conclusion the authors have suggested ways to improve accuracy of this method in the future, I would add the R2C2 method (Volden et al. , 2018 1 ) as an option to improve consensus accuracy here also, while designed for cDNA it could be applied to fragments of genomic DNA. 