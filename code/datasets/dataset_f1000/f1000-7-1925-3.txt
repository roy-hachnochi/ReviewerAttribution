The manuscript tackles a very important and ambitious topic, that of improving our knowledge about what differences there are in open access uptake across academic disciplines. A lot of bibliometric work has been done in this area, however, most of it has been fragmented as definitions and methodological approaches have varied a lot across studies. This study makes a welcome exception to most of the research within the field in not just producing yet another measurement of OA that is improved in some incremental way, yet failing to be compatible with results of earlier studies, but rather leverages what is already out there (both in terms of existing studies, but also other knowledge) in order to thoroughly discern how disciplines differ in their approaches to utilising various forms of OA. The manuscript has two main components 1) a systematic literature review of bibliometric research (which includes 11 articles), and 2) an analysis of open access in academic research disciplines interpreted through the theoretical lens of Social Shaping of Technology. I could easily see both parts being published as individual articles based on what they aim to achieve and in how challenging they are to put together, having them together like this is not a major problem but something that requires effort and rigour which this first version of the manuscript succeeds with to a satisfactory degree. The text itself is of high quality. Is the work clearly and accurately presented and does it cite the current literature? For the most part, yes, but I do think the strictness of the criteria for the systemic literature review of OA uptake requires that supplementing research that is left out of the review is still discussed/reflected upon as in some other parts of the manuscript. I have a couple of recommendations for this that are mentioned below in this section. One source which I think is a great omission and gives a lot of detailed breakdown into the differences between OA journal differences is: Crawford, W. (2018). GOAJ3: Gold Open Access Journals 2012-2017. https://walt.lishost.org/2018/05/goaj3-gold-open-access-journals-2012-2017/ . If “top-down” studies, focusing on only one type of OA mechanism, were excluded this study was perhaps not included on such grounds but I think it is doing the study a disservice – there is no better source that describes the disciplinary differences longitudinally across disciplines, including information about article processing charges, than that e-book and associated dataset. If not integrated into the meta-analysis it should at least be used in the other parts of the manuscript to frame the study and its results. Further reference you could consider, purely based on the idea that they have also explored disciplinary differences in the OA context specifically, albeit through analysis of bibliographic indexes: Liu, W. and Li, Y. (2018), Open access publications in sciences and social sciences: A comparative analysis. Learned Publishing, 31: 107-119. doi:10.1002/leap.1114 1 Ennas, G. and Diguardo, M.C. (2015), “Features of top-rated gold open access journals: an analysis of the Scopus database”, Journal of Informetrics, Vol. 9 No. 1, pp. 79-89 2 . Gadd, E., Covey, D. T. (2019). What does ‘green’ open access mean? Tracking twelve years of changes to journal publisher self-archiving policies. Journal of Librarianship and Information Science , 51 (1), 106–122 3 In general I avoid suggesting citing material that I have been involved in authoring as part of reviews I have conducted, but in this case I would like to point out two studies that give precise metrics of various types of OA in narrowly defined disciplines, and another study which deals with disciplinary differences in self-archiving rights, utilizing these references is completely optional and not something that influences my verdict or recommendation for a revised version of the paper: Laakso, M. Polonioli, A. Scientometrics (2018) 116: 291 4 . Laakso, M., Lindman, J. (2016). Journal copyright restrictions and actual open access availability: A study of articles published in eight top information systems journals (2010–2014). Scientometrics, 109(2), 1167–1189 5 . Laakso, M. (2014), “Green open access policies of scholarly journal publishers: a study of what, when, and where self-archiving is allowed”, Scientometrics, Vol. 99 No. 2, pp. 475-492 6 . Table 3, being split onto 4 pages, is massive and very hard to use for making any conclusions between time/discipline(which each study having their own way of classifying as well)/OA method by eye. It is functional but far from optimal. In this case I would save a table like this to become an appendix, and rather compose a figure where the discipline categories have been standardized according to some well established scheme that fits well with most of the studies. This would come at the cost of precision in losing sub-discipline breakdowns in many cases but in my view that is worth the cost. I would also suggest to focus less on comparisons of decimal point-accuracy prevalence of OA mechanisms between the previous studies, since they vary so much depending on other factors than inherent disciplinary differences. Zooming out would make it easier to see, and tell the reader, what is important to focus on, not just drop the decimal points but also consider putting in subheadings or structuring the “Prevalence and patterns of open access publishing practices: Meta-synthesis of bibliometric studies” so that each “era” of OA development would get its own mini-narrative, now its just a long single block of text and a lot of percentages that are hard to relate to anything. The influence of academic social networks is in my view underrepresented in the review of existing literature and conclusions of the study, they have provided a substantial share of the OA copies measured in the various bibliometric studies and many authors also perceive them as essentially “solving” the issue of OA and paywalls on a personal level since there has been very weak monitoring of adherence to copyright on such services. The concept of Bronze OA would need further unpacking since in most of the reviewed studies it is present, but not always separated and referred to as such from other OA provision mechanisms. Is the study design appropriate and is the work technically sound? The collection process for inclusion of existing literature contains both strict elements (specific indexes were queried with specific identical keywords, studies had to fulfil four pre-set criteria to be included) but also what seems like a liberal and flexible amount of bottom-up/explorative elements (authors contributing discipline specific OA findings/literature, scouring reference lists and Google Scholar profiles). This large jump between very strict and transparent, to a largely undocumented part where “anything goes” which has very little transparency other than the disciplinary analysis themselves could be expanded somewhat. For me it was a bit unclear what the first criteria in Table 1 when strictly applied entails, do the studies have to explore OA availability “bottom-up” through web-search engines/querying and giving uptake metrics for various OA mechanisms in one single study? If this is the case, which it could be by looking at the included studies, the criteria description should in my view be revised to communicate this. The time-lag between when a study has measured the level of OA and when the materials being measured were published, varies a lot across the included studies. I think this caveat/feature could be highlighted more in the text because it matters quite a lot if an article was searched for 1 year after it was published or 5 years after it was published. Are sufficient details of methods and analysis provided to allow replication by others? Yes, the study is literature-based with no need for further data. If applicable, is the statistical analysis and its interpretation appropriate? Not applicable Are all the source data underlying the results available to ensure full reproducibility? Yes, the study is literature-based with no need for further data. Are the conclusions drawn adequately supported by the results? Yes, my revision suggestions concern mainly minor points not critical to the main results and contribution of the study. The second half of the manuscript, which comprises the discipline-specific description of OA practices, I have very little to comment about since I think it does a great job at mixing research results with discipline-specific knowledge. The most central things are brought up and argued for well. 