 Summary In this study, the authors have identified gene signatures for radiation dose prediction using machine learning methodologies based on publically available microarray results from human and murine samples (mostly lymphocytes) exposed to ionizing radiation. Their signatures have been independently validated showing a high specificity for dose estimation. The authors have used a novel method, based on the concept of minimum redundancy maximum relevance. This generated signatures which often contained genes that had not previously been identified as potential radiation biomarkers. In all, this is a well-conducted study with relevance to the field. However, we do have some comments/questions/remarks, as outlined below. Introduction Several other studies have applied machine learning methodologies to identify predictive radiation exposure biomarkers. Some of these have been reviewed in Hall et al ., Mut Res 2017, Supplementary Table 3.5.1.1. Another important aspect of gene expression is alternative splicing, which also occurs in response to ionizing radiation (e.g. Sprung et al. , PLoS ONE 2011; Forrester et al. , PLoS ONE 2012; Macaeva et al., Sci Rep 2016). The latter study also showed for the first time the suitability of exon signatures as sensitive radiation biomarkers, and highlights the importance of prior knowledge at the exon level for subsequent primer- or probe-based assays (e.g. qRT-PCR). This may be discussed. Methods In the data Pre-processing “Rows and columns of microarray data that are less than 95% complete were removed and any remaining missing values were imputed using the nearest-neighbor algorithm” How many rows and columns were removed, and on which basis was the 95% threshold selected. Also, what is the effect of the nearest-neighbor algorithm on the data "over-fitting". Is it possible to perform PCA on the data after removing any row/columns with less than 100% completeness and compare to the currently presented approach (95% removal and filling the remainder of the missing data)? This would allow the visualization of the effect of the proposed methodology on the segregation between the various records. Only genes common to all datasets were retained. Does that mean common between mouse and human datasets? How were aliases identified? The second step in the process is the selection of genes based on a non-exhaustive list of publications. Why was this necessary if the mRMR method for feature selection was applied? I particularly like the idea of performing quantile normalization after feature selection. Is this something that has been published before? Can the authors speculate (or maybe even compare) about the performance of their method on pre-normalized datasets? Concerning the method used for the “Validation of models”, I would think this approach would be more vulnerable towards the test/training dataset. What would occur to the accuracy when doing the normalization over-all of the data? Would the accuracy change drastically? Is it possible to extend the testing to cover additional data? Many datasets exist on human PBMCs/whole blood irradiated with a range of doses. Why were these datasets not considered for this study while lymphoblastoid cell lines were? It would be helpful to have a comparison of model performance with that of “traditional” machine learning methods, as used in some of the indicated references. Results “We discovered radiation gene signatures using the microarray data of human and mouse peripheral blood samples and human lymphoblastoid cell lines, which were validated either according to signature.” Were the human lymphoblastoid cell line and prepheral blood samples grouped together in one model? If so, would it be possible to visualize how the expression data of the shortlisted genes for each data type separately (using PCA for example)? Can the authors comment on their observation that signatures derived from both murine datasets are not very similar? Apart from “noise, or intrinsic differences in the datasets”, could it possibly also be a consequence of the method used, i.e. mRMR in which low mutual information genes are selected? Based on Fig. 2 and 6 it seems that genes with higher mutual information in general have higher frequencies. Which seems logical. The authors state that Ms4a1 appears less frequently than Glipr2. However, from the sizes of the circles, Ms4a1 seems to appear more frequently than Glipr2. Please verify this statement. Are genes in Tables 1 and 2 ranked according to their frequency, importance,…? How do the authors explain the low frequencies of human signature genes (Fig. 6), compared with murine (Fig. 2)? Likewise, can the authors explain the large number of genes with low mutual information in the human signature (23 out of 26 0.4), compared with the murine signature (4 out of 33 0.4). Although I like the idea of mRMR, it is somewhat counterintuitive to have genes with little mutual information to be important for dose prediction. This seems to be confirmed by the fact that the compositions of human signatures are dominated by genes with high mutual information (in fact, these are all well known p53-dependent genes which appear in a high number of published radiation signatures). Discussion I understand the advantage of small signatures in terms of practicality. However, in case of a real emergency, in which individuals have been irradiated without good knowledge about the exact time since exposure, larger gene signatures may provide the additional benefit of having different dynamics per gene. This may help to also predict not only the dose, but also the time since exposure. Furthermore, one-gene signatures may suffer from higher variability among the population compared to larger gene signatures. I believe results from other, similar studies may be briefly situated in the introduction/discussion. 