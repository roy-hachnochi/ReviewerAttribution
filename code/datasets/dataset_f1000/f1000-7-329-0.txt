Referee #1 The authors correctly point out that this is a skewed industry, with the top performing schools in any given category contributing more than their share of returns. (Page 1) The selection of top 10 institutions in the initial analysis and then expanding to top 25 institutions is somewhat arbitrary. (Page 3) This data is coming from institutions of all sizes that vary according to the research dollars as well as the overall size of institutions. For an objective analysis the data needed to be normalized along those lines. (Page 3) Some of the stages described by the authors in Figure 1 are metrics that can be manipulated. For example the number of patents applied for and issued really are not metrics of measuring efficiency or impact of an office. Large number of patents can be applied for and granted, the real measure of performance will be the number of patents that are licensed. (Page 3) The approach of creating a composite metric is a good one, rather than just looking at the revenue number. The Milken study is another one that has utilized this approach and is an objective measure. The difference is that the Milken study normalized the data, as well as provided the weighting of each factor. (Page 3) In most cases the large revenue numbers can be attributed to a single large license or a monetization activity from a university. The weighting of that single license will skew the result enormously. Combined with the lack of data normalization, the results will not be reflective of the true impact to technology commercialization from universities. The weighting factor for each of the "steps" and how they overall impact the ranking is unclear. The weighting of the input metrics vs. the output metrics cannot be the same, hence the rankings would change quite radically. That explanation should be very clearly pointed out. (Page 4) The majority of institutions here are based on their size and hence a normalization is needed to avoid picking institutions by size only. Smaller schools often times are more efficient. In looking at Table 1 their analysis shows the 1% and the top 19% percent of the institutions. There are 300 institutions in the survey. Why did they pick 25 to rank? In addition Table 1 shows that the top 20% account for 60% of results. Why didn't they cite the top 60 institutions. It would seem logical to look at the top 20%. (Table 1) The sharing of best practices has existed for decades in the very collaborative technology transfer community. The AUTM Leadership Conference is precisely that effort, along with the LES IP100 meeting. The fact is that, it is not easy to solicit inventions from faculty. It takes years if not decades to get them on board with the idea that commercialization is a good thing. (Page 6) These programs are ideal in nurturing an "ecosystem" for technology commercializatin. The authors are absolutely right about taking a holistic approach rather than just trying to increase disclosures, patents, startups or revenues. (Page 6) The whole idea of hiring consultants to fix technology transfer does not work. I have personally been a consultant and a technology transfer professional for many years. The notion of fixing tech transfer through consultant is not economically viable. Consultants most of the time do not have the necessary understanding of the university structure and functions. Many aspects of technology transfer will not have economic impact or metrics based impact, but they need to be done nonetheless. Analogy of hiring consultant would be like replacing full time and dedicated servicemen with mercenaries. Having said that, hiring of consultants for focused engagements is highly recommended and successful offices do that often times in areas such as patent prosecution, valuation, licensing comparable, or market analysis. If there are significant examples that the authors can cite to support the replacement of technology transfer offices with consultants, that would cause readers to consider such an approach. Short of that authors should either reconsider adding this to the article or reframing the recommendation. (Page 6) This model has been tried in Japan, where both TTOs and TLOs existed. The model failed miserably. Not only is this model not economically viable, but it creates an atmosphere of trying to create a consortium out of disparate companies like Amazon, Google, Ford and IBM. Some consortiums on a limited scope such as a particular technology focus, e.g. IoT, with a few institutions could work. Again, the authors should cite specific examples that clearly demonstrate the validity of the proposed model otherwise reconsider adding this as a recommendation. (Page 6) The picking of two institutions which are ranked according to the authors at #3 and #19 does seem highly biased analysis. Additionally, the programs that the authors have described are student and faculty programs. Technology transfer performed by universities across the board and the data reported by AUTM do not take into account any student programs. Hence, the impact that authors have contemplated at each stage are possibly not accurate. Suggest the inclusion of universities from different "ecosystems" as well as exclusion of student programs from the analysis. (Page 7) From a practical standpoint this model will be at best difficult if not impossible with the interest of the parties diverging because of a simple reason, royalty stacking. Each inventor or assignee wants to have the highest value for their portion of the invention. That is just one factor from an economic standpoint. There are many other intangible factors that would be barriers. Anyone who has setup consortia can attest to these challenges. (Page 7) Overall, the article has significant challenges: 1. Authors are predominantly from the Boston area and are analyzing their own institution and another institution which is in close proximity. There are more than 300 institutions and a number of thriving "ecosystems". This is a significant bias and in some ways looks more like a marketing piece as opposed to an objective article. The reason the two institutions are successful could be because of the existence of industry players, local VCs, selective admission of students at two private colleges, and many other factors. 2. Their analysis seems to have consisted solely of sorting the AUTM data, not much analysis (interviews, or other supporting data) about the drivers of those outcomes. Hence, they don’t show any support for their statements about technology transfer offices being under performing. 3. The recommendations or solutions that the authors are proposing are not supported by any data or examples and are simply anecdotal. If such models have existed and worked then analysis of those models would be helpful. 4. As we all know the Milken study was published with different results than the authors present. They do not reference the Milken study or explain the differences. Other articles which have attempted to perform similar analysis have cited the Milken study. (e.g. Mature Biotech Entrepreneur in 2014) 5. Technology transfer is indeed a complex interplay of a various factors that are not always easy to understand. The intangible factors such as location, concentration of research areas, public versus private institution, can all have impact that cause significant disparity in the results. Authors need to consider analyzing or at least mentioning some of those factors in this article. (Page 8 - Conclusion) Referee #2 The years 2010-2014 is a small, five-year sample of data, from which broader trends and conclusions are difficult to draw. The industry moves and changes rapidly, so the 2014 data (which is FY2014, or July 2013 – June 2014 for most institutions), is a relatively old data set. Why were only the top 10 in each category of the Commercialization pipeline included in the MRR analysis? The parameters identified in the Commercialization pipeline (Figure 1) to measure the health of a tech transfer office are, in many instances, inadequate or irrelevant. If one is trying to measure the health of a university (not simply its TTO), then one might use research expenditures as a reasonable variable. However, this reviewer fails to see the direct correlation between an institution’s research expenditures and health of the TTO. Furthermore, technology commercialization is a non-linear, dynamic process. The notion that commercialization proceeds from left-to-right (i.e. research expenditure-invention-patent-license-startup-revenue) is incomplete. Similarly, with respect to patent applications and patents issued, such numbers depend heavily on an individual institution’s IP strategy. For example, California Institute of Technology, files provisional patent applications on nearly every invention disclosure that is received. How does such a policy speak to the health of the TTO? Licenses and options are certainly one reasonable metric that can be used to assess the health of a TTO. However, all licenses and options are not created equal. Some universities will license 30 technologies or patents in a single license, and some will license the same 30 technologies or patents in 30 different licenses. Is the second university healthier than the first? The authors have failed to address or account for differences in quality of licenses and options between universities. The quantities by themselves don’t paint an accurate picture. For the Startups metric, the authors should address, again, the quality of the startups coming out of the universities. Some universities create large numbers of startups in a given year, but some of those may not be legitimate, growth-oriented startups with qualified management. Adjusted Gross Income should not be used as a metric to measure the health of TTOs. The TTO has no control over this value. In the Results section on page 4, I find it disappointing that the authors go through so much analysis to demonstrate the truth of the Pareto principle in technology transfer. The fact that 20% of the universities contribute to 60% of “total commercialization activity” is entirely unsurprising. Again, in the Results section on page 4, “Highly performing institutions”, I don’t see that the data were normalized for research expenditures. If so, the analysis needs to be rerun. Furthermore, the authors need to remove the University of California, University of Texas, and University of Maryland systems because those systems report to AUTM statewide. Alternatively, the authors could retrieve any desired data from the individual institutions in those states. I am not clear on the analogy between the Gini coefficient of patents and US household income. How do the authors conclude that a 60% Gini coefficient for patents means that universities have significant untapped commercialization potential? Page 5, “Improving the pipeline”, along with Table 2, reads more like an advertisement for Harvard and MIT, particularly the Biotechnology Clubs of which some of the authors are members. The paper would be strengthened if they identified activities conducted outside of Boston-based universities that are contributing unexpected positive results to the tech transfer ecosystem. Discussion section comments Sharing best practices across institutions is a common and frequent activity in the university tech transfer community. Organizations like AUTM exist for such a reason. Where do the authors propose universities obtain resources to promote additional commercialization activity? Do the authors have any data to support one of their proposed solutions, the outsourcing of technology transfer to consultants? Overall questions and comments Specifically, outside of Harvard and MIT, what are institutions in the top 25 doing that other institutions can replicate to improve their TTOs? Are the authors aware of similar programs at other universities as those highlighted in Table 2? Have the authors considered non-traditional factors that may contribute to the success of TTOs, such as overall university mission, geographic location, appetite for risk, patience for technology development timelines, etc.? 