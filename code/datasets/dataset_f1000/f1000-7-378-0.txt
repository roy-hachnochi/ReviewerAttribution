In this paper, the authors describe a new pipeline for identifying disease modules from large-scale biological networks in the DREAM challenge. The pipeline builds upon off-the-shelf hierarchical community detection methods and first generates an initial partitioning of the network using a given community detection algorithm. It then integrates multiple properties of the network including modularity, conductance and connectivity into an F score to benchmark the partitioning at different levels of the hierarchy and selects the best partitioning. Next the pipeline merges modules in a way that increases connectivity, and resulting modules that exceed size threshold are partitioned again using hierarchical clustering and split at a level corresponding to the F score minimum. After a second round of merging similar to previous steps, a final set of modules are generated and ranked using a score termed inverse confidence. Using known disease-gene associations obtained from GWAS datasets, the authors verified the modules identified from their pipeline with multiple community detection algorithms, and compared performance across difference networks with that of the top team in the challenge. The authors conclude that despite the top performing team scoring highest overall, there are several cases where the number of modules identified in this paper are at least comparable to that from the top scoring method. Additionally, the authors claim that their pipeline is a generic framework for identifying statistically significant disease modules from biological networks. The methodology put forward in this paper seems novel. However, neither the utility of the module identification pipeline nor its generalizability are adequately demonstrated. This is likely due to vagueness in the description of methods and lack of theoretical justification and supporting computational experiments to validate the procedures and scoring metrics devised by the authors. Specific comments are listed in detail below. The description of the module identification framework seems elusive and lacks detail, rendering it unclear whether it is based on sound theoretical foundations. The framework works through a series of merge-split-merge steps that seems to hint at an iterative procedure to refine network partitioning. However, the method stops at the second merging step and discards all modules whose sizes exceed thresholds. The authors need to provide a rationale for this – is this because of empirical results that the modules identified from the pipeline do not change much after these steps and thus do not require further iterations in general? In addition, the paper seems to switch between methods and scoring schemes in different steps of the pipeline, for example the merging step is performed using increasing connectivity as criterion, whereas the splitting step uses a hierarchical clustering with minimum F score as partitioning criterion. At the final step, the authors used minimal change in modularity score as the criterion for setting a cutoff for module significance. These heuristics seem inconsistent with the idea of integrating multiple network properties to assess quality of network partitioning. Without justification for such a design the framework it would not appear convincing enough to be a fair and generalizable pipeline for module identification. The paper only compares their results with that from the top scoring team in the DREAM challenge and finds that there is a subset of networks where their performance is comparable to that of the top scoring method. This is rather inconclusive in terms of the method of comparison as well as the comprehensiveness of the datasets covered. Is the best scoring team’s method (not explained in detail in the paper) applicable to the pipeline that the authors proposed? If so it would be more convincing if the authors could see improvement over the best team’s method by refining modules through their pipeline. Is a single comparison adequate? Can the pipeline improve results from other teams that use hierarchical methods for module identification too? Answers to these questions will help further verify the utility of the proposed pipeline. The authors gained some insight by inspecting results from each of the off-the-shelf methods they tested using their module identification pipeline – different methods behave very differently in terms of module number and size distribution. A question that naturally arises from these observations is what goes into a good module identification method? Instead of using real biological networks, the authors may gain a better view of the performance of these algorithms by benchmarking them against synthetic networks where modules are simulated under different generative models. Because these methods, including the novel pipeline in this paper itself, only takes network topology into consideration, it is important to test if they work well with different network models. 