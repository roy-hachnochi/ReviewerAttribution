In their paper, “An unsupervised disease module identification technique in biological networks using novel quality metric based on connectivity, conductance and modularity”, Mall et al present an approach to identifying modules of genes from different types of networks, where their approach uses a novel quality metric to evaluate the quality of the partitions based on a number of network metrics such as modularity, conductance and connectivity. Through a series of steps defining the module detection pipeline employed by the authors, they identify modules for the different types of networks and assess the “truth” of the module using enrichment metrics based on GWAS findings as defined by the DMI Dream competition in which the authors had submitted their approach and results. The approach detailed by the authors seems reasonable, and the idea of the DMI to test a great variety of methods against one another is excellent. The processing of networks to identify biologically meaningful modules is still an important area of research and competitions such as DMI have helped to assess progress and identify best practices. However, while the work presented is potentially interesting, as written the general DMI approach and specific results are difficult to understand and so hard to evaluate in light of this (as detailed below in the specific comments). Further, this paper represents a detailing of one of many approaches that were used in the DMI challenge, and shows results only compared to the winner of the challenge. The approach detailed apparently ranked 15 th in the competition, and so was beaten by 14 other approaches. There is no discussion around this, no discussion on why a reader should care to know about one approach that ranked 15 th compared to, say, the 19 other approaches that ranked in the top 20 (14 of which would have beaten the described method). There is no motivation provided on why knowledge of the authors’ approach should be considered in light of 14 other approaches that beat it in the DMI competition. Do the authors believe the DMI competition was the best way in which to assess module identification methods and that the field should adopt the top-scoring methods for this as the state of the art? Do the authors believe that for the type of networks such as coexpression where their approach was comparable to the winners, that their approach has broader utility? Where the other top 13 methods similar with respect to performance across network types? Specific Comments: The paper is somewhat oddly written in that the methods section contain some methods along with some results and generally a failure to really describe the methods employed by DMI to compare methods.Without this the only way to understand the results in the paper is to invest much time going through the challenge description and the results, and so on. That is a huge burden placed on the reader. The components necessary to understand the results given in the paper should be described in the paper, and if there are references that describe fuller details, then that could be summarized in the methods so that the reader understands what was done and where to go for more details.(Further details on what is missing are given in the following comments.) The authors given preliminary experiments done in the methods section, which ostensibly drove some thinking and refining on the approach they ultimately settled on.The preliminary experiments are not really methods, they are more results. And then there is an “insights gained” section in the methods, which again is not really a method but rather detail learnings from these earlier results. While the authors do detail their own module identification process, the way in which the validity of the modules were assessed is not clearly articulated.What were the criteria set forth by DMI?How were the genes identified given a GWAS finding?There is error associated with identifying the vast majority of genes associated with a GWAS finding, so how was this handled?Was an enrichment score used for genes from GWAS being identified in the module?Was it per disease and combined over all diseases?Did effect sizes come into play? Etc.There should at least be a summary of this so that the reader can understand what it means to be able to count a module in the accuracy score for the competition.But there is nothing on this. The results speak to the paired Bayes factor that was used to compare methods, but you can’t really understand the appropriateness of that without have an understanding of the above questions. There is a link to the Synapse platform regarding the challenge, with many scores of pages of material and then a paper posted on biorxiv that provides details on the challenge and the findings.But it is not yet peer reviewed, it does not appear to be published yet, and so all of the missing detail in this present paper simply points to other papers that are not peer reviewed.It’s again a pretty tall order to ask a reviewer to sift through endless pages of material to understand the context of a paper they have been asked to review and to then review on top of that papers upon which the paper they were asked to review is based. 