Authors have made improvements to the manuscript in terms of clarity of methodology. Hence, now I can comment on their methods. Biostatistical methods are mostly sound. However, machine learning part of study seems have major issues: Authors haven’t specified number of responders in their test sets or whether split is class-balanced. But based on their description of 70 – 30 train -test split, sample size is severely limiting for model evaluation, with mere one or two examples of positive class (responders) in test set (30% of 3 male ~ 1 ; 30% of 6 female ~ 2). As a fellow researcher, I completely respect the motivation and effort behind the efforts here, but we as scientific community should understand that the real danger of generalizing observations based on handful of cases is not so much of being underpowered to detect real effect, but of generating false positives results that add to prevailing burden of irreproducible results. It seems that features (250 control gene selection, 2-gene model, 3-gene model etc..) were selected using analyses of both training and testing data partitions. This is called double-dipping and leads to invalid or over-optimistic estimates of model performance. While above two points are deal breakers, I will also mention following points for sake of completion. Hyper-parameters should also be selected ‘ in fold ’ or their choice should be explained. Baseline performance (chance level accuracy) is rather high due to class imbalance – eg: 25/28 = 89% for male responders. Reporting the confusion matrix will be more useful than sensitivity, AUC etc in such cases. For small samples, consider simple linear models than complex non-linear ones such as random forest to avoid over-fitting. Also, consider leave-one-out or k-fold cross validation instead of single test-train split for better estimate of performance. Hence, in my humble opinion, the manuscript in its current form doesn’t meet the necessary scientific rigor. That is at least without a major revision in machine learning methods, such as learning models to predict treatment response in larger undivided dataset of 60 subjects, appropriate use of feature selection etc.