F1000 research provides a platform for immediate publishing of research followed by open peer review and, if approved by reviewers, indexing in PubMed or other databases. In the current article, the authors, who are leaders in the fields of metaresearch and journalology, provide a descriptive summary of the 1865 articles that were published in F1000 Research from earliest publication in July 2012 through November 2017, verifying peer review status through May 2018. They also conduct an online survey in April-May 2018 through email requests to corresponding authors of articles that had been subject to external peer review. The results are descriptive and presented in summary format, which seems appropriate to the study design. No statistical inference is undertaken. A spreadsheet of study-specified characteristics for all 1865 F1000 articles, and survey questions (not responses, which may possibly reflect an effort to preserve confidentiality of respondents) are provided as supplementary files. The bibliometric analysis appears straightforward. The authors may wish to clarify the following points, most of which pertain to the survey and interpretation of its (partially qualitative) results. Methods 1. “All positive and negative experiences were independently reviewed by both authors and categorized into common topics. Any discrepancies were solved via discussion.” Can you provide more detail on how this qualitative analysis was performed? For example, were the “common topics” pre-defined? If not, how did you arrive at the 3 thematic categories that you present in the Results? Especially in light of the appropriately declared author competing interest, and the apparent relevance of the conclusions to the platform on which the authors are publishing the article, further information about how the study methodology may have supported objective interpretation of the qualitative data could strengthen the presentation. Results 2. Thirty-six articles had peer review “discontinued”. Under what circumstances does discontinuation of peer review occur? 3. Of 1476 unique corresponding author email addresses targeted in the survey, you received 296 responses, representing some 20% of the corresponding authors. You identify the low response rate as a limitation of the study, but can you comment further on the representativeness of this sample? Are you able to draw any useful conclusions based on the data you obtained for the full set of articles (year of publication, country income level, etc) regarding the extent to which responders may have differed from non-responders? If such an analysis is not feasible, more caution would seem appropriate in drawing conclusions on the basis of the survey results. Discussion The authors might clarify their reasoning at several points: 4. Article: “There was also a sense that there was the potential for an article to become caught up in the process, immediate publication meant that there was limited scope to remove or submit elsewhere if peer reviewers could not be found or existing reviewers failed to provide subsequent reviews. While these criticisms may have reflected the experiences of some survey respondents, this process is not dissimilar from the many journals/publishers of standard journals which request names of peer reviewers, and in some instances release articles if peer reviewers cannot be found in a reasonable timeframe.” Comment: If an author believes that an article published prior to peer review in F1000 cannot then be submitted to another journal, that author’s situation would seem different from when a journal rejects an article after peer reviewers cannot be found. In the latter situation, the author has the option to pursue indexing by submitting the article to another journal. It’s not clear from this paragraph whether or not authors have this option for articles that haven’t passed peer review in F1000. 5. While there are some similarities, I’m not sure it’s fair to say that the PLOS partnership with bioRxiv “largely mimics the idea of the existing F1000 Research publication model.” PLOS is partnering with bioRxiv in a way that other journals may choose to adapt, to complement the traditional journal publishing model by speeding dissemination while maintaining author choice regarding preprint posting and ultimate publication venue. Within this collaboration, bioRxiv does not function as an end-to-end publishing platform like F1000Research. Preprints posted to bioRxiv are not identified as having been submitted to a PLOS journal prior to publication in that journal, and authors are free to submit their posted work to another journal if the PLOS journal does not accept it. 6. Article: “There still remains a level of dogma surrounding [preprint platform] use by many, and there remain concerns about the quality of the articles published on these platforms.” Comment: It's not clear what you refer to as “dogma." The term seems dismissive, yet your research article identifies a number of reasonable concerns pertaining to preprint platform use. 