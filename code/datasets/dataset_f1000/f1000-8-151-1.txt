 Summary The authors suggest the use of “projection” layers for modeling the regulatory activity of DNA sequences. “Projection” layers perform a linear transformation from a large representation size to a smaller one, by applying a width one convolution. They present experiments with two previously studied architectures in which the addition of these layers improves performance. They suggest that inserting this layer after the initial convolution layer, which most closely resembles position weight matrices, enables the model to more easily capture the similarity of forward and reverse complement motifs for some tasks. This technique has been studied in the larger neural network literature, often by the name of bottleneck layers. The authors should consider citing Lin et al. Network in network (https://arxiv.org/abs/1312.4400) for readers who would like to learn more about the technique. The experiments appear solid and align with my own experiences using similar layers. One difference with my previous experiments is that I've always applied a nonlinearity after the "projection", effectively making it a more standard neural network layer. The authors might consider benchmarking this version. In addition to the projection layers, the authors observation that the final prediction layers for these tasks can use drastically fewer parameters by applying a global average pool is insightful. Overall, I expect this report will be useful to future practitioners of neural networks for DNA sequence analysis. 