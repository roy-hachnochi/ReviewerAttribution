 Positive points: This is a much-needed survey, and I am happy and thankful that the MiRoR team invested their time and expertise in undertaking it. The subject of the adoption of reporting guidelines in the editorial process is sensitive, relevant and worth exploring. Hearing editors is a crucial step for the development of interventions aimed to improve the quality of biomedical research reporting. I congratulate the authors for the initiative. The main result of the study (that the interventions perceived as effective are the most difficult to implement and vice-versa) points out to the need to verifying feasibility before proposing interventions, something that we should all think about. I hope the authors find these suggestions helpful and constructive, and that they find these to be positive contributions. Major concerns: methodology 1) Sample characteristics. Although the authors should be praised for the idea of asking editors about interventions to improve reporting, I have a concern about what editors should be surveyed. If one wants to explore the perceived ease of the implementation of one intervention, is it adequate to choose a population of editors whom we know are already “knowledgeable about the topic”? If these editors are aware of the reporting quality crisis and the reporting guidelines in general, wouldn’t these be more prone to adopt interventions that aim to improve the quality of publications? Therefore, aren’t these the people who would say “yes” to any intervention proposed? Should the authors not interview editors who have never heard about reporting guidelines too? Wouldn’t this heterogeneous sample be more representative of the scenario of difficulties — or the ease — of implementing reporting guidelines checks in the editorial process? Are the study results only applicable to journals that already endorse or require reporting guidelines? I would like to see comments about this limitation in the discussion if the authors agree with this or even if they do not. 2) Sample size. My second most important concern is about sample size. In the Methods section, the authors do not explain how they planned the sample size and based on what assumptions. 24% seems to be a very low response rate. Do the authors believe 24 is enough? This limitation should be discussed. 3) Effectiveness. When you asked your participants about their opinions on “how effective” something would be, what was the definition of effectiveness? Would it be their perception about the intervention improving the adherence to reporting guidelines checklists? Was it a more subjective view of “manuscript quality”? 4) Statreviewer. The authors mention in the text, in Methods: “Based on feedback from the pilot we decided not to include the intervention “Implementation of the automatic tool Statreviewer14” since participants were not aware of this software and stated that their perceptions would strongly depend on details about how it operates which are not publicly available.” However, on page 7, Results, they state: “The implementation of reading tools that automatically assess adherence to RGs, such as Statreviewer14, were seen as potentially interesting interventions.” After all, was this intervention Statreviewer evaluated (as in Results) or was it not (as in Methods)? Do you mean they mentioned other tools, such as Penelope, GoodReports, Cobweb or others? Or were they referring to Statreviewer? Major concerns: reporting 5) Journals. You interviewed 24 editors from 20 journals. What are these journals? I did not see a list of their names, and I believe it is important. 6) Figure 1. Figure 1 is difficult to read/interpret. I like the idea of having the two outcomes (ease of implementation and effectiveness) in one figure very much so that one can compare the two. However, the bar chart is not clear at all, nor easy to interpret. Colours do not help. The authors should consider building a figure with plot points (which would be easy, given the small sample) showing where each participant sits within the Likert scale — not bars. Another suggestion for Figure 1 (here, just a suggestion) is to add something to remind the readers what the interventions are. As Box 1 is far from the Figure, it is difficult to remember what each intervention is. It would be useful to have “nicknames”, for example: 1. Page numbers 2. Checklist 3. Highlight 4. Subheadings 5. COBWEB 6. Use RG 7. Check RG 8. Staff 9. Training 7) Peer reviewers’ checks. The abstract is, in general, well written and clear. However, I got intrigued by this sentence: “Participants believed that checking adherence to guidelines goes beyond the role of peer reviewers and could decrease the overall quality of reviews.” I was surprised that peer reviewers checking reporting guidelines adherence would decrease the overall quality of the paper. Then I reread it in the paper, but here put in a complete sentence, that makes sense: “Participants were concerned that the quality of peer review could be compromised as reviewers should focus on the manuscript’s content and not on the reporting issues.” Meaning, I suppose, that peer reviewers would focus on checking reporting guidelines and not on their “expert view” of the study. I get it, but it is not very clear, neither in the text, nor in the abstract, and I invite the authors to rewrite both. Especially because “reporting guidelines” are about content too. They point out the content elements that should not be absent from papers, so using the word “content” in the manuscript text is not really appropriate. Please, re-evaluate. 8) As I said above, the abstract is good, and there is room for growth (you have now 232 words). However, I could not find the conclusions in the abstract helpful. Could you please inform better what you did conclude? What “points raised in the survey” should be considered? 9) Checklists as forms. ...“assessing completeness of reporting is a subjective task”. This is one of the most important sentences in this manuscript, it tells us about using reporting guidelines checklists as evaluation tools — for what they have not been created. I saw no discussion about this in your paper. 10) Discussion. The discussion of the paper is short, and it lacks several points pointed above. Mainly, it does not discuss the limitations and potential bias of this study. Discussing limitations is a requirement from F1000Research journal. 11) You mention you did analyse qualitative information using NVivo . How did you do it? 12) There are at least eight items from the CHERRIES checklist that I could not see in your paper, and you might want to report, even if in a separate (or supplementary) box, from data protection, preventing multiple entries, to randomisation of items, and others. 13) From Box 1, I do not quite get the difference between Interventions 1 and 2. It seems that both ask authors to submit a populated/completed RG checklist. Is it that #1 asks for page numbers and #2 does not? 14) In my opinion, the sentences from “To contact editors...” to “improve adherence to RGs” belong more to the “Participants” section than to the “Procedure”. Especially the first sentence. Also “Participants could suggest further editors that they considered could contribute to the survey” is something that I would consider as a snowballing or recruitment method, not the survey procedure per se. So, I would suggest putting everything related to recruitment together in one section, to make it easier for your reader (who might be interested in undertaking a similar survey) to find information on such a critical issue as recruitment. Also: how could participants suggest? Was it a specific question in the SurveyMonkey form? 15) Were participants given an estimation of the time required to answer the survey before they began? Minor: about citations 16) Reference number 1 (EQUATOR Network website) is not a proper, complete reference. It points to the home page of the website, not to a document, and as so, it does not show from where the authors collected that information (the reader cannot find them in the home page). The correct way of citing would be adding the full URL. A suggestion: EQUATOR Network. What is a reporting guideline? Available at: http://www.equator-network.org/about-us/what-is-a-reporting-guideline/. Cited on 19/11/2019. 17) Something similar happens to reference number 13. Author and title are missing. It would help readers if it could be typed like this: Schoter S. Evidence-based publishing. BMJ. Available at: https://www.bmj.com/about-bmj/evidence-based-publishing . Cited on 19/11/2019. 18) Reference 14 also lacks authors and titles. References 13, 14, 16, 21 and 22 lack a lot of elements necessary for readers to be able to find them. When working in a printed version (as I am now), you cannot see what the document is, where it is published or by whom — these are available only by those reading online and able to “click” on the links. 19) I would change the sentence: “Raw survey results are given as Underlying data 20 .” To something clearer and that does not require the reader to go to the reference list to understand what it is. “Table XX in the supplementary file X 20 shows the responses from all 24 participants.” 20) Has the study by Hawwash et al. (reference 25) been published yet? (The authors cited the protocol only). Do the authors plan to cite other electronic tools that are out there? Thank you for the opportunity of reading and evaluating this paper. 