 Summary In the manuscript “Fast and accurate differential transcript usage by testing equivalence class counts” by Cmero et al suggest to use the ability of modern lightweight RNA-seq aligners to produce transcript compatibility counts (TCC) in combination with standard tools designed for differential transcript usage (DTU). Although the idea, as described in the introduction of the article, have been partly touched on by previous publications from the Pachter Lab, the approach used in this manuscript is novel since it describes a direct DTU analysis whereas the previous publications only inferred DTU indirectly. In this manuscript Cmero et al compares a TCC based DTU workflow against at transcript based and an exon based workflow using both simulated and real data reaching the conclusion that a TCC based workflow is superior – a novel and important finding. The manuscript is overall well presented and the analysis approach is state-of-the art. Unfortunately the analysis is not quite extensive enough and it suffers from a few major technical problems which together with a general lack of clarity in the writing means the manuscript requires major revisions. Major comments : The authors should also evaluate on the simulated data from Love et al 2018 1 to test the effect of: A different simulation scheme (since the FDRs are so high for Soneson et al data). Investigate the stability of the results using different number of replicates All analysis presented is performed on unfiltered data which is problematic. Firstly it does not reflect typical RNA-seq analysis workflows which always include a step which filters out lowly expressed features before continued analysis. Furthermore, and more problematically, the lack of expression filtering will affect all analysis presented since many lowly or zero expressed features will be analyzed thereby skewing the global comparison due to the difference in the proportion of low/zero in the different datasets/pipelines 2 . Therefore, the authors should include (or replace the current analysis with) an analysis based on dataset which have been pre-filtered for expression. For inspiration of expression filtering 1 , 2 , the edgeR::filterByExpr() function or use the classical 1 TPM cutoff. Naturally the 3 dataset should be also filtered to be comparable (same transcripts/genes tested with all methods). To ensure correct quantification and to make the genome based (STAR) and lightweight based (Salmon) analysis comparable the Salmon index should be build from all transcripts and subsequently (after quantification) the data should be reduced to only protein coding genes. This is necessary to ensure that reads mapping to both protein coding genes and lncRNAs are correctly quantified (and are quantified in a manner comparable to the genome based approach). The manuscript is in general not concise enough. Throughout, the manuscript is very hard to follow which workflow is referred to and the order in which workflows they are presented is not logical (e.g. starting a section with explain about the alternative workflow does not make sense). Figures contain data never mention or used. Especially the discussion falls short of the mark as major parts are either repetitive non-informative. Minor comments : Generally: The authors should use “transcript compatibility counts” (TCC) (aka not “equivalence class read counts” (EC) and derivations thereof) since TCC is the terminology used in the field when ECs are used for quantification 3 . Title: The title seems to lack a word after such as “analysis” or “testing” after “differential transcript usage” Abstract: In the sentence “However, recent evaluations show lower sensitivity in DTU analysis” I guess the authors mean compared to exon-level analysis but this needs to be specified. The conclusion is to broad. The authors investigate DTU but conclude about “many” analysis. Such a sentence should probably be saved for a review paper. Introduction: In addition to exon and transcript based analysis approaches the authors also need to mention analysis of individual splice events (via tools such as SUPPA2 and rMATS) as well as the types of analysis which groups multiple features together (such as Leafcutter and MAJIQ) to make clear that there are 4 different approaches (with TCC based approach being a fifth (or a deviation of transcript based)). I do not require the authors to also compared the TCC based approach to the two omitted workflows – but they should be mentioned in the introduction for completeness. I think it could be beneficial to refer more the lower part of Figure 1 in the Introduction since it very clearly present the 3 different workflows in question? The drawbacks of pseudo/quasi alignment should be mentioned/discussed either in the introduction or discussion. In the sentence “Depending on its sequence, a read can align to all three transcripts, only two of the transcripts or just one transcript. These different combinations result in four possible equivalence classes, containing read counts, for this gene” the last statement is wrong. There are 6 possible (the authors omit uniquely t2 and uniquely t3). This should either be mention or it should be highlighted the example reads in Figure 1 give rise to 4 possibilities. The authors should provide a reference 3 for the term “transcript compatibility count”. The authors should also discuss the ideas presented in Ntranos et al 2019 4 in the discussion of the Yi et al 2018 paper. Specifically the “catch-it-all” and “any transcript-level phenomena” part of the sentence in Cmero et al : “Yi and colleagues have recently introduced direct differential testing on equivalence classes in a catch-all method to identify genes that display any transcript-level phenomena” needs to be changed as aggregation of DTE p-values cannot identify isoform switches if the gene expression is also changing (as discussed in detail in Ntranos et al 2019) – hence the need for methods specifically designed for DTU detection and thereby also the need for the workflow presented by the authors Cmero et al. For Figure 1: Could it be beneficial to divide Figure 1 into A and B referring to respectively TCC and analysis pipelines? Methods: It needs to be described in detail how the fragment length and standard deviation were estimated from the Bottomly data since it is single end data. The actual values should also be reported for reproducibility. Is there a particular reason why Salmon/Kallisto was not run with the bias correction algorithms? Since the authors have to rerun salmon anyway (see major comments) it might be beneficial to update to Salmon v0.13.1 and also use the “--validateMappings” option. Please state the parameters used with Trimmomatic for reproducibility. Please provide info on how the transcript-level counts was obtained (and specify if any scaling was done with e.g. tximport). Please also indicate how the exon/transcript level analysis was summarized to gene-level for each of the 3 workflows. Please provide the unfiltered salmon quantification results (the “quant.sf” files) from the Bottomly et al data as supplementary files to facilitate reproducibility. Please provide details of how the STAR mapped data was converted to DEXSeq ready counts (currently only implied in the result section). Results: From the first paragraph in results it is not clear that you are actually doing all 3 types of analysis and comparing them. And starting with mentioning the “alternative approach” is not reader friendly. For references to previous DTU benchmarking please also cite Love et al 2018 1 . For the “Fewer equivalence classes are expressed than exons” analysis it is unclear whether it is the number of exons or disjointed exon bins necessary for a standard DEXSeq workflow (due to alternative 3’ and 5’ splice sites) which are quantified. Figure 2: Was any pseudocounts or transformation used to calculate the normalized cross replicate variance (Log2(var / mean))? Figure 3: Please add “A” and “B” to the figure in accordance with the figure legend. Figure 3A: What is visualized is not explained in figure legend. It is currently not possible to distinguish between the different methods on the plot. Please provide zoom in versions of the plot to enhance visual comparison. From the point of this paper (comparing the 3 workflows depicted in the lower half of figure 1) it is very strange that multiple exon-based workflows as well as the result of a MISO based workflow (which is never discussed) are also shown. Would it not make more sense to only show exon-based workflow used and omit the MISO based workflow? Furthermore since the supplementary figures show that Salmon and Kallisto produce the same results why not only show one of them? Figure 3B: Please report which FDR cutoff was used to call significance. Please also report the result analysis on the feature level (transcript/exon) and not just for the gene-level. The authors should discuss the much larger variance in FDR for TCC and exon based approaches as well as the generally large FDR values (gussing the target value was 0.05) Discussion: “We propose that equivalence classes are the optimal unit for performing count based differential testing” is to broad a claim since this article is about DTU analysis. Save it for a review :-). Please refer to sashimi plots in addition to superTranscripts – they had the visualization idea first. 