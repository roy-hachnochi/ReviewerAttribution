The issue of data reproducibility is central to science and is worthy of ongoing discussion. Although the authors state at the outset that "Reproducibility is said to be a core principle of scientific progress", to me it IS a core principle. Data that cannot be reproduced does not serve as a foundation upon which others can build. The authors propose that a formal definition of reproducibility be pre-defined, pre-agreed when investigators attempt to reproduce others' findings. Pre-defining criteria for failure or success is always valuable. It removes the natural bias to interpret results to suit one's prejudice post-hoc, and may also to be useful in this context. However, the paper that is cited on which I am the first author (Begley and Ellis, 2012) 1 does not really support the authors' argument. The authors state that we considered results as not reproduced if findings were not sufficiently robust to drive a drug development program. That is correct: we were focused on developing new drugs, and could not justify moving forward if the results were not reproduced. But what was truly shocking, was that in the majority of cases it was the original authors themselves who were unable to reproduce their own findings. Our 'standard operating procedure' when unable to reproduce key findings was to go to the original laboratory and watch them repeat their experiments (which required a confidentiality agreement and precludes disclosure of those laboratories). Their failure to reproduce their findings certainly negated that "research" as being sufficiently robust to drive a drug development program. Using the criteria outlined in Figure 1 of this paper, the published experiments are illustrated in Scenario 8, while the repeated (and unpublished) experiments are illustrated in Scenario 9. In our experience therefore, the pre-definition of confidence intervals appeared unnecessary. It was this experience that led us to conclude that the fundamental problem was not really one of "reproducibility", nor a problem of definition, it was rather a problem of cherry-picking, p-hacking, HARKING, lack of controls, lack of repeats, lack of blinding. This poor experimental methodology was employed so as to generate an initial data set that was sufficiently exciting to justify publication. Therefore, I do not think the issue regarding lack of reproducibility is simply one of a lack of clear definition, rather, in my view, it is systematic and driven by the perverse incentives that govern our current system. Thus focusing solely on agreeing on a definition, does not lead us toward finding a solution to a problem that is deeply embedded in our system, and in fact has been used by some to distract and argue that there isn't really an issue of irreproducibility - its simply about a definition. From my perspective, it would be valuable for these Authors to acknowledge these wide-spread scientific practices as central to the issue of "reproducibility". 