This paper explores a new approach for ranking universities through a proxy for good research practice. A positive aspect of the paper is that rather than simply complaining about the methods of existing ranking systems the authors have attempted to practically provide an alternative. I agree that no other ranking attempts to rank mainly (or entirely) by best practice, and this is a novel contribution of this paper. As such the paper provides a valuable counter-balance to the current dominant ranking methods. Although the paper focuses on the global rankings the same arguments would seem to apply to national ranking systems such as the REF (UK) and the PBRF (New Zealand). It would be good to add a note to this effect. One linkage to prior work which could be added is the discussion around research soundness in Moore et al. 1 . In discussing “soundness” rather than “excellence” they say: “… our focus should be on thoroughness, completeness, and appropriate standards of description, evidence, and probity rather than flashy claims of superiority—presents an alternative to the existing notions of “excellence” … “Soundness” can be assessed by how it supports socially developed and documentable processes and norms.” 1 These goals seem to be similar in spirit to the EQUATOR guidelines used in this paper; indeed, this paper could be regarded as an exploratory attempt to implement the “soundness” concept. The variability of institution names is a perennial problem in these types of study: did the authors consider standardising via the Global Research Identifier Database ( GRID )? This public domain data set of institutions has been produced by Digital Science as part of their Elements/Altmetrics work and is designed for this type of application. Scopus does have an open access indicator which is viewable in their web interface: does this value come through the API that was used? If so, it might be worth noting this data as one way to investigate alternative indicators. Also worth mentioning is that at least one university ranking exercise, the SCImago Institutions Rankings, does (from 2019) include a measure that rewards more Open Access publications: find here . “It is hard to imagine why most universities continue to support the current ranking schemes given that they may be reducing the positive value universities have on society.” I don’t find this hard to imagine at all, reasons could include: inertia, perceived lack of ability for an individual university to alter the rankings environment, historical autonomy of universities and consequent difficulty of coordinated global action. The institutions at the top of current rankings are probably fairly content with their position and those further down tend to have less influence/power. The paper’s criticism of universities “support” for current ranking schemes can be equally levelled at governments who organise national ranking systems: why should they not evaluate research on the grounds of good practice? I would include the term ‘Scopus’ in the Methods section of the Abstract as the data source is a critical aspect of scientometric studies. Clustering is an appropriate method to use to address the issue of small movements in ranks between years essentially being noise. The clustering definition appears fine although I don’t have experience with Bayesian Clustering so cannot give a fully informed judgement on that specific aspect. However, the key point of the paper does not depend on whether this clustering method is the best or most appropriate. Any broadly equivalent method would be fine. I do not anticipate that the precise numbers/ranks/clusters in this paper would be actually used for ranking institutions in any consequential manner. The key message of the paper is that alternatives to the current ranking systems are feasible. As such it makes a useful contribution to research policy. 