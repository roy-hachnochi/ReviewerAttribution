The author proposed a method for outlier detection and differential expression (DE) identification for RNA-seq data. While DE is surely an important problem in RNA-seq data analysis, the proposed method is based on a wrong mathematical model and thus makes no sense. The author assumes the read count $y_{gik}$ follows a negative binomial model with mean $\mu_{gi}$ depending only on the gene ($g$) and the condition $i$. Where is the sequencing depth? Normalizing/incorporating sequencing depth has been a central question in DE analysis and significant efforts have been made by many important papers in this field, but the author completely ignored this term. Similarly, without considering the sequencing depth, the null hypothesis the author wrote is also wrong. With the wrong model and wrong hypothesis for testing, the proposed method does not make any sense. 