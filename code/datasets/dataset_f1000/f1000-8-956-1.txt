This paper is a well written and thoughtful analysis of guideline developers' views on the challenges associated with guideline development in Australia. Many of the issues that were raised resonated with my experience of guideline development in Australia. I note two points that may have limited the applicability of the conclusions that have been drawn. The study was jointly funded by Cochrane Australia and the NHMRC and the sampling strategy was purposive and drawn from attendees at NHMRC guideline development forums. It is therefor possible that the views that have been ascertained primarily relate to developers' experiences with developing NHMRC guidelines, as opposed to other guidelines that do not have NHMRC endorsement. I am aware of several guideline development groups that choose not to receive NHMRC endorsement in Australia because they find the guideline development requirements of the NHMRC overly restrictive. It is possible that these guideline developers would have had different perspectives on these challenges. It would have been helpful to know the degree of experience in guideline development of the different study participants (i.e. the number of guidelines completed) and whether their views varied as a consequence. This was touched on when discussing the theme of "Synthesis and GRADE expertise". I would have liked to know whether it emerged with any other themes. This would help with understanding whether the themes identified are generalisable to all types of guideline developer. However, I do note that there was variation in size, longevity and funding of the different guideline groups interviewed. The paper has concentrated on machine learning, crowd sourcing and automation as being possible solutions to the need to maintain high standards (good quality systematic reviews) but done in a way that would be time and cost-efficient. It was clear from the Results that although these methods are appealing in concept, very few of these methods have been trialled by guideline groups. It might be useful to reflect on this in the Discussion. What sort of time and cost efficiencies have been empirically found - to date (as I know the field is moving rapidly) - with these sorts of solutions? There was a short section on pragmatic review solutions in the Results section but this was not picked up in the Discussion. Was there any discussion of rapid review methodologies by the study participants, and how these are being used? Or use of overviews of systematic reviews? Would the authors like to comment on the reliability of a well done rapid review versus a comprehensive systematic review, or discuss the empirical evidence reporting on the merits of these? Rather than adopting new technologies and IT to solve the labour and cost-problem, is the solution being more pragmatic about the evidence-base i.e. what is the additional yield from doing a systematic review that has canvassed numerous databases, and conducts independent duplicate screening, extraction and/or critical appraisal, over other types of systematic review? Some of the points raised concerning external suppliers seemed to fit better with the updating theme. Participants 7 and 8 seemed to have more quotes used in the paper than others. Were their points supported by other participants in the study? The points raised regarding living reviews were interesting, particularly regarding the need to have predefined criteria for doing proactive horizon scanning or updated searching, rather than triggering an update due to publication of a known new paper. There is less room for selection bias if the process is standardised. It was encouraging to see this sort of research being done. The authors are to be congratulated on reflecting on these issues and considering how evidence based guideline methodology may be improved upon. 