In this protocol, the authors propose to systematically survey methods used to meta-analyze results from interrupted time series (ITS) studies. ITS studies are particularly useful in systematic reviews (SRs) of interventions that cannot be studied using randomized trials (e.g., due to practical, ethical, or legal reasons). Briefly, the protocol plans to identify 100 recent systematic reviews that meta-analyze results from ITS studies and then extract and summarize characteristics of the methods used. I agree with the authors that we currently know relatively little about the characteristics of methods used in meta-analyses of ITS results. From my point of view, I would like to have evidence that can be used to help design SRs that will meta-analyze ITS results, and to help understand potential weaknesses of such SRs. For example, I would like to have evidence to inform the number of pre- and post-interruption time points that should be required of ITS studies for inclusion in a meta-analysis, and information about whether certain ITS designs and analysis methods lead to excessive bias or imprecision. The protocol says that the resulting work will inform subsequent research that addresses these kinds of questions, using simulation and empirical evaluation. My understanding is that knowing about the landscape of methods that have been used in recent SRs will allow this subsequent work to address relevant questions (e.g., it could allow the authors to design simulation studies that usefully model what is being done in practice). My review focuses mainly on the conceptual and statistical aspects of the protocol. I cannot comment on other aspects such as the literature search. I have two "major" criticisms of the protocol: Because I am more interested in the evidence that the subsequent research will hopefully deliver, I would like to see more detailed thought in this protocol about how the subsequent work will be performed. This should then inform what the product of the present protocol needs to deliver to ensure the success of the subsequent work. Perhaps this has already been thought through in detail and not presented here. However, if this work has not been done, I encourage the authors to do it and update the protocol. The analysis is not planned in enough detail that it could be implemented without having to make important choices after having seen the data. I think this potentially leads to at two problems. First, being able to choose from among several possible analyses and means of presentation risks introducing bias. Second, more detailed planning at the protocol stage may prevent problems that would otherwise only become apparent while the work is being done. I suggest that the authors substantially revise this section to specify in detail what they will do and how they will report their results, including preparing skeleton tables and/or figures. I think that thinking through these issues at the protocol stage will likely make this and subsequent work (see point 1 above) more efficient, and lead to higher quality papers. I have the following "minor" comments and suggestions: I suggest that the abstract is clarified to say that the authors will study the 100 most recent systematic reviews that include ITS analyses (and include the date range), rather than simply saying 100 (or whatever the final sample size is determined to be). My concern when reading the abstract was that the 100 SRs could be chosen arbitrarily, giving rise to potential bias. It would be useful for the authors to clarify that, with respect to estimating a binomial proportion, their proposed sample size of 100 would give them a worst-case margin of error of plus or minus approximately ten percentage points (i.e., 40.2% to 59.8%), if the population parameter is 50%. This margin of error is actually quite wide. I wonder if the authors have considered how plausible it is that the population parameter will often be close to the worst-case of 50%, and if so, whether the relatively wide confidence intervals will be informative enough for their subsequent work that will build on this paper? The sample size of 100 seems to have been chosen under the assumption that a binomial proportion will be estimated for each factor studied (i.e., that each factor will have two levels). However, many of the criteria specified are factors with more than two levels (e.g., the protocol gives the example of three types of outcome that included reviews may study: continuous, count, and rate). Given that, I would encourage the authors to think about the more general case of estimating multinomial proportions. This would require a larger sample size for a worst-case scenario equivalent to that of a binomial distribution. A quick search identified Thompson 1987 1 , which provides a table for estimating sample size for estimating multinomial proportions. Briefly, if the authors want to estimate multinomial proportions with 95% CIs that give a margin or error of plus or minus 10%, that paper shows that the authors should include at least 128 studies (irrespective of the number of levels of the factor studied). However, I encourage the authors to double-check this informal power analysis. The authors would then also need to use an appropriate method to estimate the proportions and their confidence intervals. Given the authors plan to use Stata, I think the correct way to do this is to estimate the proportions using syntax like " mlogit myvar " and then obtain the estimates and their confidence intervals via " margins, predict()" , but again I suggest the authors double-check this. With respect to the "Selection of outcomes" section, it may be useful to rewrite item 1 in terms of "number of model parameters" rather than "number of effect measures". The meaning of the current text was not immediately clear to me. I suggest the authors plan to extract a little more detail about the methods used to analyze ITS studies, so that this can be used to inform subsequent work. For example, I am interested to know how often "incorrect" model assumptions are assumed, and under what conditions meaningfully incorrect conclusions result from meta-analyses that include ITS results from misspecified models. I'm thinking about the case where an outcome is bounded, where the observed pre- and/or post-interruption levels are close to the bounds, and the analysis assumes an error distribution whose domain includes values outside the bounds. For example, imagine the outcome "percentage of prescriptions with dosage errors", which is bounded to [0%, 100%]. If a normal error distribution is assumed in the ITS analysis, that distribution would incorrectly permit outcomes 0% and 100%. This may not be a problem if the residual standard deviation is small relative to the distance from the mean to the nearest bound, but if the observed data were close to 0% and/or 100% and the residual standard deviation was sufficiently large, the estimate of relative treatment effect estimate might be biased. I would then be interested in knowing, via empirical or simulation work from the planned subsequent research, whether these kinds of errors lead to meaningfully incorrect conclusions when such ITS results are brought into meta-analyses. If this kind of misspecification is uncommon, then perhaps that subsequent work is unnecessary, but my suspicion is that model misspecification (and this misspecification in particular) is actually very common. I'm also interested in knowing what proportion of ITSs included in meta-analyses assume ITS models that are misspecified in terms of being overly simplistic (e.g., piecewise constant time series with an instantaneous post-intervention step-change). The protocol plans to survey which ITS models are used, but it's not clear whether it aims to judge whether the use is appropriate or not. This is to some extent a judgement about whether particular models are appropriate in particular contexts (all models are approximations of reality), but it would be useful to have a solid evidence base on which to make recommendations for what people should consider when they undertake ITS-based SRs. The protocol talks about ITS analyses that include versus exclude controls (i.e., where the interruption does not occur), but it's not clear that the protocol will extract data on this. It would be interesting to know what proportion of SRs of ITS studies permit or include uncontrolled ITS results, and ultimately to have some information about whether inclusion of such studies leads to meaningfully incorrect conclusions (I assume it does, though I could imagine that it may be possible to include controlled and uncontrolled ITS results in a meta-regression and still reasonably estimate treatment effect). Similarly, it would be interesting to know how many controls are used (i.e., I assume that ITSs with one control are common but that 10 or 20 controls are quite rare), and then from the subsequent research, the number of controls that SR protocols should specify. I suggest including the restriction to English as a possible limitation. I wish the authors success with their research! 