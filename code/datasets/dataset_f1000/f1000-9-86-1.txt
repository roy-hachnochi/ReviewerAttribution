This article reports on a systematic review of non-randomised studies that have evaluated approaches for improving the recruitment of people into randomised controlled trials (RCTs). The objective was “to quantify the effects of strategies to improve participant recruitment to randomised trials evaluated using non-randomised study designs”. The authors acknowledge that “currently, non-randomised evaluations of trial recruitment interventions are rejected due to poor methodological quality”, but their rationale for conducting this review is that “without collation, this body of evidence is currently being ignored, and may hold substantial/promising undiscovered effects”. This is an important area of research since recruitment into RCTs is challenging; and failed or suboptimal trial recruitment often leads to trials being abandoned, creating a substantial amount of research waste, as well as having negative effects on trial participants and delaying the uptake of clinical trial findings into policy and practice. General comments: I have read this paper with interest because most studies on trial recruitment are non-randomised and it would, therefore, be helpful to obtain as much useful information as possible from these studies to inform research practice and policy. However, a major obstacle, as the authors acknowledge, is the poor methodological quality of non-randomised recruitment studies. According to the stated rationale for doing this work I was expecting this systematic review to make the best use of the available data that could be extracted from these methodologically poor studies, thereby providing an advance on previous research. For instance, the methods section suggests that GRADE methods might enable methodologically weak studies to contribute informatively to data synthesis. But this review did not explore any methods such as GRADE that might have enabled any of the data from these studies to contribute to a comparative analysis. And the review stops short of providing any comparative analysis, thereby failing to address its stated objective. I assume that this is because the included studies were deemed too weak to be useful, although this is not explicitly stated or discussed. I agree with the authors that the statistical pooling of studies' results would not have been appropriate, due to the heterogeneity of study designs. A narrative synthesis would, therefore, be reasonable, as the authors acknowledge. However, no integration of the comparative results, strengths, and limitations of any of the individual studies is presented, meaning that effectively there is no narrative synthesis (though a summary of some study characteristics is given). I appreciate that it is not easy to do a systematic review on this topic area due to the considerable heterogeneity and weaknesses of the studies, so I sympathise with the authors. But I am surprised that some of the problems with this evidence base were not identified during the protocol development, scoping, and pilot testing of the methods. The upshot is that very weak (and seemingly useless) studies appear to have made it all the way through to the data synthesis step but then all of them were rejected from narrative synthesis. Could the evidence synthesis planning process have been done differently perhaps, to identify some of the methodological problems (e.g. limited sample sizes and ambiguous descriptions) earlier, or would this have just resulted in all studies being excluded? It feels to me that all studies have, effectively, been excluded anyway since none of them provide any comparative results to support the review objective. Are there any lessons that review teams can learn here, to improve future evidence syntheses in this difficult area? A comprehensive risk of bias assessment was conducted, but this is not reported consistently. For example, the 7 individual domains of bias are reported for the 10 studies that were excluded because they were at critical risk of bias (Extended data file 4) but only an overall risk of bias judgement is reported for the 72 studies that were included in the review (Extended data file 5). Given that a total of 92 studies were eligible for inclusion in the review, the risk of bias assessments for 10 studies appear to be missing (Extended data files 4 and 5 report 82 studies in total). The review report is not fully clear about which of the methodological issues discussed were assessed as risks of bias, and which were identified additionally. Missing data are described separately in the Discussion (e.g. for the Blackwell 2011 study) but I presume that these missing data had already been captured in the “Missing data” domain of the risk of bias assessment? It would be helpful if each of the risk of bias domains could be explained so that their interpretation in relation to non-randomised recruitment studies is clear. In summary, this review has failed to meet its stated objective and it is difficult to see how it advances knowledge in this scientific area (we already knew that non-randomised studies are methodologically weak, and the rationale behind this review was to make best use of these studies despite their limitations... which hasn't been achieved). But I believe the authors could update this manuscript to make it into a useful scientific contribution. I have suggested two possibilities that the authors could consider. I am not sure how much effort these would take but they would utilise the studies already included in the review and may require little if any amendment to the current protocol: Check the included studies and evaluate systematically whether there are any data that can be obtained from these non-randomised studies that would constructively inform a data synthesis (i.e. go back and attempt to fulfill the stated objective). Currently, the review considers only limitations, whereas it would be helpful to look for the strengths (if any) that can be taken from these non-randomised studies. Would GRADE or other approaches (e.g. sensitivity analyses) enable any of the included studies to contribute to data synthesis? Could you make best- / worst-case assumptions in sensitivity analyses to address some of the uncertainties? (see specific comments below). I would be surprised if all 92 identified studies are so poor that none of them at all can be descriptively reported in the narrative synthesis, but this should be systematically checked and transparently reported so that if it is true that the entire evidence base is useless then at least the conclusion would be defensible. If the evidence-based conclusion is that data synthesis is not feasible due to all studies being too weak, then pragmatically the review could direct its effort to systematically analysing the limitations of the studies as a basis to support evidence-based recommendations on how to improve non-randomised studies in future. In order to make the critique of the studies’ methods more systematic, consistent, and transparent perhaps a table could be provided in the results section for all the included studies indicating exactly which methodological limitations each of the studies was susceptible to? This could then support evidence-based specific recommendations about exactly what needs to be improved in the non-randomised studies for research practice to be improved. I think this could make the paper valuable – provided that the recommendations arising are evidence-based and are communicated and disseminated carefully to achieve impact. Specific comments: Methods: Types of intervention: I don’t understand the meaning of the following sentence (NB minor typo needs correcting): “Any intervention or approach aimed at improving or supporting recruitment of participants nested within studies performed for purposed unrelated to recruitment.” Methods: Search methods for identification of studies: “Adding these studies into the review would not strengthen or disprove the conclusions we had already drawn.” This statement seems to imply that the 10 studies being referred to were somehow tested for their influence on the overall results. I suspect this is not the intended meaning. Do you mean that, by virtue of their lack of clarity, these studies could not usefully inform the review? Methods: Assessment of risk of bias in individual studies: Very little description or discussion of the risk of bias assessment is provided. As ROBINS-I is a relatively new tool I wonder how many readers will be familiar with it? Could a supporting explanation be provided of how decisions were reached? If the review is re-framed to focus on the studies’ weaknesses (see my general comments above), then a more detailed discussion of risks of bias would be appropriate. How good was reviewer agreement on rating the risks of bias? Were any specific bias domains particularly difficult to interpret or agree on? Figure 1: PRISMA chart: What were the reasons that so many (33) full-text articles were not available? Is there anything practical that could be recommended to reduce this problem in future research? Results: Screening and identification of studies: “this includes seven articles which required additional data to allow for inclusion”. The meaning of this statement is not clear. Do you mean that insufficient information was reported in these studies to make an eligibility decision and since the authors did not provide any further information these studies had to be excluded? Results: Using risk of bias to select studies: The meanings of the seven bullets listed here are not explicitly defined anywhere, although to me they seem intuitive and self-evident. An exception is “Language adaptations” which I didn’t understand the meaning of until I reached Table 2, where I noticed that it is explained (implicitly). Perhaps cross-refer the reader to Table 2 to clarify what these bullets mean? Table 8: Minor typo in the heading of Column 8. Results: Postal invitations and responses: “As mentioned in the ‘face to face recruitment initiatives’ section, many of the postal interventions used a face to face method as their comparator”. This statement might be misleading as the text in the Face to face recruitment initiatives section does not say anything about postal interventions. However, the information referred to is in Table 2, so perhaps refer directly to the table? For studies that spanned these two categories, how was a decision made on which category to allocate the study to? Results: Trial awareness strategies aimed at the recruitee: Minor typo: “professional”. Results: Trial awareness strategies aimed at the recruiter: Minor typo: “fives”. Discussion: “Non-randomised evaluations have acquired a bad reputation, but they do have their merits. Randomised evaluations are not always possible because of logistics, financial resources, or ethical reasons” This statement seems to be referring to non-randomised studies in general. Are there specific reasons why some (or indeed most) types of recruitment study can only be non-randomised? Discussion: Some information about the Adams 1997 study is revealed here but it feels like this information should have been reported in the results section, along with equivalent information from the other included studies, for consistency and transparency. Discussion: “Currently, trialists are focussing on the mode of delivery of the interventions that they are working to evaluate; they omit key details regarding the content of the intervention, as well as the specific timescales that interventions were in place for.” Is this statement evidence-based? It doesn’t really link to the results section. Discussion: The issue of missing data has not been consistently covered for each study in the results section, so it is unclear how representative the discussion of missing data in the Blackwell 2011 study is of the studies in general. It feels harsh to say that 2 missing data (1%) would be sufficient to cause mistrust in the entire Blackwell 2011 study. Were such strict criteria applied across all studies? Could you do a (quantitative or descriptive) sensitivity analysis – i.e. explore whether changing the sample size by n=2 would make any tangible difference to the outcomes? I think even Cochrane Review guidance is pragmatic about not being overly strict if the amount of missing data is trivial? Discussion: The review included several study designs of which Yield studies were the most frequent and, apparently, least useful. Could the remaining study types that would be more reliable tell us anything? No results from any of these studies have been provided. Conclusions: The statement “Some interventions to increase recruitment described in this review do show promise” is not evidence-based, since no evaluation of intervention effectiveness has been provided. References: There is no information provided for reference 15. 