UPDATE AFTER READING REVIEWS  I have read the authors' rebuttal and my score remains unchanged. All reviewers seem to agree that the paper could use better clarity -- if the paper does get accepted, please consider a considerable effort towards this.  =====  # Summary * The paper addresses generalized one shot learning. In this scenario, we know what the test classes will be (although we do not have acess to data from them). At test time, however, the data may also come from training classes. This is, generally speaking, harder than classical zero-shot learning. * The paper proposes using a neural network to jointly learn descriptors for the data and the classes, putting them in a common embedding space. * The main novelty is using a temperature in the softmax classifier, which intuitively controls the 'spikiness' of the confidence over classes. Together with an entropy term, this makes the network less over-confident of knowing seen classes, and gives larger probability to unseen classes. * Experiments on four datasets demonstrate competitive performance  # Quality I think this paper has good quality. The ideas are simple -- albeit mildly well-explained. There is a clear motivation the evaluation is thorough. I think the paper could benefit from a more detailed section on related work.  # Clarity I think this is a medium point in the paper. Although the concepts are simple, I find the sentences that try to explain it a bit confusing. Take for example this phrase:  > Intuituively, probability $q_c$ should assign each seen image to the target classes as certain as possible, namely, classify it to the target classes that are most similar to the imageâ€™s label in the source classes, rather than classify it to all target classes with equal uncertainty. In other words, we should minimize the uncertainty of classifying the seen data to target classes such that source classes and target classes are both made compatible with the seen data and thus comparable for generalized zero-shot learning.  The statements are bit too long for English, and contain multiple subordinate sentences each. I had to re-read this multiple times to understand the main point.  # Originality I think this is a medium point of the paper. The contributions are mainly on the application of well known  techniques to GZSL. In terms of application this is interesting, but not very original.  # Significance I think this is a strong part of the paper. Although the modifications are simple (eg. temperature in softmax), they perform really well. This might be a new baseline that sets a new bar for future work in this area.