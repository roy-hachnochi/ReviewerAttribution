Overall this is significant and original work for learning 3D keypoints from images. The most significant contribution is the use of a differentiable Procrustes alignment layer, which directly optimizes the keypoints for the upstream task of relative pose estimation. This is a great direction for research in keypoint learning, which often settles for optimizing intermediate metrics such as repeatability, matching score or semantic meaningfulness. The authors use synthetic 3D models with known relative pose ground truth for training, and evaluate also on synthetic shapes.  The paper is well-organized with easy to follow arugments + supporting math.  One criticism of the high-level approach of the paper is that the keypoints are class specific and don't generalize to an arbitrary number of objects and points the scene. The keypoints also lack descriptors, so it's unclear how this method would deal with aligning more complicated and noisy scenes where there isn't a pre-defined correspondence mapping between points.  The proposed solution for dealing with symmetry by using two separate networks at inference time seems inelegant. Given that the two tasks are highly related, the two networks might be combined into a single multi-task network, which implicitly learns the orientation by predicting the orientation flag.  It would also be interesting to report the accuracy of the binary flag predictor network.  I love the visualizations in the supplemental material website. I found the ablation study visualizations particularly useful in understanding the importance of the various losses used. Adding some numbers to go along with these visualizations would help convey this result in the main paper.  Typo/Question: What is the 3D-SE metric? I don't see its definition in section 6.1 as is mentioned in the caption in Table 1. 