This paper extends Bayesian bootstrap ideas to optimizing a functional of some parameter with respect to an underlying but unknown distribution. Instead of simply generating random weights for the data set, the paper proposes additionally generating observations from a parametric centering distribution (with an empirical Bayes prior) and then generate random weights for the augmented data set based on a Polya urn scheme. The idea is that the augmentation acts as a regularizer, which combined with the random weights, gives a notion of uncertainty. The method is applied to generating more realistic VB posterior distributions and prior updates for Bayesian random forests. The method is tested on a small set of inference problems on UCI data sets.  This paper is more confusing than it needs to be, and the large number of use cases does not help. While I really like the ideas in this paper, it is not quite where it needs to be for publication. My largest concerns center around tunable parameters: c, T (algorithm 1), and f_{\theta} (particularly when used as in algorithm 1). As we see in Figures 1 and 2, the method is highly sensitive to c. I am guessing that the same holds for T and especially f_{\theta}. Theoretical results on what these values should be, along with robust empirical demonstrations of the effects, would move my score into the accept/strong accept category.   This line of research has a lot of promise where objective functions are generated from data and there is a large danger of overfitting to a small data sample (some RL, Bayesian optimization).