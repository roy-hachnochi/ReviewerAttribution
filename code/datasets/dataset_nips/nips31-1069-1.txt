The paper proposes a variance reduction based coordinate descent algorithm for composite optimization. The main idea is to sequentially construct an unbiased gradient estimate based on the current partial derivative and previous estimate. The proposed algorithm achieves a linear convergence rate similar to randomized coordinate descent when the problem is strongly convex.   I find the idea of combining variance reduction with coordinate descent very interesting. However, the paper looks quite messy in its writing. For instance, many of the symbols/definitions are even not defined before its first appearance. Moreover, the technical results are very hard to go through due to the lack of explanation and discussion. Overall, I find the paper interesting but also a lot of room for improvement.  Here are a list of points to be clarified: 1) It is assumed that the proximal operator according to an arbitrary metric B is available. However, such proximal operator is in general hard to evaluate unless the regularization function is separable. This makes the claim of non separable proximal term unfair. Moreover, is it true that other's analysis can not be extend to non separable regularizers?   2) The introduction of the metric B makes the presentation confusing. I would suggest to only consider the case B= I in the main paper and present the general case as an extension in the appendix.  P.S. Please put in clear that the strong convexity assumption is also respect to the metric B because we need it to pass from the last line of page 12 to line 345.   3) At line 56, a random variable \theta_k is introduced to obtain unbiased gradient. Is it true that such theta_k always exist for any sketching S_k?   4) At line 61, it is claimed that the variance of g_k goes to zero because h_k and g_k are estimate of the gradient. However, h_k is a biased estimate, which makes this statement non trivial. Please provide more details about it.  5)  In the statement of Thm 3.3, we have no idea what \sigma is. Please make clear that it is in fact an intermediate parameter which is free to chose.   6) I suggest to put Thm E.1 into the main paper because it facilities the understanding of different parameters under the standard setting. Under the current presentation, we have no idea what is the order of the step-size alpha which turns out to be crucial for the convergence rate.   7) Section 4 is very confusing because we have no idea what the vector v is and the result seems to be overlapping with the previous section.   8) The experimental section is very unclear about what is the objective function. In particular, it is more interesting to compare with the standard coordinate descent method instead of projected gradient method, as shown in the appendix. It seems like coordinate descent always outperform the proposed algorithm, is there any intuition about this observation?   EDIT: The author's rebuttal has carefully addressed my concerns. I believe it is a good paper worth to publication thanks to its theoretical contribution, which fills the gaps of non-separable proximal terms in the coordinate descent literature. 