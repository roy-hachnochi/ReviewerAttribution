The paper presents an interesting view on the batch normalization method. In particular, the authors investigated the effect of a batch norm layer after a linear layer without any activation function. For this particular type of BN, the output is invariant to the scaling of the linear layer. On the other hand, in this case the gradient over the parameters with BN is scaled. The paper further derives that the step size of the “normalized” weights also gets scaled due to the BN layer.  Based on the analysis, the authors show empirically by adjusting the learning rate accordingly, one can achieve similar performance without BN. Also the authors compared different norms for the BN layers.  In all I like the paper. BN is used everywhere but little is known how and why it works. This paper provides an interesting angle toward understanding BN.    Questions and Possible Issues: 1. I like the analysis in section 2. Still the proof in section 2 omitted some details. For example, line 107-108: taylor approximation is used without being mentioned.  Similarly for line 109-110: another different form of taylor expansion is used without explanation. Could the author add more details to it? Or put it in appendix.  2. It appears to me the same analysis goes through for the Linear + RELU + BN, too. Correct me if I am wrong.   3. The lines in the figure 1, 2, and 3 are heavily overlapping with one another, making it extremely hard to tell which is which. Could the author try some other ways of visualization of the results?   4. Line 119 claims “with out WD, the norm grows unbounded, resulting in a decreased effective step size”. I don’t see why the norm can grow unbounded even without WD. Could the authors elaborate on it?  