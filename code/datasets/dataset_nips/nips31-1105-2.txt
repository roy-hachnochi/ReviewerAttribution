The rebuttal addressed a few minor confusions I had.  ------  This paper considers the problem of subspace learning, in presence of nontrivial portion of outliers. Especially, this paper develops improved theory and algorithm that O(n^2) outliers can be tolerated, rather then O(n). The paper is clear and well written.  The paper characterize the geometric property as either orthogonal or close to the desired subspace in Lemma 1, and then derive the necessary scaling for any global solution to be orthogonal to the subspace.  Algorithmically, initialized on a point, a natural gradient descent procedure can recover a good approximation of  the normal direction. Performance guarantee wrt different step sizes are also discussed.  * In figure 2, the curve for tan(theta_k) is not monotone, could the authors give more intuition why this is the case?  * Could the author give some intuition why the geometrically diminishing step size matters for this problem or other general nonconvex problem?  * In figure 3, the proposed algorithm seems to take more time and iterations compared to ALP. This does not support the argument made earlier about computation efficiency. Maybe by increasing the size of the problem, this point could be made more clear. 