UPDATE: Thank you to the authors for addressing my concerns. With the new version of Table 1, and the clarification of ResNet-18 vs BN-Inception, my concern about the experimentation has been addressed -- there does seem to be a clear improvement over classical 3D convolution. I have adjusted my score upwards, accordingly.  ===  Recently, a number of new neural network models for action recognition in video have been introduced that employ 3d (spacetime) convolutions to show significant gains on large benchmark datasets. When there is significant human or camera motion, convolutions through time at a fixed (x,y) image coordinate seem suboptimal since the person/object is almost certainly at a different position in subsequent frames.  This paper proposes a solution, “trajectory convolution”, which performs temporal convolution along the path traced out by optical flow -- in essence following the moving object. Think of this as motion-stabilized 3D convolution. Further, the motion trajectory can be included in the feature representation, providing an elegant way to combine both appearance and motion information in a single network.  The idea is straightforward, and the paper is clearly written, but the experimental evidence is not enough to justify the claim that trajectory convolution is superior to classical space-time convolution.  Quality: My primary concern with this paper is in the experimental evaluation. Specifically in Table 6, Separable-3D, presumably [45] the number reported is 76.9. But the source paper  https://arxiv.org/pdf/1712.04851.pdf (see Table 6) reports top-1 & top-5 accuracies of 77.2 and 93.0 respectively, giving an average accuracy of 85.1. Even using the RGB-only stream (see table 4) the numbers are 74.84 & 91.93, giving an average 83.4. The other comparable space-time convolution method, R(2+1)D, achieves 82.6. The TrajectoryNet performance of 79.8 does not support the claim that trajectory convolution leads to superior performance over classical (separable) convolution.  Table 5, with performance on Something-Something, would be more convincing if it included R(2+1)D and S3D, as well as fewer blank values.  Further, the differences between results is very small in several tables, especially Table 1. Would it be possible to include error estimates? That would help to gauge which differences are significant.   Clarity: The explanation and writing is very good. Just a few minor points of clarification: In general, what is the additional runtime cost of doing trajectory convolution vs. rectangular convolution? In section 3.3, by how much does the number of parameters actually increase? In Table 2 & corresponding text, please be a little more clear about how exactly the “4-dimensional appearance map of trajectory coordinates” is formed. In table 6, Separable-3D should cite [45] if I’m not mistaken.  Originality: There has been much recent work exploring exactly how best to combine appearance and motion (optical flow) in action recognition networks. The specific approach proposed is a new combination of familiar techniques. The related work section is comprehensive and well-written.  Significance: This paper would be more significant with experimental results that clearly validate the claim that trajectory convolution is superior to classical 3D convolution. 