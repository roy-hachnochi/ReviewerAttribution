This paper studies deep learning models under the perspective of minimum description length principle. In MDL principle, the quality of the model is quantified by the minimum length of the sequence to transmit the data. The author study the traditional compression bounds for neural networks, and claimed that the traditional bounds are not efficient. Then they introduced s coding scheme called "prequential coding" and show that prequential coding can significantly reduce the description length.  Overall, I think the quality and clarity of this paper is above the standard of NIPS. So I vote to accept this paper, but I also would like to see the authors provide some feedbacks to the following questions:  1. A key question is that how can MDL theory help us improve deep learning is missing. Originally MDL is a quantity that measures a statistical model and can be used to help us choose a proper model. In this paper, prequential coding was proposed to compute MDL, but can it help us select models?    2. A second question is, since prequential coding gives much more efficient compression bounds, compared to state-of-the-art network compression algorithms, I wonder can prequential coding provides some intuition to improve network compression algorithms.  ******************************************************************************************* I'm happy that the author provide some useful intuition and discussions in the feedback. I'll keep my score as 7.