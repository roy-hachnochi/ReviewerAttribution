This paper propose a pipeline to perform view-centric single view 3D shape reconstruction. To avoid overfitting to the training data, the authors decompose the process in 3 steps, each of which is solved using a standard off the shelf deep network: (i) single view depth prediction (ii) spherical map (inferred from the depth) inpainting (iii) voxel (inferred from the inpainted spherical mal) completion. The authors claim their approach allows much better generalization than existing approaches.  I like the general approach of the paper and find the quantitative results reasonably convincing, I thus clearly support acceptance. I however have a few comments/questions.  Most importantly, I didn't find the experiments completely convincing since the only view-centric baseline is quite weak. I think it should be possible to re-train AltlasNet to perform view-centric perdictions (and even if it fails this is an interesting point), and thus have a stronger baseline.  Some points are unclear to me: - How is the center of the object determined (especially from the depth map only)? How does it affect the results? - is there an alignment (e.g. ICP) before computing the CD? If not, I don't think the comparison with object centric approaches is fair, and results with alignment should be added.   Some smaller points:  - the different parts could easily be adapted so that the pipeline can be trained end-to-end. I think the paper claims it might limit generalization, but it would be good to actually see the results. - l 121-125 make claims about generalization to real images which don't really convince me. Pix3D (Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling) would allow to test on real images, that would be a nice addition. - l. 126: I don't agree that depth prediction is class-agnostic 