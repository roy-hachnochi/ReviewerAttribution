Post rebuttal: I now understand the middle ground this paper is positioned, and the difference to propositional OO representations where you don't necessarily care which instance of an object type you're dealing with, which significantly reduces the dimensionality of learning transition dynamics.   But this is still similar to other work on graph neural networks for model learning in fully relational representations, like Relation Networks by Santoro et al., and Interaction Networks by Battaglia et al. which in worst case learn T * n * (n-1) relations for n objects for T types of relations. However, this paper does do a nice job of formalizing from the OO-MDP and Propositional MDP setting as opposed to the two papers I mentioned which do not, and focus on the physical dynamics case.  I am willing to increase my score based on this, but still do not think it is novel enough to be accepted.  ---------------------------------------------------------------------------------------  This work proposes a deictic object-oriented representation where objects are grounded by their relation to a target object rather than global information. This is very similar to relational MDPs, but they learn transition dynamics in this relational attribute space rather than real state space.    Compared to OO-MDPs this can zero-shot transfer to environments with more objects as long as the types of the objects stay constant.  They are also able to give a bound on memorization of the transition dynamics in attribute space by modifying the DOORMAX algorithm to their deictic version.  My main issue with this work is that I'm not sure how this differs from relational MDPs. The method learns dynamics of an environment in this relational attribute space which is given and borrows the DOORMAX algorithm from OO-MDPs, and are able to show nice results on two environments, but I'm not sure it is novel. If the authors could clearly explain the differences between relational MDPs and deictic OO-MDPs, it would be very helpful.