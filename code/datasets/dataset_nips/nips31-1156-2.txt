In this paper, the author(s) studies learning schemes based on interpolation. The main results include analysis for least squares regression, and classification and k-nearest neighbor schemes. It is well known that interpolation would lead to overfitting. The results in the paper hold because of the assumption that the marginal distribution is the uniform measure. This assumption is too restrictive: the data in many learning problems is irregular due to the large data dimension and data manifold structures. So strong analysis made in such a special setting should not be used to argue for the studied interpolated classification. 