UPDATE after rebuttal: I still doubt the practical impact of the paper, but overall it is a nice contribution that closes a gap in theory. I still vote for acceptance.  This paper considers methods for minimizing finite sum structured, strongly convex problems and shows that methods that compute a full gradient (i.e. do a full pass over the data from time to time, like in SVRG or SARAH) can be faster than methods without a full pass (for instance SAGA), given that the condition number of the problem is small (compared to the dimension n).  The paper considers the SVRG variant and analysis from [21, Xiao Zhag] and makes the observation that one can obtain a slightly better estimate of the convergence rate when the condition number \kappa is small compared to the dimension n. This can result in a \log n speedup over previously known results in this parameter regime (i.e. the algorithm stays the same, just a more careful parameter setting leads to the faster rate).  The main idea is as follows: stochastic algorithms that gradually improve the estimate of the solution (think for instance of coordinate descent) converge at a rate (1-1/n), i.e. by a constant factor each epoch. SVRG can converge by a factor (n/\kappa)^{1/2} instead, which is much better when n >> \kappa. The paper gives a lower bound for coordinate descent/SDCA, adapting the proof from [23, Arjevani] and a lower bound for SVRG (by slightly refining the proof from [17, Arjevani, Shamir]) that matches the upper bound.  I think this paper is a nice theoretical contribution that settles some open questions on the complexity of SVRG. However, there is not clear practical impact. The improved rate can essentially only be obtained by knowing the condition number \kappa in advance. It is an open question whether there exists a parameter free algorithm that attains the same complexity.  The paper distinguishes algorithms that satisfy the “span assumption” (like SGD) and algorithms that do not satisfy this assumption (like SVRG). However, to me the term “span assumption” sounds a bit misleading: the “trick” in all lower bound proofs based on this assumption is that the dimension n is much larger than the number of steps the algorithm performs. Clearly, algorithms that do a full pass over the data, like SVRG, do not fall into this class. But on the other hand, the iterates of SVRG also lie in the span of the first n gradients queried at x_0 and the subsequent randomly sampled gradients at iterates x_{n+i}. So, perhaps the result is more a consequence of the fact that the dimension is small (compared to the number of steps performed), and not on the fact that iterates are not contained in the span of some vectors? Maybe the authors could comment on that?  There is typo on line 139. 