This paper proposes a distributed and communication-efficient Newton-type optimization method for the empirical risk minimization problem  in distributed computing environment. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction and send to the main driver, which averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. The authors  theoretically show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Besides,  a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. Large-scale experiments on a computer cluster empirically demonstrate the superior performance of GIANT. Overall, the presentation of this paper is good.  The related works and different research pesperctives for  distributed learning are well overviewed and organized . While I'm not an expert in the distributed learning field, I have no difficulty following the paper. Through reading this paper, I feel like that I have catched one rough map of this field. From my understanding, the communication efficiency mainly comes from using the harmonic mean to approximate the arithmetic mean. The superior theorectical convergence results are mainly due to  the matrix incoherence assumption and analysis . I think  the incoherent data assumption is resonable. My main question is that, as the incoherence assumption is common in  matrix completion and approximation, is this rarely used in distributed Newton-type optimization? If such kind of work exists,  review of them and discussion of the difference between this paper are suggested.