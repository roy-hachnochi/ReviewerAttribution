This paper explores the idea of solving classification problems by first modeling the data generating distribution.  While typical discriminative classifiers model p(class|data), generative models attempt to model the joint distribution p(class,data).  The proposed MetaGAN tries to learn p(data), as part of a larger learning task trying to learn p(c|data).    The key novelty in the paper is the insight that if you:  * use GANs to capture the data distribution; * augment your classification problem with a "fake" class; * train a classifier simultaneously with a GAN;  then the GAN can help the classifier learn better decision boundaries by helping to refine the boundary between true data distributions and the artificial fake class.  The primary technical contribution of the paper is the combined loss function that blends all of these elements together.  This seems like an entirely reasonable idea.  However, I have several reservations about the paper:  1) I'm concerned about the loss function.  The loss function described in Eqs (1)-(6) is based on simple maximum likelihood learning.  However, there is a lot of research into the problems with these sorts of loss functions; significant effort has gone into alternative loss functions that typically perform much better in practice (see, eg, Wasserstein GANs, etc.).  Why should we have confidence in this particular loss function?  2) The experiments aren't compelling.  There are so many DNN papers out there, and so many different ideas, that the bar for publication (especially at NIPS) is very high.  While your method demonstrates slight improvements on classification tasks, the tasks are not particularly state-of-the-art, and the algorithms compared against also don't feel like the state-of-the-art.  This leads me to wonder: why should we publish this paper, as opposed to any of another million ideas?  I think that ultimately this paper is solidly borderline - some good ideas, and a reasonable execution of those ideas, but with nothing particularly compelling.