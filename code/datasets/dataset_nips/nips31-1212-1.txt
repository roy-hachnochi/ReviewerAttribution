This is a theoretical paper about differential privacy. The paper's contribution is a new technique for local differential privacy for periodically recomputing statistics. The privacy guarantees of previously known methods degraded quickly as a function of the number of times the statistics are recomputed. The main contribution of the paper is a model where the error required for a fixed level of privacy grows not with the number of recomputations, but only with the number of significant changes in the statistics.  I am not familiar with the state of the art in differential privacy, but the problem being solved seems interesting as a differential privacy problem, and the solution, as far as I can tell, seems nontrivial. However, I am worried that this paper would be more suitable for a conference which has more of a focus on the theory of differential privacy than for NIPS. The submission seems out of scope for NIPS; beyond the general connection between differential privacy as a whole and learning as whole, this particular paper does not seem to have any strong connection to learning or the other subject areas in the NIPS CFP.  - In light of the authors' feedback as well as the consensus among other reviewers that this paper would be of interest to the NIPS community, I updated my overall score to acceptance. I still have concerns as to how accessible the paper is for the NIPS audience; hopefully the authors will revisit the paper's exposition as they promised in their feedback.