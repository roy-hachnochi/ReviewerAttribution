In this paper, the authors propose a meta-learning style algorithm in order to optimize the design parameter of RL methods, in an interesting online fashion, while the RL agent is training.   In RL, it is always a been a question about what is a right discount factor \gamma or if TD(\lambda) method is used, what is the best \lambda. In this paper, the authors provide a method such that the agent( or the meta-learner) figures out itself what is the right choice of parameters to use at the time. For a given objective function J(\theta, \gamma, \lambda), the RL agent computes the gradient with respect to \theta (the policy (and/or value function) parameter) in order to learn a better policy ( and/or a better value function), then update the model (policy/value) parameters. Then if the agent's desire is to maximize the return under a specific \lambda' and \gamma', it chooses the \lambda and \gamma such that the agent intrinsically maximizes the return under \lambda' and \gamma' while the gradient of J(\theta, \gamma, \lambda) with respect to \theta is considered.   ***************** There are two comments which they did not affect the scoring but are needed to be addressed.  1) The first line of the abstract is not true in general. I would suggest that the authors restate it. 2) Cite and mention this work "On Structural Properties of MDPs that Bound Loss due to Shallow Planning" which study why you even do not want to use the given discount factor and why you would even rather choose a smaller one. *****************  The paper is clearly written, also deliver the idea and method clearly.   There are two major issues.  1) In equation 11 and 13, when J' objective, the square error, is used to optimize for \gamma and \lambda, the author should consider that minimizing J' also minimizes the variance of g which means that the suggested \gamma and \lambda also contribute to minimizing the variance of g which is not the desire of the proposed optimization  \citet{Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path}  2) Despite the interesting idea, the empirical results do not provide much beyond the modest performance of impala.  