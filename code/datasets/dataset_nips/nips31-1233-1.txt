This paper presents an approach for learning to navigate in cities without a map. Their approach leverages existing deep reinforcement learning approaches to learn to navigate to goals using first-person view images, in which the goals are defined using local landmarks. The authors propose a neural network architecture, in which the goal is processed separately. Extensive experiments are conducted on real-world Google streetview datasets, showing the approach works and which aspects of the approach are important.  First, I want to be explicit: (1) I reviewed this paper for ICML 2018 (2) I now know the authors due to a widely publicized blog post  Overall, this paper is well written, clear, and has extensive experiments. Compared to the ICML submission, the contributions are well-grounded with respect to related work, and accurately show that this paper is the (non-trivial) combination of prior works.  My main critique is the lack of novelty. However, given the strong motivation, clear writing, and extensive experiments, I think this paper could spearhead a good direction for learning for navigation. However, re-implementing this paper would be a huge burden that would stop many from even trying. Although the authors state they will release the code before the conference, it has been over 3 months since the ICML submission and no code has been released.  Therefore I’ll say the following: if the authors anonymously publish their code to a private github repo with instructions on how to download and run one of their experiments, I will change my overall score to an 8 and will argue strongly for acceptance.  Minor comments: (1) typo line 202 (2) Regarding section 4.2, it is correct that while the courier task is similar to Montezuma’s in that both are sparse reward, it is easy to define a curriculum for the courier task and not Montezuma’s. However, the discussion lacks the following subtlety: if you can define a curriculum, you can also define a shaped reward; a trivial example would be rewards for entering a new curriculum. Therefore the motivation for having sparse rewards for the courier task is slightly artificial. This discussion is probably too nuanced for the paper. 