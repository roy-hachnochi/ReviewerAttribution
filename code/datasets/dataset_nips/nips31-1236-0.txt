This paper studies Bayesian private learning, a new learning problem where the learner can make queries to identify the target, and tries to hide the learned target so that the queries (without responses) leak little information. The paper focus on the Bayesian setting, where the target concept is chosen at random, and the accuracy and privacy guarantees are measured with the randomness of the learner/adversary/target concept. It gives almost matching upper and lower bounds on the query complexity in this setting (Theorem 2.1).  Overall, this is a nice paper that studies a new query learning setting, and its technical results appear to be solid. Some comments:  1. Only the threshold learning problem is studied in this paper. It would be much more interesting to consider other concept classes, for example, learning intervals, linear classifiers, etc. It seems that if the concept class has some invariance (in terms of shifting, rotation), then the replicated bisection strategy may be generalizable.  2. In Definition 1.3, can we replace the two privacy parameters (delta, L) with only one? For example, define phi to be delta-private if for any adversary estimator \hat{X}^a, E|\hat{X}^a - X^*| >= \delta.   3. If I understand correctly, this paper only focuses on the case where X^*'s prior is uniform over [0,1]. What happens if one changes the prior in this setting - does the problem become easier or harder? It would be interesting to establish prior-dependent query complexity bounds in this setting.   Edit after reading author feedback: This paper has a lot of potential to be extended to more complex query learning settings (e.g. continuum-armed bandits). On the other hand, I agree with Reviewer 5 that the setting studied in this paper is a bit too artificial (only noiseless binary search is considered).  