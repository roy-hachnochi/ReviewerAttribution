The authors present a solid and exciting continuation of decades long work into reinforcement learning using world modeling. The authors use a VAE to learn an efficient compression of the visual space (for racing and DOOM) without any exposure to reward signals. An RNN is then trained to predict the probabilistic distribution of the next compressed visual frame, while a small controller is trained to use both of these other networks to determine actions in the environment. Interestingly, when the world model is forced to tackle with uncertainty, the controller is able to learn a robust policy without any interaction with the true environment.  The interactive demo website provided with the submission is one of the most unique, enjoyable, and informative presentations of a machine learning result I have ever encountered. All papers should be presented in this way!  Major Comments: Have you compared CMA-ES to more standard ES with many more parameters? Do they yield similar performance?  An MDN-RNN is used to increase the apparent stochasticity of the environment, which can be further manipulated through the temperature parameter, to avoid cheating. Do you think this construction is necessary, or could similar performance be achieved by simply adding gaussian noise to a deterministic RNN?  I’m curious whether it’s necessary to train the RNN at all. Would a cleverly initialized RNN, i.e. to maintain a history of its inputs at a useful timescale, provide a sufficient richness of history for the controller?