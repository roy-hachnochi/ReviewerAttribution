Building the "word model" that predicts the future and helps take actions is very important and has been discussed for years in reinforcement learning as model-based RL. This paper combines several key concepts from a series of papers from 1990–2015 on RNN-based world models and controllers, with more recent tools from probabilistic modeling, and present a simplified approach to test some of those key concepts in modern RL environments. This paper also explores fully replacing an actual RL environment with a generated one, training the agent’s controller C only inside of the environment generated by its own internal world model M, and transfer this policy back into the actual environment.  Question to novelty: using VAE to extract features, and using RNN or memory to form a policy are both proposed before in previous works.   Details: -- Training the variational autoencoder (V), the RNN model (M) and the controller (C) separately makes the analysis easier. It also enables the model to learn policy only based its own prediction from M. What if these modules are optimized jointly? -- In algorithm1, the rollouts are initially collected randomly. Will this be difficult to apply to complicated environments where random policy will always fail (so that training M will be difficult)