This is a very interesting piece of work, challenging a preconception that a lot of deep learning practitioners have. The authors show that batch norm has very little effect on "internal covariate shift". They postulate that its benefit is due to a smoothing effect on the optimisation landscape.  As far as I am aware, there is very little work exploring the underlying mechanisms of batch norm. The paper is easy to follow, and well written with a clear narrative. Having a theoretical analysis alongside an empirical one is encouraging.  A few comments:  - 3D plots (in figures 1 and 2) are fairly difficult to interpret (beyond observing that the width changes)  - What explicitly is the loss gradient in figure 4? Is it simply the vector of dLoss/dWeights for the network?  - Have you observed the effect of batch norm on more modern networks (e.g. Resnets/Densenets?)  ------------------------------ Post-rebuttal ------------------------------  Having read the rebuttal, and the other reviews I am happy with my verdict (7)