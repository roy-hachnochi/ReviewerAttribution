Summary  The authors present a new justification of the effectiveness of Batch Normalization. They show empirically that what was previously believed to be the main advantage of Batch Normalization, i.e. reducing Internal Covariate Shifts, is actually not something that is modified by this reparametrization and may even be amplified by it. In light of this, they present a new analysis using lipschitz smoothness and how it stabilises both loss landspace and  gradients leading to a better (or easier) optimization.  This work is very much needed for a better understanding of the underlying effects of batch normalization on optimization and generatlization and it is a natural extension of the recent literature of theoretical deep learning.  Quality:  The empirical proof on the internal covariate shift is too qualitative. The  Figure 1 gives at best a rough idea of what is going on but there is no quantititave measure of the rate of change of the moments of the distribution for instance. Subsection 2.2 is a good attempt however at quantifying the  shift.   The analysis of section 3 falls somehow short. A strong relation between the  lipschitz smoothness and optimization should be explicitly given in the paper.  The empricial analysis is not broad enough in my opinion. As listed in this paper, there is a plethora of alternatives to batch-normalization that have been proposed in the past years. Since they all have their up and downs, comparing them on the metrics of ICS as proposed in this paper or lipschitz smoothness would most probably be enligthning.  Clarity:  The paper is well written and concise. The storyline is clear and easy to follow.   Originality:  As far as I know this is the first analysis to shed light on the stability and smoothness effect of batch normalization as its most important feature for helping optimization.   Significance:  This paper is important for two reasons. 1) Batch Normalization is a widely used technique, yet its important underlying properties are not very well understood. 2) Since Batch Normalization seems to help with both optimization and generatlization, a better understanding of it could give fundamental insights for the subfield of theoretical deep learning.