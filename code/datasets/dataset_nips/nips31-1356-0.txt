This paper deals with the problem of off-policy policy evaluation in MDPs. The paper seems to extend the prior work on the contextual bandit to the multi-step sequential decision-making setting, where the algorithm focuses on learning representation for MDPs by minimizing the upper bound of MSE. The upper bound considers balancing the distribution of state representation when factual and counterfactual action history are given respectively. They provide a finite sample generalization error, and experimental results show that their method outperforms the baselines on Cartpole and Sepsis domains.  I enjoyed reading the paper. I think it solves an important problem in an elegant way and does not leave many questions unanswered. - Can the proposed method be extended to the setting when the evaluation policy is stochastic, or to off-policy control setting? - How \mathfrak{R}(M_\phi) of the fourth term in Eq. (10) was defined specifically for the experiments?  - Cartpole used in the experiment seems to be a deterministic environment. Does RepBM also work well in the stochastic environment? 