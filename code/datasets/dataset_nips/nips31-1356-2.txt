The paper proposes a new approach to handling off-policy learning that combines a form of model learning with a new objective function. Minimizing the objective function is show to result in lower variance estimates versus other typical approaches such as importance sampling.  Overall the paper was pretty good, but a little hard to follow as it feels written more in the style of papers trying to estimate treatment effects than in the style of typical RL papers. Further along these lines, there was not distinctly specified algorithm in the paper. This does not imply that one could not be inferred from the math, but it is something of an obstacle for readers who approach problems more algorithmically.   Some assumptions seem a bit strong, e.g., that phi is twice differentiable. It would be nice to see some more discussion of that.  The presentation is a bit confusing because the paper references things in the supplemental material as if they were part of the main paper. For example, there is a reference to lemma, but this doesn’t appear in the main body of the paper. It would be helpful to make the main paper more self contained with explicit and clear references to things in supplemental materials where approach. As it is, it reads as a paper that has been roughly ripped into the two pieces without any clean up afterwards.  The empirical evaluation involved two domains, one that is very familiar and easy - inverted pendulum - and another that is less familiar and probably quite difficult. It might have been nice to see an intermediate problem that would help give more insight into what is going on. The experimental results which are presented are quite good and encouraging.  Overall, think this is a decent paper with a few rough spots.  Minor: “for an specific” -> “for a specific”