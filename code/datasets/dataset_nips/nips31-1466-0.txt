This paper addresses the topic of transfering knowledge from a teacher to a student network, which is a slimmer network, in order to preserve as much as the accuracy as possible.  The new technique proposes to use "factors" that are extracted/encoded from the features maps in the last group of the teacher network. These factors are trained in an unsupervised method using reconstruction loss. The network that is trained for this encoding is called the paraphraser network (paraphrases the "knowledge" of the teacher network). In addition, on the student side, there is a translator network whose purpose is to translate the feature maps of the student network to mimic the factors of the teacher network. As such, the student network is trained end-to-end and the loss function has two components: 1) a compoennt corresponding to the task for which the network is trained (e.g., image classification) and 2) a component that corresponds to the difference between the paraphrased teacher factors and translated student factors. This way, the student network is trained not only on achieving good performance for the task at hand but also to mimic the knowledge of the teacher network.  The paper presents a series of comprehenisve results studying the performance of several teacher-student networks for CIFAR-10, CIFAR-100 and Imagenet. In addition to their own technique, they compare it to attention transfer and knowledge distillation. Overall the technique shows better results than just training the student network from scratch. There is one case where the student network trained with the "factor" technique slightly overperforms the teacher network (and consistently so even on the parameter tuning results that are shown).  The paper also shows results for object detection. While these results are mixed, it shows that there is probably merit in the "factor" teaching technique beyond image classification.  I think the idea of extracting factors from the teacher network and teaching the student network to mimic these factors and training both for factor mimicking and task accuracy at the same time are nice ideas. The results section is a bit tedious at points, but looks comprehensive.  242-245 - I didn't understand the lines here. Please rephrase.  ----------------- UPDATE after rebuttal: It would be good to include in the paper the longer discussion on differences with FitNets provided in the rebuttal. 