This paper presents a method for unsupervised learning of shape and pose with differentiable point clouds. During training, it uses multiple views of the same object as input, and estimate the camera poses and the 3d point cloud by minimizing the reprojection loss between the multiple input images. The key novelty is the differentiable point cloud rendering (reprojection) operation (Fig.2), which is similar to operations in CVPR 2018 SplatNet.   Overall, the differentiable point cloud rendering is interesting, which potential applications to many other vision tasks. The experimental results are promising.   A few questions.  1. If camera pose is unknown, even with multiple images, there is inherently ambiguity for 3D reconstruction. For example, from two views, we can obtain perspective reconstruction at best. When more information is present, for example, camera is calibration, we can then obtain affine reconstruction and metric reconstruction. Given this, I want to know how the paper handles such reconstruction ambiguity from a single input image?   2. What is the benefit of using a point cloud as the 3D representation, compared to 3D meshes, for example? There is a recent CVPR 2018 paper on nenural mesh rendering? Will that be used for the same task? If so, what is the pro and con?   3. The reprojection loss heavily depends on the background. What the algorithm will do, if the input images are real images with cluttered background?   