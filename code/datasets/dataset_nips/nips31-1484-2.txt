This paper presents an approach for estimating value functions of a given policy in a stochastic continuous-time setting. The approach is based on the theory of RKHS. Specifically, the authors construct an invertible map U between the instantaneous reward function and the underlying RKHS that the value function lives in. The authors then proposed to learn the instantaneous reward function via supervised learning, and then pass the estimated reward function through U^{-1} to obtain the estimated value function. The authors provide a theoretically sound construction of this map.  While I have some concerns about the applicability and usefulness of the authors proposed method in practice, the theoretical construction given is interesting and hence I believe this paper should be accepted.  Below are my comments:  1. What is meant in the introduction when stated that "CT formulations are also advantageous for control-theoretic analyses such as stability and forward invariance"? Both these notions of stability and invariance make sense in discrete time as well.  2. In practice, even when the system operates in continuous time, one typically implements the controller digitally. Hence, one still need to deal with discretization effects; one can only for instance apply a constant input throughout a small time step (e.g. a zero-order hold). It is not obvious to me that designing a continuous time controller, and then implementing its discretized version, is more or less preferable to directly designing a discrete controller.  3. Please state what exactly is assumed to be known to the algorithm and what is unknown. For instance, in equation (1), is the drift function h and the diffusion function eta known? I believe it must be known, because the mapping U(.) contains these terms. On the other hand, I believe that the cost function R(.) is assumed to be not known, and learned from data.  A more common setting in RL is where one assumes that the description of the dynamics is *not* available to the algorithm, and one must learn in this setting. For instance, the GPTD paper of Engel et al. works in a model free manner.  4. Let us say one knows the reward function (a reasonable assumption in practice). Does Theorem 1 then imply one can compute a solution to the HJB equation (5) by simply evaluating U^{-1}(R(.))? Can you show with a simple calculation what happens when the dynamics and cost is given by something where the analytical solution is know, e.g. LQR?  In LQR, we know that both the R(.) and the value function are quadratics. Having this calculation would be very illuminating in my opinion.  5. In Theorem 1, what happens if one has a policy that has multiple equilibrium points, e.g. different values of starting points x in X can convergence to different points? A unique equilibrium point seems like a possibly restriction assumption for non-linear systems.  6. In Figure 3, can you comment on why your method CTGP appears to have a lot more variance than the existing GPTD method? Also, how is the policy initialized and updated for the inverted pendulum experiment?