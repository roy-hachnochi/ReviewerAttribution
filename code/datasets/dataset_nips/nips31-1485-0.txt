The main idea is incorporating Nesterov's accelerated gradient descent (AGD) in eigenvalue problem. The approach relies on shift-and-invert preconditioning method that reduces the non-convex objective of Rayleigh quotient to a sequence of convex programs. Shift-and-invert preconditioning improves the convergence dependency of the gradient method to the eigengap of the given matrix. The focus of this paper is using AGD method to approximately solve the convex programs and reaching an accelerated convergence rate for the convex part. Exploiting the accelerated convergence of AGD, they reach an accelerated convergence for the first-order optimization of the eigenvalue problem.  My main concern is regarding the novelty of the paper.  There is an extensive literature on accelerating power iteration method that is totally neglected in this paper. For example, "accelerated stochastic power iteration" by CHRISTOPHER DE SA et al. is a recent one. Chebyshev's acceleration in the classic literature of optimization of Rayleigh quotient is also quite related to this paper (see "convergence rate estimates for iterative methods for a mesh symmetric eigenvalue problem" by Knyazev). Indeed, gradient descent on eigenvalue problem can directly be accelerated without the "shift-and-invert" convex relaxation. The accelerated power iteration does not have the extra logarithmic dependency to the eigengap.     In section 3, the authors are motivating the advantage of their approach in the case of the multiplicity of the largest eigenvalues. However, I think that this case is not considered in the literature due to the degeneracy. As it is mentioned in the introduction, eigenvector problem search for the leading eigenvector. Since the leading eigenvector is not unique in the multiplicity case, this case is skipped in literature. In such a degenerate case, one can still provide a convergence on the eigenvalue (function f(x)) but the leading eigenvector is not well-defined.   The required conditions on initial parameter x_0 are missed in the theorem as well as the algorithm. It is clear that if x_0 is orthogonal to the subspace V, then the algorithm can not retrieve the desired global minimum.    To make the algorithm clearer, I suggest making the terms such that approximate gradient more precise. I think that the number of AGD iterations in step 3 can be determined by their analysis, so it might be better to include this number in the Algorithm as well.   Since the shift parameter sigma is unknown, I think they should include the time complexity of searching for this parameter and analyze the robustness of the method against the choice of sigma.   Regarding experiments, I think that one baseline on real datasets is not enough. Particularly, eigenvector problem is one of the oldest optimization problems with many solvers. For example, accelerated power methods can be a potential baseline for their proposal.   ------------------------------------------------------- I have read the response. Authors had convincing responses and experiements regarding the literature and baselines, hence I decided to increase my score by one. Still, I think that handling the degenerated case lambda_1 = lambda_2 is not a major contribution.  