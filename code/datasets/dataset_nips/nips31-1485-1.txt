The paper proposes a complex combination of: -- shift-and-inverted power method [Garber and Hazan, 2015] -- shift-and-invert pre-conditioning using thus obtained shift parameter -- gradient descent  for leading eigenvector computations. The authors try to prove the method can match the convergence rate of Lanczos. This relies on a number of parameters incl. a lower estimate of the eigengap and two iteration numbers (cf. line 60), and a stopping criterion for the first step, presumably Lipschitz constant estimates  for the third (or a method for estimating it) or some additional parameters of the accelerated algorithm, etc.  This improves considerably upon [1] in terms of the rate of convergence,  which the authors stress throughout, but does not present either a major  advance over [2]. It also does not seem to be a particularly practical  method, due to the number of parameters needed; that is: while Figures 1 and 2 present very nice results, esp. on the synthetic dataset, it is very hard to know how much effort has gone into tuning those parameters  on the instances or whether one could use one set parameters across  instances of many different sizes.  [1] Ohad Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In 303 Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 304 6-11 July 2015, pages 144â€“152, 2015. [2] Jialei Wang, Weiran Wang, Dan Garber, and Nathan Srebro. Efficient coordinate-wise leading 309 eigenvector computation. Journal of Machine Learning Research (2017) 1-25.