**Update after rebuttal:** I would like to thank the authors for the responses to my questions. I would like have liked to see more discussion of the details on the P(Z|Y) part they mentioned in the response but I appreciate that this is impossible due to length discussion. I still have reservations about the approach but nevertheless authors have thoroughly investigated alternatives and made a well-reasoned choice on the basis of those considerations. Having read the response and the paper again, and as a vote of confidence in the proposed added discussions about the independence of Z and Y, I have increased my score.  The authors propose a new neural network architecture and a corresponding training scheme to solve the problem of one-shot generative modeling. It works by encoding the difference between pairs of images in a bottleneck layer, and then combining this difference information with one of the images it reconstructs the other image. At test time, the architecture can be used to generate further examples of a class from a prototype example.The authors use this model to solve one-shot classification, by first generating additional examples of the given class prototypes, and then training a full classifier on the generated examples.  Comment about the overall approach: * My main criticism of this approach is the mismatch of what this architecture is designed to do and how it is ultimately used. * This approach appears to solve a significantly harder problem (one-shot generation) in order to solve an easier problem: one-shot classification. * As a result of this mismatch, the overall method is complicated and it is not and cannot be end-to-end tuned to the task at hand. * Furthermore it is unclear how improving or modifying the training procedure for the delta encoder would effect performance on the classification task. Questions/comments about the Delta-encoder architecture: * In Figure 2 right-hand panel, X_z and Y_s are only needed to create a sample of Z so that it’s from the same distribution as the distribution of Zs during training. Is this correct? Could one regularize the distribution of Z to take a certain shape (say via a KL-regularizer like in beta-VAE) so that the encoder is no longer needed at test time?  * [major question] At training time, the two inputs to the decoder Z and Y_s are not statistically independent, yet at test time Z and Y_u are. I’m not sure how significant this is but to me this seems to create a particular mismatch between how we train and how we use the model. Would it be possible to devise an experiment to test how much of a problem this is? Is the distribution of Z class-independent, for example? If we feed the encoder a random X, and then feed the encoder and decoder (A) the same Y from a seen class, vs (B) different Ys from the same seen class, how does the distribution of reconstructed \hat{X} change? In particular, would the top layers of the VGG network still classify these correctly? * What, other than the relatively small size of the bottleneck layer, ensures that the model can’t simply store enough information in Z to restore X without ever using any information about Y at all? Have the authors considered adding an extra regularizer term which simultaneously tries to minimise the information stored in Z about X somehow? Questions about experiments *  [major question] The method uses precomputed VGG19 and ResNet features (line 158). Presumably these have been pre-trained on some particular datasets, like ImageNet or Cifar. Presumably this pre-training included classes which are ‘unseen’ classes in the one-shot setting. If this is the case I would say that the method can derive unfair advantage from this as it already knows how to represent the unseen classes. Could the authors comment on this? * In section 3.1 the authors comment on wallclock time but it is unclear to me if these figures include training the classifier once the samples are generated from the decoder? * Otherwise the results seem strong. I am unfamiliar with the accuracy numbers one should expect in this particular application but I have double checked a couple references and the numbers reported here seem to match the numbers published at the referenced papers.  In summary, I think this is a well executed and well written paper, I welcome the discussion of other architectures considered and the thorough comparisons with other methods. I am disappointed by the complexity of the solution and particularly the mismatch between what the paper sets out to solve (classification) and the eventual solution (which involves generation). In my view this limits the scope of impact this solution may ultimately have on the field of one-shot classification. This is the main reason I did not give it a higher score.  I indicated that my confidence level as 4 as I am unfamiliar with relevant work producing SOTA results in these applications so I could not judge the strength of the reported results with 100% certainty. I also had a few questions about VGG and ResNet pretraining which may alter my assessment.  Authors, if limited on space, please focuson on addressing questions marked [major question]. Thanks!