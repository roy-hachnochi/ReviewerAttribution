The paper analyzes a stochastic algorithm which builds on the  cubic-regularized Newton method. The algorithm uses gradient and Hessian-vector product evaluations and improves over existing techniques in escaping sadlles and finding local minima. Experiments are run on both synthetic and a real-world problem, and compared with SGD.  The authors provide   theoretical guarantees.   This is a solid incremental paper, which is  worth accepting (in my opinion). Other manuscripts provide almost equivalent results.  After feedback:  the authors appear to fully understand and agree on the need of  further clarifications on the  numerics:  1) the comparison  to  (at least) one other variance-reduction method will be provided 2) the hyperparameter tuning process will be critically displayed  Moreover, the authors are going to add information on the comparison with competing results. Overall, the author appear to agree with all reviewers on the potential weakness of the paper and their reply is fully satisfactory.   The numerical experiments corroborate  nicely the overall framework. The rigorous guarantees remain the strong strand of the paper. 