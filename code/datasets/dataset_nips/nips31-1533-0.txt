This submission proposes a method to learn dense reward functions for hard exploration games with sparse rewards using videos of humans playing the game from Youtube. The domain gap between YouTube videos and the simulator is closed using a set of novel self-supervised classification tasks such as temporal distance classification and cross-modal distance classification to learn a common representation. The reward function learned by the proposed method can be used to train RL agents which receive much higher rewards on 3 hard exploration games as compared to agents trained with the sparse environment reward.   Strengths: - The paper is very well written and easy to follow. - The problem setup has a lot of practical applications. There are plenty of videos available on the web for a lot of useful tasks. Learning from videos is much more useful than learning from data recorded directly from simulators under artificial conditions. The method doesn't require frame-by-frame alignment or actions at each observation. - The method achieves super-human performance in 3 hard exploration Atari games. - The method outperforms several strong baselines. - The method utilizes the audio channel in videos in an interesting and effective manner. - The visualizations of both embedding spaces and convolutional activations provide informative qualitative analysis.  Questions - The authors select 4 YouTube videos of human gameplay for each game. How were these videos selected? Randomly or based on performance, visual similarity, temporal alignment or a combination of these? - What was the average performance of these 4 videos? What was the performance of the video used to learn the imitation reward? - Is the implementation code going to be open-sourced?  UPDATE: The authors have answered my questions about the scores and selection of the demonstration videos in the response. I encourage the authors to add these details to the manuscript and open-source their implementation and dataset.