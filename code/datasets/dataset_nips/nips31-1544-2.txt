This paper is about learning optimal treatment rules with a continuous treatment, in the potential outcomes setup with ignorability. There is a considerable amount of literature on the two-arm version of this problem; however, the present setup (with a continuous treatment choice) has been less explored.  Call mu(x, z) the expected cost of deploying decision z for a sample with covariates x. A simple approach would be to to estimate \hat{mu}(x, z), and then choose \hat{z}(x) to optimize the estimated function. A problem with this approach, however, is that if the rewards from some sub-optimal actions are hard to estimate, we might accidentally pick a very bad action.  The proposal of the paper is to penalize actions z(x) for which \hat{mu}(x, z) is very variable or biased and, in case of uncertainty, to prefer actions whose rewards can be accurately estimated. The authors provide both theoretical and empirical backing for this method. Conceptually, the paper builds on work from Maurer & Pontil and Swaminathan & Joachims on uncertainty penalization; the setup here is different in that the data-collection policy may not be known and the action space may be continuous.  Overall, this method is principled, and may work well in many areas. The paper is also very well written. One advantage in particular is that, in many situations, we might have most data from the “status quo” policy, and this method will correctly preserve the status quo unless there is strong evidence that another policy is better.