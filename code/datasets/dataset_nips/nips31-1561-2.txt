The authors analyze optimizing a sample function from a Brownian motion in [0,1] with a as few "active" samples as possible. They show that they can achieve an epsilon-approximation by  querying the function at log(1/epsilon) points. The latter is an improvement over prior work by Calvin et al, which shows lower than polynomial sample complexity but not logarithmic.   Their main result uses a notion of near-optimality dimension proposed by Munos'11 which captures how flat is the function around its optimum: how many disjoint balls can be fit in the region of x's that are epsilon minimizers. Their main finding is to bound the near optimality dimension of a sample of a Brownian motion with high probability. Given this finding their algorithm is pretty simple: simply keep an upper confidence value for each partition of the [0,1] interval created by the points queried so far. Then query the center of the partition with higher UCB value.   I think the result is interesting on the near-optimality dimension of a Brownian motion of independent interest.    