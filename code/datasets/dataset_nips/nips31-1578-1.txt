This work presents an approach to learning deep neural network architectures by drawing on ideas from Bayesian network structure learning. Layers of the network are established using nth order conditional independencies, and connections may appear between non-adjacent layers.  This is an interesting idea that may, in addition to the efficiency arguments made by the authors, have benefits in terms of interpretability and identifiability of a network. The empirical results are satisfying in terms of the overall reduction in the number of parameters while maintaining predictive performance. The work is mostly well-presented but there are some details lacking; e.g. I cannot find the details of the independence test that is used (perhaps this is standard for those in the area, but I am not so familiar with it).  I am unable to comment with great confidence on the novelty of the work. The authors do discuss related work, which suggests that at least part of the novelty of this approach is that it is unsupervised, while existing approaches are supervised. I am not familiar enough with the literature in this specific area to verify that claim. It does appear that it would quite cheap computationally, however.