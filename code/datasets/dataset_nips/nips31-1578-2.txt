The paper presents neural structure learning as a Bayesian structure learning approach. It first learns a DAG generative graph over the input distribution with latent variables in an unsupervised manner. Then it computes a stochastic inverse of the learned graph and finally converts it into a discriminative graph(adds label). Each latent variable in the learned structure is replaced by an MLP. The learned model is trained for the task.   Several Concerns:  1. It is an interesting approach to model structure as a Bayesian network, however the advantages over baselines are not unclear since the experiments only show the structure learning of last layers.  This makes the method rather a parameter reduction technique than structure learning. Why can't the experiments show an entire structure learned from scratch?  2. The current experiments use features extracted from pre-trained networks, not completely unsupervised.   3. The scalability in comparison to Neural Architecture Search (NAS) claim isn't supported by experiments; not accounting for the fact that NAS learns entire structure in comparison to only the deepest layers from a trained network in the experiments.   4. Can only be applied to the deepest layers for y label? 