The authors consider a problem they call "dynamic assortment", where a learner selects a subset of items S_t amongst a ground set of items [N], and presents it to a customer. The customer chooses item i in S_t with probability proportional to v_i (an unknown parameter), and the learner receives a reward of r_i. The authors present an algorithm and prove guarantees on its regret (in particular that it is independent of the number of items).  The paper is very well written. While the algorithm is mostly a combination of well known techniques in the continuous bandit literature (bisections, trisections etc.), the authors resort to some nice tricks to reduce regret.  - In section 3, the authors prove that, in order to solve the problem, it is sufficient to solve a one dimensional problem on  [0,1], where one must maximize a piecewise constant function (or in fact find its fixed point). This problem has been addressed in https://arxiv.org/abs/1604.01999, (both in the adversarial and stochastic context), so that the authors could compare their results to this. - The trick used to reduce the regret from \sqrt{T \ln T} to \sqrt{T \ln \ln T} is very nice, and it begs the question whether or not this can be applied to other "generalized" trisection algorithms such as those studied by https://arxiv.org/abs/1107.1744 , https://arxiv.org/abs/1406.7447 and http://www.icml-2011.org/papers/50_icmlpaper.pdf - as a minor remark, in equation (1), \min should be \max (the optimal arm MAXIMIZES the revenue)