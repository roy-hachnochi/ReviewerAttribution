The paper provides a finite sample analysis of a reinforcement learning algorithm for continuous state spaces. Given a set of balls covering the compact state space, the RL algorithm learns and stores the optimal Q function for all ball center-action pairs. The Q function is generalized over the whole state-action space using nearest neighbor(s) regression. The main result of the paper is that under some assumptions on the reward and transition, such that Q* is Lipschitz continuous, the proposed algorithm will return a Q function epsilon close to Q* (in sup-norm) after T transition, with T linear in the covering time (expected time for the sampling policy to visit every ball-action pair at least once) and cubic in 1/epsilon.  I did not go through all the proofs in the appendix. Otherwise, the paper is well written and very informative. The only limitation I see is that the analyzed algorithm might not be of practical use because of the discretization of the state space and the Lipschitz assumption. The Lipschitz assumption of the reward function might be penalizing since several problems have non continuous reward functions (e.g. positive reward whenever angle is less than some small value); similarly assumption made to the transition function will not work in e.g. robotics whenever there is contact. These assumptions on the reward and transition are not present in related work cited in the paper ([41, 27]), on the other side these papers do not provide a finite sample analysis. As far as I know this constitutes a clear improvement over the state of the art.