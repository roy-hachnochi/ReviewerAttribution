The authors propose an algorithm for constructing deep neural networks adapted to the complexity of a binary classification problem. As a Bayesian probabilistic model is used for each layer, its width can be learned from the examples using Gibbs sampling or maximum a posteriori inference. New layers are added greedily until the two classes are well separated. Results show that this method achieves similar error rates as state-of-the-art classification algorithms, but the complexity of the constructed network indicated by the number of hidden units is often significantly smaller.  The paper is well written. It explains the proposed method for constructing the network clearly and discusses the results at length. The infinite support hyper-plane machine (iSHM), which is used as layer here, is an interesting and novel application of non-parametric Bayesian inference, which enables learning of the network structure directly without cross-validation. This property is a big advantage compared to conventional deep neural networks and consequently the parsimonious Bayesian deep network may find a wide use as an automatically adapting classification algorithm.  I have read the feedback provided by the authors.