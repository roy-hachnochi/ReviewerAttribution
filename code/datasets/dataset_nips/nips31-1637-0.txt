Summary:  This paper considers the problem of tree-based search by leveraging a given (learned or hand-designed) policy to solve goal-based search problems. The paper proposes two different search algorithms with varying strengths and weakness, and provides guarantees on the number of nodes to be expanded before reaching a goal state. First, LevineTS based on Levine Search performs best-first search enumeration: penalizes with depth of a node and performs state cuts using the policy. Second, LubyTS is based on existing work on scheduling for randomized algorithms. It takes advantage of large number of candidate solutions (goal states) in the form of known maximum depth at which these solutions are present. It samples trajectories of this length using the policy until solution is found. Experiments are performed on Sokoban domain using a neural network policy learned using a reinforcement learning algorithm and comparison is performed with a domain-independent planner with good results.  Pros:  - Very well-written paper. - The proposed tree search algorithms and guarantees on expected search time (in terms of expanded nodes) to reach goal state. - Technically solid work.  Cons:  - The practical significance of this problem and tree search algorithms is not clear. - Lack of experimental results on more practical real-world applications of this problem setting and algorithms.   Detailed Comments:  1. Section 2 introduces too much notation that I don't think is necessary to explain the problem setting and algorithms. These are very simple search concepts.  2. I would have at least liked to see some potential real-world use-cases of the problem setting considering this paper is submitted to a NIPS conference.  3. Experimental results validate the algorithms and guarantees, but it would have been nice to see more empirical analysis on real-world use cases.