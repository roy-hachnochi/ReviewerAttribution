The paper proposes a novel framework to consistently evaluate semi-supervised learning (SSL) algorithms. The authors pose a number of interesting questions, e.g. what happens when labeled and unlabeled data come from different distributions, or what happens when only small validation sets are available. Experimental findings show that SSL evaluation might need some re-thinking, making the contribution of the paper significant. Once released, the evaluation platform will foster further research.  Note that I'm not very familiar with the literature on SSL, so I'm not sure that the set of methods considered in Sec. 3 is complete.  The clarity of the paper is satisfactory. Sentences at l. 218-219, pg. 4, could be made clearer (how was the set of hyperparameters hand-designed?). In some plots (Fig. 2, Fig. 5), shaded regions are not-so-well visible.  A few observations/questions: - It would be interesting to discuss real-world scenarios in which the class distribution mismatch between labeled/unlabeled data is as high as the one considered in Sec. 4.4. - Do the authors think that the choices they made in terms of architecture/optimization technique for the platform can have any particular impact on the exposed results (is it expected that different architectures /algorithms would lead to the same conclusions / did the authors explore different choices?)? - The authors focus on image classification tasks. Do they have any insights on how their findings would apply to different tasks?   Minor things: - Pg. 4, line 177, typo: applied