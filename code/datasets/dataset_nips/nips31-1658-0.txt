The submission "Training deep learning denoisers without ground truth data" transfers methods from risk estimation to the setting of convolutional neural networks for denoising. The usual minimization of the l2-loss between ground truth and training data is replaced by the minimization over an unbiased estimator over training data, sampled in a Monte-Carlo fashion. The submission highlights how previous techniques for unbiased parameter estimation can be translated into the CNNs and shows very intriguing results, training without ground truth data.  A missing aspect that has to be addressed is the existence of minimizer of the SURE estimator (equation (13)) - it is easy to contruct simple (e.g. linear) networks and corresponding training data such that no minimizer \theta of (13) exists. The function value is not necessarily bounded from below, and the infimum over (13) becomes minus infinity. How can such cases be excluded, either by assumptions on the data / the number of free parameters, or by additional regularization on theta?  As minor comments  - I think it would be fair to state that the interesting idea of including test data during training also increases the inference speed significantly (e.g. to the 15 hours mentioned). - Along the lines of including the test data during training, the authors could train on as few as a single test image, yielding an interesting connection to the recent work on "Deep Image Priors" by Ulyanov et al. - Possibly related prior work on using the SURE estimator for denoising can be found, e.g., in Donoho and Johnstone, "Adapting to unknown smoothness via Wavelet Shrinkage" or Zhang and Desai, "Adaptive Denoising based on SURE risk".   - It should be noted, that although  equation (13) is an unbiased risk estimator for the MSE loss for fixed parameters theta, the minimimum over theta is not necessarily an unbiased estimation of the MSE loss of the estimator parametrized by this minimizer, see, e.g., section 1.3 in Tibsharani and Rosset's "Excess Optimism: How Biased is the Apparent Error of an Estimator Tuned by SURE?". - Please have your submission proof-read for English style and grammar issues with a particular focus on missing articles.  - I am wondering if completing the square in equation (14), ariving at a term \|y^{(j)} + \frac{\sigma^2}{\epsilon} \tilde{n}^{(j)} - h(y^{(j)}; \theta)\|^2 and one remaining linear term provides an intering point of view. What happens if one just trains with the above quadratic term? This is just a question out of curiosity and does not have to be addressed.      In summary, the work is interesting and the ideas are clearly explained. The experiments are detailed and well-formulated and show promising results. The paper is, in general, well structured and easy to read. I recommend the acceptance of this manuscript.     I appreciate the clarifications of the authors in their rebuttal. Although it is a minor technical issue (typically avoided e.g. by weight decay), I'd like to point out that the existence of minimizers is not only a question of the network architecture, but also of the training data. I agree with the other reviewers that the limitation to (fixed) Gaussian noise is severe in practice, but I'm willing to accept such a limitation as this is one of the very first papers to train denoising networks without any ground truth. Because the general idea of the paper is very interesting and the first results are promising, I keep my score and recommend the acceptance of this manuscript.