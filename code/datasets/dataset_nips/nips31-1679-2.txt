The paper reports an improvement of data-efficiency in hierarchical RL. This improvement is achieved by a novel method, which is a modification of existing methods. The modification consists of two parts: the novel method 1. is capable of off-policy learning, which in turn increases data-efficiency 2. uses the proximity to raw states instead of internal states as low level reward function. The method is tested on four variants of a grid world benchmark and compared to several other methods as well as systematic alternatives.   The paper is written very clearly and well structured. It's of very high quality. Still, there are some aspects, where clarification would help.  I think it should be made more clearly, which assumptions have to be met in order to apply the method or parts of it: - is the method (or parts of it) limited to deterministic MDPs?  I think the reader would benefit, if the authors would make it explicit, that the considered benchmark variants are deterministic.  ...  that the method makes the assumption, that introducing goal states can help in solving the problem and some kind of motivation, for doing this.  On page 2 line 77 there is a very strict statement "these tasks are unsolvable by non-HRL methods". I think this statement is simply not true in this generality. Please consider make this statement more precise, like "none of the testet non-HRL methods was able to solve the task for the investigated amount of interactions".  There is an inconsistency between the reported results of table 1 and the text: The text saying "significantly out-performed" while the table states for the "Ant Gather" benchmark "FuN cos similarity" 0.85 +/- 1.17 compared to "Our HRL" 3.02 +/- 1.49. With reported uncertainties of 1.17 and 1.49 these results are "equal, given the uncertainties". It might be a good idea to reduce the uncertainties by repeating the experiments not just 10 times, but 100 times, thus being able to make the results significant.  As the text is really well written I assume that the missing capital letters (maxq, Vime, laplacian, ...)  in the literature will be fixed for the final version.   Choosing the title is of course up to the authors, but I think that the reader would benefit from a title that is less general and more specific to the presented method, e.g. "Data-Efficient Hierarchical Reinforcement Learning in Grid World Environments", or "Data-Efficient Feudal Reinforcement Learning in Deterministic Environments"  Post Rebuttal ===========  Unfortunately the authors did not respond to the concerns, which I expressed in the review, starting with "There is an inconsistency".   This reduces the trust in the correctness of the experiments, the presentation of results, and the conclusions drawn from those. I have therefore lowered the overall score to 6, and expect that the authors either  - increase the number of trials to reduce the uncertainties, thus increasing significance, or - come to the conclusion, that the given values after the +/- are in fact the standard deviations of the ten trials, and correct this to the standard error, or - explain, how the values after the +/- sign have been derived, and on what grounds the claims for significance have been derived.   