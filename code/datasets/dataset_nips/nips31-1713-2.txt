This paper deals with the analysis of the kernel random feature scheme for linear SVM. In line with the recent work by Bach on optimal sampling scheme for random features and by Rudi and Rosasco on kernel random feature regression, the authors prove that, under certain hypotheses, learning was feasible with a number of features that is actually lower than the sample size. I thus consider this work to be important, since despite their success due to their simplicity it is still not clear in which situation using random features brings an actual theoretical advantage.  While I very much liked this paper, there is still a bit of room for improvement. First of all, the main definition of the paper, which is the optimal sampling scheme, is quite confusing. Who is mu ? Is its existence assumed or always guaranteed ? Similarly, although their meaning is clear the lambda(Sigma) have not properly been defined. I would maybe displace everything that concerns optimized sampling scheme (including the discussion on limitations and the approximate algorithm by Bach) to a dedicated subsection, since the optimal sampling scheme and the practical algorithm that goes with it constitutes for me the crucial point of the paper. Similarly, I would mention it more clearly in the intro. Secondly, I would elaborate a bit more on the role of the dimension, even though the authors already do that. In particular, while reading the paper the reader has the impression that the "sub-exponential" case is always better than the polynomial case, unless I'm wrong in high dimension this is not the case and the number of features may be worse in the second case. This should be stated clearly. Although, is it possible to give a few examples of usual Sigma and the decay of their eigenvalues after the definition of the decays?  If space permits, I would include a small sketch of proof in the body of the paper, to outline the novel parts of it.  Finally, a few typos and comments: - l94-95: I believe the variance for the RF sampling is gamma^{-2} for the gaussian kernel (inverted compared to the kernel) - l128-129: "we consider a simpler hypothesis class only by forcing..." -> not clear - l142: gaurantees - l185: "to remove" -> removing - App F: Supplimentary  In conclusion, I believe this work to be an important step forward in the still popular random feature scheme. Although the results have some limitations due to optimal sampling scheme and possible exponential dependence in the dimension that could be stated more clearly, the overall paper is easy to read and well-explained.  --- After feedback --- I thank the authors for answering most of the points I raised. I understand their choice to keep the structure of the paper as is. A strong accept.