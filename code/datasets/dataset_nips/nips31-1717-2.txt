The paper re-introduces a formalization and framework to apply machine learning to optimize Tensor operations, and evaluates it for deep learning workloads.  The formalization is familiar if one reads HPC papers on automatic code generation using heuristics. However, reiterating this formalization and fitting it to deep learning workloads such as Tensor operations is appreciated. The formalization is based on: - expression: describes the computation - schedule: maps the computation onto hardware - cost: runtime of the {expression, schedule} on the hardware   The authors' goal is to apply statistical learning methods to the problem -- in particular, Gradient Boosted Trees and TreeGRU neural networks. Because Gradient Boosted Trees need input featurization, the authors first propose extracting features on the low-level loop AST (a standard low-level AST) as the features to the GBT. Typical features such as "loop length" and "touched memory" are used. Because the authors want to learn low-parametric statistical models, they need features that generalize across loop nesting patterns, so they define these features in a relative manner (eg. in "for i for j for k for l", features have a chance of similarity across ij, jk, kl). The relative weighting is log-spaced, capturing the loop nesting spacing correctly. Using these features, the authors define and train a GBT and a TreeGRU.  Additionally, the authors see how their models perform in a transfer-learning setting. Finetuning for out-of-domain, or domain-distant samples and seeing how the performance + learning time fare.  The results are very well calibrated, with strong and robust baselines. For example, they make sure their baseline is the latest CuDNN version, TensorFlow, Tensor Comprehensions and ARMComputeLib.  The results look very strong, often outperforming the baselines, and by a significant amount.  This paper convincingly introduces modern neural network techniques (not just small MLPs) and GBTs to optimize code generation, in particular for deep learning workloads. It also introduces featurizers for the loop ASTs that are more suited for statistical models. In convincingly showing that there is feasibility in approaching these problems from a statistical parametric learning perspective (apart from the traditional Genetic optimization, polyhederal compilation, integer search), this paper will become a catalyst for the deep learning community to attack this problem space effectively. Hence, I think this paper is one of the top 50% (or even one of the top 10%) papers I've reviewed for NIPS this year.   