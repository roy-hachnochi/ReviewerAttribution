This paper provides near-optimal bounds on the sample complexity of estimating Gaussian mixture models under the total variation distance.   A novel scheme of leveraging results on compressibility to give bound on learnability is employed, allowing for an improvement of O(d) vs. existing bounds. The results are well motivated and presented, and provide an improved understanding of the complexity of learning GMMs, a fundamental classical task in ML.   Some suggestions: In terms of writing, the paper seems to peter out towards the end, with little text connecting a list of lemmas and remarks. Adding additional connective tissue and intuitive explanation here, possibly including a conclusion/discussion section, would improve readability. Several of the lemmas may be moved to the supplement to achieve this.   Edit: The authors in their response addressed my concerns, so I reiterate my recommendation.