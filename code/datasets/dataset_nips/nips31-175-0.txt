An approach for joint estimation of 3D Layout, 3D Object Detection, Camera Pose Estimation and `Holistic Scene Understanding’ (as defined in Song et al. (2015)) is proposed.  More specifically, deep nets, functional mappings (e.g., projections from 3D to 2D points) and loss functions are combined to obtain a holistic interpretation of a scene illustrated in a single RGB image.  The proposed approach is shown to outperform 3DGP (Choi et al. (2013)) and IM2CAD (Izadinia et al. (2017)) on the SUN RGB-D dataset.  Review Summary: The paper is well written and presents an intuitive approach which is illustrated to work well when compared to two baselines. For some of the tasks, e.g., 3D Layout estimation, stronger baselines exist and as a reviewer/reader I can’t assess how the proposed approach compares. Moreover, a fair comparison to IM2CAD wasn’t possible because of a different dataset split, leaving a single baseline from 2013 (5 years ago). In addition, I think many important aspects are omitted which hinders reproducibility. The latter two reasons (comparison to stronger baselines and dropping important aspects) are the reason for my rating.  Quality: The paper is well written. Clarity: Important aspects are omitted which prevent reproducibility according to my opinion. Originality: Could be improved by focusing on omitted aspects. Significance: Hard to judge given a small number of baselines.  Details: 1) Why is there an offset \Delta^I between the 2D box center an the projection of the 3D box center in Eq. (1)? This doesn’t seem intuitive. Can the authors explain?  2) I think the parameterization of the room layout remains unclear. Many papers in this direction exist and only three lines are spent to explain how eight 3D points are computed from `center, size, and orientation.’ I don’t think I could reproduce those results based on this short description.  3) Very little is mentioned about the two employed deep nets L_GGN and L_LON. E.g., are they regression losses or classification losses? How are they composed? The paper states `we adopt a hybrid of classification and regression for predicting the sizes and heading angles’ (l.135) but doesn’t provide any more information. I understand that space in the main paper is limited, but the supplementary material is a good place to discuss those aspects.  4) A fair comparison to IM2CAD wasn’t conducted because IM2CAD manually selected a subset of images from the dataset. I’m wondering whether the authors reached out to Izadinia et al. to obtain the list for the test set? A fair comparison would be more compelling such that a reader/reviewer isn’t left with a single baseline from 2013.  5) Beyond a fair comparison to IM2CAD, additional state-of-the-art baselines exist for some of the proposed tasks. E.g., RoomNet (C.-Y. Lee et al., `RoomNet: End-To-End Room Layout Estimation,’ ICCV 2017) and references therein for 3D layout estimation. Comparison to those would make the paper much more compelling since a reader is able to better assess the strength of the proposed approach on individual tasks. Note that there is no need for state-of-the-art results on all tasks, but I would want to know the differences between joint learning and specifically designed deep nets.