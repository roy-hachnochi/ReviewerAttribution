Summary:  The paper presents a connection between actor-critic algorithms (usually framed in single-agent RL) and counterfactual regret minimization (framed for multiagent learning). The results show that the counterfactual values and the standard value functions are related (scaled by a normalization constant) and that advantage values are immediate counterfactual regrets (scaled by another constant). Experiments in two poker versions show that the algorithms converge close to the Nash equulibrium in self-play and also works well against other opponents (bots).  Comments: I appreciate the thorough revision of related work, if anything I must comment on a few recent works that are relevant. Two recent surveys on agents modeling agents and dealing with non-stationarity in multiagent interactions. -S. V. Albrecht and P. Stone, “Autonomous agents modelling other agents: A comprehensive survey and open problems” -P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. Munoz de Cote, “A Survey of Learning in Multiagent Environments - Dealing with Non-Stationarity” A recent work on Deep MARL. -Z. Hong, S. Su, T. Shann, Y. Chang, and C. Lee "A Deep Policy Inference Q-Network for Multi-Agent Systems"   I would like if you can comment more on the implications of 3.2, in particular about "If there is a low probability of reaching s_t due to the environment or due to opponents' policies, these values will differ significantly"  It is not clear why two transition buffers are needed (in 4.2)  Post rebuttal: The work shows a nice connection between two areas.  I strongly recommend the authors to improve the presentation: - background (CFR and recent papers),  - contrasting with well-known algorithms (A2C),  - check carefully notation  - solve typos and broken sentences.