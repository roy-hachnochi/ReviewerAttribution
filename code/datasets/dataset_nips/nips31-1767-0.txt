This paper proves several results related to the step size (learning rate) in gradient Descent (GD). First, the paper examines linear feedforward neural nets with a quadratic loss, trained on whitened data with outputs which are a PSD matrix times the input. It is found that GD can only converge to a critical point if the learning rate it is below a certain threshold. Then, it is proven that the weights converge at a linear rate to the global minimum if they are initialized to identity, and the step size is below another threshold. The latter result is also extended to matrices with negative eigenvalues. Second, the paper examines a single hidden layer neural network with quadric loss, and finds that the weights can converge to a critical point only if the learning rate is below another ratio.  Clarity: The paper is easily readable.  Quality: I couldn't find major problems (some minor points below). Significance: These results suggest another explanation for why deeper networks are more challenging to train (since we require a lower step size for convergence), why residual connections are helpful (easier convergence to the global minima), and why the training algorithm (GD) is biased towards global minima with smaller "sharpness" (higher sharpness makes global minima more unstable) and possibly better generalization . Originality: I'm surprised this analysis hasn't been done before. I think it is common knowledge that on smooth functions we require 2/(step size)  be larger than the max eigenvalue of the Hessian for GD to converge for almost every initialization (in a related subject, if 2/(step size) is larger than the global smoothness, then this is sufficient for convergence, e.g., https://rkganti.wordpress.com/2015/08/20/unconstrained-minimization-gradient-descent-algorithm/). However, the analysis following from this fact (as well as the other results) seems novel to me.   Minor points: 1) Lyapunov instability implies that we can't to converge to fixed point *independently of initialization*, not in general (since for certain initializations, on stable manifolds, we may still converge). The authors mention this in certain places in the text, but the phrasing in the Theorems missing this distinction (e.g. in Theorem 1 we only have "the gradient descent algorithm can converge to a solution" without mention of the initialization). This makes the Theorems 1 and 4 (and Corollary 2) technically wrong, so it should be corrected. 2) line 209: "provides a case where the bound (4) is tight" seems inaccurate: note the bound in eq 4 is x2 the bound in Theorem 2 (in the right case of the max), so it's nearly tight. 3) In the numerical results part, I would mention that the bound from Theorem 4 gives \delta<3.8e-4, which is close to the range of values you found numerically (in which the destabilization occurs). So while the bound is not tight, it seems to be in the right ballpark. 4) I recommend the authors to make their code available online.  %% Edited after author feedback %% Thank you for correcting these issue. One last thing I noticed - we can replace the identity initialization with a diagonal initialization, and everything will hold, right? Maybe worth mentioning what is the largest family of initializations that will work, as this can be of practical intreset (e.g., I think that in RNNs diagonal initializations sometimes work better than identity initialization). 