Updated comments:  I carefully read the authors’ response and the paper again. I was mistaken when I read Algorithms 1 (Eq(2.3)) and 3 (Eq.(2.18)). The authors control the variance by simply averaging a batch of unbiased stochastic gradients.   =========================================================  In this paper, the authors considered the problem of zeroth-order (non-)convex stochastic optimization via conditional gradient and gradient methods. However, all the techniques are already known but none are mentioned in the paper. The authors should try to access the recent advances in this topic. In the sequel, the reviewer would like to discuss it in detail.  First, the main trick that makes Algorithm 1 works is Eq. (2.3) (and (2.18) for Algorithm 3). To the best of the reviewer’s knowledge, this technique was first applied to the conditional gradient method and stochastic optimization in the following line of work  Centralized setting:  [1] Mokhtari, Aryan, Hamed Hassani, and Amin Karbasi. "Conditional Gradient Method for Stochastic Submodular Maximization: Closing the Gap." arXiv preprint arXiv:1711.01660 (2017).  [2] Mokhtari, Aryan, Hamed Hassani, and Amin Karbasi. "Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization." arXiv preprint arXiv:1804.09554 (2018).  Specifically, in [2], Mokhtari et al. applied the averaging technique to conditional gradient method for stochastic optimization, which is exactly the same setting and almost the same algorithm. Even the proof technique is essentially the same (page 13 of this paper).   Their method with the unbiased estimator in the equation right below line 96 works in this zeroth-order setting because the unbiased estimator naturally provides randomness in the gradient oracle.   Both Mokhtari et al.’s algorithm and the proposed algorithm has an O(1/\epsilon^3) rate (to be precise, the rate of Algorithm is O(d/\epsilon^3), where the linear dependence on d is due to the unbiased estimator). However, the authors claimed “But the performance of the CG algorithm under the 36 zeroth-order oracle is unexplored in the literature to the best of our knowledge, both under convex and 37 nonconvex settings.”  In addition, Mokhtari et al.’s algorithm only needs one sample per iteration while the proposed algorithm (Algorithm 1) needs a batch of several samples, which is even worse.  Algorithm 2 is basically like a proximal Nesterov-type acceleration. Again, the main trick in Algorithm 3 is the averaging (2.18).   The authors should do extensive literature review and compare their results with existing work. This paper, in its current form, cannot be accepted without proper comparison with existing work. 