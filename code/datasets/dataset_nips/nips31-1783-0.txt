This submission proposes to use the stationary distribution of the stochastic function under the GP model for the prediction of streaming data. The required matrices follow the DARE, and the authors propose to solve for these using standard matrix libraries. The method is extended to non-Gaussian likelihoods. Hyper-parameter adjustments based on mini-batches are proposed. Experimental results on four real data sets are given.  [Quality] This paper gives a new approximation approach to the state-space formulation of Gaussian processes, and it is especially useful for streaming data using processes with comparatively shorter length scales. However, the "on-line learning of hyper parameter" (abstract, L34 and S3.3) is much more appropriately termed as re-estimation of hyper parameter based on mini-batches. This approach to hyper-parameter adjustments is trivial and not interesting --- I suggest that the authors move this part to the appendix and devote more space to improve the clarity of the paper (see points below). The results in sections 4.2 to 4.4 are convincing applications of the proposed approximation.  [Clarity] The paper is very unclear in certain critical aspects. 1. L25 --- the term "kernel components" is vague and not defined. 2. The term "boundaries" is used at several places but not well-defined at all. 3. For ADF, the authors should also cite the work by Casto and Opper's Sparse Online Gaussian processes, 2002. 4. L69. The authors need to expand on "scale poorly in long time-series models by still needing to fill the extending domain", since this seems to be an important motivation of the approach. 5. L72. Need to be more precise for "an optimal representation (without approximations) is rewriting". 6. L129. What are the function names in some of the matrix libraries? 7. L155-157: The "convolutional interpolation" needs to be expanded on --- the way it is used here is for $\gamma$, which is different from the way it is applied in [33]. 8. In section 4.1, the details of the experiments, for example, the example functions are not given in the main paper nor the supplementary material. The short forms in Table 1 needs to be explained. Also, are the MAE on the hyperparameters and not the prediction? 9. L225: The text needs to explain more how "Fig. 5 shows that .. reproduce".  That is, what is being compared in order to show this? 10. L244: How are the separate learning rates used?  [Originality] Section 3 (other than 3.3) is original: the use of the stationary distribution, and the use of DARE to find the stationary solution.  [Significance] I deem is work to be of significance, especially when the such streaming data is becoming more common.  Minor points [I would have given an 8, if not for so many of these minor points] a. There should be a better wording than saying "data are very long". b. L29 statepaces c. L39: there is be a more appropriate reference for general non-Gaussian likelihoods than [18], which focuses purely on binary classification. d. The word "latent" is used in various places as a noun, when it is an adjective. e. L66. genral f. L68. they they g. Section 2.1 seems to be based almost entirely on [19] or [26] --- suggest the authors to cite these here. h. L118: convereges j. L155: insipired k. L250: This only applies to stationary GPs.  [Comments on author's reply] Unfortunately, I have become more confused with the clarification that "kernel components" are the summands in additive kernels, and with the reply the R2 from lines 6 to 11. First, the paper have not given any explicit example or experiments on "additive kernels". Second, I am not sure how much Markovian structure is retained after kernel addition, so the state-space representation may in this case become sub-optimal.   