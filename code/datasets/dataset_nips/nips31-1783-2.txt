Summary ======= The submission proposes an approximation for Kalman filters that reduces the computational burden w.r.t. the state-space size. Under the condition that observations are equidistant, the state covariance matrix is replaced by the steady-state covariance matrix which allows to simplify the expressions for Kalman gain and filter covariance. With this substitution, certain matrix-matrix multiplications and inversions are no longer required and the complexity reduces from O(nm^3) to O(nm^2) where m is the size of the state space. The authors furthermore show how to apply their approximation when using non-Gaussian likelihoods. To this end they apply EP and demonstrate empirically that a single sweep is enough. Another contribution is a suggestion how to tune hyper-parameters in an online setting.  Clarity ======= The writing style is clear. Nothing to complain.  My knowledge about Kalman-filters is limited, and so I struggled with Section 3.1 about steady-state filters. I had difficulties to see why the Kalman gain converges until I read elsewhere that this is actually not so easy to show. For readers like me, more references to existing work would be nice (e.g. justification of lines 89 and 118) and references to books (e.g. [5]) should be augmented with page numbers.  As far as I understood, Section 3.1. is not novel. This would become clearer if the section would be part of the background.   Quality ======= I did not fully understand the paper but the equations appear correct and there is no reason to doubt the results of the experiments.  Figure 2 b) shows that the approximation is off initially but quickly becomes almost exact. Is this always the case or it could happen that exact solution and approximation diverge due to the approximation error in the beginning?  Originality =========== This submission presents three main ideas: replacing the state covariance matrix by the steady-state covariance matrix, assumed density filtering to cope with non-Gaussian likelihoods and incremental gradient descent for online hyper-parameter tuning. Nothing ground-breaking but definitely novel enough for acceptance.  Significance ============ A reduction in complexity without loss of too much precision is a significant achievement.