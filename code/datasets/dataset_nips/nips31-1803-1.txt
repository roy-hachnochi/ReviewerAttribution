The paper introduces a straightforward application of the results of ref 5 to the case of Poisson GLMs for large population of neurons. The idea is to replace the terms involving the nonlinearity exp(w’x) by a quadratic or 4th order approximation. This allows then for analytical computation of the posterior under a Gaussian prior and analytic MAP estimates. Because of that, Bayesian regularization techniques can be easily incorporated in that framework. Overall, this is not the most innovative work, but definitely a worthwhile contribution warranted by new recording techniques. The analysis on artificial and RGC data is nice and shows that the obtained approximations are good, even in the quadratic case. The authors should point out clearly that the analysis in Fig. 3 is just for a single cell. Could the authors add a timing analysis to the analysis of RGC data like for Fig. 2? What I don’t quite understand from Fig. 3 is why the ML estimate in panel b is so smooth, while the MAP estimated in e and f are quite noisy. Is that because of finer binning?  For the neuropixel recordings, what was the computation time of the fitting procedure? What would be the largest network that could be fit by classic procedures within that time?  As the main selling point is a fast approximation that allows fitting larger networks than before, I would be curious about some more discussion on that point. Is the network at a scale where memory constraints are coming into play? Recently a number of DNNs have been used for fitting neural data and GPU implementations are quite fast – how does the GLM compare here? Figure 1: The e of exact has been flipped in panel b 