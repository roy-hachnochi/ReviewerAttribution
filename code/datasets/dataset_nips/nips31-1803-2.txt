The authors consider the problem of fitting Poisson generalized linear (GLM) models to recordings from very large populations of neurons.  They leverage recent work to approximate GLM log-likelihoods as polynomials which allow recordings to be summarized with simple sufficient statistics that can be calculated from data.  They then show how priors on the GLM weights can be introduced and discuss a method for optimizing hyperparameters of these priors.   While the majority of this work focuses on the log link function, they also discuss how their work can be applied with other link functions.  They then demonstrate their method on simulated and real data, concluding by fitting GLMs for a population of 831 parameters.   I believe this work addresses an important computational neuroscience problem with an eloquent approximation technique that may likely be useful to others, and I believe this paper would provide a valuable contribution to NIPS.  I found the description of the core ideas and methods easy to follow and was impressed with the three demonstrations the authors provided.  With that said, there are minor points of clarification that may help readers:   1) Equation 7 seems to be missing the term Ta_o.  For optimization purposes, this may not matter but still tripped me up a bit, since it is written as equal to 6.  2) I believe the only quantification of run time was done in section 5.1 when estimating an MLE solution without searching for the best approximation interval.  It would be nice to have a sense of run-time when also doing evidence optimization to tune hyperparameters and cross-validation to search for the best approximation interval, as it seems in realistic applied settings, both of these may need to be done.  Quantification may be unnecessary but it would be nice for the authors to give a sense of the how much relative computational cost is added by these procedures.  3) It would be helpful if the authors could say something about the possibility of applying this method to datasets with 100,000 or more neurons.  While we can’t currently record this number of neurons with electrophysiological methods, this is possible in some animal models with calcium imaging and it seems approaches such as this might be adapted for these datasets.  However, it seems the optimizations involved here still involve inverting matrices of dimensions equal to the number of neurons.  Given this, do the authors have in mind a way this method could be applied when the size of the matrices collected as sufficient statistics grows very large?  This is a minor point that I think (at the author’s discretion) might be interesting to include somewhere in the text.   After author's response: After reviewing the responses of the other reviewers and the author's response, I still feel this is a high quality paper, worthy of publication at NIPS.  I agree with the concerns about the novelty of the work as an application of PASS-GLM, but I still feel the paper is novel enough and addresses an application that is important and likely to become more important in the years ahead to warrant publication at NIPS.  Therefore, I have revised my overall score down slightly to 7 these considerations.  