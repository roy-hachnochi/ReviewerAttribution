This paper addresses the challenge of an environment with discrete, but large number, of actions, by eliminating the actions that are never taken in a particular state. To do so, the paper proposes AE-DQN which augments DQN  with contextual multi-armed bandit to identify actions that should be eliminated. Evaluation conducted on a text-based game, Zork, shows promising results, as AE-DQN outperforms baseline DQN on several examples.  This idea of eliminating actions which are never taken in a given state is a sound on. The paper is clear and well written. On the flip side, I have several questions about the evaluation.  First, the evaluation is pretty "thin", as it considers only three Zork tasks. Furthermore, in only one task (i.e. the troll quest) AE-DNQ achieves high reward and significantly outperforms the baseline, DQN. For another task, egg quest, both AE-DQN and DQN achieve similar rewards (though AE-DQN converges faster). Finally, for the hardest task, Open Zork, while AE-DQN performs better than previous results, it doesn't really solve the task achieving a reward no larger than 39 (out of 100). It would be great to add more tasks. Also, besides Zork it would be nice to consider other applications.   Second, there is little discussion about the parameters used in various experiments. For instance, for the simpler tasks I believe you are using T = 100 steps, while for the more complex task, Open Zork, you are using T = 500 steps. You should provide more context. How well does your solution perform in the case of Open Zork for T=100? Also, how does the baseline perform for the simpler tasks if you use T = 500?  Why doesn't DQN solve the test for the larger action set, i.e., bottom plot in Figure 5(a). I understand that it might take later to converge, but it should eventually converge. It would be nice to run the the experiment longer. Similarly, can you run the experiment in Figure 5(b) longer? It doesn't seem that the ACT + * solutions have achieved the plateau.  During training you are using a discounted factor of 0.8 but, while for learning you are using a discounted factor of 1. Why? 