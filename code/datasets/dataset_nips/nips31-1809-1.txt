In this paper, the authors propose AE-DQN, a deep RL algorithm for large action space tasks where at each time step, an addition signal on action elimination is also provided. AE-DQN learns an action elimination model which eliminates a part of action space and enhance the sample complexity of exploration.   The paper is well written (can also be improved) and the importance of the problem is well motivated. The theoretical analysis is insightful and motivated.   In order to make the paper more clear, I would suggest the author use Linear contextual bandit terminology instead of contextual bandit since they use linear pay-off framework.    The paper motivates that the elimination signal is binary, therefore, I would suggest to the author to instead of providing theoretical insight using linear contextual bandit, please provide GLM style analysis where the output is actually a logistic. I believe expanding the current analysis to GLM is straightforward. If it is not, the authors can ignore the GLM part, but address more on the binary side of the motivation  Regarding action elimination incorporated as negative penalties, the authors mention that adding negative reward might couple the MDP and elimination, therefore not desirable. I would recommend that the authors do an experimental comparison on it. For example, using Levin et al framework, but instead of e-greedy exploration on the best n_max actions, also the prior (lambda in the ridge regression), properly chosen such that the actions with low samples stay small. 