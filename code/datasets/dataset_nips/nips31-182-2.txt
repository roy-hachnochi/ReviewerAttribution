This paper studied the relationship between sparsity and adversarial robustness for several classification models (linear and DNN based). Both theoretical and empirical results are provided to reveal the different behavior of linear model and DNN under l2 attacks and sparseness increase the robustness of DNN in general. All in all, the paper is well organized, and the empirical experiments are sufficient. I did not go through the detail proof of the theorem, but the sketch looks valid. Detail comments and question are as follows.  - It seems the two proposed metrics and analysis are only for l2 and deepfool type of attacks. Can the result and conclusion have been extended to other adversarial attacks. Such as L-BFGS, BIM, Jacobian-based Saliency Map Attack. - It would be nice to discuss the connection between proposed evalution metrics and the recently proposed rebustness metric used to measure DNNs, such as Noise Sensitivity Score and skewness proposed in this paper https://arxiv.org/pdf/1806.01477.pdf - In the experiment part, it would be nice to show the result under different \epsilson for L_infinity attack to show empirically the conclusion still hold for a larger pertubation.   Minor comments.  1. The notation of \hat{x} and \tilde{x} in define r_2 and r_infinity look much alike. It may be better to change to other notation to avoid confusion. 2. Captions in figure 1 and figure 2 to explain of FGS and Benign will help reader better understanding the results.  **** I have read the author's rebuttal and it clarify some of the detail. And I will keep my score. 