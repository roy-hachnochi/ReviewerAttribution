Paper describes a new software library focused on fast training of generalized linear models. It describes details of the architecture, how it maps to modern computing systems (with GPUs) and what specific aspects of the design and implementation are critical in achieving high performance. Finally they evaluate the performance of their implementation against existing popular implementations and show how SnapML is much faster in achieving the same results. There are many libraries for various ML algorithms available today, and SnapML focuses on one class of algorithms (Generalized Linear Models) and pushes the envelope for these algorithms.  Strengths: - Well written paper that articulates the details of what's important. - Combines a number of optimizations seen across different systems but put together here in a good way. - Leverage GPUs well for these problems - something that hasn't been done well before.   - e.g. implements a custom kernel for TPA-SCD on GPUs, and extends it to Logistic Regression while adding Adaptive damping. - Strong results compared to the state of the art for this target.  Weaknesses: - Some of the baselines seem to use prior runs, but aren't quite fair.    - For single node performance     - SnapML impl caches dataset on GPU     - Scikitlearn caches everything in memory     - TensorFlow impl reads from disk - it is quite easy to cache entire dataset in memory (and likely in GPU memory) for this too.      - This makes the comparison all about I/O - will be good to get a better comparison. - For both these setups it is good to highlight which of these optimizations got each of the performance wins. While this has been done briefly for SnapML as part of their explanations, some of these optimizations e.g. the TPA-SCD kernel and NVLINK could be used with TensorFlow quite easily as well. Hence the win maybe less because of the "overall library", than the specifics that are being talked about here.  Overall the description and implementation of the optimization for GLMs seems like a good step forward, and it seems worthwhile to accept this paper. However it will be good to improve some of the baselines for comparison - even if they are shown separately as - improved published benchmarks via X, Y and Z, so the authors get full credit for the new numbers. 