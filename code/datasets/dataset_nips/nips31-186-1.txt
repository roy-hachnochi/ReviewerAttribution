Post rebuttal: I read the rebuttal and other reviews. I will keep the initial rating (6) since there are some issues that still need to be addressed: - Obtaining intrinsic and extrinsic parameters of the camera is not trivial since in many cases we do not have access to the camera to calibrate it. - There are several revisions suggested by the reviewers that need to be incorporated.   **************************  Paper summary: The paper tackles the problem of semantic scene completion, where the idea is to find 3D regions corresponding to an object (occluded or not) and its category. The proposed model has three modules. First, it performs 2D semantic segmentation. Then, it maps the segmented image to the 3D space. Finally, it assigns category labels to the voxels of the scene.   Paper strengths: - The proposed model outperforms the previous models. - The paper provides an extensive set of ablation experiments.   Paper weaknesses: - My understanding is that R,t and K (the extrinsic and intrinsic parameters of the camera) are provided to the model at test time for the re-projection layer. Correct me in the rebuttal if I am wrong. If that is the case, the model will be very limited and it cannot be applied to general settings. If that is not the case and these parameters are learned, what is the loss function?  - Another issue of the paper is that the disentangling is done manually. For example, the semantic segmentation network is the first module in the pipeline. Why is that? Why not something else? It would be interesting if the paper did not have this type of manual disentangling, and everything was learned.  - "semantic" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper.  - During evaluation at test time, how is the 3D alignment between the prediction and the groundtruth found?  - Please comment on why the performance of GTSeeNet is lower than that of SeeNetFuse and ThinkNetFuse. The expectation is that groundtruth 2D segmentation should improve the results.  - line 180: Why not using the same amount of samples for SUNCG-D and SUNCG-RGBD?  - What does NoSeeNet mean? Does it mean D=1 in line 96?  - I cannot parse lines 113-114. Please clarify. 