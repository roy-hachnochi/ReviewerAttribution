This paper proposed an attention based model for unsupervised image-to-image translation. The attention mechanism is employed to untangle the image's foreground objects from the background, which is novel the interesting. The quality of generated images is very good and better than previous work,  demonstrating the effectiveness of this model on several datasets.  One limitation of the proposed model, besides the discussion in future work part, is that such attention model can only be applied to the images includes clear and separable foreground and background, e,g., zebra<->horse. It requires the images from source domain and images from target domain have some overlapping region (i.e. background) so that it cannot be applied to many problems such as style translation and colorization or many other datasets such as day <-> night, Google maps <-> aerial photos, labels <-> facades and labels <-> cityscapes.  The experiments do not provide the influence of many hyperparameters, such as the number of epochs trained before applying (7) and the threshold in (6). Such discussion can make the experiments more convincing.  This paper missed the discussion of an important reference "Generating Videos with Scene Dynamics". They also learned a binary mask to learn the static background and dynamic foreground in video. Although it is not called "attention", I think this paper share the same spirit. 