The authors present an approach to incorporate continuous representations through neural predicates into probabilistic logic (specifically ProbLog). Their approach is able to handle continuous input, learn the parameters of neural predicates and the probabilistic program and induce partial programs. The experimental evaluations cover all these aspects but could have more interesting intermediate results. The writing is clear but there are some key details that rely on prior work making it hard to understand this work.    - #72-73 What would be an example of b1, ... bm ? Also currently it looks like there is a negative sign on b1 but I assume you intend it to be ":-". - The gradient semi-ring for aProbLog was hard to follow. I am guessing eq 4 and 5 are the identities whereas e_i is a 1-hot vector. What is the difference between L(fi) in Eqn 7 vs L(f) in Eqn 8  i.e. why do they have different forms ?  - Figures 1 and 2 were immensely helpful, Could you create a corresponding figure for the equation in #147-150 ? - The example 165-178 (and corresponding fig 2) seems unnecessarily complex for an example. Couldn't the same point be communicated with just the coins ? - Figure 2c. I am not sure how to interpret the numbers at the bottom of the nodes. - Relative to prior work on aProbLog, is the key difference that the gradients are propagated through neural predicates here ?    Experiments - The faster convergence and generalization of the approach really shows the power of incorporating domain-knowledge through logic programs. As you mentioned, in principle, prior work on combining logical rules with neural models could be applied here too. Would they behave any differently on this task ? For example, in this domain, would their gradient updates exactly match yours ? - It would also be interesting to see the accuracy of the digit predicate over time too.  - Do you have any hypothesis on why the diff-Forth doesn't generalize to longer lengths ? Shouldn't it in principle learn the accurate swap behavior (which it did with shorter training lengths) ? - While the toy domains show the promise of this approach, I would have really liked to see a real application where such programs would be useful. The closest dataset to a real application(WAP) has comparable performance to prior work with not much discussion. Please also report the exact accuracy on this set.  ================ Thanks for your response. Please provide your explanations in the final version and clarify the contribution over aProbLog. 