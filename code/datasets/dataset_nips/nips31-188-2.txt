The work presents an alternative but more explicit analysis of the softmax bottleneck problem based on the dimensionality of the log-probability space that can be spanned by the model. Based on this reanalysis, desirable properties needed to break the bottleneck are discussed, followed by the proposed Sigsoftmax parameterization of a categorical distribution. Empirically, the proposed Sigsoftmax clearly improves upon the softmax formulation. Together with the previously proposed mixture structure, MoSS achieves the SOTA on two benchmark datasets for language modeling.  Strength: - An elegant analysis and method motivated by the analysis - Good empirical results on standard benchmark datasets  Weakness: - From the performance and empirical rank of ReLU in table 2 & 3, we can see that a higher empirical rank does not necessarily mean a better performance. Hence, it suggests a difference between producing some arbitrary log-probability matrix with a high rank and learning the "correct" log-probability matrix which is high-rank. From this perspective, no analysis/theory is provided to show that either the Sigsoftmax or the MoS has the "capacity" to model an arbitrary truth log-probability that resides in vector space up to M-dimensional. It is only shown that Sigsoftmax and MoS have a relatively better representational power than Softmax, an augment that holds for other non-linearities such as ReLU or softplus (a smooth version of ReLU with a stable grad).  - The superior performance of Sigsoftmax compared to Softmax may come from other unintended sources, such as easier optimization, other than a higher rank. Hence, I will suggest the authors conduct an additional experiment on character-level language modeling like the one in the original softmax bottleneck paper [1] to eliminate such unintended benefits.   Overall, this is a good paper, which can be further improved by providing the analysis suggested above.   [1] Yang, Zhilin, et al. "Breaking the softmax bottleneck: A high-rank RNN language model."