The paper proposes a method for learning a D-DAG, or a set of directed and undirected edges which represent causal relationships which are different across two datasets (in magnitude or existence), where it can be assumed that the causal relationships have the same topical ordering across the datasets and linear Gaussian models apply. Detecting changes in genetic regulatory networks is given as a motivating example. Existing methods are used to first estimate the undirected graph of differences (or the complete graph is used) and then a PC style approach with invariance tests using regression coefficients and residual variances are used to remove and orient edges. The method is shown to be consistent under some assumptions and evaluated using synthetic data and genetic regulatory network data.  The paper address an interesting and relevant problem that, to my knowledge, has thus far remained unaddressed. The proposed method draws on previous related approaches but contains significant novel contributions.   The general approach appears theoretically sound assuming the theorems in the supplement are correct (I was not able to check).  There are some parts of the paper that are confusing and could be made more clear.   At one point the paper seems to indicate that using the complete graph instead of KLIEP as input to algorithm 2 is for consistency purposes, but then later suggests both methods can be proved consistent.   In section 2, paper says learning the DAGs separately and taking the difference cant identify changes in edge weights "since the DAGs are not identifiable" - I assume this just refers to the fact that in addition to learning the graph structures, you would need to regress each variable on its parents to find differences in edge weights but if this is what is meant, I'm not sure what "the DAGs are not identifiable" then refers to.  Corollary 4.5 sounds like a completeness result (assuming I'm interpretting 'the D-DAG \Delta' correctly). The preceding text says it follows directly from Theorem 4.4 (orientation consistency). In general, this would not be the case. Some discussion explaining why this is the case in the setup of the paper would be clarifying.  The assumption that both DAGs have the same topological order has implications both for the significance of this approach since it may be applicable to fewer problems, but also for clarity: effectively, the approach is for estimating differences in edge weights, but where it is assumed that the causal structure is the same (only some edges disappear as their weights go to zero), but the impression given from the abstract and initial description is that this method is for learning actual differences in the causal structure (not just edges that go to zero). This is a fundamentally different and harder problem.  I have some concerns about the empirical results.   The metric 'consistenty estimated D-DAGs' is not defined explicity, but I assume the use of 'consistency' indicates this metric is true whenever all of the edges in the D-DAG correspond to differences between the two graphs - in other words, it incorporates precision, but not recall. If this is the case, I have two concerns: 1) if it counts cases correctly where the D-DAG does not contain some edges that are different in the two graphs, then the increase in performance over PC/GES could be due to DCI being more conservative rather than more accurate; 2) only counting cases correctly where all of the edges in the D-DAG are 'consistently estimated' may not be telling the full story, e.g. it's possible that many of the cases that PC/GES did not consistently estimate were only slightly off, which might make the difference in average performance less dramatic - it would be interesting to see what mean/median precision and recall look like for the different methods.  Additionally, the lowest sample size tested in the simulations is 1000, which seems high for many applications (the real data tested on only includes 300 samples). Why were the simulations not done at sample sizes more in range with the real data? The obvious worry is that the improvement over PC/GES may not be as significant at lower sample sizes.