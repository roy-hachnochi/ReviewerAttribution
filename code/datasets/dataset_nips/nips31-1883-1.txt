The paper gives a sublinear time algorithm to obtain a low rank approximation of a distance matrix A. (Entry (i,j) is the distance between p_i and q_j).  The intuition is that because A is a distance matrix you do not have to look at all its entries to get a good low rank approximation: The triangle inequality imposes some structure on the entries that you can use and look at only a few.  The main idea is to fast estimate the norms of the columns and the rows, use these estimates for a weighted sampling of the rows and the columns such that the resulting submatrix "preserves the projection cost" on low rank subspaces.   The final algorithm is complicated and I must admit that I could not fully absorb what it does even after investing a substantial amount of time. It may be better to put first the simpler scheme (the one that you implemented) which is in the appendix right now and not the most general scheme (replace the order of presentation between Algorithm 3 and Algorithm 2).   The description of the main result is heavy and confusing and can be improved a lot. For example, I see clustering into weight classes only in Algorithm 3, but you talk about this in the main text before Algorithm 2 (line 193).  You do not make clear the exact differences between the algorithms, why the come about, and what are their consequences.  I think you can make a substantially better job in making this more accessible.  minor comments: 1) The text in lines 119-130 is confusing. Better to write an outline of the proof. I only could get it by reading the proof. For example diam(Q) is not defined ? you have strange factors of "4" and "2" that are not clear, and its not clear that you cover all cases. Don't use a different notation for diam(Q) in the appendix, it may be confusing. 2) line 183 "minimizes" over what ? (say that you minimize over X explicitly) 3) line 192: "stem" ==> step 4) line 380, 381 why do you write A_{x,j} twice in the displays 5) line 445: what is the Matrix Bernstein inequality ? 