Inspired by recent work that have shown that the continuum limit of inertial schemes for gradient descent may be formulated with variational principles, this paper extends the idea to general manifolds, with an emphasis on the infinite dimensional manifold of diffeomrophisms. The main contributions are: 1) a variational approach to inertial smooth optimization on manifolds; 2) a Riemannian metric tailored to produce inertial schemes on diffeomorphisms, which is the kinetic energy of a distribution of moving mass; 3) A numerical discretization scheme which requires entropy schemes. The approach is illustrated on several computer vision and image processing tasks including registration and optical flow. The contribution is very interesting and original enough. Having said, I have several comments: 1) The authors keep talking from the very beginning about accelerated schemes, but this terminology quite inadequate here (in fact unjustified). First, in the Euclidean case, such terminology is legitimate according to a complexity (lower) bound argument on the class of convex smooth functions (with Lipschitz gradient). Here, given the setting on general manifolds, there is no such complexity bound. Moreover, and more importantly, the authors did not provide any convergence rate analysis on any appropriate quantity (e.g. poytential energy), neither in the continuous nor in the discrete settings. This is mandatory to be able to state formally that the presented scheme leads to acceleration (with dissipation, which is known to lead to inertia/accleration in the Hilbertian case). I would then suggest to completely rephrase this. 2) In relation to 1), in the experiments, it would be wise to report the convergence profiles in log-log scale of the potential energy to be able to read directly the slopes. 3) I think that one of the most important parts to the NIPS audience would be the discretization, which is postponed to the supplementary material.  4) In relation to 1) and 3), if one is able to prove a convergence rate (on e.g. the potential) in the continuous case, it would be much more convincing to see a result stating that the proposed discretization preserves that rate too.  5) There are a few typos such as P6, L227: epsilon lacking in the limit. 5) Section 4: regularity parameter alpha -> regularization parameter alpha. 7) Figure 3: in the Hilbertian case, it is known that the upper-bound on the descent step-size is twice larger for GD than AGD. It would be interesting to discuss this aspect in the Riemannian setting here.