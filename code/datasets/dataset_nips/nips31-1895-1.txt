This paper describes a very interesting application of RL to the task of regulating temperatures and airflow inside a large-scale data center. It is demonstrated that the RL agent  is able to effectively and safely regulate conditions in the data center after just a few hours of exploration with little prior knowledge. This will lead to a reduction in the hand-tuning effort that goes for data centers which is often specific to the equipment architectures and configurations.   This has advantages over the recent work by DeepMind which used machine learning to manipulate the temperature of the water leaving the cooling tower and chilled water injection setpoints. Particularly, this paper uses model-predictive control which learns a linear model of the data center dynamics using random (but always safe) exploration requiring no physics-based model or even any historical data. Then, the controller generates actions at each step by optimizing the cost of a model-predicted trajectory.  Key highlights: The novelty and the neatness of the problem definition is striking and this will motivate new research in this application. The paper clearly describes all the assumptions and motivates them from the perspective of this novel application.  The paper describes the development and demonstrated effectiveness of the various structural modeling assumptions in this domain which is impressive. Particularly, the method even during exploration is always safe.  Some of the main issues: The paper alludes to prior work by DeepMind as close to this work. But this is not described to compared to in the experiments. It would be good to have that comparison or atleast a discussion of why that is not possible.  There is some notation that has not been clearly defined in the paper. While that is understandable from context, it will be good to have a table of notation to make it convenient for readers. For example, delta, u_min, u_max are undefined.  Some of the model choices can be better motivated. For example, lines 169-171 - "we estimate the noise standard deviation σs for each variable s as the root mean squared error on the training data, and update the model with an example only if its (current) prediction error exceeds 2σs" Why this choice?  Some of the writing, especially in section 4.3 can be simplified. Equation 3 is difficult to parse, This is essentially weighted least squares. It will be beneficial to the reader to start with that information, And maybe introduce notation before the equation. Why are x_sp^s and u^c_min used in the difference in the square.  Questions: a) What is "target setpoint value"? how's it computed? b) Lines 183-186. What is the reason that the reformulated optimization converges well before the cvxpy implementation? This can be better discussed.  I read the author response. Thanks!