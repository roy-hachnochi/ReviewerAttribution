The paper describes a reduction from minimizing smooth convex function to a Nash zero-sum game. It is shown that after T steps, the players are O(1/T^2)-close to an equilibrium. Then it is shown that the online to batch conversion here yields the well-known acceleration of GD due to Nesterov.   What is new (and what was known)? 1. Known: the formulation of the game via Fenchel duality, where x-player plays MD and y-player plays optimisticFTL ([1]).  2. New: While [1] required the loss function to be both smooth and strongly convex, here only smoothness is required. This is a nice contribution. 3. New: showing that performing online-to-batch while using FTL rather than optimisticFTL,  yields the HeavyBall algorithm.   My only concern is that this view on acceleration is not new as several papers (mainly [2]) explored a similar primal-dual view.  However, I still find some of the aspects (e.g., importance of being optimistic) interesting. Therefore, I lean towards accepting the paper.  [1] Faster Rates for Convex-Concave Games [2] Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent