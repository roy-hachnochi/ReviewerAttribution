* summary This paper deals with the generalization bound of the neural net distance. Under the standard assumptions, the minimax lower bound of the generalization error was provided for two classes of probability distributions. In addition, tight upper bounds for generalization ability were established by using a new McDiarmid's inequality and a tighter Rademacher complexity of neural networks.    * review: A new McDiarmid's inequality is thought to be the main theoretical contribution. It enables us to evaluate the generalization error under the probability distribution with an unbounded support. Also, the Rademacher complexity of neural networks under sub-Gaussian distribution is interesting. A minor defect is the lack of numerical experiments to confirm the theoretical findings. Overall, theoretical results of this paper are solid and interesting.   comments:  line 181: the pattern of the equation of C(P_B) is not clear. Do all  M_F(*)s in the second term have the negative sign?  line 191: "the distribution class P_{uB}" may be a typo. I believe P_{uB} should be replaced with P_B.  line 273: the definition of the random variable, epsilon, should be clearly mentioned, though the Rademacher complexity is a common tool.  