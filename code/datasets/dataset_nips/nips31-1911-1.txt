Update to rebuttal: The author's should include from the Sparse VD paper in Table 2.  And fix the typo that the best error was .8% when it was in fact .75% for Song's method.  The author's should run on CIFAR-10 and provide results that can be compared to many other recent pruning papers.  I would like to some more discussion of how many iterations the pruning procedure lasts after training.  Is it comparable? 10%?  The paper presents a novel approach for pruning weights for network sparsification.  On two tasks (LeNet on MNIST and VGG-16 on ImageNet) it achieves higher compression than prior methods.  The technique is presented as a modification of the SGD update rule, then shown to be interpretable as a regularization term, as a general form of a L2 regularizer.  Overall, I find this to be a well written, easy to follow and (most importantly) useful paper.  The technique would be easy to add to existing code / frameworks and seems to achieve state-of-the-art results.  I think the paper should include more details on how long the sparsification phase lasts relative to the first phase, ie how much training time is added due to this procedure?  Does it work just as well with optimizers other than SGD?  Vanilla SGD is rarely used these days...  How were the regularizing factor and threshold determined (any useful guidelines, is a grid search required)?  I appreciate the derivation of (15) from the update rule, however it would also be interesting to see an attempt to explain term for other activation functions.  Is there any further intuition to be had beyond the relationship to l2?  My other main suggestion would be to try the technique on problems where we don't expect them to be quite so over-parameterized to begin with.  That would set better expectations on what is possible for more modern architectures.  It would also be insightful to see results on a RNN, as prior work has suggested they are harder to sparsify than the convnets examined in the paper (while at the same time, more likely to benefit computationally).  Line 235: sows -> shows  (I would also change over training to while training or during training).  There are a variety of other recent sparsification techniques that are not mentioned / compared against, such as (sorry titles are not complete): "To prune or not to prune", "L0 regularization", "Fisher pruning", which makes it hard to be sure how this technique compares against all others.