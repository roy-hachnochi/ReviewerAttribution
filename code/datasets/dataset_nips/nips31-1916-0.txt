 A) My main concern with this paper is with respect to the main results (Theorems 2 and 3). It seems the authors have not put sufficient care to the fact that \partial \hat{Q} in Algorithm 2 is a biased estimator of the true gradient \partial Q. Also, \hat{Q} defined in Line 189 depends on \hat{\pi} which is an estimate of \pi. Thus, a probabilistic proof would require to look at a conditional probability of the estimation of Q depending on the estimation of \pi. Assumption 1 somehow disregards this.  B) Regardless of the above, the final high probability statement in Theorems 2 and 3, seem to be missing the union bound of the error probability in Assumption 1. That is, Algorithm 2 calls \partial \hat{Q} a total of T times.  C) I let the authors know that there is some literature on convergence rates using biased gradient estimators. For instance, in Section 3 of [Honorio, "Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models". ICML 2012] results are given for general Lipschitz functions and samplers with decaying bias and variance. Similarly, [Hu et al, "(Bandit) Convex Optimization with Biased Noisy Gradient Oracles". AISTATS 2016] analyzes a general regime. I wonder whether the current problem fits or not the above mentioned frameworks.  === AFTER REBUTTAL ===  Regarding A, I thank the authors for the clarification. After inspecting Lemma 3, everything seems clear. Regarding B, I am satisfied with the authors' response. Although the authors should notice that Algorithm 2 calls the random part T times, and Algorithm 1 calls Algorithm 2 m times, thus I believe the final result with respect to Algorithm 1 requires an additional log m term. Regarding C, while I partially disagree with the authors, I am not basing my decision on this point.  After inspection, I believe Assumption 1 is somehow misleading, as it hides dependence with dimension d. Let me elaborate a bit.  Intuitively speaking w^* is the vector that we would learn by using logistic regression from infinite data (Thus, approaching the expectation with respect to the data distribution). Then w_t is what we would learn from t data points from the unknown data distribution. I believe the only assumption is boundedness (Line 208 in manuscript, Line 72 in proof of appendix). After that, Lipschitz continuity of logistic regression allows to bound \eta pointwise.  Assumption 1 (w_t approaching w^* in L2 norm) is a bit misleading. As Reviewer 4 points out, in general one could get O(1/\sqrt{t}) generalization bounds for the risks, but getting convergence in L2 norm requires more restrictions. One piece of literature that I can recall is the restricted strong convexity framework. In this case, some restrictive data distribution assumptions are necessary, such as: data being produced by a logistic regression model.  Also importantly, without any assumption on simplicity of representation of w^* (e.g., sparsity) the constant C' in Assumption 1 will very likely depend on the dimension d, since the authors are bounding the L2 norm. This happens for several learning theoretic methods: VC dimension, Rademacher complexity, primal-dual witness and restricted strong convexity.  The above makes Lemma 3 and Theorem 2 dependent on data dimension, thus, obtaining O(d/\sqrt{T}).       