This paper proposed an approach to developing accelerated gradient descent methods. Following the differential equation approach in existing literature, the idea of this paper is to approximate the continuous path using Runge-Kutta discretization. The resulting algorithm can give a convergence rate arbitrarily close to O(1 / k^2), if the objective function is smooth enough.   The idea is novel to the best of my knowledge, though it is perhaps direct to people who know the Runge-Kutta method.   The title is too strong in my opinion. First, the optimal O(1 / k^2) convergence rate is not achieved, but only approximated. Second, how well the optimal rate can be approximated depends on how smooth the objective function is (boundedness of derivatives); this additional condition is not required in existing literature.   It is interesting that now one can achieve a convergence rate better than the standard O(1 / k) via a principled approach. However, since this paper is arguably a very direct application of Runge-Kutta scheme, it does not seem clear to me that one may actually achieve the optimal O(1 / k^2) rate following this research direction. The authors should elaborate on why they believe that this work will not be obsolete in the future, in the concluding section.   In Assumption 2, the objective function has to be *continuously* differentiable instead of merely differentiable, if one wants to argue for existence of an upper bound of the derivatives, by exploiting compactness of the sublevel sets.   I did not have time to check the proofs.   The numerical results may not be reproduced, as the synthetic dataset is not given. 