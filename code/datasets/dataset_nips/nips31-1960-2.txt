The authors devise graph kernels based on return probabilities of random walks. They prove that this captures the spectral information of the graphs. Based on the return probabilities, Hilbert space embeddings for graphs with discrete and continuous attributes are derived, which induce valid kernels. Moreover, two ways to speed up the kernel computation are proposed: (i) sampling random walks and (ii) using Random Fourier features.  The main strengths of the paper are:  * Seemingly new idea how to use random walks to derive graph kernels.  * The approach is well motivated and justified. I like the toy example in Figure 1!  * The paper is well-written and clear.   * Experimental comparison on many data sets.    The weak points of the paper are:  * It is not clear to me why the proposed return probabilities should perform better than directly using the information provided by the eigenvectors of the adjacency matrix. The relation proven seems natural. The eigenvectors are for example used in this publication to obtain node embeddings: "Matching Node Embeddings for Graph Similarity", Giannis Nikolentzos, Polykarpos Meladianos, Michalis Vazirgiannis, AAAI 2017 I would like to propose to use the eigenvector embedding of nodes (just as the return probabilities) to obtain a Hilbert space embedding and use these as a baseline for comparison.  * The accuracy values in the experimental evaluation give rise to several questions:   1) The tables contain a kernel "CSM", which is not explained in the text. I guess that this is the "Connected Subgraph Matching kernel" (Ref 17). The accuracy values seem to be copied from that reference.   2) Which values are obtained based on your own experiments and which are copied from the literature? Comparing against the values from literature is often not fair, since the data sets may differ (e.g., edge labels are ignored or taken into account, only a subset of the attributes are considered etc.) and test splits are different.  * Other state-of-the-art competitors for comparison would be the Weisfeiler-Lehman optimal assignment kernel and the Multiscale Laplacian kernel (both NIPS 2016).     Remarks:  * It would be interesting to provide bounds on the number of random walks needed to approximate the exact kernel with a certain quality.  * Simply summing up the running time over all data sets is not very insightful.  * Table 3: HGK-SP is not considered.  * Table 4: HGK can also handle discrete+cont. attributes (the numbers are actually provided in the corresponding paper).  * p2, l64: 'permutataion map' should only be used when domain and codomain coincide.   In summary, I can only recommend the acceptance of the paper, provided that (i) the difference to the natural eigenvector embedding is discussed adequately and (ii) the issues regarding the experimental evaluation have been sorted out.   ========================= Update after the rebuttal:  My concerns about (i) have been addressed. Regarding (ii) I would like to suggest to make clear, which values were copied from the literature and which were obtained from your own experiments. For the values copied from the literature, please make sure that they are obtained for the same data set. This is, for example, not the case for CSM on ENZYMES, since in the experiments in Ref 17 only a subset of the available attributes were used.