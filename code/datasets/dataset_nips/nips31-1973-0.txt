In this paper, a deep generative Markov state model is proposed for inference of dynamical systems and prediction of trajectories.  An encoder maps observed data to latent variables and the dynamics of the latent variables are modeled as Markov chains.  Accordingly, a generator is trained to generate synthetic data from latent variables, and the transition matrix is estimated by the rewiring trick.  The proposed method shows some potentials to the tasks of molecule dynamics.   My main concerns are 1. Besides MSM, authors should consider more existing sequential models as baselines. 2. The architectures of proposed encoder and generator should be explained with details. The influence of the architectures on learning results should be analyzed in the experimental section. 3. The rationality and the superiority of using conditional energy distance (ED) should be explained with more details. To measure the distance between two distributions, there are many choices, e.g., KL divergence and Wasserstein distance. Compared with those metrics, what is the advantage of ED? Additionally, have authors tried to use other metrics to learn the proposed model?  ---------------------------------- After rebuttal ---------------------------------- Authors solved my main concern about the rationality of ED, so I change my score to 7. Additionally, I still wish that authors can consider more baselines containing transition matrices (maybe not Markovian but interpretable transition matrices), e.g., nonnegative auto-regressive model. Overall, it is an interesting work. 