This paper tries to solve the text to scene image generation problem. Technically, it proposes a conditional variant of VAE with a learnable prior distribution. As for scene generation, a concept mapping operator and four aggregation operators is proposed to obtain a distribution of latent variables that carry the semantics given by the tree structured concepts. The operators are compositional and reusable, making it possible to generalize. The methods are compared with popular baseline methods on two real datasets both quantitatively and qualitatively. Generally, the paper is well-written and clearly structured.   Pros: 1. The problem of scene generation is of general interests and quite challenging. Though the results are very preliminary, these show the basic ability to composite visual concepts and generate scenes. 2. The experimental settings are detailed and the results support the main claim.  Cons: 1. As the PNP-Net uses semantics information and ground truth bounding boxes in training, itâ€™s not proper to call it an unsupervised method. I also suggest that the author state clearly that PNP-Net leverages more ground truth information when comparing with DCGAN and Pixel-CNN. 2. One possible weakness of PNP-Net is the lack of coherence among objects. For instance, in the CLEVR-G dataset, the lighting conditions are shared among different objects, while in the generated scenes, objects often have inconsistent shadow.  Nevertheless, I think these are not major issues and the paper make a good contribution towards scene generation. It should be accepted. 