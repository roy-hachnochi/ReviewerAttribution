Summary:  The authors provide theoretical and empirical analysis of a new rollout scheme for running inference on arbitrary neural network graphs.  Additionally, they develop a general theory of rollouts for networks graphs, and (will) provide an open source toolbox implementing their method.  Strengths:  The paper is exceptionally clear and well-supported by numerical experiments.  The authors even include a snapshot of the eventual open source tool that they will provide to use the method.  Weaknesses:  For someone unfamiliar with the general art (or witchcraft) of unrolling networks, the choice of baselines make sense, but seem…sparse?  Does it make sense to, for example, additionally perform a random, representative hybrid roll-out to compare against the fully-sequential and fully-streaming data in Fig. 3?  More specifically, it’s unclear to me how the gains scale with amount-of-streaming, besides the obvious fact that more streaming = better.  This is a comparatively minor point though!   Overall, the paper is an excellent contribution to the community, and I look forward to seeing the open source package!