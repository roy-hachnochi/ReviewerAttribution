The authors introduce a technique to measure the generality (extend to which they transfer to another task) of hidden layers in neural networks. The authors do this by analyzing the singular vector canonical correlation (SVCCA), which makes use of the output of the hidden layers evaluated for different points in the network’s input domain. The authors apply their technique on neural networks that are trained to solved differential equations. The results show the first two layers of a NN generalize, the third layer only generalizes depending on the width of the network and the last layer doesn’t generalize.  The paper is a novel application of SVCCA (introduced at NIPS ’17 by Raghu et al) to the problem of measuring generality/transferability of different layers in a NN. The method is inspired by Yosinski et al’s definition of generality, but has a favorable computational cost compared to the original method.  The original SVCCA paper used the metric to introduce freeze training (freeze early layers in the network after they become stable during training), measure learning dynamics, and compare two models trained on CIFAR with different initialisations. The submitted paper repeats some of those experiments in the context of DENNs and has a more specific focus on transfer learning.  The paper is well written with clear and extensive experiments. My main concern is the significance of the findings, given that many were already hinted at/explored in the appendix of the original SVCCA paper. However given the depth of the experiments in the proposed paper I think it would be a worthy contribution to the conference.  I would be interested to know if the findings on generality of different layers extends from the differential equation domain to image classification, e.g. it would be interesting to see results similar to figure 3 on the CIFAR dataset that SVCCA uses.  ------------------------  I would like to thank the authors for the extensive feedback. Based on the results for the MNIST dataset I have decided to increase my score.