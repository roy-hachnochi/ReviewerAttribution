The paper proposes a technique for learning a model by consolidating weights across models that are trained in different datasets. The proposed approach thus attempts to solve an important problem that arises by the limitations of sharing and pooling data. The authors take on the brain segmentation problem by using MeshNet architectures. The proposed method essentially starts from the model learned from one dataset, performs variational continual learning to parallel train across multiple datasets, and then performs bayesian parallel learning  to fine tune the model on a dataset by using as prior the weights learned in parallel from the rest of the datasets. The proposed approach has been tested using free surfer segmentations for data part of the Human Connectome Project, the Nathan Kline Institute, the Buckner Lab and the ABIDE project. The proposed approach shows advantageous performance in a left out dataset when compared to simpler continual learning and MAP approaches.  The paper is clear, well-written and organized. The figures are relatively small and difficult to appraise. Maybe a collage of the different panels would be more appropriate to present the results. There some author comments from the draft version that should be removed, and it is not clear what the Peking dataset is. I would also suggest the author tone down the “in parallel” argument given the sequential nature of the algorithm. Reading the paper, I was expecting to see a method that takes models that have been derived totally independently from each other and it optimally merges them, which is not the case. Additionally, and regarding the datasets, it would be useful to know what the strength of the scanners is, and if any additional preprocessing steps were taken to account for inhomogeneities, etc.  The proposed approach is interesting, but it seems to be a creative combination of existing ideas to perform variational continual learning in parallel. Having said that, my most important concern regarding the paper is its experimental validation. First I find unfortunate the use of the segmentation task to exemplify its value. First, the use of free surfer segmentations as ground truth is problematic. Free surfer itself makes segmentation errors, and now we have a technique that makes even more errors. Second, sharing segmentation results and derivative datasets such as features extracted by ROIs is something that is commonly done, as exemplified by the success of the ENIGMA consortia. It is quite straightforward to run free surfer in every site and then pool meta-information to do analyses. Another task would be more appropriate. Importantly, I would have appreciated a more expensive validation. For example, how important is the choice of the starting dataset? How the results would change if NKI or Buckner data would have been used first? Critically, I would like to know what is the performance when training with all 3 datasets at the same time. This should be the ceiling that can be achieved. Last but not least, I find the ensemble baseline a bit poor. I would have preferred an ensemble method that learns how to combine the different models than simply average the probabilities.