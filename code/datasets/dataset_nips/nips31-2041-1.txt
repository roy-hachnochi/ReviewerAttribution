This paper suggests a clustering method as a prelude to compression of neural network representations by weight pruning. This method strikes a middle between the coarse and granular pruning methods based on sparsity and allows for a better affinity with modern hardware and linear algebra libraries by reducing branching.  The exposition of the paper is clear and the related work is explained and cited, though I'm not particularly familiar with this sub-domain. The presentation of the optimization problem for the input and output permutations that are sought in the reordering algorithm, as a k-means optimization is conveyed clearly, and the derivation of the successive steps of the greedy algorithm is straightforward. This reviewer would have loved to survey source code to check the details of the algorithm in action.  The accounting of the cost of reordering input and output of layers is correct in accounting a modest cost overall, compared to the time necessary in fine-tuning the model. However, in a distributed setting using model parallelism, this re-sharding of the weights across a set of workers may prove more costly.  The experimental part of the paper accounts for the trade-off between model size and accuracy, and that between speedup and granularity at different block sizes, and offer insightful learnings on inflexion points in the speedup gains as well as hints at optimizations for layer-wise choices of pruning rates.  Overall a very good paper.  Minor points: - the paper lacks the use of articles in various places English grammar would demand them, and could take a round of editing. - the end of section 1 and that of section 2 would gain by being better merged. Section 1 l.38-41 and Section 2 l.70-73 repeat each other.