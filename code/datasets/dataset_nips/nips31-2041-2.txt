 The paper uses structured sparsity methods to overcome the trivial practical speedup of DNN inference by connection pruning because of the irregular sparse data pattern.  Existing structured sparsity methods can learn regular sparse pattern and achieve higher speed, but they generally have lower sparsity because they introduce stronger optimization constraints. The paper reorders/clusters redundant/small weights into neighbor regions so that useless weights can be effectively removed simutaniously, and thus improve the structured sparsity and achieve higher speedup. A greedy method is proposed to search the permutation for reordering.   Strengths: (1) It makes sence to reorder/cluster small weights and remove then together. In this way, a higher structured sparsity and thus higher speedup should be achieved. The idea is interesting.  (2) It may further advance DNN inference acceleration for structured sparsity methods.   However, the quality would have been better delivered if the following weaknesses were solved: (1) brute force (gready) search is simple and may work, but it's more interesting to compare it with existing clustering methods (e.g. biclustering) (2) reordering trained DNNs is straightforward, but intergrating it into the training/pruning process may be more interesting and more effective (3) the paper should compare with existing structured sparsity methods (like group Lasso)  Minors and clarity: (1) "For generality, we assume that the weight tensor has d dimensions." Please clarify "d dimensions". For a tensor with R^{MxNxK}, sometimes people call it 3D but sometimes MxNxK dimensions. (2) clarify and define M = P(W). (3) "From the figure we can see that, block size has little impact benefit from our reordering method." however, there is certain correlation between block size, sparsity and accuracy. The correlation is more obvious in Figure 5.