This paper describes a method for training mixture density networks to act as re-usable block proposal distributions for a Metropolis-in-Gibbs sampling scheme. The proposals are trained to approximate the actual full conditional distribution as closely as possible. When these learned proposals can accurately represent conditional distributions for large blocks of latent variables, this can be much more efficient than e.g. random walk Metropolis-in-Gibbs approaches.  The paper is clearly written, and outlines an approach based around identifying structural "motifs" in probabilistic graphical models which frequently occur, and training proposals which will function well in these settings. In the toy example shown in figure 1, the obvious criticism is that this process only generalizes well to alpha and beta values similar to those seen in training. The re-usability of the trained proposal for different instances of a particular motif depends on the values of the parameters having high probability under the training distribution that randomly generates alphas and betas. This overall corresponds to choosing prior distributions in eqns 2 and 3 which will be appropriate across multiple models. I think these issues are discussed fairly well throughout the examples, which do a good and honest job of showing how this approach can be applied to new problems — both the aspects which are straightforward, and those which are somewhat fiddly (e.g. targeting the mixture model with the discrete variables marginalized out).  I believe the core innovation of this paper is the observation that many motifs are re-usable across different models. This is a generalization of the re-use of of proposals in both Gu et al "Neural adaptive sequential Monte Carlo" and Paige & Wood "Inference networks for sequential Monte Carlo in graphical models" — in both of these, when considering sequential models, the same proposal model was re-used at each timestep, despite the parameters (and one-step SMC / particle filtering target) changing. Obviously, both those papers consider SMC as the base inference algorithm, rather than Gibbs sampling, and as such aim to learn approximations to the filtering distribution rather than the full conditional. While I am not aware explicitly of work which uses trained proposals like these as block Gibbs updates, the fact that proposals designed for use in a sequential Monte Carlo setting (perhaps with blocks of latent variables per step, as in Paige & Wood) can be adapted for use as Metropolis-in-Gibbs proposals, is not a huge conceptual leap, and in fact the objective function for training the network is essentially the same in all three of these papers, as well as in Le et al. However, to the best of my knowledge no one else has made an effort to re-use trained proposals even across entirely different models. I also think this paper overall presents the approach clearly, and in a way which could encourage others to go ahead and train proposals in this manner.  One small annoyance: given the focus on probabilistic programming languages in the introduction, I was hoping to see this implemented in a re-usable way in some probabilistic programming language which is at least as expressive as needed for the open-universe GMM example. Such an implementation would certainly help adoption — by my understanding, although much of the cited related work attempts to target very general classes of models, in the end the implementation code ends up being quite model-specific; unfortunately this seems to be the case here, as well. 