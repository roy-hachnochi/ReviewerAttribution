this paper proposes training a recurrent network to predict concurrent neuroimaging and behavior timeseries, using as input the stimuli and rewards seen by a subject in an experiment, along with the actions they emitted. although the model is trained on a combined supervised loss of predicting brain data and behavior, the weighting of these two factors is set such that the behavior takes precedence. because the state of the network is a restatement of the voxels, you can use calculus to find voxels 1) that are sensitive to reward (or the stimulus, although this isn't explored) and 2) to which the policy is sensitive. it seems like this method could be very powerful.  however, because the model is so powerful, it's also kind of difficult to understand what's going on. suppose a reward happens at time t, and we consider an action at some later time t+T. in between, there is a series of voxel patterns. but is the RNN actually explaining anything about the computations happening in that series of voxel patterns? or is it just a fancy way of finding voxels that are related both to reward and action?  i was also wondering about the lack of statistics to help understand how meaningful the patterns show in figure 3 are.  the small size of the RNN is what prevents it from memorizing the data; it would be interesting to see how the results change as the capacity of the RNN changes. related to this i would be interested in more analysis of how the representational capacity of the model is divided between the brain and behavior. by the way lambda is set, it looks like the model fully explains the behavior (which is low dimensional). is it then the case that whatever parts of the brain data are correlated with behavior the model can then explain "for free"; and it uses its residual capacity to explain other parts of the brain data?  why convolve the RNN's outputs with an HRF? shouldn't the network be able to learn the HRF, and maybe it would be more accurate (and region-specific) than a canonical HRF?  since fMRI data has a lot of spatial structure, would it help to include a spatial deconv on the network's output? 