This paper designs, implements and analyzes a gradient aggregation algorithm for a network architecture following the BCube (Guo et al 2009) topology.  The context of distributed stochastic gradient descent is well-explained and the comparison between Fat-Tree and BCube is well laid out. The context around the BCube topology is very sparsely cited. Since adopting the BCube topology involves some investment in hardware and configuration, it would be useful to the readers to have an idea of its value outside of the scope of SGD optimization. The original impetus behind the introduction of BCube was fault tolerance and some bibliography to that effect (e.g. de Souza Couto 2015) would help.   The gradual explanation of the algorithm in section 3.2, including its example, is well-appreciated, even though the dataflow in Figure 4 is difficult to grasp from visual elements.  The paper may gain from citing and comparison with _Butterfly Mixing: Accelerating Incremental-Update Algorithms on Clusters_ (Zhao et al 2013) which also has a 1/k over a Fat-Tree AllReduce, though this falls very slightly out of the bounds of the strict BSP focus of the authors. In any case, the state diagrams represented there offer a more easily readable representation of the gradient transmission algorithm than Fig.4  The experimentation section is very useful as a validation, and an open-source release would be very appreciated. The use of the very small MNIST dataset is somewhat disappointing (CIFAR-10 would be a great alternative).  Overall a good paper.  Minor points: - GST (gradient synchronization time I assumed) is never spelled out