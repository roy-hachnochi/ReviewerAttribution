This work describes algorithms for differentially private stochastic contextual linear bandits, and analyzes regret bounds for the same. Privacy in this context means protection against inference of both rewards and inference. The paper first notes that from the action taken in T round it is possible to infer the context even in relatively benign instances. Therefore, it proposes a (reasonable) privacy model where the privacy adversary attempting to infer the context in round t is not aware of the decision/action in all rounds but round t. The algorithmic techniques to achieve this are quite straight-forward -- do linUCB on estimates of cumulative sums made private by tree based aggregation protocol.   What to like? 1. It identifies a reasonable privacy model -- since the obvious one leads to linear regret. 2. The problem is important in terms of the number of applications that use the paradigm. I'd even say that it is more practical than typical k-arm stochastic/adversarial bandits that have been studied in context of DP so far. 3. I appreciate the note on lower bounds, esp the lower bound on excess regret for the private variant. Such bounds were somehow missing from the literature so far.  I'd like to see a couple of comparisons beyond the ones listed. 1. (Neel, Roth) Mitigating Bias in Adaptive Data Gathering via Differential Privacy. This work used DP to control the bais of empirical means of arms in bandit settings. In the process, they also establish private versions of linUCB. Only the rewards are made private in this setting. 2. (Agarwal, Singh) The Price of Differential Privacy For Online Learning. Here the authors establish regret bounds for full-information online linear learning that has the same regret as the non-private setting in the dominant T^0.5 term. I'm wondering if the authors can comment if their excess regret lower bounds can (for example) say that for the experts settings an excess regret of O((number of expers)/\eps) is necessary.  - After the authors' comments. - Score unrevised. 