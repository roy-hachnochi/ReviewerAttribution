STRENGTHS:  Overall I found this to be a very interesting and well-written paper.  The main contribution is to show that for relatively general network architectures, if either an additional augmenting skip neuron is added directly from the input to the output (or one augmenting neuron is added per layer) all local minima will be globally optimal.  In addition, the authors also show that the output of the augmenting neuron(s) will be 0 at all local minima, which implies that one can add an augmenting neuron to a relatively arbitrary network and still be guaranteed that a recovered local minima will also be a global minimum of the original network.  The authors provide a comprehensive review of existing results and clearly place their contribution within that context.  The proofs are well explained and relatively easy to follow.  WEAKNESSES:  I found this to be a high quality paper and do not have too many strong criticisms of this paper, but a few points that could improve the paper follow below.  1) There is no experimental testing using the proposed augmenting neurons.  A clear prediction of the theory is that the optimization landscape becomes nicer with the addition of the augmenting neuron(s). While understandably these results only apply to local minima and not necessarily arbitrary stationary points (modulo the results of Proposition 3), a fairly simple experiment would be to just add the augmenting neuron and test if the recovered values of the original loss function are smaller when the augmenting neuron(s) are present.  2) Similar to the above comment, a small discussion (perhaps following Proposition 3) regarding the limitations of the results in practical training could be beneficial to some readers (for example, pointing out that most commonly used algorithms are only guaranteed to converge to first order stationary points and not local minima in the general case).