=== added after author response === I have two more comments: (a) the scaling issue mentioned by R5 actually leads to an immediate trivial proof for Lemma 1 (i) and invites the following question: the exponential neuron, from an optimization perspective, is entirely "redundant" as it must vanish at any local minimizer but yet it changes the potential set of local minima, by putting more stringent conditions on the local minima. This phenomenon is a bit curious and perhaps deserves more elaboration.  (b) I want to emphasize again "eliminating local minima" by itself is no big deal, because you can get a reformulation that eliminates all local-but-not-global minima and yet is NP-hard to solve (e.g., finding a local minimum). This, I am afraid, is likely what is going on here (if you drop the separable assumption). Prove me wrong. === end ===   Deep neural nets are known to be empirically "immune" to poor local minima, and a lot of recent efforts have been spent on understanding why. The main contribution of this work is to prove that by adding a single exponential function (directly) from input to output and adding a mild l_2 regularizer, the slightly modified, highly nonconvex loss function does not have any non-global local minima. Moreover, all of these local minima actually correspond to the global minima of the original, unmodified nonconvex loss. This surprising result, to the best of my knowledge, is new and of genuine interest. The paper is also very well-written and I enjoyed most in reading this paper out of my 6 assignments.    As usual, there is perhaps still some room to improve here. While the main result does appear to be quite intricating at first sight: any local minima is global? and they correspond to the global minima of the original network? Wow! But if we think a bit harder, this result is perhaps not too surprising after all: simply take the Fenchel bi-conjugate of the original loss, then immediately we can conclude any local minima of the biconjugate is global, and under mild assumptions we can also show these local minima correspond to the global minima of the original loss. So, this conclusion itself is not surprising. The nice part of the authors' construction lies in that the modified function is explicitly available and resembles the original neural network so one can actually optimize it for real, while the biconjugate is more of a conceptual tool that is hardly implementable. Nevertheless, I wish the authors had included this comment.  It would be a good idea to point out that the so-claimed local minima of the modified loss does exist, for one need only take a global minimizer of the original loss (whose existence we are willing to assume) and augment with 0 to get a local minima of the modified loss.   But most importantly, the authors dodged an utterly important question: how difficult it is to find such local minima (of the modified loss)? This question must be answered if we want to actually exploit the nice results that the authors have obtained. My worry is that we probably cannot find good algorithms converging in reasonable amount of time to any of those local minima: simply take a neural network that we know is NP-hard to train, then it follows the modified loss is also NP-hard to train (without the realizability assumption of course).  If this question is not answered, I am afraid the author's nice construction would not be too much different from the conceptual biconjugate...  Another concern is the authors only focused on the training error, and did not investigate the generalization of the modified network at all.. If we are willing to assume the training data is separable (linear or polynomial), then achieving a zero training error in polytime is really not a big deal (there are plenty of ways). Proposition 2 alleviates some of this concern, but I would suggest the authors add more discussion on the realizability assumption, especially from a non neural network perspective.  Some minor comments: Line 77: b_L should be b_{L+1}.  Eq (4): the regularization constant lambda can be any positive number? This to me is another alarm: the modified loss likely to be very ill-behaved...