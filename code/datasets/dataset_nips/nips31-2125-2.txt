This paper considers neural networks and claim that adding one neuron results in making all local minima global for binary classification problem.  I might be misinterpreting the theoretical statements of the paper, but I don't quite get why adding the neuron is useful. Assumption 1 readily provides a huge information on the loss function (e.g., every critical point is a global minima) and Assumption 2 implies the the neural net can solve the problem to zero error, both of which (in my opinion) are really strong assumptions. Furthermore, Theorem 1 claims that \tilde{\theta} minimizes \tilde{L} and \delta minimizes L, so I don't quite get why do authors add the extra neuron. They discuss this issue after Corollary 1, but it is not satisfactory. If there exists a local minima that is not global (which is the case in RELU nets as the authors state), then the statement of Theorem 1 doesn't hold, which suggests Assumption 1 is not valid for those scenarios.