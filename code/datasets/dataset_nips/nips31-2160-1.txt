In this paper, the authors address the 'amortization gap' introduced in recent work and propose means to regularize inference networks in order to ameliorate its effects. The amortization gap is an effect where bottlenecks in amortized inference networks lead to both suboptimal inference as well as suboptimal generation due to the fact that the generator can never be fit adequately. Specifically, the authors propose a fix by injecting noise into the inference networks by adding it to the observations that are conditioned upon.  The authors also derive a loss function for this regularizer R which can be combined with variational inference.  Quality: The paper has high quality in its style as it blends intuitions with formal proofs for its conceptually simple key idea.  I particularly enjoyed the link to importance weighted AEs and the clear explanation that in that case the regularization term will not work as well in high sample sizes, since the variational family becomes very general as is. The key idea of the paper is intriguing, but causes some problems from the modeler's point of view: 1. what is the distribution of the noise that one should add to the inference network inputs? 2. What is the link of that noise to the generator noise? Can we posit a model of the perturbation for the inputs to learn those? 3. Is the addition of Gaussian noise the best we can do or just convenient for proof-reasons? We already know that Gaussian noise is a bad observation model for image patches with VAEs, why would it be the 'right' regularizer for inference? The paper inspires such questions but offers little in terms of exploration of these topics, which ultimately may lead to more useful general regularizers. Experimentally the paper is limited, but does a convincing job of showing the effects it discusses in practical settings. A main question that remains is the following: where is the tradeoff between regularizing inference networks and defining better variational families through inference algorithms? We already know that we can get over the limits posed here by various methods blending VI and sampling techniques, is the proposed alternative a scalable way out in the opinion of the authors?   Clarity: - The paper makes lucid points and has a pleasant style by formally presenting motivations and then proofs in its main text in clear order. While this may be a burden to the casual reader, it is a very complete story. -It is unclear in the flow of the paper what role Section 3.2 plays as it does not follow from the previous ones It seems to not be necessary for the key points of the paper since it is an almost orthogonal idea to the main idea but additionally also is not presented at length.  It is intuitively understandable that smooth weights will have regularizing effects, but this aspect of inference regularization might benefit from a more thorough treatment in future work.  Originality: The paper follows closely in the vein of the cited work "Inference Suboptimality In Variational Autoencoders", but adds a potential solution to the literature. It is as such appropriately novel for a nips submission, since the cited work is also very recent.  Significance: The topic of regularizing amortized inference machines is timely and important, as these objects drive a lot of modern probabilistic Deep Learning. I have doubts that the proposed solution will be sufficiently general to be widely adopted, but it is a good formal step.  UPDATE Post Rebuttal: raising score to 7.   