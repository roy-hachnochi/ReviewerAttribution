The paper extends maximum entropy-based approaches for imitation learning by replacing information entropy by Tsallis entropy. The motivation is that while the former approach leads to softmax distribution policies, the latter finds sparsemax distribution policies, allowing zero probabilities on bad actions.  Although a direct extension, the approach is novel and well-motivated. I appreciated its interpretation with the Brier score.  In the experiments, it would have been interesting to evaluate the case where k is misspecified in the multi-goal environment. How was k chosen for the other domains?  The exposition could have been a little bit more self-contained and clearer. For instance, the definition of a sparse distribution could be recalled.  Minor remarks and typos: l.70: the optimal policy distribution -> an optimal policy distribution (there's no unicity) l.79: casual l.112: that that Th.1 is an "old" result, it can already be found in [Puterman, 1994] l.137: \gamma is missing (11): \rho_\pi(s) is not defined l.258: is an \alpha missing?