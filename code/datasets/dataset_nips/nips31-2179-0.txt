This paper proposes a declarative specifications for semi-supervised learning (SSL) and specifies various different SSL approaches in this framework. The experiments on document classification and relationship extraction tasks show improvements compared to other methods.  I like the approach as the paper improves on a previous method D-Learner and is able to combine different SSL strategies, e.g. co-training, neighbourhood based label propagation, ... using adaptations to entropy regularisation rule. It also shows good experimental performance on smaller datasets.  Where I think the paper can improve is to experiment on text classification or RE datasets that are considerably larger than the ones studied. These datasets provide possibilities of comparison to simpler methods that rely on large unlabelled datasets.   For example for RE there're models such as "Neural Relation Extraction with Selective Attention over Instances", Lin et al.,2016 or similar techniques rely on full weak supervision rather than SSL.  For text classification, large scale unsupervised methods with pre-trained language models are often seen to perform really well with small number of labeled examples ("Learning to Generate Reviews and Discovering Sentiment", Radford et al 2017, "Improving Language Understanding by Generative Pre-Training", Radford et al 2018).