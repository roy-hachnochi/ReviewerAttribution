The submission introduces a variant of the D-Learner declarative language, which is limited to learners based on ProPPR probabilistic logic. The proposed variant, DCE-Learner is based in TensorLog which, while a more constrained language, has the benefit of a more extensive library of learners, among other things. A set of declarative rules are specified to encode different SSL settings. Experimental results are given showing improved performance on text classification and entity extraction.  Unfortunately, for someone unfamiliar with declarative languages, it's difficult to distinguish how novel these rules are, given the TensorLog logic.   The second experiment appears to be where the methods are most effective, but the limited space allotted to describing the problem setting and methodology makes the logic difficult to follow. The paper might be stronger if more room was given either to how TensorLog implements these rules or to the relation extraction task.  - in Figure 2, what is the difference between near(.,.) and sim(.,.)?  - in Table 1, doesn't GraphEmb perform better than DCE on the Cora dataset? Is that an error in the table, or in line 178?  - in Table 1(a), is there an intuition as to why the base supervised DCE outperforms the othe other learners? Is this simply an effect of the TensorLog implementation or is there something about the design of the DCE that contributes to performance?  - In Table 1, why do the DSE Supervised and All results differ between (a) and (b)? - in section 4.1, is it possible to more explicitly state the difference between a 'relation' and a 'type'?  - how important is the Bayesian optimization used to combine the SSL ensemble?  Are there any hyperparameters to tune here?  ----------------------  Thank you to the author's for their clear response. I feel they have done a good job of addressing any issues raised and propose changes which will make the paper clearer.