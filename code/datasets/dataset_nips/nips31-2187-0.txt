The paper proposes a new learnable model DeepPot-SE for inter-atomic potential energy surfaces (PES) based on deep neural networks. The authors start by introducing a number of requirements that a PES model should fulfil. Compared to other proposed model, this proposed model is the first to fulfil all these requirements, including differentiability and preserving natural symmetries. In the empirical evaluation, the performance of the proposed model is comparable to or better than state-of-the-art models as measured in MAE for energy and force predictions.  Generally the paper the paper is well written and easy to follow. However, the paper may be hard to follow for someone with little experience in PES modelling, e.g., it is never explained in the paper what is meant by "Finite and Extended Systems" (which is part of the title of the paper).  One of the central aspects of the paper is the feature matrix D^i in equation (11) that is invariant to translations, rotations and permutations. The authors have shown that some of the subexpressions of (11) are indeed invariant. However, given that this feature matrix plays such a central role in the paper, I think that the authors should give some details beyond "It is straightforward to see that D^i, and hence the DeepPot-SE model, preserve all the necessary symmetries" (line 141-142), e.g. in supplementary material.  The work seems to be an elegant improvement of the Deep Potential model [17], which unfortunately also makes the work seems a little incremental. However, the idea of the feature matrix D seems novel and could have potential applications outside the domain of PES modelling (as also noted by the authors). Such applications are however not considered.  The authors evaluate the model empirically on a wide selection of systems. For small organic molecules, the proposed DeepPot-SE model is compared to three state-of-the-art models. However, given that DeepPot-SE is an improvement of the Deep Potential model, I find it surprising that the Deep Potential model is not included as a baseline. Generally, the DeepPot-SE model has a performance that is comparable to the other models. However, it does not outperform the other models. The results for bulk systems are compared to the DeePMD model, and on these datasets, the proposed consistently outperforms the baseline model. However, for this dataset I am slightly puzzled about the training and test split: 90% of the snapshots of an MD trajectory are randomly selected as training data and 10% are selected as test data. As snapshots are only 10fs apart, I worry that the examples in the training and test set are highly similar. Could the authors please clarify this in their rebuttal.  To summarise, the paper is of high quality and clarity. The work has some both incremental aspects and some novel/original aspect (that may have application beyond PES). The results on small organic molecules are not highly significant, however, on bulk systems, the baseline model was systematically outperformed.   Minor comments:  1. Figure 1: ".= (b3)" -> ". (b3)"  2. Equation (8): it would be useful to remind the reader that r_ji = $|\vec{r}ji}|$.  3. Figure 2:  "DeepMD-SE" -> "DeepPot-SE"   Comments after rebuttal:   The authors have addressed most of my issue in their rebuttal, and I recommend accepting the paper. However, I would like to stress the importance of added a rigorous proof for that D^i (equation 11) preserves all the intended symmetries in the camera-ready version. 