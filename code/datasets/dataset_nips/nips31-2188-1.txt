The paper talks about the convergence analysis of a specific type of SGD for convex problems, where the gradients are k-sparsified:  either top-k or random chosen. It provides proofs that the proposed algorithm converges at the same rate as vanilla SGD.   The authors claim that it is the first convergence result for sparsified SGD with memory.  The numerical study verifies the convergence theory, shows its effectiveness comparing with QSGD in terms of performance and memory requirements. Also, MEM-SGD scales up to 10 cores. The results are overall encouraging.   In Page 6, line 179, in the sentence “As the usefulness of the scheme has already been shown in practical applications“, what is the scheme? Is it MEM-SGD? Or Quantization?  Reducing communication overhead is mostly interesting in training deep learning models. Can this work be extended to non-convex setting?   As to the memory containing pass information, it is better to do an experiment to comparing the results of using or not using the memory. 