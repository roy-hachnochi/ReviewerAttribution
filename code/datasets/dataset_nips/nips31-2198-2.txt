This paper proposes “voice cloning” system to synthesize the voice from unseen speaker during training. It has three separately trained components: (1) a speaker encoder trained on speaker verification task using an large-scale untranscribed dataset; (2) a multi-speaker Tacotron 2, which is a text-to-spectrogram model conditioned on the speaker embedding from speaker encoder; (3) a WaveNet vocoder.  Detailed comment: 1, In speaker encoder, what is the similarity metric used for speaker verification training?  2, Any particular reason to use 40-channel Mel in speaker encoder instead of 80-channel as in synthesizer? 3, The training dataset consists of 1.6 seconds audio clip, while an arbitrary length utterance is broken into 800ms windows, overlapped by 50% at inference. It seems to introduce additional training test mismatch. Why not use the same setting? e.g., also apply overlapped 800ms windows in training? 4, During synthesizer training, speaker encoder doesn't need to be constrained by only using target audio as input. Have you tried to feed different utterances from the same speaker into speaker encoder to extract speaker embedding?  5, In Table 1, higher MOS on unseen speakers seems not an overfitting problem, considering the definition of overfitting. It'll be good to check different held-out speaker sets to reduce MOS variance; randomly sampled “easy” speakers might be the issue.  Overall, I think this is a good work given the impressive results. The novelty might be limited, as it does not propose a brand new architecture or method. However, it made solid contribution to neural TTS. 