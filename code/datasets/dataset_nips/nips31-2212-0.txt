This paper deals with learning linear models in a decentralized setting, where each node holds a subset of the dataset (features or data points, depending on the application) and communication can only occur between neighboring nodes in a connected network graph. The authors extend the CoCoA algorithm, originally designed for the distributed (master/slave) setting. They provide convergence rates as well as numerical comparisons.  The authors should state more clearly that they are extending CoCoA to the decentralized setting. The adaptation of the setup, the local subproblems and the algorithm itself are fairly direct by restricting the information accessible by each node to its direct neighbors (instead of having access to information from all nodes). Despite the limited originality in the algorithm design, the main technical contribution is the derivation of convergence rates for the decentralized setting. This non-trivial result makes the paper an interesting contribution to the literature of decentralized optimization. In particular, the resulting algorithm has several properties (inherited from CoCoA) that are useful in the decentralized setting.  I have a few technical questions: 1/ Can you clarify and comment the dependence of the network topology (spectral gap) on the convergence rates of Thm 1-3? A typical dependence is on the inverse of the square root of the spectral gap (see e.g. [1, 2]). In Thm 2, why is there no dependence on the spectral gap? 2/ CoCoA achieves linear convergence even when only g_i is strongly convex (not f), but this decentralized version only achieves sublinear convergence in this case (Theorem 2). Is there a fundamental reason for this or is it due to the proof technique? 3/ The convergence guarantees hold for the quantity \bar{x}, which is an average over several iterations. As each node holds a block of the iterate x^(t) at each iteration, it looks like making the model \bar{x} available to all nodes as learning progresses (so they can use the best current model to make predictions) could require a lot of additional communication. Is there a way around this?  Regarding the experiments, experimenting with only 16 nodes is quite disappointing for the decentralized setting, which is largely motivated by the scalability to large networks. It would be much more convincing if the authors can show that their algorithm still behaves well compared to competitors on networks with many more nodes (this can be done by simulation if needed).  The clarity of the presentation can be improved: - The authors remain very elusive on what the matrix A represents in practice. They do not explicitly mention that depending on the task, the dataset must be split sample-wise or feature-wise (sometimes, both are possible if both the the primal and the dual match the assumptions). Giving a few concrete examples would really help the reader understand better the possible application scenarios. - This aspect is also very unclear in the experiments. For each task, what is distributed? And how are features/samples distributed across nodes? - It would be useful to give an interpretation of the form of the subproblem (3).  Other comments/questions: - Lines 33-34: most decentralized optimization algorithms for sum-structured problems do not rely on an i.i.d. assumption or completely fail when it is violated (but they can of course be slower). - Line 139: the dependence of the communication cost on d (which can be the number of features or the total number of samples) should be made clear. Depending on the task and dataset, this dependence on d may make the algorithm quite inefficient in communication. - How was the rho parameter of ADMM set in the experiments? - It is false to argue that the proposed algorithm does not have any parameter to select. At the very least, one should carefully choose \Theta, the subproblem approximation parameter. There may be additional parameters for the local solver.  Typos: - Before eq (2): "Let set" --> "Let the set" - Line 132: "Appendix E.2" --> "Appendix E.1" - Line 178: missing tilde on G - Line 180: we recovers  References: [1] Duchi et al. Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling. IEEE TAC 2012. [2] Colin et al. Gossip Dual Averaging for Decentralized Optimization of Pairwise Functions. ICML 2016.  ============ After Rebuttal ============ Thanks for the clarifications and updated results.