In this paper the authors present parameterized convolution layer. The parameterization is done using the wavelet transform whose parameters are learnt using gradient descent. The benefits of this parameterization are that the convergence is faster and minimal hyperparameter tuning is necessary either for initialization of the CNN layer parameters or the pre-processing steps.  The authors report improved performance using the proposed method compared to CNNs with hand-crafted features in 2 different tasks.   Compared to previous approaches which either learn the temporal convolution layers from random initialization a major purported advantage of the current approach is interpretability of the wavelet transforms.  In the paper the authors compare with CNNs which operate on log mel features but not with CNNs which operate on raw waveforms. Despite the advantages of the proposed method compared to the raw waveform CNNs in terms of computational complexity, interpretability and stability during optimization it would be interesting to see this comparison in terms of performance.  The authors describe the challenges in optimization with vanilla SGD and uniform learning rate across all parameters and the significant advantage due to use of techniques like Adam. It would be informative to expand this section and present a more detailed analysis.  In figure 3 it is not clear how many wavelets were used in the WD layer and how this specific subset of 10 were chosen ?  