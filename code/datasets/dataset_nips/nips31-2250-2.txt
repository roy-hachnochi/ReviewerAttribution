The paper tackles a recently studied question in (parallel) optimization: robustness to adversarial gradients. It fits quite well in the growing body of work on the topic and presents an optimality results (although there is a significan overlap with what was proven by Yin et al in ICML 2018: https://arxiv.org/abs/1803.01498 )  Strength: optimality, information-theoretical limit, elegant (and easily verifiable) proofs.  Weakness:  - only valid for convex loss functions. Although I would fight for the relevance of convex optimization, in the context of poisoning attacks, the main challenges lies in what an attacker could do because the loss function is not convex. Not to mention that the state of the art models are non-convex (deep learning for instance), in this case, the solution of the authors not only cannot be used as is, but I doubt it has any valid adaptation: each quorum will keep optimizing far away from the others, each in a distinct valley of the loss function. The learned models by each quorum will therefore be impossible to aggregate in a useful manner.  -relevant to the previous point: complete absence of experimental evaluation (not even in the convex case), this could have illustrated the claimed speed guarantees, which are the central claims of the paper.  I enjoyed reading the proofs in the paper and found them interesting for their own sake. I would however be happier if the authors provided experimental illustration, and considered the more interesting/relevant case of non-convex optimization, or at least hinted at how one could transpose their solution there. So far I could only see the aforementioned limitation on quorum that makes it impossible to re-use this in practical non-convex ML.  In that sense, the conclusion of the paper is abruptly short, more efforts should be put by the authors there than a mere "Avenues for future work include exploring practical implementations of our algorithm".