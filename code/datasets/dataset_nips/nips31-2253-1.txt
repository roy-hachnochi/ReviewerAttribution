Summary: The authors address the important problem of efficient Thompson sampling (1) for regret minimization in logistic contextual bandits. While UCB and its various extensions have been analyzed in the literature, Thompson sampling has been shown to give good empirical performance in real-world settings. The typical Laplace approximation that is employed for sampling from the posterior distribution can lead to suboptimal arms is shown via simulations and real-world experiments. They ameliorate the situation by proposing to use Polya-Gamma augmented sampling which was recently introduced for Bayesian inference in logistic models (Polson et al 2013).  Comments: The problem is well-defined and the presentation is very clear with links to the relevant literature. The main contribution is the application of Polya-Gamma distributed variables for the latent variables of the Gaussian distributed parameter vector Theta to the problem of logistic contextual bandits. The experiments are convincing by using both the synthetic examples and real-world experiments.  Overall a good application paper and could have been strengthened if any of the next steps highlighted in the discussion section such as multinomial models were also tackled. My concern is the heavy reliance on previous work for improving the posterior sampling step and if it is offset by the improved results shown when applied to bandits.  (1) Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband and Zheng Wen (2018), “A Tutorial on Thompson Sampling”, Foundations and TrendsR in Machine Learning: Vol. 11, No. 1, pp 1–96. DOI: 10.1561/2200000070 