Overview:  This paper develops fast excess risk bounds for ERM and develops stochastic optimization algorithms for losses that (at the population level) satisfy the so-called "error bound condition". The error bound condition generalizes a particular inequality implied by strong convexity to 1) non-convex but still curved losses 2) convex losses that are not strongly convex but enjoy similar (possibly slower) growth around the optimum. The authors give 1) An ERM guarantee that interpolates between \sqrt{d/n} and d/n as a function of the exponent for which the EBC holds, and does not assume convexity and 2) A faster/optimistic rate that depends on the loss of the benchmark, but requires convexity in addition to EBC 3) A stochastic optimization routine matching the (not optimistic) guarantee 1). Lastly, they instantiate their guarantees for a few simple families of losses.  Originality and Significance:  My takeaway is that the results in this paper do not require huge changes in proof technique compared to previous results that assume strong convexity/convexity (eg van Erven et al. "Fast rates in statistical and online learning" for Theorem 1 and Zhang et al., "Empirical risk minimization for stochastic convex optimization: O(1/n)- and o(1/n^2)-type of risk bounds" for Theorem 2), but I do like that the paper is fairly comprehensive and I think it will probably serve as a useful reference point for future research. Some further coments:  * Theorem 2 is fairly restrictive since it has to assume convexity, but I see from the examples that there are indeed losses that satisfy EBC for theta > 0 yet are not strongly convex.  * Theorem 3: To make the paper more comprehensive it would be better if this result could be extended to L*-dependent regime studied in Theorem 2. Also, please add more discussion as to why adapting to the parameter theta is a major technical challenge,   Quality and Clarity:  Overall I found they paper to be reasonably well-written and fairly easy to follow, though the intro has quite a few typos so please give it another pass. I do have some minor comments: * Line 42: Please be more precise on your assumptions on data and loss for the rates you give for ERM and SA. ERM can also get \sqrt{1/n} also depending on the assumptions on the data norm. * The PLP setup / (12) should apply to piecewise linear *convex* functions, correct? * Ass 2: Specify the norm wrt Lipschitzness is defined. * Line 107: The result of Srebro you mention can easily be adapted to the unknown L* case using the doubling trick, no?  ------------------ After rebuttal: I am keeping my score, as the rebuttal only addresses minor technical comments and not the overall issue of novelty in analysis and proof techniques.