After reading the rebuttal: the authors addressed my concerns about orthogonality (a misunderstanding on my part) and the other minor issues I pointed out. My score of accept remains the same.  This work presents a multiple kernel learning approach to selecting explicit nonlinear features to approximately solve kernelized empirical risk minimization problems. This work is part of a line of recent work--- motivated by the promise of random feature maps to reduce the computational complexity of kernel methods--- to find low-dimensional explicit representations of hypothesis functions that are competitive with the implicit representations that rely on the computationally unwieldy kernel matrix.  The novelty of the work lies in that unlike most recent approaches, it presents a deterministic, rather than randomized, approach to selecting these explicit features. It does so by framing the problem as one of greedily selecting basis functions from each of several hypothesis RKHS spaces to reduce the current residual error. The authors show empirically that this approach can beat (in terms of test error), prior randomized algorithms for generating explicit features.   Strengths: - The framing of the problem as an OMP-like problem is new as far as I am aware, and may open a fruitful area for further work - The paper is clearly written - The theoretical analysis looks reasonable (I did not read the supplementary material)  Weaknesses: - This is not actually an OMP algorithm as the authors claim, because their algorithm selects for multiple orthogonal bases, instead of just one. This is worth mentioning since it means the intuition that we have from the analysis of OMP, that it will converge to a minimum, does not hold here  - The authors analyze their algorithm assuming that the basis functions are orthonormal, but it is not clear that the experiments conformed to this assumption: are the basis functions obtained from the Taylor Series expansion of the gaussian kernel orthonormal? The authors should make it clear either way. - The theory is provided for hard constraints on the mixture coefficients, while the experimental results are given for a lagrangian relaxation. It is possible to obtain theory that applies directly to the lagrangian relaxation - I would like to have seen a comparison with a fast randomized feature map such as FastFood: the authors compare to the basic random Fourier Feature map, which is much slower because it uses more iid random variables. I believe the practice is to use FastFood. - The graphs are illegible when the paper is printed in black and white. It would be preferable to use markers and different line styles to help avoid this issue.