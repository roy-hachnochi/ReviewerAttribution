Summary:   This paper presents a greedy feature selection algorithm (called MFGA) which sequentially selects features from a set of pre-defined kernel features. A generalization guarantee is given for the proposed algorithm. MFGA is shown to give better test performance than several kernel feature selection/reweighing algorithms on real datasets.  Strength:   Overall the paper is well-written and easy to follow. The proposed algorithm is simple to implement.  Weakness:  - The proof of Theorem 2 directly uses a previous result (i.e., Theorem 3 in the supplementary) given by Shalev-Shwartz, Srebro and Zhang’2010 (denoted as [SSZ10] in the following). However, there is a l2 norm constraint in the proposed MFGA algorithm, while there is no such constraint in the corresponding algorithm (the Fully Corrective Forward Greedy Selection Algorithm) proposed in [SSZ10]. The author should give justification of directly using the result in [SSZ10] to the proposed MFGA algorithm. Intuitively, the convergence rate of MFGA should also depend on the l2 norm constraint, however, the current result (i.e., Theorem 3 in the supplementary) does not have such dependence.  - Important references seems missing. Not sure if the authors are aware of the following related papers: Ian E.H Yen et al., “Sparse random features algorithm as coordinate descent in Hilbert space”, NIPS 2014; Brian Bullins et al., “Not-So-Random Features”, ICLR 2018. In particular, the empirical performance of method proposed in the “Not-So-Random Features” paper is claimed to be much better than the baseline algorithm LKRF chosen in the paper.  --------------------------After reading the rebuttal------------------------- I read the authors’ response. The authors have addressed my comments. Specifically, one of my concerns is on the missing dependency of the l2-norm bound in the proof of Thm2. The authors point it out this is because they *assume* that the optimal solution in each iteration of MFGA always lies in the interior of the l2 ball. Besides, the authors mentioned that the recent ICLR paper focuses on classification while this paper focuses on regression. It would be interesting to see how the two methods compare over classification tasks. Overall, I am happy to increase my score.