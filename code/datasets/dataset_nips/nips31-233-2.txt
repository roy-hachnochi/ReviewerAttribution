This paper proposes a simple attention-based method to capture global features with CNN-based models. The idea is to use attention mechanisms twice where the first one is to generate a set of global features and the second one is to attend the global features at each position. The experimental results are positive as expected.   The strength of the paper is the simple and clean formulation. I imagine that this would become a building block of many CNN-based models. The proposed method is well contrasted against other methods.  A possible weakness is that it is not extensively compared against SENet [10]. If SENet is good enough for most cases, the significance of the paper decreases since it means that the generalization is not such important in practice.  A minor comment:  Figure 1 could be improved a bit. There are n and #n which seem to be identical. Also, it looks the depth of Z should be m.