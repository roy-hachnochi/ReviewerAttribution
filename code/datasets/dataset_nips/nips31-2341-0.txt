The paper builds on the influential "Fairness Through Awareness" paper of Dwork et al., which provided a framework for learning non-discriminatory classifiers by requiring the the classifier to treat similar individuals similarly. This similarity was defined by a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand. This paper extends the results to the more realistic scenario when the entire metric is not known to the learner, while relaxing the requirement that all pairs of similar individuals be treated similarly. The paper provides both learning algorithms and hardness results for this model.  Fairness in machine learning is still a very young field, with many competing models and definitions, and more questions than answers. In this area, asking the right basic questions, usually in the form of fairness definitions, is still of crucial importance. The Dwork et al. paper is one of the original seminal papers of the area, but it had weaknesses, first and foremost the requirement of access to a task-specific similarity metric for individuals. This paper, by relaxing this assumption as well as the fairness constraints, extends the results of Dwork et al. to much more realistic and fine-grained scenarios. The new definitions in this paper are clever and thoughtful, and the reasoning behind them is explained clearly with examples and counterexamples. The theorems are nontrivial, stated precisely and explained clearly. Overall this is a high quality paper with significant novel contributions to the theory of fair learning.  The main weakness of the paper is the lack of empirical results. It seems that no experiments were done to validate the model on real-world data. It would be helpful to explain this lack of empirical results. In particular, if it is because this more realistic model is still not quite realistic enough to be used in practical applications, I would prefer that the authors honestly acknowledge this fact and discuss what the roadblocks are and what is needed for this framework to become applicable - whether it's further algorithmic advances or the availability of specific kinds of data.