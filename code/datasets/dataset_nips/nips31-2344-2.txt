This paper investigates methods that use approximate negative curvature directions to accelerate optimization of non-quadratic convex functions. The negative curvature direction is the eigenvector corresponding to the smallest eigenvalue, which for saddle points and concave points are negative. A difficulty in such problems is that finding this eigenvector requires repeated hessian/vector products via power method or Lanczos method, which is much more expensive than simple gradient methods. Therefore, a key focus of this paper is to study the convergence behavior when only noisy curvatures can be computed, e.g. Lanczos or some other algorithm terminated early.  The main novelty of the paper is that at each iteration, a noisy curvature is computed, and if the direction is "negative enough", that direction is chosen. If not, a standard gradient step is chosen. This allows for analysis of much noisier negative curvature steps.  Major concerns: There should be some empirical comparisons against known methods (so far I only see comparisons against variants if their own proposed methods).  To clarify, Lanczos methods ARE using repeated Hessian/vector products. The language in the main text seems to suggest the proposed method is doing something different, when in fact both this and Oja's method are doing basically the same thing.   Overall I think this is a nice paper with nice contributions (although I am not an expert at the latest work in this area) but the writing could be tightened up a bit, to ensure nothing is misleading or unclear.  Proofs:  The argument for proposition 1 is entirely circular; the result is basically restated in line 1 of the proof.   It's possible I'm missing something but for the proof of lemma 1: for clarification, add assumption that ||v||=1? also I think line after 61, the first equality should be inequality, and 8 should be 4. (minor)  Minor suggestions:  line 146: there should be a gradient sign  line 154: we don't really need z. It never features. 