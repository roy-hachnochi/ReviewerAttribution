This paper proves near-optimal regret bounds for Q-learning with UCB-bonus style exploration in the tabular, episodic MDP setting.  I really like how the algorithm is just Q-learning with a simple annealing learning rate and simple exploration bonus. It's much simpler than Delayed Q-learning.   ==== After Author Rebuttal ==== As I'm not as intimately familiar with the proof of UCBVI, I cannot determine how straightforward it would be to come up with a version of UCBVI that has reduced space complexity. I agree with the other authors that some claims are perhaps overblown; however I do think that the reduced space complexity is novel (unless I'm missing some other prior work). Even though this algorithm is not just Q-learning since it is using UCB exploration, I do think that showing the Q-learning-style updates can still result in efficient learning is interesting. I would tend to keep my decision of acceptance, but this could change as I am not confident in my judge of novelty.