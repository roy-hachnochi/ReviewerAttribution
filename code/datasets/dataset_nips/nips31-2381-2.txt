The paper proposes a new span prediction model for reading comprehension (RC) together with Bidirectional Attention Connectors (BAC) which densely connects pairwise layers (outputs of two RNN) in the model. The paper conduct experiments on 4 RC datasets and shows that their model provides state of the art performance on all the tasks.  The BAC generalizes matrix attention used often in RC models and outputs a low dimensional representation for each timestep (dim=3) employing factorization machines (from previous work) for the projection to lower dimension, which explicitly models the interaction of features in the input vector.  The paper presentation is clear (although math needs a lot of proofreading, see below), it presents the model, comments on the choice of the datasets, experimental setup, and also briefly describes the SOTA models for the RC tasks attempted. Moreover, the ablation study of features of the model suggests that all parts are important.  - The hidden layer sizes tried in tuning (up to 75 dim) seem a bit small, but the multi layer architecture seems to make up for that. Would be interesting to see less layers with larger hidden sizes (e.g. 128, 256) for comparison. - It would be nice if the ablation included using a simple linear projection instead of the factorization machines approach. - Eq. 2: the transpose might be incorrect since E is l_p x l_q, please double check this. - Line 117, H^P, H^Q in R (this is probably a vector) - Eq 5 quantifies over i,j but there is no i in the expression - Line 126: should Z^*_{kj} be Z^{kj}_* ? (* for p,q) to match Eq 5 notation. - Section 3.2 uses two types of the letter ‘l’, and l_p and l_q is used in previous sections for similar quantities, which is confusing. - Eq 6, extra parenthesis - Eq 12 has p^1 and p^2, should these be p^s, p^e as in Eq 11?