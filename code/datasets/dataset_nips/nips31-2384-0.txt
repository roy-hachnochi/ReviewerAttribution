 The paper studies the interesting problem of learning with externalities, in a multi-armed bandit (MAB) setting. The main idea is that there might be a bias in the preferences in the users arriving on on-line platforms. Specifically, future user arrivals on the on-line platforms are likely to have similar preferences to users who have previously accessed the same platform and were satisfied with the service.   Since some on-line platforms use MAB algorithms for optimizing their service, the authors propose the Balanced Exploration (BE) MAB algorithm, which has a structured exploration strategy that takes into account this potential "future user preference bias" (referred to as "positive externalities"). The bias in the preference of the users is translated directly into reward values specific to users arriving to on-line platform: out of the $m$ possible items/arms, each user has a preference for a subset of them (the reward for this being a Bernoulli reward with mean proportional to the popularity of the arm) and the rewards of all other arms will always be null. An analysis of the asymptotic behavior of the BE algorithm is given, as well as a comparison to the behavior of UCB and explore-then-exploit MAB strategies.    The paper is well written and the studied problem is interesting. The proofs seem sound. I also appreciate the analysis of the cost of being optimistic in the face of uncertainty (as in standard MAB strategies) when they hypothesis of having positive externalities is verified.   On the other hand, the motivation of the paper comes from real-world applications and the paper contains no empirical evaluation. It would have liked to observe the behavior of the algorithm on a dataset coming from platforms cited in the introduction of the paper or on some synthetic data. This would have been helpful to have a more thorough view on the practical significance of the contribution and on the type of bias that appears in user's preferences and its impact on the performance measure of recommendations/allocation strategies.   =======================  Post author response edit: I thank the authors for their response and clarifications, I also appreciate the added simulation results.    