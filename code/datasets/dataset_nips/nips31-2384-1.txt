Summary of the paper: The paper deals with a setting of stochastic multi-armed bandits with positive externalities. The setting is reasonably novel and the same is well motivated with the practical applications. First,  they have derived lower bounds on the expected regret of any policy in the setting considered. Second, they have shown that the classic UCB algorithm need not perform well in the considered setting and argued that it can incur linear regret. Then, they have considered another policy called Random-Explore-Then-Commit and showed that it is better than UCB in some cases but it can still incur linear regrets. Next, they proposed a policy called Balance-Exploration which does not require any system parameters and showed that this policy's performance is near to the achievable lower bounds. Finally, they proposed a policy called Balance-Exploration-with-Arm-Elimination which requires few system parameters and proved that this policy is indeed optimal.     Overall, the paper is well written and easy to follow. Providing some simulations would have been better in order to illustrate the linear regret behaviour of UCB and Random-Explore-Then-Commit policies. It would be great if they could provide proof sketches for their key results at least.   There are few typos in the appendix. For instance, the second equation in the Section 8.1 should be an inequality, and $\alpha$ should not be there in the following inequality, $N^{\alpha}_a(t-1) \leq \theta_a + (t - 1),$ in the same section.