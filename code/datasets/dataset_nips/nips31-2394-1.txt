Biological sequences which are close to each other in edit distance tend to have similar functional properties. The existing neural network-based solutions for biological applications have architectures that were originally proposed for image/speech processing applications and therefore do not leverage this property of biological sequences. The contributions of this paper are two-fold: (i) This paper proposes edit-invariant neural networks (EINNs), which have the ability to assign similar predictions to inputs that are close to each other in edit distance. (ii) This paper also shows some examples where deep CNNs with concatenations can handle edit operations well. The authors show numerical results for both these contributions on the protein secondary structure prediction problem.  Strengths: • EINNs are a novel contribution. While these networks are a lot slower than CNNs due to the underlying dynamic programming operations, they can be expected to work better in biological applications where there is very little labeled data to work with (which can be the case often) due to their potentially superior modeling power. • The adaptation of the NW algorithm to make everything differentiable is done nicely. The argument that the proposed filter using the NW algorithm is a generalization of the standard convolutional filter is explained well.    Weaknesses: • The numerical results shown in this work is extremely weak. o For the results shown in table 1, 2% of the available training data is used, and the accuracy improves from 42 to 43% when CNNs are replaced by EINNs. This is very unconvincing on two aspects: (i) The improvement is not sufficient enough to demonstrate the superiority of the EINNs. (ii) What happens when 1%, or 5%, or 10% of the training data is used? It would be better to show a curve here as opposed to just what happens at 2%. o The improvements shown in table 2 is very unconvincing as well. DenseNets were originally proposed to improve accuracies in image processing applications where edit distances are not used. An increase from 70.3% to 71.5%, using a more superior CNN with DenseNet-like concatenations, data augmentation, as well as multi-tasking by predicting solvent accessibilities is nice but hardly surprising. More precisely, I am not convinced that the improvement is due to better handling of edit operations which is the main purpose of this paper.   • The section on deep CNNs with concatenations is motivated poorly. In particular, the following are unclear to me: o Is the paper arguing that CNNs can handle edit operations well? If yes, then what is the need for EINNs? o Is the paper arguing that CNNs with concatenations can handle edit operations better than CNNs without concatenations? If so, a more concrete analysis is necessary. o The few specific examples are informative, but one would need concrete bounds on the number of filters/ size of the models required for CNNs with or without concatenations to be able to make strong statements about CNNs handling edit operations.  Minor comments: • Figure 3: “concatenation” is spelt incorrectly. • Figure 5: The legends need to be split into (a), (b) and (c) and explained in more detail. • Figure 5a: The architecture description is unclear. Specifically, what does the 32-convs represent? Isn’t the number on the top-left the number of filters?