This paper studies pairwise group comparison from a fixed pools of players. Based on the training data, both individual and interaction scores for each player can be learned based on a generalized linear model with potential low-rank factorization representation.   It is interesting to see that such a straightforward model outperforms both previous models [11] and [12] on the datasets that the authors have tested. But it would be nice to see the comparison with more recent methods.   For sample complexity analysis, assumably, the low-rank model (4) shall be better than the basic model (2). But it seems that the analysis is converted to analyzing (6), which essentially goes back to (2) with the norm constraints. It is not clear how meaningful such an analysis is. Also, there are quite a few notation inconsistency, typos, and other language errors in these parts, especially in supplement, which make these parts difficult to read.   Is the problem (5) convex? If not, how initialization of SGD is done? If the global optimum can not be guaranteed, is the presented sample complexity meaningful, again?   In experiments, for HotS Public (P), the basic model gets worse performance than simple logistic regression model, which seems to suggest that it is very possible that the interaction model can overfit. The authors probably should include the rank k for the factorization models for each data set. Finally, it is not really clear how Fig. 1 can reveal "interesting patterns" for player interactions. Even if the final model is rank-2, this can only tell whether the players complement each other by checking their inner product values.   By the authors' response, the problem is a generalized low-rank formulation. Assuming that the convergence analysis in the provided ICML 2017 reference is correct, it does address my concern on the sample complexity analysis.        