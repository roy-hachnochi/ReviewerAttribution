 This paper introduces a meta-unsupervised learning (MUL) framework for unsupervised learning, where knowledge from heterogeneous supervised datasets is used to learn unsupervised learning tasks like clustering. The main idea is simple, but the authors define it in the perspective of agnostic learning and discuss its connections to related topics in a principled way. Using the empirical risk minimization, this paper demonstrate MUL on several unsupervised problems such as choosing the number of clusters in clustering, removing the outliers in clustering, and deep similarity learning. In particular, this paper shows that  meta-clustering circumvents Kleinberg’s impossibility theorem by extending scale invariance to a meta-level. Experimental results show improvements over simple heuristic algorithms for the problems.   + The proposed MUL framework is novel and interesting to my knowledge.  + MUL is defined using agnostic learning and empirical risk minimization in a principled way.  + Practical MUL instances are discussed and experimentally demonstrated.  + The possibility theorem of meta-clustering is shown.  - Only simple baseline algorithms are considered in experiments.    I’ve enjoyed this paper and think MUL is interesting enough although the main idea can be viewed as a simple extension of meta-learning to unsupervised problems. This attempts may have been tried before. To my knowledge, however, the explicit formulation of MUL is novel and I don’t find any serious flaws in its analysis and discussions. Since I’m not an expert in this area, I would lower my confidence for now. But, I think this paper deserves publication.   [Post-rebuttal]  After reading other reviews as well as the rebuttal, I'm more confident that this paper is worth being presented in NIPS.  