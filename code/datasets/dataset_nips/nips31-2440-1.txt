This paper proposed an implementation of gradient descent by using stale gradient computation on some data. I found this method is useful and practical. This paper is well written and easy to understand.   My only concern is the connection between this paper and some recent works, e.g., [A] and [B], that study the convergence and speedup of asynchronous parallel SGD. To me, the proposed method and the followed analysis are just a special case of asynchronous SGD with data variance equal to zero (\sigma = 0). Authors are expected to highlight the novelty of the propped method over [A] and [B].  [A] Asynchronous stochastic gradient decent for non convex optimization [B] A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order   ================= Authors mostly addressed my concerns. Authors are also expected to compare / discuss the connection to SAG algorithm which is very similar to this paper except  assuming randomized sampling. 