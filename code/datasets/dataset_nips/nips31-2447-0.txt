General comments -------------  The paper is well written and propose a well motivated, although incremental, idea for improving robustness of matrix factorization with unknown values.  Details --------  Model + convergence ================== - The paper would be much easier to read with less inline equation, in particular in Proposition/Corollary. I think it would be beneficial to drop the \dot on the proposed algorithm, as they are not essential (you may just override the definitions provided for l1 loss)  - Although sparsyfying regularization are mentionned in the introduction, they are left undiscussed in the rest of the paper. Can the proposed algorithm accomodate eg l1 penalty on the term V ?  - The proposed algorithm involve APG in an inner loop: what is the criterion to stop this inner loop ? Even though the convergence rate of APG is better than ADMM, this is not guaranteed to provide faster convergence for the entire algorithm. This should be discussed more.  Experiments ----------- The only experiment performed on a dataset of reasonable size (Movielens 10M) shows bad performance: a well tuned l2 matrix factorization algorithm tuned with eg SGD or coordinate descent, performs under 0.79 RMSE on this dataset (see, among many, Mensch et al., Dictionary Learning for Massive Matrix Factorization). As such, the interest of robust non-convex losses is hardly clear on a real dataset --- and the baselines reported in the paper are wrong.  Of course, RMFNL does perform better on datasets generated to require robustness during fitting. However, this is hardly a valid argument to promote the use of new losses.