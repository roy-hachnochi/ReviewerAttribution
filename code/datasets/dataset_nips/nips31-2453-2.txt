SUMMARY  This paper addressed the problem of aggregating information from multiple camera views into a 3d tensor feature that keeps geometric consistency. The method uses the relative camera poses to align the unprojected feature tensors and combines them with GRU-based recurrent networks. This 3d tensor feature is used for instance segmentation, object classification, and voxel occupancy prediction. To select the most informative camera views, the authors used reinforcement learning (REINFORCE) to train a policy that selects the next adjacent view based on current states.  TECHNICAL QUALITY  I enjoyed reading this paper overall. Both the technical approach and the experimental results are clearly presented. The authors leveraged the structure of the active perception problem to bake in the 3D geometric consistency into the learned feature representations. This approach has outperformed previous approaches that do not take advantage of such geometric structure. Furthermore, it adopted a similar approach as in previous work, e.g., [14], to learn a policy to select best camera views.  However, this approach presents several limitations as follow:  1. The approach works only in low resolution, which might constrain its scalability in more complex scenes. In the experiments, the 3D scenes are voxelized into a size of 64x64x64. Accordingly, the unprojected feature tensor has a size of 64x64x64x7, which contains more than 1 million elements. Increasing the resolution would lead to a larger model size and GPU memory consumption, and a longer computation time. Looking at Fig. 4, even the ground-truth voxels roughly approximate the 3d meshes of two objects. Such low-resolution approximation is likely to lead to notable issues in cluttered scenes with a larger number of objects.  2. This approach adopted a segmentation embedding method to obtain instance segmentation masks. It used k-means algorithm on these embeddings to group the voxels into k-sets. This seems to suggest that the model knows the number of objects in the scene a priori? How would this model work when the number of objects is large and unknown?  Question about the technical details:  When training the geometry-aware visual memory, what are the action sequences being used for selecting the camera views? Did you train the visual memory, and train the view selection policy, and then alternate? Or just using a random policy when training the visual memory? If so, would it perform better if the visual memory and the view selection policy are trained iteratively?  EXPERIMENTAL VALIDATION  The quantitative and qualitative results have shown that the modelâ€™s performance has improved as information from additional views is aggregated into the visual memory. Qualitatively, the new views selected by the RL policy look sensible for gathering informative geometric information of the scene. I am particularly impressed by the chair example in Fig. 2 where the model learned to reconstruct the thin arms of the chair after seeing two to three views.  Questions about the experiments:  1. What is the intuition why the 3d reconstruction performance (Fig. 3) slightly dropped in the car category when the fourth view is added?  2. Can you also include curves of oracle view selection policy performance in Fig. 3, where the policy always selects the next camera view that improves the 3D reconstruction IoU the most (assuming known ground-truth)? This offers an insight into the upper bound performance of the visual memory independent from the view selection policy.  OVERALL  While some limitations of the proposed approach exist (as the authors also discussed in the last section of the paper), this work has offered a good attempt to learn 3D scene features with geometric consistency and has shown effective in simple domains for joint segmentation, classification, and reconstruction. My initial impression of this paper is positive.