 The paper proposes a method that explicitly separates relevant factors and irrelevant factors in a supervised learning task. The separation is unsupervised and combines several techniques to make it successful. It uses a reconstruction process to capture all information from the input.  However, the latent representation is split into relevant ones and irrelevant (nuisance) factors.  Only the relevant ones are used for prediction.  For reconstruction, the relevant features are corrupted by noise in order to make them less reliable. An important step seems to be to make both parts of the latent space independent with an adversarial training.  The paper is well written and the presentation is clear.  Strong aspects: - important topic - relatively simple architecture  - yields significant improvements  - interesting adversarial training setup for independence - no additional information needed  Weak aspects: - No standard deviations/statistics for the results given. How many independent runs did you do etc? - the comparison is mostly against CAI (probably the authors own prior work), except for the Yale-B and the Amazon Review task - the datasets/problems are relatively small/simple: Yale-B, Chairs, MNIST (with variants), Amazon Reviews. How does it perform on classification in the wild type, e.g. Imagenet or so? Do you expect any improvement or does it only work for fine-grained classification? - The weighting of the Loss terms introduces 2 hyperparameters, beta and gamma (however they were fixed for all experiments and rather generic).  - it is not mentioned that the code for the paper will be published   Line 211: Introduce z explicitly. E.g. as the "labeled nuisance factors"  To summarize: the paper is interesting and makes a contribution. I would have liked to see technically more serious reporting with statistics, which I expect to happen in the final version. Also, a large-scale problem would have been nice. 