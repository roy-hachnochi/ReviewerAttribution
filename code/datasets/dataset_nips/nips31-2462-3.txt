The paper studies the problem of convergence of agents' strategies to a Nash equilibrium where the agents play repeatedly the same game and adjust their strategies their according to a simple no-regret algorithm.  This is a very fundamental problem. The existing results focus convergence in the Cesaro's sense (i.e. convergence of the running average) to a correlated equilibrium. The current paper proves convergence in the standard sense, hence it is much stronger.  This paper proves convergence for a certain class of games (which authors call "variationally stable games") when the agents use online gradient descent.  The paper adds several bells and whistles to this basic result: Namely, the agents do not need to observe their reward in every round. The agents only observe the reward with certain probability. As long as the probability is known (at least approximately) and the probability is positive, the agents can use importance weighting to fix their learning rate.  I did not check the proofs.  ------  Suggestions for improvement:  1) It is puzzling the agents need to use the same step size. This fact seems to be crucial for the proof of convergence; otherwise there would be no point in importance weighting in Algorithm 3. Please give an intuitive explanation.  2) Definition 3.1: L has two arguments. You use only one. The appendix explains the issue; but that's too late and reader is only confused. Please explain your abuse of notation in the definition.  3) Definition 2. x_i --> a_i  4) Line 203: convext --> convex 