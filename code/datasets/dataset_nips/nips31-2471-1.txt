Summary: The authors solve an important NMF problem namely the symmetric NMF which arises in a wide variety of real world problems such as clustering in domains such as images, document analysis. They propose to rewrite the objective as a regular NMF problem with an additional regularization term of requiring the two matrix factors to be the same. This enables them to now apply standard NMF alternating update algorithms such as ANLS and HALS to solve the symmetric NMF problem. Real-world results are shown by experiments on CBCL, ORL, MNIST datasets which obtain qualitatively good results and also are pretty fast.  Comments: The problem is well-defined and the approach/results are clearly presented. Solving this efficiently is of great interest to the NMF community and also to ML in general because of its connections to other problems such as equivalence to K-means clustering.  How does this differ from the PALM (2) approach? It seems like the key novelty is proving that the alternate updates converges to a critical point which is the same for (2) and (3). Proposed algorithms do not have any proximal terms unlike previous approaches but it would be good to highlight why that is important --- in theory/practice. For instance expanding on the point of exploiting the sufficient decreasing property could be helpful.  Can we have other norms on the objective and or constraints on U and will the approach presented generalize to those settings? For instance like the sparse NMF problem considered in the PALM paper (say sparse symmetric NMF ). Also, it would be interesting to run the ``faster'' GCD algorithm (1).  This could potentially show your approach to be much more efficient and make it more compelling. Does the proof have to depend on each base algorithm or are there properties they need to satisfy so that we don't have to redo the proof each time for each new algorithm? Lambda is chosen to be 1, can you show the "theory" value for the data sets? The fact that your algorithm is robust against a wide variety of datasets is interesting.  Overall, I like the experimental results and theoretical contributions of the paper. I am a bit unclear with the novelty factor and hoping to clarify with further discussion.  Papers:  (1) Fast Coordinate Descent Methods with Variable Selection for Non-negative Matrix Factorization --- Cho-Jui Hsieh, Inderjit S. Dhillon KDD 2011  (2) Proximal Alternating Linearized Minimization for Nonconvex and Nonsmooth Problems --- Jerome Bolte, Shoham Sabach and Marc Teboulle 