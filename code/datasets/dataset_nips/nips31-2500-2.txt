 The paper proposes a performance metric for generative model evaluation. Different to Frechet Inception Distance (FID) and Inception Score (IS), which only give a single performance number, the proposed metric gives a precision--recall curve where the definition of precision and recall are tailored for comparing distributions. The precision measures the quality of generated samples, while the recall checks the number of modes in the original distribution that is covered by the learned distribution. This evaluation metric appears to be more informative than FID and IS.  Strength:  - The paper studies an important problem in generative model evaluation. A good evaluation metric can help the community better evaluate generative models.   - The idea of using precision-and-recall curve for evaluating generative model is novel. The idea is well-motivated and justified in the paper. The derivation of a simple approach to compute the PRD curve is also quite interesting.  Weakness:  - The fact that a closed-form solution for computing the PRD curve does not exist is a bit disappointing. Since the performance number depends on the K-mean clustering, which can give different results for different random seeds, it will be great if the authors could show the variation of the PRD curves in addition to the average. Also, the authors should analyze the impact of number of clusters used in the K-mean algorithm. Is the performance metric sensitive to the choice of  number of K-mean centers? What would be a reasonable number of clusters for a large dataset such as the ImageNet dataset.