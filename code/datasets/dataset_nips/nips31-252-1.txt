This paper characterizes the sample complexity of learning a CNN, unlike other works that analyze the convergence rate of optimization schemes such as gradient descent. To establish an upper bound of the expected population mean-square prediction error for a convolutional filter, the authors adopt a localized empirical process approach (van de Geer, 2000) instead of attempting a closed-form solution due to difficulties analyzing the distribution properties of the input. They follow standard information-theoretic approaches to establish a lower bound on the expected population mean error. With these results in hand, they consider a network with a single hidden convolutional layer and establish an upper bound that depends on parameter count of the network as well as filter stride. The lower bound for the 1-layer network is shown to scale on the parameter count. A small set of experiments comparing a convolutional filter to a similar fully connected parameterization show that error goes up as filter size increases for a stride of 1, which corresponds to the derived error bound. The theoretical bound in which stride is not a factor has empirical evidence in the experiment setting the stride equal to the filter size. The experiments in Figure 4 show that a 1-hidden layer CNN with similar number of parameters and stride=1 exhibits similar performance to a FNN, but shared parameters and increased stride widens the gap (CNN outperforms FNN).  Overall, I think the analysis is a helpful step towards understanding how CNNs learn more compact representations and require fewer samples for learning. It would have been nice to see some further empirical evidence - for example changing the input dimension or the type of data.  Explanation of the experiments in Figure 4 are unclear. First it is stated that filter size is fixed to m = 8. Then, further explanation states that filter size is chosen to be 4 and 8. Is this supposed to be the stride?  