Sample complexity for a linear convolution network based regression model is provided. Extension of the provided theoretical analysis for nonlinear models is not straight forward. In that sense, I would not refer to it as the sample complexity for a convolution neural network. Nevertheless, the theoretical results are valuable, possibly a stepping stone for deriving sample complexity on deep convolution neural networks in the future.  Here are some suggestions.  It may benefit to explain the difference between population and empirical loss.  The mathematics in the introduction section may be avoided, and put in some other section.  What are the connections, if any, between the sample complexity and optimization algorithm such as stochastic gradient ? For deep convolution neural networks, sample complexity analysis, explicitly accounting for the details of an optimization approach, could be beneficial and of high practical value.   In Figure 4, the range for the number of training data is too short.   For experiments if not theory, deep convolution neural networks should also be considered in the comparison.  ------------------------------------------ I have read the rebuttal. While keeping same scores, I am in the favor of accepting this paper.  I have increased my score by one point after a discussion with the other reviewers. 