 The paper studies linear regression and top eigenvector computation. In particular, the focus of the work is improving the computational complexity by exploring the structure in the data. It is shown that if the data points are approximately sparse, the time complexity can be improved from O(d) to O(s), where "d" is the ambient dimension and "s" characterizes the sparsity. The major technique used is combining stochastic variance reduction gradient (SVRG) and coordinate sampling.  ##Detailed Comments##  - The paper is well-written and appears interesting. It turns out that the main contribution is an analysis of combining coordinate sampling and SVRG. Is this a new sampling technique, or has been studied elsewhere?  - The paper considers very special problems, i.e. eigenvector computation and linear regression, which does offer a possibility of leveraging the structure of data. Can these results be extended to other problems, say a general optimization program?  - From middle column of Table 1, the running time of this paper has a better dependence on the dimension, but at the same time depends on "sum ||a_i||^2" which can result in a worse complexity. The appearance of "lambda_1" also seems unusual. Same issue in Table 2.  - Authors introduce too many notations that make the paper hard to follow.  I have read the author response.  I believe this is an interesting paper, but my concerns are not addressed in the author feedback.  My first concern is the generality of the proposed method. It turns out that the technique applies only to quadratic loss functions, and in the feedback authors cannot point out applications beyond it.  The second concern is the significance of theoretical results. The obtained computational complexity improves upon previous one in very limited regime (e.g. numerical sparsity is constant, hard sparsity is as large as dimension, condition number is d^2). It is not clear to me under which statistical setting can these assumptions be satisfied.