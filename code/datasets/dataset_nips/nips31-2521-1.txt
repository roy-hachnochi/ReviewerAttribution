This paper proposes algorithms for faster run times for regression and top eigenvector computation of numerically sparse matrices. The authors show that improvements are achieved even when row norms and numerical sparsities are non-uniform and demonstrate that run times depend on input size and natural numerical measures of the matrix (e.g., eigenvalues and l-p norms).  The problem of accelerating runtimes of regression and top eigenvector computation is relevant as data sets grow even larger and scaling up becomes mandatory for real-world application of machine learning. The problem can be reduced to a general  finite sum optimization of minimizing a convex function f, decomposed into a sum of m functions (f_1, .., f_m). Past work has focused on `improving the dependence on the number of gradient evaluations of f_i that need to be performed (improving the dependence on m and other parameters). Here, the authors focus on what structural assumptions on f_i allow faster run times, i.e., by computing gradients of f_i approximately. They apply coordinate sampling techniques to stochastic variance reduced gradient descent (SVRG) to reach their results. Subsampling of the data matrix's row entries is done; however, this can change the shape of the variance, and the authors admit to this inherent issue. To mitigate this issue, they provide subsampling techniques to ensure a decrease in l2 error for small increases in samples taken per row.  I have read the authors' feedback and thank them for their comments.