The paper proposes a method for solving least squares and top eigenvalue computation problems asymptotically faster than alternative approaches for numerically sparse matrices by proposing a new sampling method to approximate different quantities. My main problem with this paper is comparison to other approaches. I would love to see memory asymptotics, experimental comparison to see the constants involved, and comparison with Lanczos method (which is hard to analyse but which works well in practice). Also, line 239 says solving system with B is slow because of the condition number, and the condition number indeed get worse, but since one doesn’t have to solve the system exactly (see inexact inverse iteration) the time to solve it can still be ok. Plus, one can consider Jacobi-Davidson method which avoids this problem. Also, applying the method to A^t A leads to lost precision, it’s better to consider the augmented matrix [0 A A^T 0]  UPD: I see that you focus on the theoretical side and propose a method with best worst case guarantees in this particular case. I personally would prefer to experimentally compare the method with the baselines anyway, but if others are OK without experiments, I would not object accepting the paper.