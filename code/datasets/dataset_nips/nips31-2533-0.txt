Response to the rebuttal: Thanks for the useful clarifications and new experiments. I have increased my score rom a 4 to a 7. The main concern in my original review was that the reward signal was switched from dense to sparse rewards for evaluation but I am now convinced that it's a reasonable domain for analysis. I think an explicit discussion on switching the reward signal would be useful to include in the final version of the paper.  ---  This paper proposes a way of using MAML with policies in RL that have a stochastic latent state. They show how to use MAML to update the distribution of the latent state in addition to using standard MAML to update the parameters of the policy. They empirically show that these policies learn a reasonable exploration policy in sparse-reward manipulation and locomation domains.  I like the semantics of using MAML to update the distribution of a policy's stochastic latent state. I see this as an implicit and tractable way of representing distributions over states that are worth exploring without needing to explicitly represent these distributions, which can quickly become intractable.  I am not sure how to interpret the empirical results because if I understand correctly, all of the methods are trained in the dense reward setting (Fig 2 of the appendix) and then all of the results and analysis in the main paper use the sparse reward setting for validation, which are out-of-distribution tasks. When training in the dense reward setting, MAML outperforms the proposed method (MAESN) but in the sparse reward setting for validation, MAESN outperforms MAML. The paper claims that succeeding in the out-of-distribution sparse reward setting shows that MAESN is better than MAML at exploring and that MAML does not learn how to explore. It is not clear that out-of-distribution performance is indicative of explorative behavior or if it's just showing that a different policy is learned. I think this is especially important to consider and further analyze because MAML slightly outperforms MAESN for the training tasks with dense reward. I would be more convinced if the paper showed an experiment where MAESN outperforms MAML for training in a more complex task that requires exploration.  Also, the reason training was done in the dense reward setting is because none of the methods are able to be trained from scratch in the sparse reward setting as shown in Figure 2b. Have you considered using the trained models from the dense reward domain to start training in the sparse reward domain?