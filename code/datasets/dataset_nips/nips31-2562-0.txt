Thanks to the authors for the detailed response. The new results presented in the rebuttal are indeed convincing, hence I am updating my score to an 8 now. This is with the understanding that these would be incorporated in the revised version of the paper. ========================================= Several works in the last year have explored using hyperbolic representations for data which exhibits hierarchical latent structure. Some promising results on the efficiency of these representations at capturing hierarchical relationships have been shown, most notably by Nickel & Kiela (Nips, 2017). However one big hindrance for utilizing them so far is the lack of deep neural network models which can consume these representations as input for some other downstream task. This paper attempts to solve exactly this important problem.  Specifically the paper starts from the Poincare ball model of hyperbolic spaces and developes several standard neural network operations / layers for transformations data lying on this manifold. Section 2 presents the basic definitions of addition and multiplication from the theory of gyrovector spaces and mobius transformations, and derives versions of these for the Poincare ball. Then in section 3 these basic operations are extended to derive the following neural network layers -- (1) multi-class logistic regression, (2) feed forward layers, (3) recurrent neural networks (as well as Gated Recurrent Units). While I am not an expert in Riemannian geometry, the formulations all seem sound. All operations are derived in terms of the curvature, which can be smoothly varied from the Euclidean to Hyperbolic case, however the possible implications of doing so are not discussed.  Strengths: 1. The paper tackles an important problem and provides theoretically sound solutions. The results presented here would be highly useful to others working on hyperbolic embedding spaces. 2. The approach is technically sound, and builds on previous theoretical work on gyrovector spaces to provide mathematically sound formulations of hyperbolic neural network layers. This is in contrast to ad-hoc methods which may work empirically but without any clear justification. 3. While the paper is math-heavy, it is well written and the main arguments and claims are easy to follow.  Weaknesses: 1. The biggest weakness is that there is little empirical validation provided for the constructed methods. A single table presents some mixed results where in some cases hyperbolic networks perform better and in others their euclidean counterparts or a mixture of the two work best. It seems that more work is needed to clearly understand how powerful the proposed hyperbolic neural networks are. 2. The experimental setup, tasks, and other details are also moved to the appendix which makes it hard to interpret this anyway. I would suggest moving some of these details back in and moving some background from Section 2 to the appendix instead. 3. The tasks studied in the experiments section (textual entailment, and a constructed prefix detection task) also fail to provide any insight on when / how the hyperbolic layers might be useful. Perhaps more thought could have been given to constructing a synthetic task which can clearly show the benefits of using such layers.  In summary, the theoretical contributions of the paper are significant and would foster more exciting research in this nascent field. However, though it is not the central focus of the paper, the experiments carried out are unconvincing.