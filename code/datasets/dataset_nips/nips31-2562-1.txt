This paper takes a principled approach to adapting many of the core operations of neural networks to hyperbolic space. This generalization is done by analogy to the structure of gyro-addition, which gives a convenient pattern for adapting an arbitrary euclidean operation to hyperbolic points while preserving several desirable properties.  In particular, this analogy is applied to adapt matrix multiplication, bias addition, and scalar multiplication to hyperbolic space, which, along with a more involved treatment of multinomial logistic regression, give the tools needed to build neural networks.  The authors apply this adaptation to hyperbolic GRUs applied to simple tasks.  The weakest part of this paper by far is the experiments. The only tasks where the proposed methods outperform the baselines are synthetic.  There are also no experiments with hyperbolic MLPs, which I found quite surprising.  Most of the paper is devoted to the development of the basic machinery required to build an MLP and I expected to see some experiments in this simple setting before launching into an investigation of recurrent networks (which seem to not work particularly well?)  In spite of the disappointing empirical work, I think this paper should still be accepted.  Hyperbolic representations are getting a lot of attention recently, and the framework presented in this paper is by far the clearest and most complete application of hyperbolic geometry in neural networks to date.  I expect the framework in this paper to be foundational to future applications of hyperbolic geometry to neural networks, to the extent that it has a role to play beyond embedding models.  Detailed questions:  Why work in the Poincare model instead of one of the other available models?  It requires quite some work to derive the exponential and logarithmic maps in this setting, why not work in a model (like the hyperboloid) where they are more easily computed?  It would be nice to have some comment on why Poincare (or if it is an arbitrary choice then some indication of that).  In Equation 22, what exactly does <-p +_c x, a> mean?  As I understand, a \in T_pD and -p +_c x \in D, so what is the <> operator?  Have you considered non-linearities that are not derived as the mobius version of standard non-linearities?  Perhaps this is worth exploring in light of the comment after Eq 28 in order to make the resulting networks "more hyperbolic"?  Following that can you comment on when transforming f(x) to exp(f(log(x)) would not be appropriate? For example, if f(x1, x2,...) is a weighted midpoint operation will exp(f(log(x1), log(x2), â€¦)) compute a hyperbolic weighted midpoint? Is there some characterization or guidance on what properties will and will not be preserved by this analogy?  ---  After author response:  I am happy with the author response, and I still believe this paper should be accepted. 