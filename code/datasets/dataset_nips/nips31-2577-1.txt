This paper proposes a method for learning specifications (binary non-Markovian rewards defined as predicates over trajectories) from demonstrations. The motivation is to address some problems with reward function design, specifically to provide a better way combine rewards for different subtasks. They formally specify the problem of inferring specifications from demonstrations, define an algorithm inspired by Maximum Entropy IRL, and present theoretical results and experiments. The paper is interesting, clearly written and relatively well-motivated. Overall, I think the method has strong theoretical grounding but weak experimental support.   The experiment section does not contain all the experiments and details needed to back up the claims of the paper (and neither does the supplementary material). They test MaxEnt IRL and the proposed specification inference algorithm on a gridworld with non-Markovian dependencies, where the agent needs to enter a drying tile after stepping into water before going to the goal. Figure 1 shows rewards inferred by MaxEnt IRL for the two subtasks and the whole task, and the text describes the behavior of agents optimizing these subtask rewards or various combinations of these rewards, but does not mention how the agent optimizing the reward inferred for the whole task (Figure 1b) would behave. The paper claims that the rewards learned using IRL would lead to unsafe behavior, but does not include the corresponding experimental results.   The paper shows results for specifications learned by the proposed algorithm, but not for the agent behavior that results from optimizing for these specifications. No details are given on the type of agent used in these experiments. The paper states that "we demonstrate how learning specifications can help avoid common reward hacking bugs", but there are no experimental results for an agent using the learned specifications to support this claim. I also think the authors are using a nonstandard definition of "reward hacking", which includes any agent behavior that results from optimizing a poorly designed reward function, while the standard definition (given in Amodei et al, 2016) refers to the agent finding a loophole in an otherwise well designed reward function. It would be good to see experimental results on environments designed to test for reward hacking, such as the "boat race" environment from the AI Safety Gridworlds suite (Leike et al, 2017).  UPDATE after reading the author feedback: - I'm happy to see that most of the missing details for the experiment have been provided. - The argument for why this method would help avoid reward hacking in the boat race scenario seems convincing. I would still like to see experimental results on this environment, which should be easy to obtain. - Overall, the rebuttal has addressed most of my concerns, so I have increased my score.