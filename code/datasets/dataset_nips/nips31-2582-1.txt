The paper provides tight bounds on the performance of dimension reduction using (very) sparse matrices on non-worst case input. In general, the performance of dimension reduction using linear maps has been analyzed but the worst case instances are vectors that are extremely sparse. In previous work by Weinberger et al., they focused on the setting where the maximum coordinate value is much smaller than the vector length i.e. the vector is not too sparse. This is motivated by the fact that input vectors e.g. document-word incident matrices are not pathological and they are fairly spread-out. The speed of dimension reduction is proportional to the sparsity of the embedding matrix. In the worst case, the embedding matrix has to be reasonably dense. However for the setting of Weinberger et al., one can use even matrices with 1 non-zero per column, which is the minimum possible sparsity. This paper gives tight bounds for the relation between how well-distributed the input vector x is, the target dimensions, the distortion of the length, and the failure probability. As this tradeoff involves many parameters, the precise relation is quite complicated.  The result follows from a careful analysis of the high moment of the error |||Ax||-||x|||. The basic approach is classical: one expands the expression for the high moment and count the number of terms. The terms can be associated with graphs and the problem becomes counting graphs (with the indices of the coordinates as vertices). Getting accurate bounds on the number of graphs is complicated and I have not checked the details here.  Overall, this paper establishes tight bounds for a complicated tradeoff for an interesting problem. A downside is that the experiments are done only with synthetic vectors. It would be interesting to see if the theory here is predictive of the performance on real datasets such as the settings considered by Weinberger et al.