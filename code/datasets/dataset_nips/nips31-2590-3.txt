  This paper proposes to use evolutionary strategy based meta-learning approach to learn a loss function for gradient-based reinforcement learning tasks. Before policy optimization, this algorithm uses evolutionary strategy to optimize a temporal convolutional network which generates loss function to train policies over the distribution of environments. After the meta training, the agent using the learned loss function out-performs an agent using PPO. The authors also demonstrate the advantages of this approach comparing with other meta-learning algorithms such as MAML or RL^2.  pros: - The paper is well-written and the figures are helpful for understanding. - No reward during test time; - Simple, and using evolutionary strategy as a tool for meta-learning with RL tasks opens new directions. - The proposed algorithm is well verified in the experiment section, and the details are explained in the appendix.  cons: - Although the training process is parallelized, it still requires to use large samples to learn a loss function that generalizes. - It is not clear whether this method can be utilized for real-world robotic learning tasks. - The generalization ability of this method should be further investigated, such as how well the loss generalizes acrosses domains. 