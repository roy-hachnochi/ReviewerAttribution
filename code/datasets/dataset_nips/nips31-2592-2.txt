The authors investigate the impact of local curvature of the loss surface to model optimization. They do this by computing the eigenvalues of the Fisher information matrix in the context of a neural network with a single hidden layer with Gaussian data and weights. The authors find that nonlinear networks are better conditioned than linear networks and it is possible to tune the non-linearity in such a way to speed up first-order optimization.  The paper is well written with a lot of attention to detail. The derivation is well explained and the figures are useful. The empirical evaluation is interesting and novel.  In general I think the paper is a useful extension of Pennington/Worah (2017) and a contribution to the conference.  My main concern is that the empirical evaluation is limited. The analysis of the nonlinearity is interesting, but limited in scope. I would be curious if the method can also give insight into optimizers (e.g. momentum, learning rate, or fancier such as adam).  Nit: Line 93: spurious "a" 