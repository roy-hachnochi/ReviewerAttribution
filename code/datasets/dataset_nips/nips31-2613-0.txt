Summary =======  The authors present a reinforcement learning technique based on importance sampling. A theoretical analysis is performed that shows how the importance sampling approach affect the upper bound of the expected performance of the target policy using samples from a behavioral policy. The authors propose a surrogate objective function that explicitly mitigates the variance of the policy update due to IS. Two algorithms are proposed based on natural gradients for control-based (learning low-level policy) and parameter-based problems (discrete low-level controller with stochastic upper-level policy). The algorithms were tested on standard control tasks and are compared to state of the art methods.  Major comments =============  The authors investigate a rather general problem in modern reinforcement learning, that is, how we can efficiently reuse previous samples for off-policy reinforcement learning methods. The importance sampling (and similar) approach has been often used in the literature, but the actual impact on policy update variance have not been explicitly addressed in a (surrogate) objective function. The authors give a thorough theoretical investigation and provide two efficient algorithms for control and parameter based learning problems.  The clarity is superb, the paper is easy to follow, while some details are difficult to understand without reading the supplementary material. I particularly liked the presentation of the related work from the aspect of control, or parameter based policy learning. The evaluation of the results is also impressive and the authors do not shy away of pointing out shortcomings of the proposed algorithms.  Another aspect would be interesting to highlight is scalability w.r.t. the problem dimensionality. The tested methods are rather low dimensional, but sufficient for comparison to related methods.  Minor comments =============  Is the assumption on constraining the reward between Rmin and Rmax really necessary? Does it have to be known explicitly?  Response to Author Feedback =======================  The questions and suggestions have been properly addressed in the author feedback. 