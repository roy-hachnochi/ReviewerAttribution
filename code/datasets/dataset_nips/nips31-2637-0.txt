This paper proposes a variant of community exploration problem in which the explorer allocates limited budget to explore communities so as to maximize the number of members he/she could meet. The authors provide a complete and systematic study of different settings (offline optimization vs. online learning, non-adaptive learning vs. adaptive learning) of the community exploration problem. The authors prove that, for offline learning, greedy policy obtains an optimal budget allocation in both non-adaptive and adaptive setting. For online learning, the key is to estimate the community sizes. With multi-round explorations, the authors propose a combinatorial lower confidence bound (CLCB) algorithm that achieves the logarithmic regret bounds. Furthermore, by combining the feedback from different rounds (full information feedback), the algorithm can achieve a constant regret bound.  Pros: This paper is clear and easy to follow. I spent much time going through all proofs in appendix and don't see any major errors. One contribution of this paper is that the authors propose and formulate the community exploration problem in rigorous mathematical language. As for algorithmic part, I especially like how systematic the paper solves community exploration problem. The authors discuss various scenarios and give full and complete analysis. The conclusions are insightful. Usually for adaptive submodular problem, a greedy policy can only achieve (1-1/epsilon) approximation. Here the analysis show greedy policy is guaranteed optimal for this type of problems.  Cons: A minor drawback of this paper is that it lacks experimental verification. For a simulated setting, I would not be surprised if the experiments match exactly the theoretical bounds. It may be more of interest to see a real-world application. However, given the paper's solid theoretical contributions, missing experiments is acceptable. I believe this paper qualifies for NIPS requirement. 