The paper presents a novel decoding strategy for seq2seq models, which starts from the middle of the output and goes both left and right. To make the left and right branches consistent the authors suggest to add attention over both left and right states and outputs. The demonstrate their approach on a synthetic task and on a youtube (YT) to captions task and show promising results. On the YT task they use a simple predictor to decide on the first word and show that there result is competitive with a strong baseline. Moreover - they show that if the first word is known (oracle) they get nice gains over a suitable baseline. The method is motivated by 1. Need to enforce a particular word in the middle of the output. 2. Dealing with (somewhat) symmetric sequences.  The paper is clearly written. The approach is novel, the formulation is sound and the experiments are convincing. This is a natural tool and I believe it is of interest for the seq2seq compunity.   