## Summary of paper  The authors provide a novel analysis of a minor variant of the variance reduced algorithm (SVRG) for the composite setting--proxSVRG+. The main contribution is a new simpler analysis which recovers most previous convergence rates, and even improve upon them in certain regimes of the data-size and accuracy required. Further, they analyze the case when arbitrary batch sizes are used at each iteration of prox-SVRG instead of the full batch, and theoretically show that smaller batches can be optimal when the variance of the stochastic gradient is small and we do not require a high accuracy solution.  ## Significane of results  This work has results which may perhaps be significant for practitioners--i) the smaller mini-batch requirement, and ii) adaptivity to strong convexity. Theoretically, though the authors unify a number of previous results, they do not recover the rates by the SCSG when $h=0$ indicating that the prox-SVRG algorithm requires further study.  ## The positives  1. The paper is well written and is easy to read--the authors make an effort explaining previous result and how their result compares to related work (e.g. pp 3) 2. The analysis indeed seems simpler (or is at-least more *standard*) than previous works. 3. The extension to the case when a smaller batch (B) is used seems very useful in practice. The authors confirm this experimentally where they show that the optimal minibatch size of their algorithm is indeed small. 4. Further the authors show that the algorithm is adaptive to strong convexity (the PL-condition), which is important for faster convergence in practice.  ## Negatives  1. The metric used to define accuracy (Def 1) is not algorithm independent! In particular it depends on the step-size the particular algorithm uses. This means that technically the results comparing different algorithms are meaningless because the error metric used is different. 2. The experimental section lacks a lot of very important details--i) what iterate is being compared in the different algorithms? For the prox-SVRG+, is it the randomly chosen one for which the theoretical results are proved? How is the $B$ parameter set? Since the 'default' parameters without tuning are used for the other algorithms, it would be an unfair comparison if the $B$ parameter was tuned for the current algorithm.  Other minor points: 3. In Fig 1. Natasha is not plotted, so harder to compare. 4. Lemma 2 in the remark (143, pp 5) is mentioned but not stated. In general comments such as ones mentioned below in the Remark on pp. 5 seem vacuous and could potentially be improved to provide meaningful insights into the proof technique.  "This is made possible by tightening the inequalities in several places, e.g., using Youngâ€™s inequality on different terms and applying Lemma 2 in a nontrivial way"   ## What would increase my score  1. Clarify how error as defined in Def. 1 is a meaningful metric to use for comparison.  I understand past works may have used it without a second thought, but I find this unsatisfying. 2. Demonstrate that the comparisons performed in the experiments were indeed fair (wrt B)  ## Edits after rebuttal  1. The authors do a good job of addressing the meaningfulness of the metric. It would be nice if this discussion in the rebuttal is added to the paper too. 2. The fact that B is set to n/5 and need not be tuned is enough to convince me that the experiments are valid. I would however appreciate more details about the experimental setup in the final version to improve the replicability. 3. The rates of SCSG are indeed recovered by ProxSVRG+--thanks to the authors for pointing out my oversight.