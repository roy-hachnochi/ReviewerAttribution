This paper proposes a generative model for multi-modal data, i.e. different aspects of the same underlying "concept" (image and its caption). The modalities are considered conditional independent given a latent variable z, trained with a VAE-type loss by approximating the posterior distribution p(z|x_1, ..., x_N), where x is a modality. The novelty of the paper is to show that this posterior can be a PoE (product-of-expert) distribution which have a closed form solution in case of multivariate Gaussians and, as the authors show, is a reasonable approximation of the true posterior (itself a product of posteriors corresponding to each modality). The use of the PoE make the model scale to multiple modalities, without having to train a posterior for each combination of observed modalities. The authors show that their proposed posterior approximation is robust when only some of the examples contained all the modalities in the joint (weak supervision).  Pros: - The paper reads very well and the math is sound. The authors do a good job in explaining the model and the contributions. - The proposed solution to make approximate posterior scalable is simple and seems as least as effective as other models.  Cons: - I feel that the novelty of the paper (the approximate posterior q(z|X) is a product of experts of the posteriors for each modality \prod_i q(z|x_i)) is rather small. - From Table 2, I cannot see a clear difference between the proposed approaches. The reported LL scores are very close to each other. - It would have been interesting to have experiments in more realistic settings that could better highlight the usefulness / attractiveness of the general problem, i.e. images and captions in different languages , maybe ?  -- After rebuttal  I thank the authors for the additional effort in making a strong rebuttal. This has contributed to reinforce the contribution as well as to give a nice example on how this method could be applicable in more realistic setting. Therefore, I am willing to increase the overall score.