Summary of the paper:  This paper presents a training procedure to increase a network's robustness to adversarial attacks. The main idea is that the normal cross-entropy loss function is augmented with another loss term penalizing low norms of the adversarial perturbation (such as computed by "DeepFool" or the "fast gradient sign" method) for the training data. A slightly more sophisticated version of the adversarial loss is actually used where for correctly classified training examples low norms of the perturbation are penalized while for incorrectly classified training examples high norms of the perturbation are penalized.   + Convincing results are obtained. The training regime presented produces a better defence to FGS adversarial examples on MNIST and CIFAR-10 datasets then other defensive schemes such as Parseval networks, extending the training set with DeepFool adversarial examples. The regime also increases the Though the boost becomes less pronounced for the more sophisticated networks. The introduced training regime also gives a slighter bigger bump in test accuracy on the benign set (when updating a pre-trained model using fine-tuning with the augmented loss function) than the those achieved by the other training regimes.  + The paper presents a straight-forward and sensible idea to help make a network more robust to common adversarial attacks while at the same time not affecting accuracy on the benign set.   - The details of how the "adversarial" loss is implemented are a bit unclear to me from reading section 3.3 and examining figure 1. Is the exact DeepFool procedure for constructing an adversarial perturbation replicated or is it approximated? The authors say they build a recursive regularization network which, I think, emulates calculating one update step in the DeepFool method for computing the adversarial perturbation. Then is this network recursively applied until an adversarial example is found for each training example at each update step?  - There are missing details on the effect on the new loss on the practicalities of training. How much slower does the introduced training regime make one epoch of training? On average how many updates are required to generate an adversarial perturbation for a training example? How is the convergence of the training effected etc?  - The practical details of some aspects of training are missing some details. When computing the adversarial loss for training example how is the target class chosen for the adversarial example chosen?  And does this approach differ for the correctly and the incorrectly classified training examples... Without these details it would be hard to reproduce the results.    A summary of my rationale for the paper is that the idea in the paper is nice (not revolutionary but nice) and produces good results. I have some problems with the presentation and clarity of the technical detail. However, I think these could be relatively easily rectified.   Comment:  I think the authors should include elements of Table 2 in the supplementary material into the main paper. There is a clall in the adversarial community to training regime's robustness to other tougher adversarial attacks than FSG (ie "Bypassing ten detection methods" paper of Carlini and Wagner!) and this table is more in the spirit of this call then the table 1.