Summary: This paper is an extension of the results presented in “A Consistent Regularization Approach for Structured Prediction” by Ciliberto et al. It focuses on the specific case where the output space is a Riemannian manifold, and describes/proves sufficient conditions for loss functions defined over manifolds to have the properties of what is called a  “Structure Encoding Loss Function” (SELF). Ciliberto et al presents an estimator that, when used with a SELF, has provable universal consistency and learning rates; this paper extends this estimator and these prior theoretical results to be used also with the aforementioned class of loss functions defined over manifolds, with a specific focus placed on the squared geodesic distance. After describing how inference can be achieved using the previously defined estimator for the specific output spaces defined here, experiments are run on a synthetic dataset with the goal of learning the inverse function over the set of positive-definite matrices and a real dataset consisting of fingerprint reconstruction.   Comments: This work is well-written and well-organized, and it is easy to follow all of the concepts being presented. Furthermore, its primary achievement - further characterizing a set of output spaces/functions for which previous methods can be applied - provides a useful extension of that work.  The primary criticism I have of this paper is that it is somewhat incremental in nature - the SELF framework, corresponding consistency and learning rate results, and estimator are all borrowed from Ciliberto et. al. In contrast, all this paper does is describe one set of output spaces/losses for which those previous results can be applied. This might be more forgivable if it was then shown that these methods, applied to many examples of problems satisfying the required assumptions/properties, prove themselves to be very useful. However, though the experiments provide interesting preliminary results, only one task consisting of real data was used to demonstrate the utility of the described approach. At the very least, more examples need to be provided of tasks whose output spaces satisfy assumptions 1 and 2 that are not discrete or euclidean - but it would be preferred if more complete experimentation was done for more tasks.  Miscellaneous: -Line 60: “we consider a structured prediction approach to ma following…” -Line 82: “section section 3”  With further experimentation on a variety of tasks on which other methods may struggle, this paper would make an excellent contribution. As it is currently, however, I think it is a borderline case. I am leaning towards the side of accepting it since everything that is presented is useful and interesting.  POST AUTHOR RESPONSE UPDATE: Thanks for the response. The comparison against the other baseline definitely improves the quality of the synthetic experiment. Additionally, I am glad that you are able to provide another example that will clarify the utility of the proposed methods. Regardless, the point still stands that your experimental comparisons are somewhat weak. If were able to provide interesting results for this new task, then I might have increased your score, but as the work stands I feel like the score I originally gave is still appropriate.  