This paper proposes fast approximation of the maximum a posteriori (MAP) inference for determinantal point processes (DPPs), which is NP-hard problem. The submodular nature allows greedy algorithm to approximate DPP MAP, but its direct computation has expensive complexity. In this work, authors provide efficient greedy DPP MAP inference algorithm using Cholesky factored properties. In other words, every greedy update requires marginal gain of log-determinant and this can be exactly computed using only vector inner products. Therefore, the computational complexity can be improved to O(M^3) for given M ground sets (the na√Øve implementation is O(M^5)). Practically, it runs much faster than the state-of-the-art DPP MAP inference approximations. Authors also provide the algorithm for diverse sequence using sliding window approach without hurt of algorithm complexity. In their experiments, the proposed algorithms show the best results on sequence recommendation under real-world dataset.  In overall, this paper is very nice work as it adopts a novel idea for fast and efficient computations and shows better practical speedup and accuracy than the state-of-the-art algorithm. I believe that this paper is enough to accept to NIPS.  One drawback is a lack of rigorous analysis. For example, authors could mention at least approximation guarantee (i.e., 1-1/e) of the general greedy algorithm. I am wondering the theoretic guarantee of sliding window version of greedy algorithm (Algorithm 2). When w=1, it corresponds to Algorithm 1, but how user could decide the value of w?   The minor comments: The description of algorithm 1 is somewhat confusing, e.g., is c_i increases over the both inner and outer iterations? Authors should modify the notations.  It seems that the proposed algorithms can adopt lazy evaluations. It is expected that lazy evaluations boosts the algorithms even faster. Authors can clarify the possibility of adaption of lazy evaluations in their algorithms. 