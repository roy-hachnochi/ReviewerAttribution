This work extends the interactive particle filter to solve I-POMDP where the intentional model of the other agents (i.e., the transition, observation and reward functions) are unknown as a prior. Hence, when making decision, each agent must learn the other agentsâ€™ belief about the state, the transition, the observation, and the reward function. With initial belief samples, the proposed algorithm starts from propagating each sample forward in time and computing their weights, then it resamples according to the weights and similarity between models. This is very similar to Bayesian learning for POMDP but for multi-agent cases modeled as I-POMDPs. This work is evaluated mainly empirically.   It is not clear how the reward function can be learned by sampling because there is no signal about the reward function in the observation as in the standard I-POMDPs.  Update: Thanks for answering my question. I understand the reward function is part of the sampled model. 