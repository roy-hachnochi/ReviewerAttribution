The paper proposed a new framework for initializing deep neural networks. The proposed initialization is based on the analysis on variances of layer outputs when the inputs contains additive or multiplicative noises.  The idea of designing initialization heuristics to maintain the scale of outputs stable over different layers isn’t something very novel. The “He” initialization methods mentioned in the paper was originally proposed to maintain the standard deviation of each layer. This paper extended this idea to the general additive and multiplicative noises by deriving the explicit formula for the variances of layer output. The idea was interesting and it leads to some neat result on the initialization approach for network with dropout noise.  But I feel there’s still a big gap between the analysis the paper and the practical phenomenon the paper tries to address. The paper only discussed the distribution of the layer output when the weights are random, but in realty the weights are no longer randomly distributed after some iterations of back-propagation. So the analysis in the paper cannot really explain why the new initialization work in practice. Even though the authors provided some discussion about that in the final section, it’s worthy some full-fledged discussion if the authors really want to develop a theory around the proposed initialization approach.  It’s also kind of hard to choose which initialization methods can be used in practice as we can never know the exact noise type from the input distribution. So I can hardly see any practical implication of the analysis proposed in the paper.