EDIT:  The authors response addresses my concerns about reproducibility and the appendix they promise should contain the missing details of their setup.  Summary and relation to previous work: This paper presents projection weighted canonical correlation analysis (CCA) as a method to interrogate neural network representations. It is a direct continuation of previous work on singular vector canonical correlation analysis (SVCCA), addressing several limitations of SVCCA and applying the method to new models and questions. They observe that some CCA components stabilize early in training and remain stable (signal) while other components continue to vary even after performance has converged (noise). This motivates replacing the mean canonical correlation coefficient (used as a similarity metric in SVCCA) with a weighted mean, where components that better explain the activation patterns receive a higher weight. The new method is then applied in several different scenarios resulting in a number of interesting insights: when analysing converged solutions across different random initializations, mean CCA distance reveals five subgroups of solutions that map well to the clusters of solutions based on robustness to ablation found in Morcos et al 2018.  networks which generalize converge to more similar solutions than those which memorize wider networks converge to more similar solutions than narrow networks RNNs exhibit bottom-up convergence over the course of training (the same result was found for CNNs in the SVCCA paper) across sequence timesteps, RNN representations vary nonlinearly  Quality:  In general, the claims are well-supported by rigorous experiments involving several architectures and datasets. I am thoroughly convinced of the usefulness of projection weighted CCA. The relationship between CCA vector stability over training, vector importance and generalization is not explored and would be interesting for a follow up paper.   Clarity:  The submission is very clearly written and easy to follow. Main results and contributions are clearly stated. However, specific details about the architectures tested and how they are trained is omitted making it impossible to exactly replicate the experiments.   Originality: Projection weighted CCA is a small, simple but important improvement over SVCCA. Related work is discussed well.   Significance:  Others are highly likely to use projection weighted CCA and/or build on the unique experimental observations presented in this submission. Although only slightly different from the original SVCCA, researchers should use this improved version or risk not capturing important aspects of network/layer similarity. The experimental observations in particular are likely to inspire new experiments or new learning methods.   Minor edits: - lines 141 and 142 repeat the same statement (that the randomization of labels was the same for all networks) - several references only include author, title and year, without any publication name, conference or arxiv ID. For example reference 8 was published at ICLR 2016 but your entry says "October 2015"  References:  Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On theimportance of single directions for generalization. In International Conference on LearningRepresentations, 2018  Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, 2017.