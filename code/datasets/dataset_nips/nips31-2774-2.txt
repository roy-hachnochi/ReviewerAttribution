When approximating alpha divergence from samples, the estimation variance can be very high due to the unbounded density ratio. In this paper, a new variation of f-divergence is introduced. It guarantees a finite mean of importance weights and its reparametrization gradient can be easily calculated from empirical samples. Experiments on variational inference and reinforcement learning show promising results.   This is a very interesting paper which addresses an important question: in many density ratio-based divergence estimation (such as KL divergence), the fat-tail property of density ratio causes large estimation variance. The alternative definition of the f-divergence shows an interesting way of avoiding such a problem as well as computing the empirical gradient of the divergence minimization problem. The use of emperical CDF of density ratio for suppressing the expectation of f(ratio) is interesting but I still have some concerns:   Proposition 5.1, authors proved the expectation of phi(ratio) is bounded:  1. With respect to what distribution, the expectation is taken? p or q?   2. The proposition only states the bounded mean. Since authors talked about estimation variance of alpha divergence, I thought the second order moment of phi(ratio) might be more interesting than the mean?   3. What is the estimation variance of the gradient in algorithm 1? How does it compare with the alpha divergence?   Otherwise, this paper is well-written and easy to understand. I would like to recommend for acceptance.   -------------------------------------------------------------------------------------------------------------  I have read the feedback from authors. I would upgrade my score by one after the clarification.  