[edit]  I found the rebuttal to my points fairly brief and unsatisfying - if the performance of an overall system is improved by using a less principled component, or a component without various proofs associated, that's an interesting result. That said, I still think this paper is interesting and worthy of inclusion.  [/edit]  The paper presents a method of improving the stability of Bayesian optimisation by selecting a sample by:  1. setting x maximise min_d(ucb(x+d)) for some distance d in a constrained region around the intended sample location 2. setting x_sample = x + d_s  by min_{d_s}lcb(x + d_s)  This attempts to make the modelling robust to the worst case maximally adversarial perturbation. The two step procedure first finds a robust optimiser, then actually samples the most pessimistic point in the neighbourhood of this point. The reported point is the the point in the samples with the __highest__ pessimistic lcb - the end result being that the reported point is expected to be most stable.  The authors relate their work qualitatively other approaches to robust BO in the literature - particularly "Bayesian Optimization Under Uncertainty" by Beland and Nair, however do not perform experiments comparing their results to those of the other papers. As similar in spirit approach not mentioned is presented in "Unscented Bayesian Optimization for Safe Robot Grasping" by Nogueira et al - there, random noise is added to the inputs of the sample, and propagated into the prediction using the unscented transform. In the paper under review and the two mentioned, the end result is that the optimiser avoids sharp optima.  I was unclear as to the meaning of the line 42 comment "and applied to different points" - I would appreciate further explanation in the author's rebuttal. My understanding of that paper is not deep so I may have missed some key difference, but it seems like the formulation used to optimise the Branin function would be applicable to this task (if suboptimal).  "Overall score"  My overall score reflects an average over the high quality of the presentation and exposition in the first part (an 8) and the quality of the experiments (a 5 or 6). The authors reject comparisons to "Bayesian Optimization Under Uncertainty", however it appears that the numerical studies in section 3 of that paper on the Branin function cover a very similar problem, albeit without the __adversarial__ perturbation. I would also have appreciated a comparison to "Unscented Bayesian Optimization for Safe Robot Grasping". These experiments would have justified the additional machinery introduced in this paper.  Additionally, although less importantly, I would have appreciated a comparison to other robust techniques not based around BO (e.g. line 52's '[5, 9, 19, 31, 36]') - again, do we need the additional machinery introduced by BO to handle these cases?  "Confidence score"  I am fairly knowledgeable on the subject of BO, however my knowledge of robust optimisation is lacking. 