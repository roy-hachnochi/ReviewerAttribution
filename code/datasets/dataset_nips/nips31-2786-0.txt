This paper studies the problem of multiset prediction, where the task in to predict a multiset of labels out of the set of allowed multisets. The proposed method does sequential predictions of labels and is trained to imitate the optimal oracle strategy. The method is evaluated on the two tasks: MultiMNIST and recognition of multiple objects on the COCO dataset. The paper is clearly written, explains the method and some theoretical properties well. The description of the experiments looks good enough.  1. As explained in Lines 191-197 the proposed method is essentially an instantiation of the learning to search approach [2, 1, 10]. However the method is run in a regime, which was considered suboptimal in the previous studies (learned roll-in and reference roll-out). In particular, [1] and [10] concluded that one should do learned roll-in and mixed (reference and learned) roll-outs. Comparisons to different versions of the method are important to understand the benefits of the method.  2. It is very hard to judge how important the task of multiset prediction is. The experiments in the paper in toy settings (I'm not saying that those are easy to solve). Lines 16-19 give some contexts where the setting could be useful, but there are no attempts to apply the method in any of those.  3. The proposed loss based on KL is not really novel. It is very similar to the KL loss used in [10] and in [Norouzi et al., Reward Augmented Maximum Likelihood for Neural Structured Prediction, NIPS 2016].  4. Table 3 (the paper comparing the baselines) of the paper does not have error bars, so it not clear how stable the results are.  === after the response I've read the response and would like to thank the authors for reply on question 1. However, I still have concerns that the contribution of the paper is relatively incremental and the task is not proven to be significant (it is mentioned that multiset prediction can be used as a proxy for other tasks, but I'd really want to see examples of that) so I'm keeping my old score.