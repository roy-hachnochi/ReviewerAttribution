This paper presents a new method proposes a new class of reparameterized gradient estimators based on implicit differentiation of one term in the expected gradient.  Explicit reparameterization depends on the existence of an analytically tractable standardization function. Every distribution with an analytically tractable CDF has such a standardization function, since the CDF is invertible by construction and maps from samples into Uniform[0,1], but many continuous distributions of research interest (Von Mises, Gamm, Beta, Dirichlet) do not have tractable inverse CDFs. Prior approaches have approximated this intractable term or applied a score function estimator to all or parts of the objective to avoid reparameterization. This work derives an implicit differentiation estimator for this termâ€”the gradient of the latent variable reparameterized through an invertible, differentiable function of an independent random variable and the parameters of the distribution to learn. An implicit gradient estimator for this term sidesteps the need for inverting the CDF and/or standardization function, and thereby expands the class of distributions to which reparameterization can be easily applied.  The mathematical derivation is pleasingly straightforward, the paper very well written, and the empirical results convincing. The conclusion contains some interesting pointers for future work for expanding implicit reparameterization gradients into the class of distributions for which a numerically tractable inverse CDF or other standardization function is not known.  I recommend publication and expect this technique to expand the class of commonly-used variational approximate distributions that are commonly applied.   The scope and novelty of this contribution is not huge since, at its core, it will only expand the scope of reparameterizable distributions. But, the insights and presentation are very valuable and interesting. 