This paper adopts the capsule vector idea from the capsule network and proposes the idea of dividing the feature space into multiple orthogonal subspace, one for each classification target category. Given a feature vector, this model first project it onto multiple orthogonal subspace and then use the 2 norm of the image vector to calculate softmax probability. Experiments show promising accuracy enhance. The idea is valuable, but it may have discarded some core thoughts of original capsule network.   Pros: 1. It is a novel idea to project feature vector into orthogonal subspace. It’s motivated by the vector representation and length-to-probability ideas from capsule network, and the author did one more step to uncover new things. This is an interesting idea with a good math intuition. The proposed orthogonal subspace projection provides a novel and effective method to formalize the principled  idea of using the overall length of a capsule.   2. On several image classification benchmarks, the CapProNet shows improvement (10%-20% reduced error rate) than state-of-art ResNet by incorparating the CapProNet as the last output layer. Additionally, the ResNet with CapProNet caused only <1% and 0.04% computing and memory overhead during model training than original ResNet. The CapProNet is highly extensible for many existing network. It might become a useful method that can be used in a more general setup.  3. The way the authors handle inverse matrix gradient propagation is interesting.  4. The presentation of the paper is clear; e.g., the presentation of visualization results of projections onto capsule subspaces in Section 5.2 is good.  Cons: 1. It may be arguable if models presented in this paper should be called a capsule network: only the neuron group idea is inherited from the capsule network paper and other valuable core thoughts are discarded. For example, the capsule network introduces dynamic routing which grab confident activation through coincidence filtering, and different levels of capsules can learn part-whole hierarchy. However in this paper the second last layer is a single feature vector, which is bound to diverge from some core thoughts because it is likely we cannot find pattern’s that agrees during votes.   2. While the way the authors handle inverse matrix gradient propagation is interesting and it does not harm the gradient that is back propagated toward lower layers, I wonder whether it’s efficient enough to perform the m*n route by agreement scheme proposed by the original capsule paper.  3. The comparison in Table 1 doesn't include the latest state-of-art models on these image classification benchmarks, i.e., DenseNet (Huang et al., 2017) which achieves better results than CapProNet/ResNet on CIFAR-10/100. I think it may be more convincing to perform CapProNet experimants based on DenseNet or other latest state-or-art models. The comparison in Table 2 was not detailed enough. There should be more description in this part since CapProNet is very similar to "GroupNeuron" in the surface form. More detailed and analytical discussion between CapProNet and "GroupNeuron" would be helpful.  4. Still the paper can be better written, e.g., improving the positions/arrangement of tables and fixing exiting typos.