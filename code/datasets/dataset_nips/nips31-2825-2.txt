In this paper the authors advocate the use of the Sinkhorn distance over the "regularized" sinkhorn distance for computing divergence between discrete distributions. They show that the gradient of the former is better and leads to sharper results especially on barycenters. They also provide a close form expression for the gradient of the Sinkhorn distance using the implicit function theorem. Another contribution is a new generalization bound for structured prediction with Wasserstein distance. Numerical experiments are very short be show a better barycenter with the Sinkhorn distance and better reconstruction for images.   The paper is nice and some important results are presented. Still it lacks a few references and the numerical experiments are very limited as discussed more in details below.  + The Sinkhorn distance in equation (7) has already been used (and optimized) in the literature over the regularized version for a few years but it needs to be properly differentiated from the regularized version and the authors did a good job there.   + In [1], a reference cited in the paper, the authors did the gradient computation of the Sinkhorn distance using autodiff tools (gradient propagation along the Sinkhorn iterations). Those approaches are very efficient and come with a small overhead but are not discussed at all in the paper. The Sinkhorn distance has been also derived for discriminant subspace estimation [2]. Even though autodiff was used, the implicit function theorem is used in the supplementary material of [2] to compute the gradient of a bilevel optimization problem with Sinkhorn distance wrt to a linear operator on the samples.   + The discussion about the complexity of the proposed approach is a bit misleading (with vague assumtion about teh necessary number of iteration of Sinkhorn). The complexity of the proposed SInkhorn+gradient computation  is O(n^3) to get a sharp gradient. In other words the authors propose an approach to compute a better gradient that regularized sinkhorn but it comes at the cost of the same complexity of the unregularized OT (that is O(n^3) for solving, the gradoients are the dual potentials also returned by a dual solver). Since one of the important property is that the proposed gradient recover the true unregularized Wasserstein barycenter, why not use the true gradeint with same complexity?  Also  Autodiff is O(Ln^2) where L is the number of sinkhorn iterations so unless there is a true numerical (as in precision) gain to the propose gradient autodiff will be more efficient and needs to be discussed.   + The numerical experiments are interesting but a bit short. Figure 2 is nice  but it should be nice to show the impact of the regularization (in computational time if not on the final result).  Also what is the result when using Autodiff as in [1]?   + The generalization bound for structures learning are very nice. But the numerical experiments are very limited and hard to interpret even as a proof of concept. For instance the accuracy of a classifier on the reconstructed image is used to see if  the reconstruction was accurate. The authors have to be commended for using a measure of performance that is not tied to any of the divergences used . But those performances cannot really be compared or evaluated without the performance of the classifier on the original images. please add this columns to the Table in Fig. 3 so that we have an idea of the loss incurred by the reconstruction.  [1] Genevay, A., Peyr√©, G. and Cuturi, M., 2017. Learning generative models with sinkhorn divergences. arXiv preprint arXiv:1706.00292.  [2] Flamary, R., Cuturi, M., Courty, N. and Rakotomamonjy, A., 2016. Wasserstein Discriminant Analysis. arXiv preprint arXiv:1608.08063.   Author feedback ============  I want to commend the authors for their detailed and impressive feedback and strongly recommend them to include this material in the final version of the paper if accepted.  The new comparison and discussion of autograd make the paper much stronger. I'm still very surprised by the time for true Wasserstein, it has been known that a modern CPU bound network simplex can solve the OT problem (and give gradient estimates) for n=100 in the order of the ms so a time of 1000s for barycenter estimation  suggests bad implementation.