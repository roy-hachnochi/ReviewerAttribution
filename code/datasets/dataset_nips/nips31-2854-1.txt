This paper introduces a new technique that trains a diverse ensemble by assigning hypotheses to samples. Its main benefit compared to traditional ensemble methods is the gradual specialization of experts through the addition of intra/inter-model diversity terms in the objective.  While the notion of specialization has been studied previously, this specific formulation with its switching between two MOs ('model selecting sample' and 'sample selecting model') is new and is of significance to the ML community, and may have especially high impact for datasets with a large amount of sample heterogeneity (for instance, data collected across multiple populations).  I find it a bit surprising that a boosting-type method (maybe gradient boosting) was not used in the comparison. These methods are known for allowing hypotheses to complement each others' strengths. According to Table 2, DivE performs clearly better in terms of magnitude on CIFAR100 and STL10, however, for CIFAR10 and Fashion the improvements are small - are these results statistically significant? Are there cases when Dive is expected to perform a lot better? Maybe the more complex the dataset, the higher the benefit? Also, it seems that the Top-k-NN-LP strategy performs better in general regardless of the ensemble type used, with the exception of Dive all avg. on CIFAR100. It may be worth investigating why this happens - can we draw any intuition about the relative merits of these scoring strategies.   Clarity: Figure 2 is illegible unless magnified. As a table is also provided, it may be a good idea to select only two more representative (or interesting) examples to show here, with the rest relegated to the appendix. It may be an idea to show only the best for BAG, RND and DivE on the same plot for each dataset.  Suggested reference: "Projection Retrieval for Classification" (NIPS 2012) presents a data partitioning ensemble method with a submodular objective.  The implementation of this technique is highly non-trivial, so acceptance of the paper should be conditioned on making the code and the experiments publicly available.  Update: The authors explained why they haven't compared against boosting in the feedback, so my main misgiving was addressed.  