The authors study a new classification setting in this paper, where only positive samples and their confidence to be positive are given. The authors find several interesting applications of the proposed new setting. I think the new setting is meaningful and may interest many readers. Specifically, the authors proposed an unbiased estimator for learning a binary classifier from the positive samples and their confidence. Then, sufficient experiments have been done to empirically show the effectiveness of the proposed method. Overall, the paper is well-organised and well-written.  My primary concern is how to obtain the confidence of positive data. Compared with the one class learning setting, obtaining numbers of confidence looks harder than obtaining integer hard labels.  I do understand that if we just employ the integer hard labels, some more details are missing, e.g., the ranking information. In the rebuttal, I would like the authors to discuss how to estimate the confidence without the help of negative examples.  How to tune the regularisation parameter in the proposed objective function? Since we only have positive examples, the traditional cross-validation method may not apply. We should be every careful to tune the parameter because it is closely related to the class prior information as shown in Eq. (2). For different class priors, the regularisation parameter should be different even if the training sample size is fixed.  The authors have set a section to discuss how the overlap between the positive and negative examples affects the performance of the proposed method, which is quiet interesting. However, I think we should focus more on here. What are the covariance matrices of the Gaussians? Can the proposed method still work well when the overlap is huge? ========== I have three major concerns with the paper. In the rebuttal, the authors have well addressed the second and third concerns. Although the authors haven't provided a detailed method to obtain the confidence for positive examples, the proposed unbiased estimator is interesting and can find potential applications. I am satisfied with paper and do not change my score.