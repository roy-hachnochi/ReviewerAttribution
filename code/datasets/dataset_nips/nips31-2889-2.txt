Update: After reading the rebuttal, I am updating my rating to 6.  -------------  This paper studies natural gradient descent for deep linear networks. It derives the expression of the gradient for quadratic loss and shows that natural gradient converges exponentially to the least-squares solution. The authors show that the Fisher information matrix is singular for deep linear networks and one has to resort to a generalized inverse of this matrix to compute the natural gradient. The paper reports an online implementation with successive rank-1 updates for an auto-encoder on MNIST. The authors make an interesting observation that a block-diagonal approximation of the Fisher information matrix is sufficient for the linear case.  This paper is very well-written with a good overview of relevant literature. The focus on the linear case seems pedagogical and the paper would improve immensely if the authors provide experimental results on larger networks and harder datasets. I have reservations about (i) using the expressions for Fisher information matrix for the quadratic loss for say, a large-scale supervised learning problem with cross-entropy loss, and (ii) the computational overhead of the matrix-vector products in (10) and the expectation over the distribution p_theta.  The speed-boost for the sanity-check-example in Fig. 1a, 1b is not seen in the auto-encoder in Fig. 1c. In particular, the X-axis in Fig. 1 should be wall-clock time instead of the number of iterations to make the comparison with SGD fair. The authors use K=10 forward passes for each sample in the mini-batch to estimate Lambda_i in (20); using the natural gradient is computationally more expensive than SGD. Why does such a small value of K seem sufficient to estimate (10) accurately?  The memory complexity of the natural gradient updates (even with block-diagonal approximation) should be high. For instance, for the auto-encoder on lines 241-247, the expression in (19) incurs a large memory overhead which makes the applicability of this approach to larger problems questionable.