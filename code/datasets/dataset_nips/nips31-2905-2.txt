This paper proposes the first convergence guarantees for sparsification-based distributed training of massive machine learning models.   It’s a solid theoretical work and the proof is not difficult to follow.  If I understand the paper correctly, the Assumption 1 is the core assumption, but it’s quite abstract. It seems better to give some easy to understand “meta assumptions”, when these assumptions are satisfied, the Assumption 1 holds true, or show some specific examples, for what kind of objective function and order of magnitude, the Assumption 1 can be satisfied.