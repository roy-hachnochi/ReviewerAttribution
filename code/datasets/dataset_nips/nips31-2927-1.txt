Since the efficiency of Bayesian optimization mostly hinges on properly modelling the objective function, picking an appropriate model is essential.  Usually, the model of choice is a Gaussian process with a simple Matern or SQE kernel, which is prone to model miss-specification.   The proposed method extends the main loop in Bayesian optimization by an additional step to automatically select promising models based on the observed data. To solve this inner optimization problem, they use Bayesian optimization in model space to find a composition of kernels that account for the uncertainty of the objective function.    Overall I do think that the method is sensible and addresses an important problem of Bayesian optimization, i.e model miss specification. Also, the paper is well written and clearly structured and I really enjoyed reading it.   However, even though the authors show that the proposed method outperforms a set of baselines, I am bit concerned that all the necessary approximations might make the method brittle against the choice of its own hyperparameters in practice.  Questions for rebuttal:  1) Could you give some more insights how sensitive the method is against its hyperparameters?  2) How high is the additional optimization overhead of the method. Could you plot the performance also over wall-clock time?  3) Does the method also allow to combined models other than Gaussian processes, i.e random forest, Bayesian neural networks etc.?  4) Do you have any insights how the model changes during the optimization? I assume that the components of the ensemble become at some point relatively similar.  5) Does it make sense to combine the model selection with from the acquisition function to gather data that helps to obtain a better model? I think of an active learning phase in the beginning where Bayesian optimization is mostly exploring anyway.   --- Post Rebuttal ---  I thank the authors for answering my questions and I appreciate that they will clarify them in the supplementary material.