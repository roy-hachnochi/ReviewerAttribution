This paper tackles the relevant problem of cross-model compression for applications with memory limitations. To do so, a multi-task zipping (MTZ) approach is introduced, which automatically and adaptively merges deep neural networks via neuron sharing. The method iteratively decides on a layer basis and based on a computed functional difference, which pairs of neurons to merge and how to update their weights in order to mitigate any potential error increase resulting from the compression. A proof of concept is performed on MNIST for fully connected, convolutional and sparse networks; and additional results are reported on image classification tasks (ImageNet vs CelebA pre-trained models).  The presentation of the paper is clear and ideas are easy to follow.  In the related work section, authors discuss knowledge distillation based approaches, referencing [2][3][15]. Although [2][3] are indeed model compression methods, they are not rooted in the knowledge distillation approach introduced in [15]. However, FitNets (https://arxiv.org/pdf/1412.6550.pdf), which does extend the knowledge distillation idea of [15] in the context of model compression is not referenced nor discussed. Please cite all of those works appropriately.  The residual adapters modules and their motivation (https://arxiv.org/pdf/1705.08045.pdf) seem fairly related to the MTZ approach; the goal being to build a single network to learn multiple visual domains while keeping the number of domain specific parameters low. However, authors fail to discuss this paper. It would be beneficial to compare the MTZ results to the ones reported in this paper, and for some overlapping tasks.  In Equation (5), what is the influence of alpha? How do you set it in the experimental section? What is the impact of the value of this hyper-parameter?  In Algorithm 1, a light retraining is computed after merging each layer. What is the effect of re-training after each merging? What would be the impact in performance of re-training only once after the whole merging procedure is completed? In tables reporting results, does the number of re-training iterations include all the intermediate retraining steps?  It might be worth adding to the results (1) the compression rate of the merged model w.r.t to the smaller single model in terms of number of parameters (or operations), (2) the compression rate w.r.t. all the models prior to zipping, (3) number of domain specific vs number of domain agnostic parameters in the final networks, (4) report the number of re-training iterations w.r.t. re-training the full model (speed-up instead of number of iterations itself).  There is no mention about intentions on making the code publicly available.  So far, the model has been tested to merge networks with the same number of layers and which are relatively shallow. However, this seems to have rather limited applicability (networks trained for different tasks may eventually have different number of layers). Have the authors considered extending the framework to networks of different depth? Moreover, it might be worth testing the approach on a more recent (deeper) state-of-the-art network, such as resnet.  It would be more informative to report the plots on Figure 2 for the ImageNet vs CelebA experiments, since for MNIST it is hard to perceive any differences while augmenting the number of shared neurons (in the MTZ case).  Have the authors tried to zip more than 2 networks? That would be an interesting experiment to compare to the residual adapters on several visual tasks.  In the experimental section 4.2, how was the number of shared neurons per layer selected?  Finally, in lines 262-264, it seems that the number of parameters shared for fully connected layers is lower than in convolutional layers. However, it is worth mentioning that fully connected layers are usually the ones containing most of the parameters.  -----------------------------  I thank the authors for their detailed answer; addressing many of concerns raised by the reviewers. The paper could still benefit from a proper comparison with the multi-task baseline. 