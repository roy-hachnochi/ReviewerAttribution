This paper proposes Multi-Task Zipping (MTZ), a framework to automatically and adaptively merge correlated, well-trained deep neural networks for cross-model compression via neuron sharing. It decides the optimal sharable pairs of neurons on a layer basis and adjusts their incoming weights such that minimal errors are introduced in each task. Evaluations show that MTZ is able to merge parameters across different architectures without increasing the error rate too much.  I liked this submission and the motivation of neuron sharing for the compression of multiple trained deep neural networks where the paper focuses on the inter-redundancy of multiple models. The layer-wise computation for the Hessian-based difference metric nicely solves the computational overload.  Details: (1) Can this approach be used in other architectures such as RNNs in sequence modeling? The experiments on LeNet is relatively toy. (2) How to extend this approach to multiple models (>=3)? What if we have two models with different sizes and layers?   