Update:  After reading the reviews from other reviewers, and the author response, I decided to stick to my original score of 6. The author response was inline with my expectations and but was not significant enough to persuade me to improve the score to 7. That being said, I'm supportive of this work and I believe it is (marginally) good enough to be accepted into NIPS2018. I would like to see more works in the future combining the best of evolutionary computing with traditional deep learning methods as I think this will help push the field forward and escape from local optima.  Original Review:  Review for "Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks"  Summary: This paper proposes a scheme to combine traditional gradient-free evolutionary algorithm with SGD. The basic idea is to have a population of mu parameters of a neural network, and for each generation, train each population K_s SGD steps, and use evolution to keep track of m best solutions and randomly mutate the next for each generation.  Good points: - Idea is well motivated, and I think we as a community should explore more ways to combine the best of EA and SGD methods. The algorithm is nice and simple, and works on a variety of tasks. - Experiments were chosen well (covers a lot of ground), and thorough. I didn't expect them to choose SOTA methods for everything (though perhaps other reviewers of NIPS might), but they chose a representative respectable architecture as a baseline for CIFAR-10, PTB, etc., and I think 92-93% ish on an off-the-shelf ResNet and 63-64 perplexity is good enough as baseline methods for them to convince me that their approach can improve over SGD. - They prove that their scheme guarantees that the best fitness in the population never degrades.  Criticisms (and things that might be addressed): - confusing notation (see discussion points). Lots of subscripts and lower cases, hard to follow. - you should discuss the compute requirements as a tradeoff. It seems you need to basically train a neural network mu times, so when comparing to a baseline of vanilla SGD, it might be unfair to have generation on the X axis? You also need I think mu GPUs that run in parallel - from the previous point, what if I just train mu different networks, and use them as an ensemble? If I use your ResNet architecture and train mu different ResNets with different random seeds from scratch, the ensemble might be better than ESGD (or maybe it might not be!). I think you might want to discuss this, I'm not expecting you to beat an ensemble but worth a discussion on pros/cons.  Discussion points:  In algorithm 1, in the "for s=1:K_s do" loop, shouldn't it be nested with a "for j:1:mu" loop as well? Since you use j as a subscript, but it will be undefined. This should be addressed.  For the EA, what do you think about using other algorithms such as CMA-ES or even a quasi-gradient inspired method (like the OpenAI's ES paper [1] or PEPG [2])? Is the vanilla EA scheme all we need?  If someone has a single GPU machine can they still run the experiment? Is this something that can be plugged in and made to work with a few lines of code change? Can you convince a random NIPS researcher that it will be worthwhile to use this method in practice?  Rating: 6  I think this work is marginally good enough to be accepted at NIPS, since I think they did a good job with proposing a well motivated idea, with theoretical foundations to back their method up, and also ran a thorough set of experiments. But honestly it's not a top inspiring paper and I feel this method as it is might not be common place (given the points I raised above).  Note: I'm willing to increase the score if I believe the concerns are addressed. Please get back to me in the rebuttal, thanks, and good luck!  Here are the links to the papers: [1] https://arxiv.org/abs/1703.03864 [2] http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&rep=rep1&type=pdf  (can't find [2] on arxiv.) 