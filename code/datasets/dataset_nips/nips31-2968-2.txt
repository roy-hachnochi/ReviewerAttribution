Update:  After reading the reviews from other reviewers, and the author response, I decided to stick to my original score of 6.  Original Review:  Authors propose to combing Evolutionary Strategies with Stochastic Gradient Descent to train Deep Neural Networks. There have several recent papers that apply ES to train DNNs and many papers applying SGD, this paper provides a novel combination of the two.  A comparison with an ensemble of models is missing as a baseline. Without the code it would be really hard to replicate this paper.  quality: The paper presents several experiments on speech recognition, image recognition and language model.  clarity: The paper is well written and clear.  originality: Marginally original, the paper is a combination of two previous ideas, although the specific combination is novel.  significance: Marginal, the increase computational cost is not clear if it is worth compared with running an ensemble of models.