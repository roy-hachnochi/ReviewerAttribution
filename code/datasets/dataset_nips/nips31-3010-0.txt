**Post feedback update**  I would like to thank the authors for their feedback. Based on the information provided, I would encourage the following changes be made to the paper in addition to what was suggested in the original reviews:  * Improve comparison with related work by Suciu et al. and Koh and Liang (using points on the more realistic treat model and also experimental results mentioned in your feedback). * Tone claims down with respect to transfer learning or support with additional empirical evidence.  **End post feedback update**   This manuscript introduces a poisoning attack that departs from prior work by not requiring that the adversary control the labeling mechanism (the poisoning operation is not what is commonly referred to as “label flipping”). They also don’t require the presence of a trigger at test time (as done in what is commonly referred to as a “backdoor”). To the best of my knowledge, it includes pointers to relevant prior work both on shallow and deep learning algorithms. For these reasons, I think the manuscript is original and should contribute to increased visibility of poisoning in the adversarial ML community. Furthermore, the manuscript is well written and easy to follow.   The attacks discussed are intuitive, in particular to readers familiar with the adversarial example literature. Some observations made in the submission, although perhaps simple in retrospect, are interesting to pounder on: for instance, end-to-end training affects how lower layers represent the data in feature space while transfer learning only shifts decision boundaries of the last layer (because that’s the only layer that can be modified). This should shine some light on potential mitigation mechanisms.  Perhaps the main limitation of the submission is the strong assumptions it makes about adversarial capabilities in its threat model: the adversary needs to have access to both the model architecture and parameters resulting from training.   Another limitation of the present work on poisoning is also found in the adversarial example literature, and thus it would be nice to anticipate it to avoid some of the pitfalls encountered in the context of adversarial examples: L_2 norms are not an idea characterization of how a perturbation affects human perception. Thus, low L_2 norms do not ensure indistinguishability to the human observer. While there are likely no solutions to this problem at this time, it would be useful to include a discussion of the attack’s applicability to different domains (beyond vision), and which metric they would call for. Will it always be possible to define a differentiable constraint on the perturbation (e.g., for emails in the SPAM scenario you discuss)?   Finally, I had some concerns about the scalability of the approach. Does the attack require the model to overfit on its training data? Some evidence that controls for overfitting would be beneficial. Is the number of poisoning points high relatively to the number of training points per class? Furthremore, the transfer learning evaluation only considers a binary classification problem. How would it extend to multi-class tasks?  Some additional comments that would be nice to clarify in the rebuttal: * Does the base instance need to be part of the original training dataset?  * How does the approach scale when the adversary is not targeting a single test input but instead multiple target test inputs? Would the architecture size and potential overfitting issue mentioned above matter more in this setting? * Could you clarify the motivation for the procedure used in Section 2.2? Why not apply a more common optimizer like Adam? Is there something different about the particular loss used? * In practice, is watermarking the most efficient poisoning attack in these settings? Does it consistently outperform the procedure from Section 2.2?  * Why is angular deviation relevant in high dimensional settings? 