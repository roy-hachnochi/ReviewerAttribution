This paper study a targeted poisoning attack on neural nets. It proposed a clean label attack which is involved in creating new examples with adversarial noise that will be classified correctly by the classifier, however, it will cause the network to misclassify examples in which the adversarial noise was constructed from.  Exploring the robustness of NN to different attack is an important research direction and the main idea of this attack is interesting, however, I have some concerns reg. this work. According to the authors: "...Our strategy assumes that the attacker has no knowledge of the training data but has knowledge of the model and its parameters...". Although white box attacks are interesting to start with, in order for this attack to be realistic, I would expect the authors to run black-box attacks as well. Assuming the authors has access to the model and its gradients but not to the training data is somewhat weird setting.  It seems from the experiments that the authors use only binary classification, is there a specific reason for that or maybe I did not understand it correctly?  In general, I find the experimental results not sufficient, I would expect to see this experiments on large-scale multiclass problems when proposing a new attack.  Reg, defenses, it is highly important to see how this attack will work under popular defenses.  The authors wrote: "...As long as the data matrix is full-rank (no duplicates), the system of equations that needs to be solved to find the weight vector is under-determined and as a consequence has multiple solutions: overfitting on all of the training data is certain to occur." do the authors have evidence to support that?  In section 4.2.1, it is not clear how the authors generated the poison examples. Did you use the same base class for all examples? did you use a single base for each poison?  Lastly, the authors pointed out that this approach is akin to adversarial training. The main difference between the proposed approach to AT is that in AT we draw adv noise from various instances rather than one.  Since adv. training might be the most efficient way to deal with adv. examples (for now), it would be very interesting nice to analyze this side effect, if it also exists in AT.   Notations: in section 2, the authors should properly define the notations, I find some missing ones, like t, b  ----- RESPONSE ----- Since this specific attack was conducted using something very similar to adv. training technique (the authors also pointed it out in the conclusion section), I would expect to see some black-box/grey-box settings. Additionally, I would expect to see some detailed analysis of this phenomena (when will it work/when it won't, how many examples do I need to make it happen, what is the acc. of an end-to-end model on CIFAR10 after fine-tune with the poisoned examples?). As I see it, this is closely related to adv. training (which is one of the critical components in defending against adv. examples), hence it is crucial for the comunity to understand if this attack will affect models that have been trained like that. On the other hand, this might be out of the scope of this paper.