This paper proposes to regularize AEVB objective with kernel based measures of independence (d-variable Hilbert Schmidt Independence Criterion) to learn latent representations with desired independence properties. Experiments are performed to use this kind of regularization to test its effectiveness on 3 scenarios: learning interpretable representations, learning invariant representation and denoising representations. The motivation behind the proposed approach is clear and seems to yield desirable representations. The main question to the authors is that for purposes of stochastic optimization, is the empirical estimate of HSIC computed over each mini-batch separately? And how much of a slowdown do you observe practically while optimizing with the regularized objective?