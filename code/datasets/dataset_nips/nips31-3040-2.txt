Update: I have read the author's response and would like to emphasize the importance of including the additional experimental results in the paper. It would have been even better if the domain used for comparing with MAML was one found in their original paper, so that we can see those results reproduced as the baseline here.  ----------------------  This paper approaches transfer learning in reinforcement learning under the intuition that the set of optimal value functions of past (source) tasks can be used to more quickly learn the optimal value function of a new target task when all tasks are drawn from the same underlying distribution. Their approach is to model the distribution over the weights of the optimal value functions for the tasks based upon the learned optimal value functions of the source tasks. This approximate distribution is then used as a prior over value function weights, and a variational posterior is optimized using the Bellman residuals from an observed dataset of transitions (s, a, r, s'), essentially optimizing the weights to reduce the TD-error while remaining close to the prior distribution over weights provided by transfer from the source tasks.  The authors use a mellowmax Bellman operator instead of the Bellman optimality operator because of the need for differentiability, and motivate this choice with a theorem connecting the fixed-points of the two operators. Finally, the authors provide theoretical results on the finite sample complexity and empirical results on small RL problems for two prior parameterizations (Gaussian and Mixture of Gaussians). These results very clearly show the effects of transfer learning in their algorithm compared with non-transferring baselines.  This is a very well written work, which goes nicely from a well-expressed insight through to giving theoretical contributions and showing experimentally that the idea works in practice. Overall, I think this is a strong contribution.  The experimental results are quite convincing. The additional figures in the appendix do a great job of more fully exploring the results. I found the experiment shown in Figure 1b to be particularly surprising (in a good way). I would be interested to better understand why this type of transfer is happening, as I would have expected this not to work very well.   In Figure 4 in the appendix, I found it strange that going down to 2 source tasks there does not seem to be any real loss in transfer performance for the mixture case. Doesn't this suggest that the optimal value functions for the task distribution is fairly simple? Maybe I am misinterpreting this result, and maybe a similar plot for a more difficult task would show decaying performance as source tasks become fewer (and thus the prior less informative).  One limitation of the experimental results is obviously that these domains are quite simple. The maze navigation task is very interesting, and I think should satisfy concerns that this might not work for more challenging problems. But, nonetheless if we could have anything, that might be an area for further work. 