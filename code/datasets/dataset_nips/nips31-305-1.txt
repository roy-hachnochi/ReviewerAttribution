       This paper is well written and easy to follow. The authors tackle the problem of future video frame prediction (binary and simple MNIST and bouncing ball) and propose an improved variational autoencoder architecture for this task. The key insight is to model the time-invariant content, and the time-dependent content separately, and to force the time-dependent parameters (motion parameters), to be very low dimensional.       The paper is complemented with experiments that demonstrate improvements of the previous state-of-the-art results.       However the results are only tested on relatively low-dimensional and binary input frames of the MNIST and Bouncing ball dataset. Moreover, the authors needed to fix the number of possible objects from 1-3 in these experiments. It is unclear how the proposed architecture can scale beyond toy datasets.       The authors note that "videos may contain an unknown and variable number of objects".       There are many works in the ML community which also try to predict the future video frames such as: "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning", or "Deep multi-scale video prediction beyond mean square error ". As such it would be good to compare results on more realistic video.       