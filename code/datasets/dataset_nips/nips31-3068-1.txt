Overall this is significant and original work because it is one of the first works to formulate keypoint detection and description as an end-to-end learning problem without relying on the output of hand-crafted detector for supervision. The approach follows the traditional SIFT recipe closely, replacing each component with a learning based counterpart, much like in LIFT. The authors side-step the non-differentiability of the Non-Max-Suppression step by setting up a siamese training paradigm and "freezing" one of the towers (meaning that no gradients are back-propped through it). The frozen tower computes keypoints which provide supervision for the non-frozen tower. The authors show good results on indoor and outdoor datasets.  Some criticisms:  The authors claim that end-to-end differentiability is important to simultaneously train the keypoints and descriptors and a core motivation of the proposed approach relies on the fact that selecting multiple extrema from a score map is not differentiable (line 33). But the empirical support for end-to-end differentiability isn't backed up by evidence.  To back up this claim, the authors can add one simple ablation study where the descriptor loss is not back-propagated all the way through to the keypoints and compare the results.  The evaluation uses too large of a pixel threshold (5 pixels) for computing matching score which makes it hard know if keypoints are precise. Precision of keypoints is important for many feature-based matching systems. Since the architecture is designed to output sub-pixel keypoint locations by using the softargmax (line 137), matching score should be evaluated at more strict thresholds which are less than 5 to provide evidence for this design decision.  The indoor training component of the systems relies on depth maps for establishing correspondence. The authors acknowledge that depth measurements are often missing around 3D object boundaries (line 109), which is often true. Since image regions which do not have depth data are disregarded during training, this may create an unwanted bias away from detecting edges and corners, which are critical for traditional detectors such as Harris or ORB. The bias might then cause the detector to lack the ability to latch onto local textures. To address this, the authors can evaluate to the matching score at lower thresholds as described above. It would also be interesting to see the keypoints overlayed on top of the depth maps to see if the network is detecting any keypoints where the depth maps are undefined.  The authors excluded an important result from Table 2: training indoors and testing outdoors. The authors cite the reason for doing so is the lack of photometric transformations in the indoor dataset (line 290). This may be true, but it doesn't justify excluding the result, as it may provide insight to further work which builds upon this. Selectively hiding results is bad science.  In terms of clarity, the loss functions and network outputs are explained clearly, but the main Figure 1 could use some work. It's hard to tell that two images are being input to the diagram, possibly due to the lack of visual symmetry. 