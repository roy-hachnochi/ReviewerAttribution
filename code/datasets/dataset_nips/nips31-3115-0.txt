Thanks for the feedback. To clarify, parallel NMT in the worked I pointed to get rids of auto-regressivity and not just recurrence. I agree it's orthogonal but could be the fastest NMT system --------- The paper develops a method for fast computation of top-k hypotheses (words in this paper) during decoding for natural language generation. In an offline pre-processing step, a graph is constructed with words as nodes in the graph and edges using parameters of a model trained in the usual way. The paper provides thorough experiments comparing to previous methods in the literature. The speedup offered by these methods is impressive. Other Comments:    1) There has been lot of recent work on parallel neural machine translation (https://arxiv.org/abs/1711.02281 ; https://arxiv.org/abs/1805.11063). Importantly, beam search is not required by these methods but computing the softmax is still necessary. It would be interesting to see how the proposed speedups compare to them and see if both these directions can be combined. 2) It would be useful to include decode time numbers in Table 1 3) Even though attention methods generally do not require k-best lists, there are some methods that do top-k attention instead of full attention (https://arxiv.org/abs/1607.01426). Having experiments on these models would make the paper even stronger. 