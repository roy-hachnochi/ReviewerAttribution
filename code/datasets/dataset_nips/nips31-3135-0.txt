The contributions of this paper are:  - Provide counter-examples to prove the hierarchical softmax with pick-one-label heuristic is not consistent unless the strong independent assumption is made  - The probabilistic label tree is a generalization of the hierarchical softmax when Precision@k is used in the loss. Based on this reduction, a no-regret bound can be proved.   - The authors provide a new implementation of PLTs and demonstrate its strong performance.   Pros.   - The paper is nicely written, providing nice motivation and clear explanations.   - The paper provides a comprehensive study and makes contributions in both theory and practice perspectives.   - Extreme classification is an important problem, and the paper provides a better understanding of the softmax method.   Cons.   - Although I found the results in the paper are interesting, the key contributions of this paper are mainly based on [17]. The extension of the no-regret proof of PLT with   Precision@k metric is relatively straightforward.   - The discussion is limited to only when Precision@k metric is used.   Other comments:  - Comparing the experimental results demonstrate in the paper with [17], is there a typo of FastXML on Amazon dataset in P@1? The results show in previous work is around 36.65, but this paper shows 54.1.   - XT is significantly worse than DISMEC on most datasets except Delicious-200K. Is there a specific situation that XT is expected to perform better? 