This paper studies a notion of generalization that allows multiple data-dependent interactions with the dataset. This notion is termed post-hoc generalization (or robust generalization in some previous work). The idea is that the output of the algorithm does not "reveal" any statistical query, i.e., the value of the query is similar on the dataset and the underlying population.  Post-hoc generalization was studied in a COLT'16 paper by Cummings et al. More broadly, this paper is continuing the line of work on "adaptive data analysis" that has been studied in statistics for a while, and more recently in computer science (starting with a STOC'15 paper by Dwork et al.)  The main contribution of the paper is a set of lower bounds for post-hoc generalization. In particular, the authors prove that:  1) A tight sample size lower bound of \sqrt{k}/eps^2 for any algorithm that can preserve k SQs up to error eps. Previously a lower bound of sqrt{k} was known for constant eps.   2) A similar computational lower bound (under cryptographic assumptions) that applies when the dimensionality of the data is smaller than the number of SQ queries.  3) In general, post-hoc generalization does NOT compose, in the worst-possible sense. (In contrast, it is known that special cases like differentially private algorithms are known to compose.)  Overall, this is a nice paper that I believe should be accepted to NIPS. While I am not an expert in the techniques from the previous works (hence, I cannot judge the technical novelty given the time I spent on the paper), it seems to me that this is a solid contribution. At the conceptual level, the paper makes several interesting points that would be of value for the NIPS community. 