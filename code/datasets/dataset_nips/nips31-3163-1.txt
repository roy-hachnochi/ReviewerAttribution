The paper proposes a unified framework of probability mapping functions that induces softmax, sum-normalization, spherical softmax, and sparsemax as special cases. Two novel sparse formulations are proposed under the framework that allows for a control of desired sparsity level. Experiments on synthetic and some standard NLP tasks demonstrate the effectiveness of the new mapping functions.   The paper is clear and well written. The proposed Sparsegen framework is novel that not only houses a number of existing probability mapping functions but also generates two new ones showing positive results on standard NLP tasks.   It would be interesting to see whether the new functions are able to help improve/get close faster to the state-of-the-art performance on these tasks, though it may be hard to sort out the contribution as attention is just one component of the network.