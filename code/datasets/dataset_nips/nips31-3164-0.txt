The authors propose a rather simple method to accelerate training of neural networks by doing a form of linear extrapolation to find the optimal step size.  The idea is simple enough, and the authors provide no theoretical analysis for this method. In a way, most optimization papers focus on convex methods, which they analyze well, and then they apply them to non-convex problems (e.g. neural networks). Iâ€™m not sure if the authors stumbled upon a very smart idea, or if they are a bit mistaken about how good their method is.  I am not very confident in my ability to determine the validity of this method. It seems to me that this could be a good idea that other people could try, and we could see how well it performs on other deep learning problems. I would accept this paper, but I would not be surprised to find out that some expert in optimization would chime in and point out why this method is a bad idea and it has already been tried.