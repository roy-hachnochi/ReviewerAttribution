The paper proposes a variant of a VAE model for labeled datasets. The model explicitly encodes the information corresponding to the label in a predetermined set of components of the latent code. Experimental evaluation on facial images are presented, showing that the model is able to capture the intra-class variability linked to a given class and performa tribute transfer.   The paper is well written and easy to follow. A few references are missing in my opinion (please see bellow). The idea of the paper is very interesting. The experimental evaluation seems convincing but limited. I believe that the paper would be stronger if more examples where presented (not only facial images). An very interesting example could be to have different dog breads as classes.  The model assigns a low dimensional subspace for each of the classes that the given label can take. This model is clear when there is little information shared among classes, such as in the binary labels (e.g. with or without sunglasses). But when this is not the case, this might not be the best way of factorizing the latent space (the number of variables grows linearly with the number of classes). An extreme example would be to use as labels the identity of the person in the picture.  In the case of the facial expressions, It would make sense for some of the latent variables to be shared among the different classes. As the labels seem a discretization of a continuous space, and an complete factorization. In Figure 3, what happens if one changes the value of some of the variables corresponding to the other classes while keeping the class of interest fixed?  It would be interesting to consider more than one label at the same time, as normally there are many factors that explain the data (as in the CelebA dataset). For instance to consider both facial hair as well as the presence of glasses (and two variables y_1 and y_2). How do you think that the model would scale up?  The authors should cite the variational fair auto encoder [A]. The authors propose a VAE that factorizes the latent variables from a given sensitive variable (e.g. gender). In order to reduce dependence of the representation on the sensitive variable they use MMD as an alternative to using minimizing the mutual information.  The idea of using an adversary to minimize the dependence of the representation with a given variable has also been explored in [B] and [C].  While the formulation is different, the work [D] seems a relevant reference.  [A] Louizos, Christos, et al. "The variational fair autoencoder." ICLR 2016 [B] Edwards, Harrison, and Amos Storkey. "Censoring representations with an adversary." ICLR 2016 [C] Ganin, Yaroslav, et al. "Domain-adversarial training of neural networks." JMLR 2016 [D] Mathieu, M, et al. "Disentangling factors of variation in deep representation using adversarial training." NIPS. 2016.