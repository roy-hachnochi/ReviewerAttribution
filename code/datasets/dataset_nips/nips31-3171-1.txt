Summary: This paper proposed a joint aproach for learning two network: a capitonbot that generates a caption given an image and a drawingbot that generates an image given a caption. For both caption and image generators, the authors use existing network architecture. LSTM - based network that incorporates an image feature produced by Resnet is used for caption generation (the specific architecture is not clearly described). Attention GAN is used to generate an image from caption. The main contribution of this paper is joint training of caption and image generators by constructing two auto-encoders. An image auto-encoder consists of a caption generator feeding an image generator. A caption auto-encoder consists of the same image generator feeding the same caption generator. Both auto-encoders are trained jointly to minimize reconstruction loss of images and captions (in alternative manner).  So for example, in the caption auto-encoder, caption generator is trained to generate captions that not only match the ground truth but are also predictive of the underlying concepts in the image. Same holds for training an image generator.  This joint learning approach is shown to perform better than independent training of caption and image generators on the COCO dataset. Additionally, authors propose a semi-supervised training modification where a small number of captioned images are used with a large number of uncaptioned ones. The proposed joint (turbo) learning approach results in a caption generator that outperforms an independetly trained one since the latter cannot incorporate unlabeled images.  Quality: The idea is interesting and the experimental results validate the advantage of the proposed turbo learning approach. Clarity: The paper is mostly clearly written. However, I did find several places a bit confusing. - When the LSTM-based caption generator is introduced its not clear how the image feature is incorporated in the network. - When the loss for the alternating optimization is explained, I expected it to be a function of the ground truth caption. I guess it’s implicitly buried inside other terms.  But it would be good to be more rigorous in your definitions. - Reconstruction loss between a true image and a generated image is measured via KL divergences w.r.t to distribution of object classes found in an image by a pre-trained image classifier. I think this is an important concepts that deserved more emphasis.  Originality: While previously explored in Machine Translation, the turbo-learning approach of joint training caption and image generation is novel and interesting. Also similiar  “butterfly” architectures have been used in image transfer learning (e.g.  Unsupervised Image-to-Image Translation Networks , Lie et al, 2018) Significance: This idea of jointly training two inverse networks can potentially be applied to other domains where such set up is possible. 