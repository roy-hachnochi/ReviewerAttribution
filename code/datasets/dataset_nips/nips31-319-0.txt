This paper proposes a combinatorial optimization approach, wherein GCN is applied to compute the probability of each vertex being part of the optimal solution. The technicality focuses on the MIS problem and other problems are first equivalently reduced to MIS. Then, GCN is trained to output a probability map, where the supervision is the ground truth from a synthetic training set. If the problem has multiple optimal solutions, multiple probability maps are obtained. Finally, the maps are used to guide the tree search of optimal solution for new problem instances.  The up side of this approach is that with a single synthetic training set, the trained network generalizes to other combinatorial problems, data sets, and much larger graphs. The experimental results are quite satisfactory.  The down side of this approach is that it solely focuses on MIS and solves other problems through reduction. Sometimes the reduction may result in very dense graphs that pose memory/time bottleneck for graph neural networks.  Overall, the paper demonstrates a successful use of GCN for combinatorial optimization. It also sets a good example for solving more than one NP-hard problem in a unified framework (through equivalence reduction). This work has good merits.