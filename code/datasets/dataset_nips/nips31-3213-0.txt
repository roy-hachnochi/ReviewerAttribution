The paper presents a theoretical analysis on the presentational capabilities of generative network, which is limited to a full-connected network architecture with ReLU activation function. It gives detail discussion on different cases where the input dimension is less than, equal to or greater than output dimension.   This paper is out of my area. Though I can follow most of the technical parts, I'm not familiar with the details in math and the assumptions. Overall it's an interesting reading to me, and the result can possibly shed light on generative networks training analysis.    Clarity Overall I think the paper is organized. One suggestion on related works: mention (Box et al., 1958) in section 1.1 instead of in conclusion section.  Minor: line 64 where citing “[Goodfellow et al., 2014]”, “Goodfellow et al. [2014]” to be consistent with other references format. 