REVIEW 3218:   Summary:  The paper introduces a first order algorithm for unconstrained convex optimization called AcceleGrad. AcceleGrad can be applied to three different settings without any modification with ensured guarantees. It ensures O(1/T^2) for smooth objectives, O(\sqrt(log T) / \sqrt(T)) in general non-smooth objectives, and guarantees an standard rate of O(\sqrt(log T) / \sqrt(T)) in general stochastic setting where we have access to an unbiased estimation of the gradients. AcceleGrad simultaneously applies to all these three settings. The algorithm is based on an adaptive learning rate technique with importance weights together with an update that linearly couples two auxiliary sequences. Additionally the paper presents new results regarding AdaGrad algorithm in the stochastic setting with a smooth loss. The paper ensures a convergence rate of O(1/T + \sigma/\sqrt(T)) for AdaGrad in this setting. Finally the paper backs up its theoretical guarantees with experimental results.   Main Comments:  The paper is very well-written and straightforward to follow. The problem and the existing work are described very well. The contributions of the paper seem novel and significant in the field of convex optimization.   