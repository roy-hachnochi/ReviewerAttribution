This paper proposes a method to capture long-range relations in images and videos. It's done by modeling interactions between any positions of any channels of a set of feature-maps. It's inspired by non-local module (NL) [31]. While NL aggregates all channel information together to encode position dependencies, the proposed method encode position dependencies between any channel.    I like the paper flow: it addresses a valid drawback of the non-local module, with a clear visualization in fig. 1 and proposes a generalized non-local (GNL) module to tackle the problem. Then, it counts the limitation of a naive implementation of the proposed method and tries to overcome it by proposing a compact representation to approximate GNL.  The paper seems to be technically correct. the formulations are correct as long as I checked them. It's well-written, well-structured, easy to follow and in details. The related work is okay and up to date.   The novelty of this paper is sufficient. It addresses the valid problem of NL in capturing fine-grained interactions of objects and actions. The paper proposes a generalized extension of NL where all the interactions between every position of every channel are modeled. As this Generalization is computationally prohibitive, the paper approximates it by Taylor series up to a certain order. I think this paper seems to be a useful contribution to the community.  The experiments are conducted on well-known datasets both in image and vision domain. Experiments are comprehensive on three tasks of fine-grained bird classification, action recognition, and object detection and in most of the cases, the proposed method outperforms others. Ablation study is there and informative. it seems experiments are reproducible.   minor: missing closed parentheses in table 1. L69-76 some repetitions.