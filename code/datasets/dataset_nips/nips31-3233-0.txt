This paper uses the concept or "back-matching" (target propagation) as a way to analyze deep networks, and particularly resnets. After showing that the standard gradient descent correspond to block-gradient step of back-matching, the authors use this to analyze the convergence of deep networks. They link the slow convergence of networks to ill-conditioned Hessian matrices of the blocks, using theoretical and empirical arguments, and linking to previously published papers. They explain how skip connections and batch normalization produce better conditioned Hessians, which could explain their success. Finally, they use this analysis to derive a simple variant of SGD that improves the conditioning of the Hessian and produces better empirical results.  Pros: - This paper is well written, clear and cites previous work. - The paper uses sound theoretical arguments to explain things that have been observed in other works. - The author derive a simple algorithm that indeed produces good results, which is an indicator that the theory and insights are likely to be correct. Besides, this algorithm (scale-amended SGD) can be used in practice.  Cons: - The experimental results could be stronger. Although many experimental results this paper cites come from ImageNet, their own experiments are only run on CIFAR-10 and CIFAR-100. Therefore, it is unclear if scale-amended SGD will be useful in practice on large dataset.