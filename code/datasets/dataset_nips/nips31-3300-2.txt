This paper provide new results on coresets (a reduced data set, which can be used as substitute for the large full data set), for use in logistic regression.  Once the data is reduced, black-box logistic regression code can be run on the reduced set (now faster since the data is smaller) and the result is guaranteed to be close (with relative error i the loss function) to that of the full data set.    Specifically, this paper    (a) shows that no coreset of size o(n / log n) is possible, even if the coreset is not a strict subset of the original data.  This is quite a strong negative result.    (b) introduces a data parameter mu, which represents how balanced and separable the dataset can be (the smaller the better).  If the input data has this data large, this often corresponds to ill-posed problems (data is not well-separable, or heavily imbalanced), and hence logistic regression is not a good fit.    (c) provides an algorithm, using a variant of sensitivity sampling, that if mu is small, is guaranteed to provide a small coreset with strong approximation guarantees.  The algorithm essential reduces to a (approximate) QR decomposition to learn weights, and the sampling.  It empirically performs favorably to uniform sample, and a previous approach which essentially performs approximate k-means++ clustering.      This is a clearly written and precise result on a topic of significant interest.  The paper provides a careful and informed view of prior work in this area, and builds on it in  careful ways.  While the use of mu, may not be the final result on this topic, this is a real and important concrete benchmark.      Suggestions and minor concerns:  (these are secondary to the main contributions)  * the dependence in the coreset size on mu, d, and eps are large when the size is logarithmic in the size n:  mu^6, 1/eps^4, d^4.  plus some log factors.   (alternatively, the size requires sqrt{n} space with smaller values in other parameters :  mu, 1/eps^2, d^{3/2})  * The paper says mu can be estimated efficiently (in poly(nd) time).  In this context, this does not seem very efficient.  How should this be estimated in the context of the near-linear runtime algorithms?  What are the values for the data sets used?    * it would be interesting to see how this compared to sensitivity sampling-based approaches (e.g., Barger and Feldman) designed for clustering applications.  