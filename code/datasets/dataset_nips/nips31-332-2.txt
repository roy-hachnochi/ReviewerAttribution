Self-Handicapping Network for Integral Object Attention   The paper studies the problem of generating an object-level attention map using image-level supervision. This is a fundamental problem and is helpful for understanding deep CNNs. However, the problem is very challenging since only weak supervision is given. The paper follows the work of adversarial erasing [35] for weakly-supervised object attention, which iteratively mines object pixels by erasing the detected object regions. However, when stopping the erasing process is a problem. Thus, the paper proposes a self-handicapping strategy. The main idea is to fully utilize background prior. It designs a new attention network in which the background prior can revise the sign of feature map to constrain the discovering of object attention in a zone of potential instead of the whole image.  Pros: + It propose a new self-handicapping network which extents the original CAM network with multiple branches. By threshing the score map generated by CAM with two thresholds, it obtains a ternary map acts as a switch to edit the feature maps in the following branches. The ternary map can both help to suppress background prior and erasing the mined foreground. It is an elegant design. + The method has been proven to be effective in solving the weakly-supervised semantic segmentation problem. Testing in the Pascal VOC dataset, the proposed ShaNet outperforms some recent methods. In addition, the effectiveness of self-handicapping has been visually illustrated. + The paper is well-written and easy to follow.  Cons: - In figure 3, I can not observe the difference between the branches of S_B and S_C. However, from the description in Self-handicapping strategy I and II, the C-ReLU functions are different in the two branches. Thus, figure 3 should be improved. - The reason why S_C is ignored in testing is not given. The authors should justify why S_C is helpful in training while not helpful in testing.