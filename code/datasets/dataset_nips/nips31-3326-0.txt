Summary: This paper investigates Real-Time Recurrent Learning (RTRL) for training recurrent neural networks. As RTRL has a high-computational/memory costs, authors propose to approximate RTRL using the Kronecker Factorization.   For a subclass of RNN, authors demonstrate that their approximation, KF-RTRL, is an unbiased estimate of the true gradient and has lower variance than previously proposed  RTRL approximation such as UORO (relying on a low-rank approximation).  KF-RTRL is empirically evaluated on a copy and language modeling task. Authors validate that KF-RTRL is a competitive alternative to UORO, also it underperforms truncated Backpropagation through time.  Clarity: The paper is well written and pleasant to read. Few minors point: - While z is properly defined in the text, it is not clear what z is from algorithm 1 alone.  - What does the x-axis correspond to in Figure (1) (a) and Figure (2) ? Is it the overall training time in second?  Quality: The submission seems technically sound. KF-RTRL shows an advantage over UORO both theoretically and in the empirical evaluation. Authors also  compare their approach with truncated backpropagation through time, and show that RTRL approaches can still be improved.  It would be informative to reports the memory requirement/running time of the different algorithms. In particular, does KF-RTRL outperform UORO in term of training time ? Is TBPTT-25 more memory intensive than KF-RTRL?  It seems that KF-RTRL do not capture well long-term dependency? The maximum T used in the copy experiment is 36 while in the second experiment TBPTT with a context of 25 outperforms KF-RTRL.  Is  it related to the requirement on the spectral norm in Theorem 1., which states that the spectral norm on H_t must be less than 1, hence leading to vanishing gradients?  Originality: Using Kronecker Factorization  to approximate RTRL algorithm appears novel to me.  Significance. The memory cost associated with training RNN is an important limitation. Tackling this issue could allow the exploration of larger RNN models. This paper is one step in this direction and seems relevant to the community.  This paper introduces a novel approximation of RTRL which improve upon previous RTRL method. While KF-RTRL does not outperform truncated backpropagation through time, it is one step going in that direction. The paper is well written and pleasant to read, and the proposed method  is interesting. For those reasons, I recommend acceptance.  After Rebuttal: Thanks for the feedback! After reading the authors respond and the other reviews, I still feel that score of 7 is appropriate for this paper.