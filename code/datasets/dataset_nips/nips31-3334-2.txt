Within the area of adversarial learning, the idea of multiple parties providing data to a learner (and one party attempting to mislead the learner by corrupting data) is often considered. This paper does a good job of crystalizing that setting and exploring some of the more subtle issues such as privacy and the corrupting party remaining undetected. It's nice to see everything about the setting made explicit. The algorithms (of both the attacker and the learning system) are elegantly simple and reasonably well motivated.   The paper is very clearly written, and the empirical analysis seems solid. The theoretical results strike me as an incremental step beyond the results of [20], but the authors do not claim otherwise. The empirical results are convincing, although I would have liked to see better motivation for the (admittedly interesting) measure of "contamination accuracy".  The authors point out that their work is similar to work on poisoning attacks. I would have liked to see a deeper discussion as to how(/if) this paper is more than just an example of a poisoning attack. To me, what makes the work interesting is the fact that the learning system has a general underlying learner (that is, the attacker is attacking a system wherein two models are trained and one is selected based on validation error, rather than the typical setting where the attacker is attacking a specific learner(e.g., SVM as in [2])). Of additional interest, there is this notion where the attacker is actively trying to avoid detection (that is, it's constraint is defined by the fact that the party model must be selected over the local model). These aspects are what differentiate this work from similar methods of attack design, but the discussion of that is missing in the paper.  Overall, the paper is well written and performs a good empirical investigation of an interesting setting. The ties to prior work, I feel, are somewhat lacking. While the differences (between the paper and prior work) are interesting, it's left to the reader to articulate exactly why.  Some smaller comments: - On lines 72, 73: The authors cite [4, 9, 18, 27, 24] in terms of attacks on models, and [1, 2, 15, 17, 34, 35] as attacks on learners. [1] actually attacks a learned model, not a learner. - On line 113: b \in \mathbb{N}^{|D|} means b is a vector of |D| natural numbers, but in Procedure ManipulateData, b is used as a scaler. I'm not sure what's meant there.  I thank the authors for their feedback. 