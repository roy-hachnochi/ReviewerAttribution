This paper compares the embedding of a 3-layer LSTM to the neural responses of people listening to podcasts recorded via fMRI.  The experiments vary the number of layers in the  LSTM, and then context available to the LSTM and compare it to a context-free word embedding model.  This is a strong paper, well written and clear.  The results are thorough and there are a few interesting surprises.   I have a few questions of clarification.   1) How do the authors account for the differences in number of words per TR due to differing word length and prosody?  No mention of this adjustment is made.  Other questions that would have been worth answering in this paper are the effect of corpora or LSTM variant on the performance.  In particular, I see the argument for using the Reddit corpora, but wonder what the effect would have been to use another corpora of similar size.  Here’s a great resources for finding such corpora, which includes some examples of transcribed speech (possibly a better fit to the podcasts being used as stimuli) http://courses.washington.edu/englhtml/engl560/corplingresources.htm  I think many (most) NIPS attendees will not be familiar with the flat map, so more waypoints in the figures would be helpful (I see zero labels in F3, and only 3 in F4/5), as well as a link to a resource for interpreting flat maps.  Also, BA doesn’t appear to be defined and it’s not clear which ROI it is assigned to in the flat maps, particularly in the LH.  To my knowledge, the closest work to this is the 2014 Wehbe paper.  It would be interesting to see a more thorough comparison to the results of that paper.  There is only one sentence really comparing to previous results (line 258-9) and a deeper comparison would be nice to see.    Minor comments: -Line 106: another critical flaw in some/most of these models is that the words are presented in isolation -Should cite the new Hale paper (https://arxiv.org/pdf/1806.04127.pdf Hale paper was *not* out when the current paper was submitted) -Line 173-176 are pretty unclear.  Are you just describing the typical SEM?  Or is it simply the standard deviation?  If it’s a standard formula, this description can probably be dropped as it’s convoluted and not particularly helpful.  Otherwise, stating it as an actual formula would be better.   