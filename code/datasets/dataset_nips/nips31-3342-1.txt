The paper identifies and fixes a particular type of target leakage when processing categorical features for boosted decision trees. The authors fix this by imposing a (random) ordering of the samples and they use the same principle for building multiple trees.   The paper shows strong empirical results against well known baselines. It is however hard to attribute the observed improvements to what the paper mostly talks about (i.e. the ordering principle). In the appendix for example, we see that a lot of lift comes from feature combinations. There are also other design decisions (e.g. use of oblivious trees) and I suspect the authors know how to tune the hyperparameters of their algorithm better than for the baselines.   What would greatly strengthen the paper would be to show what would happen to XGBoost/LightGBM if the ordering principle was applied in their codebase. Otherwise we might be just observing an interesting interaction between ordering and some other design decisions in CatBoost.  Finally one part that was not clear to me is why isn't there a new permutation in each iteration but instead only s of them. 