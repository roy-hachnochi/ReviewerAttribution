There has been a lot of interest recently in clustering using answers to pairwise membership queries of type "Are a and b in the same cluster?" obtained from humans for example via crowdsourcing. This paper considers the problem of finding an (1+\eps)-approximation of k-means using an oracle that can answer such pairwise queries. Both noiseless and noisy oracle settings are studied and upper bounds on number of queries made to obtain an (1+\eps)-approximation to k-means under assumptions on smallest cluster size are provided.   Under the noiseless oracle setting, using assumptions on smallest cluster size, it is shown that an (1+\eps)-approx. for k-means can be obtained using O(K^3/\eps) with high probability (where K is the number of clusters). This is an order wise improvement in terms of K and \eps over the previous bound of O(K^9/\eps^4) albeit the previous result doesn't assume any constraints on the cluster sizes. The proof technique used is interesting compared to the existing literature in k-means approximation.   Line 49: The order of queries in [12] for noiseless setting is O(K^9/\eps^4). The bound O(ndK^9/\eps^4) is on the run time.   For the noisy setting, the algorithm proposed essentially uses the existing work by Mazumdar and Saha[24], to obtain the a clusters (enough points in each clusters) on a subsampled set which is then used to estimate the cluster centers to obtain an (1+\eps)-approximation. The upper bound on the number of queries here seems to be sub-optimal in terms of error probability p_e of the noisy oracle: 1/(1-2p_e)^8 ignoring the log terms. Is this an artefact of using using the algorithm from Mazumdar and Saha[24]?   Description of Algorithm 2 is not very clear. Why is A an input while in phase 1 set A is being drawn from X? Naming  of phase 1 makes it very confusing as it reads as if  Alg 5 is run on A that is input and the whole process is repeated again. Probably, "Seeding" is just enough to describe this phase.  While the case with outliers is considered, very stringent constraints are assumed on the outliers as well as the noisy oracle when outliers are involved in the queries. What is the intuition for the second term in Def. 2.4?  Line 301-302: Is the comment on the noisy sampling method referring to the noisy oracle? Can you be more specific as to why it substantially differs from your setting? From the description in [12], their noisy oracle (faulty query setting) is exactly same as the one described in this paper (except that [12] does not consider outliers).  How do the results in this paper compare with Mazumdar and Saha, NIPS 2017, "Query Complexity of Clustering with Side Information"?  Minor comments: ============== 1. Paragraph starting at line 114: There seems to be something missing in the description of this algorithm. It starts off with saying there are two steps and describing the first step. Then the discussion never comes back to say what the second step is. It looks like the second step is essentially computing the centroids on the output of step one, but it is not mentioned. 2. Line 100: The lemma referred to here is stated later Section 2, however it is not mentioned that it is stated later. 3. Line 120: Did you mean to say fraction instead of faction? 4. Line 124: 'queries' should be queried. 5. Line 167: point set S: S here should be script S to be consistent with notation in the equation that follows. 6. Line 200: Def. 2.4: Using \eps here is a bad notation unless this \eps is the same \eps as that used for 1+\eps approximation. 7. Paragraph starting at line 221: Again there is a mention of two steps and the first step is described. The second step is never mentioned.  8. Line 225: Lemma 3.1: Using \alpha \in (0, 1) here is a bad notation. \alpha \in [1, n/K] is very important quantity used to define imbalance of clusters. 9. Line 241-244: Notation for point set A is not consistent. Either use A or script A.  ---- I thank the authors for their response to all the reviews. I hope that the description of the algorithm will be revised if the paper is accepted.