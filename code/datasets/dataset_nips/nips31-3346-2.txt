This paper studies the problem of query k-means clustering, which was initiated in reference [11] (published in NIPS 2016). Query k-means clustering is in a semi-supervised active setting, where queries can be sent to an oracle to receive answers like two points are from the same cluster or different cluster. Following the work in [12] (published in ITCS 2018) which addresses the issue of noisy same-cluster queries in the context of the K-means++ algorithm, this work 1) adds constraints on the smallest cluster sizes to reduce sampling complexity; and 2) identifies outliers that are at large distance from all clusters. Theoretical proofs are provided for the performance guarantees in the case with and without outliers (Theorem 1.1 and 1.2, respectively). The proof is based on generalizations of the double Dixie coup problem. Comparing to [12], the proposed method is more efficient in query complexity. Results in experiments valid the performance of the proposed method on 1) the  closeness to the optimal solution, the query complexity and the misclassification rate.  The difference to existing work is clearly presented, with theoretical analysis and experimental evaluation. However, the work can be highly improved by including  1) the evaluation of misclassification ratio on real data; The current evaluation on real data is only about the query complexity. It is interesting to know the classification accuracy of the presented method, and that of baseline in [12]. 2) the evaluation of accuracy on outlier identification. Authors claimed that “we wish to simultaneously perform approximate clustering and outlier identification”. Algorithm 5 in the Supplement treats outliers as separated clusters. Can the performance of outlier identification be also evaluated?  The paper is generally very well written. There are few typos to correct, like “an framework”, “Theorem 5.3” (should be 1.2).  Authors made clarification in the feedback. Wish that the clarification can be included in the revision before publishing. 