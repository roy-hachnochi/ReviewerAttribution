This paper proposed the feature replay algorithm to decouple and parallelize the backpropagation of neural networks. They also provided the theoretical analysis to prove the method is guaranteed to converge to critical points under certain conditions. Their experimental studies demonstrated their method can not only achieve faster convergence but also lower memory consumption and better testing performance comparing with baseline methods.  Comparing with other existing decoupling and parallelizing methods, the proposed method, which only uses historical features to update weights, does not need to use additional memories to store stale gradients and also guaranteed the convergence.    The strengths of this paper are: 1.    The feature replay algorithm has good novelty. The way it decoupled and parallelized the backpropagation of neural networks can solve backward locking, memory explosion and accuracy loss issues.  2.    Provided solid theoretical convergence analysis of the algorithm.  3.    Deployed good experimental studies to demonstrate the performance of their method.  The weakness is that the paper only demonstrated their method on Cifar-10 and Cifar-100. However, more experiments on large datasets like image-net should be included. 