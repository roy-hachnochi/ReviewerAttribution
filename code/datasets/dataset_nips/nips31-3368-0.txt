*** Update after author feedback ***  Thank you for your feedback. I didn't understand first that the model cannot only be used for the LMC models, but also for convolutions. This is great and extends the usefulness of the model further. *****************************************  The paper introduces a new multi-output model for heterogenous outputs, i.e. each output can have its own likelihood function. The models builds upon the linear model of coregionalization for coupling the latent regressors of the multiple outputs and uses stochastic variational inference to allow for non-Gaussian likelihoods.   The main concern I have about the paper is its novelty in terms of technical contribution. The paper combines the LMC model for the Gaussian case (Bonilla et al., 2014, I missed the citation!) with progress for other types of likelihoods (Saul et al, 2016). Are both models special cases of the aforementioned model or are there any differences in the inference scheme?  However, the paper is well written and the experiments are convincing which is why I would recommend a "weak accept". The paper also comes along with a Python implementation that allows to quickly try out multi-output models with new data types which could lead to a wider spread of multi-output models for that type of data.  Minors: L 186: Why would the Monte Carlo approach be slower than Gaussian-Hermite quadrature? Please be more explicit. L 291: Please state the standard deviations. L 267: What are the features? I first thought it is the time but then the sentence (270-272) does not make any sense. L 268: The dataset consists of 604,800 samples but only 750 samples are used for training. How is the performance        w.r.t. runtime/NLPD if the training setsize is increased?  Literature: - Nguyen, Trung V., and Edwin V. Bonilla. "Collaborative Multi-output Gaussian Processes." UAI. 2014. - Saul, Alan D., et al. "Chained gaussian processes." Artificial Intelligence and Statistics. 2016.   