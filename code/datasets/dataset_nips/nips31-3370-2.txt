For the valuable problem of large-scale and sparse stochastic inference on simplex, the authors proposed a novel Stochastic gradient Markov chain Monte Carlo (SGMCMC) method, which is based on the Cox-Ingersoll-Ross (CIR) process. Compared with the commonly-used Langevin diffusion within the SGMCMC community, the CIR process (i) is closely related to the flexible Gamma distribution, and therefore more suitable for inferring a Dirichlet distribution on simplex, since a Dirichlet distribution is just the normalization of Gamma distributions; (ii) CIR has no discretization error, which is shown to be a clear advantage over the Langevin diffusion on simplex inference.  Besides, the author proved that the proposed SCIR method is asymptotically unbiased, and has improved performance over other SGMCMC method on sparse simplex problem via two experiments, namely inferring a LDA on a dataset of scraped Wikipedia documents and inferring a Bayesian nonparametric mixture model on Microsoft user dataset.  I think the quality is good; the presentation is clear; as far as I know the proposed technique is original and of great significance. Therefore I vote for acceptance. However, the experiments are okay, but not strong. More experiments would be better.   I have read the authors' response, I would suggest additional experiments, such as exploiting the presented SCIR for more complicated non-conjugate models like the PGBN [1]; (2) to make a direct comparison with the TLASGR-MCMC developed in [2], which is also developed for inference on simplex and is shown better than the SGRLD. Other potential reference methods include the ones in [3,4].   [1] The Poisson gamma belief network. NIPS 2015. [2] Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC. ICML 2017. [3] "A Complete Recipe for Stochastic Gradient MCMC. NIPS 2015" [4] "Stochastic gradient geodesic MCMC methods. NIPS 2016"