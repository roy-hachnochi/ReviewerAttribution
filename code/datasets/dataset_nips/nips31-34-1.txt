This paper introduced a new normalization method for improving and accelerating DNN training. The proposed method, Kalman Normalization (KN) estimates the statistic of a layer with the information of all its preceding layers. This idea is vary similar to to shortcut link in the ResNet, however, KN provides better statistic estimation.  The experiments presented solid results and conclusions that show KN not only improves training accuracy, but also help convergence as well.    Strength of this paper: 1. The proposed normalization method is new, although it mimic the process of Kalman filter and has similar idea as ResNet. This approach shows a new angel and as experiment showed, very effective.  2. The experiments are well designed. The author not only compared with the state-of-the-art results , but also showed the evidence of more accurate statistic estimations that KN can provide.   Some mirror suggestions: 1. In experiment section 4.1.1, it would be interesting to report how many step did KN use to get the best performance (74%) 2.Typo: Table 5 AP^{maks} ->A^{mask} 3. In Figure 5, the vertical axis represents neurons if different channels, and we can see that there are some kind of channel patterns: can you explain why some channels have very bad gap in (a) but seems to be a lot better with KN?   Overall, I believe this is a paper with high quality and the proposed normalization method will help different applications in DNN area.     