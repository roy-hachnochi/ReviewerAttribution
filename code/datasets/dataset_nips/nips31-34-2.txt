This paper presents a normalization method, called Kalman Normalization (KN), which is a variant of Batch Normalization (BN). It aims to alleviate the issue of BN in case like the micro batch size, which is hard to estimate true means and variances. The solution is to utilize the statistics of preceding layers.  The empirical results seem good(e.g., Table 1).  My main concern is that KN introduces many learnable parameters. For instance, in Eq. 6, the transition matrix A, the covariance matrix R and the gain value q are required to learn for each layer during the training. Thus, in the case of large neural networks as line 202, it is hard to fit the model in GPUs. I’d also like to see the parameters comparison in Table 1. Practically, for large neural networks, model parallelism is an option as well.  In the extreme case, such as batch size is 1, I am wondering if it is necessary to use BN/KN. What’s the performance without any normalization?