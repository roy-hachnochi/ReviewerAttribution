Summary: This paper shows how we can compute gradients of rational and tropical structured losses using weighted finite state transducers to set up a shortest path problem. Doing so can allow us to use gradient-based training for such losses, which includes n-gram overlap and edit-distance as losses.  Review:  The paper presents an interesting technical contribution -- namely, a blueprint for computing gradients of certain families of losses. This could be quite useful for several applications.  How long does the construction of the WFSTs take in practice? Of course, from a practical point of view, the WFSTs \mathcal{M} could be constructed when a dataset is being read because it needs to be done only once.   The paper says in the introduction that if we could train with non-decomposable losses (such as F1), it might help train better models. However, this could be a bit misleading because eventually, the paper does consider decomposable losses.  Related work: The direct loss minimization work of McAllester et al., 2010 and its followup work. While the proposed method is clearly different, the goals are similar. So please comment on this.  While the math in the paper seems to check out, it would be useful if the paper (perhaps the appendix) provides a worked example of how \mathcal{B} is constructed for the tropical case. Right now, the reader has to infer this information from the examples â€” a bit more explanation would be helpful to generalize this to other new losses.  Minor quibbles: - Some notation is confusing. Line 92 uses subscripts of y to   indicate the i^th label. Elsewhere, y_i is used to refer to the   i^th labeled example. - As an aside, perhaps it would be useful to index \mathcal{M} with   the index i or the reference structure y_i because it is a   function of the reference structure.