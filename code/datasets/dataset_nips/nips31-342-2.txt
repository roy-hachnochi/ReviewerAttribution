Summary:  This paper presents a framework (referred as to LinkNet) for scene graph generation, i.e. the task of generating a visually grounded scene graph of an image. In a scene graph objects are represented as nodes and relationships between them as directed edges. State-of-the-art scene graph generation results are reported on VisualGenome.  Strengths:  - The proposed approach seems really effective. It combines three main components: i) a global-context encoding module, ii) a relational embedding module, iii) a geometric layout encoding module. I believe the overall approach is going in an very interesting direction by trying to encode all the main ingredients that can lead to scene graph generation. This is achieved in an end-to-end architecture.  - Experimental results are very interesting. The proposed model outperforms recent related works on predicate classification, scene graph classification, and scene graph detection, on the large VisualGenome dataset.  Weaknesses:  - Presentation could be improved. In general the method is well presented and I am somewhat confident that the model could be implemented by the readers. However, although very effective, the approach seems incremental and the authors should better clarify their main contributions. For example, the authors claim that comparing to previous methods (e.g. [17,32]) which focus on message passing between nodes and edges, the proposed model explicitly reasons about the relations within nodes and edges and predicts graph elements in multiple steps [34]. I think the author should clarify what are the main differences w.r.t. to both [17,32] and [34].  - The relational embedding module explicitly models inter-dependency among all the object instances. This seems a major difference with respect to similar previous works (see above). The authors claim that this significantly improve both object classification and relationship classification. I ma not fully convinced that this has been clearly highlighted by the experimental results.  Questions and other minor comments:  - What those it mean that the Predicate Classification and Scene Graph Classification tasks assume exactly same perfect detector across the methods (Table 1)? Are you using ground-truth data instead of the object detection outputs?  - Ablation studies and qualitative evaluations are very interesting. However, I think Figure 2 is quite hard to follow. Maybe you can just report three examples (instead of six) and try to visualise the relationships encoded by the relational embedding matrix. Section 4.3 is probably ok, but I think it takes a while to parse those examples.  - Another comment regarding Fig.2 and Section 4.3: it seems that the ground truth matrix for the first example is wrong. The cell (7,0) should be also in yellow since there are four instances of 'rocks' (4,5,6,7). Moreover, I believe you should also report the color bars in order to better understand the effectiveness of your relational embedding matrices.