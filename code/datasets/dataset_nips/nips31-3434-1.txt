The authors describe a large-scale transfer learning approach for tuning the hyperparameters of a machine learning model. They describe an adaptive Bayesian linear regression model. They attach one Bayesian linear regression model for each task and the information is combined by coupling the models through a deep neural net. They show extensive experimental results to validate the claims as well as good comparisons with the state-of-the-art.  The paper is very well written and was a pleasure to read. It is highly significant to the field of autoML as it greatly reduces the time in obtaining optimal hyperparameters. The paper is technically correct and I would vote for a marginal accept. My detailed comments are given below.  1. My one concern is that there is a huge similarity with Deep Kernel Learning (DKL) which the authors acknowledge but they claim that "it is substantially simpler to implement and less costly to run". However, unless the code is shared this is a tough argument. Moreover, the DKL code is openly available and it would have been nicer to see a comparison between the two setups, especially using the KISS-GP approach for scalability.    2. An algorithm section seems to be lacking. Unless the reader is very familiar with the previous literature it might be a bit tough to follow. There are several mentions of BO and Acquisition Function but for someone who is unfamiliar with its use, it will very difficult to tie things together.  3. I really liked the extensive experiments section as it highlights the major impact of the paper. The only thing lacking is a comparison with DKL to portray the simplicity / less cost of this algorithm.  Update: Although the authors gave a description of DKL and its drawbacks, the code was not made available and as a community, it might be tough to reproduce such results. 