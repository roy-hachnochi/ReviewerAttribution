This work presents a method for transfer learning in hyperparameter optimization. The transfer learning takes the form of a feature mapping that is shared by all tasks (datasets), upon which a Bayesian linear regression model is then superposed. The main contribution of the paper is to propose an end-to-end learning algorithm able to jointly tune the underlying neural network features shared by all tasks. Some clever Bayesian parameterization allows to tune the representation jointly while integrating out the weights for each independent task.  Quality: the quality of this paper is high, although the experiments were a bit underwhelming. The paper is lacking some clarification or justification on the use of a surrogate model for evaluation.  Clarity: I found almost no errors or imprecisions in the presentation of the method.  Originality: the work is novel and builds upon previous works in the literature. The ideas are straightforward and that is what makes them great.  Significance: I think this work has its place in the literature on AutoML. Once people start sharing representations learned on thousands of problems, the process of optimizing a new algorithm's hyperparameter will be straightforward and quick. It does not suffer from the drawbacks of many other works on transfer learning in hyperparameter optimization (limited number of possible algorithm configurations, can only be used for warm-starting optimizations, etc.). As stated above, the experimental section of the work does hinder its significance. More comments on this below.  General comments and questions:  I find it very interesting the ABLR-NN with transfer learning was able to perform about as good with or without the context information, which was very strong information.  On the OpenML experiments, to be clear, you didn't train any model on the OpenML datasets for the transfer part? What about for the final evaluation? From reading the supplementary material (line 65), I seem to understand that a surrogate model trained on the OpenML observations was used rather than training models? Am I correct in my assessment?  Relying on a surrogate model for performance measurement, how can we be sure of the validity of the results? What if a model was sampled in a under-represented region of the hyperparameter space (esp. true for the 10-D space of XGBoost)? I think this should be explicited and justified in the paper, not pushed back to the supplementary material. Right now it feels a bit misleading. I think some follow-up experiments with a real evaluation of the performance by training and testing classifiers would make for a stronger case.  Also, why use a nearest neighbor? From the experiments in Eggensperger et al. (2012), it was clearly the worst of all surrogate models.  The performance of the models in Figure 2b) does not seem to have plateaued at iteration 200, they seem to be on a steady downward slope. Could this be a side effect of the use of a surrogate model for evaluation? In my experience, tree ensemble models have relatively easy to tune hyperparameters and do not benefit greatly from longer hyperparameter optimization budgets.  line 298: "[running ... in this] setting take" should be "[running ... in this] setting takeS"   Due to the potential issues with the use of a surrogate model, I am giving this paper a weak accept. I believe the experiments are still valid, but they are made weaker by this in my opinion (esp. when we can see artifacts of the use of such surrogate model, e.g. Figure 2b). It is possible that I misinterpreted some elements, in that case please enlighten me (and make the paper more clear on the matter).  UPDATE: after reading author feedback, I still think the use of a surrogate model is strong limiting factor of the experimental study. This work would benefit strongly from doing real-world experiments (or at least doing part of the evaluation in the classical method).