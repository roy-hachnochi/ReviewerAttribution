The work proposed a nonparametric Bayesian model for event tensor decomposition. Existing tensor works lack of a way to integrate the complete temporal information in the factorization. This work formulates the so called “event-tensor” to preserver all the time stamps, where each entry consists of a sequence of events rather than a value. To decompose the event-tensor, the work hybridizes Gaussian processes and Hawkes processes to model the entries as mutually excited Hawkes processes, and base rates Gaussian processes on the latent factors. The authors exploit the Poisson process super-position theorem and variational sparse GP framework to derive a decomposable variational lower bound and develop a doubly stochastic algorithm for scalable inference. The method is examined on both simulation and real datasets, exhibits superior predictive performance to commonly used factorization methods, and finds interpretable discoveries.   This is a well-written paper, and I enjoy reading it. The work is very interesting --- temporal information are common in practical applications, and how to effectively capture rich dependency structures from these temporal information, and use them in tensor decomposition is important. This work is the first one to address this issue, and the proposed model is novel and interesting. I particularly like the idea that incorporates the latent factors into the design of the triggering kernel so that those excitation effects can be encoded into the latent factors as well. The inference algorithm is also scalable. The derived variational lower bound has an interesting structure that can be decomposed over both tensor entries and interaction events; so the authors can develop a doubly stochastic estimation algorithm.  The experimental results show interpretable discoveries on 911 and ufo data. Overall, since the proposed data formulation exists in a wide variety of applications, this work seems to have a broad spectrum of usage.         Here are a few minor questions: 1. For a decomposition model, I would like to know how to reconstruct missing entries? In this work, it would be missing event sequence 2. It seems like this work can be extended to general embedding learning approaches for high-order interactions/relations. What do you consider?