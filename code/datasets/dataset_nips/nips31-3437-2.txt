The authors present a method of scaling GP regression that takes advantage of the derivative information of the function. Scalability in the number of samples and dimensions is achieved by approximation of the kernel matrix using structured kernel interpolation (SKI) and SKI for products (SKIP) methods. The authors demonstrate the promise of their method on several numerical examples. The method is extended for Bayesian optimization by using dimension reduction via subspace projection.   The method is simple and combines several well-known methods for better and scalable GP regression. The manuscript is well-written and numerical examples are diverse.    Major issues:  - A major criticism of this paper is that the work in incremental. The authors rely on SKI and SKIP for scalability. The use of derivatives (first and second) is well known in Bayesian uncertainty quantification literature and is a special case of multi-output GP. Finally, the dimension reduction approach is also well-known. This work is good but the authors fail to emphasize the "NIPS-level" novelty of this work. - No guarantees are provided in terms of the quality of approximation. In particular, how does the approximation error decay as a function of n, d, number of inducing points, and smoothness of the function.  - How do the authors propose to use their method where the function is non-differentiable? Some of the most widely used GPs are non-differentiable; for example, Brownian motion.