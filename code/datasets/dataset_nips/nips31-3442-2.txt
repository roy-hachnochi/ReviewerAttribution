To the authors:  I have read the rebuttal and it helped me gain better understanding of the paper. I maintain my rating on the paper in hope that it gets accepted.  Adversarial machine learning has focused mainly on solving a minimax problem, with the inner part representing the worst loss that can be caused by an adversary that is constrained to perturb within a certain region or according to a certain distribution. A potential weakness, as is brought up as the motivation of this paper, is that when the adversary is beyond the pre-defined constraints, the model performance could deteriorate sharply. With this in mind, the authors proposed a Bayesian approach that impose a distribution over the adversarial data-generating distribution, and aim to learn the optimal posterior distribution of the learner given observed data, obtained by Gibbs sampling.  While I think the idea is quite interesting and worth being considered for publication, I have a few questions and comments:  1. Why design the distribution over data distribution as (1) and (2)? 2. What is the consideration for the design in the paper where the learner uses a sampling strategy to determine its model parameter instead of performing optimization? 3. From the experiment result in the left-hand plot for both figures 1 and 3, it seems to me that BAL is not generating much better performance than the benchmarks. In fact, for small epsilon it is performing slightly worse. What is the potential cause behind this? 4. I think there are grammar errors in the paper. For example, Line 26 defense -> defend. The authors are advised to make a full check if the paper gets accepted.