EDIT: I've read the response and I am satisfied with the answers. I maintain my grade.   # Summary of the paper and its contributions Determinantal point processes (DPPs) have been investigated in recommendation systems. In that framework and once a DPP has been learned, the authors remark that finding the sample with maximum DPP likelihood (the so-called "MAP recommendation") does not lead to meaningful recommendations. The contributions are as follows. The authors introduce another utility function, the maximum induced cardinality (MIC), and explain how to approximately optimize it using submodular approximations. Algorithms to optimize the MIC criterion are compared on synthetic datasets.  # Summary of the review The contribution is interesting and novel, and the paper rather well-written. I have some suggestions to improve clarity in Section 1, namely when describing the data-generating process, see major comments below. My main concern is the lack of real-world datasets that would allow comparing the MIC and the MAP in practice, which would empirically validate the significance of the contribution. However, the semantics of the MIC are rather well-explained, and clearly make more intuitive sense than the MAP, so maybe the lack of experimental validation is OK.  # Major comments - Section 1.2: this section can gain in clarity. For instance, it is not clear whether any of the considered sets is actually drawn from DPP(L). From what I gather, there is a big matrix L, and given a set R of recommended items, we model the set S of engaged items as a draw from a DPP with kernel a *submatrix* L_R of L indexed by R. Since you have many independent copies of this procedure (one for each R_i), the loglikelihood becomes (1).  But strictly speaking, the DPP with matrix L is never explicitly used. - One way to clarify this is maybe to use [Kulesza and Taskar 2012, Section 2.4.3 "Conditioning"], and model each S as a draw from DPP(L) conditioned to lie within a given subset R. This seems to lead to (1) as well. Then the interest of the maximum induced cardinality later on is clearer: Find the set R such that conditionally on R, DPP(L) has the biggest expected cardinality. - Section 1.2: From what I understand, there is a W matrix of size m by m and you are learning a matrix L  = BWB' which is n by n, of rank less than m. This should be maybe underscored in Section 1.2, as you only explicitly define the matrices L_R for the various Rs, but not the parametrization of L. This is minor. - Relatedly to L-ensembles, maximizing (1) will lead to W picking up the cardinality of each set of engaged items. In the end, it is hard to interpret W as it both encodes the importance of each feature in the measure of diversity, and the distribution of the number of engaged items. This is a general intepretational issue with L-ensembles, not with this particular paper. L-ensembles are fine if you consider k-DPPs of mixtures thereof as in [Kulesza and Taskar 2012], but once you want to also model the number of items with the kernel matrix, you run into parametrization trouble. Maybe everything would be easier to interpret and parametrize if you used the correlation matrix K to define the underlying DPP, see also previous bullets. - Section 1.3: should the variable S not be renamed R? What you are trying to model is the set Y in S of engaged items, so I thought the set S is the set of recommended items. If this is the case, I would rename it as R, for consistency with Section 1.2. - Section 4.2: maybe an experiment with a real recommendation set and actual features would be interesting. A minima, why are the three types of matrices you consider good proxies for the L matrix in a recommendation task?  # Minor comments & typos - L65: I think [Urschel et al 2017] is advocating the method of moments rather than maximum likelihood. - L79: to each feature 