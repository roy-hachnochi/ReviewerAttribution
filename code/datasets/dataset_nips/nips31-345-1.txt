Summary  This paper covers a theoretical basis for training deep neural networks and how initialization/architecture choices can solve early training problems. This paper focuses on early training "failure modes" that prevent a network from just getting off the ground ("better-than-chance" performance), rather than improving performance at later stages.  The two early stage failure modes they wish to avoid are:  1. mean length scale in the final layer increases/decreases exponentially with the depth.  2. empirical variance of length scales across layers grows exponentially with the depth.  The paper fits in with literature and recent interest in ReLU activation functions and residual networks to ease training so one can construct deeper neural nets.  Their results are:   - For fully connected nets, there is a critical variance to use for initialization distribution to avoid failure #1 - For fully connected nets, wider and constant width layers helps avoid failure #2 - For Residual networks, decaying weights on the residual modules help avoid *both* #1 and #2  Quality  The paper's motivations are apparent and strong, providing a rigorous basis for the variance of the distribution for the weight initialization and subsequent architectural considerations on residual nets.  Particularly of interest is the fact that both failure modes can be prevented by the same process of minimizing the sum of the weightings on the residual modules.  This leads to very practical considerations, however -- for a network of a given depth, it just indicates that your weightings on modules should exponentially decay, or have some sort of convergent sequence.  For a fully connected net of given depth, the width of each layer should be maximized and equal widths are ideal -- to minimize the sum of reciprocals of layer depth -- to avoid the second failure mode.  These are practical but perhaps simple / incremental ideas.  Clarity  Although I appreciate the attempt to motivate results for the reader, and cover the same material with increasing rigor, sections 1, 3, and 4 could be reorganized in perhaps a more efficient manner, to avoid a sense of some redundancy of sentences.  I think at the end of line 90 you should add the words "the normalized output length", as you refer to it in as such a sentence later, but I think just including this quick naming before the formula could only increase clarity on a first read.  Figure 1 is a good motivating figure, adds to understanding beyond the text.  Figure 3 is a less convincing figure.  Originality  The fact that many popular initializations fail to meet the criteria presented in the paper (like Xavier, truncated He, etc.) shows that the knowledge in the paper is not immensely widespread prior to its publication.  The paper does rely heavily on reference [8], He ResNet paper -- initialization with critical variance is already implemented in PyTorch.  Although only one of the possible, popular initializes meets their criteria -- so the results of this paper would help you choose the correct initializes among current options, not choose a new one.  Perhaps the greatest argument against the paper could be that the results are only incremental on the ResNet and ReLU results in the literature.  Avoiding failure mode 1 for residual nets seems like a possibly obvious result / conclusion -- you need a bounded sequence for a bounding on the expected length scale.   Significance  A good theme for this paper comes around Line 114, roughly paraphrased by me here: other workarounds to solve this problem exist, but our suggestion is equivalent and much simpler (and possibly addresses multiple problems in one).   The work applies to deep nets with ReLU activation functions.  The authors note how the work might be extended to other functions, or more rigorously to convolutional architectures, but those are further directions.  Of course including more architectures or activations adds more to the results, but it may especially be warranted as some of the papers results were perhaps forshadowed by earlier work.  The papers strength is that the results are well motivated, intuitively presented, and rigorous.  I could certainly see the work being extended to more architectures, activations, and failure modes in early training of deep nets.  However, the ability to easily extend could be seen as a weakness as well -- perhaps there were obvious further directions to take the work that were left unexplored.  As the paper itself built upon an variance for initialization previously discovered, perhaps the results did not move the needle enough. (post-rebuttal) The proposed change to figure 3 as well as the additional text should help the reader. I would insist on this being done.