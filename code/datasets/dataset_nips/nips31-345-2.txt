The paper proposes an analysis of the behavior of the activities in a neural network, according to the initialization of the weights and the architecture. It addresses two problems: (1) the activities explode or vanish with the depth and (2) their variance grows exponentially with the depth.The article provides conditions over the initialization of the weights and the architecture of the neural network in order to avoid these failures. Experiments and a theoretical analysis prove that these conditions are sufficient in the cases of fully connected neural networks and residual networks.  The article is part of the field of weight initialization and analysis of the activities, such as "Resurrecting the sigmoid" (2017, Pennington and Schoenholz). The  main contribution of this article lies in the generality of the theorems: no limitation over the number of layers, no assumption about the data set. Moreover, the sketch of their proof is likely to be reusable. The only strong restriction is the use of ReLU as activation function.  The experiments in the case of fully connected networks show that the problems (1) and (2) can occur when the conditions given by the article are not fulfilled. The proof of theorem 5, that gives sufficient conditions to avoid problem (1), is clear and rigorous.   Nevertheless, for the problem (2), it misses experiments with neural networks with non-decreasing layer width (*) (for example in figures 3a and 3b). These are necessary to evaluate the condition given to avoid problem (2): this condition (sum of reciprocal of layer widths) is possibly far from necessary.  Concerning proofs: (a) The proof of corollary 2 is not clear: on line 400, the approximation is either incomplete or false (the constant c disappears, and exp(\beta x) should be exp(\beta (1 - x))). However, replacing the inequality on line 396 by the first inequality of equation 5 leads exactly to the claimed result on line 400. (**) (b) Theorem 5: the claim that limsup(M_d) is finite (equation 4) does not appear to be proven. The fact that E[M_d] is uniformly bounded is not sufficient. (***) (c) typos in theorem 5 and its proof: line 236: a constant N is defined but is never used; the second-order moment \nu_2^{(d)} is frequently squared when it should not.  In general, the paper is very well written, the main ideas are clearly explained and arise readily.   Overall, this paper provides a good analysis of the problem of exploding/vanishing activations and gives an interesting point of view to study activations. The results over their variance are a good start, but suffer from a lack of experiments (see (*)). Some parts of the proofs need improvement (see (**) and (***)). If the last two points are solved, this article should be published.