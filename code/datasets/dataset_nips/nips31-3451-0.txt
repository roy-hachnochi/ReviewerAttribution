This paper studies rho-POMDP, where the reward function maps a pair of a belief state and an action to a real value.  The authors show that the value function is Lipschitz continuous (locally with vector Lipschitz constants) if the reward function is Lipschitz continuous.  This property is then exploited to design a planning algorithm, similar to Heuristic Search Value Iteration (HSVI), where an upper bound and a lower bound are iteratively updated with the representation using sets of cones.  Numerical experiments show that the algorithm works as expected.  Although I have not checked the proofs, the theorems make intuitive sense and lead to an HSVI-like algorithm, which is shown to work as expected in numerical experiments.  The HSVI-like algorithm exploiting Lipschitz continuity is a first of a kind, and this novelty is the strength of this paper.  Although the practical merit of the proposed algorithm itself is limited, improvements along the lines of point-based approaches studied for POMDP can be expected as future work.  Unfortunately, writing is rather sloppy, and some critical points are unclear in the paper.  For example, I have no idea what is stated in Line 234-236.  The incremental variant from Line 208 is also unclear.  As far as I understood, this incremental variant is a heuristic to reduce computational complexity at the expense of not being exact.  Experiments are not well designed.  The focus of the experiments appears to be the comparison of different heuristics of incremental variant, but I do not think this is the incremental variant is the main topic of this paper.  It is difficult to see what I should take away from these experiments.  It appears that the proposed algorithm runs as expected, which however can be shown much more clearly with simpler experiments. 