This paper shows that uncertainty sampling is performing preconditioned stochastic gradient descent on the expected zero-one loss, which provides some explanation of the empirical finding that active learning with uncertainty sampling could yield lower zero-one loss than passive learning even with fewer labeled examples. Experiments on synthetic and real data are performed to illustrate and confirm some theoretical findings.  Overall, the theory in the paper seems interesting, in particular Theorem 9 and the two supporting lemmas 13 and 14 that derive the gradient of the expected zero-one loss, which seems somewhat surprising because the empirical zero-one loss is not continuous. The experiments on synthetic data provide a nice and clear illustration of how uncertainty sampling may help to converge to lower zero-one loss, but the experiments on the real data seem less enlightening.  The technical details seem mostly correct, although some typos are present and further clarification is needed in some places. In particular, in the proof of Lemma 14, it is not clear why O(h^2) in equation (61) can be ignored simply because h approaches 0. In the region where S(x, \theta) changes by O(h^2), couldn't the probability mass still be big, say o(h)?  Minor comments on notations and typos:  * In equation (8), it seems more clear to first define F_t to be the conditional expectation conditioned on \theta_{t-1}, and then say in the case of random sampling, F_t  equals the gradient of L at \theta_{t-1}.  * In Assumption 8, what exactly does being smooth mean?  * In the statement of Theorem 9, what is \Theta_{valid}? Is it \Theta_{regular}? What is \psi? 