This paper provides mathematical results regarding the link between the structure of a deep neural network and its tendency to be affected by Exploding or Vanishing Gradients. It estimates this tendency using the Jacobian matrix of the function represented by the neural network, under the rationale that the EVG problem is directly linked to the spectrum of this Jacobian and more specifically the fact that it has very large and very small singular values.  The authors study the statistical distribution of the coefficients of this matrix along two axes. First (Theorem 1) by focusing on a single coefficient of the matrix and studying its second, fourth and higher moments when considered as a random variable across initialization of the neural network. Second (Theorem 2) by focusing on a given instance of initialization, and studying the variance of the square of the coefficients across the matrix.  The main result of this paper is the derivation of bounds on these values, showing that in both cases they grow exponentially with a particular quantity "beta", the sum of the inverse of the widths of the layers in the network. Thus showing that given a fan-in scaled initialization, the variance of the values of the Jacobian is mostly determined by the architecture of the network.  The paper also shows hints at experimental results correlating the value of "beta" (and hence the variance of the Jacobian coefficients) with the speed at which neural network starts their training. More experimental results showing the actual impact of this quantity to the training dynamics would have been a nice addition.  The main limitation of this result is that it only applies to ReLU fully-connected networks, as acknowledged by the authors.  This paper describes in detail its results and their interpretation and significance. This is a good paper with interesting results towards a better understanding of the training process of neural network.