This work presents a new Gaussian process model for multiple time series that contain a) different temporal alignments -or time warpings- (which are learned) and   b) are related (in the multi-task sense), where the level of relation between tasks is also learned. This is applied to toy data and a very small real-world example of power generated by wind turbines.   The model is well described and the diagrams used to explain it are useful to understand it. Relevant connections with existing literature (multi-ouput GPs, deep GPs, warped GPs, etc) are established. Connection with previous literature on GPs for time series alignment seems to be missing, for instance "Time series alignment with Gaussian processes", N Suematsu, A Hayashi - Pattern Recognition (ICPR), 2012.  The model and the corresponding inference scheme, despite is laborious derivation, is by itself not very novel. It is a relatively straightforward combination of ideas from the above mentioned literature and requires the same tricks of introducing pseudo-inputs and variational inference to obtain a lower bound on the evidence. The main contribution could be providing the concrete derivation of the Phi-statistics for this particular model.   I think the derivations are sound. However, I am more concerned about the practical applicability of this model:  - The model is only applied to toy(ish) data. The computational cost is not discussed and might be very large for larger datasets, and with more than two signals. - The relation between tasks is governed by the set of hyperparameters \ell_{dd'}. In the simple examples given with only two tasks, this might work well. For cases with many signals, learning them might be subject to many local minima. - The learning of this model is very non-convex, since there are potentially multiple alignments and task-relatedness levels that achieve good performance (there are even multiple degenerate solutions for more that two outputs), so the hyperparameter optimization and inference can get very tricky for any non-trivial size dataset.   Minor:  - The clarity of the exposition could be improved. For instance, my understanding is that X is actually always one-dimensional and refers to time, but this isn't clear from the exposition of the paper. I incorrectly thought those would correspond to the locations of the turbines.  - Continuing with the previous point, am I correct that the location of the turbines is not used at all? That would seem to provide valuable information about task relatedness. - There could be a comment mentioning that learning \ell_{dd'} is what learns the "task relatedness".  After reading the authors' response: The response from the authors doesn't address the questions that I raised above.  What's the computational cost? The authors manage to "address" this latter issue in no less than 11 lines of text without giving the computational complexity of the algorithm nor the runtimes. Also, they don't define X properly in the rebuttal either.