In this paper, authors present a hybrid macro- and micro-level error backpropagation approach to train spiking neural networks. The idea is to depart from previous works by directly utilizing spiking activities and exact timings for error backpropagation - this enables measuring the direct impact of weight changes on the loss function and direct computation of gradients with respect to parameters. The error is defined, computed, and back-propagated with respect to (a): firing rates (macro-level) and (b): temporal sequences of action potentials for each neuron (spike trains) (micro-level). At micro-level, the model computes the spike-train level potential for each pre/post-synaptic spike train pair based on exact spike times. At macro-level, the model aggregates the effects of spike trains on each neuron's firing count. Authors propose a model to disentangle the effects of firing rates and spike-train timings to ease gradient computation at the micro-level. The model is compared against several baselines and outperforms the current state-of-the-art spiking neural network on two versions of MNIST dataset.  Although spiking neural network is not my area of expertise, I have the following suggestions for authors: - lines 35-37: I think it is good to further explain how desired firing counts (labels) are computed for the output neurons and why it makes it challenging to compute gradient of the loss with respect to weights.  - I think, in the contributions, it is informative to describe the temporal aspect and related challenges of the problem. For example, authors do not mention that the action potentials may decay over time. ---------------------------------------------------- Comment added after author response:  I appreciate that the response sheds light on the characteristics of the proposed model; I hope authors report these analyses and comparisons in the final version of this paper. Also, the response and other reviews make me more confident about my assessment.