This paper considers online learning with a family of losses defined on the L1 ball with some parametrized curvature. The algorithm automatically adapts to this curvature information. In the case of strongly-convex losses, a variant of the algorithm can adapt to the sparsity (L0 norm, _not_ the L1 norm) of the competitor.  The algorithm itself is a somewhat technical modification of an experts algorithm that mainains a set of experts corresponding both to (point, learning rate) pairs, where the points come from some specified subset of the L1 ball.  Quality: This paper seems to have nice results. I liked that the algorithm can adapt to the curvature without knowing the curvature parameters, and I was also impressed by the second algorithm's ability to adapt to the L_0 norm of the competitor. I really liked the inductive technique used in the proof of Theorem 3.3. The results of Theorem 3.2 are a little hard to interpret, due to the unusual distance term. It seems clear there is a runtime/regret bound tradeoff at work here, but is there some example for a regret bound that is better than 1/sqrt{T}? It seems that (at the cost of exponenial runtime, as in Theorem 3.1), this should be achieveable. Is there anything better one can do?   Clarity: I found this paper a bit difficult to follow at times. In particular, the intuition for the proofs is not presented well, which I found very distressing since the statements seem relatively powerful and the corresponding proofs seem very technical.  In the sketch of proof for theorem 3.2, on line 221, it seems to be implied that regrets is linear in the comparison point, which confused me since the loss functions have curvature. I believe this was actually a statement about linearized regret, but this was not clear without some poking in the appendix.   In the proof of theorem 3.3, the value "a" is initially defined to have a term \log(3d), but later (line 594), this seems to become \log(2d).   In line 89, it is stated that under the Lojasiewicz condition the algorithm obtains low regret for comparitors with l1 norm bounded by c<1. However, the corresponding theorem (which I believe is Theorem 3.4) I believe replaces c with 1-\gamma. It seems best to use the same notation.  Equation (7) in the appendix is stated to be a restatement of Theorem 3.2 of Wittenberger(2017), but it is not obvious to me that it is the same as it seems to be missing a log(E_\pi_0[1/\eta]/E_{\pi[1/\eta]}). No doubt I'm missing something here, but the proofs are technical enough that I'd appreciate even small gaps being filled.   The use of NP to describe an algorithm in line 175 was also a bit confusing to me. The algorithm proposed is exponential time. But to be precise, algorithms are not in NP - problems are. And this problem seems not to be since it is solved in polynomial time in the next theorem. Maybe there is some other meaning of NP, but in general the meaning is clear if you just say exponential time.  Also, there are some shenanigans with the typesetting. The paper is over the length limit, and it appears to have smaller text and bigger margins than it should.  Finally, there are few minor grammatical errors 84: "it exists" -> "there exists" 159: "it is worth to point out" -> "it is useful to point out" 175: sentence starting with "In Section 3.1..." seems to switch between plural and singular tenses several times.   Originality:  I believe these results are original.  Significance:  The adaptivity of the algorithm to the curvature and L0 sparsity is interesting. I think the analysis of the algorithm of section 3.3 had some interesting techniques. Overall I think there is reasonable hope the techniques may be used to design future algorithms.