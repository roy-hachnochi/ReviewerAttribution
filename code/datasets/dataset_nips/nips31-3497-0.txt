Overall I think is a very good paper and it is one of the better papers I've seen addressing evaluating GANs. I myself are fairly skeptical of FID and have seen other works criticizing that approach, and this work sheds some light on the situation. I think anyone who follows this work would be better informed than work that introduced inception or FID in how to evaluate GANs.  That said, there is some missing discussion or comparison to related work (notably mutual information neural estimation (MINE) by Belghazi et al, 2018) as well as some discussion related to the inductive bias and boundedness of their estimator. I'd like to see a discussion of these things.  Comments:  Don’t forget "Quantitatively Evaluating GANs With Divergences Proposed for Training” (Im et al, ICLR 2018) as another method for evaluating GANs.  Since you are estimating mutual information, it would be worth comparing to MINE (Belghazi et al, ICML 2018), or at least a good discussion relating to this model.  Is there any way to tell if your encoder is overfitting? What would be the consequence of overfitting in your model? For instance, could your encoder preference to learn to map trivial noisy information from x to z, ignoring or overlooking important but more difficult to encode structure?  Your generator and encoder are deterministic and the variables continuous, are there issues with the boundedness of the mutual information? If so, how well do we expect this estimator to do as the true mutual information between the generator input and output is high? In addition, how can regularization of your encoder (a la Roth et al ICML, 2017 or Mescheder et al ICML 2018) change the quality of the estimator (is it better or worse)?  Furthermore, what is the potential effect of the inductive bias from the choice of convolutional neural network as your encoder?  -------------- I have read the response, and I am happy with the response. However, I encourage the authors to think more about the potential unboundedness of the KL mutual information and the boundedness of their family of functions / regularization.