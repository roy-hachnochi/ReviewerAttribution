The ideas in this work are both promising and interesting and the general direction should be continued. The authors propose a novel biologically plausible framework on non-negative similarity-preserving mapping (NSM) and achieved manifold local tiling results. The arguments are further supported by some analysis results. If the following concerns can be reasonably addressed, this work should be considered as a paper for the conference.  1. Since manifold learning was initially proposed, most of the attention has been focused a parsimonious embedding guaranteed by both Whitney embedding theorem and Nash embedding theorem. A relatively higher dimensional embedding has been under appreciated. However, I suggest the author to further discuss the benefits of this practice and give more clear settings of these benefits. At the end the author mentioned "... the advantage of such high-dimensional representation becomes obvious if the output representation is used not for visualization but for further computation". This argument is not very satisfying and should be supported further in the discussion or experiments.  2. The connection between manifold tiling and manifold charting should be discussed. Building an atlas of a manifold could also be considered as a higher dimensional embedding. The following references should be cited: Brand 2003, Charting a manifold Pitelis et al. 2013, Learning a Manifold as an Atlas  3. The formulations throughout the whole paper changed quite a few times. I found this a little distracting and makes the central idea weaker. The authors should work on simplifying the formulations to make them more systematic and consistent.  4. The proof of in the supplement has a tendency to treat KKT conditions by using only part of the constraints, this is not safe. E.g. The Lagrangian in (20) should contain another term corresponds to the non negativity of Y. In general, to used the KKT condition for a constrained optimization problem, all the constraints should be considered at once. By only considering part of the constraints may lead to a false claim. I verified proposition 1 is correct, but certainly the argument is not very standard. In theorem 1, an addition closed cone constraint has been added. The authors should justify this new constraint doesn't break anything established in proposition 1. The result might be fine in this particular case.   5. The assumption for theorem 1 is quite strong. The author should provide more insights and explain the significance of this. If possible, please also provide some conjecture for a potential generalization.  6. Section 5 is very opaque and certainly not self-contained. It requires the reader to read another journal paper to make sense of it. If these update rules are not the contribution of this paper, it's better to put them into the supplement and discuss something more important.  7. In section 6, the third paragraph. Why is \beta T an upper bound of the rank? This is false without further constraints.  8. Under equation (21), "where the Lagrange multipliers ... must be adjusted so that ... sum to 1". Why?  9. Figure 4, the second subfigure from the right. The tiling doesn't seem very uniform. Is possible to provide an explanation to assist readers to understand further of the issue?  10. For reproducibility, I also encourage the authors to put a documented code online since the formulations and solution methods are quite complicated.  