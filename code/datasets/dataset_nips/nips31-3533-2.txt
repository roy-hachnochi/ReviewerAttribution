This paper considers training Generative Adversarial Networks (GAN) using the technique of Regularized Optimal Transport, shows the convergence of this method, and also considers a variant with more robust performance. In particular, it considers regularized Wasserstein distance for training the generative model. It first shows that this objective is smooth w.r.t. parameters in the generative model, then shows that an approximate solution for the discriminator allows to approximately compute the (stochastic) gradient of the generator, and finally shows the convergence of SGD/GD to an approximate stationary point where the solution quality depends on the average error of the discriminator steps. The work then points out that in practice a small regularizer is needed but will lead to computational challenges, and proposes to use a more robust objective Sinkhorn loss (the convergence analysis also applies here). The method leads to promising empirical results on standard datasets MNIST and CIFAR-10.  Pros: 1. The work proposes a simple and effective method for training GAN, with solid theoretical analysis. The work adopts techniques from several lines of existing work and the proposed framework is clean and mathematically well formulated. It also provides an analysis showing global convergence to approximate stationary points and points out factors affecting the approximation guarantee, building on top of the smoothness from the regularization.  2. The experimental results seem promising. In particular, the inception score of the proposed method on CIFAR-10 is well above those of several existing methods.   Cons: 1. The convergence is to (approximate) stationary points. It is unclear whether the stationary points are good solutions from the generative model perspective and under what conditions such a guarantee can be achieved. But I understand that this is beyond the scope of this work.   