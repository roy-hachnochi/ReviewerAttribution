Summary: The authors study multiplicative noise using theoretical tools as well as numerical experiments.  They demonstrate how feature correlation (arising from use of naive multiplicative noise) degrades model performance, and develop a regularizer which boosts performance by removing the correlation generated by these methods.  The authors relate this new regularizer to shake-shake normalization, theoretically.  Finally, they support their findings with considerable experimental work.  Strengths:  The paper is clear, compelling, and reasonably well supported by the theoretical and numerical arguments presented within.  The effort to connect the work theoretically to shake-shake regularization is particularly useful and intuitively helpful.  The work is clearly original, and could be useful in the community.  Weaknesses:  It was not obvious how sensitive the method is to hyper parameter tuning (i.e., sensitivity to standard deviation of noise used).  For example, the authors mention (line 37): “Moreover, these techniques require extra hyperparameters to control the strength of the penalty, 38 which further hinders their practical application.”  But then, on line 226:   “As a side note, we empirically find that the optimal noise 226 variance tends to be proportional to the width of the network, if other settings remain the same.”  I think this is a pretty important plot to show!  I am convinced that the *type* of noise doesn’t change the results too much, but seeing a simple plot of test performance vs. standard deviation of noise scale would solidify the experimental work section.   Overall, it was a solid contribution, worthy of acceptance.