Answer to rebuttal Given your response (and most importantly "DDPG is exactly ATOC without communication") I have increased the score one point. Please make sure to make this more clear in the final version. Also, make sure to include the reward function.  Learning Attentional Communication for Multi-Agent Cooperation  The paper address the problem of multi-agent cooperation with focus on local inter agent communication. Hence, apart from taking actions affecting the environment the agents are able to share their state with a group of m other agent. However, as I understand the paper, the communication is not as one might expect a one to many broadcasting of information but rather a mind meld where all participating agents states are integrated using an LSTM and fed back to all agents in the group. These communication groups are formed using input from the individual agents “Attention Unit” indicating if each respective agent is interested in starting a state sharing group. If at least one agent is interested a group will be created and populated based on proximity (and if the agents are already part of another group). However, there is no way for an agent to opt out. Further, to stabilize training all agents share weights which bypasses the problem of non stationarity usually associated with MARL and also makes it computationally easy to scale up with more agents. To validate the performance of their proposed method three experiments of well known multi agent problems have been run and compared to baselines and two SOTA systems.   Comments: § It is interesting that you get good results even when communication is turned off during testing (but on during training). You attribute this to the communication helping with training but provide no evidence for this. It would be nice to see results where communication was off both during training and testing for comparison.  § I don’t see why you are referring to the RNN that decides whether to initiate state sharing as an “Attention Unit” since, as far as I can see, it has no impact on the agents own input.  § You employ a lot of analogies and examples in the text which is great to help understanding, however, I sometimes found the text lacking in details regarding the algorithm, e.g. I was not able to find the actual reward functions used in the experiments.  § Are figures 3 and 5 showing data from one training instance or the mean over many runs?  If it is the former I would be hesitant to draw any conclusions from it and if it is the latter you should make sure to indicate that in the paper along with the number of runs you have computed your average over.  Minor comments: § Though the text is reasonably well structured grammatical problems throughout the text makes it hard to read something that would need to be addressed if the paper were to be accepted.   § in section 5.2 you refer to figure 3 though I think you you actually mean to refer to fig 5.