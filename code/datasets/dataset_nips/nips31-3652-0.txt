The paper provides a way to obtain an approximate Bayesian posterior for MAML parameters on a task. The posterior should be proportional to the prior * likelihood. Stein variational gradient descent is used to train a set of particles to approximate the (unnormalized) posterior. The obtained posterior can be used to answer probabilistic queries. The Bayesian method also prevents overfitting when seeing the same training data multiple times.  I'm glad I had the honor to read the paper. The method is principled. The explanation is well written. The experiments cover many domains and are well designed.  Comments: 1) A minor detail for the prediction construction on line 130: For a general loss, the prediction should be chosen to minimize the expected loss, where the  expectation is taken with respect to the posterior distribution. E.g., the absolute error would be minimized by the median instead of the mean.  2) In the experiments, you compare SVPG-Chaser to VPG-Reptile. Why do you say that the chaser loss performs only 1st-order gradient optimization there? Do you use a variant with d(\Theta_0 || \Theta^{n+s}) instead of d(\Theta^n || \Theta^{n+s})?  Minor typos: Line 286: VGP should be VPG.  Update: Thank you for the answers in the rebuttal. About the comment 1): The Bayesian ensemble of the prediction is not always the best prediction. The best prediction depends on the used loss function (e.g., a loss can be higher for false positives). You should choose the prediction that minimizes the expected loss.