Summary:  Meta-learning is motivated by the promise of being able to transfer knowledge from previous learning experiences to new task settings, such that a new task can be learned more effectively from few observations. Yet, updating highly parametric models with little amounts of data can easily lead to overfitting. A promising avenue towards overcoming this challenge is a Bayesian treatment of meta-learning.  This work, builds on top of recent work that provides a Bayesian interpretation of MAML (model-agnostic-meta-learning).   strengths:  - This work addresses an important topic and significant contributions within this field would be of interest to many researchers within the NIPS community - The work is well motivated - A novel Bayesian variant of MAML is proposed, with a two-fold technical contribution:     - approximating the “task-train” posterior via SVGD (task-train posterior is the posterior over the task optimized parameters). This contribution is a direct extension of (Grant et al 2018) - where the task-train posterior was approximated via a Gaussian distribution. Applying SVGD instead allows for a more flexible and (potentially) more accurate approximation of a highly complex posterior.      - Measuring the meta-loss as a distance between two posterior distributions: 1) the task-train posterior obtained from SVGD on training data only and 2) the task-train-posterior obtained from SVGD on training+validation data. This loss has been termed the “chaser loss”. The objective is to optimize meta-parameters such that distance between these two posteriors is minimized. - An extensive evaluation and analysis of the proposed algorithm is performed, covering a variety of learning problems   Shortcomings/room-for-improvement:  Clarity of presentation of the technical details and some experimental details.  - Section 3: the proposed method. I found this section hard to pass for several reasons: - in the background section you give a very short intro to MAML, but not to Bayesian MAML (as proposed in Grant et al). This creates two major issues: a) when coming to Section 3, the reader has not been primed for the Bayesian version of MAML yet b) the reader cannot easily distinguish what is novel and what is re-deriving previous work.   - Section 3: presenting the technical details. You spent very little amount on explaining a few of the steps of deriving your new algorithm. For instance:     - Line 107: “we see that the probabilistic MAML model in Eq (2) is a special case of Eq (3) “ -> that is not easily visible at all. Try to give a one sentence explanation - or make it very clear that this connection is shown in Grant et al      - To arrive at equation 4) two approximation steps happen (from what I understand): 1) you approximate the task-train posterior via SVGD - this effectively gives you a set of particles theta_0 and theta_tau. (This is actually very unclear - do you maintain particles for both the meta-parameters theta_0 and the task-specific parameters theta_tau?) 2) you now plug in the task-train posterior into equation 2 - but can’t evaluate the integral analytically. You utilize the fact that your posterior is a collection of particles and approximate the integral via a sum.  You do not very clearly derive this and the algorithm 2 has very little detail. It would be way easier to understand your derivation if you could break down this derivation and include a few more details into your algorithm. Note that this part is one your technical contributions, so this is where space should be allocated.  - Section 3.2 : chaser-loss vs meta-loss. You present a new loss function for your meta-updates and intuitively motivate it very well. However, here your work takes a big step away from the “original MAML” framework. In MAML the meta-loss is , in a sense, a predictive loss on a dataset. Here you move to a loss that measures the distance in posterior-over-task-parameter space. So you change 2 things at once: 1) measuring loss between distributions instead of point estimates 2) measuring loss in “posterior space” vs “predictive space”. This step should be discussed in a bit more detail - why not measure the distance between predictive distributions? What is the rationale here?  - Algorithm 3: what’s the stopgrad? - Experiments: Figure 1: What is the MSE over? Is it the mean-squared error that the meta-parameters achieve on D_val? Or is it the MSE after the meta-parameters were updated to a new task with K data points - measured on that new task? - Experiments: Classification: you mention you follow Finn et all 2017 to generate the tasks for this experiment. However, I cannot find the description of what a task is, and how you generate 10K to 800K different tasks on the miniImageNet data set. I also don’t know what the number of training examples per task is? Please improve that explanation.  - Experiments: Classification: you mention that you move from 800K to 10K tasks and call that “such a small number of tasks” - that is a very interesting notion of small. When reading this - the first thought I have is that we have replaced the huge number of training observations with a huge number of training tasks. How many examples do you use per task? 10? So a 800K tasks would produce 8M data points?   