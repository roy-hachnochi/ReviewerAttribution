The paper is about unsupervised alignment of speech and text embedding spaces. The paper basically connects 2 papers: (i) Speech2Vec, (ii) ICLR 2018 paper on unsupervised alignment of word embedding spaces of two languages. The idea is that a cross-modal alignment can allow to carry out retrieval across the speech and text modalities which can be useful for speech recognition (in one or other languages). The experimental setup is very well executed and goes from varying level of supervision used to get speech embeddings (dependent on the segmentation algorithms used). The authors also carry out the synonym retrieval analysis pointing to, as expected, semantic nature of the embeddings as also observed in (Kamper, Interspeech 2017). Overall, a really nicely executed paper!  Two suggestions: (a) Can the authors include the majority word baseline i.e. for both spoken word classification & translation predict the most commonly paired word. (b) Some of the references ([14], [35]) are missing author names.   ************** After rebuttal ***************** Thanks for answering all queries by me and my fellow reviewers.  One suggestion for future work that I have after a second read is how do you minimize the gap between A* and supervised methods and hopefully later a trickle down effect to unsupervised approaches. As footnote 2 says, a more complex transformation between the embedding spaces doesn't really help. I'm pretty sure the problem is with L2 loss function where you're penalizing for not exactly matching the text embedding, but what you really care about is ranking. For the supervised approach(es), it's easier to try other losses (triplet etc) but not really sure about how to do it for unsupervised approaches. Might be a good avenue to work in future work.