This paper is based on the idea of detecting an out-of-distribution example by training K regression networks to predict *distributed representations* of the target, and then declaring an outlier if the summed L2 norm of the K predicted embedding vectors is below a threshold (as in eq 4). In this paper the distributed representations are five different word embeddings of each target class, as per sec 4.1.  The problem of out-of-distribution detection goes by many names, e.g novelty detection and "open set recognition". This is not sufficiently discussed in the paper. See e.g.  https://www.wjscheirer.com/projects/openset-recognition/ and refs in [A] Finding the Unknown: Novelty Detection with Extreme Value Signatures of Deep Neural Activations. Schultheiss et al, German Conf of Pattern Recognition, 2017.  The use of such distributed representations is reminiscent of Dietterich and Bakiri's work Solving Multiclass Learning problems via Error-Correcting Output Codes https://arxiv.org/pdf/cs/9501101.pdf (JAIR 1995), although the motivation is very different---there it is to make use of binary classifiers to tackle a multi-class problem.  The idea is quite simple and the paper does not provide any theoretical basis, but from the experiments it seems that it outperforms the baseline, ensemble and ODIN competitor methods. One criticism is that it is not clear what exactly the 1-embed and 3-embed methods are -- is there optimization over which of the 5 embeddings you are using? If so this seems unfair. Please clarify the set up.  I believe that ref [A] above is state-of-the-art for novelty detection. Thus I would like to know (in the rebuttal) the performance of this method on the authors' datasets.  Strengths -- simple method, easy to implement. Appears to beat its competitors.  Weaknesses -- lack of theory behind the method. Arbitrary nature of the distributed representations used -- for example would word embeddings be a good choice for predicting skin disease names from medical images? Why not other representations, or even say the higher layers in a deep network?  Quality -- there is not much math that needs checking for technical correctness -- this paper is heavily empirical.  Clarity -- clear enough.  Originality -- as the authors acknowledge (line 124) their criterion in eq 4 is inspired by [39].  Significance -- out-of-distribution/novelty detection/open set recognition is an important problem.  I am giving this paper a marginal accept for now, but the rebuttal will be very important wrt comparison to [A]. 