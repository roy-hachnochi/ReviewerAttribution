This work aims to proposed a stochastic gradient descent for spectral optimization. Unbiased stochastic gradients for spectral-sums are proposed based on the Chebyshev expansions. Besides the calculation of stochastic gradients, the truncation distribution is also designed for fast and stable convergence. It is worth noting that the major difficulties of applying Chebyshev approximations to spectral-sums has also been addressed in [1]. Here are the comments:     To calculate the gradients, one need to draw M Rademacher vectors. What the value of M is used in the experiments? Is M sensitive to the results? How to determine this parameter?    Followed by the above comment, memory requirement is also a target of interest in stochastics learning, it is suggested to include the analysis of space complexity as well.    It is also encouraged to apply the proposed method to the rank minimization problems, such as the robust PCA [2] or robust subspace discovery [3], where the nuclear norm is used.    Although the authors mentioned that Chebyshev approximations are nearly optimal in approximation among polynomial series theoretically, it is suggested that a comparison with standard SGD (i.e., Taylor expansions)   Please specify the notation ψ^((t))and ψ ̃^((s)) in Algorithm 1 and 2    [1] Han, Insu, Malioutov, Dmitry, Avron, Haim, and Shin, Jinwoo. Approximating spectral sums of large-scale matrices using stochastic chebyshev approximations. SIAM Journal on Scientific Computing, 39(4):A1558–A1585, 2017.  [2] Xinggang Wang, Zhengdong Zhang, Yi Ma, Xiang Bai, Wenyu Liu, and Zhuowen Tu. “Robust Subspace Discovery via Relaxed Rank Minimization”, Neural Computation, 26(3): 611-635, March 2014.  [3] Emmanuel Candes, Xiaodong Li, Yi Ma, and John Wright. “Robust Principal Component Analysis”, vol. 58, issue 3, article 11, Journal of the ACM, May 2011.  