This paper presents a simple and effective approach for fine-grained image recognition. The core idea is to introduce max-entropy into loss function, because regular image classification networks often fail to distinguish semantically close visual classes in the feature space. The formulation is clear and the performance is very good in fine-grained tasks. I like the ablation study on CIFAR10/100 and different subsets of ImageNet, showing that this idea really works in classifying fine-grained concepts.  The major drawback of this paper lies in its weak technical contribution. Max-entropy is a well-known theory, and this paper seemed not to provide any improvement on it, but just applied it to fine-grained recognition (I searched on Web, but I did not see any work with the same solution, so I vote for moderate novelty). It would be a good contribution to the computer vision community, but not a significant one to NIPS which leans more to theory.  In addition, another concern is that the formulation is closely similar to that in [30] (see the first equation in Sec 3 (arXiv: 1701.06548), KL divergence and cross-entropy differ from each other by a constant). Please clarify the difference, otherwise this work would be considered much less novel.  ===== POST-REBUTTAL COMMENTS =====  After reading the rebuttal and other reviewers' comments, I can accept that this paper is an extension to [30] with a proof why a max-entropy-based approach works well on fine-grained recognition. Again, this is a nice work, but the technical contribution is not significant, which limits its quality especially for NIPS. Considering other reviewers' comments, I would change my rating to weak accept.