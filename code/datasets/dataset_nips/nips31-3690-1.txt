This paper studies the similarity under the domain adaptation (DA) setting. Authors first gave a new definition of similarity function under DA, and then provided theoretical guarantees for the adaptation problem. Authors also presented theoretical analysis for an important margin term, which can be estimated from finite samples, and it can also be used to design new DA algorithms.  Pros: - The paper is well written. The preliminary section on the similarity learning under supervised classification is informative, and it motivates for the work of extending similarity learning to DA problems.   - Authors gave two versions of relating similarity for source and target domains: one under the setting of shared landmarks distributions, and one under the setting of different landmark distributions.   - Under certain assumptions, the error (given by the similarity study) of source domain has a multiplicative effect on the divergence term, which has certain advantage over standard DA bounds where the error of source domain and the divergence term are additive.   - Authors designed a new DA algorithm based on the learning bounds, and demonstrated its effects on synthetic dataset.  Cons: - The adaptation result in the different landmarks setting seems less useful. Authors didn't discuss much about the term eps'', which is larger when the landmarks distribution is less similar between source and target domains; and unlike eps', this term is not just restricted to the support of hinge loss.  - It will be more interesting to see experiment results on real world applications.