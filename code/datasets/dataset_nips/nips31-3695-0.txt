Updated score from rebuttal. With improved presentation and a clear message, this could be a good paper.   This paper further generalizes the useful notion of mixability for prediction with experts advice, building off a long history of results from the regret minimization community and solving an open problem about characterizing \Phi mixibility. This is mostly accomplished by defining the "support loss", which allows one to upper bound the given loss but possesses nicer properties and the authors to characterize Phi mixability completely, solving an open problem.   A new algorithm (AGAA) is proposed and analyzed. The regret bound is intricate, making comparisons difficult. Some simulations are provided in the appendix, but I feel that this algorithm is under explored.   The technical contributions seem solid, and a long line of work on Phi-mixability is essentially closed by this paper. However, I do have concerns with the presentation and impact. First, the paper is very densely written and would be better suited as a COLT or jmrl paper. Yes, there is a lot of background and definitions to go through, but many of the definitions are unnecessary or unnecessarily technical (e.g. line 87-90). While the technicality of the definitions are necessary for the most general case, the authors should consider presenting a less general case in the main body for venues like NIPS.   The second worry is the interest to the NIPS community. Mixability and aggregating algorithms are not even mainstream in communities like COLT, and I'm afraid this paper will not find the right audience an NIPS and not have the impact it should.    A few more comments:  The typesetting and symbols used are sort of out of control, e.g. the symbol on line 169.  I don't think S was explicitly defined to be the Shannon entropy.  