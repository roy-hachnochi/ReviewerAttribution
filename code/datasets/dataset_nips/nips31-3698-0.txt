 The authors propose a Bayesian formulation of Generative Adversarial Imitation Learning (GAIL) and demonstrate that this leads to better sample efficiency in terms of environment interaction.   Contributions:  1. The authors show that GAIL can be viewed as a iterative maximum likelihood point estimation method. 2. The authors introduce a Bayesian formulation of GAIL (BGAIL) that uses posterior sampling. 3. The authors demonstrate the sample efficiency of BGAIL on simulated domains with high-dimensional states and actions.  The contributions in the paper are clear. The paper is well-written, and derivations are presented in a straightforward manner. The result for the HalfCheetah task in figure 1 are rather curious (why does GAIL perform so poorly?). It would be useful to comment on this. However, in general, it does seem that BGAIL results in fewer environment interactions for policy learning.  That said, the experimental evaluation is a key aspect of the claims of the paper. The formalism to show the Bayesian perspective would leave a reader convinced if experimental evaluation was a bit more thorough. This would mean performing experiments in a number of environments (even if in sim) to show that any gains achieved by BGAIL are not due to some dynamics characterstic in mujoco continuous control suite, but indeed due to the algorithm. 