This paper shows how inference in Deep Gaussian processes (DGPs) can be carried out using an MCMC approach. The approach proposed is not vanilla MCMC, but contains a novel way of considering samples which is more efficient and is claimed to prevent overfitting.   While most of the recent DGP papers have focused on variational inference for approximating the intractable posterior, the present paper attacks the problem using MCMC. I find this approach to be particularly significant, because: (a) at least in small problems, we can get a feeling of how the exact posterior behaves (in contrast to variational inference that does not tell us how far the bound is from the real evidence). This, in turn, allow us to study DGP properties better.  (b) the paper also demonstrates very good predictive capabilities of the new construction, which is an additional (if not the main) benefit.  (c) I know many researchers in the field have thought about applying MCMC in DGPs to see what this alternative inference would achieve, but either almost no-one actually tried it "properly" or they got stopped by the highly correlated samples in the high-dimensional space; so it's great to finally have this new inference tool for DGPs.   However, I think the recent approach of Dunlop et al. 2017 should be discussed and referenced. It is not the same approach taken here, but it is still an MCMC approach for sampling u given y in DGPs.  With regards to novelty and technical merit, this paper: (a) starts from a good motivation (alternative inference for DGPs without the disadvantages of VI) (b) makes good observations (MCMC doesn't have to scale badly if we're clever about how to do it, e.g. lines 61-64 and the moving-window algorithm)  (c) is correct and experimentally seems to work well.  Concerning clarity and readability, I am a little conflicted but mostly positive overall. On the one hand, the paper reads nicely and explanations are extremely clear and concise and claims are balanced. On the other hand, I feel that some of the space in the paper is taken up by text that could be omitted (or put in Appendix) in favour of more important pieces. Specifically, Section 3 describes (in a clear a nice way nevertheless!) the multi-modal posterior aspect of DGPs which is basically common knowledge in this sub-field. Further, section 5 seems quite irrelevant to the main point of the paper, especially since the decoupling method cannot be applied to the MCMC case. In place of these parts, I'd prefer to see stronger theoretical analysis for moving-window MCEM (see below) or more plots from the experiments (see below). Overall, the presentation of this paper can be improved but it is nevertheless quite good.  As mentioned above, it would be great to see further anlysis for MW-MCEM to make it more convincing and understandable. I understand that, compared to MCEM, the new approach changes samples continuously, however, the changes are in a more constrained way, so it is not clear to me why this would prevent overfitting or improve convergence speed. The analysis of MW-MCEM could also be isolated as it is the main methodological novelty of the paper, that is, I would prefer to see MW-MCEM tested additionally in a more general sampling setting instead of section 5. For example, MW-MCEM could be tested against Hensman et al. 2015 even for the shallow GP.   Finally, I find the experiments conducted well and adequately. It would be interesting to see an explanation about the typically larger std for the MCMC method in Fig. 4 (I would actually expect the opposite to happen due to better exploration of multiple modes and therefore more consistent performance across runs, but maybe it's the number of samples that needs to be increased). It would also be great to have some accuracy/F1 measures to complement the LL results in Figure 4 (since experiments are run anyway, the authors can have this metric for free, so why not report it).   References: Dunlop et al. 2017: How deep are deep Gaussian processes? ArXiv: 1711.11280  Other: - Are the DGP hyperparametres tuned in the 2nd phase of the algorithm (see lines 157-158) or not? - In references "gaussian" should be "Gaussian"  - I think nowhere in the paper is mentioned explicitly what is "DGP 3/4/5", please explain with words for completeness. - Suggest to present MNIST results in a table.  - Which data was used for the runtime plot?  --- Update after rebuttal: I have read the authors' responses which address some of my questions. I think this is a good paper, but would further benefit from including discussion regarding the typically larger std that the MCMC method obtains in the experiments (this review point has not been addressed in the rebuttal).