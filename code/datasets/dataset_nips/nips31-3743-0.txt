Update after rebuttal:  This is a very interesting and well executed paper. A large number of studies have said that DNNs yield performance that resembles that of humans and animals, but this study provides a more fine-grained comparison of the behavior and finds real differences. In particular, the study probes sensitivity to perturbed inputs, something that current DNNs are much less capable at than humans. This provides a valuable dataset and benchmark to aim for, and I expect the ML community will have algorithms that perform far better in short order.   I would encourage the authors to make as fine-grained and specific comparisons of similarities and differences as possible; to suggest potential reasons why these discrepancies might exist (clearly marking these reasons as somewhat speculative if they aren't backed by direct evidence); and to expand on potential routes toward better algorithms (again marking this as speculative if it is).  _________  Summary:  This paper probes the generalization abilities of deep networks and human observers, by undertaking an extensive psychophysical investigation of human performance on ImageNet in the presence of strong distortions of the input images. The results point to a large gap between DNNs (which can perform well when trained on a specific distortion) and human observers (who can perform well across all distortions). The paper releases the psychophysical dataset and image distortions to aid future research.  Major comments:  This paper undertakes an impressively controlled comparison of human performance and DNNs at a behavioral level. While previous work has shown broad similarities between DNNs and human behavior, this evaluation undertakes a much more challenging test of generalization abilities by considering extreme distortions. Additionally, in contrast to prior work which relied on Amazon Mechanical Turk to generate data, the present submission uses carefully controlled laboratory viewing conditions that remove some prior concerns (such as variation in timing, viewing distance, and how attentive AMT subjects might be). Overall this paper contributes a rigorous psychophysical dataset that could spur a variety of follow up work in the field.  The most striking deviation between human subjects and the DNNs investigated here is in the entropy of the response distribution. The DNNs apparently become stuck, responding to all distorted images with a single class. On the one hand this is indicative of a clear failure of the DNNs in their vanilla form. On the other hand, one could imagine simple procedures to improve the situation. For instance, one could adjust the temperature parameter of the final softmax to attain equal entropy to human subjects, and assume responses are sampled from this. It is well known that the final softmax layers are not calibrated probabilities, so this may be a fairer comparison. Another route would be through explicitly calibrating the probability distribution in the final layer, eg through dropout [75]. 