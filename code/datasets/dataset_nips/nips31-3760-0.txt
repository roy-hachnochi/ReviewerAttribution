This paper introduces an efficient and general approach to GP inference, which is based on Blackbox Matrix-Matrix multiplication and capitalizes on recent advances in ML hardware. The work is precisely explained, with appropriate references for related and relevant work, and is of great impact. The authors plan to release their work as an easy to use and well maintained software, which would facilitate the adoption of Gaussian process models by the ML community.  The key contribution of the paper is a framework that computes GP inference via a blackbox routine that performs matrix-matrix multiplications efficiently with GPU acceleration. It reduces the complexity of implementing Gaussian process models, while still allowing inference to be as efficiently as possible with minimal user intervention.  The technical contribution is a modified batched version of linear conjugate gradients, and a method for preconditioning it, which they clearly explain in Section 4 (after providing a solid background, both to GP inference in Section 3 and the key numerical methods involved in Section 2). The authors clearly separate their work from the state of the art:     (a) "(...) We utilize the same log determinant estimator (as in [11]); however we avoid explicitly using the Lanczos tridiagonalization algorithm which has storage and numerical stability issues"     (b) "Preconditioning is widely acknowledged as an effective tool for accelerating the convergence of conjugate gradients. (...) we do not seek a general purpose preconditioner (...) the preconditioner should afford roughly linear time solves and space. (...) should be able to efficiently compute the log determinant of the preconditioner matrix."      Due to the fact that the proposed method reduces the bulk of GP inference to matrix-matrix multiplications, it can be easily adapted to complex GP models or structured GP approximations.  They evaluate these claims in Section 6, where they show that (a) the proposed engine provides substantial speed benefit over alternatives (particularly when using GPUs) with comparable inference accuracy, and (b) the proposed preconditioning results in an important improvement in the efficiency of the method.  All in all, I argue for the acceptance of this work and encourage the authors to revise the manuscript for minor typos (e.g. "convergence bounds for of our preconditioner" in line 82) and improved clarity (e.g. SLQ is undefined in line 124). The authors have additionally provide interesting insights within their author rebuttal, which I encourage to include in the final manuscript.