This paper presents a way to model a network of textual documents.  To this end, the paper models the problem in vector space, where each edge (corresponding to a text document), the graph structure and the joint text and graph structure are all embedded in vector space.  Text embeddings are modeled using word embeddings, and optionally using an RNN.  The model parameters of the full network structure is learned together using a loss function that tries to generate neighboring vertices given a vertex.  The authors measure performance on multi-label classification and link prediction tasks on various datasets.  They get extremely good results across the board.  I really like this paper and it presents a nice vector space take on the text-network modeling idea.