# Review for "On Learning Markov Chains"  This paper concerns two fundamental question about Markov chains. Consider observing $n$ steps of an unknown Markov chain on $k$ states, then doing one of the following:  (1) predicting the distribution of the next state. (2) estimating the entire matrix of transition probabilities of the chain.  How does the performance of minimax-optimal estimators for these tasks depend on $n$ and $k$?  To make this question precise requires choosing a loss with which to measure the performance of the predictor. This paper makes substantial progress on question (1) when the loss is the KL divergence between the predicted distribution and true distrbution of the next state. It almost closes the book on question (2) for a variety of interesting losses, including KL divergence, chi^2 divergence, and Hellinger divergence.  To give an idea of the flavor of results, one of their theorems is the following: the minimax-optimal loss for predicting the distribution of the n+1-st state of a k-state markov chain is between  $C (k-1) \log \log n / n$ and $C' k^2 \log \log n /n$, for some constants $C$ and $C'$. The best previously-known result [Learning Markov Distributions: Does Estimation Trump Compression?, ISIT 2016] was that this risk is $\Theta_k(\log \log n / n)$; the dependence on $k$ was unknown. However, especially in machine learning applications the number of states of a Markov chain may be large; studying the dependence of the optimal estimation rates on $k$ is important.  This paper is quite well written; I did not have time to verify the technical sections but the exposition in the first few pages is clear and convincing. The paper makes significant progress on core problems for the NIPS community. I strongly recommend that it be accepted.