The paper proposes an alternative architecture to mainstream captioning models which usually follow a sequential structure. The model has two stages: 1) generating explicit semantic representations given images using noun phrases. 2) composing the noun phrases using a recursive architecture to generate the final caption.   - Idea/approach wise I think this is an interesting direction to go for generating captions, especially with the issues currently associated with captioning models (e.g. lack of diversity).  - The paper is very well written. It is easy to follow the ideas and get to the details of the approach, the illustrations (figures) are motivating and clear.  - While the current approach only outperforms the state of the art with one metric (SPICE), the experimental results are strong -- with good analysis to show where further improvements can be made (e.g. providing ground truth noun phrases), and where the strengths of the model is (generalization to other datasets).  Things to improve: - It would be great to show the training and testing time of the model. For example, how long does it take to train the connecting module? Comparing all pairs of left and right might take a long time.  - While SPICE is shown to have correlated more with human judgement, it would still be good to conduct some human evaluation experiments to strengthen the paper. - It would be nice to show some failure cases of the model, and see whether it is n-gram grammatical structure that fails in the most cases. - Are the modules trained jointly? If not, would joint training help?