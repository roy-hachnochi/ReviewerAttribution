This work deals with a memory system that is intended to be able to use relevant pieces of information from the potentially distant past in order to inform current predictions.  The work is well motivated by examples of biological systems, but even without these arguments, memory systems are of course critical parts of many machine learning systems so any significant contributions here are relevant to almost the entire community.    I find the architecture appealing and I think it has features that intuitively should help with the type of settings they consider, which are difficult for standard RNNs due to the time horizons which are found in training in practice.  This is also borne out in the empirical results they show in a number of very different domains.  On the other hand, I do have some reservations, which lie with the comparisons made both in the Related Work / section 2, and Experimental Results / Section 4.  Section 2 only mentions  resenets and dense networks (originally for convnets, although the connection to RNNs here is clear), and transformer networks which replace recurrence with attention.  I do feel that this section almost completely ignores innovations in memory systems that do allow information to flow more easily over long periods such as hierarchical/multiscale RNNs, among others.  Likewise these comparsions are also missing from the experimental results; outperforming an LSTM is not so impressive when other architectures with similar design considerations exist.    -- After rebuttal -- I appreciate your comments and work responding to my, and other reviewers queries.  The additional work is significant both in terms of effort and quality of results and warrants a revision of score.