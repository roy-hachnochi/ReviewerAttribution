  The main goal of this work is to lower the precision of training with deep neural networks to better use new styles of hardware. In particular, the technical idea is to lower the size of the accumulator in dot products and the representation of all weights. The main observation is that a form of chunking, which is well known in the optimization and math programming community, has sufficiently better error properties to train DNNs. The paper is clear, simple, and effective.  - The techniques are standard in this area, and so it's unclear if its met the novelty bar for NIPS.  - A more thorough citation of the work on reduced precision could help better position the contribution.  - Claimed contributions appear to be overstated or under validated.  The title is a bit misleading, since it really is a mixed precision paper as a result of many of the numbers actually being FP16--not 8. And it's not about all of deep learning--just CNNs and while this is substantial enough to be interesting, it's not a general purpose deep learning solution.   FP8 format contribution. There is no experimental support for the particular choice of FP8 (one could choose other mantissa/exponent combinations). As this is claimed as a contribution but not justified in experiments. The claim of designing the format is also a bit of a stretch here: unless, I've missed something, it's just filling out the parameters to the standard FP model (which gives one all of the analytic results on blocking from below).  I think the paper could be helped by more careful positioning of the contributions, both techniques are well known in this area: Reduced precision with stochastic rounding has been used by several authors in this area as has blocking. More below.  While phrased in terms of dot products, the core issue here is the numerical stability of sums. This is very well studied in the HPC literature. There is a single citation narrowly around dot products [1], but this has been well studied and is often used in intro courses. The novelty lies in its application to deep learning and showing that this simple technique can obtain good error properties.  This work does not subtract from the contribution of this paper, but I think would help give context about the differing role of stochastic rounding. A representative example is QSGD from Alistrah et al in NIPS17. Other communities including the architecture community have picked up on these lines of work (see recent ISCA or MICROs). A more full accounting of related work would help. That said, these works have focused on stochastic style rounding as the only technique--neglecting the nice insight here about the effectiveness of blocking. This is well known technique in the HPC community, and is an even simpler version of the blocking technique.  The validation is light to make the claim about all deep learning as it only examines CNN. If the paper have claims about other neural networks that would strengthen the paper.  No details of the hardware were provided. This is probably unavoidable.  Author feedback: I believe folks are excited, and I think adding in more related work would strengthen the paper. It's exciting work.