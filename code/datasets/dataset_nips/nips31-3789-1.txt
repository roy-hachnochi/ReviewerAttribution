The work at hand demonstrates that it is, given certain modifications, possible to train DNNs using 8-bit floating point numbers (instead of the commonly used 16- and 32-bit representation). Section 1 motivates the training of DNNs using only 8-bit representations and also sketches the decrease w.r.t. the test error in case of reduced precisions (1-2%, see Figure 1). Section 2 describes the modifications made by the authors. In particular, the authors show that one can successfully train the networks using 8-bit representations for all of the arrays used in matrix/tensor operations when using 16-bit sums for accumulating two 8-bit operands. This is achieved using two novel schemes, chunk-based accumulation and floating point stochastic rounding, which help to prevent swamping. Section 3 provides an experimental evaluation, which indicates that one can successfully train a variety of networks using a reduced precision without a decrease in model accuracy. Section 4 provides a detailed discussion of the results achieved. Conclusions are drawn in Section 5.  The paper is well written and addresses an interesting and challenging problem. While this is not precisely an area of my expertise, I have the feeling that the work at hand depicts a solid piece of work, which might influence future implementations of DNNs: The problem is motivated nicely and the modifications made seem to make sense and are justified experimentally. The final results (Figure 4) also look promising and indicate that one basically further reduce the representation to 8/16 bits in the context of training DNNs. This will potentially lead to a speed-up of 2-4 in future. The authors have also implemented their modifications in hardware.  To sum up, I think this depicts a solid piece of work, which might lay the foundation for new hardware platforms (as stated by the authors). Due to the importance of such implementations, I think this work is of interest for the NIPS community.  Minor comments: line 120: chunk-based line 161: url to sharelatex project Table 2: FP8 not in bold letters