Update: Thank you for your feedback. Given your comments as well as the discussion with the other reviewers I have slightly adjusted my score to be more positive. I still stand by my comments below in that I think the work is interesting, but that the presentation in its current form is misleading. Assuming the paper will be accepted, I implore you to reconsider the presentation of this work, particularly with respect to claiming that this is a fully fledged dialogue system. The idea that the last image chosen represents a distillation of all previous rounds of dialogue is fanciful. Likewise, as pointed out by several reviews, this work is strongly related to a prior art on the subject of relative and incremental captioning. I would appreciate an effort to better represent the state of those fields in your introduction and background sections.  Original review: This paper discusses a dialog based image retrieval system as well as a user simulator that this system is trained with. The dialog here is multi-modal in that while the user speaks, the retrieval system only returns a single image in lieu of an answer.  The model for this problem looks reasonable, combining encodings for the previous candidate image and the user input, feeding these into a recurrent state model and subsequently proposing candidates using a KNN lookup. The user simulator is somewhat less inspired, ignoring dialog history and simply generating a sentence given a target and a previous candidate image.  In the evaluation of the paper, the proposed model is shown to perform well compared with an ablation study of simpler models.  All of this looks fine, and there are some nice bits about performance gains through model-based RL in the discussion section, but in balance there is not enough in this paper to merit a higher score in my opinion. The task is very specific without a good justification of why we should care about it or convincing pointers on how the research proposed here would be relevant to other problems. Likewise, the dataset is quite small, and lacking more detailed analysis of train/test results; the quality of the user simulator model is difficult to judge (which seems to spit out surprisingly good answers on presumably examples from a validation set, considering the limited size of training data), and the remaining results are difficult to put into perspective considering the only comparison is with the ablation study and some strongly crippled baselines.  Unfortunately with the task and analysis falling short, the model itself does not provide enough novelty to really motivate accepting this paper in this format. I feel the current paper would be an OK workshop or short paper, but falls short of what would be expected of a long paper.