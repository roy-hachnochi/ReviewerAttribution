Post Rebuttal and Discussion Update ============================ I remain in support of this paper and think the community would find it interesting. That said, I agree with the other reviewers that more careful, qualified use of the word dialog would benefit the paper and leave scope for future work which does allow references to past rounds. ============================  This work presents an interactive dialog-driven image retrieval problem and explores it in the context of shoe retrieval. While logically only a small step away from caption or phrase based retrieval, the interactive nature of the problem leads to interesting challenges. This problem is well motivated and will likely be of interest to the community at large. The paper is well written and provides substantial detail regarding design decisions of both the proposed method and the data collection process (additional details in supplement). Overall the results point towards the proposed method working quite well, as do the supplied video demos.  1] A minor gripe is with respect to novelty of 'relative image captioning' defined as the task of 'generat[ing] captions that describe the salient visual differences between two images'. The work "Context-aware Captions from Context-agnostic Supervision" [51] presents a very similar problem called 'discriminative image captioning'. In this task, methods must take in two images and produce a caption which refers only to one of them. As the authors note in the appendix, a common user mode from the dataset is to simply describe the target image independent of the retrieved image when they are sufficiently different. This portion of the problem seems to mirror discriminative captioning more than relative captioning. For other comments that describe target visual features relative to the retrieved image, relative captioning is a distinct problem.  2] It would have been nice to see a SL based attribute model for completeness.    Minor: L164 - "Training an RL model for this problem requires extensive exploration of the action space, which is only feasible if a large amount of training data is available." I do not understand this point, why does RL training require substantial training data collection?  Is this in reference to SL pretraining cost? Or user-in-the-loop training?  I do find myself wondering how well these models and training regimes would extend beyond such a narrow scope (product images of shoes) but this is surely outside the scope of this submission. 