This paper introduces a large set of experiments to compare recently proposed GANs.  It discusses two previously proposed measures -- inception score (IS) and Frechet Inception Distance (FID); and it proposes a new measure in the context of GAN assessment, based on precision, recall and F1.  Precision (P) is measured as the fraction of generated samples with distance below a pre-defined threshold \delta; while recall (R) is measured as the fraction of inversely generated samples (from test set) with squared Euclidean distance below \delta (F1 is the usual mean between P and R).  The paper argues that IS only measures precision and FIS measures both, so IS is essentially dropped as a measurement for GANs.  Then the paper argues that it is important to show the mean and variance of FID and P-R-F1 measurements instead of the best values, computed over a set of random initialisations and hyper-parameter search points.  This is an extremely important point that is often overlook by several papers published in the field, so I completely agree with the paper in this aspect.  The paper also shows the performance of each GAN in terms of computational budget (not clear in the paper what this is, but I'm assuming some fixed amount of GPU time, for each tick in the graphs) and conclude that all models can reach similar performance with unlimited computational budget.  The paper also reaches these conclusions: 1) FID cannot detect overfitting to the training data set, and 2) many dimensions have to be taken into account when comparing different GANs, but the paper only explores a subset of them.  I think that it is laudable that a paper tries to address this challenging task, and the results are potentially useful for the community.  I think that the experimental setup is interesting and compelling, but hard to be replicated because that would require a very large amount of computational resources that is not available for the majority of researchers.  The main conclusion of the paper is expected (that there is really no model that is clearly better than others in all conditions and for all datasets), but not very helpful for the practitioner.  There are interesting points, particularly considering the P-R-F1 measure -- for instance, looking at Fig. 5, it seems that NS GAN is a clear winner, but there is no strong comment about that in the paper, so I wonder if there is any flaw in my conclusion.  Also, what is the correlation between F1 and FID measures?   Other issues: - In Figure 1, Why does the text say "rather high bias", while the figure caption says "slight bias"?  How is it possible to have a bias much bigger than the variance from samples of arguably the same set?   - Please clarify what budget means in Figure 3. - Where is the VAE result in Figure 5? 