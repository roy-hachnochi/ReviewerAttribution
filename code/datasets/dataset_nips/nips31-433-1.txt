The papers performs some knowledge distillation between two networks using GAN related loss. The trick is to consider the output from a classifier as a generative process fed by the examples to classify.  Then using a discriminator it becomes possible to train the "student" network to be undisguisable from the generative process from the "teacher".   pro:  - Related work section is nicely done and very pleasant to read - The paper uses some recently introduced tricks as the Gumbel-max to tackle the difficulty to learn GAN with discrete distributions  - Thanks to soft label annealing it seems easier to learn the student network. This point could deserve more attention since an obvious discriminator is the scalar product of the one hot encoded classes emitted by the two generators  cons: - The papers claims to be able to learn lighter classifiers but never discusses classical elements of KD as compression or loss of precision.  - Is the paper aiming to reduce the required number of samples to achieve learn a classifier? If YES it would be good to compare to works close to one-shot learning (see https://arxiv.org/pdf/1606.04080.pdf as an example which have higher accuracy on harder datasets with very few examples). If  NOT,  the discussion about the number of samples becomes somewhat irrelevant unless transformed into something like "number of samples requested to exhibit KD" and then prove empirically that there is scenarios where the proposed approach requires less samples. - I'm not sure to understand why the initial input x should be provided to the discriminator - The reported scores for the final student nets are quite low both on MNIST and CIFAR. Thought I understand this is not the sole objective of this work, I feel this cast a shadow on the the empirical evidences of this paper. 