The paper deals with the problem of deep hashing, namely learning a deep CNN to produce binary codes at its output, with application to retrieval with nearest neighbor search and other tasks. The main challenge in this formulation is the discrete nature of the target output hash codes. Their non-differentiable nature prevents the use of vanilla SGD for training. There are various approaches in the literature to circumvent this issue, such as using a tanh(beta.x) function with annealed beta or adding a penalty to encourage the codes to be near-binary. The approach advocated by the paper is to  adopt a pass-through approximation for the backward step: One simply uses the pointwise sign(x) function in the forward step (both during training and inference), and simply leaves the gradient intact during backward propagation, as if the sign(x) function was not there, which have also been advocated by other deep learning formulations that employ activation discretization. The authors evaluate their approach roughly following the experimental protocols of the related DSDH [15] approach, showing reasonable improvements over [15] and other baseline methods. Pros: Simple and effective method, reasonable improvements in performance and training speed compared to the closest DSDH baseline. Cons: Relatively small novelty. Discussion of the proposed approach in Sec. 2.2 and 2.3 is too lengthy.