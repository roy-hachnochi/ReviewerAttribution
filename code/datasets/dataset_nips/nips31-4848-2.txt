This paper introduces a self-explaining model which progressively generalizes linear classifiers to complex yet architecturally explicit models. Two parallel networks are utilized to generate concepts and relevances, respectively, then a linear aggregator is applied to do the classification. Three desiderata for explanations are proposed, including explicitness, faithfulness, and stability. Experiments on three datasets (including MNIST, UCI datasets, and Propublicaâ€™s COMPAS Recidivism Risk Score dataset) are conducted to demonstrate the explicitness/intelligibility qualitatively, the faithfulness and stability quantitatively.  I have several comments to this work: 1. The model proposed in this paper seems novel and interesting. Different from the posteriori explanations for previously trained models, the authors introduce self-explaining model trying to achieve explicitness, faithfulness, and stability for predictions locally. However, there is a key point for self-explaining model: can the self-explaining model achieve comparable prediction or classification accuracy to the black-box deep model, and at the same time be more interpretable? If not, what are the advantages of the self-explaining model compared with the posteriori explanations? Could the authors explain this and show more experimental results to demonstrate the prediction accuracy of the self-explaining model? 2. The authors indicate that pixels are rarely the basic units used in human image understanding, instead, people would rely on higher order features. However, in the experiments, the authors use only MNIST dataset which is simple for vision task. Other datasets (e.g., CIFAR-10) are expected in the explicitness/intelligibility to demonstrate whether the proposed SENN can generate these higher order concepts.