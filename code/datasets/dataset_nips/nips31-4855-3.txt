The authors propose an interesting and reasonable variational autoencoder-based model for graph generation. The model works by sampling all potential nodes in the graph at once, which are then connected stepwise via edges in a breadth-first traversal. The model is then evaluated in several tasks measuring the properties of the generated graphs, and an optimization task. It is commendable that the authors performed ablation studies, which justify several nonobvious steps in the model design.  Overall, I think this paper is a good ML paper, especially compared to other recent work on novel generative models for molecule design. Given the importance of graph structures for almost any domain, I think it would be worth presenting this work at NIPS. However, a few additional experiments should be considered to make the paper stronger, and to back up the claim of state of the art, which I currently don’t see fully supported.   If the additional suggested experiments (in particular a SMILES-LSTM baseline) are provided, I would increase my evaluation from 7 to 8, regardless of the outcome of the comparison experiments.   Data:  The use of the QM9 dataset is rather unconvincing. Since QM9 is a small, enumerated dataset, based on hard constraints (everything which is not in this dataset is supposed to be very unstable), a proper generative model, which has learned the underlying distribution, should not generate anything outside of this data. Also, when it is practically feasible to enumerate a whole set of molecules, why is a generative model needed?   Thankfully, experiments with ZINC, and the photovoltaics dataset (which by the way does not contain “very complex” molecules, [lines 225/226], but in fact simpler ones than ZINC, please rectify this!) are provided, which are better. The goal standard however is ChEMBL, and it would be highly recommended to run experiments in that dataset to replace QM9 if that’s still possible.  Baselines.  First, it is great that the authors compare against several VAE and other baselines. However, a simple autoregressive character-based SMILES-RNN  baseline is missing. Even though this rather primitive model is less fancy than VAEs, in practice it provides not just syntactically, but also semantically correct molecules, which is in contrast to those by e.g. GrammarVAE or SMILES-VAE, which tend to generate semantically problematic molecules, which can likely never be made in the lab.   Already the Li et al paper indicated (even though hidden in the Appendix!), that the KL divergence of the property distributions of generated vs. test molecules, for several key properties, is actually lower for the SMILES-RNN than for their autoregressive graph model.    Also, in another recent paper ( https://doi.org/10.1186/s13321-018-0287-6 ), it turned out the SMILES-based RNNs are actually competitive with graph-based conditional models.  Therefore, in my opinion, to claim state of the art, it is absolutely necessary (and straightforward, because trivial to implement, Also note that the SMILES-RNN does not need any masking, or sophisticated preprocessing!), to compare to a well-tuned  SMILES-based LSTM (start with 3 layers x 512 dim) – also keeping in mind the results by Melis et al. ICLR 2018 for language models. In practice, the SMILES-RNN is widely and successfully used, and actually molecules designed by it have been made in the lab and tested (https://doi.org/10.1002/minf.201700153)    Furthermore, in addition to Atom, Bond and Ring Countx, which are not very meaningful on their own, please add histograms for the following other physicochemical descriptors:    # requires rdkit # from rdkit.Chem import Descriptors  descriptors = [Descriptors.MolWt, Descriptors.BertzCT, Descriptors.NumHDonors, Descriptors.NumHAcceptors, Descriptors.MolLogP, Descriptors.TPSA, Descriptors.NumRotatableBonds] for descriptor in descriptors:    value = descriptor(mol) # now collect value ... ##   Style and Layout  It would be desirable to have larger, proper histogram plots of the property comparison.