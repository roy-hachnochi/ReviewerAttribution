 This work introduces a new method for inference in deep dynamical latent Gaussian models. When working with the filtering assumption during variational learning, the current time-step's [t] posterior estimates are independent of the entire past and depend only on the variational parameters found in the previous time-step [t-1]. To calculate the approximate posterior distribution for the current time step [q_t], this work holds fixed q_{t-1} and performs local updates (by maximizing the ELBO) to find the best q_t possible for the current time step. This is used for inference and parameter estimation.  The idea is straightforward and follows upon related recent work in the static (non-temporal) setting. e.g. http://www.people.fas.harvard.edu/~yoonkim/data/sa-vae.pdf http://proceedings.mlr.press/v80/marino18a.html https://arxiv.org/abs/1801.03558 https://arxiv.org/abs/1710.06085  The paper is generally clearly written and Figure 1 was useful to understanding the high level idea. I like the idea, its simple and does appear to work well in some situations. That said, I think the paper could be improved with some additional detail: * How is the argmin in Equation (11) solved?  In the experimental section, I felt the paper jumped straight to the benchmarks without pause to explore any of the algorithmic contributions in detail: * What happens as the number of inference steps is increased? Did only a single step of inference yield the best results? * It would be useful to provide some intuition on where this approach helps -- one way to achieve this might to categorize which time steps see the most improvement in likelihood from improving the fidelity of the filtering based variational parameters. 