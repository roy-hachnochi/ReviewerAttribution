# Summary  The paper proposes a novel--fairly straightforward--network architecture to learn image rendering. Several rendering techniques from computer graphics are imitated while still providing a differentiable model. Trained only on chairs, the model is then used to invert the rendering process to reconstruct faces, relight them and show them from a different perspective.  # Paper Strengths  - Relevant problem: inverse rendering and learning to render tie an important connection between our knowledge of image formation and image semantics. In particular, it allows better reasoning about 3D relations in scenes. - Well written work, clear presentation. - Well chosen experiments to support their idea. - Fairly straightforward idea that smartly picks the fixed and learned parts of the architecture: learning the projection from 3D to 2D allows the model to reason about occlusion. - Although the model is only trained on chairs, it generalizes well to unseen objects. - Ambiguities during the face reconstruction are solved by regularising possible shapes with a pre-trained 3D auto-encoder.  # Paper Weakness  - The scene representation--a voxel grid or the texture representation--might not fit all use cases and feels limited.  # Further Questions  - Will the authors release their code upon publication of this paper? - What are the limitations of your work? Voxel volume? Output size? What would make it break?  # Conclusion  I like the simplicity of the proposed method and the clean presentation of this work. It makes a strong contribution towards inverse rendering.