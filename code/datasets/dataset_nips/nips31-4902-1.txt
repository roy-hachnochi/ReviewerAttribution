This work focuses on an approach to detect adverserial inputs to a given DNN. The authors' approach involves understanding the output distribution  of the neurons in a DNN in response to the natural intended input. They argue that  this distribution will be different from the distribution produced by adverserial  input.  The authors model the distribution of the hidden states through Gaussian Mixture Model (GMM). The decision whether an input is natural/adverserial is based on a threshold placed on  the distribution.  The merit of the approach is explored empirically for Black-Box, Semi White-Box and   Grey-Box adverserial scenarios. The authors show that at times their approach can  outperform baselines. They provide some reasoning (such as speed, performace on unseen samples etc,)  for still using their approach where other baselines might be better.    It would be interesting to know how the performance changes with change in DNN architecture.  Do wider/deeper networks have any influence on the performance. For very large and complex models,  is it possible to use only layers close to the input/output and still be able to achieve decent performance.  How about bigger tasks such as ImageNet classification.  How about other domains apart from images?  Currently, GMM is used to model the neuron distribution. Does it make a difference  if the model of hidden neuron distribution is changed? 