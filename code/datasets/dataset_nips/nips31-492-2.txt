This paper proposes a method (DCP) to prune away channals in CNN by introducing additional losses to the intermediate layers. Original model weights are fine-tuned with the additional weights and channels are selected in a greedy manner layer by layer.     Extensive experiments show improved performance against the state-of-the-art methods.     The authors made the paper easy to following by clearly presenting the motivation, contribution and the method itself.     While adding discrimative losses to intermediate layers of neural nets is not entirely new, the proposed method does demonstrate reasonable amount of novelty to obtain good empirical performance.     I feel the strength of the paper lies in the extensive experiments; comparison to existing methods seems exhaustive and the ablative analysis appears robust.     Some questions:    * Why are the gradients with respect to the weights used in greedy channel selection, not the weights themselves? Did you try channel selection based on weights?    * Is there a rule of thumb to determine the number of channels to prune for each layer? E.g. prune fewer channels in the first layer and prune more aggresively in the later layers?    