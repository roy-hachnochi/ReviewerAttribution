Thanks for the author feedback! ---- The paper proposes a modification to how attention is performed in sequence-to-sequence models. Instead of attending at the last layer of the encoder, each layer in the decoder attends to the corresponding layer in the  encoder. This modification allows the models to cover more source words compared to vanilla models. The paper presents a good comparison to previous methods. The visualization provide  useful information on what the model has learned and the ablation studies are also much appreciated. It is very interesting that the layer-wise attention helps them get better performance as they increase the number of layers compared to baseline model.  