This work proposes a layer-wise coordination for encoder and decoder networks of sequence-to-sequence model by jointly computing attention to both of encoder and decoder representation of each layer. In a conventional transfomer model, each decoder layer computes attention over the last layer of encoder layers, and self-attention to the decoder's history and attention over encoder representation are computed separately as different layers. In this work: - Encoder and decoder share the same number of layers. - Each decoder layer computes attention to its corresponding encoder layer, not the last encoder layer. - Attention does not differentiate encoder or decoder, but they are simply mixed so that attention layer could focus either on particular position at encoder or decoder. Experimental results indicate significant gains over strong baselines under multiple language pairs.  It is an interesting work on improving the strong transformer baselines, and experiments look solid. I think larger gains might come from more layers since the proposed model does not have to differentiate whether it is performing self attention and/or encoder attention. However I have a couple of concerns to this work.  - It is still not clear to me why mixing encoder and decoder for computing attention is better though attention visualization could be a clue. It would be good to analyze whether particular layers might be serving only for self-attention and/or encoder-attention.  - I could not find any notions of multi-head attention, though it is one of the features in transformer which implicitly performs projection from encoder attention to decoder representation. Do you use multi-head attention and if so, how many heads employed in your experiments? If no multi-head attention is employed in this work, it is a bit of mystery in that encoder representation will not be projected into decoder representation. As indicated in the response, please mention it.  Other minor comment:  - It has an implicit assumption that source/target token ids must be shared given the embedding parameters are shared. If possible, I'd like to see the effect of using the shared embedding for the original transformer model.  