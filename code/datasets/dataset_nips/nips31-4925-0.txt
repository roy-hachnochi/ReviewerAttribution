          This submission proposes a new prior on the topic-word         distribution in latent topic models. This model defines a         multi-layer feedforward graph, where each layer contains a set         of valid multinomial distributions over the vocabulary, and         weighted combinations of each layer's "topics" are used as the         Dirichlet prior for the "topics" of the next layer.          The key purported benefits are sharing of statistical strengh,         inference of a hierarchy of interpretable "abstract" topics,         and modularity that allows composition with other topic model         variants that modify the document-topic distributions.                  The authors present an efficient fully collapsed Gibbs sampler         inference scheme - I did not thoroughly check the derivation         but it seems plausible. Although: what is the computational         complexity (and relative "wall clock" cost) of the given         inference scheme? The experimental datasets used seem         relatively small.          The quantitative evaluation shows improved results on typical         topic modeling metrics: perplexity, coherence, and auxiliary         classification task accuracy. A handful of chosen topic         hiearchies suggest nice "abstraction" in the parent topics,         although I would have liked to see some quantitative analysis         of this aspect as well. Exploiting the compositionality with         document-topic variants for the experiments was a nice touch         that I liked. I was curious how sensitive the results were to         variation in the hierarchy depthy or width. Some data about         the inferred connectivity weights between layers would have         been interesting as well (relative sparsity, concentation         of weight, etc).          Overall I found this paper to be an original, high-quality         contribution that should admit further extensions and         combinations.  The paper is clearly written thoughout,         well-contexualized with respect to the relevant literature,         and the core idea is simple (in a good way).                  I found the topic hierarchy diagrams in Figure 2 (and the         supplement) to be too small to easily scan and read.          L217: can the correlation between the the topic and labels be         quantified somehow?                                UPDATE: I have read author feedback and found the answers to my questions useful. Space permitting, it would be nice for some versions of those answers to be incorporated into the final submission.