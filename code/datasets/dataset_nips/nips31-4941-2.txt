This paper studies gradient-based methods for optimizing a deep submodular function. The starting point is a natural concave relaxation where each variable is allowed to take fractional values. The main contribution is a bound on the gap between the concave relaxation and the multilinear extension, generalizing a bound for coverage functions by Karimi et al. For large k, and depending on the parameters of the DSF, the resulting approximation ratio can improve on 1 - 1/e.   Providing improved tools for maximizing DSFs is a natural problem, and the ability to improve on the approximation ratio and scalability of the greedy algorithm is an attractive property. However, the improvements (both theoretical and empirical) appear somewhat marginal in many cases. Additional information on a few points would be helpful to assess the significance of the work:  First, the bounds depend on parameters of the DSF like |V_1| and w_min/max. It is claimed that these quantities are often favorable in practice. However, none of the papers cited use DSFs. What evidence is there that real problem instances are amenable to such techniques?  Second, what is the value of n (the size of the ground set) for the experiments? This is important context for understanding the scalability comparison vs greedy.   Third, the comparison vs greedy uses a partition matroid constraint. Is there any improvement in solution quality (compared to greedy) for a uniform matroid? This is the more common case in practice, and would help disentangle two factors that may be driving the performance improvement: (a) the concave relaxation becoming tighter with large k, and (b) continuous methods in general simply being more appropriate for general matroids.  Fourth, the motivation for using the concave relaxation is that the multilinear extension might require many samples to approximate. However, this is often pessimistic because the multilinear extensions of common families such as converage functions or facility location can be computed in closed form. Do similar techniques extend to the compositions used in DSFs?  Update after author response: The response helps address some of my concerns and I have raised my score by one point. The experimental evaluation is still weak overall though; this paper introduces some nice ideas and it would be good to see them tested more thoroughly. 