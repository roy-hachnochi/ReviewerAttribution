Building on the WaveNet-inspired paradigm of neural audio generation, the authors consider how to generate longer duration audio. This is an unsolved problem at present, with many architectures unable to encode all timescales of audio structure in neural representations.  The method performs a kind of hierarchical autoencoding, with fine timescales being summarised and then re-generated by a next layer. The authors are thus able to train a system that generates tens of seconds of raw audio waveform. The feasibility of training is a contribution of the work.  The authors argue correctly that listening is currently the best way to evaluate generated raw audio in these settings. The audio examples are interesting and persuasive, with good general structure and some interesting quirks. The paper discusses these well.  The author response includes results of an additional listening test, which is useful.  In summary, I find the paper is a useful practical step forward in neural nets for audio generation, incremental on the theoretical side, but a good demonstration in practice.  