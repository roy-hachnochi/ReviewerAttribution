This paper presents work on autoregressive discrete autoencoders, which are models that are well-suited for modeling long-range temporal structure and generating audio sequences. The authors present a set of tips and tricks that enable learning for such a problem, and in the process they introduce a new type of quantizing VAE.  This paper is very well-written. It is easy to follow, there is a sense of purpose in the narrative, and many obvious questions are properly addressed. Kudos to the authors for not simply listing what worked, and actually motivating their approach. The resulting outputs are definitely an improvement as compared to the usual deep-AR models, exhibiting better consistency across time.   I was, however, a little disappointed that the definition of "long-term" and "high-level" used in this paper only spanned a few seconds. I would argue that these terms encompass composition form (e.g. ABA structure), and not so much the content from the last few seconds.  As with many such papers, I will agree with the authors that coming up with metrics is often pointless since listening is probably the best way to evaluate things. This of course creates a problem in evaluating significance of this work, although informally I found the submitted soundfiles to be pretty good.  I would perhaps detract a few points since I don't see a wider applicability to this work. I would be more convinced had I seen a bit more complex music (in terms of timbres), or some glimpse that this work is relevant outside of this specific problem. This is however a solid paper, and I'd be happy to see it accepted.