The authors demonstrate how SVD on learned representations can be used to remove poisoned examples from the dataset.  Perhaps the most interesting result is that the watermark is enhanced through training in higher layers of the network and that this is a function of how models are overparameterized.    This leads me to wonder about the effects of distillation on these types of attacks.  For example, if a trained model was distilled into a much smaller representation would this separation technique still hold?  Phrased differently/relatedly, I assume these effects change as a function of (1 − β)m?    It would be great to see an experiment in which the number of filters is varied and test how robust this approach is to those changes.