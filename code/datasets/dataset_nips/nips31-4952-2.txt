The authors propose an approximate inference algorithm named collapsed compilation based on collapsed importance sampling and knowledge compilation (which is responsible for the exact inference part). It first compiles factors to SDDs based on a specific order to some size limit, and then runs online importance sampling with samples from a proposal distribution aware of the exactness based on the current partially compiled graph.   The method is kind of complicated to use in practice since it depends on many parameters (e.g., circuit size, variable selection policy). Those instances reported in the experiment are quite diverse, and the probabilistic program inference benchmark is interesting. The writing is generally good.  To the authors: 1) “ we decide its proposal distribution by computing the marginal probability of Xj in our currently compiled SDD” (line 461), so you do not need any external proposal distribution, right?  2) Table 2 provides results on individual instances only, which is not that convincing. Do you have aggregated results for multiple instances of each benchmark?  3) From Table 1(b), we can observe that the “100k” entry for “FD” is better than the “1m” entry, why? 