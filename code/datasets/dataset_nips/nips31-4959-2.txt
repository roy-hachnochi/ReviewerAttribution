The paper proposes neural arithmetic logical units (NALUs) for learning numerical related tasks with deep neural networks. Specifically, NALUs explicitly enforce learning the systematic numerical operations, so the networks can perform well on values not seen in the training set.   The proposed method is clean, general and easily applicable to existing neural networks. The paper gives strong empirical evidence on various numerical tasks and shows superior performance on networks with NALUs.  I think the paper might need more studies like Figure 3 on both NALU and ordinary networks to better understand the network behaviors on various tasks.  Update: Thanks for the rebuttal! I have read all reviewers' reviews and the corresponding response. There are some flaws (mathematical notions, figure clarity, and lack of background work discussion), but overall I think the paper is of good quality.