This is indeed one of the best papers I've read in a while. The paper simply studies how the dimensionality of word-embeddings relates to the bias-variance trade-off. The paper builds upon the fact that word-embeddings are unitary invariant. Thus, the technique to compare two word embeddings should also respect this property. As a result, the paper introduces the PIP loss metric which simply computes the cosine similarity matrices of words for both embeddings, subtracts the two matrix and compute the norm.   The significant contributions are that, based on this loss metric, they can describe the bias-variance trade-off when selecting the number of dimensions. Moreover, by minimizing the PIP loss, one can determine the ideal dimensionality for their setting.   The paper is well-presented and the results are strong, and I think the NIPS community would benefit from reading the paper. 