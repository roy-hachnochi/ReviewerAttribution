[I include in square brackets, below in my point-by-point criticisms, my responses to the author's feedback on the review. Overall, my opinion has not changed: this is a strong submission.]  The authors build a new neural network architecture (D-nets) in which inputs impinge on different dendritic branches, each of which takes a weighted some over those inputs. The unit output is the max over the activations of its dendritic branches. This is a variant on the maxout networks, which have a similar architecture. The key difference appears to be that the D-net dendritic branches get sparse non-overlapping sets of inputs, whereas in maxout, each branch gets all of the inputs.  The authors use a similar proof strategy to Ref. 6, to show that D-nets can be universal function approximators, and evaluate both the expressivity of their networks (using transition number as the measure), and the performance of their networks on a wide range of ML tasks. Performance is compared with maxout, and with traditional neural nets, and the D-nets are found to have the highest expressivity and best performance of the architectures considered.  Overall, I was impressed by this submission. The comparisons to previous network structures are very thorough, the performance appears to be quite good, the D-net idea is straightforward but (as far as I can tell) somewhat novel, and the paper is relatively well-written.  I have a few comments / suggestions, listed below.  1. The proof in Section 3.2 is nice, but a little restrictive. It would be nice to have a more general proof, applying to conditions other than d_1=1. If such a proof is possible, I would highly recommend including it. Also, if there are conditions on d such that the networks are not universal function approximators, those conditions should be identified and stated.  [I am glad to hear that the proof can be generalized beyond d_1=1. I am somewhat concerned about the need for non-random connectivity that the first layer: is the requirement very restrictive? It would be worth clarifying that requirement (when it is likely to be satisfied or not), so that readers know clearly when they should, or should not, expect the DNNs to be universal function approximators.]  2. In the discussion of Sparse neural networks, there are a few recent neuroscience papers that are relevant, and that the authors should consider reading and/or citing (listed below). These might inform the optimal "d" values of the D-networks, and the methods from these papers could be relevant for understanding the optimal "d" values in the D-nets.  a) Litwin-Kumar, A., Harris, K.D., Axel, R., Sompolinsky, H. and Abbott, L.F., 2017. Optimal degrees of synaptic connectivity. Neuron, 93(5), pp.1153-1164.  b) Cayco-Gajic, N.A., Clopath, C. and Silver, R.A., 2017. Sparse synaptic connectivity is required for decorrelation and pattern separation in feedforward networks. Nature Communications, 8(1), p.1116.  [I am glad that you found these useful.]  3. It took me awhile to figure out that the acronyms  "BN" and "LN" in the figures and captions were batch norm and layer norm. I would suggest spelling those out in the legend to Fig. 4.  [Thanks for adding these; I think they will help with clarity.]  4. Why give the Avg. rank diff. instead of just Avg. rank? As far as I can tell, these diff values are just avg. rank - 4.5, and reporting them makes the actual rank values less transparent.  [It sounds like you will change these to avg. rank, which I support. I understand that there is precedent for the other way of reporting the results (avg. rank diff.), but sometimes previous work was flawed, and it's best not to repeat those flaws...]