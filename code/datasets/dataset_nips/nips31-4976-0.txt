The focus is safe reinforcement learning under constrained markov decision process framework, where safety can be expressed as policy-dependent constraints. Two key assumptions are that (i) we have access to a safe baseline policy and (ii) this baseline policy is close enough to the unknown optimal policy under total variation distance (this is assumption 1 in the paper). A key insight into the technical approach is to augment the  unknown, optimal safety constraints with some cost-shaping function, in order to turn the safety constraint into a Lyapunov function wrt the baseline policy. Similar to how identifying Lyapunov function is not trivial, this cost shaping function is also difficult to compute. So the authors propose several approximations, including solving a LP for each loop of a policy iteration and value iteration procedure. Next, based on policy distillation idea, safe policy and value iteration are extended to safe Q-learning and safe policy improvement that allow using function approximation. Experiment was done for a grid-world setting.   Overall, this is a good paper. The authors provided a nice overview and comparison against previous approaches to solving Constrained MDP. But I do have some reservations about the scalability of the approach, as well as the validity of the assumptions: - the main condition of baseline policy being close to optimal is not verifiable computationally - Do we further need restricting the cost-shaping function to be a constant? (line 207) - Solving both the LP in equation (5) and policy distillation LP in equation (6) repeatedly is not scalable to large / continuous state space? (I saw from appendix F that every optimization iteration fin SPI and SVI takes 25 seconds. How about for Safe DQN and SPI?) - How would we compute the set F_L(x) (line 132-133) in order to solve the optimization in Step 1 of algorithm 1 and step 0 of algorithm 2? - Are the results reported in Figure 1 over randomized, unseen grids, or over grids used during training?   Some other minor details: line 252 -> seems like some typo / D_JSD is missing  The phrasing of Lemma 1 : Don't we have infinitely many possible Lyapunov function candidates? "the Lyapunov function" sounds a bit strange.   The equation link in Algorithm 3 (policy distillation) is missing. Unfortunately this missing link is pretty important for understanding the algorithm. I assume this refers to equation 6.   ----- After rebuttal update: I've gone through the paper again, and have read other reviews and author response. Overall I believe this is a good paper and deserves an accept.   However, I'm still torn by the strong assumptions needed, the fact that the actual algorithms are still very computationally expensive to run even with the grid world domain. In addition, after reviewing the pseudocode for safe q-learning (sdqn) and safe policy improvement (sdpi) algorithms, and the grid world planning experiment, I realize that the dynamics model also needs to be known in order to update the policy in the inner loop (policy distillation step). This may further restrict the applicability of all of the proposed algorithms. 