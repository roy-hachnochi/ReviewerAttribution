This paper proposes an approach to "safe" RL where the notion of safeness comes from a constraint on another long-term expected quantity function (just like long-term cost, it could be regarded as a secondary objective function as in multi-objective RL). This approach fits in nicely with prior work on constrained MDPs. Extra care is taken in developing this safe RL algorithm so that this additional constraint is not violated at training time.  This paper is very technical and can be hard to follow as a result. Some extra high-level intuition and "how things fit together" would be very useful for orienting readers. The algorithms that result from this approach (in particular algs 1 or 2) are actually pretty simple. I wonder if presenting the basic algorithms first and arguing correctness second would result in an easier reading experience.  The key development of this paper is an extension to the Bellman-backup (used in value iteration for example), which accounts for the safety constraints. Developing this backup operator is not trivial (for example, the optimal policy is no longer greedy) and the analysis of it's properties requires lots of attention to detail, which the authors have taken. The end result is pretty elegant and fits into existing approaches to RL which are based on contraction mapping and fixed-point algorithms, which prior work did not do.  The experimental results show promising results on a simple maze task with constraints on state-visitation frequency. Could the approach be applied to a conventional RL task like cart-pole or mountain car (with extra constraints)? If not, why?  I would strongly recommend adding some discussion of scalability (runtime/sample complexity). The experiments presented are very small scale.  Questions ========  - Proposition 1: What is the specific regularizer that will be added? Does it affect the solution to the optimization problem or does it only affect tie-breaking behavior?   After discussion and author feedback ============================  I don't have much to add to my original review. It seems like paper #4976 is a very promising piece of work there is definitely substantial-enough contributions for a NIPS paper.   However,  1) If it were my paper, I'd completely reorganize it: it's hard to disentangle the approach until you look at the pseudocode (in my opinion). The proposed algorithms are actually pretty simple, despite a dense/technical exposition of the main ideas.  I am hopeful that the authors will be able to address the readability issues for camera ready.  2) As R2 explains, some of the technical assumptions required by the analysis may not be practical (see R2's discussion of bounding the hitting times and proper policies).  Please be sure to add discussion of practicality and scalability for camera ready. 