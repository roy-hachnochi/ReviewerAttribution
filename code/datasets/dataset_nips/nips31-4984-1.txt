This paper deals with event model learning from an event dataset, by considering that the probability of an event depends on the occurrence of some other event (the "parent" events) in the recent history.  The Proximal graphical even model proposed in this paper is an extension of the Graphical event models of the literature, where the authors have added the explicit definition of the time window the parent events must occur to "generate" each target event. The authors proposes one learning algorithm decomposed in several steps : learning the windows for already known parents, then identifying the optimal parent set by a forward-backward algorithm. optimizing the windows for multiple parents simultaneously is one hard problem, so the authors proposed 2 heuristics, one simple one where each parent is treated independently, and another one where a block coordinate ascent strategy is used over the parent set. Experiments are performed with synthetic datasets where the more complex strategy doesn't seem to give better results than the first (independent) strategy. This last strategy is then used with real datasets and gives interesting results compared to competitive solutions.  Some questions or comments: - why considering instantaneous events in your event datasets where process mining algorithms can deal with events defined in time windows. Is it possible to extend GEM and PGEM in this context ? - GEM deal with "transitions" between events, but how do you deal with the "initial" distribution of events ?  - why considering the last time between an event and the end of the sequence in the set t_{xx} ??? in a "process" point of view, it should be the time between x and the "END" state ?  - in your PGEM definition, you restrict yourself with the occurence of one parent event in only one time window, could you define the occurence of B with a presence of A in one time window and the absence of A in another one ??? (so considering that A could be present twice in the parent list of B, with 2 different time windows ?)  - 3.1, equation 1. The log likelihood is defined for every value \lambda. \hat\lambda is the argmax of this log likehood rwrt \lambda. What your are defining in eqn 1 is the optimal value of the logL.  - line 170 : typo : the computation still NEED? ...."  - theorem 6 : is your assumption over 2^|U| realistic ?  - line 197 : can RUN algo.2 ?       