** summary ** This paper proposes to tackle the problem of solving Markov Decision Processes  whose parameters are poorly known.  The authors focus on situations where Reinforcement Learning cannot be used directly,  neither through direct interaction with the environment nor via a simulator. For that they adopt a Bayesian framework. After having mentioned related works and the main advantages  of the framework exposed in this paper, the definitions and notations  specific to Markov Decision Processes are provided. The Bayesian policy optimization criterion for MDPs with imprecise parameters is then presented followed by the description of the proposed framework. An offline framework planner, using a Gaussian Process Temporal Difference (GPTM)  learning algorithm, computes an optimal policy for several sampled parameters  from the prior distribution.  The online planner uses data collected online and an MCMC algorithm  to sample posterior parameters.  The values associated with the posterior parameters are also computed  with the GPTM algorithm and allow the calculation of the optimal actions. This framework is tested against two other algorithms, namely  Multiple Model-based RL and One-step lookahead policy, and with two different environments. The first environment contains a state variable and an action variable,  both of which are continuous, and the imprecision concerns the transition function. The second environment is a belief-MDP, with three actions and the imprecision  also concerns the transition function (via the belief-MDP observation function). The results show that the approach is better in terms of learning speed  and policy optimality than the other two algorithms.  ** quality ** This paper has a nice introduction, the authors look at an important problem,  develop a framework adapted to the problem and test it on several systems. However, the paper lacks theoretical results or risk analysis of the produced policies.  ** clarity ** This work can be read and understood quite easily because it is mostly clear.  However most of the equations on pages 4 and 5 are not very readable and understandable.  They should be simplified and accompanied by intuitive and educational remarks and explanations,  or even a visual illustration.  Other less important remarks that would improve clarity follow.  page 4: Equation 7 is sticking out.  page 5: The variances are sticking out.  page 7: The notations "N=10" and "N^{post}=100" could be reused   < As expected, the minimum average reward > As expected, the maximum average reward  OBDM is not defined (what's the O for?)  page 8: < in a simplex of size 128 > in a simplex of size 127  ** originality ** The authors have done a good bibliographical work to position their study  in the relevant literature. However, work has also been carried out with different models of uncertainty. There are other models taking into account the mentioned imprecision  which is unfortunately often present in practice. For example work was produced for planning under Knightian Uncertainty, (Trevizan "Planning under Risk and Knightian Uncertainty." IJCAI 2007), with Dempster-Shafer belief Theory (Merigo "Linguistic aggregation operators for linguistic decision making  based on the Dempster-Shafer theory of evidence." International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 2010), with Possibility Theory (Drougard "Structured Possibilistic Planning Using Decision Diagrams" AAAI 2014), and with Imprecise Probability Theory (Itoh "Partially observable Markov decision processes with imprecise parameters."  Artificial Intelligence 2007). Some of these references should be cited since they address the same problem.  ** significance ** Experimental results are quite impressive and the motivation of this framework,  i.e. the fact that reinforcement learning is not always applicable  and that the parameters of a MDP are often imprecise, is of great importance in practice. However, the use of a Bayesian framework requires the knowledge or the choice of a prior distribution.  The framework of this paper also requires the choice of N and Npost.  How are these choices made in practice?