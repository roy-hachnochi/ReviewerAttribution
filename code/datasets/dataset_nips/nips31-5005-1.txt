The paper is a straightforward extension of Thewlis et al [31] to do unsupervised learning of symmetries. The main contribution is to realize that the formulation in [31] applies for the problem. This is achieved by a rather trivial addition of "reflection flip" m to to the loss from Eq 5 in [31]. The experimental results show that this recovers reasonable models of symmetry from images for simpler objects like faces and vehicle with limited transformations - and that harder cases are still potentially challenging. The paper also includes a set of lemmas on object symmetry in the presence of transformations, which appear pretty straightforward.   I found that the paper is hard to read -- it is heavy on group theory notation early, while lacking details on a practical instantiation of the method, which makes it hard to follow the exposition and to understand the differences from [31]. Ultimately, the paper lacks concrete method implementation details, requiring a careful read of [31].  Examples of lack of clarity include:  -- Network architecture is not described. Training details are missing.  -- In Eq 6, how is min H(ÐŸ) selected. How does one compute / approximate the integrals exactly over du and dw.  -- How are areas of the training images that don't contain the object embedded / handled?  -- In equation 6, should the second (diversity loss) by p(v | u,h) ?  -- In line 208, mgx is mentioned but Eq (5) contains mtx.   In Equation 6, if one does not explicitly enumerate a discrete set of rotations (as it appears to be done for the protein case), is it still possible to recover the symmetry?   There's a fairly rich literature on detecting symmetries in 3D, potentially more work can be cited:  -- Cluttered scene segmentation using the symmetry constraint, A. Ecins et al, ICRA 2016 -- Shape from Symmetry Sebastian Thrun, Ben Wegbreit: ICCV 2005 