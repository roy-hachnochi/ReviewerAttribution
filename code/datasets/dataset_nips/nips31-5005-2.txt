This work focuses on learning the structure of categories  from raw images of symmetric objects. Their approach focuses  on symmetries in object deformations rather than that in the  geometric shapes. They attempt to extend the object frame model to capture symmetries. They claim that their approach can provide  reasonings regarding ambiguities that can occur while recovering  the pose of a symmetric object.  The authors study an interesting problem where instead of studying  symmetries in shape they study the symmetry in object categories. They claim  that intrinsically symmetric objects might not appear symmetric due to  different pose/point of view etc. Therefore, studying symmetries in object  deformations may be helpful. Instead of modeling object categories as  shapes, poses etc., they model them as embedding a sphere into R^3. Their notion  of symmetry as a subset of linear isometries is also interesting. Their approach to  ambiguous poses as rigid shifts of reference poses is also interesting. They also  demonstrate empirically that their approach can work better than their main baseline.    I am curious about the influence of the warp distribution T. Any thoughts  about different warps? Also, the images would be much clearer if they were made bigger and higher resolution.   Furthermore, what sort of transformations h can be easily handled by the proposed approach (Eq 6). The authors seem to provide an example that was synthetically created based on rotational symmetry in nature . It would be interesting to know how the system might perform in non-synthetic scenario and what issues it might face.