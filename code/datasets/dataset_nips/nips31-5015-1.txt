Summary   The authors propose a sample-efficient framework for variational inference for low-dimensional (D<=10) problems in which evaluation of the joint is computationally expensive. This framework approximates the log joint with a GP surrogate and employs a variational posterior in the form of Gaussian mixture. Given this choice of variational distribution, the posterior mean and variance of the expected value of the log joint can computed analytically. The authors now iterate between an active sampling step that improves the GP approximation of the log joint, and updates to the variational distribution via stochastic gradient descent. The authors evaluate their framework on three families of synthetic functions and a computational model of neuronal activity in the V1 and V2 regions of the visual cortex in macaques.   Quality, Originality, and Significance  This is a fairly well-written and overall reasonable submission. Combining Bayesian quadrature with variational inference seems like a nice (if inevitable) idea, and the authors execute on this idea in a manner that results in a fairly straightforward framework, which in my opinion is a plus.   The main thing that I have trouble with in this paper is identifying when VBMC is the right tool for the job. The paper would be much stronger if the authors were able to come up with a compelling motivation for the problem at hand. In other words, what is the set of problems for which we (a) have a distribution with a low-dimensional prior and (b) a really expensive likelihood, for which (c) we would really like to obtain an approximation to the marginal likelihood and the posterior, and (d) where we can reliably learn a GP surrogate in <1000 samples (at which point GP scalability starts to become an issue). I also found it difficult to evaluate the significance of the experimental results (owing in part to the lack of detail in the description).   Clarity   The paper spends a fair amount of time establishing notation for all the background components of the proposed work (varitioanal inference, bayesian quadrature), which I appreciate. Unfortunately, the discussion of the experiments ends up being fairly terse. It's quite hard to understand exactly what problems are being considered in the experiments. For the the synthetic data, 3 families of functions are described in a long paragraph that contains the relevant information, but could be easier to parse.   For the experimental neuronal recording from the macaque V1 and V2 cortex, it would be helpful to have a more self-contained explanation of the problem at hand. The authors cite a reference in Neuron [12], which I presume very few readers will be familiar with. They then reference "auxilliary parameters" and transformation of constrained parameters via shifted and scaled logit transform, but it is very unclear what is going on.   I of course appreciate the challenges of fitting details in the NIPS page format, but at the very least the authors could provide additional details and discussion in the supplementary material?   Minor  - How is q^*(x) a "proposal" in proposal uncertainty sampling? Can this be interpreted as some form of importance sampling?  - For both dataset -> For both datasets   - likelihoood -> likelihood (please run an extra spell check)  - approximated via -> approximated with/by   Post-Response Comments  I've read the author response, which overall is reasonable. I am inclined to agree with the point that the ability of VBMC to provide uncertainty estimates in cases where we would otherwise perform Bayesian optimization is interesting. I was a little disappointed by the assertion that the the space of problems in which VBMC applies "is quite large and of outstanding interest in fields such as computational neuroscience". While the authors provide a citation, I would have appreciated more of an actual explanation or concrete examples. That said, I'm inclined to say that this is just about above the bar for acceptance, and have adjusted my score upward.