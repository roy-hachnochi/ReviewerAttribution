============ after rebuttal Some of my concerns in my original review have been addressed by the authors, and would like to commend them for following up with more experiments in the rebuttal (props to the new format). I overall think that this is a solid and very interesting paper, and will upgrade my score from 6 to 7.  ============ original review The authors aim to connect sharpness and variance of a given solution found by SGD with its generalization ability. The results presented are motivated from simple quadratic problems, and then are validated on experimental setups.  This is a mostly experimental paper on the sources of model generalization, and uses simple quadratic models to motivate the presented conjectures. I found the results of this paper extremely interesting. I also found the simplified examples quite interesting and intuitive. For example, Fig2 is quite enlightening in all its simplicity. I have not seen something similar, and although simple, it gives rise to an interesting observation. My main concerns are mostly on presentation and clarity, and also on the limited number of experimental data.  Here are my comments on the paper:  - the use of the word “stability” in this paper is very non-standard. Stability (in the context of generalization) is used to explain how robust are output models to perturbing the data set by a little bit (by say removing one sample), eg check the work by Elisseeff and Bousquet.  - in Section 2.2 it’s unclear if the authors focus on the simple quadratic problem of (1), or something more general  - I was a little confused by section 2.3 In this case there is only 1 global min. In this case, I don’t understand what the value of sharpness vs non-uniformity implies.  - In sec 2.3 the authors mention “At the level of matrices, H is a measure of the sharpness and Sigma is a measure of the non-uniformity. This is hard to parse as matrices themselves are not “measures” I don’t really understand what the sentence is trying to explain.  - It was not clear to me why the condition is (9) is needed for convergence.  - The authors mention that “ In the old days when typical models have only few isolated minima, this issue was not so pressing.”. What does that even mean? Adding one extra parameter creates infinite many global minima.  -“The task of optimization algorithms has become: Find the set of parameters with the smallest testing error among all the ones with no training error.” Not true, optimization algorithms still find the model that minimizes risk.  - “one observes a fast escape from that global minimum and subsequent convergence to another global minimum.” Unclear if that is what is happening in Fig 1. At 2k iterations both train and test acc stabilize, and before that train acc was *not* at 100%  - “the ultimate global minimum found by SGD generalizes better”  Unclear as the SGD model flactuates around the performance of the GD model and is always within +- 5%.   - I think the experiments are quite interesting, but also limited. For example, it is unclear how well sharpness and non-uniformity would correlate with gen error in CIFAR10. How does the behavior change when using Resnets or DenseNets  Overall, this paper presents very interesting observations that I have not seen in other papers, however the presentation needs quite some more work, as the clarity of many of the sections is lacking in parts.   typos: accessible by an stochastic gradient -> accessible by a stochastic gradient  In other words, how the different optimization algorithms -> In other words, how do the different optimization algorithms  though the GD iterations is reasonably -> though the GD model is reasonably  We then extend this analysis to high dimensional case. -> We then extend this analysis to the high dimensional case   and hence accessible for SGD -> and hence accessible by SGD  The left two diagrams in Figure 3 shows these two domains -> The left two diagrams in Figure 3 show these two domains   Dashes indicate that GD blows up with that learning rate. ->  Dashes indicate that GD diverges with that learning rate. 