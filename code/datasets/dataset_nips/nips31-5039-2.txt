This paper investigates the behavior of the convergence of SGD (with a fixed learning rate \eta and batch size B) to a global minimum in an over-parameterized ERM problem. The paper introduces two quantities called non-uniformity and sharpness (which capture the variance of the Hessian at the global minimum), and argues that the ability of SGD to reach a global minimum depends on these quantities.   The paper focuses mainly on the quadratic case in the over-parameterized regime, and derives necessary conditions on the aforementioned quantities (in terms of the fixed step size \eta and batch size B) for SGD to converge to the global minimum (Sec 2.3).   Except for the toy example, which I find interesting, the main claims in this paper seem to be a direct consequence of the previously known results/analyses concerning the convergence of SGD with fixed learning rate.  There have been several works that studied and analyzed the convergence rate of SGD and derived selection roles for the fixed learning rate in various settings (e.g., [Moulines-Bach, NIPS 2011], [Needell et al., NIPS 2014], [Ma et al., ICML 2018]). The last reference analyzes the learning rate of mini-batch SGD as a function of the batch size and some properties of the Hessian in the over-parametrized regime . In fact, the conditions in Sec 2.3 seem to be related to the analysis for the quadratic loss in that reference. That is, it seems that the characterization of the "non-uniformity" and "sharpness" in terms of the learning rate and batch size in this paper is essentially equivalent to the problem of characterizing the learning rate in terms of the batch size and the properties of the loss, which has been studied in several previous works. So, I don't really see much of a technical novelty in the paper.  That said, I think the paper started with an interesting toy example in an interesting setting (non-convex functions with multiple global minimizers), which seemed to be the "right" setting to study the phenomenon of "SGD favoring flatter minima". However, the discussion suddenly went back to the convex (quadratic) case for the remainder of the paper. It would be interesting if the claim is formally substantiated in a more general class of objective functions.   ------------------------------ ------------------------------  **** Post Rebuttal Comments:   All in all, after reading the authors' rebuttal and the views of other reviewers, I think, despite the technical similarity with previous works, the paper looks at the problem from a different angle and may provide a new perspective on a very important problem. For this, I would increase my initial score by 1 point. I do think nonetheless that the authors should include proper citations to the relevant works mentioned in the review and discuss the connections to/differences from these works so that the context and the message of the paper becomes more clear. 