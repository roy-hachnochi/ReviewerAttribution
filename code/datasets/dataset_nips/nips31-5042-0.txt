The paper presents a technique of making the MPC cost function differentiable to enable end to end learning. The MPC cost is analytically differentiated through convex approximation at a fixed point using the KKT condition. The MPC cost is formed as a box constrained quadratic program and the system is solved using an LQR solver and the derivatives are computed based on the OptNet approach. The advantage of the approach is that the backward pass is only computed once instead of the normal unrolling strategy used in the past literature. The approach is tested for linear dynamical systems and in continuos control domains like pendulum, cartpole and car model setting and is compared with other imitation learning approaches.  The paper applies the method of solving quadratic program using efficient differentiable solver introduced in [1] to the MPC cost function which makes the novelty in the paper very limited. There are various limitations of these type of solvers like high memory complexity of the solver which makes it difficult to scale to networks with large number of parameters and also as discussed in the paper, its only applicable to systems where the controller reaches a fixed point which makes it limited in the applications it can be used.   Minor Correction: The formatting of the references is incorrect.  [1] Brandon Amos and J Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pages 136â€“145, 2017.  Update:  I have changed my score to a borderline accept after going through the author's feedback and other reviewers' comments. I agree with the authors that the work lays the foundation for differentiable control but is missing discussions on effectiveness of the approach in real world settings which the author have agreed to address in the revised version.