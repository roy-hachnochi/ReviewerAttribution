The paper proposes a new kind of adversarial attack to deep neural networks. Instead of perturbing an existing input image, the proposed method trains a generative adversarial network (GAN) and searches for generated image that will be misclassified.  The paper is well written and idea is quite novel to me. I think it poses a new kind of threat for DNNs and opens a new research direction. L_p norm is not the best metric for computer vision, but so far it is the only focus in most existing work, due to its mathematical convenience. This work shows an attack beyond L_p constraint while still keeps the adversarial images legitimate.  However, I have the following concerns and comments:  1. The result against certified defense and adversarial training is somewhat misleading, as they are proposed under a different threat model (i.e. perturbation-based attack). The authors should mention more explicitly that this is not a fair evaluation on these defense methods.  2. The authors use PGD in adversarial training, but I think a more natural way is to train on those generated adversarial examples.  3. The experiments are only on small and simple datasets. As it is hard to generate   bigger and more complex images using GAN, it is unknown whether the proposed method can be used in other tasks (e.g., ImageNet classification).