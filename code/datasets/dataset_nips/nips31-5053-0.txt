— This paper proposes a graph-based network for visual question answering.  — The network first predicts a question-conditioned and image-grounded graph which is then processed by a graph convolutional network to reason about spatial and semantic interactions, followed by a multi-layer perceptron to predict the answer.  Strengths  — Interesting and novel approach.  — Qualitative results look great! The predicted graph connectivity (at least in these few examples) looks quite intuitive and interpretable, even when the model predicts the incorrect answer.  Weaknesses  — Figure 2 caption says “[insert quick recap here]” :)  — The paper emphasizes multiple times that the proposed approach achieves state of the art accuracies on VQA v2, but that does not seem to be the case. The best published result so far — the counting module by Zhang et al., ICLR 2018 — performs ~3% better than the proposed approach (as shown in Table 1 as well). This claim needs to be sufficiently toned down.  Also, the proposed approach is marginally better than the base Bottom-Up architecture. Is an improvement of 0.1% on VQA v2 test-std statistically significant?  — “Performance on the VQA v2 dataset is still rather limited, which could be linked to issues within the dataset itself.” — this is quite a strong claim, and does not seem to be sufficiently backed with evidence.  The 2 reasons given are 1) “several questions require subjective answers”. Agreed, seems plausible, but it would be useful to highlight what % of the error rate is made up of subjective questions, by looking at agreement among humans for example. If that % is significantly high, that could be problematic. 2) “require answers that cannot be found in the current multi-class classification set-up” — this is not an issue with the dataset at all, but the consequence of a particular modeling choice. There is a rich, heavy tail of answers in the real world, much beyond 3k answers most models end up going with.  Evaluation  Jointly predicting a scene graph and using a graph CNN for reasoning for VQA makes a lot of sense. Results don't outperform state-of-the-art and claims related to that should be toned down appropriately, but the approach is sufficiently promising to deserve publication nonetheless. 