This paper proposes to decrease the computational cost of neural architecture search by applying transfer learning. Weight sharing has been previously considered for neural architecture search within the child models, but it seems that this work is the first to apply transfer learning in the controller to this setting. This is apparently a novel idea in the Deep RL based controller setting of neural architecture search although it has been done in other settings, for example, with matrix factorization.   Overall the weakness of this paper to me is that the approach is of limited novelty and is very straightforward from past work on transfer learning and neural architecture search. On the other hand, the use case is compelling and I find the evaluation of the model to be relatively convincing, thorough, and well explained. That being said, it would be nice to also see experimental justification for some of the design choices like task embeddings and task specific advantage normalization.  