In this paper, the author has proposed doubly stochastic like primal-dual method to solve empirical risk minimization with linear predictors. As this method considers only one data point and one direction at a time, hence per-iteration complexity is of the constant order. The authors also propose a variance reduced optimization approach. The paper has been written clearly. The idea of using this kind of update has been around for some time. A minor concern is as following:  I have not seen an application where it is really helpful because the SGD and coordinate descent are both cheap. Hence I would like to see the analysis of this algorithm in distributed environment. Also I am not sure right now that why do the bound in theorem 2 depends on the dimension d which makes this approach useless for all the kernel base algorithms. However coordinate descent or SGD kind of algorithm just run fine in kernel based learning algorithms also. Where does the analysis break ? Am I missing something ? I am keeping my score right now 6 under the assumption that I might have misunderstood something, however depending on the clarification from the authors, I am happy to change my score. 