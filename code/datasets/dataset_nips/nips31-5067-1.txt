# Summary This paper proposes a new learning method for minimizing convex finite sums with strongly convex regularizer and linear predictor. One notable feature of the method is the per-iteration computational cost is O(1) while usual SGD needs O(d). The idea of the method is to take primal-dual reformulation and apply the doubly stochastic primal-dual method to the problem. A variance reduced variant of the method is also proposed by combining SVRG, naturally. For convex and strongly convex problems, this paper derives convergence rates of these two methods. Experimental results show the superior performance of the proposed method over competitors including notable methods such as SGD, SVRG, and SAGA. Especially, the method outperforms the other methods in high-dimensional settings.  # Detailed comments The algorithm derivation and obtained algorithms seem to be reasonable and interesting. A convergence analysis for non-strongly convex problems is also good, which corresponds to the rate of SGD. However, there is much room for improvement of an analysis for strongly convex problems as mentioned in the paper. The paper argues about only the dependency on $T$, but another concern is the dependency on $d$ and $n$. In the case of PSGD, convergence rates are independent of $n$, but the proposed method is affected by both $d$ and $n$. For the SPD1-VR, it would be nice if the authors could investigate large stepsize is acceptable in the method to achieve a faster convergence rate. I think that the following paper is also useful for further develop this work.  Nutini, Schmidt, Laradji, Friedlander, and Koepke. Coordinate Descent Convergence Faster with the Gauss-Southwell Rule than Random Selection. ICML, 2015  # After reading the author's response A new convergence rate is nice as a stochastic method.  I vote for acceptance because my concern has been resolved. 