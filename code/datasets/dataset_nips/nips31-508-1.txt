Paper is concise overall.   It has thrown new light on variational dropout with clear literature survey and step by step delineation of its ideas. This paper uses a novel combination of variational dropouts, KL divergence to do variational inferencing in a two step process. First a crude selection of candidate classes is achieved by automatically dropping out the wrong classes using a neural network. The second step, involving using a standard softmax, is simpler because the network has to attend to only a few classes while classification.  The dropout probabilities are learned in a novel way, by using a graph that combines target, input, 2 different neural networks that learn dropout-layers. In the two networks, one uses the target to prioritize non-dropping of target while the other network tries to mimic the first network via minimizing KL-Divergence. This is a necessary step as testing phase doesn't involve a target.    The experiments are presented, although the setup could've been more detailed (choice of optimizer, number of iterations etc...). The explanation of the results using plots is clear, and so are the explanations in Qualitative Analysis section.  Interesting thing to notice is, the approach getting better results (compared to other standard techniques) on smaller datasets.