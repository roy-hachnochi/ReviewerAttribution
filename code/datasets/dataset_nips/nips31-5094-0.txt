This paper proposes 3 improvements of [1] for training provably robust neural networks. The 1st is an algorithms for automatically constructing a "dual network" for calculating the dual bound. The 2nd uses random projection to reduce computation. The 3rd introduces the cascade trick to further reduce robust error at the price of increased clean test error.  The paper is clearly written. These improvements over [1] is novel and significant, allowing the training of larger provably robust networks with more versatile structures.  In addition, I need some clarifications for Figure 3. The cascade models are trained sequentially? is that correct? say, Cascade 2 is trained after Cascade 1 is completely trained? So the epoch number in Figure 3 (train) doesn't indicate concurrency? For the testing curves, when we look at Cascade 3 epoch 1, the Cascade 3 model just started training? and Cascade 4 and 5 hasn't been trained?    I recommend acceptance for this paper.   [1] Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.