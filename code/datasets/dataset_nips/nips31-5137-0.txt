Summary:  The paper proposes to learn the regularization function in inverse problem. This idea is that the regularization should take large values for non-plausible images and low values (not necessaritly bounded below) for true images. While many variational formulations use hard coded functionals such as TV for images, here it is learnt using an approach inspired by the Wasserstein GAN method.  To learn the regularization the GAN has to discrimitate between measurements backprojected to image space using a simple pseudo-inverse strongly affected by noise, and some ground truth images.  By promoting a regularization function which is "close" to have 1-Lipschitz gradient, results on stability can be obtained and hyperparameter \lambda can be calibrated using the same network that does not depend on the noise level.  The paper is well written.  Qualitative evaluation:  - While the GAN promotes a regularizer with a 1-Lipschitz gradient it is not a hard constraint of the learning problem. However theoretical results make the assumption that the function has a 1-Lipschitz gradient everywhere. This weakens the theoretical claims of the paper, if not makes the theorems innapropriate. This should be discussed.  - One strong claim of the paper is that the method does not need lots of ground truth data.  "Our approach is particularly well-suited for applications in medical imaging, where usually very few training samples are available and ground truth images to a particular measurement are hard to obtain, making supervised algorithms impossible train.""  However the procedure described in Algorithm 1 requires to simulate data using ground truth images. This seems to be a paradox and there is no experimental evidence that shows that the method needs very little ground truth data to perform well.  Misc / Typos:  - Careful with capital letters in the titles of the papers in the bibliography.  - Kontorovich -> Kantorovich