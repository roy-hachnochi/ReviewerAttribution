This is an interesting paper. The main point it makes, which it studies formally, is that Kleinberg’s impossibility result become more meaningful if we explicitly consider the number of clusters. In particular, the most essential and controversial axiom, consistency, should only hold if the underlying “consistent change” does not modify the number of clusters.   It is then shown that, with the new formulation of consistency, Single Linkage satisfies the resulting set of axioms. Next, it is shown that under certain clusterability assumptions, k-means and related methods also satisfy the new set of axioms.   Fundamentally, I really like this new direction, particularly the incorporation of the number of clusters into the axioms. I believe that it is promising and can lead to additional interesting findings.   It is important to note that there have been many successful reformulations of Kleinberg’s axioms where they become consistent for Single Linkage. Perhaps the simplest is when we allow the number of clusters to be part of the input in our definition of a clustering function.   The positive results of k-means require strong assumptions, both on cluster separation and balance. The practical implication of these combined assumptions isn’t clear (alpha>5.3 and cluster balance) - does it leave enough room for any ambiguous/realistic cluster structures?   Nevertheless, I like the results, and elegant incorporation of the number of clusters into the formulation of consistency. This framework may also lead to additional interesting future work.   Additional comments:   - I wonder if the new formulation of consistency allows for any clearly unacceptable functions, which unnecessarily vary the number of clusters in order to avoid having to adhere to the core of the consistency requirement.    - Perhaps it is worth incorporating a figure showing why consistency should consider the number of clusters? I realize that space it tight, but as this is the main message of the paper, it may be worth illustrating it with a figure, even though such figure appeared in previous work.   Page 2, Step 2. Perhaps better written as “Find the best k-clustering” or “Find the best clustering with k clusters”  Lines 114-116, it is not exactly a ‘perturbation’, since the change change in the dissimilarities can be significant.  129-131. There has been a couple of papers by Ackerman et al that aim specifically at understanding the type of properties that outline the advantages of k-means and similar methods, particularly:   Margareta Ackerman, Shai Ben-David, Simina Branzei, and David Loker. Weighted Clustering. Proc. 26th AAAI Conference on Artificial Intelligence, 2012. Margareta Ackerman, Shai Ben-David, Sivan Sabato, and David Loker. Clustering Oligarchies. Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS), 2013.    - Jarrod Moore and Margareta Ackerman. Foundations of Perturbation Robust Clustering. IEEE International Conference on Data Mining (ICDM), 2016.   It is worth noting that the framework considered by Ackerman at al doesn’t necessarily propose k-means and similar methods are better than others, but rather seeks to understand under what conditions the advantages of k-means are relevant. This is most clearly seen in the first paper above, where it is proven that k-means (and similar methods) are sensitive to weight (or, data density). The other two papers show that k-means and related approaches are more robust to noise and outliers than other popular techniques, under clusterability assumptions. I see that some of these papers are already cited in your work, but are not contextualized to highlight that they do formally study the advantages of k-means and related methods.   In that sense, focus on potential advantages of k-means is not a distinguishing element of the current work from the literature by Ackerman at el. What is different, however, is that they attempt to understand differences between algorithms for the purpose of determining which algorithms to use under different conditions, and your focus here is on the discover on consistent axioms of clustering that apply across diverse methods.   Page 4: Perhaps it may be worthwhile to redefine notation to eliminate the repeated double bracketing.   Line 198: Small comment: it’s -> its 