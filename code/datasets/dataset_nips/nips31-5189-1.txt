In this paper the authors introduce a framework for inference in population genetic models with a focus on recombination hotspot identification, composed of two main ingredients: (1) “simulation-on-the-fly” whereby datasets with known parameters are simulated (under the coalescent) with the model trained on each dataset exactly once, and (2) an exchangeable neural network that maps from the simulated data to the posterior distribution over parameter values. The observed data can then be used as input in a similar manner to recognition networks to infer the posterior distribution for the parameters given the observed data. The authors demonstrate that theoretically the posterior is well-calibrated and show that their method outperforms state-of-the-art for recombination hotspot discovery.  The proposed method appears to perform excellently on detecting recombination hotspots and could easily be considered state-of-the-art if a more comprehensive evaluation was undertaken. Of particular note is the ability of the model to perform amortized inference on unseen datasets, potentially meaning it could be distributed as a pre-compiled model. That said - I think the overall claim that the authors have developed a general framework for likelihood-free inference from population genetic data is untrue. In particular, it is not obvious how the method extends to continuous parameters, and is only demonstrated in the context of recombination hotspot testing. Further comments are below.  The notation in the problem setup (section 3.1) is unclear. Does the index (i) introduced on line 101 reference each simulated dataset? If so this should be stated. Further, X is a dataset-by-sample-by-SNP tensor (array) while bold-x is a sample-by-snp matrix, while unbold-x is a vector of SNPs. This notation is quite unstandard (ie bold-x would be a vector, etc), and while this is a very minor point, it makes for confused reading.  On line 114 the authors introduce the exchangeable neural network that learns the posterior distribution over parameters given a simulated dataset. In the example they provide, this is essentially a classification problem of whether a section is a hotspot or not, and thus the representation of the posterior is as a softmax over two values per window. However, this limits the generality of the proposed method - how does it work if the quantity for which I wish to derive a posterior distribution is continuous? Then such a neural net would input a simulated dataset and output a point estimate rather than a distribution. This problem could be circumvented for categorical variables (though with a large increase in the overall number of parameters required if one-hot encoded), but it is not obvious how this extends to continuous variables in this setting (e.g. mutation rates), which conflicts with the authors’ claim in the abstract that implies the method is “general-purpose”.  In section 5.3 the authors compare their simulation on the fly approach (new dataset per epoch) to a fixed training set size. In the fixed approach they state that a training set size of 10000 was used while in the on-the-fly approach 20000 batches were used, which I assume means 20000 data sets? If this is the case then the on-the-fly approach has used twice as much data, which could easily explain the increased test-set performance? I may be misunderstanding the setup here.   Line 233 - \alpha are defined as “physical distances” - are these the widths of the windows?  Line 238 - k probably needs defined as k > 1