The authors consider using neural networks (NN) to learn a mapping from data to parameters for population genetics models. Such models typically have highly intractable likelihoods as a result of their complex latent structure. However, sampling from these models is relatively straightforward. As a result a typical approach has been to use Approximate Bayesian Computation which compares user-specified statistics from the simulation and real data to decide if the sampled parameters are "good". The choice of statistic is clearly critical however and relies on expert knowledge for a specific model. There has been work using NNs to output parameters based on a set of summary statistics. The current paper proposes using NNs where the input is the raw genotype data itself. Since this data is exchangeable (the labeling of the individuals doesn't matter) it is logical to consider exchangeable NNs (ENN) that leverage this structure to learn more efficiently. The authors construct the following ENN: a) apply the same (convolutional) NN to each individual's genotype vector b) apply a symmetric function (specifically element-wise top decile) to collapse across individuals c) apply a final dense NN. The idea of performing "simulation-on-the-fly" is discussed, i.e. generating every new training sample from the generative model so that estimation error is small. Results are presented on recombination hotspot testing, with improvements over the state-of-the-art at comparable or lower computational cost.   The paper is well written and clear. I work in genetics so recombination hotspots are a familiar concept: I wonder if they are sufficiently motivated for a general ML audience? Also the term "calibrated posterior" is used extensively but not defined. I would guess this means an estimate of the posterior whose variance is an unbiased estimate of the variance of the true posterior? (noting that is dependent on the choice of generative model).   The motivation, theory and experiments successfully demonstrate the value of the approach relative to current approaches in the field. The key question to me is how important is the choice of symmetric function? A wide class of symmetric functions could be used, and indeed the function itself could be learnt - although the suggestions are all fixed functions. It would be straightforward to test a few possibilities empirically (maybe the authors did this before picking top decile?): presumably in practice one would want to include this as part of hyperparameter/architechure tuning.   In typical uses of such an approach we will have a dataset we are particularly interested in. In that case we don't care so much about the general accurarcy of the method but instead its accuracy at this one dataset. This suggests to me that there should be potential gains by focusing training on regions of parameter space that produce data "close" to the real data. I suspect this would look something like a supervised VAE. I'd be interested to hear the authors thoughts.   Finally I'm curious about how large a region of genome the authors envisage modeling simultaneously. Recombination hotspot testing is a case where it makes sense to look at small genomic regions. However, if you were trying to look at something like demographic history there might be signal coming from the entire genome. Would you divide up into small regions and somehow aggregate predictions?   Overall I think this is a valuable contribution to the pop gen field and is hopefully of interest to a wider ML audience. There are some questions I would like to see addressed but I realize it is difficult to cover all yours bases in a conference paper.   After the reading the author response I'm uping my score to top 50% of papers. 