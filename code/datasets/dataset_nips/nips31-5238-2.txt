This paper considered the problem of estimating (multivariate) mutual information from the i.i.d. drawn data samples. While this problem has been rather extensively studied in the literature recently, most current methods focus on estimating entropy/differential entropy and then use them to build a plug-in estimator for mutual information. Apparently, this method does not work for general probability spaces with mixed value components. This paper proposed a new angle of viewing  (multivariate) mutual information as divergences between a given distribution and a  graphical model structure. The most important advantage of the proposed angle is that the Radon-Nikodym derivative is guaranteed to exist and thus the divergences are well defined for general probability spaces. The authors then proposed an estimator to estimate such divergences based on the coupling trick used to construct the well-known KSG estimator and proved the consistency of the proposed estimator. The paper is well organized and well written. In my opinion, the technical innovation is sufficiently novel for publication. My main complaint about this work is that just like the KSG estimator, the computational complexity of the proposed estimator is very high, especially for multi-dimensional distributions. This limits the applicability of the estimator in practice.