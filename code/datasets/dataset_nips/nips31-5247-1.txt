In this paper, authors explore the construction of meta-learning algorithms, through defining a viable search space and corresponding proxy tasks that are specifically designed and assessed for the task of dense prediction/semantic segmentation. The proposed method and the resulting network is tested on three different relevant datasets (Cityscapes, PASCAL VOC 12 and PASCAL person part).  Main strengths: + The method improves the state-of-the-art on three datasets including the competitive cityscapes and PASCAL VOC 2012. + Most of the method ingredients (e.g. the proxy tasks) are well-thought-out and explored.  + The paper is fairly well-written and is easy to follow.  Major weaknesses: - Even though the meta-learning procedure is strived to become as cheap as possible, it is still impossible to be tried by the research groups with small to moderate available computational resources: "We deploy the resulting proxy task with our proposed architecture search space on Cityscapes to explore 28K DPC architectures across 370 GPUs over one week." - The selected space search algorithm (random search) might not to be the most optimal one. Further improvements are possible by incorporating search algorithms that can achieve good solutions more efficiently (e.g. by better incorporating prior knowledge about the search process)  Other (minor) remarks: - It would have been useful to get more insights on the objective function landscape. For instance, the distribution of fitness of the explored architectures could have given information about the amount of variations in the objective function, or a diagram of best objective value based on number of architectures explored, could have indicated how far the search needed to continue. - As mentioned in sections 4.4 and 4.5, the same best found DPC on the cityscapes dataset is used for the PASCAL VOC and PASCAL person part datasets and so no dataset-specific meta-learning is performed. However, it is interesting to assess how well the (close-to-)optimal structure in one dataset can generalize to be a (close-to-)optimal structure in the other ones. This assessment becomes relatively important given the intractability of meta-learning for new dataset for majority of researchers.