Update after rebuttal:  This is a strong paper. The rebuttal has addressed several of my concerns. ____________________  Summary:  This paper proposes a new biological implementation of an approximate backpropagation algorithm that relies on multi compartment neuron models. Using two compartments allows errors and activities to be represented within the same neuron. The overall procedure is similar to contrastive Hebbian learning and relies on weak top down feedback from an initial ‘self-predicting’ settled state, but unlike contrastive Hebbian learning does not require separate phases. Experimental results show that the method can attain reasonable results on MNIST.  Major comments:  This paper presents an interesting approach to approximately implementing backpropagation that relies on a mixture of dendritic compartments and specific circuitry motifs. This is a fundamentally important topic and the results would likely be of interest to many, even if the specific hypothesis turns out to be incorrect.  The paper is admirably clear and easy to follow.  The method would appear to suffer from a similar difficulty as contrastive Hebbian learning: it only faithfully reflects backpropagation when top down feedback is weak, but this means that weight updates become very small in a deep network. Correcting this by scaling up the weight updates by the inverse of the top down feedback strength is problematic because this would require high precision and would drastically amplify small noise in the neural activity. Most of the experiments presented in the paper focus on shallow, single hidden layer networks. The MNIST experiment with a deeper two hidden layer network might provide an opportunity to test the robustness of the system by showing training curves with frozen first layer weights. More broadly, recent results have shown that schemes like feedback alignment often work well on simple datasets in relatively shallow networks, but struggle to match the performance of backprop in deeper networks on more complex datasets. The paper could be greatly strengthened by testing whether the proposed method really solves the credit assignment problem in substantially deep networks.  In general, the test error rates reported here are typically in the range of what is achievable by a SVM which does not learn its internal representation (e.g., SVM with Gaussian kernel gets ~1.4%). Because of this it is important to show that the learning performance is critically dependent on learning the internal representation. It would be useful to report learning curves with random but frozen first layers (with optimized variance), in addition to the shallow network performance.  