The authors develop a biologically plausible network model with synaptic plasticity that is able to learn specified tasks in a manner that approximates the backpropagation algorithm. The authors provide mathematical derivations showing the similarity between their biologically-plausible model and backpropagation, and convincing results for the performance of their model under a couple of tasks (nonlinear regression task, and classification of handwritten digits). This work is technically sound and will be a valuable contribution to the conference, in particular for the neuroscience inclined. A few comments:  -the manuscript provides a comparison of the proposed model with a machine learning model (backpropagation in a densely connected network) in the MNIST classification task. For completeness, it would be important to also show the performance of the Guerguiev et al. 2017 model (that the authors cite), another biologically plausible model attempting to implement backpropagation;  -the authors provide evidence for the biological plausibility of the model, by citing key experimental findings in neuroscience in recent years (see section Conclusions). In particular, the authors discuss recent work on connectivity and electrophysiology that is consistent with the implementation details of their model (connectivity between areas, cell types, neural activity signaling prediction error...). It would be interesting if the authors could also comment on how much data and how many trials this model needs to achieve good performance, and, if possible, how biologically plausible that is;  -the authors should comment on the choice of hyperparameters across tasks (learning rates, noise magnitudes, numbers of neurons in the hidden layers): why the particular choices, and whether different parametrisations affect the performance.  -- after rebuttal -- The authors' feedback addressed satisfactorily all my comments, and I therefore maintain my support for the acceptance of this paper.