When estimating statistics, learning models, and performing optimization on the private properties of a population, local-differential-privacy techniques have many advantages---notably, to protect their privacy, each respondent need not trust the data collector to keep their answers secret.  For this type of distributed learning to be practical, it is important to minimize the sample complexity and rounds of interaction.  This theory paper provides new methods and analysis for non-interactive distributed learning, establishing results that improve on the bounds from [19] with higher efficiency and wider applicability.  However, for this a number of assumptions are required, in particular on smoothness of the loss.  Overall score: This paper clearly makes advances in the theory of non-interactive local differential privacy, directly motivated by [19] and improving on their results.  The paper provides new algorithms and sample-complexity bounds for smooth loss functions, shows how efficiency may be increased, as well as describing methods applicable in some high-dimensional cases. In addition, the paper is well written and structured. As such, its contributions clearly deserve publication. This said, the results are limited by assumptions about smoothness, and the impact of those limitations isn't clear, at least to this reviewer.  Also, the paper contains no (empirical or analytic) evaluation of datasets, methods, or concrete applications that might shed light on the impact of the results.  Confidence score: While this reviewer is familiar with but not expert in much of the related work, including [19].  The lack of empirical validation of the assumptions and application of the results make it hard to assess impact.   Update: I thank the authors for their response the the reviews. I'm leaving my score as is, but I'll note that the paper would be much stronger if it more clearly offered substantial new insights or paths to practical benefits. 