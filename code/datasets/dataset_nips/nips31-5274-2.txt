In this paper, the authors studied uniform convergence of gradients based on vector-valued Rademacher complexities and KL conditions of popular risks. With applications to non-convex generalized linear models and non-convex robust linear regression, the authors showed the effectiveness of this uniform convergence to study generalization bounds of any model satisfying approximate stationary conditions.  (1) It seems that the gradient uniform convergence in Section 2 follows from standard applications of existing results in the literature. Since Section 2 provides the foundation of the subsequent analysis, I am not sure whether the contribution is enough.  (2) For generalized linear models, the authors only considered the specific least squares loss. Is it possible to extend the discussions to more general loss functions?  (3) Assumption 1 seems to be a bit restrict. It would be helpful to give some examples of $\sigma$ satisfying (a) and (b). Furthermore, the part (c) requires the conditional expectation to be realized by some $w^*\in\mathcal{W}$, which is a bounded set encoding linear functions. In this case, the learning problems in Section 3 must be relatively easy. This comment also applies to Assumption 2.  (4) In Theorem 8, the supremum is taken over a random set $\mathcal{W}(\phi,\hat{\mathcal{D}})$, which seems not natural. In my opinion, it would be interesting to consider the supremum over a non-random set.   Minor comments: (1) $\beta$-smooth norm is used in Theorem 3 but the definition is given in the appendix.  (2) for any $\delta>0$ should be for any $0<\delta<1$  (3) There are repeated references in [20,21]