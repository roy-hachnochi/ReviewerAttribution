This work extends the work in Battaglia et al., of learning the dynamics of physical systems using leanred message passing, to large systems. It is an important and interesting work and mostly well written, although some key points need clarification.  Detailed remarks: - In line 52 you mention how a full graph scales badly (and in other locations in the paper), but there is the option of a sparse graph, e.g. only KNN edges. It would be important to compare and contrast this alternative.  - Your clustering can introduce boundary effects (which might be the reason behind your sub-par perserve distance MSE), it should be discussed and an empirical evaluation (e.g. how much of the per-dist MSE is between clusters vs in clusters) is in place. - The high level part of the hierarchical effect propagation isn’t that clear. The way I understand it is that you compute e^0 on leaves, then send e^L2A up the hierarchy till the root, then compute e^WS at each level and finally compute w^A2D down the hierarchy to the leaves. If this is true it would be helpful to say that in the paper, if not then it is not clear enough and needs clarification. - If it wasn’t for the consecutive message passing, it would be equivalent to a graph neural network with 3 edge types (L2A,A2S and WS), would be useful to compare.  Minor remark: - Line 94 “,excluding e.g. …” sounds bad. “Excluding, for example, …” is a better option.   Update: Thank you for your response. Good point about kNN needing multiple rounds (at least the naive way).