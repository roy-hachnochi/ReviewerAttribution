The authors proposed a new approach to policy evaluation and learning in causal inference. Traditional approached to this problem have used either regression-based techniques, propensity score weighting (i.e., the probability of receiving a particular treatment at a fixed level of the covariates) or so-called doubly robust methods (that use both weighting and regression). The paper proposes instead of using weights inversely proportional to the propensity score to use balance weights which are computed directly by optimizing for balance between the weighted data and the target policy. The authors then derive some theoretical guarantees based on their proposed methodology. The key insight of this paper is to recognize that the weighting can be computed directly rather than relying on the propensity score.   I think this research is closely connected with the work done by Jose Zubizarreta (citation given at the end of this). Although the application is different, the key insight of using weights that are directly optimized is similar to  Zubizarreta's work. It would be good if these connections were discussed further, in particular, what are the similarities and differences (besides the application)?  The technical work is excellent, although at times a little challenging to follow. This is mostly due to the amount of information that is packed in the paper. Using simpler and clearer examples could help to alleviate some of this. For example, Example 1 is not a conventional example that I am familiar with, I wonder if using something closer to a real-world application would help this? I would also like to see some more discussion regarding the different assumptions that are made. For example, what happens if strong overlap does not hold?   Zubizarreta, J. R. (2015), "Stable Weights that Balance Covariates for Estimation with Incomplete Outcome Data,"