This paper introduces a new algorithm for solving multitask RL problems. The claim of the paper is that by introducing a new solver, the authors were able to derive a quadratically convergent algorithm.  The contribution of the paper is that it introduces a solver for optimizing the 'shared parameters'.  There seems to be a extensive background literature that is related to the current paper that I am not familiar with at all. The authors, however, makes extensive references to these previous approaches.  I also found the premise of the paper very narrow. I don't think the proposed approach is directly applicable to complicated domains. (For example, I don't think the proposed approach can help solving problems on Atari games where one has to solve multiple games at once.)  I could be greatly misunderstanding the paper and therefore I would like to ask the following questions.  Questions:  1. How is the policy parameterized? Is it a Gaussian policy? 2. Does the algorithm have access to the state transition probabilities during the optimization process? 3. Does the algorithm have access to all states of the MDP or does it have to conduct exploration? 4. What is jump start versus asymptotic performance in the experiment section? 5. Why do we care about quadratic convergence at all when it comes to multi-task RL? 6. What are some of the applications of this multi-task RL setup?