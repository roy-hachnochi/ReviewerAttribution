This paper proposes a practical extension to neural program synthesis by leveraging program execution traces in addition to the I/O pairs associated with example Karel program. The authors reason that by providing explicit execution traces from Karel interpreter, learning algorithm has less to internally reason about specific mapping from input to output.  Furthermore it simplifies the learning problem given that the  execution trace does not contain any control flow constructs.   The proposed deep learning model has a two-stage approach (I/O -> Trace and Trace-> Code). This approach is shown to perform better than a recent one-stage I/O-> Code baseline (Bunel 2018) on different types of programs (specifically longer and complex programs).    The paper will be of practical interest to programming community. It is well-written, and experiment analysis is adequate. The novelty is limited in my opinion as execution trace is a standard programming construct and it is not surprising that addition of such feature improves overall performance.  