The authors propose to break down the neural program synthesis pipeline into two steps: a model that maps IO examples to corresponding execution traces, and a model that given a set of traces generates a corresponding program.  The model is tested on the dataset of [Bunel 2018] and slices thereof and compares favorably to the baseline.   Pros: - The paper is well written (except for the model description, which is not really formal enough. thankfully code will supply what the paper is lacking.) - The related work is satisfactory - The idea is catchy and charmingly simple; the fact that it frees the model from having to keep track of the internal state of the interpreter is appealing - The comparison to the baseline is favorable  I think that this is a fairly good empirical paper, worthy of acceptance.   Minor issues:  - It is always unfortunate when there is only one baseline, but I do agree that comparing to methods developed for different settings or languages (e.g. the end-to-end differentiable prover of Riedel et al.) may be difficult in neural program synthesis  - In the related work section most articles are missing