This paper argues for pre-training not just embeddings but also a task-independent graph structure, inspired by (but different to) self-attention.  It's a neat idea and I believe we need to pre-train more of our models in NLP so it's the right direction for research.  Paper is well written and the technical description is clear.  Some nit-picky comments:  After reading the abstract and intro I am still a little confused about what the paper is trying to do.   Work from last year started to pre-train/learn more than single word vectors. In particular, at last year's NIPS, CoVe (Context Vector by McCann et al) was introduced which pre-trained an LSTM with translation. This was then later extended with ELMO at EMNLP this year.   2.1.1 Desiderata This should probably be a \subsection instead