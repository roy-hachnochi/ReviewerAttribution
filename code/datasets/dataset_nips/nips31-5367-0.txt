The submission studies online learning for the task of predicting the outcomes of binary measurements on copies of an n-qubit system. Specifically, the authors provide positive results based on 3 approaches: (1) FTRL algorithm; (2) Quantum Postselection; and (3) (Sequential) fat shattering dimension.  (1) follows more or less the standard approach for online learning, but some mathematical ninja moves are required to deal with convexity for complex numbers. (2) gives the weakest bounds, and is presented in the least NIPS-reader-friendly way. While it is certainly a nice addition to the paper, I expect it will generate the least interest for even the quantum-curious NIPS audience. (3) seems to follow almost immediately from combining Theorems 8 and 10 from previous work. The downside is that this is non-constructive (similar to VC-type guarantees if I’m not mistaken).    While the introduction does a nice job of motivating online vs offline quantum tomography, it doesn’t explain why learning quantum states is interesting to begin with. (E.g. where do all these copies of an n-qubit system come from?)  PRO: It’s probably good that someone wrote this paper, and verified that FTRL indeed works in the quantum/complex setting. It will probably be a refreshing addition to the NIPS program. It’s also a good opportunity to celebrate with the community another cool application of learning theory. CON: From a learning perspective, I find the results a bit boring. Given that the problem has low dimension (3), and is convex, it is not a big surprise that FTRL works. In this sense I can imagine NIPS audience putting all the effort understanding the quantum part, and then learn very little from the learning side.  QUESTIONS TO AUTHORS: 1. I didn’t quite understand what is the obstacle for achieving tight bounds for L_2 loss. Perhaps that would be more interesting than the bounds that do work. 2. The last paragraph of the submission asks whether any algorithm that produces hypothesis states that are approximately consistent with all previous data would work. I’m probably wrong, but naively that would seem to contradict the impossibility results in the first paragraph of the submission. Namely, the latter imply that after a subexp number of measurements there still exists a measurement that would separate two states consistent with everything seen so far, no? I.e. your algorithms answer according to the state that is "more likely", but answering the other way is still consistent (yet likely to be wrong). ********************************************************************* [POST REBUTTAL UPDATE]: Here is my shallow (and embarrassingly classical) understanding of your problem.  1. First, for simplicity, let's assume the space of all possible quantum state is approximated by a discrete set (this is consistent with the approximation, Lipschitz loss function, etc.). Suppose that the hidden quantum state is chosen uniformly at random from this set. 2. Now, if you could come up with a sequence of measurements that recursively agree with half of the set, then until you learned the quantum state, your algorithm cannot do better than guessing at random.  3. Therefore, you would have regret = \Theta(# of samples to learn).  Since this contradicts your algorithmic result, I'm guessing that #2 is false: after a while, any measurement that you could come up with is very lopsided; i.e. one outcome agrees with almost all remaining consistent quantum states. Your algorithms predict this more likely outcome, but an algorithm that would predict the opposite outcome is bound to preform poorly.   What am I missing?  *********************************************************************  MINOR Page 2: “which measurements are feasible *is* dictated by Nature” Section 2 vs 3: the use of \ell vs f for the loss is a bit confusing. Section 5: It would be better to define sfat dimension before explaining how to bound it using Theorem 7. 