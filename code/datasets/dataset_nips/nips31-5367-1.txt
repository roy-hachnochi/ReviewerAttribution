Recently, in a breakthrough paper it has been shown by Aaronson that using ideas from computational learning theory, one can learn a n qubit quantum state using a number of measurements that grows only linearly with the number of qubits-- an exponential improvement. The assumption there was i.i.d sampling for training and testing. The current paper extends this result to adversarial/online learning settings in which i.i.d assumption is not longer safe to make.  The authors generalize the RFTL and analyze its regret bound using both the standard analysis of FTRL and analysis of Matrix Multiplicative Weights method for online learning, with some tweaks required to make it work  for complex matrices and approximating the Bregman divergence.  The authors also prove regret bounds using the notion of sequential fat-shattering dimension.  I think the paper studies and interesting question and provides novel theoretical results. The presentation of the paper was mostly clear. The claimed contributions are discussed in the light of existing results and the paper does survey related work appropriately. The paper is technically sound and the derivations seem to be correct as far as I checked (I carefully checked the regret analysis and skimmed over the sequential fat-shattering part though).    