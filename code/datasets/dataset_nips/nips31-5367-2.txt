The paper gives new results in the context of online learning quantum states. Namely, there is an algorithm that can produce an estimate $\omega_t$ of  n-qubit quantum state $\rho$ so that when performing any two-outcome measurement $E_t$ on the estimate, the error occurs for at most $O(n/\epsilon^2$,  where $\epsilon$ is the error parameter. The paper also gives the error bound for the regret minimization model that achieves regret at most $O(\sqrt{Tn})$.  What I found more interesting from the results are the implications of the results: they improve the shadow tomography result of Aaronson 2018, and the implication of  any online learning for bounded sequential fat-shattering dimension will automatically give an online learning quantum state, as well as the extention of Nayakâ€™s bound.   The weakness of the paper is perhaps the algorithm is the standard regularized follow-the-leader, and the techniques of updating by sub-derivative and matrix multiplicative  weights are not new. However, I think the results are quite interesting despite of the simplicity.   Some minor comments: (*) I think Theorem 3 and 4 should just be Lemmas as they are not really main results. Also, I think they can be merged into one statement. 