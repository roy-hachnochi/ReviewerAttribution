Summary:  This paper considers the problem of learning mixed quantum states, in the sense of being able to predict the probability that a two-outcome measurement E would accept on the unknown quantum state rho. Prior work has considered this problem in the statistical setting, where it is assumed that there is a distribution D over two-outcome measurements, we have sample access to the distribution D, and our goal is to estimate a mixed quantum state so that with high probability for a measurement E drawn from D, the acceptance probability of the estimated state is close to the acceptance probability of the unknown state. This work considers the same problem in the online setting, where it is no longer assumed that measurements are drawn iid from some unknown distribution D. Instead, an arbitrary sequence of measurements is chosen by a (possibly adaptive) adversary, and the goal is design learning algorithms with either mistake bounds (i.e., bounds on the number of times the predicted acceptance probability is wrong my more than epsilon), or regret bounds (i.e., a bound on the cumulative loss of the predicted acceptance probabilities). The online setting is especially interesting given that the sequence of measurements being learned from is usually constrained by nature and chosen by an experimentalist (rather than being iid).   The paper has two main results:  The first result shows that (in the realizable setting) there exists an algorithm that receives a sequence of two-outcome measurements E_1, E_2, ... one at a time and for each outputs an estimated state omega_t such that the acceptance probability of E_t on omega_t is within epsilon of the acceptance probability of E_t on the unknown hidden state rho for all but O(n/epsilon^2) rounds. This feedback given to the algorithm comes in the form of an estimate b_t of the true acceptance probability of E_t on rho, which should be accurate to within error epsilon/3.  The second result applies in the non-realizable setting, where we do not assume that the sequence of losses truly arise as the acceptance probabilities of some two-outcome measurements applied to some fixed mixed quantum state. Instead, we suppose that for each measurement E_t there is an L-Lipschitz convex loss function ell_t mapping acceptance probabilities to losses. The theorem guarantees that there exists an algorithm with regret bounded by O(L sqrt(Tn)) after T measurements, where n is the number of qubits.   Section 3 shows that both of the main results can be achieved using variants of the regularized follow the leader algorithm or the matrix multiplicative weights algorithm. The regret bounds for these algorithms follow the standard proofs, however some modifications are necessary to deal with the fact that the input domain is complex. The mistake bound is a corollary of the regret bounds.  Sections 4 and 5 provide alternative proofs for the two main results using different techniques. In particular, Section 4 provides an alternative mistake bound of O(n/epsilon^3 log(n/epsilon)) using repeated application of a post-selection operator (and this algorithm appears to be significantly different from the FTRL and multiplicative weights algorithms). In section 5, the authors show that a learning algorithm with regret O(L sqrt(nT) log^{3/2}(T)) exists by bounding the sequential fat shattering dimension of the prediction problem.  Comments and Questions:  I am relatively unfamiliar with quantum computing and the related literature, but the problem in this paper seems to be well motivated, especially the shift to the online setting from prior work using statistical assumptions on the source of measurements. The main results are clearly stated and convincing, especially given that the authors present several different techniques all arriving at similar bounds. For the broader NIPS community, I think it is interesting to see these methods applied to problems somewhat distant from more typical prediction problems. It might be helpful to include slightly more background information about the underlying problem and what the benefits of having online prediction algorithms might be.  In the abstract it is mentioned that we measure multiple copies of the unknown state using the same two-outcome measurement E_1. I assume that multiple measurements are used to estimate the acceptance probabilities, since each measurement leads only to the binary accept or reject outcome. In the rest of the paper we suppose we use each measurement only once, but as feedback we get an estimate of the acceptance probability. It might be worth mentioning this again in the main body somewhere.