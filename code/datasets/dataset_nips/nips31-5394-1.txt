This work presents a variant of recurrent neural networks, including its gated version. The key idea is a so-called "peephole" connection which mixes the previous hidden unit with the network hidden output. The proposed method is well supported by theoretical findings and good empirical evidences. The resulting networks are very compact and can be used in embedding products.  Several things need to be clarified:  1) It is unclear how large is the learned beta. Figure 4 in appendex only gives the alpha results. How did you set beta? Was it close to 1-alpha? 2) In the non-learning setting, alpha>0 and 0<beta=1-alpha<1. Applying Eq.2 along a long chain, the mixing can bring bias because early hidden units are involved many times. Please elaborate how to avoid or correct this. 3) By Line 134, beta is close to one. Eq.2 is very close to residual networks. Please elaborate their connections and differences.  The names FastRNN and FastGRNN are not good. It reads like occupying intent and therefore must be discouraged. These names are by no means pertinent because they provide no information about key idea of the method.  