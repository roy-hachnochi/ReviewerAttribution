This paper presents an end-to-end audio synthesizer, which generates output at the sample level, while the loss is computed at a frame-level. This alleviates the main problem in neural audio synthesis, which is the exorbitant cost of optimizing sample-based methods. By performing sample generation and model optimization at different rates (16000 vs 100 per second), the proposed approach avoids the main bottleneck, and seems to achieve good results.   The paper describes a reasonable set of experiments, although I feel the authors could have provided more analysis of the convergence behavior (is the non-convergence without pre-training a fundamental issue when separating prediction and optimization steps, or is this just a fluke in their particular setup?), and a comparison with Griffin-Lim based methods. The authors write that with Griffin-Lim mapping, artifacts are "likely" to appear, but still these methods work exceedingly well, so clearly there's something there. Still, it is good to have an alternative method like the proposed one.  The paper could use some proofreading by a native speaker, although the quality of the writing and presentation is ok in general.  