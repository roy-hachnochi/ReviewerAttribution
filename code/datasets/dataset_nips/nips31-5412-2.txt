SUMMARY.  The task addressed in this paper is a computationally feasible way to synthesize audio, where the focus is to generalize by interpolating in pitch and velocity space for given instruments. This is a narrower aim than, e.g. the NSynth wavenet-based autoencoder, but it is 85 times faster for training and 2500 times faster for inference. Essential components of the system include a combined convolutional auto-encoder, a 3-layer RNN with LSTM cells, and a loss function that operates in log-spectrogram space. The results are approximately comparable to the wavenet-based autoencoder, although the problem being solved is a different one.  QUALITY.   Overall, the steps and choices described in the paper all seem quite reasonable, given the problem the authors have set out to address.  My disappointment occurred when I listened to the results after having read Section 5.3, which had apparently set my expectations slightly too high. In particular: (1) I found the SING samples to generally be slightly more distorted than those produced by the NSYnth system, and  (2) I also found the SING samples to generally be further from the ground truth sample than those produced by the NSynth system.   I am well aware that these kinds of samples are very hard to generate, and in particular to do so efficiently, so the fact that they are in the ballpark is itself a success. I am just rather surprised at-- and don't quite understand-- how it is that they were rated more highly--if only slightly--than the NSynth-generated samples. They are not bad at all; they sound to me like they have been passed through a slight ring modulator. It should be further noted that the NSynth samples were ones that the (NSynth) system had seen during training, whereas they had not been seen by the current (SING) system.  CLARITY.  The paper is written clearly and relatively easy to read. Some typos and a few minor stylistic issues need to be checked.  ORIGINALITY.  The paper has a number of nice ideas, all revolving around towards making their more efficient synthesizer work, ranging from the basic idea of using an RNN+decoder to generate frames of 1024 at a time, to using a loss function that involves the L1 distance between the STFT of the source and predicted waveform. Furthermore, there are clearly lots of details (described in the paper) required to make this whole system work.  SIGNIFICANCE.  I hesitate on this. It is quite possible that some of the ideas here will generalize or otherwise be applicable to other contexts, but I am not entirely sure how directly they might translate or extend. If the audio results were excellent, or if the experimental section were impressively thorough and systematic, then this question might not arise for me in the same way, but the slightly distorted audio results, make it, altogether, slightly less than convincing. On the other hand, the speedup is significant, by orders of magnitude! All this leads to some uncertainty for me in my assessment, and thus I lowered my confidence from a 4 to a 3.