This paper proposes an approach to tackle the problem of low-shot learning with a feature augmentation method using GANs. It builds upon previous feature augmentation approaches by using a generative model of the latent feature distribution for novel classes and selectively borrows from the known bases classes using weights based on inter-class distances in the learned metric space. The proposed GAN incorporates cycle-consistency and covariance matching to encourage the features generated for the new classes to generalize those of the base classes in a useful way. Experiments on low-shot ImageNet demonstrate a noticeable increase in accuracy relative to previous methods.  Pros  - Approach is novel and incorporates ideas from generative modeling into feature augmentation in an interesting way.  - Results clearly show relative contribution of each proposed component.  Cons  - Clarity is low in several places (most prominently in Section 3).  - Comparison to many recent few-shot learning methods on e.g. miniImageNet is lacking.  The clarity of the paper is OK but there are some sections should be made more precise. Section 3 in particular has undefined expressions and refers to the supplementary material too much to be self-contained. It was unclear to me whether the generator takes as input a noise vector. The notation G_n(y_n; x_b, y_b) suggests that there is no noise as input, but the discussion about a mixture of Gaussian noise distribution in lines 160-163 suggests that there is. Also in Section 4.1, the four disjoint shattered sets discussed in lines 225-227 is not entirely clear. I would recommend explaining this in greater detail.  The equations seem to be correct to me, but I did not attempt to verify the subgradient of the Ky Fan m-norm in the supplementary materials.  Overall, the paper explores several intuitively motivated concepts for improving low-shot learning and demonstrates good performance gains on the standard benchmark for this task.  Minor issues:  - Line 1: grammar, attacks the low-shot learning problem  - Line 2: grammar, in the low data regime  - Line 77: consider changing incur to another word e.g. impart  - Line 78: grammar, into their novel counterparts  - Line 79: typo, computational  - Line 83-84: grammar, from a handful of observationss  - Line 86: grammar, Bayesian methods  - Line 88: typo, Literature  - Line 93: consider changing incurring to another word e.g. applying  - Line 98-99: grammar, rather than certain  - Line 115: grammar, requires a meaningful  - Line 123: grammar, the current batch  - Line 130-131: grammar, For more details  - Line 149: typo, proven  - Line 151: grammar, that further constrains  - Line 152-153: grammar, the translation cycle should be able to recover the original embedding  - Line 213-214: grammar, after training the whole network  - Line 218: typo, decays  - Line 225: grammar, The original benchmark  ===== After Author Feedback =====  I thank the authors for providing feedback -- I maintain my original recommendation.