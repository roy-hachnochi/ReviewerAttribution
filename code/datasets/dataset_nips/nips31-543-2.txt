This paper considers the problem of few-shot learning by exploring ways in which data can be generated in order to augment small datasets so that the model can generalize better. It considers feature augmentation, avoiding the unnecessary complexity involved with generating raw data (in this case, images). The setup is the following: we have a base set of classes (consisting of a lot of examples) and a novel set of classes (consisting of a small number of examples) and the goal is to learn to generate data for a novel class based on data from the base classes.   Though there has been previous work on feature augmentation in this kind of setup (Hariharan 2017; Wang 2018), the paper’s novelty is that it does not use all base classes equally to generate new data for a novel class but attempts to use only base classes whose semantics and statistics match the novel class. The paper’s main contribution is to formulate a GAN objective that facilitates such feature augmentation. In addition to the normal conditional GAN objective (which is supplemented by a relation mapping which only considers closest base classes to a novel class), the following extensions are added to improve feature generation (both of which help provide similar intra-class variance between related base and novel classes): Cycle-consistency objective  Covariance-preserving objective  Experiments are conducted on ImageNet low-shot learning benchmark used in previous work, and performance improvement is shown over previous feature generation work in a variety of setups. Ablation studies are done to show that each addition to the GAN objective benefits performance.  Pros Figures were very useful in understanding how method is working. Figure 3 especially helps determine the benefit of each part in the objective. The experiments are thorough and the ablation studies also demonstrate the utility of each suggested addition to the GAN objective.  Cons Method seems to have a lot of different parts and associated hyperparameters (with respect to weighting GAN objectives and within GAN training of each objective) which may make results hard to recreate.  Comments  Typo @ line 144: “into exsted observations” => “into existing observations” It seemed to me that some experimental details were missing (specifically, how model is trained in the low-shot learning framework for ImageNet) and I had to refer to Hariharan 2017 for those details. Might be nice to include a short description of this procedure to make the paper more self-contained. At first I found Figure 3 confusing as to why generated data was being visualized for the base classes also. I believe this is to show the benefit of cycle-consistency objective but might be good to make a note as to why generated data for base classes is also shown when we are only interested in generated data for novel classes.  Hariharan, Bharath et al. Low-shot Visual Recognition by Shrinking and Hallucinating Features. 2017. Wang, Yu-Xiong et al. Low-Shot Learning from Imaginary Data. 2018.