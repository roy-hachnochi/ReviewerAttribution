MetaReg in a nutshell: This paper reinterprets and further develops few-shot meta-learning ideas for the  challenging domain generalization paradigm, using standard supervised benchmarks. The main contribution is the learning of a regularizer, as opposed to learning an initial set of parameters well “positioned” for finetuning. Scores seem to be significantly improved in several cases, but I am not an expert.   Pros: - The paper goes beyond the original inspiration and adapts the approach to serve a substantially different problem. While in meta-learning the degree of similarity between problem instances is substantial, e.g. random subset of classes from ImageNet, dealing well with a multi-modal meta-training set, due to differences between source domains is interesting in itself. - Several practical issues are resolved such that the approach is less expensive and more accurate than baselines. - Several baselines are considered, but I cannot say for sure how strong they are since I am less familiar with the state-of-the-art. - Analysis shows graceful degradation with reduced amounts of data, which is particularly impressive.  Cons: - A substantial bit of the approach could be viewed as derivative if not enough explanation is given for why the miniImageNet task (for example) is not a good paradigm for testing “domain generalization” as precisely defined in the paper, at the conceptual level. - Furthermore, even if the paradigms are clearly different conceptually, say by definition, it would be good to discuss at length if these conceptual differences are really well embodied by the benchmarks used in the two cases. If the answer is negative, then proposing improved benchmarks in both cases would be useful. - Is learning a regularizer better than MAML because the assumptions of MAML are broken by this different paradigm and the benchmark is sensitive enough to show this? Or simply applying MAML with the same tricks and care would yield similar results? For example MAML is model-agnostic, so it could very well be applied only to task-specific networks. Furthermore, MAML but with meta-optimized per-parameter learning rates (Meta-SGD) could also be argued to be (implicitly) learning to regularize.   Explanation of overall score: Quality: Well balanced paper with substantial experiments and some detailed analysis. Clarity: Haven’t had any difficulties. Originality: Ultimately I find it difficult to judge. Significance: Good scores on supervised benchmarks in a challenging paradigm are fine for research in that sub-field. It is not trivial to relate these results to few-shot meta-learning tasks. 