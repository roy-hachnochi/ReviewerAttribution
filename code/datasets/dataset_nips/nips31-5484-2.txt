This paper considers a multi-prototype model for binary classification problem, aiming to find a compact and interpretable model. The problem can then be relaxed and transformed into a smooth convex-concave saddle-point problem, which can be solved by Mirror Prox (a variant of Mirror Descent) algorithm generalized for saddle points. For experiments, this approach outperforms state-of-the-art baselines in various datasets.  Strengths: The formulation of convex-concave saddle-point problem is interesting. Extensive experiments are conducted.  Weaknesses: 1. It is confusing to me what the exact goal of this paper is. Are we claiming the multi-prototype model is superior to other binary classification models (such as linear SVM, kNN, etc.) in terms of interpretability? Why do we have two sets of baselines for higher-dimensional and lower-dimensional data?   2. In Figure 3, for the baselines on the left hand side, what if we sparsify the trained models to reduce the number of selected features and compare accuracy to the proposed model?  3. Since the parameter for sparsity constraint has to be manually picked, can the authors provide any experimental results on the sensitivity of this parameter?  Similar issue arises when picking the number of prototypes.  Update after Author's Feedback: All my concerns are addressed by the authors's additional results. I'm changing my score based on that. 