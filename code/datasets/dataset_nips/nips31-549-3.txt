Summary:  In zero-shot learning for object recognition the system learns to recognize object types for which it saw no labeled images.  It does this by learning projections between (or co-embedding) class label embeddings and image embeddings.  This paper presents two relatively straightforward but experimentally effective innovations: (1) rather than learning a projection in one direction only (e.g. image embedding to class embedding), learn to project in both directions, somewhat like auto-encoders.  (2) Induce a one-level class hierarchy by clustering the classes, and leverage the hierarchy as part of training.  The authors also do some interesting optimization due to some transductive (min-min) optimization on the unlabeled data.  The paper is clear and well written.  There is extensive citation of past work (although it is possible that there are more relevant missing additional citations of which I unaware).    The paper brings together some rather straightforward ingredients, in my opinion.  Of course simultaneously learning projections in two directions, ala auto-encoders, is not new.  It would be interesting to read commentary from the authors about other problem settings which have traditionally used one-direction-only projection learning but which the authors believe bi-directional would also be helpful.  There are many, many examples of using a hierarchy of classes to improve classification.  So again, this idea isn’t a big leap, but it is nice to see it working well again here.  Simple isn’t a bad thing, especially when the method provides good experimental results, and this in my view is the most attractive strength of this paper.  Across-the-board empirical improvements due to their method.  So I’m most interested in understanding why it works better, and in “what data/problems it would be expected to help a lot or not at all”.  I was happy to see the ablation studies.  But I would have like to see much more error analysis, and analysis answering the question in quotes above.  Could you construct a data  set (via subset, or any other method) on which your method would not work well, and analyze/explain why it did not improve results?  You tout your work on the min-min optimization.  But I wondered why you learn with min-min rather than min-softmax, which might beneficially spread the gradient signal across the data.  This sort of “soft assignment” is common in work on EM for semi-supervised learning in the 1990s by Kamal Nigam.  You talk about using a hierarchy to improve classification, but your hierarchy seems pretty weak: just a flat clustering.  Did you consider a hierarchical clustering?  Even a simple method, such as hierarchical agglomerative clustering would produce a deep hierarchy.  Small notes: Line 104:  The first sentence in this line didn’t seem quite grammatical to me. Line 105: You are using citation references as grammatical entities in the sentence; better to use author names, and keep citation refs out of the English grammar. Line 147: I’m surprised that linear formulations outperform more complex non-linear ones.  If you have space to say more here, that would be helpful. 