Summary: SVGD iteratively moves a set of particles toward the target by choosing a perturbative direction to maximumly decrease the KL divergence with the target distribution in RKHS. The paper proposes to add second-order information into SVGD updates, preliminary empirical results show that their method converges faster in few cases. The paper is well written, and the proofs seem correct.   An important reason in using second-order information is the hope to achieve a faster convergence rate. My major concern is a lack of theoretical analysis of convergence rate in this paper:  1) An appealing property of SVGD is that the optimal decreasing rate equals to Stein discrepancy D_F(q||p), where F is a function set that includes all possible velocity fields. D(q||p) = 0 iff q = p. Taking F as RKHS, a closed form solution of the perturbation direction is derived in [Liu et al., 2016, Chwialkowski et al., 2016]. However, the authors didn’t draw any connections between their updates and Stein discrepancy. It’s not clear at all why SVN would yield a faster convergence rate than the original SVGD. And if so, in what sense that SVN is faster? Would the updates of SVN equal to a sum of Stein discrepancy and some other positive terms?   - Liu et al., 2016, A kernelized Stein discrepancy for goodness-of-fit tests  - Chwialkowski et al., 2016. A Kernel Test of Goodness of Fit  2) It would also help l if the authors could discuss the convergence properties of their approach in terms of `the number of particles`.  e.g. Liu et al., 2017, Stein Variational Gradient Descent as Gradient Flow.  3) Calculating the hessian-matrix is computationally expensive. For the purpose of efficiency, the authors used few wild approximations in their update, e.g. (line 127-138)  the cross term in H_l is dropped, and the second-order term is replaced by its Gauss-Newton approximation, etc. These simplifications could reduce the computational cost, but it’s not clear how they would affect the convergence analysis.    4) In the current examples, comparisons were only done with low dimensions and toy examples. Probably the results would be more convincing if the authors could test with high dimensions, e.g. the Bayesian neural network example.   