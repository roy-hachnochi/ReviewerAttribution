Thank you for an interesting read.  This paper proposed a newton-like method in function space for approximate Bayesian inference. Specifically, the authors define functional newton direction, and propose Monte Carlo approximation to it. Furthermore the authors also proposes an improvement of the kernel by using the approximately computed second-order information. Experimental results show significant improvement over naive SVGD.  I think this paper can be a good contribution to the approximate inference community, however the presentation can be improved. Specifically, the "inexact newton" paragraph needs substantial explanation: 1. Why it is safe to drop the \nabla_x k \nabla_x log p term? 2. I don't understand the justification of approximation (18) to (16). I would also like to see at least a numerical analysis on the error of this inexact Newton approximation. You can construct synthetic densities that you can compute the exact hessian of log \pi(x).  Also the experiments, although with significantly better results, seems a bit toy. The original SVGD paper tested the algorithm on Bayesian neural network regression, and the dimensionality there is at least ~300 I believe.   ================================================  I have read the feedback. I don't feel the feedback has addressed my concerns on the "inexact Newton" part. While further theoretical analysis might be hard at this point, I would like to see at least a toy example that you can compute exact Newton, and compare it to your approximation method.