This paper proposes an algorithm for learning goal-conditioned RL policy, in which a goal is defined as a single image. The authors propose to encode a state (an image) to a vector in latent space using variational autoencoder, and define reward functions inside the latent space. The paper shows that such reward function outperforms baseline such as pixel based reward functions. The authors then proposed latent goal relabeling, which generates new goals and rewards given an exist tuple (s, a, s’). In this way, the off-policy algorithm essentially obtains more training data. Finally, the authors propose goal imagination, which samples goals from latent space during training, essentially allowing training without specifying a particular goal. (interacting with the environment is still requires, because the transition function is unknown).  The experiment results justified the proposed methods, showing that the proposed algorithm is more sample efficient and achieve better performance.  The proposed method is novel and the experiments are thorough, and I'm inclined to accept this paper. Here are some suggestions/comments: 1) A goal in the paper is defined as a single image, which limits the application of the algorithm. Usually goals are more abstract than an image (e.g. in auto driving, probably one goal is to stay in the lane), or sometimes multiple images maps to the same goal. Indeed, defining the goal space the same as observation space is convenient, but has its own limitations. 2) The algorithm first trains a VAE model using some exploration policy, and then fixes the VAE parameters and trains the policy. However, this requires the exploration policy to explore extensively to cover better the state space so as to collect enough data to train the VAE model. This is a time and sample consuming step. Is it possible to incrementally update both VAE and policy parameters? 3) Experiments of reward specification comparison (Figure 4). It would be nice if the authors could add another baseline which is GRiLL plus hand-specified reward function (the reward function Oracle used in Figure 3). It would also be nice if the authors can add another figure which demonstrates the correlation between the proposed reward function and the hand-specified reward function used by Oracle. 4) Line 291: the authors state “VAE is significantly better than HER or not relabeling at all”. However, Figure 5 shows that VAE (the grey line) is doing worse than HER (purple line). The labels in the figure is wrong? 5) Figure 6: it would be better if the authors also run baselines for the variable numbers of objects setting.