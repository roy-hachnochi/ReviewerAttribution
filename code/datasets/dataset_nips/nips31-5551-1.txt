Summary: The paper presents a new PAC-Bayes bound for stable Hilbert space valued algorithms that is obtained by using a specifically defined prior distribution. In case of SVM, the proven bound is compared to the respective results from the literature. In addition to discussing the formal comparison, the authors show the empirical behavior of the new bound on two datasets from UCI.  Quality:  I like the idea of using a specifically designed prior distribution in PAC-Bayes framework to show a better bound. The proofs appear correct to me except for one place: I am not sure that the formula for the KL-divergence between the Guassian remains the same in the formula without a number between page 174 and 175 (it is also not clear what option for defining the Guassian distribution from supplement E is used) and I would like the authors to address that in their response. Moreover, I don't like that all quite important technical details regarding the definition of the Gaussian distribution over the Hilbert space are moved to the supplement and I think these details needs to be a part of the main paper.  Clarity: In general the paper is well-written. The exposition is clear except for the part when the authors switch to discussing distributions over the Hilbert spaces. Some relevant work is not discussed though, e.g. London, "Generalization Bounds for Randomized Learning with Application to Stochastic Gradient Descent" and other work on the mix of stability with PAC-Bayes.  Originality: The presented results are new to my best knowledge.  Significance: Given that the authors address the issue of Guassians over Hilbert spaces, I think the paper presents an interesting approach for mixing PAC-Bayes with stability arguments and should have a positive impact.