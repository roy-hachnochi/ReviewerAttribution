# Summary The work tackles the important problem of graph based representations for CNNs in the context of computer vision problems. It is a direct application of a convolution technique presented before in combination with an approach to map image data to and and from graphs. The work presents strong results for semantic segmentation, object detection and object instance segmentation.  # Paper Strengths  - Relevant problem. - Fairly easy idea and combination of relevant work. - Improved prediction performance on several hard computer vision tasks. - Paper well written.  # Paper Weaknesses  - The presented node count for the graphs is quite low. How is performance affected if the count is increased? In the example of semantic segmentation: how does it affect the number of predicted classes? - Ablation study: how much of the learned pixel to node association is responsible for the performance boost. Previous work has also shown in the past that super-pixel based prediction is powerful and fast, I.e. with fixed associations.  # Typos  - Line 36: and computes *an* adjacency matrix - Line 255: there seems to be *a weak* correlation  # Further Questions  - Is there an advantage in speed in replacing some of the intermediate layers with this type of convolutional blocks? - Any ideas on how to derive the number of nodes for the graph? Any intuition on how this number regularises the predictor? - As far as I can tell the projection and re-projection is using activations from the previous layer both as feature (the where it will be mapped) and as data (the what will be mapped). Have you thought about deriving different features based on the activations; maybe also changing the dimension of the features through a non-linearity? Also concatenating hand-crafted features (or a learned derived value thereof), e.g., location, might lead to a stronger notion of "regions" as pointed out in the discussion about the result of semantic segmentation. - The paper opens that learning long-range dependencies is important for powerful predictors. In the example of semantic segmentation I can see that this is actually happening, e.g., in the visualisations in table 3; but I am not sure if it is fully required. Probably the truth lies somewhere in between and I miss a discussion about this. If no form of locality with respect to the 2d image space is encoded in the graph structure, I suspect that prediction suddenly depends on the image size.