This paper presents an approach to learn the dynamics of the unsteady flow around a cylinder immersed in a fluid in order to use model-based control to reduce the unsteadiness of the flow. The paper has three main parts:     1. The dynamics of the fluid flow are modelled as a linear state-space model operating on a modified state that is the result from applying a non-linear transformation to the actual state of the flow.     2. Model Predictive Control (MPC) is used to reduce flow unsteadiness by actuating the cylinder immersed in the flow. Quadratic MPC can be used because the dynamics are linear in the modified state representation.     3. Insight from the MPC solution is used to design a very simple feedback control law that reduces flow unsteadiness without the need of running MPC.  The paper is solid, well written and mostly clear. I would class this paper as a good "application paper" that makes good use of a number of techniques that have been developed in their respective fields. I think that this is a very good paper to introduce Machine Learning techniques to the aerospace engineering and computational fluid dynamics communities. However, I am not sure that it offers useful insights or techniques to the NIPS community.  The control part of the paper makes straightforward use of known techniques. The modelling of the dynamics is, in my opinion, more interesting. The basic idea is to learn an auto-encoder that nonlinearly transforms the state in such a way that the dynamics are linear when represented in the encoded state.  Section 2.2 is key as it presents the dynamical model and how it is learnt. In my opinion, this section would benefit greatly from explicitly stating what is the loss that is minimised during training. I find Figure 1 nice to get an overall view of what is going on but I also find it slightly ambiguous (although some of the ambiguities are cleared up in the text.) For instance, it could seem that Yhat = decoder(Ytilde) whereas I believe that in actual fact Yhat = decoder(Ytilde_pred). In other words, the element xhat_2 in Xhat is not necessarily the same than the element xhat_2 in Yhat.  Something that I have found interesting which I do not remember seeing before is that within this non-standard autoencoder, only the first half the time series is used to compute matrix A via least squares. Presumably, since the model is trained end-to-end, there is backpropagation through the least squares operation and the second half of the time series acts like an internal validation set.