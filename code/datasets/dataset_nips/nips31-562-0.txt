This work is deals with risk measure in RL/Planning, specifically, risk measures that are based on trajectory variance. Unlike the straight forward approach that is taken in previous works, such as Tamar et al. 2012, 2014 or 2; Prashanth and Ghavamzadeh, 2013, in this work multiple time scale stochastic approximation (MTSSA) is not used. The authors argue that MTSSA is hard to tune and has a slow convergence rate.  Instead, the authors propose an approach which is use Block Coordinate Descent. This approach is based on coordinate descent where during the optimization process, not all the coordinates of the policy parameter are optimized, but only a subset of them. After assumption relatively standard assumptions (ergodicity of the MDP and boundedness of  rewards), the authoers provide a convergence proof (to a local optima).   Afterward, the authors provide a finite sample analysis of of Nonconvex BSG Algorithms, where their provided RL algorithm belongs to this family algorithms. I followed the mathematics and it seems correct (the sketch).  Finally, the authors provide experiments, in the lines of Tamar et al. 2012 and 2014 and show their algorithm validity.   In general I like this work. It provides additional methods for optimizing the mean-variance trade-off. Also, the finite sample analysis in this case, where one do not need the finite sample analysis of multiple time scale stochastic approximation is a good technique in the general case of BCD. My only concern is that I'm not sure about the argument of the authors that such method provide a faster convergence with respect to Tamar et al. 2012 for example (or that Tamar et al. convergence rate is low). I would like to have clarification regarding this.      I refer also the authors to a recent paper "Concentration Bounds for Two Timescale Stochastic Approximation with Applications to Reinforcement Learning" of Dalal et al. that show how to compute finite sample analysis in the case of multiple time scales problems.   ---- After author feedback -------- I read the feedback. I thank the authors for it. I maintain the same feedback and score.