i had some difficulty in understanding what was being done. but this paper is outside of my expertise, so it may be mostly my fault. let me start by checking whether i can correctly summarize the two methods (one with gradient descent as the inner learning algorithm, and one with an RNN as the inner learning algorithm).   in standard MAML, you might do a behavioral trajectory, then do a gradient step using the rewards and actions sampled in that trajectory, and then do another trajectory. the outer learning algorithm optimizes the initial parameters such that the policy after the gradient step leads to higher rewards. E-MAML adds another term to be optimized, which encourages you to do actions *before* the gradient step which lead to a good policy *after* the gradient step. in other words, it encourages you to do actions that lead to the most useful gradients.   in standard RL^2, you use policy gradient as the outer learner to train an RNN (which receives rewards as inputs) to be a good inner learner. if i understood correctly, the only modification in E-RL^2 is to ignore rewards (for the sake of policy gradients, but still providing them as inputs to the RNN) during a randomly chosen subset of rollouts. this is meant to avoid trapping the learning process in exploiting immediately available rewards.  the paper also introduces a task environment where some of the rules are frequently re-randomized so that exploration is important.   the results are currently minimal; it's a little bit difficult to know what to take away from them. i would be interested to see more analysis of the performance, including robustness to hyperparameters for each algorithm. it would also be nice to see more analysis of what effect the extra optimization objective in E-MAML has on the policy, versus MAML. is it possible to show more detail of what the exploration looks like in krazyworld or in the mazes?  stray parenthesis in equation (1)  tau isn't defined when it's introduced in equation (2)  missing a marker for which is equation (3) 