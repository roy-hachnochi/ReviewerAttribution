This paper presents a novel method for explaining black-box models. The model proposes to find representer points, i.e., the training examples that are influential to the model’s prediction and decompose the prediction to the sum of representer values. By learning the weights corresponding to the representer values, the importance of different training samples are indicated. Extensive experimental results are provided to show the validity of the method, such as data debugging, misclassification analysis, etc.  The proposed method is valid and provide insightful interpretation on the results for black-box models. The experimental results seem promising. The presentation is clear and easy to follow.  This is a good work on interpretation. I only have one concern. For interpretation of a black-box model, this method needs to know how the black-box model is formulated and seems not applicable when the loss function of the black-box model is not available.   ------------------------------------------------------------------------------------ AFTER REBUTTAL:  Having read all the reviews and the feedback from the authors, I would like to lower my score a bit (7->6) for the following reasons:  1. As I checked Section 4.1 again, I am a bit concerned about the validity of data debugging setting, as the authors propose to check data with larger |k(xi, xi, αi)| values. I am not sure if the representer values can be a good indicator of label correctness. Does higher training point influence necessarily suggest larger mislabel probability? In particular, training points with large positive representer values tend to be similar samples with correct labels as shown in Fig. 4 and 5.  2. I agree with other reviewers that we should not be limited by the desire on quantitative results in a research paper. However, since the authors propose a method for training an interpretable model in Section 3.1, it would be helpful to report the performance (e.g., prediction accuracy) of such interpretable model, at least on some toy data sets, like MNIST.  3. Compared with influence function, Table 1 provides a compelling advantage in terms of computational efficiency. This is the main reason for my enthusiasm about this work, as efficiency is critical for applying an interpretation method to large-scale scenarios. However, the comparison on explanation quality seems subjective. I could see that Fig. 5 provides some better negative example choices. While in Fig. 2, the definition of "dogness" seems a bit vague and subjective to me.  In general, I think this paper presents an interesting idea (somewhat reminiscent of the layer-wise relevance propagation method, which is yet for features-based explanation). Compared with influence function, this method has been proven to be more scalable. The comparison on explanation quality is a bit subjective and may be improved via additional results.