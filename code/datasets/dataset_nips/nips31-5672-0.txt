This paper studies the influence of depth and width on the performance of large-batch training of neural networks. By presenting some theoretical results on shallow non-linear networks and deep linear networks, they argue that larger width would allow for larger batch training, which is thus beneficial for distributed computation.   Overall I find the paper well-written. Discussion and presentation of results are clear. Systematical experiments on synthetic and real-world datasets could demonstrate to some extent the selling points. I also like the main messages/observations in the paper.  However I would like to understand a bit more the implication of results on practical training of neural networks. In particular, as mentioned in the paper, using larger batch sizes can lead to speedup gains in distributed settings, but in practice we often care more about generalization performance than just fitting the training set. At this point, I'm curious to see how's the generalization of tested networks as a function of width resp. batch size?  It might be worth to mention that apart from the benefits of width already listed in section 2, prior work have also shown that larger width could lead to a well-behaved loss landscape which makes the training of these networks become possible at all (see e.g. optimization landscape and expressivity of deep cnns, 2018, and the references therein).   Line 161: it seems that W^* needs to be introduced. -- Thanks for the clear rebuttal. I find the results and analysis of the paper interesting and a very good complement to prior theoretical work on the influence of network width on training of practical deep NNs, given the fact that most of previous work focus on the power of depth.  I believe the paper is worth to be published at NIPS and hope to see further developments in this direction.