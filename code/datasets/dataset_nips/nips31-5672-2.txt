Large mini-batch size can speed up the training of deep learning models using a lot of data. There are many researches in the direction. The paper studies how network structure (width and depth) affects the mini-batch size during training. It presents several interesting findings. It found that gradient diversity increases monotonically as width increase and depth decreases. Seem like wider networks provide more space for the gradients to become diverse, so wider networks allow larger batch size than deeper one. The results of experiments confirm theoretic findings.   I was able to follow the paper and the topic is interesting. For fully connected and ResNet models, the results are inspiring. I think it will be more interesting that the study evolves other layers like filters, pooling, etc. We think depth helps build better models. This study show wide/shallow models actually trains faster for a given accuracy and parameter budget.   In Figure 2 (b), why does batch size drop sharply? also for (c), why does batch size increase exponentially?  