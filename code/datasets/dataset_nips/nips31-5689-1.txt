The authors provide a bound for sample complexity in a  non parametric setting for multi-class classification  task using semi supervised learning.  They introduce assumptions based on the differences between the   mixture distribution learned from unlabeled data and the true mixture model induced  by class conditional distributions.   It is good that they tackle the different sub-problems in a systematic manner. They learn decision boundaries for the classifier using the unlabeled data. The labeled data  is used to associate the regions with classes. They utilize MLE and Majority vote based estimators  to learn the decision region -> actual class mapping. They also provide bounds for classification errors  based on the learnt mixture model (based on literature) and the region->class mappings (permutation) Furthermore, They propose algorithms for utilizing MLE to learn the permutation.   I am also curious about the following:    Are there any other estimators that the authors think might be of interest in practice.  Any thoughts about jointly learning both the mixture model and the permutation.  The focus of this work seems to be on theoretical contributions. It would be interesting  to know how the proposed mechanisms work in practice with a variety of datasets in different  domains where the underlying distributions can be much more complex.