Summary: The authors study the problem of regression when the loss function is \ell_1 and the noise is heavy tailed. They provide results for a truncated estimator when the input and output are unbounded.  Pros: 1. The proofs are well written and are easy to follow.   2. The authors do a thorough literature survey and do well to motivate the problem and place their work in comparison to past work on this problem.  Cons: 1. The estimator proposed is highly non-convex and it is not clear if it is a computationally tractable in practice for any problems of interest.   2. Corollary 2 is misleading in its claims. It assumes that \sup_{w \in W} R_{\ell_2}(w) is bounded by a constant. This is not true even when the noise process is Gaussian. For example if the set W is the unit ball in \ell_2 and noise added is Gaussian, then the term \sup_{w \in W} R_{\ell_2}(w) will scale as O(d) making their result d^{3/2}/n.   Apart from this the results of Theorem 1 fail to capture the scaling of the sample complexity with the variance of the noise process. It feels to me that this scaling is hidden away in terms like \sup_{w} R_{l_2}(w) which are harder to interpret.  3. All of the proofs following by simple standard arguments and it is not clear to me where the technical novelty lies in this paper. Their proposal of the estimator is new (although modified from Audibert and Cantoni). However as previously mentioned this estimator is computationally intractable.   4. Theorem 3 appear to be (very) easily obtained from classical results such as those in Bartlett & Mendelson 2002. I do not see what is novel in this result.  Suggestions: 1. Adding empirical results will be very useful I think to convince readers that this estimator is tractable. Currently this estimator appears to be non-convex and hard to optimize. It would also help to verify the scaling obtained theoretically.  2. An analysis that eschews bounding the excess risk in terms of maximum ell_2 risk would also be insightful to reveal the scaling of the sample complexity with the properties of the noise process.  Minor Comments:  1. On Ln. 160 Converging number -> Covering number 2. On Ln. 209  by the standard technique of concentrations -> by standard concentration techniques.