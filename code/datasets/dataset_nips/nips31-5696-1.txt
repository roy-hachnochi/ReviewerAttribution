This paper presents a method for transfer learning between robots with various kinematics and dynamics. The main idea is to concatenate the state vector with a "hardware representation vector". The hardware representation vector is based on the physical structure and properties of each robot. The authors study explicit hardware representations vectors (based on the kinematics) and implicit (learned) ones (by back-propagating through the policy network).  Strong points: - Works with existing RL methods (just concatenate the state vector with the hardware representation vector) - Seems easy enough to implement - Extensive experiments in simulation  Weak points: - No experiments on real hardware experiments  - Learning the implicit hardware embedding seems a bit naive. As it stands, this needs to be done for each new robot. How does this scale to few-shot or zero-shot learning?  Questions: - Can you comment on the relationship between your work and this recent paper: https://arxiv.org/abs/1806.01242 . The architecture is very different, but both works claim to embed/learn physics related properties and allow for transfer learning. - Do you have any insights in the implicit embedding? What do the implicit hardware representation vectors look like (e.g. do they similar for similar robots)? The hardware representation vectors are a function of the robot's physical structure (h_v = f(robot)). Can this function f somehow be learned?   