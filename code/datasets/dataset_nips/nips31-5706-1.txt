This paper addresses the topic of unsupervised domain adaptation, i.e. how to adapt a classifier which was trained for one task to another one using unlabeled data of the target task only.  There is a large body of research on this topic. Many algorithms are based on the alignment of the source and target feature distributions.  This paper proposes to extend this idea by constructing multiple feature spaces and aligning them individually, while encouraging agreement between them. This approach is surprisingly successful and outperforms the state-of-the-art on the digit classification tasks MNIST, MINSTM, SVHN, DIGITS, STL and CIFAR.  In Table 1, you provide the results for both classifiers. How do you select the one that you would use in a real application ? One could probably chose either one since their performance are always very close. Can this be guaranteed ?  Since we have two classifiers, one also wonders whether we could get additional improvements by combining them, e.g. by majority vote or averaging the predictions.  You mention that you only use two classifiers in this paper. Obviously, the question arises what would happen when you used more. Do you expect additional improvements ? Or is this computationally intractable ?  How would you align multiple feature spaces - pairwise or with respect to one "master feature space".  It would be nice to comment on these topics in the next version of this paper, with our without running experiments.