This paper considers the computational and statistical trade off for the kernel K-mean via the Nystr√∂m approach, which aims at reducing the computational cost and storage space. This paper proves some surprising results, maintaining optimal rates while the computational cost can be reduced greatly under some basic assumptions. The derived results under the framework of unsupervised learning is very meaning, while some similar results concerning the balance of statistical error and computational accuracy have been established in the supervised setting. 