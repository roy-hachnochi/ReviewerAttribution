This paper proposes a new paradigm for generative memory using attractor dynamics. This is an important advance in the generative memory models, such as the Hopfield network, Boltzmann machine, and Kanerva machine (KM). Specifically, this work presents a non-trivial dynamic version of the KM , and associated optimization (dynamic addressing & Bayesian memory update), and learning (training & prediction) rules.  The new methods have been tested in two benchmark datasets, and compared with the result of KM.  Overall, the conceptual idea is innovative, and technical foundation is sound.  The result comparison is compelling.   A couple of suggestions:  1) the memory M in the generative model is modeled as a Gaussian latent variable with a set of isotropic Gaussian distributions. If the columns of R are mutually independent, then V is diagonal , but U is generally a full matrix. Can that be generalized to an exponential family?   2) One of appealing feature of the dynamic KM is it is flat capacity curves (Fig. 2, right). I wonder what would be the capacity limit? is there any empirical rule to estimate the capacity? Does the capacity curve depend on the SNR? I would imagine a low SNR would induce a lower compression ratio. The denoising application shows noisy images with 15% salt-and-pepper noise, how does the performance change with the SNR or noise statistics?   3) The author shall discuss how to extend the idea from point attractors to more general cases such as line attractors.  Also, what are the limitation of the dynamic KM?  Finally, the authors shall consider sharing the open-source software to enhance the reproducibility.