This paper proposes a new component for residual networks to improve the performance on image classification task. The proposed idea is to find channel-specific attention from each feature maps, which is to gather the spatial information by a particular layer such as the average pooling, and then the scatter layer such as nearest neighbor upsampling is followed. Experiments are done on the datasets including ImageNet-1k and CIFAR-datasets, and the study of the class sensitivity compared with the original ResNets is provided to support the effectiveness of the proposed method.  Pros) +) The proposed spatial attention idea is simple yet promising. +) The authors provide sufficient experimental grounds. +) The class sensitivity study could be a strong supportive ground for the proposed method.  Cons) +) Some notations are not clearly presented throughout the paper. +) There is no detailed architecture description of GS-P.   +) Eventually, the global average pooling is the best. Then, the spatial information may not be useful, and the proposed method is quite similar to SE-Net. +) The authors claim that the method does not involve extra parameters, but they mainly do experiments with GS-PP which uses depth-wise convolution layers that clearly have additional parameters. The authors should refine the writing.  Further comments) 1) Please explain GS-P architecture in detail. Specifically, please explain the scatter operation using depth-wise convolutions in detail.  2) With GS-P, How did you achieve several extent ratios? How did you get the global extent ratio with it? 3) Please explain why GS-PP-50 (seems to use depth-wise convs + fcs) could outperform ResNet-SE-50? 4) It is better to show the result of GS-PP-101 and ResNet-101-SE in Table 3. 5) It is better to show the result of GS-P-SE  in Table 2, 3 and 4. 6) How about applying GS-net on the second or third stage features?  7) Could you provide any intuitions why the global average pooling is better than the operator with a smaller extent ratio?