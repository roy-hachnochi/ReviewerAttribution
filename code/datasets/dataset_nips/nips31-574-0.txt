Strength:  - The continuous relaxation of KNN that this paper introduced is novel. It is based on replacing the one-hot coded weight vectors with continuous expectations This new selection rule overcomes the limitation of non-differentiability of KNN. The differentiability enables the proposed network to be end-to-end trainable.  - An overall good review of non-local methods, and an extensive comparison with them in the experiment part.   - This paper is structurally organized and clearly written  Weakness  - We know the traditional KNN has obvious limitations – slow and memory inefficient. While this paper’s main novelty is introducing a KNN relaxation, could you also discuss on the computational cost incurred by using N3 block, and compare with KNN block?  - In the ablation study, a more fair comparison to demonstrate the effectiveness of your individual components is to add 2 × DnCNN (d = 17), KNN block (k = 7) and 2 × DnCNN (d = 17), N3 block (k = 7).  An alternative is to run a 1 × DnCNN (d = 6) as baseline, and then do 2 × DnCNN (d = 6), N3 block (k = 7). In this way, it is cleaner to see how much improvement that one KNN or N3 block brings.   - Minor point: please give more clues on the choice of hyper-parameters, such as k (just like KNN is sensitive to K choice, is here the same?)