This is a paper on the derivation and use of PAC-Bayesian results for the problem of learning classification trees. It provides - theoretical results for the model that consists in weighting the subtrees of a general tree to have an efficient (multiclass) classifier; - algorithmic strategy to efficiently compute the weighting of each subtree from weights associated with all the nodes of the "base" tree; - empirical evidence that the theory-guided learning procedure is indeed relevant.  The paper is extremely clear and easy to follow. Going from theoretical results to algorithmic procedures to compelling results is something that is very important in machine learning. This paper combines these three features.  I just have some questions/remarks: - is it possible to provide the value of the bound on the excess risk? that would indicate how precise the Pac-Bayesian results are. - Germain et al, worked a lot on the connection between PAC-Bayesian bounds, the Gibbs risk and the possibility to use the majority vote, together with a learning algorihtm. Is there a way to adapt their min C_q bound/procedure for the present case? - more comments on the results of section 5 would be welcomed.  Typos Let Q^* minimize (without S) at everY node l. 93 weird parenthesis  UPDATE: rebuttal read. I am satisfied with the answers provided by the authors to all the reviews.