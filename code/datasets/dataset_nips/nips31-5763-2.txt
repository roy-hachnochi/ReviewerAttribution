Paper describes an approach for analyzing of the generalization performance of the subtrees of classification decision tree and evaluates it empirically against the well-known pruning approach.   The strong points of the paper:   + The paper addresses an interesting question of decision trees regularization techniques and proposes what seems to be novel strategy of choosing a subtrees by using PAC-Bayesian framework.   + Experimental section, where the proposed approach, based on the dyadic and kd trees is compared to the pruning method of regularizing of decision trees.  + Theoretical contribution is solid and seems to be correct.   The weak points of the paper:  — Generally, I found the paper quite convoluted and partially hard to follow due to some reasons. Several points should be specified: in line 68-69 authors assume that the diameter of set is $1$, which is possible when underlying norm is the supremum norm, but do not specify the latter. Also, both term “leaves” and “nodes” are used in the paper, which is some place is confusing. In line 108-109 the constant C is defined, however precise meaning of it is not discussed (for example one would be interested how C scales with dimension).   — Definition 2 could be presented in more formal way, introducing formal (mathematical) notion of hierarchical partition and trees elements.   — Typo: at figure 1 fof —> of  — Generally the paper was hard to understand and I tend to reject it for clarity and coherence reasons. However, I am absolutely not an expert in the field of PAC-Bayes learning.  ============================ UPDATE: I thank authors for addressing my questions in their rebuttal. I have changed my score towards acceptance.  