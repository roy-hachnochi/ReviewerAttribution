The paper aims to answer to often-asked questions for assessing the quality of an explanation method for complex ML algorithms. This is in-line with an increasing amount of literature on assessment of explanation methods such as what explanation method should achieve, such as • Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017. • Kindermans, Pieter-Jan, et al.  " Learning how to explain neural networks: PatternNet and PatternAttribution." ICLR. 2018. •  Their main contribution are two simple strategies for testing the faith of the explanation method to the model and the data. For testing model faithness, they propose comparing the explanation output of a trained model to that of a randomly initialized one. For faith to the data, labels are randomly shuffled for training. They come to the conclusion, that many of the currently used methods are independt of data and model to be explained. Additionally they compare outputs of common explainability methods to that of a Sobel edge detector. The methods they propose are easy to implement and provide a valid and intuitive ‘sanity check’ for two conditions that all explainability methods should fulfill. There are extensive experiments, applying these two methods to a variety of often used explainability methods. The writing style is very clear and the paper has a clear logical structure apart from the problems mentioned. The authors motivate and explain the problem, they are solving. A weakness of the paper is, that the analysis is often restricted to qualitative instead of quantitative statements. Starting with l. 246 they argue that edge detector images look very similar to the output of many explanation methods. This in itself is not surprising, as NN do process edges and shapes as well. Simple correlation analysis of edges and explanation output would give more insight into whether NN do not prioritize important edges in the input.  The structure of the paper is lacking. The discussion section includes more experiments and does not actually interpret or discuss the earlier results. In my opinion this paper proposes two sound methods for assessing explainability methods. Unfortunately there is no significant discussion or proposed explanation of the results they found.  Due to this and the fact that very easily obtained quantitative measurements are missing, this paper is marginally below acceptance Minor: l. 101 existing, not exisiting 190 sentence incomplete “Learning to explain …”  is published at ICML – replace arxive citation 