This paper proposes a probabilistic meta-learning method that extends MAML to probabilistic version with prior parameters defined on the model. The motivation is to use probabilistic method to capture the uncertainty that is disregarded in deterministic gradient descent methods. Variational inference is adopted to solve the problem and specifically a variational lower bound is used to approximate the likelihood to make the computation tractable. Some regression and classification problems are studied in the experiments.  I would agree that the proposed probabilistic MAML method is able to deal with more uncertainty for meta-training compared with MAML, while I have some concerns below.  The experimental section considers two toy problems and an image classification problem while all of them are relatively simple cases with regard to their problem scale. In the image classification problem, the coverage metric that “receive at least one sample for each set” is somewhat loose that it is not a strong criterion to access the performance of MAML and ProMAML.  The ProMAML method is studied only for some simple supervised learning problems, while in the MAML paper both supersized and RL settings are studied and evaluated. I think it could be much more interesting to apply ProMAML to RL problems many of which may be considered with much more ambiguity and uncertainty.  Minor:  The subfigures in figure 1 should be subscripted with (a), (b) and (c) which are referenced in the main text.  In figure 1, should “p(\phi|x_i^train, y_i^train, \theta)” on the top of the middle subfigure be “p(\phi_i|x_i^train, y_i^train, \theta)”?  There is a “?” in the references in line 136.  Line 158: “a inference network” -> “an inference network”  Line 178: Eq. (1) does not have the second line.