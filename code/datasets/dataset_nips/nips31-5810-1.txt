This work augments the natural language inference (NLI) task with explanations for entailment relationships between the premise and hypothesis. The authors introduce a new dataset including these explanations, and explore models that exploit these explanations for natural language inference and sentence-level representation learning.   I appreciate the general idea of employing supervision for model interpretability. I am also impressed with the authors' meticulous annotation protocol that employs a number of techniques during and after annotation to ensure data quality. However, I see three main problems that prevent me from recommending the acceptance of this paper. First, the experimental results of this paper fall short in demonstrating the strengths of the authors' approach. Second, the SNLI dataset seems like a suboptimal choice for such a dataset. Third, the contribution of this paper seems to better fit an NLP venue than a general ML venue. See details below.  Comments:  -- Unlike what the authors claim, the experimental results presented in this paper do not indicate that the dataset is useful in any downstream task. Currently these results are better seen as negative rather than positive evidence. While this does not mean that the dataset will not turn out to be useful in the future, it is discouraging to see that the authors were not able to show any positive result using it. In particular: - Table 1 is not very informative as it stands, since there is no perplexity comparison to other baselines, and perplexity itself may not be an accurate way of measuring the quality of generated explanations (low perplexity may not equate to coherent sentences).  - Running multiple seeds and reporting standard deviation is important. However, in both Table 4 and 5, the improvements that are seen over baselines in almost all cases are well within standard deviations, implying the null hypothesis. - The SNLI results reported on section 4.1 are quite lower than state-of-the-art (https://nlp.stanford.edu/projects/snli/), which further weakens the authors' claims.  -- The choice of SNLI as a base dataset for adding explanations seems suboptimal given recent evidence on the type of information encoded in it.  - It seems that choosing another dataset, at the very least MultiNLI, which also suffers from some of the same problems, but to a less extent (Gururangan et al., 2018, Poliak et al., 2018). While the works pointing to these problems are relatively new and might not have been available when the authors started working on this project, (a) the authors do cite some of these works, (b) while this is not entirely fair to blame the authors for not being aware of those problems, it is still substantially reduces the usefulness of this dataset. The authors actually mention that the annotated explanations in their dataset heavily correlate with the entailment classes, leading one to suspect that the explanations might be reinforcing stylistic artifacts in the training data. Indeed, the examples in Figure 1 seem very templated and lexical. - To explore this phenomena, the authors are encouraged to perform a more global analysis of the generated explanations, perhaps by using crowdworkers to label the correctness of generated explanations across the full evaluation dataset. Such an analysis would be useful towards reinforcing the author's claims that the models is learning to explain entailment phenomena, rather than associating specific words or templates with the inference class and premise/hypothesis pair.  -- The authors point towards a tradeoff between l_label and l_explanation. I was surprised to see that the authors took the model with the best l_label score. It seems that the highest alpha value explored (0.9) was selected, indicating that the best l_label might have been obtained with alpha=1. Optimizing a different function might make more sense here.  -- I would have liked to see a better exposition and analysis around Section 4.4 (Transfer), which is quite sparse as it stands. As the authors state, multiNLI is a good testbed for cross-genre transfer, so it would be pertinent to see, for instance, comparisons for each genre.   Questions to the authors:  -- The annotation protocol is missing some important details (section 2.2): - How many annotators took part in creating this dataset? How much did they get paid per explanation?  - Quality assurance: what do the author refer to as an error? is it a binary decision (any score < 1?), or averaged score?  -- What is L_explanation in equation 1?    -- Typos and such:  - Table 2(b): shouldn't the label be "entailment"? - Table 5 caption: "fine-tunning" should be find-tuning - Line 179: "with the same 5 seeds as above.": seeds only mentioned later - Line 246: "consisten" should be "consistent"     ======= Thank you for you response. While I partially agree with some of the comments, I am still unsure about the quality of the generated explanations. I would advise the authors to include the analyses described in their response, as well as other ones suggested by all reviewers, and resubmit to an NLP conference. I would also suggest working harder to improve results using e-InferSent (potentially using a different model).