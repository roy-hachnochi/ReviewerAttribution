Summary:  The paper extends SNLI with an additional layer of human-annotated explanations of the entailment relations. The paper implements Seq2Seq encoder-decoder trained on it and produces explanations at test time. It shows that the proposed corpus with explanations can be used for various goals such as better interpretability and improvement of universal sentence representations on a range of tasks.  Pros: 1. The paper is clearly written. The examples in Table 1, 2, 3 help readers to understand paper easily. 2. The paper provides consistent improvements on a range of tasks compared to universal sentence representations learned without explanations. 3. The proposed dataset shed some light on NLP tasks for using natural language explanations.  Cons: 1. There are no significant test for Table 4 and Table 5. 2. The improvement on Table 4 and Table 5 over baseline is minor. 3. When training a model on SNLI to also generate explanations, why there is a minor drop in performance from 84.76% to 84.25%? The author should give more explanations here. 