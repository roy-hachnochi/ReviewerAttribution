The authors make an interesting observation that KFAC approximation of the Fisher information matrix results in a pre-conditioner with potentially mis-scaled variances of the gradient in the Kronecker-factorized basis. The paper propose to fix the issue by correcting the eigenspectrum of the KFAC pre-conditioner and develops a simple extension of the KFAC algorithm, termed EKFAC.  The paper is well written, easy to follow, and makes an interesting observation. A few comments / questions for the authors:  - As noted in line 133, the original KFAC rescaling is not guaranteed to match the second moments of the correct preconditioner, it would be interesting to see when that happens and much the off. Is there a way to bound error due to the approximation?  - Related to previous point, would be nice to see quantitative differences between the spectra of the preconditioners computed by KFAC vs. EKFAC on a toy problem where one can compute correct preconditioners exactly/analytically.  - The paper applied EKFAC to MLPs and convnets. Should one expect any improvements when using the method for training recurrent networks?