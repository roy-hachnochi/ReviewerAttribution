This paper proposes a new method of image restoration based on  neural proximal gradient descent using a recurrent resnet. The basic idea is to extend the proximal gradient descent algorithm to use a recurrent neural network as the proximal map. Essentially, the recurrent network (they use a ResNet) is used to define the image prior term that, after each update step, moves the estimate towards the image manifold.  This paper is well written and contains strong evaluations. However, discussion of related work could be improved, particularly to give an overview for a reader unfamiliar with the problem.   Some questions:  -Clarity of section 3 could be greatly improved. For example, the superscript/subscript notation appear inconsistent in line 129 and (P1). Also, should an x in the first like of (P1) have a hat? - The consistency term in P1 is noted as being important for convergence. This could be elaborated on for clarity. -Why are the two terms in (P1) weighted by beta and (1-beta), why this inverse relationship? - The method is evaluated on MRI reconstruction and super-resolution. The MRI results appear convincing, outperforming existing methods and proving computationally efficient.  - It is unclear how convincing the super-resolution results are. The authors note that super-resolution is a highly studied problem and existing approaches perform very well. Why chose this task to evaluate your approach on then? I'm not sure what is gained from this experiment.   This paper falls outside my area of expertise. I am unfamiliar with the problem, the methods, and relevant related work, so this review is honestly just an educated guess.