This work attempts to provide a theory/definition for how to define a match between two clusters of neurons (e.g. two layers), each from a neural network. The definition (Definition 1, in paper) allows one to study the similarity between a pair of neural clusters (of the same size). In this paper, this definition of match is employed in this paper to study two networks of the same architecture but trained with different random initializations. This work seems to be a follow-up work from Convergent Learning [1] and SVCCA [2].  The authors characterize the representation of a neural cluster C as the subspace spanned by the set of activation vectors of C. This representation is based on the general assumption that neural activations in a layer are followed by affine transformations and studied neural clusters with ReLU activation functions. An algorithm for finding the maximum match between two neural groups are also provided. The empirical results are tested on a few VGG variants trained on CIFAR-10.  + Clarity - The paper is very well-written and clearly presented. - Notations are defined properly. Intuition provided for the definitions.  + Significance - This work provides an important tool for quantifying the similarity between two neural clusters, and might have an impact to the community.  + Quality - Overall, this is a solid work with a strong theoretically motivated characterization of similarity of two neural clusters. - The empirical results could be stronger to better support the theory provided.  - It would be great if the authors could provide a comparison or connection between their match definition with measuring the mutual information among two neural groups. - I would love to see an experiment on quantifying the similarity between layers in a base network and a transferred network in transfer learning setup where the base and target tasks are similar. Would we see expected results as in Yosinski et al. 2015 [3]?  + Originality - Novel method.  Overall, an interesting paper! A clear accept!   [1] Li, Yixuan, et al. "Convergent learning: Do different neural networks learn the same representations?." ICLR. 2016. [2] Raghu, Maithra, et al. "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability." NIPS. 2017. [3] Yosinski, Jason, et al. "How transferable are features in deep neural networks?." Advances in neural information processing systems. 2014.  ---- After rebuttal ----- I have read all the reviews and the rebuttal. My final recommendation stays the same: accept! Great work!