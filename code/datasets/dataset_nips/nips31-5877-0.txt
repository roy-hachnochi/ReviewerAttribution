This submission describes a model for unsupervised feature learning that is based on the idea that an image and the set of its transformations (described here as an orbit of the image under the action of a transformation group) should have similar representations according to some loss function L. Two losses are considered. One is based on a ranking loss which enforces examples from the same orbit to be closer than those of different orbits. The other one is a reconstruction loss that enforces that an all examples from an orbit should be mapped to a canonical element of the orbit using an autoencoder-like function.  Two broad classes of transformation groups are considered, the first one is a set of parametrized image transformations (as proposed by Dosovitskiy et al. 2016) and the other one is based on some prior knowledge from the data - in this case the tracking of faces in a video (as proposed by Wang et al. 2015).  The proposed approach is described with a very clear framework, with proper definitions. It makes the paper easy and nice to read but this abundance of definitions and notations may be a bit of a burden for some readers.   On a high level, this submission is the generalization of the work of Wang et al. and Dosovitskiy et al.. It gives a more general framework, where different self supervised learning models correspond to different definitions of the orbits. The idea of using two losses, namely the reconstruction and ranking loss makes the framework more general, including the model of Wang et al. and a variant of a denoising autoencoder, where the noise corresponds to samples from the orbit.  Even though this paper draws a lot of inspiration from previous work, it fails to compare to them on a common ground. All experiments are ran on MNIST, NORB, Multi-PIE and a video talk show dataset. As baselines, the authors consider [9] and [28] which are the two paper mentioned above. However, none of those methods report numbers on these dataset. I understand that producing experiments on larger scale datasets (as [28]) can be a bit heavy and not accessible to all institutions. However [9] reports an extensive evaluation on STL-10, CIFAR-10 and Caltech-101, which is tractable, even on CPU only machines. Moreover, the setup considered here is 1-shot learning with a NN classifier, resulting in huge confidence intervals and making the results kind of mixed.  On Mnist, the there seems to be a strong impact of using both losses (OJ versus OT or OE), justifying the importance of using both of them. I would like to see if this transfers to datasets that were used by [9]. Right now, except on MNIST, the proposed approach falls roughly at the same spot as [9] or [28], making the results hard to interpret.  Because the model description is basically just a formal description of previously proposed approaches and that the experimental section does not provide strong evidence for the quality of the model I rate this paper as borderline with a 6. I eagerly await the authors response and the reviewer discussion to make my final decision. 