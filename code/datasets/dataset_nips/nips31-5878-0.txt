This paper proves the invertibility of two-layer convolutional generative networks when partial measurements are given, under certain conditions: (i) weights are Gaussian or sub-Gaussian; (ii) the activation function is ReLU; (iii) the stride is the same as filter size.  An empirical evidence, based in GAN training under MNIST and Celeb datasets, is provided to conjecture that the invertibility holds for more-than-2-layer convolution networks with different activation functions and strides sizes.   Strength:  S1: As indicated in a couple of places, the paper can provide a sufficient condition (stated in Thm1) for ensuring a mode collapsing problem, one of the critical problems in the GAN context.   S2: Technical contribution looks non-trivial, based on the permutation technique.   S3: The paper is very well-written and the proof is well streamlined.  S4: An interesting empirical evidence is left for sparking further research directions.  Weakness:  W1: The proof is limited to the 2-layer network and s=filterSize case, while [9] (which forms the basis of the proof technique in this paper) deals with an arbitrary number of layers. At least the main difficulty that prevents extension if any should be explained in the paper, and any insight for progresses are desired to be included.  W2: One of the interesting works is to identify the minimal sampling rate for perfect reconstruction, which is missing in the current work.   W3: Comparison to [7] (which looks quite related) is not clearly described. Elaboration might help. 