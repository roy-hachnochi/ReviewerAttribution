The paper combines variational auto-encoder training with adversarial training in a novel way. Rather then having a separate discriminator, the KL term of VAE is used as the discriminator (as well as in the same way as in VAE). The goal of the KL term as discriminator is to assign high likelihood (low KL) to input samples and low likelihood to both samples and reconstructions. The generator tries to generate samples that have high likelihood under (low KL). The resulting training seems the be more stable and 1024x1024 samples of faces are produced without stage-wise training. The method is nice and natural and the paper is done well (see some comments below though). However there should be (at least) two important improvements for the paper: 1. Show how sensitive the training is to the hyper parameters (how well they need to be tuned to get good results and what the results look like for different values), 2. Train on rich dataset such as ImageNet where training is not all about overfitting.  Detailed comments:  - It would be good to show generated sample and performance measure for different hyperparametners (e.g. as a 2d grid). - Why not training on ImageNet? - 182-183: It is a little confusing discussion regarding two types of samples - reconstruction and samples (x_r and x_p). One might think that we discrimintate these two from each other, which might also be reasonable given that reconstruction can be close to the input. Line 195 talks about x vs x_r, x_p but it would be good to say it earlier: We discriminate real sample FROM both the model samples and reconstructions. - Formula (11) has alpha in front of both x_r and x_p but the Algorithm has it only in front of one of them. Which on is correct? - The reconstructions in Figure 3 are quite far from the originals - is it because beta is so small? What happens when it is larger (as mentioned before, it would be good to see what happens for different hyper parameter settings - not only the quality actually but also their property). - Is there a reason why in Algorithm 1, the there is beta for the L_AE but there is no beta for the same gradient going into the encoder? - 215: alpha -> \alpha