The work presents a version of VAEGAN (a VAE whose decoder is replaced with a GAN) with interesting contributions in the training methodology and in the quality of results generated. The formulation is a development on ideas from the EBGAN (Energy based GAN) and an earlier work (Autoencoding pixels beyond a learned similarity metric).   General comments 1) This is a VAE/GAN in which the KL divergence term of the VAE is adapted to setup the GAN minmax game, wherein the first player tries to minimize the KL divergence between real samples and prior, while maximizing it for the generated samples. The formulation is developed based on EBGAN, with the discriminator in the EBGAN now being replaced by the KL divergence term of the VAE. To this, a reconstruction term is added to complete the VAE setup [regularizer + reconstruction terms]  2) A training methodology in which one does not need a separate discriminator to train the GAN. The training procedure itself is similar to the older VAE/GAN paper "Autoencoding Pixels beyond a learned similarity metric" [ref 21 in the current paper]. The same machinery of [ref 21] seems to be adopted to add additional signal to the system by decoupling the encoder and decoder.   3) The results are extremely impressive. The authors present comparison with the highly regarded "PCGAN" as reference.  In addition to the above, the paper is a good survey of existing flavors of GANs and VAEs.   Questions and clarifications:  One aspect that wasn't very clear to me was why one needed an extra pass through the encoder after images were generated )reconstructed) from G to use in the GAN, instead of using mu and sigma generated from the first pass.   Minor comment: It would help to have a flow diagram describing the encoder+generator setups although, they are presumably the same as in [ref 21].   Minor comment: the formula (12) is inconsistent with formula (11) in that L_{AE} should take in two inputs (x,x_r) in (11) whereas it only has one input in (12) L_AE(x).  One of the main points of the paper seems to be that the training methodology results in better samples. However, the paper does not discuss why this should be so, in the sense that while the samples produced do indeed seem to be excellent, the circumstance seems to be arbitrary. Perhaps we should have a few studies (empirical or theoretically motivated) explaining why we get good samples, and what the practitioner should do to get them by way of tuning.    Specific comments on review criteria: 1) Quality: Good. The paper is well written, easy to follow and generates excellent results.  2) Clarity: Very good.  3) Originality: Highly original. This development of a GAN without a separate discriminator is novel. However, the AGE paper has already demonstrated that this can be done. 4) Significance: Extremely significant. The reviewer sees this method being of great practical use in VAE architectures in general. Pertinent practical examples are domain adaptation problems where it is necessary to polish samples produced by VAE to be of practical use. 