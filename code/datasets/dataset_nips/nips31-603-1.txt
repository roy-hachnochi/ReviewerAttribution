This paper proposes a method for video-to-video synthesis (as the title states). It is like Image-to-image translation but for videos. In this way they are able to generate HD images (2MP) with highly temporal coherence and very photorealistic.  The method tries to model the conditional distribution using a GAN. Many GAN improvements are present such as several discriminators, multi-scale patch discriminator, or coarse-to-fine approach. Also it makes use of Optical flow.  The method is compared both qualitatively and quantitatively with state of the art. It shows a clear improve on all the experiments.  It has been tested in driving videos and in faces. Two extra experiments are very nice: modifying classes and predicting next frames.  Minnor comments:  - Remove text from the videos  Comments after rebuttal ----------------------------------  The authors have addressed most of the points of the reviewers. The answer for using optical flow groundtruth instead of the computed one is surprising. Usually optical flow estimation results are not very good. However the method is robust to these errors. Maybe the model is not relaying much on the optical flow? The analysis of the failure cases is very interesting. 