------- Summary --------:  The paper addresses the video-to-video synthesis problem. It considers translating a video from a source domain to a target domain, which the target domain is high-resolution, realistic videos. The source domain on 2 of the datasets (Cityscape, Apolloscape) is semantic segmentation map and in the other one (Face Video dataset) is face sketch (obtained by face landmarks). In addition they generate frames of the future of the video. Their approach is a spatial-temporal adversarial learning framework.        They designed an objective function, consisting 1) a sequential generator, 2) conditional video generator (help estimating optical flow), and 3) conditional image generator. They also employ segmentation labels to identify background/foreground information and leverage this information to improve the temporal consistency, and quality of the foreground objects (i.e: pedestrians, cars), and having a better motion estimation. Moreover, they present the results for longer term video prediction conditioning on the source domain image, and former generated frames. In addition, they present a sample of image manipulation.             --------------------------------------------------------------------------------------- I believe that the paper is valuable to the NIPS research community. It targets a new and challenging problem of video-to-video translation.  The paper has good quality and clearity.  There are some strong claims in the paper might not be accurate, and the related work part can improve:  Conditional generation in videos is highlighted as one of the main contributions of the paper in several parts of the paper. In the related work section, introduction and further part of the paper, it is claimed that this paper is the first paper on conditional video generation using GAN. However, the concept of conditioning in video generation is not a novel idea. Bellow, I put a few references on video prediction using conditional generation: Generating Videos with Scene Dynamics, Vondrick et al. Stochastic Adversarial Video Prediction, Lee et al.  Unsupervised Learning of Disentangled Representations from Video, Denton et al.  Video Pixel Networks is a conditional model.    Spatio-Temporal consistency: The paper claims due to their objective function and the design of the adversarial training, they have spatio-temporal consistency. Videos in the supplementary materials, and the quantitative evaluations in the paper support the claim but there is not much literature review for other approaches addressing the same issue. Basically spatio-temporal consistency is the main problem addressed in most of the video prediction papers. Please discuss it in the literature review.  Video generation by estimating/employing Optical Flow, was used in the literature. The format and the formulation was different with what was presented in the paper. However, it is good to cite the relevant work on this area. For example: *Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture*, Ohnishi et al.  ----------------------------------------    