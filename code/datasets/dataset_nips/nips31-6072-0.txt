This paper deals with non-convex optimization problem using Ito diffusion processes. More specifically the paper consider the problem of minimizing a function f on R^d (smooth enough). To treat this problem,  the analysis of the authors is as follows. 1) They first consider the problem of sampling from the density proportional to exp(-\gamma f). Indeed using an existing result the mean of this distribution is close to the minimum of f for \gamma \to \infty.  To sample from exp(-\gamma f), they consider an Euler discretization (X_k) of an Itô diffusion for which this density is stationary. They give some bounds on the bias of some empirical distribution and the target measure which are based on a Poisson solution.   2) They give some conditions to ensure that the Poisson solutions satisfied the given assumption.  3) They apply their result for minimization of functions which appear in learning problems and give some indications on appropriate choices of Itô diffusion to deal with this problem.   I think that it is a nice contribution but that the main message of this paper which is about really non-convex optimization is lost because on the really technical section 3. In this section, they give a quantitative results on the weak errors estimates given in Mattingly et al depending on the Poisson solution they considered and give really  nasty and non explicit bounds on the constant associated with the Poisson solution. I would be really happy if the authors really made some conclusions about these new bounds exept that the complexity is of order O(\eps^{-2}) which was known. In addition, I do not really understand what is the role of the integer n_e in the results of this section.   I really like section 4 which would have required I think more explanation and details in particular the examples could be presented more deeply. In my opinion it is the main contribution of the paper. Finally  it is a little unfortunate that the authors do not tries their methodology on some simple examples and real ones.   