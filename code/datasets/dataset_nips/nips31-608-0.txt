Normally, we analyze the convergence rates of gradient descent algorithms for convex problems in terms of f(x)-f(x^*). However, this paper argues that studying the rate of convergence for the gradients is also important. This paper provides a comprehensive and elegant analysis in terms of the gradients for several variants of SGD algorithms, which is the main contributions of this paper. Basically, I think this paper is a wonderful work. The only two weaknesses of this paper I can find are  1. The motivation of this paper is not so strong. 2. It is encouraged to provide the experimental results to support the theoretical results in this paper.