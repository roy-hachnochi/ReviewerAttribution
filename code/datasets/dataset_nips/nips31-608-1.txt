The paper addresses stochastic optimisation, with the goal of deriving bounds for finding points with small gradients.  They improve previous bounds that were in the order of \epsilon^{-8/3} to \epsilon^{-5/2} and \epsilon^{-2}log^31/ \epsilon. Such work is useful for non=convex optimisation.  The authors use ideas by Nesterov to improve convergence rates for accelerated gradient descent (AGD), one being iterations of AGD followed by GD, and the other being AGD after regularizing the objective.  The authors apply these ideas to the stochastic setting.  The paper is technically sound and correct.  It is a little disappointing that the authors do not discuss practical implications of their results.  There are no empirical studies, and as such it remains a theory-only paper.    