The paper focuses on the key problem of transfer learning policies that can generalize across a range of task and environments despite being trained on a sparse subset. The paper forces disentanglement between tasks and environments by introducing discriminative losses. However, the paper uses behavior cloning to learn these policies rather than any of the DeepRL approaches. The paper tests their performance against other baseline approaches on a set of GridWorld environments and tasks as well as more challenging 3D THOR environment and tasks and show significant improvement in performance. The paper also does an ablation study to find which components of their system is more important than the others with Disentanglement losses being significantly important.  The paper clearly distinguishes tasks and environemnts and identifies  three progressively difficult settings for adaptation and transfer. It proposes learning separate task and environment embeddings that can be composed together to give a policy (some related ideas in [1] as is one of the baselines).  It's a little disappointment to see behavior cloning being used instead of RL. It's easy to say that this can be integrated with RL algorithms but it's not as easy as it looks. Gradients are much worse in RL and the reward signal a lot noisier. So learning would be a lot harder and might not work at all. It would have been useful to see some example at least. And it seems like you do need one-shot demonstration in a new task environment and it's not clear until end of section 3.5 and should have been mentioned earlier and more clearly.  Figure 6 is hard to see (but understandable given NIPS page limit). Although the cross pair study and the marked difference Is quite interesting.  Not sure if the baselines have been carefully tuned. One would expect ModuleNet to perform atleast a bit better than a plain MLP but there seems no difference. It's also unclear how many runs are the results averaged over, although this being supervised learning, probably not that big of a difference.  Hopefully the source code for the experiments will be released.  [1]: https://arxiv.org/abs/1609.07088