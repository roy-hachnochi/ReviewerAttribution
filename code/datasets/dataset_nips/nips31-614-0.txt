In this paper, the authors study the robustness of a classifier with respect to perturbations in the input. In particular, the authors obtain high probability upper bounds on robustness with respect to perturbations within the input manifold in terms of the norm of perturbation. The input manifold is modeled using a network g that maps from a latent space to the input space. The authors further show that for any classifier with in-distribution robustness r, one can obtain a classifier with unconstrained robustness r/2. The results are generalized to the scenario when the network g does not model the input manifold perfectly.  The paper is very well written and easy to read. A theoretical understanding of how classifiers behave when the input is perturbed is necessary in building robust classifiers. In this regard, the work done in this paper appears significant. In particular, the result that maximal robustness with respect to perturbations occur when the induced classification boundaries in the latent space are linear, can be used to construct robust classifiers. 