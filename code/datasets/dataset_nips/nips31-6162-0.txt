The authors derive a dual representation of the ELBO and leverage it do develop an original and interesting "particle flow" algorithm for fitting latent variable generative models.  Comments: ========  1) Proof of Theorem is quite straightforward and the main novelty of the paper -- I think that the proof should be clarified and included in the main text. At the moment, the proof is not up to the standard of a NIPS paper. For example: * there is no KL term in the original definition of \ell_\theta while the proof of Theorem 1 starts by applying Fenchel duality to KL * the authors then use the bracket notations for expectations and then switch back the \mathbb{E}[...] notation. * I do not think that the notation \mathcal{F} has been defined.  All this can be understood with some effort, but I think this should be clarified  2) The sentence "More importantly, the optimization embedding couples the implicit variational  distribution with the original graphical models, making the training more efficient." is very important, and I think that it deserves more comments / clarifications.  3) For clarity of exposition, is it really necessary to talk about Mirror Descent while it seems that most of the experiments are done using vanilla gradient descent.  4) The text is very difficult to read -- notations are not always defined and/or optimal, and not much effort has been put into polishing the text. This is a pity since the idea is interesting!     There are many unnecessary other parts of text that could be deleted without hurting the clarity & flow of the paper.