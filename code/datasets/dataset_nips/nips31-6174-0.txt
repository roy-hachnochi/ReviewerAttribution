Summary: motivated from annealed importance sampling (AIS), the paper introduces a method for variational inference that specifies a sequence of annealed posterior targets f_t(z) for the marginal variational posterior q_t(z_t). These marginals are given by distributions q(z_t|z_(t-1)) defined on an extended state space of latents z_0,…,z_T.  The paper shows that in making these marginal variational posteriors approach each target, this eases the optimisation of the q(z_T) towards the true posterior, and helps it explore the various modes of the true posterior, as does alpha-annealing.  The method is novel, well-motivated and justified, with a nice overview of the related work in amortised variational inference and AIS. The paper reads well, and shows good technical quality - the AVO is well justified in section 4.1 with Proposition/Corollary 1. My main concern for the method is its practicality. Firstly to evaluate the AVO for each t, one needs to evaluate p(x,z_t), hence a full pass through the decoder is necessary. I imagine this will make the learning quite slow, as is the case with Hamiltonian VI in Salimans et al, where they need one pass through the decoder per leapfrog step (I think this is why they use T=1 in their experiments). It would be good to know how these two compare in running time, when you use the transitions specified in section 4.2 for AVO. Another point that should also be made clear is that these transitions will not leave the intermediate posteriors invariant, which is required for AIS to be valid. If instead you use Hamiltonian dynamics for these transitions (without accept-reject step to keep the procedure differentiable), I imagine the learning would be a lot slower, so although you say the two are orthogonal, it seems that using both would be impractical. Also the marginal log likelihoods in Table 1 appear to be similar if not worse compared to Hamiltonian VI. It would be interesting to have comparisons for OMNIGLOT as well.  Also if I understood correctly, AVO is not applicable to deterministic transitions e.g. Normalizing flows, Inverse autoregressive flows, RNVP etc. These all tend to give better likelihoods than those in Table 1 with much faster training (also with fully connected architectures), and are capable of modelling multimodal posteriors, so in what cases I use AVO over these methods? The paper seems to claim in lines 144-145 that these methods are not robust to beta-annealing, but AVO is only compared to Hierarchical VI in Section 5.3.  Other minor comments: - line 120: you mean “discourage” instead of “encourage”? - What were the different beta-annealing schedules you tried in section 5.3? - line 225: “Appendix D” the supplementary materials only seem to have up to Appendix B? - Figure 4: On the y-axis, how does the negative KL reach positive values? - Section 5.4 is titled Amortized inference, but for all previous experiments in sections 5.1-5.3 you were also using amortized inference?  Overall, the paper is technically sound, novel and reads well, but its significance appears rather limited.