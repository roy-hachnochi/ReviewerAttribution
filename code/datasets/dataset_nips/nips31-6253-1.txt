This work introduces variational alternatives to soft and hard attention models, ubiquitous components in modern deep learning architectures. The authors clearly motivate why attention can be considered an approximation of a latent alignment graphical model, show why exact inference of this model is difficult, and propose two variational methods for training such a latent alignment model. Their experiments show that a latent-alignment treatment of attention, trained using marginal log-likelihood or their variational methods, outperforms state-of-the-art baselines for neural MT and VQA.  For a researcher who's accustomed to using soft attention without thinking about an underlying graphical model, the paper was a very helpful new perspective; the argument from the point that "for general functions f, the gap between E[f(x, z)] and f(x, E[z]) may be large" was particularly compelling.  One thing that I was looking for but didn't find was a quantitative claim about the efficiency benefits of the variational approach as compared to exact marginal likelihood training. It's a factor of O(N) in principle, but to what extent is that hidden by available parallelism in the implementation?  I would also ideally like to see the method evaluated in the context of contemporary self-attention architectures like the Transformer, especially because it wasn't immediately clear to me how the approach works when the "enc" function includes additional layers of attention. 