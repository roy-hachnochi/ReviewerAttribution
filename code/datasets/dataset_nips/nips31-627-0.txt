I have red the author's response. Thank you for doing an extra set of experiments in such a small time frame. I will raise my rating of the paper. I still think that, give the heuristic nature of the method, more experiments are necessary. But I understand these will have to go onto a new paper, which I hope the authors publish in the future. Finally, I hope that the authors will include the new set of experiments in their rebuttal, and maybe more if they have time, on the appendix of their paper. It will only help their paper. Congrats!  ----------------------------- Summary  The authors propose a method to improve the decision rules at the nodes of a decision tree.   Their method requires an initial decision tree. The topology of this tree will be fixed, and only the decision rules at each node will be adjusted.  The idea behind the proposed adjustment is based on the observation that, fixing all of the parameters of all the nodes except the parameters of node i, the likelihood function for the whole tree reduces to the likelihood function of a simple K-classes classifier. This simple classifier can be trained efficiently (using existing techniques) and doing so will always guarantee that the overall loss will decrease when compared to the loss for the initial decision tree. Furthermore, if node i (resp. j) is not a descendant of node j (resp. i), the simple classifiers for nodes i and j can be trained in parallel. Each simpler classifier can be sparsified using l1-regularization, which facilitates deriving interpretable models.  Their algorithm, TAO, systematically improves the decision rules at different nodes until the loss does not decrease substantially.  After the decision rules are improved, and  especially when sparsity is used, many nodes become irrelevant, which allows to effectively reduced the size of the original tree, again aiding interpretability.  The method currently lacks theoretical analysis.   Quality  The idea proposed is simple but seems very promising. Great job !   Using TAO won’t hurt accuracy, and it might even improve accuracy. The heuristic nature of their overall algorithm implies that, as far as the potential for improving accuracy goes, and at least until theory is developed, one needs to make arguments via numerical experiments.  There are two different kinds of experiments in this regard. The first type is starting with an existing decision tree learning algorithm, and showing how much their method can improve on the initial tree. The authors do these kinds of experiments. The second type of experiment is comparing their method with other methods for classification that are also heuristics and very fast, but that are not of the decision-tree type, and hence cannot be improved via their method. The authors mention comparisons with nearest-neighbor methods. Unfortunately, regarding this second kind of experiments, the authors could have done a better job.  It is vital that they compare CART+ their algorithm against other non decision-tree methods more extensively. A few of suggestions are cover trees [1], k-d trees [2], and the boundary tree algorithm [3]. These algorithms can be very trained fast, and classify points equally fast,  and have accuracy greater than 5% on MNIST.  [1]  @inproceedings{beygelzimer2006cover,   title={Cover trees for nearest neighbor},   author={Beygelzimer, Alina and Kakade, Sham and Langford, John},   booktitle={Proceedings of the 23rd international conference on Machine learning},   pages={97--104},   year={2006},   organization={ACM} }  [2]  @article{friedman1977algorithm,   title={An algorithm for finding best matches in logarithmic expected time},   author={Friedman, Jerome H and Bentley, Jon Louis and Finkel, Raphael Ari},   journal={ACM Transactions on Mathematical Software (TOMS)},   volume={3},   number={3},   pages={209--226},   year={1977},   publisher={ACM} }  [3]  @inproceedings{mathy2015boundary,   title={The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning.},   author={Mathy, Charles and Derbinsky, Nate and Bento, Jos{\'e} and Rosenthal, Jonathan and Yedidia, Jonathan S},   booktitle={AAAI},   pages={2864--2870},   year={2015} } ]   Significance  More numerical experiments are needed to assess how much their method can improve the existing heuristics for decision trees, and if these improved trees give a better classification accuracy than other non-decision-tree type of classification algorithms.  At this point, the method is significant because (1) it cannot hurt performance, and (2) it seems, at least empirically, that it can be used to produced compact trees with simple (sparse) decision rules at the nodes.  Originality  The idea is new as far as I can tell.  Clarity  The paper is very clear and well written. Great job !  A few suggestions  Line 127: do not use { . , .} for theta_i. Use instead ( . , . ). Line 132: I do not understand what theta_i \cap theta_j = \emptyset means. Line 189: using “fate” is confusing. Line 251: \lambda = 1/C should not be inside parenthesis but rather more emphasized Line 252: 0.5% of what? Line 337: it is misleading the use of “globally over all nodes”. The algorithm optimizes a global cost function. But it does so heuristically, by optimizing a few decision rules at a time. 