Summary:  This paper propose an unsupervised approach to estimate 3D facial structure from a single image while can predict 3D transformations that match a desired pose and facial geometry.  A Siamese-like architecture is used to predict relative affine transform  between the two given images and depth of keypoints of each one, based on which a matching loss is computed which drive the learning of the networks.  Novelty: This work propose a novel explicit 3D structural understanding for deformable face through 2D image matching (with or without keypoints representation) by inducing affine transformation with a direct closed-formed solution. It could be seen as an extension to unsupervised rigid scene based works, and has potential to further extend to articulated ones like human pose etc.   Reference: Overall, it is complete, and I think it could be related with Thewlis et.al. Unsupervised learning of object landmarks by factorized spatial embeddings. ICCV 2017, and their extensions. Both of the works use similar matching strategy, while the one I mentioned is more implicit.    Experiments: In general, it is complete by providing different variations of the method and out perform previous state-of-the-art (MOFA) by a relative large margin.  The only thing it needs some adjustment of table and figure since they are to small to look.      