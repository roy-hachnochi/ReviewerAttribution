This paper presents a method to estimate 3D facial structure (the depth of facial keypoints) from a single image while also predicting 3D viewpoint transformations that match a desired pose and facial geometry without using any ground truth depth data (unsupervised learning). The authors show the possibility to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3D affine transformation matrix that maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on a desired target facial geometry or pose. The proposed method can be used to infer plausible 3D transformations from one face pose to another, allowing faces to be frontalized, transformed into 3D models or even warped to another pose and facial geometry. Thorough experimental results on VGG, 3DFAW and Multi-PIE demonstrate the effectiveness of the proposed method. I believe this paper shows a promising approach for the face-related applications (e.g., depth estimation, 3D face rotation, replacement, face frontalization, pose-invariant face recognition) that I have not seen elsewhere so far.   The paper is written clearly, the math is well laid out and the English is fine. I think it makes a clear contribution to the field and can promote more innovative ideas, and should be accepted.  Additional comments: - The paper is missing discussions and comparisons with some related works and also does not cite several related works, e.g., FF-GAN [Yin et al., ICCV 2017], DA-GAN [Zhao et al., NIPS 2017], FaceID-GAN [Shen et al., CVPR 2018], PIM [Zhao et al., CVPR 2018], etc.  - It is not clear how the proposed method performs on instance of large variations in expressions, lighting and illumination.  - How did authors update each component and ensure stable yet fast convergence while optimising the whole framework?