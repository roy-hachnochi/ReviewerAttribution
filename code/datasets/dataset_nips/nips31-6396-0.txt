The paper presents improved high probability bounds on the generalization error for uniformly stable algorithms. The new bound is of order sqrt(gamma + 1/n) for the gamma-uniformly-stable algorithms, as opposed to the well-known (gamma + 1/n) * sqrt(n) of Bousquet and Elisseeff. The first consequence is that one obtains non-vacuous bounds for algorithms even when gamma = O(1/n^a) for some a > 0, as opposed to a >= 1 in previous bounds. This allows to show high-probability bounds for not necessarily strongly convex objectives. For example, as authors also note, Hardt et al. show that SGD on convex and smooth problems is 1/sqrt(n)-stable. The second consequence is that for regularized ERM one gets the correct shape (obtainable through uniform convergence type bounds) w.r.t. regularization parameter (lambda), i.e. 1/sqrt(n*lambda), which improves upon 1/(sqrt(n)*lambda) of Bousquet and Elisseeff. This was, however, also observed in prior works on stability. On a disappointing side, authors did not sufficiently discuss recent works in the literature on stability (and did not properly position their results), which obtain similar bounds albeit in slightly more restrictive settings. For example, in [18] authors derive similarly shaped bounds for algorithms that output hypotheses in Banach spaces Another result is by A. Maurer [*1] which analyses regularized convex ERM.  Technically, the paper shows these bounds by exploiting proof techniques used in differential privacy, and (rather compact) analysis might be of interest on its own. On this note, one question to authors: how easily this can be extended to weaker notions of stability, i.e. distribution-dependent ones? What are possible challenges?  In addition, the paper is well-written and presentation is well-done.  [*1] A. Maurer "A Second-order Look at Stability and Generalization" (COLT 2017) 