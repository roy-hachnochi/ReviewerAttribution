This paper studies estimation of the Shanon entropy for discrete Markov chains on finite state space of cardinal S, which sufficiently mix. The authors first give two kind of estimators for the Shanon entropy which are consistent in the asymptotic S \to \infty. Then they provide positive lower bound on the bias of the first one if the number of samples is of order S^2, and a minimax result for the second estimator. In a last section, the authors give some applications of their result to language modeling. More specifically, they estimate entropy rate of English based on two datasets to be able to discuss on efficacy of existing models.  Overall, I think it is a very nice contribution, well written. Authors' comments on their result are good. Finally, the numerical application of their results is interesting. The only suggestion I could make is that the author could illustrate their theoretical results by some numerical experiments.