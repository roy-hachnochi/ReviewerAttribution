       This paper introduces a way of optimizing the device usage for accelerating neural network performance in a heterogeneous compuatational environment. It combines        PPO and Cross Entopy Loss by taking a weighted average of them and optimizing the  "preference" vector. The most likely device is then chosen to be the argmin of the        obtained vector via the optimization process. The paper also tests the system "Post" on different heterogeneous environments with different architectures which shows        that the proposed system can work with different types of architectures.        However, the paper indeed has some flaws:        1. Figure 1 doesn't illustrate the author's intention properly. It's improper labelling is a hindrance to understanding what GPU ultimately gets chosen and how many GPUs are in the system.       2. In Figure 3 d) it is interesting that the post learns the pipelining operation, which are widely used in parallel computing. Since this device placement could also be viewed as scheduling problems, which could be converted as optimization problem. I wonder if author compared post with scheduling algorithms in parallel programming.         3. From the graphs provided in Figure 2 it becomes apparent that Cross Entropy beats PPO. Post combines both methods as a simple weighted average, but still beats        both methods. The reason why this works is missing in the discussion.         4. In Table 1 on single GPUs Inception architectures have nearly the same performance as "Experts". There is no explanation as to why this happens.        