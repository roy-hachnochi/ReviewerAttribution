This paper presents a new approach (MAPO) for policy gradient optimization in discrete and deterministic environments such as program synthesis (translating from natural language question to an SQL query). The main contribution is introducing a concept of policy memory that stores previously encountered high reward trajectories that is paired with a distributed sampling mechanism, a mechanism biasing initial sampling to high reward trajectories and systematic exploration of the trajectory space.  For someone who is unfamiliar with the specific field, the paper is a bit challenging to follow. The reader must have some familiarity with policy gradient methods and the state of the art to fully appreciate the authors’ contributions.  To evaluate their approach, the authors implement (with some modifications) a Neural Symbolic Machines architecture (Laing, ACL, 2017). This architecture first uses a sequences to sequences network to translate a natural langue question to a sequence of tokens that are converted to a valid SQL query using a List interpreter.    Two datasets are used for testing: WikiSQL and WikiQuestions. The proposed approach generally outperforms accepted baselines. The general trend is that MAPO learns slower early in the process but achieves higher overall accuracy in the end. MAPO also outperforms state of the art approaches on both datasets by a non-trivial margin.  Overall the paper is very well written (albeit a bit dense as a lot is covered from introduction of policy gradient, author’s innovations and specific NN implementation of the models). The field of automatic program synthesis is an important one and the proposed ideas can be applied to other domains exhibit similar phenomenology. Lastly, the proposed algorithm is demonstrated to outperform state of the art models on the benchmark datasets. 