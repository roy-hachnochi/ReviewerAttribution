Update:  I had doubts about the smoothing of PD using gaussians, which would normally lead to a continuous distribution even when the set \Theta is finite. However, based on the authors' response, it seems that a different choice of smoothing is used in practice (algorithm 1 , line 2), which is compatible with the results stated in section 3. However, this should be clarified in the paper.    The proposed kernel seems to take advantage of the particular geometry of persistence diagrams and leads to better results than more standard kernels (RB, ...).   Although it is not a breakthrough and the theory is already well establish, the contribution is relevant, the paper is clearly written and empirical evidence is strong. I vote for accepting the paper.   ------------------------------------------------   The paper introduces a new kernel to measure similarity between persistence diagrams. The advantage of such kernel is that it can be computed in linear time (approximately) and is positive definite, unlike other proposed kernels. The paper provides also standard theoretical guarantees for the generalization error. Experimental evidence shows the benefit of using such kernel in terms of generalization error compared to other existing methods. The paper is globally well written although section 4 would gain in clarity by adding some discussions about how these results, which are specific to the Persistence Fisher kernel, compare to more standard ones. Also, I think that the following points need to be clarified: - Up to equation (4) it seems like the fisher distance involves computing the integral of the square root of product of mixture of gaussians densities. However, starting from paragraph on Computation (line 148), this inner product becomes finite dimensional between vectors in the unit sphere of dimension m-1. This transition seems non-trivial to me, Does it comes from the Fast Gauss Approximation? If that is the case, it should be made more explicit that the remaining theoretical results, from proposition 1 to proposition 4 , are specific to this approximation?   - In proposition 1, shouldn't the eigen-decomposition depend on the probability \mu? It seems intuitive that the eigenvalues of the integral operator would depend on \mu which is the distribution of the data points as suggested later on in proposition 3 and 4. However, the result is derived for the uniform probability over the sphere, and later on in propositions 3 and 4, the same expressions for the eigenvalues are used for more generic \mu.   Propositions 3 and 4 deserve more explanation. How this result compares to standard one in the literature? In equation (9) the Radamacher complexity seems to be defined for the set of unit ball RKHS functions with an additional constraint on the second moment of f under \mu, what justifies such choice of set?