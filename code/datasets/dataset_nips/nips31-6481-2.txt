A key issue in using persistence diagrams as a topological summary in machine learning applications is define a positive definite inner product structure or kernel based on topological summaries. In particular using the persistence diagram is a problem as straightforward specifications of kernels based on persistence diagrams are not positive definite. In this papers the authors provide an approach that defines a positive definite kernel for persistence diagrams. This is useful and a genuine contribution.   There are a few concerns I have mostly in exposition.  1) Typically one starts an information geometric formulation with a likelihood and then the Fisher information matrix is defined based on this likelihood, this is the statistical perspective. Also one can think of the manifold of measures or densities parameterized on a Riemannian Manifold. The smoothed diagrams are not a likelihood in the sense of a data generating process. They may be a likelihood for the summary but again this is not stated. The comparison to the Lafferty and Lebanon work is interesting here but again the multinomial/Dirichlet likelihoods are data generating models.  2) The problem with the persistence diagram is fundamentally the space of diagrams with the Wasserstein metric or the bottleneck distance is problematic. It may be good to reference a paper that states the fact that this space does not for example have unique geodesics, they are CAT spaces bounded below by zero. 3) Does this new kernel matter, is it the case that empirically a more naive kernel based on persistence diagrams at the end of the day works just as well and in practice the empirical kernel matrices are positive (semi) definite.