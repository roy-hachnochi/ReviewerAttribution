This paper investigates cloning voices using limited speech data. To that end, two techniques are studied: speaker adaptation approach and speaker encoding approach. Extensive experiments have been carried out to show the performance of voice cloning and also analysis is conducted on speaker embedding vectors. The synthesized samples sounds OK, although not in very high quality given only a few audio samples. Below are my details comments.  1. The investigated approaches have been used in the ASR domain under the name of speaker adaptation for years.  When GMM-HMM was dominant, speaker adaptation was typically conducted by linear transformations. Since DNN-HMM became the state of the art, using speaker embedding vectors is one of the popular approaches for speaker adaptation. Among various speaker embedding algorithms, i-vector is the one that is widely used but there are also other speaker embedding and encoding techniques. Some of the techniques are actually very similar to what is used in this paper. For instance, [1]  Ossama Abdel-Hamid and Hui Jiang, "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code," ICASSP 2013. [2]  Xiaodong Cui, Vaibhava Goel, George Saon, "Embedding-based speaker adaptive training of deep neural networks," Interspeech 2017. From this perspective, the speaker adaptation techniques investigated in the paper are similar in spirit. That being said, ASR and TTS have very different lines of using speaker information, almost in the opposite directions. It is good to see those techniques can help the neural voice cloning. This usually didn't happen in the GMM-HMM domain.   2. In terms of speaker embedding, the argument of whether using a separate speaker encoder or a joint one is not clear from the paper. The authors claim that one of the drawbacks of I-vectors is to train a separate speaker encoder while the proposed framework can conduct the joint training. However, in the experiments, the observations on the joint training are that the estimated embedding vectors tend to smear across different speakers and a separate encoder has to be trained. This is a very common issue for joint training of speaker embedding with the main network. Given the nature of the problem, that is, using just a few samples to adapt a large network with substantial number of parameters, overfitting is the crucial issue. My sense is that there should be some way to actually isolate out the speaker embedding part to make it stand-alone.   3. I am confused by the cloning time for the speaker adaptation column in Table 1. Embedding-only takes about 8 hours with 128 parameters per speaker while the whole model takes half to 5 mins with 25M parameters. Is this a typo or I am missing something here?   4. It would be helpful to probably show the trend of performance using various amounts of cloning data. With only a few audio samples, the cloning may probably just pick up some very fundamental features of the speaker such as accent, pitch, gender, etc.. How would it perform in the given framework when presented large amounts of data? Will the performance keep improving or will it simply plateau at some point and never reach the stand-alone TTS performance?  In ASR, people usually show the trend with various amounts of adaptation data. For some techniques when having a large amount of data they approach the speaker dependent system while for some other techniques they will never reach that level. I think this scenario worth investigating.   5. Instead of just showing the text to generate the audio samples, it would be more rigorous to show the audio samples in, say, seconds or minutes. At the end of the day, it is this metric that really matters. 