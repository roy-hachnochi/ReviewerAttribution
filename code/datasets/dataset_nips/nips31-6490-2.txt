This paper proposed a concave-convex relaxation of Laplacian K-modes problem and solves it by optimizing an upper bound of it. The optimization is done in a fashion similar to K-means algorithm: alternating between modes and assignment vectors.  Experimental results show that the proposed approach outperforms state-of-the-art performance with shorter running time.  Strengths: This paper is well written and easy to read. The proposed relaxation is empirically proved to be tighter than state-of-the-art and its upper bound is easier to solve in parallel and therefore more scalable.  Weaknesses: 1. A interesting baseline to compare with (both in terms of running time and iteration complexity) would be some optimizer that optimizes eq (2) directly, which would provide more intuition about the effect of using this upper bound.  Response to Authors' Feedback: I totally agree with making an approximation on Eq (2) and I didn't mean to optimize Eq (2) directly since it would be impractical. What I suggested is to take a small problem and compare direct optimizer on Eq (2) and the proposed optimizer, which shows the quality of approximation on Eq (2).  2. Although some baselines wouldn't work on larger datasets, it is still helpful to consider datasets with larger number of clusters, e.g. imagenet with reduced number of samples and feature dimensions. This makes the story more complete.