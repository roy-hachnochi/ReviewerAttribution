Thank you very much for the response. This is an interesting paper to see in the conference. I'm still wondering if the model is relying on the repetitive patterns in the small training examples and curious to see your clarifications.  ================================ The authors propose a conditional text generation model which retrieve similar example from training data based on input and generate output conditioned on the input and the retrieved example. They kept their decoder to be a standard sequence-to-sequence model with attention and coping mechanism and put more effort into a retriever network. Their retrieval network is trained in two steps. They first trained a hyperspherical VAE model where its recognition model project an input and output pair to a point in the hypersphere. Then they trained a retrieval network which only has access by encouraging the model to point to the same place as the recognition model trained in the first step. They applied their model on code generation task and code completion task and obtained substantial improvements over baseline models.  The paper is clearly written. They explained their motivations and ideas well and demonstrated solid results with interesting examples from their model. Although there are many models which try to generate text with retrieve and edit framework, their model is different in a sense that they learned a task-dependent model for retrieval instead of using existing search engines or other heuristics.   To make the paper stronger, it will be great to see code generation experiments with different baselines, e.g. InputReader etc. For me, it is not clear why the InputReader is not enough as it has to contain the enough information to query.  Also, it is worth discussing if the same model is applicable for tasks with large corpora such as translation and dialogue generation. Although the results are solid in this paper, their dataset is quite small (only 500 training examples) and there will be a lot of common patterns in examples. The model is suitable for this kind of dataset as they need to run a search over all examples and it may suffer from searching over a large corpus.