The authors give a framework for improving the robustness of classifiers to adversaries, which is based on a Sparse Discrete Fourier transformation that is robust to worst-case L0 noise. The techniques, as you suggest, can correct the corruptions made by the adversarial attacks and produce an approximate image which is close to the original image. However, I have some concerns that need further clarification from the authors:  1.     Since your approach owns a benefit of generality, i.e., the framework is a general purpose, is it possible to attacks your algorithm in a targeted manner? In other words, if we input the same image twice, we can get the same approximation results in certain parameters setting, right? Thus, does it mean that your algorithm is deterministic? Is this the drawback that can be easily found or contradicted? For example, as shown in Figure 4, can we design an attack image as the input of Patchwise IHT algorithm, yet, output an image as exact as figure 4(b). 2.     In Section 4 Experiments, have you compared with other defended model e.g. against the L0 or L1 noise? If no, please perform it and make a comparison. 