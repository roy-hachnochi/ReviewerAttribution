The authors propose to speed up decoding by taking advantage of parallel generations via Transformer.  Specifically, by having models predict the kth next word in parallel and then validating those predictions in parallel, they are able to achieve a speed of 2m/k for a sequence of length m and prediction of up to k.   This is further reduced to m/k+1 via combining proposal and verification steps.  The primary use case is presumably for single sentence decoding because batch decoding won't have spare compute.    The result I find most surprising is that the models perform better with k>1.  Can this be justified better?  Can Fig 3 be augmented with or duplicated for MT?  My main concern is that I don't have a great understanding of where this technique fails?  Is there something interesting about the predictions that are shorter?  For example, for MT how much of the speed gains are just from predictable stop words or selectional preferences?   Given that there is basically an entire free page, I'd really like to have  1. Examples be provided for Super-Resolution since there is no direct evaluation 2. Examples of how Both succeeds for ave k > 4 while regular/distillation fail.