This work describes a novel approach to automatic differentiation as a purely local program transformation on an IR with delimited continuations, and a code-generating deep learning framework built on this idea in Scala that achieves the staged-programming performance benefits of TensorFlow and the natural, language-native expressivity of PyTorch. This represents a compelling convergence of ideas from the programming languages and machine learning communities, and the prototype presented (along with related papers/prototypes by Conal Elliot, Richard Wei, and Olivier Breuleux) suggests numerous promising directions for future work in deep learning frameworks and systems. The paper itself is impressively clear, with my only clarity quibble being that the notation in Figure 3 should be explained more completely for readers not familiar with the PL literature.  Some things I noticed/questions I had:  - The use of continuations for AD in deep learning, while new to the literature, is not new to the practice. The popular Cython-based natural language processing toolkit SpaCy uses a simple continuation-based AD approach in its thinc (https://github.com/explosion/thinc) deep learning library. This should probably be mentioned.  - It's not so much the case that delimited continuations allows AD without any auxiliary data structures like tapes, as that it allows reusing the language-native call stack itself as the Wengert list/tape.  - If the only problem with the Pearlmutter and Siskind (2008) approach is that it requires nonlocal program transformation, isn't that solved more simply by extending the single-assignment condition to a single-assigment single-use condition by making computation graph "splits" explicit, which I believe allows the AD to be purely local?   - The speed comparison should probably have included DyNet at least for the TreeLSTM, as that's widely seen as the fastest define-by-run framework for NLP tasks.