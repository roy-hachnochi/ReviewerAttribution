Edit: My score remains the same because no additional experimental results are shown at the time of author rebuttal.  Summary  This paper proved a novel error bound of the solution of kernel ridge regression learned by stochastic gradient descent and random features approximation.  The first theorem proved the high probability bound calculated from batch size, learning rate, iterations, sample size and the dimension of approximation feature. They demonstrated the several settings that only O(N^(1/2)) features are required to ensure O(N^(-1/2)) error, resulting in O(N^(3/2)D) time complexity and O(N^(1/2)D) space complexity. This is smaller than existing O(N^2) time complexity and O(N^(3/2)) space complexity when D<<N.  They also demonstrated the refined analysis using the decay rate of the eigenvalues of integral operator using data distribution and kernel function and moreover the case that learning rate decays as the iteration grows.  They experimentally demonstrated using SUSY dataset that the relationship between feature dimension and passes over the data, and step size and batch size to reach the same error coincides with the proved bound.  Qualitative Assessment  The analysis when we trained Kernel Ridge Regression with stochastic gradient descent and random features seems novel. Actually their derived bound is useful to design the training setting and resulting in smaller time and memory complexity.  Though I have no doubt about the value of the proved bound itself, I do not understand well enough the novelty of the analysis itself. Do the paper propose some novel proof technique, or is the proof simple combination of existing analysis of stochastic gradient descent like [15] and analysis of random features like [23]? In any case, I think the proof itself is valuable.  It seems that more experimentally evaluation will be useful. They fixed the number of sample to 10^5 and relations among other parameters. I think the relation between the number of samples and other parameters such as the feature dimension is more userul. The comparison of accuracy and computation time to other methods such as kernel ridge regression with random features will be helpful to show the superiority of the proposed method.  Also, the experimental setting corresponding to Theorem 2 and 3 seems missing.  Overall, I have small doubt about the novelty of proof technique and they lack experimental evaluation, but the idea and proved results are enough important for acceptance.