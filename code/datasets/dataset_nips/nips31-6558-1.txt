The paper builds on previous work (RealNVP (Dinh et al)) on deep generative models with tractable log-likelihoods. Each layer in RealNVP permutes or reverses the channel dimension and then applies a coupling layer that is invertible.   The paper's primary contribution is the addition of a learnable 1x1 invertible convolution to replace fixed permutation matrices used in RealNVP. This applies a linear invertible transformation to the channel dimension and can be seen as a generalisation of permutations. In addition, the paper uses activation normalisation instead of batchnorm. This allows the use of batchsize 1 per GPU and hence allows scaling to larger image sizes. The paper uses gradient checkpointing to scale to move GPUs and deeper architectures.   The experimental evaluation is thorough. In a first set of experiments, the authors compare 1x1 to fixed permutations or simply reversing the order. The results show faster convergence and better final performance. Note that 1x1 convolutions linearly blend the different channels, thus allowing different dimensions to affect one another after fewer steps. It could be interesting to study the performance of a fixed 1x1 convolution (say, a random orthogonal matrix) as an additional baseline. Glow outperforms RealNVP in terms of log likelihood on a variety of standard benchmarks.   The qualitative section is also very interesting: samples from the models look (subjectively) very realistic, and interpolation in latent space works well. Interestingly, it is possible to vary attributes by perturbing the latent space, using a simple procedure proposed in the paper. Since the paper uses tricks to scale to deeper models than previous work I would be interested in a qualitative comparison with RealNVP. Does Glow beat RealNVP visibly in terms of sample quality and interpolation in the latent space? Is the latent space manipulation proposed for Glow also successful for RealNVP?   The paper is very clearly written. While the addition of 1x1 convolutions seems relatively straightforward I believe because of the impressive empirical results both in terms of sample quality and learnt representations this paper will be of big interest to the NIPS community and generate a lot of follow-up work.   Thank you to the authors for their response. I continue to think that this paper will be of interest to the NIPS community and look forward to reading the final version. 