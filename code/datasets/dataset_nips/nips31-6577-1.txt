Response to feedback: Thanks for your feedback and especially for clarifying issues around reproducibility and details concerning the hypernymy task and comparison with the N-K reference.  Many of my questions stemmed from having looked at their arxiv version and not the final NIPS version which indeed has some of the clarifications you mentioned in the rebuttal.   It is reassuring that the code for your experiments will be made available on-line.   On the negative side, the rebuttal failed to address my concern that some discussion should have been provided in connection with the most compelling set of experimental around the hypernymy task, about the source of these gains and how they might be grounded in properties of the proposed elliptical embeddings.   Are these gains also possible with the Vilnis-McCullum approach?  Or will we see similar inconclusive comparison as in Table 2?  Main ideas: The authors study embedding into the 2-Wasserstein metric space of elliptical probability distributions whose pdfs have ellipsoidal level sets.  The simplest case and of main focus in the paper is that of uniform distributions over ellipsoids.  Such distributions are characterized by a mean vector and covariance matrix and the 2-Wasserstein distance between any pair can be expressed in closed form in terms of these parameters, as established in the literature. Also recalled from the literature is the Bures product which is a counterpart of the Euclidean inner product in these spaces.  A factorized representation of the covariance matrix and the corresponding steps to efficiently carry out SGD (wrt to this parameterization) on functionals of pairwise embedding distances and/or Bures products is described.   Embeddings are computed for several different applications involving multidimensional scaling, word embeddings,  entailment embeddings, and hypernymy where the latter refers to 'type of' relationships among nouns, as conveyed by a directed acyclic graph WORDNET.  Relation to prior work: Prior work of Vilnis and McCallum proposed embedding into diagonal Gaussians with KL divergence metric.  It is claimed that proposed approach avoids some drawbacks of KL divergence such as not blowing up as Gaussian variances decrease and a graceful reduction to vector embedding with Euclidean metric in such cases.  Strengths: 1. Excellent presentation of all the necessarily tools from the literature that are required for optimizing embeddings, from the closed form Wasserstein distances, to the algorithm for computing matrix square roots. 2. Nice complexity reduction trick via (7). 3. Significantly improved experimental results for Hypernymy modeling over prior work, as shown in Figure 6.  Weaknesses: 1. Concerning K-L divergence, though it blows up with decreasing variance, it does behave like Euclidean distance between means scaled by the inverse of the variance.  2. Insufficient details for reproducing word embeddings experiments such as the data set used for training, parameter initialization, context window size, number epochs over data set, etc.  3. The experimental results on word embeddings in Tables 1 and 2 are not very compelling as compared to Gaussian embeddings, the chief baseline from the literature.    4. For the Hypernymy experiments some confirmation that negative examples (for training and eval) for a relation (u,v) include (v,u) and (u,u) would be reassure that an apples-to-apples comparison is being made wrt. Nickel and Kiela 2017.  Also missing are implementation details like how parameters are initialized, number of negatives per positive, training epochs, etc.  Also no explanation of choice behind have one embedding for context and target and RMSProp vs Adagrad in the word embedding experiments.  5. Given the significant improvement over Nickel-Kiela Hypernymy results, there should be some attempt to explain the source of these improvements and tie them back to properties of the proposed embedding.  This is critical especially given that the Bures product is symmetric while Hypernym relations are not.  That the Mean Rank drops to 1 is especially surprising given this.  6. Some discussion about difference between (6) in Nicekl Kiela and l. 303 loss would be nice.  In N-K (6), the positive example contribution does not appear in the denominator whereas in l. 303 it does.  Typos:  l. 18 'entanglement' -> 'entailment' (I think) l. 155: repetition of text starting on line 146 l. 310: 'embedd' -> 'embed' l. 323: 'interesections' -> 'intersections'