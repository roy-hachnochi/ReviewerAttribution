The paper proposes a method to embed objects into uniform elliptical probability distributions. It, then, proposes to use the Wasserstein distance in the embedding space to compute the discrepancy between distributions, which is advantageous to alternatives such as KL divergence, but is heavy to compute. Selecting Gaussians as target distribution solves the computational burden by offering a closed form solution for this setting. The model is used for learning word representation and in a visualization scenario.  - I find this an interesting paper. Many nontrivial technical observations are highlighted and fit well together.  This is a nice continuation of the works on embedding to distributions (as opposed to points) and also a natural and good application of Wasserstein distances. Perhaps will bring interest to the paper as a reasult. - To my knowledge the method proposed is novel, and seems appropriate for NIPS. - Figure 1 is nice, informative and useful.  - The method is kind of limited, in that, it works only for embedding into a specific target distribution, but is still a good start for this direction of research. - While embedding to "uniform" elliptical distributions is the one figured out in the paper, the title of the paper does not reflect that, which a little looks like an oversell.  - The equation after "minimizing the stress" in line 242 has a typo. - The related work is covered well overall. However, a related work which worths mentioning is [1] below, in which object classes are embedded into high dimensional Euclidean norm balls, with parametrized center and radii. The geometry enables for logical constraints such as inclusion and exclusions between classes to be imposed (like in a Venn diagram) and is guaranteed at test time. The relevance of [1] is two-fold, first the embedding is performed into a uniform elliptical. Second, the future works in the current submission mentions that the authors are considering imposing similar logical constraints, which is addressed there. - The experimental results are acceptable. However, the visualization issue which is brought up is not developed enough and arguments are not convincing.   The paper is well structured and overall well written. However, the text write up seems rushed, with a large number of spelling problems in the text. - minor:  "the interest of" -> the interestingness of? "entanglement" -> entailment "matrix their covariances" "gaussian" "euclidean: "embedd" "asymetry" "sparameter" "prove to expensive"  Overall, I vote for the paper to be accepted after proofreading.  ----------- [1] F. Mirzazadeh, S. Ravanbakhsh, N. Ding, D. Schuurmans, "Embedding inference for structured multilabel prediction", In NIPS 2015. ------------------------------------- After reading other reviews, author rebuttal, and reviewer discussions, my remaining concerns are: - The visualization arguments can be improved. - Citation to [1] above is missing. - Intuitive discussion about why the method proposed should have advantages over  the main competitors would be useful. The rest of concerns raised in my review is addressed in the rebuttal.