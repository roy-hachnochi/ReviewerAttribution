The authors present an unsupervised learning framework to recognize actions in videos. They propose to learn a representation that predicts 3D motion in other viewing angles. They leverage a view-adversarial training method. They run experiments on multi-modal datasets for action recognition.  Pros: - Unsupervised learning for video is a relevant problem - The suggested surrogate task that learns view-invariant representation is interesting. - It s interesting to see that their proposed method based on flow input outperforms other methods...  Cons: - l25: The claim for view specific is debatable. Some of the previous works, e.g.. [31], models 3D information to be view invariant…Their performance is the same the proposed method although the authors report different number (see next bullet).  - In table 3&4, why are the authors re-implementing  [31] and [32] since the exact numbers are available in the original papers for the same used datasets? In fact, the authors are reporting numbers for other methods AND NOT for these 2 ones. Why not do the same for 31 and 32? At least, the authors can share the reported numbers as well as the ones with their own re-implemented version. Discarding numbers is not correct. Similarly for table 4, the performance  for [4] is different from previous papers. Why is it 80 instead of 85? That again communicates a questionable performance evaluation.  Because of the above weaknesses, I have strong doubt on the performance evaluation section. I have identified few inconsistencies. Hence,  I am not confident that the proposed method outperforms previous methods. 