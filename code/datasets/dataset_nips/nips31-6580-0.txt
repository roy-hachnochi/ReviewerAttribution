This paper describes a method for regularizing an agent’s policy to either advertise or hide the agent’s goal from an observer using information theoretic regularization. They consider an MDP where a goal is revealed only to one agent, Alice, but not to an observer, Bob. If Alice is told Bob is a friend (foe) her policy is regularized to maximize (minimize) the mutual information between the goal and either the actions she takes or the state.  Significance:  I’ll start by evaluating this paper’s significance because I think this is where it’s biggest issue lies. To me, this paper reads as if the authors have a solution without being quite clear on what the problem is. A loss function with mutual information regularization is presented the solution but we are never formally told what the problem is. We are only told that Alice wants to make Bob’s task of inferring her goal either easier or harder depending on whether Bob is a friend or foe. Information regularization sounds like it might be the right solution but I would have liked to see a formal derivation.   The reason I think this is important (and not just unnecessary formalism) is I’m not convinced that this solution solves that problem laid out in the introduction. The stated high-level goal was to train an agent such that the observer can infer its intentions from its actions (so that human operators can understand them). But this solution requires supplying the agent with a goal (because it forms part of the regularizer) while not revealing the goal to the observer. I don’t see how this aligns with the motivation that the observer is a system designer who doesn’t understand the agent’s intentions? Why wouldn’t the system designer know the agent’s objective function? The technique seems more useful in the competitive setting where the agent is trying to hide information (there the observer is the opponent), but I would have liked to see a motivating game (in the game theoretic sense) for which information regularization is a good (optimal?) strategy.  If we accept that this is a solution to a useful problem the paper is well written and seems technically solid (albeit with an empirical evaluation that focuses only on toy problems). But I’m not convinced it is, so I think in it’s current form it is a good workshop paper that needs a clear motivating example to be turned into a NIPS paper.  Quality:   - The paper derives the appropriate policy gradient updates that are required in order to apply mutual information regularization. The derivation is relatively lengthly, but as far as I can tell follows standard score function estimator-style arguments.  - The experimental evaluation tests the effect of adding information regularization two simple grid world problems. The spatial navigation task gives clear intuition for how the regularization breaks ties in an under constrained problem while the key-door problem shows the trade-offs between information regularization and optimizing the efficient direct policy.   - I liked the toy problems for giving intuition about how the method works in very simple domains, but I’m not sure what the experiments are testing beyond “is our Monte Carlo policy gradient update correct?”.   I would have liked to see some evaluation that answers “is this the right approach to the problem?” against a strong baseline. Again, it’s tricky to suggest what this experiment would be without a clear motivating example (these toy problems don’t make to the system-designer / observer problems laid out in the intro).   - These are very small toy problems and there was no mention of how the method scales to more real-world problems in higher dimensions.  Clarity:  The paper is well-written and clear.  Originality: I’m not aware of previous work with exactly this approach, information theoretic regularization has been used in [Florensa, Duan, Abbeel 2017] (which should have been cited).   Typos: Line 69: citation should be before the fullstop Line 70: “relaxes the assumption *to* that Bob…” remove to Line 82: “Alice’s actions or *states*..” 