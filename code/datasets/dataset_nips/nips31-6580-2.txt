Summary:  In the framework of a (multi-goal) MDP, one agent (Alice) samples and optimizes a goal, while another agent (Bob) is trying to learn the goal Alice is optimizing from observing states and/or action trajectories. The authors propose two methods, based on mutual information, that allow Alice to learn to reveal (cooperative) or to hide (competitive) her intentions (goal) from Bob while training. Both methods are based on the idea of information regularization, where the objective function of Alice includes a mutual information term; both cooperation and coordination can be handled in the current framework by controlling the sign of the regularization coefficient. However, the methods differ in that the first method, called “action information regularization”, optimizes the mutual information between goal and action given state, whereas the second, called “state information regularization”, optimizes the mutual information between state and goal. The authors show how the two forms of the mutual information can be optimized within a policy gradients (REINFORCE) framework, and propose two algorithms to do so. They illustrate the effectiveness of Alice’s approach in both cooperative and competitive settings across two small examples.  Contributions:  The main contributions of the paper are as follows:  1. Two methods, state and action information regularization, are proposed to allow an agent to reveal or hide their intentions in the presence of information asymmetry, in the policy gradient setting, and is scalable for deep reinforcement learning  2. By following information regularization, the current method allows the agent to cooperate (signal intentions) or compete (hide intentions) by choosing a positive or negative regularization coefficient  Overall comments:   I think this is an exceptionally written paper, with a strong motivation and novel theoretical and algorithmic insights. I very much like the information theoretical approach presented here for policy gradients, and how it is able to handle both cooperation and competition, in a very cohesive mathematical framework. In particular, the information penalization method proposed here addresses clearly the tradeoff between optimizing immediate reward and the long-term effect of hiding or sharing current information that can influence the long-term reward. I would like to re-iterate that the ability to handle both cooperation and competition cohesively is a very strong contribution in my view.  Motivation:   The motivation presented in the current draft is quite strong. However, I do not believe that the authors have argued why their work is relevant for real-world practical examples that would be difficult to solve with existing methods, but have instead only provided abstract and general arguments.  Empirical Evaluation:   The numerical examples considered are simple but were well-chosen to illustrate the main features in the current framework and lead to some insightful conclusions. In particular, the second domain clearly illustrates the tradeoff that can exist in many problems between a higher immediate or short-term rewards with the long-term rewards that can be gained by hiding or sharing information.   Some general comments:  1. It is interesting to see that in both domains, the ambivalent strategy and cooperative strategy perform roughly the same from Bob’s standpoint. It seems plausible to view the ambivalent strategy as a combination of the cooperative and competitive strategy, in that some information regarding the goal is revealed gradually as opposed to in the beginning, as in the cooperative case. My guess is that for larger domains, the difference between the two would become larger, but it is not visible here. Perhaps, if the authors would consider a somewhat “scaled-up” version of the problem(s) (say a 10-by-10 grid-world), the distinction would be clearer. But this is of minor importance.  2. In both examples provided in the current draft, it is clear that Alice is able to hide her intention clearly from Bob and to increase her chance of winning. In the grid-world example, Alice can defer the left or right actions until near the goal. In the key-and-door example, Alice can take the longer route and select the master key. However, there are clearly cases where the opportunities to hide or reveal intentions can be quite limited and ambiguous, due to the structure of the problem. In a noisy grid-world example where actions are no longer deterministically selected, it may be more difficult to reveal intentions with the current approach.  In the current grid-world except where the goal states are moved closer and stacked vertically with respect to the initial position, it may not be possible to communicate the intention to Bob until Alice is close to the goal. Such features can make it difficult for Alice to communicate her intention when goals are relatively indistinguishable with respect to KL-divergence, in an information theoretic setting. To me, it is not at all clear how the algorithm will behave and whether or not Bob will be able to respond well to Alice’s attempt to reveal or hide information once both agents are close to the goal. I think it might be important to provide examples or reasoning to highlight and clarify these issues with respect to the current method.   Minor comments:   On line 221, the reference to Figure 4.1 should probably be changed to 4.2. Similarly, from lines 251-259 the references to Figure 4.2 should likely be 4.3. 