The authors present a solution for a difficult problem: how to use the information conveyed by a complex classifier, when practical circumstances require that a simple classifier is used? Now one might question why this is a problem at all, but the current development of very complex classifiers seems to suggest that more interpretable classifiers would be generally welcome as long as the accuracy penalty could be minimized. This is what the authors try here, by importing what seems to be an intuition behind curriculum learning: they feed an interpretable classifier with easy examples, where "easy" is indicated by the complex classifier. The authors show that this intuition works by presenting some empirical analysis that is very interesting and some theoretical analysis that I don't find very convincing. Mostly the proof of this idea is in empirical success in useful applications, for I could easily imagine an "intuition" that would point in exactly the opposite direction: that is, one might argue that the interpretable classifier should be fed with HARD examples that would more precisely specify the classification boundary. All in all I think the authors have succeeded in showing that, at least in some situations, their idea makes sense; more work is needed to actually understand when this idea works.  The text is well written and flows quite well, even if it uses some (perhaps inevitable) jargon that ranges from boosting to probes and to many other notions. I think the presentation is quite effective overall.  One point: the text mentions "Line 7" of Algorithm 3, but it would be better to simply say "Step 4".  Also, I believe Ref. [4] is by "Caruana". And there are problems with capitalization in the references. 