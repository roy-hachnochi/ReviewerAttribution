 The paper aims to learn simpler classification models by leveraging a complex pre-trained model. The task is achieved by leveraging the representations learned by the intermediate layers, and learning the weights for individual data points that are then used to train a simple model. The methodology is quite intuitive and results do show the potential of the proposed scheme. However, the paper still needs to address a number of issues (detailed below). On a high level, the paper can benefit from a better positioning w.r.t. related work, a more comprehensive evaluation, and a bit more work on the clarity of the text.  Detailed comments:  - Line 27: The reason does not sound that compelling. Perhaps the authors want to hint at the "interpretability" of decision tree style models since one can see how the model is classifying examples (if feature_1 <  a and feature_2 > b then classify as +ve). If a model (no matter how deep or "complex") provided interpretability as well as high accuracy, there might be no reason as to why domain experts would not opt for that model.   - How does the technique compare to that of "Adaptive Classification for Prediction Under a Budget" from NIPS'17 (https://arxiv.org/pdf/1705.10194.pdf)? This technique also consists of training complex models, and then training simpler models to approximate the complex one in appropriate regions of the feature space.   - Line 53: Are these real outputs of a network? Specifically, does the accuracy always go up as we move more towards the final layers (intuitively, it sounds like that should be the case)?   - Line 69: If the simple model only focuses on the easy examples, then what happens to the hard one? Also, if would be nice to formalize how focusing on the simpler examples improves generalizability.   - Line 112: What is the rationale behind training the complex model on a different dataset? Providing smaller dataset to the simpler model sounds like already setting it up for failure.   - Line 118: Softmax probabilities are not necessarily well-calibrated (https://arxiv.org/abs/1706.04599). Does that make any difference on the performance of the proposed method? Also, in line 167 in the evaluation section, since the last probe is not computed in the same way as the traditional backpropagated softmax probabilities, is are the probabilities provided by these probes expected to be well-calibrated?    - Table 1: The resnet paper (https://arxiv.org/pdf/1512.03385.pdf) report very different accuracy on the CIFAR-10 dataset as compared to what is observed here. Any particular reason for that?   - Algorithm 2: Referring to the quantity computed here as AUC is very confusing. Are there any reasons about why this quantity corresponds to the traditional ROC-AUC?   - Algorithm 3 is very difficult to understand and potentially very confusing. In step 3 of the algorithm, when computing the new weights \beta^i, why is the previous model S_{w^i-1} passed to the loss function?   - Line 130: The paper notes that the weights should not all go to zero, but provides no insight as to why that might happen. Also, is there a possibility of weights becoming too high?   - Section 2.2: It is not clear what part of line 7 in Algorithm 3 this section is trying to justify. Also, what precise implications do Theorem 1 and Lemma 2 have for line 7?   - Line 203: Are the probes not convex? Then why not train the probes to convergence?   - Line 245: Since the time since last cleaning is a feature that depends on time, wouldn't taking the last 30% samples as the test set induce some temporal bias in the evaluation? The concern might be alleviated by shedding more light on the feature "time since last cleaning" since that might affect the quality of the product.   - Section 3.2: What would happen if the simple model was something other than CART. E.g., a linear SVM (which one could argue is still fairly interpretable -- specially if one were to introduce L1 regularization to zero out weights for unimportant features). 