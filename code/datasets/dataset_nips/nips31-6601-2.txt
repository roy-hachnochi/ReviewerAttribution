The paper gives a formal basis for the explanation of those techniques which operate pca to visualize the parameters of a neural network and how do they change during training. In the details, they show that  PCA projections of random walks in flat space qualitatively have many of the same properties as projections of NN training trajectories. Substituting then the flat space with a quadratic potential, the Ornstein-Uhlenbeck process, which is more similar to NN optimization. This paper is a real pleasure to read. It tells things in a formal yet comprehensible fashion. Nonetheless, at the end, all of the formalization of the paper misses in my opinion what is the net value, apart for the formalization which is nice.  Is it true that at some point I can do a sort of forecasting on how parameters will change during the training? Is there any way to decide, given the type of trajectory followed by the parameters, an early stop of the process of training? This can be put in the conclusions, which are very short and not very informative.  