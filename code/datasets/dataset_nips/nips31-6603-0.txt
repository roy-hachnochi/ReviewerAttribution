The paper discusses the problem of solving SMT formula by means of combinations of operations on formulas called tacticts. Such combinations form simple programs called strategies operating on a state represented by the formula. The paper presents an approach for learning the strategies. The approach works in two phases: first a policy is learned, that defines a probability distribution over tacticts given the state, then this is transformed into a set of sequences of tactics that are combined into strategies by adding branching instructions. The approach is experimentally evaualated by comparing its performance with respect to hand crafted strategies using the Z3 SMT solver. The learned strategies are able to solve 20% more problems within the timeout and achieve speedups of x1000 in some cases.  The idea of applying machine learning to induce SMT strategies is very promising, with several works pursuing it. The paper is clearly distinguished from previous work for its focus on strategies, simple program combining elementary operations. The only other work performing strategy learning uses genetic algorithms. The experimental results are impressive and show that the approach outperforms evolutionary approaches that get stuck in local minima.  However, the presentation left some points not sufficiently clarified. In particular, you say that you first induce policies that are probability distribution over tactics given the STATE, which means the formula. In other points it seems that policies are probability distribution over sequences of tacticts, please clarify. How do you convert policies to sequences of tactics? Do you replay the policies over the training formulas? What is the difference with using your procedure for generating the training set to generate the best sequence of tactics for each formula and use those as input for the second phase? In your second phase you do not consider the repeat instruction to build the strategies, why? The paper also contains notation imprecisions: the log-likelihood formula for the bilinear model is incorrect, as y_i is Boolean and the output of \sigma(UV...) is a vector, please correct, I guess you want to use cross-entropy. At the end of Section 4 you say that "try s for c" is a predicate, while it is a strategy (Table 1).  The difference between the combination of tactics with ; or with or-else is not clear: on page 8 you say that for the strategy “simplify(local_ctx : true); sat; bit_blast; sat“. " either the first one already solves the formula or it fails in which case bit_blast; sat is executed" so ; and or-else seem the same.  Please clarify what are the speedup percentiles in Table 2