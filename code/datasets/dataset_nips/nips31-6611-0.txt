The paper develops a connection between quantum graphical models and inference with Hilbert space embeddings (HSE). It shows kernel formulations for sum rule and Bayes rule with quantum graphical models, connecting kernel mean embeddings as well as cross-covariance operators. The paper proposes to use HSE on top of HQMMs, arguing that it improves empirical performance against PSRNN and LSTMs.  The paper is nicely written and does a very good job at explaining difficult quantum concepts using tools that should be familiar to a machine learning audience. I personally really enjoyed reading and learning about the connections developed in the paper. I also think the work is very original and could lead to interesting follow up work.  I have a few questions regarding the experiments:  - Why not compare with a simpler baseline, such as regular HMM?  - There are plenty of existing standard datasets for sequence prediction / modeling, yet none of the experiments in this paper seem to use any of them, which makes it harder to convince others that the improvement is not because of weak baselines and unusual train/test splits.  - What are the per iteration BPTT efficiency and modeling complexity for HSE-HQMM compared with other baselines, namely LSTM and PSRNN?   - I am rather confused by the PTB experiments. The log-perplexity for all these methods are no more than 2, which means that the perplexity is below 10. However, the current SOTA for PTB perplexity should be significantly higher (in the 30-50 range).  Minor points:  Heat map resolution and aspect ratio. Typo on line 106: embeddingsx   ==========  Thanks for the clarifications!