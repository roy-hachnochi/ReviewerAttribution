This paper proposes a new measure of fairness for classification and regression problems based on welfare considerations rather than inequality considerations.  This measure of fairness represents a convex constraint, making it easy to optimize for.  They experimentally demonstrate the tradeoffs between this notion of fairness and previous notions.  I believe this to be a pretty valuable submission.  A welfare-based approach over a inequality-based approach should turn out to be very helpful in addressing all sorts of concerns with the current literature.  It also provokes a number of questions to follow up on which, while disappointing that they are not addressed here, means that the community should take interest in this paper.  I am not at all convinced that this is really an 'individual' notion of fairness.  The requirement is to ensure only that the sum of individual utilities is sufficiently high, rather than enforcing that every utility is high (as would be the case if you forced \alpha=-\infty), or something similar.  Thus I think it is not particularly appropriate to call this the "first computationally feasible mechanism for bounding individual-level (un)fairness."  While on the subject of the abstract, the abstract states "our work provides both theoretical and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes."  Empirical evidence is certainly provided, but to what theoretical evidence does this statement refer?  The connection to Atkinson's index?  I ended up waiting for generalization results that didn't exist in this paper, so that could be cleared up.   A few very small things: - It looks like there's overlapping notation between Pareto domination of the benefits and the welfare's total ordering (line 206) - "consistently" --> "consistent" (line 271) - Figure 2 has "accuracy" where the Dwork measure should be instead