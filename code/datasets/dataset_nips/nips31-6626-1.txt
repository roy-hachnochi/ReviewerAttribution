The paper studies the robustness of classifiers to adversarial perturbations in the (simple) setting where all the input features are binary. It provides upper bounds on the number of features that the adversary must flips to make the prediction irrelevant. The paper also provides empirical comparison of their measure of robustness with other measures of the literature. The proposed measure seems more appropriate, as it measures the risk compare to the ground truth  As a reader unfamiliar with the adversarial attacks topic, I enjoyed the fact that the paper is really clearly written. It is easy to follow without being trivial, and provides both theoretical and numerical results.   One may complains that the simplicity of the setting clash with most nowadays machine learning applications. From my point of view, the less realistic assumption is that the data distribution is uniform over the input space. I would like the authors to comment on this point. Can this assumption be relaxed?  That being said, the fact that the problem is well circumvented allows to study it with rigor.  Hence, it appears to me to be the kind of work that may be the starting point to study more complex frameworks.  Other comments and typos: - Page 3: Please provide details about the experiments of Table 1 - Page 5, Line 5: missing reference after "robust optimization"  - Page 5, Line 7: "definitoin" (typo) - Theorem 2: Please state that "HD" stands for the Hamming distance metric, and thus r is an integer value 