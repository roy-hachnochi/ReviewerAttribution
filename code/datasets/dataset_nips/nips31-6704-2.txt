UPDATE: Read the review, still think this is a good paper. Score remains unchanged.  The paper proposes a discretization scheme for variational inference with continuous latent variables. ELBO computations are reduced to summations, as distributions become high-dimensional tensors. While the approach may sound questionable at first due to the curse of dimensionality, the authors convincingly demonstrate that all relevant objects in mean-field variational inference appear to be low-rank tensors, whose inner products can be carried out in linear time of the latent variable dimension (as opposed to exponential, when done naively).   Furthermore, the authors show that data-dependent terms can be efficiently precomputed, such that the resulting optimization problem scales independently with the number of data points. I found the results of the paper very surprising and interesting.   I have two comments. First, the notion of a discrete relaxation is confusing; a more natural terminology would be a discretization. Second, it is certainly misleading to claim that the resulting gradient estimators are unbiased. Maybe the authors should stress that there is still a bias in the approach.  I could not identify any obvious flaws in the paper, although I did not check the math in great depth. This is potentially a very interesting new research direction, which is why I consider the paper a significant new contribution. 