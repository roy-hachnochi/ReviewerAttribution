This paper proposes to estimate the Gaussian process (GP) mean and covariance function using previous observations generated by functions sampled from the GP, and then use this estimated GP prior to conduct Bayesian optimization on a new test function, which is also assumed to be a sample path from the same GP.   The main contribution of this paper is the derivation of the (simple) regret bounds of such a procedure using a special setting of the Upper Confidence Bound (UCB) or Probability of improvement (PI) policies, on both finite and compact domains.   The idea of such transfer learning seems to be a formalization of the recently proposed BOX algorithm [24] and that of Google vizier [16] (except here is not sequential), and is generalized to continuous domains. The generalization is simply degenerating the GP to parametric Bayesian linear regression in a nonlinear space transformed using given basis functions (in experiments, the basis functions are learned using a single-layer neural network by minimizing the average loss over all training sets).    The regret bounds seem to be novel. I would like to know how this theoretical analysis is related to, e.g., that of Srinivas et al. (2010) [42]. Though one being simple regret and the other cumulative regret, you both analyzed the UCB policy and the notion of information gain is involved in both.   The paper is well-written and easy to follow.   I'm a little concerned about the experiments. In the first simulated experiments, where the true GP is known, the PlainUCB using the true GP appears to be significantly worse than your algorithm, which uses the estimated GP. I would expect the two lines in Figure 2(d) to be very close if there is enough training data to estimate the GP (or at least PlainUCB shouldn't be worse). Perhaps this is only due to lack of repetitions. Could you conduct more trials to reduce the variance and see if you can verify these?  In the second and third experiments, the comparison seems unfair for PlainUCB since an inappropriate (as also noted by the authors) square exponential kernel was chosen. In practice it's true that we may not know what kernel is appropriate, but at least we could avoid using a kernel known to be inappropriate, unless a better one is not known. Another comparison that I think would also be interesting is what if you also tune the hyperparameters of the PlainUCB kernel using the training data by empirical Bayes?  In Figure 2(f), how come the line of PemboUCB started off much higher than others? Shouldn't they all be initialized using the same set of seeding points? And "rand" (I suppose it means random?) is better than the other two baselines? 