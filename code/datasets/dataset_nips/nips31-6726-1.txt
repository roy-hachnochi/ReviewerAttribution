The paper proposes a neat approach on including knowledge constraints in deep generative models, namely, GANs. The idea relies on applying Reinforcement Learning (RL) analogy for learning knowledge constraints that has high potential in many applications, such as, image analysis, medical imaging, text analysis etc. The authors explain in detail how the widely used Posterior Regularization (PR) framework for non-learnable knowledge constraints relates to RL. Further, they present how RL could be applied to learn knowledge constraints.  The problem is non-trivial and learning the whole approach end-to-end is challenging. However, the authors propose a bunch of approximations that make it happen, e.g.: - approximation of the normalizing constant; - learning p_{\theta} using the reversed KL similarly to the wake-sleep algorithm.  I find all these techniques properly chosen and reasonable.   The experiments are clear and well performed and show usefulness of the proposed approach. Both image analysis and text analysis are convincing applications. In the future, the method could be used in more challenging problems like medical imaging where having knowledge constraints is very natural.  Remarks: 1) The paper lacks a detailed explanation of training process and architectures used in the experiment. I highly recommend including it in the appendix in order to allow the community to reproduce the results. 2) The authors use Eq. (9) to learn the constraints, namely, MC samples from the distribution p_{\theta}. I fully agree that this is a "natural choice", as stated by the authors (line 189), but there are other techniques that could be utilized like REBAR or RELAX. Did the authors consider other approaches? 3) Lines 82-84: The sentences were used in earlier sections.