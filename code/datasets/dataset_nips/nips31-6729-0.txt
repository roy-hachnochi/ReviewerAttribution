 This paper presents an unsupervised learning method called sparse manifold transform for learning low-dimensional structures from data. The proposed approach is based on combining sparse coding and manifold embedding. The properties of the learned representations are demonstrated on synthetic and real data.  Overall, this paper is a long-winded presentation of a new manifold learning approach, which (at least in my view) is explained in heuristic and vague terms and seems very hard to perceive.   - First of all, it is not clear what modeling of data is being considered. Conventional manifold learning makes the assumption that the high dimensional data points lie close to a low dimensional manifold, and different manifold learning methods find low-dimensional embeddings that preserve different local/global properties of data. This paper seems to be considering something more than that. It introduces a concept called h-sparse function, which has no precise definition and presumably means that each data point in the original data space correspond to multiple points on the low-dimensional manifold. If this is the case, then how are sparse coding coefficients interpreted? Should one assume that the vector of sparse coefficients in this case is the sum of sparse coefficient vectors for each of the point on the low-dimensional manifold? This seems to be illustrated in Figure 1, but it does not seem that the figure provides enough information for what is being computed in there. Another confusion is that the proposed SMT seems to be utilizing temporal smoothness of data, with movies being an example. Does this mean that SMT is designed for manifold learning of temporal data only? This should be made clear at the beginning of the paper rather than just not bringing this up until page 6 of the paper.  - There are also a lot of technical details/terms which are explained only vaguely. One example is the so-called "h-sparse function" mentioned above which does not have a clear description of the concept (unless it is a well-known concept). Another example is line 141 which mentioned section 1.2 but it is unclear to me how section 1.2 is related to the context here. Also, line 150 mentioned "2D manifold", I don't see which is the manifold. There is also no formal description of their algorithm. It is said that the first step is sparse coding, but how is the dictionary learned for sparse coding?  - It appears that the method is closely related to LLE and LLL, in the sense that all of them are based on data representation. In fact, the paper started with a review of LLL in Section 1.2. But later on in the description of the proposed method there does not seem to have any comment on the connections with LLL. I feel that a comment on such connection will be helpful to clarify the technical contributions of the paper.  