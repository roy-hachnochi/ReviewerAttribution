This paper proposes an adversarial defense.  They train a class dependent discriminator to distinguish between real images and those generated by a standard adversarial attack.    The central idea of the paper is a good one and the visual evidence of their approach is compelling.  I fully expect that the paper will be a good one, but seems to require another revision.  The current submission is lacking in several ways, and overall feels rushed.  Please have your submission proof-read for English style and grammar issues.  The paper purports to propose a novel adversarial attack in section 4, but as they themselves point out on line 210 it is just the basic iterative attack with an adaptive step.  This hardly constitutes a novel contribution, and the paper should not pretend it is.  I believe the authors would agree, especially considering their later experiments don't use their own novel attack method but instead report results just for DeepFool / Carlini and Wagner and the basic iterative method.  If this paper was really proposing a novel attack method worth mentioning as a novel contribution, then I argue it would have been a necessity to at least have included the method in their own evaluations.    Overall, I have some concern about the paper.  While I like the idea and the method and the visual evidence is good, they relying on the implicit dependence of the generated adversarial image to the parameters of the current classifier, as they note on line 214.  However, the attack they use is an iterative one with a stopping condition.  This discrete stopping criterion introduces a non differentiable path in the computation graph.  The paper does not address this, and so I suspect the authors did not either realize or address this themselves.  Simply ignoring the dependence of the stopping criterion (p(hat x) < C) on the parameters of the classifier amounts to a sort of straight through gradient estimation.  This will produced biased gradients.  This may very well be adequate in practice, but the paper needs to address the issue and does not.  While the visual evidence is rather striking, the numerical results are less impressive.    Table 1: the [4] vs [10] element presumably has a typo in 2.021. Table 2 has two tables, while the text nor the caption addresses why there are two tables, and the tables have different numbers.  Also, none of these human evaluations have any sort of statistics.  I don't know how many humans were asked. I have no sense of whether these numbers are statistically significant.