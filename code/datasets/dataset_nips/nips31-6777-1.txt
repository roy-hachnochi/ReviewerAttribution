The paper establishes connections between the regularization path and optimization path of first-order methods for empirical risk minimization. This is established for both continous and discrete gradient descent and mirror descent. Finally the morivation for doing so is established via providing excess risk bounds. The connection seem to be novel to my knowledge but at the same time seems intuitive once reading the statement of the theorem. But the consequences of this connection to excess risk is not clear. Are there any other consequences ?  According to the rates given in line 182, if 't' is picked as 1/m* log(sqrt{n/p}*sigma^{-1}), then the overall bound is still O(sigma* \sqrt{p/n}). This choice of 't' is almost same as the one in line 137, in terms of dependence on 'n'. Can the authors elaborate on the advantages of the proposed approach specifically with respect to early-stopping ?  I went over the proofs and they seem correct to me. In the current form, this looks like a direction worth exploring but the results in the current draft are rather preliminary.