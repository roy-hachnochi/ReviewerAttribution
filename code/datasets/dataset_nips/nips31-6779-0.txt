This is a good paper that proposes acoustic modeling techniques for speech recognition on mobile and embedded devices. I can see the following highlights:  1. The authors propose a multi-time step parallelization approach that computes    multiple output samples at a time with the parameters fetched from the DRAM    to reduce the number of accesses to DRAM. 2. The authors achieve good speech recognition performance on WSJ and    Librispeech datasets using models running on a single core of Cortex A-57. 3. The authors provide relative complete experiment results including CER, WER,    real time factor, breakdowns of computation cost, and so on.  But I also have the following suggestions:  1. From what I can see in the paper, after optimizing the acoustic model, the    computation of the LM part now dominates the total computation time. To make    this really useful for mobile or embedded devices, the authors will have to    tackle the LM issue as well. 2. The performance on Librispeech is good, but not excellent. And I would also    be interested in seeing results on the noisy datasets from Librispeech. 3. It will be great if the authors can cite state-of-the-art results on WSJ and    Librispeech just for the purpose of comparison.  Overall, I think this is a good paper that can potentially guide the direction of speech recognition on mobile and embedded devices.