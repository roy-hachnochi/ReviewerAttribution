This paper proposed an infinite family of algorithms that generalize Lloyd's algorithm with two controlling parameters. The optimal parameters can be learnt over an unknown distribution of clustering instances which is a data-driven approach.   1, The paper requires tuning parameters optimization. Since \alpha and \beta are correlated, can then be chosen simultaneously or iteratively instead of optimize one while fixing the other constant. Any guarantee the optimal values are global optimal?  2, What is the underlying true clusters are unbalanced/unevenly distributed? Is the initialization able to cover the minority clusters as well? It will be interesting to see how the optimal objective function look like. Since the loss function is always tricky  in this situation.   3, The proposed method also requires a distance metric and a desired number of clusters (k) as inputs. The paper used l2 distance. Is the algorithm sensitive to the choice of distance measure and number of clusters? What happen if the predefined k is far away from the truth (eg. much smaller than truth)? Can those be included as model parameters and updated (data-driven) as well?   4, Any comparisons with other method in the empirical study?