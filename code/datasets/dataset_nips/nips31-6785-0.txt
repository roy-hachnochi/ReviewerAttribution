The paper studies the convergence of EM algorithms for learning mixture of Gaussians.  -------------  Results and comparison to existing work. 1. Thm 1: Shows that for learning mixture of two Gaussians with well separated means at -\theta^* and +\theta^* (and covariance of identity), the  EM algorithm on the population objective for learning mixture weights and mean parameters converge to the true parameters (\theta^*,w^*) or almost all initializations.  --The above result generalizes part of the results the existing work of Xu et al. 2016 and Daskalakis et al. 2016 which analyze EM under the above conditions when the mixture weights are fixed to be 1/2.  --On the other hand, the results in Xu et al. 2016 extend to general means (\theta^*_1,\theta^*_2) and not necessarily (\theta^*,-\theta^*). From a preliminary check, I could not extend the current proof to general means.  --Daskalakis et al. 2016 also give more rigorous analysis with finite samples.   2. Thm 2: for non-uniform mixtures, when the weights are fixed to true parameters, a constant fraction of initializations will converge to a wrong solution.  --This negative example essentially relies on picking a theta^(0) that correlates better with the smaller component (say C2=(-theta^*,w2)). If the weights are learnable, the model will simply change the ordering of learned components, while effectively learning the same model. E.g. learn C_1=(-theta^*,w_2) and C_2=(theta^*,w_1) instead of C_1=(theta^*,w_1) and C_2=(theta^*,w_2) --The example highlights interesting phenomenon on the optimization landscape wherein, in the presence of invariances, fixing a subset of parameters to its optimum value restricts the set of invariances the model can learn.  E.g., in this problem, for almost all initializations, there is a “good” basin of attraction in the joint parameter space of (w,\theta) that globally optimizes optimization objective by learning the first component to be either C1=(theta^*,w_1^*) or C1=(-theta^*,w_2^*). However, with w_1^* fixed, a constant the fraction of initializations will have a non-global basin of attraction in the space of just \theta.  --However, for the problem of learning MOG itself, this result is not that interesting since such bad initializations can be easily avoided by using some very reasonable initialization schemes such as initializing with overall the average of samples or choosing an initiation uniformly randomly but with norm close to 0.  ---------------------- Summary of evaluation: --While Thm 1 is a non-trivial extension of Xu et al. 2016 and Daskalakis et al. 2016 for the case of symmetric means, the significance of the result is somewhat limited since the analysis does not seem to naturally extend to general means (\theta^*_1,\theta^*_2) as in Xu et al. 2016 or to finite samples as in Daskalakis et al. 2016.  --Thm 2 in my opinion is more interesting for the insight into the optimization landscape the optimization parameters have invariances. I would suggest elaborating on the optimization landscape more in the paper --Finally, the mixture of two gaussians is a very special case where EM converges since the landscape does not have bad local optima. The paper misses discussions on the following relevant results: (a) Jin, Chi, et al. "Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences." — show that local optima are common in GMMs with more than two components (b) Yan et al. “Convergence of Gradient EM on Multi-component Mixture of Gaussians” (2017)—show local convergence of EM for >2 component GMMs  Overall there are some interesting results and the theorems are correct from my limited check, but I find the extensions somewhat incremental compared to existing work.   ----- Read author response