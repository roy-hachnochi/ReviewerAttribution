Summary ------------- In this paper, the authors focus on the convergence of the celebrated EM (Expectation - Maximization) algorithm for mixtures of two Gaussians with unknown means. Departing from the recent results of Xu et. al. and Daskalakis et. al. the authors consider the case of unbalanced mixture, where the probability of each component is different.     Let w_1 and w_2 = 1 - w_1 be the probabilities of the component 1 and 2 respectively. In has been observed before that when w_1, w_2 are known then the EM iteration has a spurious stable fixed point and hence does not globally converge to the true parameters of the model. The authors make this observation formal in their Theorem 2.    The main result of this paper is to show that if instead we run the EM iteration with unknown w_1 and w_2 then the EM algorithm globally converges to the correct estimation of the parameters of the model. Theorem 1 of this paper generalizes the Theorem 1 of Xu et. al. and Theorem 2 of Daskalakis et. al. in the sense that it accommodates unbalanced mixture of Gaussians. It is not clear though from the statement of Theorem 1 whether it implies geometric convergence or not, as it was the case for the corresponding theorems in Xu et. al. and Daskalakis et. al.. Moreover Lemma 2 in Section 2.3 implies that the convergence is guaranteed from the existence of a potential function m(.) without any information on how fast convergence this potential function implies.     The technique used to prove Theorem 1 has two parts (1) reduction from the multi-dimensional to the single-dimensional case, (2) proof of convergence in the single-dimensional case. Part (1) of the proof is almost the same as the one used in Xu et. al. for the balanced mixture, although it is interesting that it generalizes in this case. Part (2) of the proof is novel and more interesting. The authors succeed to carefully constructing a global potential function based on the local properties of the EM iteration in each region of tha space.    Finally, the results of this paper give one of the few formally proved examples where the over-parameterization is indeed helpful. The fact that over-parameterization helps in machine learning tasks has been observed in practice in the recent year without a solid theoretical understanding of why.  Summary of Recommendation --------------------------- I believe that this paper is a novel and important contribution in the direction of understanding the global convergence properties of non-convex methods in machine learning and hence I strongly recommend acceptance if the following comments are addressed.    Comments to the authors --------------------------- 1. Does your proof of Theorem 1 imply geometric convergence? If not please rephrase Remark 1 since in this case Theorem 1 of Xu et. al. is a stronger statement.    2. What about the sample dependence of EM? Is it possible to generalize Theorem 6 of Xu et. al. or Theorem 3 of Daskalakis et. al.? If not, an explanation of the bottleneck towards such generalization would be very informative.    3. In the statement of Lemma 2 it is claimed that since m(.) is a continuous function with a single root m(\theta, w) = 0, and m(\theta^t, w^t) -> 0 this implies (\theta^t, w^t) -> (\theta, w). Im afraid that this is not correct, consider for example the single variable function f(x) = x*exp(-x). This function is a continuous function with a single root f(0) = 0, but the sequence x^t = t has f(x^t) -> 0 although definitely x^t does not converge to 0. Looking at the proof of Lemma 2 though it seems like this is not an important issue since the authors show something stronger, namely that (\theta^(t + 1), w^(t + 1)) is always strict inside a rectangle D that depends on (\theta^(t), w^(t)) and contains the global fixed point. I didn't have time to very carefully verify the proof of this lemma so a comment from the authors on this issue will be appreciated.    4. Continuing to comment 3. it seems like the construction of m(.) with the area of the rectangles could be transformed to a contraction map argument instead of a potential function argument. If this is possible then the statement becomes much clearer with respect to the convergence rate of EM.