The paper presents a convergence result for the Gaussian mixture model. In particular, for GMM with two components, each with an identity covariance matrix, the paper shows that the EM algorithm will converge to the optimal solution. It further demonstrates that parameterizing the mixture weights is helpful. Estimating mixture weights as unknown parameters in the EM prevents the algorithm from being trapped in certain local maximas.  Comparing to existing results for the convergence of GMM: - The result extends that of Xu et al. and Daskalakis et al. in that the two mixture weights don't need to be equal. It is weaker than these results in the sense that only asymptotic convergence is guaranteed. - The result depends on a strong assumption that the covariance matrices must be the identity matrix. - With more than two mixture components, it is known that the EM algorithm is not guaranteed to converge to the optimal solution [1]. Therefore the conclusion of this paper doesn't generalize to more than two components.  Overall, the paper presents an interesting result, but it is an incremental improvement over some already restrictive theoretical analyses. I am not sure if the result will make a broader impact to the NIPS community.  [1] Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences, C Jin, Y Zhang, S Balakrishnan, MJ Wainwright, MI Jordan