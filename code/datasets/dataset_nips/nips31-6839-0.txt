This work proposes a regularization method similar to dropout but specifically designed for convolutional blocks. Rather than dropping random neurons, and therefore disregarding the local correlation in the feature maps, the authors propose to drop contiguous blocks of neurons. This way, some of the semantic information in the features can be dropped altogether. This new regularization method is evaluated on state-of-the-art architectures for image classification and object detection, and shows improvement of the accuracy when compared to dropout and other similar methods.  I recommend acceptance of the paper for the following reasons:  Originality: To my knowledge, the idea is novel, and well motivated.  Clarity: The paper is clear and well structured, both for the method description, and for the experiments. The experiments are also well detailed, and reproduction of the work should be possible.   Quality: The authors provided a thorough explanation of each of the parameters of the method both in the description of the proposed regularization and in the conducted experiments. They also considered multiple schedules for the parameters of the method: fixed and increasing drop probability. Moreover, they compared the proposed method to multiple related works.   Significance: Although the shown results are already interesting, it would be interesting to see how it works on segmentation models.