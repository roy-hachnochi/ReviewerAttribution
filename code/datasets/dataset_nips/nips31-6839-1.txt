Summary: This paper proposes a variant version of dropout called DropBlock as training regularization for deep neural networks. Rather than drop features randomly, DropBlock put a spatial mask on each dropout center so that a spatial region is dropped out in training. It claims this kind of dropout can better drop more semantic information. The experiments show that on ResNet50 the dropblock brings about 0.5% - 1% improvement over the baselines.   Strengths: + A variant of dropout is proposed to improve the training of deep neural networks. It is a general training regularizer which could be plug in on a lot of different architectures.  + It shows improvement over resnet for imagenet classification and retinaNet for object detection.  + The sensitivity analysis of the parameters has been done.  Weakness: - One drawback is that the idea of dropping a spatial region in training is not new.  Cutout [22] and [a] have been explored this direction. The difference towards previous dropout variants is marginal.  [a] CVPR'17. A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection.  - The improvement over previous methods is small, about 0.2%-1%. Also the results in Table 1 and Fig.5 don't report the mean and standard deviation, and whether the difference is statistically significant is hard to know. I will suggest to repeat the experiments and conduct statistical significance analysis on the numbers.  Thus, due to the limited novelty and marginal improvement, I suggest to reject the paper. 