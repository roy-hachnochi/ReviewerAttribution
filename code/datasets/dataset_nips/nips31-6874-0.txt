This is a very interesting paper using inverse reinforcement learning (IRL) as a tool to learn to generate point processes (PP). The generation is very flexible and the authors argue for its merits over GAN-based and other generative methods discussed recently.  Flexibly generating point processes to match the properties of data is a subject of recent interest, and this paper is an exciting contribution to it.  The benefits of this method can be broadly compared to the WGAN-PP seen recently, in that the optimisation is based on distances between real and simulated PP realisations and not explicitly on likelihoods. As the authors stress, the present method seems likely to be easier to train since it is not adversarial. The authors include analytic expressions for the inner loop of their method.  I am not an expert in reinforcement learning and so I mark my confidence score as lower than full.  The distance function between pairs of PP realisations is absolutely crucial here, and here is specified via a choice of RKHS kernel. This choice must therefore have an effect on the quality of the results, and I would like to have seen the authors explore this, in experiments and/or in discussion.   Minor feedback: * Fig 1: "are fitted back" should be "are fed back"? * line 74: the Hawkes process formula is over-simplified, constants are missing. * line 158 and 164: unexplained "f" term. * line 196: Loose wording "...the same point process" - no it is not. * line 210: "square is a monotonic transformation" - not strictly true  The authors' response to reviews added further useful baseline/comparisons and dealt well with concerns raised by other reviewers.