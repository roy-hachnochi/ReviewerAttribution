This paper targets the problem of identifying top-K frequent items in a dataset, and presents Topkapi, a new algorithm combining the pros of two conventional algorithms (Count-Min Sketch and Frequent) while avoiding the cons carefully. The illustration of the proposed idea is clear and straightforward, and the evaluation shows its advantages over existing algorithms. However, I think the paper needs further improvement: - Some details should be presented in the paper rather than in the supplementary material, e.g., the key properties of Count-Min Sketch and Frequent, the *K* used for evaluation, etc. Also, the abstract mentioned the speedups over Spark but related results are not included in the paper but given in the supplementary material. - Each cluster node has 12 cores but the experiments used only 8 of them. Why not using all of them? Please explain. - The experiments only tested top-100. What would the results be when K varies (i.e. smaller and larger)? As the algorithms (and the way to parallelize computations) look cache-sensitive, the performance of compared implementations could be very different. - What about other variations of Count-Min Sketch, e.g. discarding the heap and re-scan the dataset to identify top frequent items? - Some related work is not discussed, e.g. Efficient Frequent Item Counting in Multi-Core Hardware [KDD'12].  UPDATES: According to the response, it turns out that the proposed and compared algorithms are all cache sensitive (at least in the conducted experiments), so it would be better if we can see the impacts of K in a larger span e.g. 10, 50, 100, 200, 1000, etc. Some profiling results (e.g. the number of instructions, memory accesses, cache misses for each algorithm, etc.) could help.