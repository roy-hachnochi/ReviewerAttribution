This paper proposes a deep-RL based approach for jointly learning a classifier along with an active feature acquisition strategy. Overall, this is a relatively less studied area of ML, and this work is a good step towards building a general solution.   Major strengths:  - Overall learning approach seems sound, interesting, and novel  - Use of feature-level set encoding is an interesting application of some recent work to this area - Experiments on both synthetic and real world datasets, including attempt to get it verified by domain experts (in this case medical professionals)  Major weaknesses: - Some missing references and somewhat weak baseline comparisons (see below)  - Writing style needs some improvement, although, it is overall well written and easy to understand.   Technical comments and questions: - The idea of active feature acquisition, especially in the medical domain was studied early on by Ashish Kapoor and Eric Horvitz. See  https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/NIPS2009.pdf There is also a number of missing citations to work on using MDPs for acquiring information from external sources. Kanani et al, WSDM 2012, Narsimhan et al, "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning", and others.   - section 3, line 131: "hyperparameter balancing the relative importances of two terms is absorbed in the predefined cost". How is this done? The predefined cost could be externally defined, so it's not clear how these two things interact.   - section 3.1, line 143" "Then the state changes and environment gives a reward". This is not true of standard MDP formulations. You may not get a reward after each action, but this makes it sound like that. Also, line 154, it's not clear if each action is a single feature or the power set. Maybe make the description more clear.   - The biggest weakness of the paper is that it does not compare to simple feature acquisition baselines like expected utility or some such measure to prove the effectiveness of the proposed approach.   Writing style and other issues: - Line 207: I didn't find the pseudo code in the supplementary material - The results are somewhat difficult to read. It would be nice to have a more cleaner representation of results in figures 1 and 2.  - Line 289: You should still include results of DWSC if it's a reasonable baseline  - Line 319: your dollar numbers in the table don't match!    - The paper will become more readable by fixing simple style issues like excessive use of "the" (I personally still struggle with this problem), or other grammar issues. I'll try and list most of the fixes here.   4: joint  29: only noise 47: It is worth noting that 48: pre-training is unrealistic  50: optimal learning policy 69: we cannot guarantee  70: manners meaning that => manner, that is,  86: work  123: for all data points 145: we construct an MDP (hopefully, it will be proper, so no need to mention that) 154: we assume that  174: learning is a value-based  175: from experience. To handle continuous state space, we use deep-Q learning (remove three the's)  176: has shown  180: instead of basic Q-learning 184: understood as multi-task learning 186: aim to optimize a single  208: We follow the n-step  231: versatility (?), we perform extensive  233: we use Adam optimizer  242: We assume uniform acquisition cost 245: LSTM  289: not only feature acquisition but also classification.  310: datasets 316: examination cost?          