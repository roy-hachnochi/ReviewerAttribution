The manuscript “Regularization Learning Networks” introduces a new type of neural network, trained using a new loss function, called Counterfactual Loss, together with stochastic gradient descent. The main hypothesis of the manuscript is that more flexibility in regularization should enable a better treatment of features that have a varying degree of importance. With this the authors wanted to enable DNNs to perform better in scenarios where they currently loose against other learning methods like gradient boosting trees. The authors motivate in their conclusion why this might make sense, but in the application they show it is not clear, why one has to use a regularization learning network (RLN) instead of a GBT model.   Quality The overall quality of the paper is good. The authors really tried to show the effects of the new regularization scheme, which seems to produce reasonable results.  Clarity The presentation of the method is very clear. Sometimes, it would have been nice to include more details in the manuscript and not in the supplement (e.g., training setup).  Significance The regularization scheme seems to have certain benefits over standard DNNs. The authors could not show on the data set they chose that this leads to a model with better performance than the best other method, but at least compared to standard DNNs the performance increase was significant. Therefore, especially for settings where joint models on several types of data are supposed to be learnt, this new approach might be a good way forward. Since the authors only gave this application as an outlook and also only mention that the initial results are positive (in the author response), it remains to be seen how well the performance will be for different prediction tasks of this type. 