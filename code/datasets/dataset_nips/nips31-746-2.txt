This paper introduces a smooth thresholding technique which enables practically standard gradient descent optimization to be applied to spiking neural networks. Since the spiking threshold is usually set at a certain membrane potential, the function "spike or no spike" is a function of voltage whose distributional derivative is a dirac Delta at the threshold. By replacing this Dirac delta by a finite positive function g(v) with tight support around the threshold, and which integrates to 1, the step function "spike or no spike" is replaced by a function that increases continuously from 0 to 1 across the support of g. In turn, this setup can be placed into standard differential equation models governing spikes, while retaining the possibility of having meaningful gradient signal for parameter optimization.  Two experiments are evaluated, an autoencoding task and a delayed-memory-XOR task, which are both shown to be trainable with the proposed setup.  This is a good contribution which removes some previously required restrictions for training spiking neuron models (which are enumerated in the introduction).  An analysis that is missing and which imposes itself quite naturally is the behavior of the training as a function of support size/smoothness of g. One could for example evaluate g_a(v) = a g(a(v - v0) + v0) with g starting as very smooth and becoming more pointy as a increases. At some point training should become impossible due to convergence of this method to standard thresholding (+ numerical issues). Next, one could also evaluate making this function continuously sharper during training. 