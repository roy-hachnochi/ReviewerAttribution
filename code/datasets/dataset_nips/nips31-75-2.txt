The paper deals with the problem of transfer learning and tries to compare approaches under the seemingly different umbrellas of metric learning, transfer learning, and few-shot learning. It's a brilliant paper that makes the following empirically backed claims: (a)  Weight adaptation is a surprisingly bad choice as it gets only marginal gains over training from scratch for a range of quantity of adaptation data available. (b) Methods trained on deep embedding losses in source domain do better at weight adaptation (AdaptHistLoss being proposed by authors) and outperform the previous SOTA by an average of 30%  Some minor nits: (a) Line 104: "is is" -> "is" (b) Line 97: "has only one hyperparameter" -> state that the only hyperparameter is number of bins for completeness.