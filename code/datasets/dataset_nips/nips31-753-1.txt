This paper proposes a scheme that learns the internal dynamics of an angent, under the assumption that the agent's actions are nearly optimal under its internal dynamics (which deviate from the real ones). The paper discusses some uses for learning such an internal model and performs tests to validate the assumption that IRL for humans is aided by the proposed scheme, and the underlying idea (that humans move optimally wrt a wrong model).  While there are definetely some issues, which will be discussed below, I commend the authors for this work. I found the paper a joy to read, it  seems free of technical errors and makes a clear contribution to the field.  ========== QUALITY ==============  - A large amount of the paper's abstract/introduction discusses the IRL setting, whereas most of the experiments (including the human experiment) concerns the shared autonomy setting. This should be better balanced. - Ideally,  an extra experiment on the lunar lander, with slightly different objective (say hovering) would make the story-line in the paper complete. While it is difficult to assess the improvement of the IRL for human tests in general, a simple task such as hovering would provide some insights.  ========== CLARITY ==============  - The problem of regularizing the internal dynamics model is quite important, and the fact that it exists (due to the way the model-learning is setup) should be announced clearly. I suggest adding a sentence or two in both the introduction and the start of Section 3.  - The whole choice of assuming optimality with a possibly completely incorrect model has many implication. The proposed regularization reduces the potential 'incorrectness' of the model somewhat, but it would be interesting to explore a more principled way of combining model error and non-optimality. The limitations of the current approach in this respect should be made clearer in the paper. - In equation 4 and 5, the match between the delta is suggested to be a constraint. In effect, you solve this with a penalty function, essentially turning delta^2 into an additional objective. The two terms in equation 6 are better explained as two cost functions, skipping the 'constraint'-explanation. That way the rather optimistic assumption that delta can be set to zero is also removed.  ========== ORIGINALITY/SIGNIFICANCE ========= - The paper takes an interesting idea, and does a good job of exploring the solution. - The proposed algorithm is promising, but will not have an immediate large impact on IRL and shared autonomy. However, the approach taken might have such an impact in the future.  