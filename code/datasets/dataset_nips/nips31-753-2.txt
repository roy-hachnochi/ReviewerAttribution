In this paper, the authors study the inverse reinforcement learning from a new perspective, i.e., they try to model the human intended dynamics (which is referred to as the 'internal dynamics'),  also they show how the learned internal dynamics can be applied to the 'simulation-to-real transfer' scheme. Several examples are provided to support their assumption (i.e., the human recognized dynamics might be different from the real dynamics) and main results.   One problem I am mainly concerned is that the authors first assume the internal dynamics severely deviates from the real dynamics, and then argue that it is not possible to learn the correct reward function using traditional IRL. Intuitively, if the user has very bad interpretation of the dynamics, the user will usually adapt their rewards accordingly, namely the dynamics (i.e., transition probability) and the reward function should be learned together. In the evaluations, the authors explained that the classical IRL can not perform well and meanwhile compare the classical IRL with their work, which looks kinda unfair, after all the assumptions behind both approaches are quite different. It would be more convincing if the authors could learn the human intention dynamics and the reward function simultaneously, and then compare this hybrid learning with their work.   Other small issues that arise when I go through this paper are provided as follows:  - in line 36 it is not clear about the difference 'suboptimal' and the 'near-optimal', in my opinion the second 'near-optimal' should be 'optimal' since the user should naturally choose the best action based on his recognition and understanding.  - in line 97, the authors metion the 'near-optimal' again, but in line 197, it becomes 'but optimal with respect.....' Since this description is very important, the authors should check them carefully.  - in line 133, it is unclear how to choose this 'random policy' as this will influence the sampling significantly, also this information is missing in the evaluations.  - for equ.(7), I found that it is not very straightforward to interpret the function 'f', it seems this function models the action error between desired action and real action, please clarify the notations of 'a' and 'a^{int}'.  - in line 186, it would be more clear if the authors mention fig.1 after equ. (8)  - in fig. 2, the pure IRL performs poorly, so what happened if the internal dynamics is slightly different from the real dynamics? 