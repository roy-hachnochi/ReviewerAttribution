Post Authors' Response  Thanks a lot the authors for their response. I am happy with the responses they have provided to my initial concerns which have improved the manuscript. I would encourage authors to add an appendix should they believe they can convey a more complete message without having to wait for drafting another another longer manuscript.   ------------------------------------------------------------ Initial Review  The author/authors propose the first neural network producing fast high-dimensional convolution algorithms. They utilise the new Acceleration Network (AccNet)  to express the approximation function of splatting/blurring/slicing (SBS) and generalise SBS by changing the architecture of AccNet. Upon finishing the training process, the proposed fast convolution algorithm can be  derived from  the weights and activation functions of each layer. They conducted various experiments that prove the  effectiveness of the proposed algorithm.  The papers is very well written and of good quality. The main part of the paper, i.e. 3rd to 6th page, would benefit from making clearer the exact contribution of the authors. Reason is that it sometimes provides too much information about the fundamentals and the exact implementation and adjustments are likely to be missed. Section 3.2.4 that describes the proposed algorithm could serve for this purpose, i.e. what is being added or proposed to further enhance the SBS. Figure 3 shows the fast filtering approaches based on gCP and gHT decompositions. Is this based on section 3.2.3 that has been proposed by Hackbusch and Kuehn or includes other tensor decomposition methods? I reckon the latter, but it needs to have a better connection to what is being proposed further on especially when describing the reuse of convolution kernels by multiple times.  -Having said all the above, I think that the results are very interesting and demonstrate a solid computational improvement.  Is Figure 5 accurate? Have you really achieved such a reduced time in the detail enhancement, i.e. 66.1s vs 0.65s. That would be a significant improvement. By the way the font size is quite small to be honest.  -The weak point in my humble opinion is the few examples that are presented to demonstrate the computational efficiency, such as figures 4 and 5. I think further evaluation in larger data set is needed so that the bias can be reduced and ascertain the generalisation of this approach, as one could now claim that the examples presented have been selected to suit the proposed algorithm.  -Please expand slightly the conclusion if possible, as it reads as if you have been running out of space. Highlighting the main findings and contributions will enhance the paper.  Minor issues: a) Line 162, page 5 correct "approximates" to "approximate" b) Figure 3 caption second to last sentence "outputs" c) Line 273, page 8, please remove the redundant "the"