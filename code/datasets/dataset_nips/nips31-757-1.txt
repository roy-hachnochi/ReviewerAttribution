The paper proposes an idea to accelerate high-dimensional convolutions with non-gaussian kernels by approximating the splatting-blurring-slicing operations developed for gaussian convolution by learned neural networks.  First of all let me point out i'm not an expert in this domain. I found some of the mathematics hard to follow, sometimes also seemingly due to poor writing. For example "g-aff mapping changes the element number of each red fiber" -- fibers are nowhere else mentioned in the paper. What fiber ? Also acronyms are profusely scattered through the paper, often way before they are defined, e.g. "gCP" is in the abstract and in the first 4 pages before in the fifth there's an attempt to define it.   Experiments-wise, i think there is one problem with the paper: it's main goal - accelerating high-dimensional convolutions - does not seem to be properly evaluated; there is not a single table with a timing comparison, just an evaluation of how accurate it is. How much faster are the resulting convolutions compared to non-accelerated standard versions ?  Regarding writing, it is sometimes very coloquial, for example: "a ton of convolutions ", or "Philipp has to perform multiple times Gaussian filtering", where Philipp is the an author's first name.  Other remarks: - table 1: "filtering accuracy: " is not a good description of mean squared error. A reader will assume higher is better.