The paper focused on the stochastic composite mirror descent algorithms which  update model parameters sequentially with cheap per-iteration computation, making them amenable for large-scale streaming data analysis.  The setting is quite general including general convex objectives and penalty terms, which could be just convex or strongly convex.   The main contribution of the paper is the almost optimal rates with high probability  which match the minimax low rates for stochastic first-order optimization algorithms. The techniques are quite delicate and novel which are summarized in lemma 1 and Theorem 3.  Another technique is a weighted summation which originates from [29].   In summary, the paper presents novel theoretical convergence rates for SCMD which are almost optimal, up to a logarithmic term.  These comprehensive results are in the form of high probabilities which are non-trivial extension and refinement of existing literature.   The paper is well presented and Section 5 on related work well summarized the main contribution of the work in a detailed comparison with existing work.   Minor comments:   1. The discussion about Theorem 4 and Corollary 5 in lines 136-140 should be moved after the statement of theorem 4 and corollary 5.   2. In line 198, Assumption 3 is mentioned before it is introduced.   3. Personally, I do not see what is the main purpose of Section 6 (simulations).   The main meat of the paper is novel theory which is quite satisfactory to me.    I read the authors' feedback.  I am satisfied with their response. 