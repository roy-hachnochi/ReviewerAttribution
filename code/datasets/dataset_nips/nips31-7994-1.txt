This paper gives algorithms for recovering the structure of causal Bayesian networks. The main focus is on using path queries, that is asking whether a direct path exists between two nodes. Unlike with descendant queries, with path queries one could only hope to recover the transitive structure (an equivalence class of graphs). The main contribution here is to show that at least this can be done in polynomial time, while each query relies on interventions that require only a logarithmic number of samples. The author do this for discrete and sub-Gaussian random variables, show how the result can be patched up to recover the actual graph, and suggest specializations (rooted trees) and extensions (imperfect interventions).   The paper is generally well written. While the main result is not terribly deep or surprising (that transitive structure is easier to recover and that logarithmic number of samples are enough for an acceptable query quality), the general ideas and treatment are novel to me and relevant to the community. I have not found any major issues with the portions of the theory that I looked at (though, as a disclaimer, I did not go over every bit of it). More importantly, the proposed algorithms are intuitive and simple enough to be useful, the analysis is mostly straightforward, and this line of inquiry also opens up many new questions.  I recommend the paper for acceptance. Here are some specific major remarks.  - Being able to do something simpler when the more complicated thing is computationally hard is generally interesting. But I would be much more motivated if the simpler solution is in itself useful. Are there situations where the transitive structure in itself is useful?  - We do not quite see the topology of the graph explicitly influencing anything except when we talk about rooted trees. This is somewhat strange. And indeed there are many implicit spots where the topology plays a role and which I think are important to highlight. The most important of these is when the authors claim (L265-266) that "the sample complexity for queries [...] remains polynomial in n" when answering transitive queries. Now think of a sequence of graph families, indexed by n, where the mean number of parents increases linearly, while the influence of any given parent gets diluted the more parents a node has. Then, for a sequence within such families, \gamma as defined in Theorem 3 (L268) would decay, possibly even exponentially since there are exponentially many S in the number of parents, therefore invalidating the claim. Conversely, if one assumes that \gamma stays constant in the asymptotics, then one in effect is making the claim only for a specific sequence of graph families. I think this kind of implicit topological assumptions should be much more transparent in the final write-up of the paper.  Here are more minor remarks.  - Put the word "causal" (Bayes nets)  in the title, it is misleading otherwise. - The first time n is used (L52), it's not yet defined (unless I'm missing it). - The \tilde O notation just for constants is strange (footnote 2). Why not just O? - L111, Why not just say p(Y) in the |Dom[Y]| simplex? That would be more accurate. Perhaps write \Delta_{Dom[Y]}. Also if you go through that much trouble to describe discrete random variables, just saying one line for continuous ones is strange, as these can be much trickier. At least say that you assume some densities with respect to a common measure. - L138, the only time you do not require correctness is when there is a directed path between i and j *and* i is not a parent of j. - L170, the remark "Under this viewpoint..." is a bit confusing. It makes allusion to using a topology that is not a priori known. Or perhaps are you saying if the topology is known to be in a restricted family, then one could hope for reduced effort? Either way, it's worth clarifying. - Generally, it would be nice for propositions/theorems to invoke the subset of the assumptions that they use. - L185, it should be clear that using the expectation can generally miss out on the probability discrepancies, unless the expectation (or expectations) are sufficient statistics. - L189, I think it's more accurate to say that the statistics are inspired from Propositions 1&2, while the needed threshold is inspired from Theorems 1&2. - A lot hinges on knowing lower bounds on the threshold / upper bounds on variances. Is there any work on adapting to them being unknown? (Beyond the remark on L203.) - Please clarify the role of S in Definition 5. The text on L252-254 is not adequate, especially that the proofs of Theorems 3 and 4 hand-wave too much and refer back to those of Theorems 1 and 2. As I see it, either S shouldn't even be part of the definition or we should say that the \delta-noisiness property should hold for some choice of S. - In Algorithm 3, just say the input is the output of Algorithm 1 (to be consistent to the main text). - L297, at most \epsilon. - Would Definition 6 help in the general setting? - L309, it's inaccurate to say the sample complexity for sub-Gaussian random variables under imperfect interventions is unchanged. As I understand it, the effective variance changes. Please clarify.  [After author feedback]  I thank the authors for their detailed answers. My assessment is unchanged, though I raised my confidence. 