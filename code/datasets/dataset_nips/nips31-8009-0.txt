The authors propose a system that is composed of a state encoder (using a graph convolutional network and assuming object and relational information from the environment). They learn a policy in the representation space that outputs a state-action pair embedding, an adversarial instance classifier to try to learn domain independent embeddings, and also have an action decoder for each task and a transition transfer module to ease transfer.  Does your baseline A3C include using a GCN?  They do provide ablation studies in Table 2, which show that the instance classifier (IC) helps with zero-shot transfer.  I don't understand Figure 3? It looks like none of the additional pieces on top of GCN improve performance on the first task -- which is not surprising, so not sure why this figure is here.   Transfer experiments: There is no detail on the transfer experiments. How many samples do you use to train the action decoder and transition transfer function between tasks? Is the point being made that these samples "don't count" towards transfer because they don't exploit the reward function, only the dynamics of the new task?  The proposed method is promising but I want more information about the transfer experiments.