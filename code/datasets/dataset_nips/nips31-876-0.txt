This paper shows that exposing the constraints in an AND-OR tree to a neural-trained search heuristic can make the search succeed sooner and generalize better.  In particular, the paper uses constraint logic programming language miniKanren, guided by such a heuristic to decide which disjunct to explore next, to synthesize simple tree-manipulation programs.  The idea is valuable and the paper explains it well.  I am disappointed that the language is so simple as to not include conditionals.  Of course, conditionals can in principle be encoded using lambda and app, but the tree-manipulation programs synthesized do not perform tasks that call for conditionals, such as feeding every element of a list to a given function.  A more adequate evaluation might take advantage of other tasks that existing systems such as [11-13] aim for.  After all, the paper says "we are interested in exploring manipulations of richer data structures."  It is inaccurate to say that to "focus on list construction" is "following the programming language community".  The discussion of related work is limited to program synthesis, and should be expanded to include other uses of neural-trained heuristics in search.  The discussion in Section 4 about whether and how to embed logic variable identity in a neural representation is interesting.  Thus, claims such as "the logic variable identity is of limited use" and "we lose considerable information by removing the identity of logic variables" should be supported empirically, such as by ablation experiments.  On page 4, "variable identify" -> "variable identity"  On page 6, please elaborate on what you mean by "a supervised cross-entropy loss".  The next paragraph can also use elaboration.  I read and appreciate the author response. Regarding "a supervised cross-entropy loss", I'm still not completely clear -- what does "ground truth step" mean (in particular, what is its type) in "L = âˆ’ log p(ground truth step)"?