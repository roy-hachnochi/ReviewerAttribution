The authors propose a new RL framework that combines gradient-free genetic algorithms with gradient based optimization (policy gradients). The idea is to parameterize an ensemble of actors by using a binary gating mechanism, similar to dropout, between hidden layers. Instead of sampling a new gate pattern at every iteration, as in dropout, each gate is viewed as a gene and the activation pattern as a chromosome. This allows learning the policy with a combination of a genetic algorithm and policy gradients. The authors apply the proposed algorithm to Atari domain, and the results demonstrate significant improvement over standard algorithms. They also apply their method to continuous control (OpenAI gym MuCoJo benchmarks) yielding results that are comparable to standard PPO.  In general, the idea of combining gradient-free and gradient-based algorithms is interesting as it can benefit from the fast convergence of gradient-based methods while avoiding the problem of converging to bad local minima. I’m guessing that the gym benchmarks do not suffer from bad local minima that much making the improvement over standard PPO less prominent. What slightly confuses me is how the experience is used to update the actors. As far as I understand, in the first phase, the elite actor is learned following a standard training procedure (either A2C or PPO), and in the second phase, all actors are used to collect experience, which is used to evaluate the fitness of each actor and also to train the elite actor. If that is the case, then how are the off-policy samples collected in the second phase incorporated in the elite updates?  My main concern is that the experimental evaluation does not clearly answer the question what makes the method = work so well in the Atari domain. The ablations in Figure 5 indicate that, compared to standard A2C, G2AC learns initially equally well. However, the ablations are run only for the first 40M steps, and do not show what happens later during training. I’m assuming standard A2C gets stuck in a local minimum, however this cannot be inferred from the figure. Improved ablation study and inclusion of a toy problem to pinpoint what makes the proposed method work well would make the submission substantially better.  --- I have read the authors response, and I have revised my score accordingly. However, I still think the experiments in continuous domains are flawed which can partially explain why the improvements are marginal. For example, state-of-the-art model-free off-policy algorithm can obtain a return of 4k ... 6k in 2M steps on Ant (see TD3, SAC), so the results reported in the paper are not close to global optimum.  