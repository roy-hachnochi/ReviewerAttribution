The paper introduces a modification to the Bellman equation in which a relationship is enforced not only with the the next state in a trajectory, but also the previous state.  This modified Bellman equation is shown to have the qualities we require of a Bellman equation, namely, it is a contraction, it converges to the value of the original Bellman equation.  The paper then experimentally demonstrates its use on domains designed for its strengths, as well as more general Atari problems.  On the more general problems, a temporally-regularized version of PPO results in an improvement over standard PPO on many examples.  First, a complement: I am surprised this hasn't been done before.  The writing is clear, and the motivation and results are explained well enough as to be obvious.  My concern is that this is not likely to be that impactful.  There are of course many modifications to the Bellman equation out there to address different small opportunities, which have been explained, nodded interestedly at by an audience, and then set aside.  I suspect this will be the same.  Many of those modifications were deservedly presented at ICML or NIPS; I'd like it if this was as well, but in today's highly competitive acceptance environment, there many be too many more-impactful submissions.