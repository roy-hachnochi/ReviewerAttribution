# Paper ID 914  Simple static linear policies are competitive for RL  ## Summary  The paper performs a thorough empirical evaluation of the performance of basic randomized search over the class of linear policies on the MuJoCo locomotion tasks. The main idea is to demonstrate the effectiveness of these simple algorithms compared to the much more complex state-of-the-art RL algorithms proposed and evaluated on MuJoCo tasks. The results of the empirical evaluation are startling. The paper convincingly demonstrates very strong performance of the simple algorithm and policy class on the MuJoCo tasks. The evaluation is extremely thorough, the results are compelling and raise serious questions about the current state of RL algorithm evaluation methodology using MuJoCo. In my opinion, this paper is an excellent contribution to the RL literature.  ## Detailed Comments  - The paper performs a comprehensive evaluation of MuJoCo using a very simple class of linear policies. The learning algorithm is simple randomized search, improved by a few tweaks (reward std. deviation scaling, state normalization, top-k direction filtering).  - The main contribution of the paper is to show the strong performance of the above algorithms (ARS) in an extremely thorough evaluation on the MuJoCo control tasks. In most cases, ARS is competitive with the state-of-the-art on the MuJoCo tasks and in some cases (Humanoid-v1, Walker2d-v1), outperforms all known methods. This is a very interesting finding given the sophistication of the RL algorithms, which typically involve the use of deep neural networks with many parameters and hyperparameters, and suggests problems with using MuJoCo as a testbed for evaluating these algorithms.  - The 100 seed evaluation is the largest I have seen on the MuJoCo testbed. The variance and local optima, well known to practitioners, is made apparent. This clearly suggests the need for larger seed sets in evaluations for the more sophisticated algorithms but it's not clear to me if the computational overhead of the more involved RL algorithms would even permit such large-scale evaluation.  - Overall, the empirical evaluation is extremely thorough and the results point to issues in current RL evaluation methodology. The authors have made the code available and the appendix contains even more experimental detail. I couldn't find much to criticize about this paper. I congratulate the authors on their fine work.  Update -------- After reading the reviews and the author responses, I reiterate my original score. As reviewer 3 has pointed out (and the authors have agreed with in their response), having the main contributions clearly stated up front could strengthen the paper.