he paper proposes a combined objective of generating both informative and diverse responses in short conversations using a CNN+LSTM deep learning architecture.  The objective is referred to as Adversarial Information Maximization, where informativeness is promoted by optimizing a  variational lower bound on the mutual information between stimulus and response, and diversity is promoted by the adversarial goal encouraging a match between the distributions of synthesized and natural responses, under the assumption that the diversity in natural responses provides good guidance.    In the descriptions of the architecture and algorithms, additional details are mentioned that are introduced to combat certain known problems, such as the instability of GAN training.  Some minor improvements on some respective merit scores are observed for two datasets from Reddit and Twitter. Between the two proposed versions (basic AIM and a bi-directional option called DAIM), there are tradeoffs between informativeness and diversity.  It is not clear whether such tradeoffs can be easily tuned by the parameter controlling the mixing of the two objectives.  The proposed ideas are well motivated and reasonable, and apparently work to some extent in two real-world datasets.  The paper is well written, and represents some incremental contribution for response generation in short conversations that are not goal-oriented.  The work seems to better fit a linguistic/NLP related conference as it drills deep down to a special application scenario with some minor though well justified innovation.  There is not much that can be learned and applied to problems outside the chosen use case. 