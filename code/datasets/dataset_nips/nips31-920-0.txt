This work theoretically analyzed entropies and mutual information in a wide class of fully-connected deep neural networks. In details, it derived a novel analytical estimator of entropies (Claim 1) based on the replica method under certain random parameterization and high dimensional limit.   A rigorous proof is also given in two-layered networks by using the adaptive interpolation method. The Author(s) confirmed the validity of the theory in various experiments. Especially, it coincided well with the exact solutions of mutual information in deep linear networks.   The studies of information bottleneck have ever been limited to small networks or simplified linear ones [Shwartz-Ziv&Tishby 2017, Saxe et al. ICLR2018]. This work highly contributes to give a novel and helpful insight into them and suggests that the compression of the information happens even in ReLU networks but it will be uncorrelated to generalization capacity. Both main text and supplementary material are well organized and their claim seems to be clear enough. Therefore, I recommend this paper for acceptance without any major revision. If possible, It would be helpful to give comments on the following things.   * It would be better to generalize your results to models with bias parameters. It will make your theory more easy to use in practice, although it will make your estimator a bit messier.   * I am wondering on the optimization properties of the estimator (2) in Claim 1. In Supplementary Materials,  Author(s) proposed two types of algorithms (fixed-point iteration and ML-VAMP state evolution). Are there any sub-optimal solutions for minimizing the potential \phi_l when you use these algorithms?     Typo in Line 237:  samples,they ->  samples,_(space) they 