Summary: --------  The authors consider variational auto-encoders with discrete latent variables under a Boltzmann machine prior. They propose two approximations to the ELBO as well as extensions to previously proposed relaxations of the discrete latents.   Comments: ---------  This is a solid paper that advances the methodology of training latent variable models with complex latent distributions. The paper is mostly well written (except for the comments below). It seems technically correct, and the experiments convincingly show that the proposed methods outperform previous approaches.  Detailed comments:  1) Eqn 2: Why is the last term $\log p(z)$ averaged over $q(\zeta\vert x)q(z\vert x,\zeta)$ instead of just $q(z\vert x)$? In the DVAE++ it made sense as they separated into $z$ and $\zeta$ into two groups based on the RBM.  2) I found the oder of the sub-sections of section 3 confusing, which lead to some initial confusion  which objective are actually used (in combination with which relaxation). Altough this was clarified later, this section would benefit from better structuring.  3) Section 3.4 is a neat applicaton of the inverse function thm.  4) Do the authors have an intuition for why the power-function approximation work better? If so, it would be valuable to include it.  5) In general, I remain undecided if discrete latent varibles are a good idea. As the authors argue, they might allow for more effiencient models and might be more interpretable; in practice however, we end up with more expensive algorithms and weaker models. So, it would be important to convince readers like myself, that discrete models have something to offer in terms of interpretibility. Have the authors investigated the learned latent representation?