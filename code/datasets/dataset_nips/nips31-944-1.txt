This paper proposes to learn to extract scene representations by incorporating 3D models into a deep generative model. The learned inverse graphics model can be applied to 3D scene manipulation.  The paper is well motivated, noting that contemporary deep generative models do not support simulation tasks involving object manipulation, which could be very important to agents interacting with and planning in real world environments.  At a high level, the model works by disentangling 3D structure from texture in the encoder and the decoder models. Re-rendering can be performed by e.g. editing 3D attributes while fixing other, disentangled attributes such as texture.  - Can the model handle occluded objects? - L117 - what does it mean that “the distance conditioned on the patch is not a well distributed quantity”? Do you mean the patch resulting from the crop according the encoder’s bounding box prediction? And what distance is being referred to - is it the translation offset t? I would have expected the bounding box itself to encode t. Overall this part of the paper was confusing to me. - How do you learn the encoder of 3D attributes such q and t? Do you assume there are ground truth labels? (Later in the implementation details it seems there are either ground truth labels or an approximately differentiable renderer is used. It may improve the clarity of the paper to explain this earlier, and give more details). - How important is it to use a GAN for texture synthesis? Since the network is trained as an auto-encoder, I do not see a need for multi-modality in the generator network, so in theory a simple deterministic model could be used, which might speed up training. - Fig 6: is there a reason that the background details change in the edited images? It would seem possible to copy pixels directly from the source for the regions that do not contain objects.