This is a form of an ablation study for the case of Conv-LSTMs where the inputs are specifically 3DCNN feature maps. The paper demonstrats that some standard operations of the ConvLSTM module (namely convolutional structure of gates and attention) do not, in fact, contribute meaningfully to learning better models. This is intuitively sensible since the spatiotemporal relationships are already captured in the input features.  This is evaluated for gesture recognition. Furthermore, this intuition and experimental verification inspire  a new architecture different from ConvLSTM or a fully connected LSTM which retains the performance of Conv LSTM with considerably less parameters.   The paper is written clearly. I would advise to keep 8-15 in one block w/o page break for readability.  The main reason for the score is that this is practically useful but from a research point view it's a relatively minor contribution as it is a form of relatively straight forward architecture exploration.  How is the run time affected by the reduced parameterization? That should be reported in the paper.  