Originality -Related work is cited throughout the paper, but the authors do not directly include a related work section.  The authors should verify that they are the first to try SVRG on deep neural networks that use batchnorm, dropout, and data augmentation.  -To the best of my knowledge, the authors are the first to propose such changes and implement and test the updated algorithm, as well as compare the updated SVRG to SGD on a ResNet on ImageNet.  Quality -The updates made to SVRG are technically sound, well motivated, and do address the issues with deploying the algorithm on modern deep learning architectures. -The authors compare SVRG and SGD on a LeNet and DenseNet for CIFAR-10 and a ResNet for ImageNet, which are reasonable datasets and architectures to test, but I would have liked to see more than one ImageNet architecture (and an architecture closer to 80% top-1 accuracy on ImageNet rather than 70%) -The authors should compare SGD and SVRG only after tuning the learning rate, the momentum, and possibly even the learning rate decay scheme  Clarity -The paper is well written, and the explanations given by the author are easy to read and informative. Clarity is a major of strength of the paper.  Significance -The results are important because they show that even if one attempts all these fixes for SVRG to make it amenable to modern deep neural networks, SGD still outperforms SVRG.  This means that variance reduced optimization is less effective for these style of problems, and the authors provide very thorough and sensible explanations for why the algorithm fails.  