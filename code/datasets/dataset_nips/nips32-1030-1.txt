Starting from the well-know paper [2] about the explanation of acceleration, this area has been developed for several years. For this area, I have two criteria to measure the importance:  1) A good explanation should explain a large class of acceleration phenomenons. Nesterov's acceleration can be used for many settings, such as convex/strongly convex, composite/non-composite, first-order/high-order, etc. If we can explain various settings, then our explanation is less possible to be "overfitting". Inspired by estimation sequence, [1] proposed a unified theory for first-order algorithms to explain nearly all the settings of first-order algorithms, and showed substantial importance and elegance of the estimation sequence technique.  2) A good explanation should be able to give some new results and insights. Although the explanation itself is important, its practical value is its impact of developing new algorithms. As examples, [2] and [3] propose some  restart heuristics according to the explanation.  Unfortunately, the reviewer found that this paper can not satisfy the two criteria properly. As a unified theory has been given, this paper can only address the strongly convex setting. Meanwhile, I can not find something new induced by this explanation.   The differential geometry perspective is novel. However, the dual linearity of Bregman iteration is a common sense in this community. The concept of flat connection is just another description of dual linearity. When the concept of differential geometry is used, we always expect something different from Euclidean space. If the author can show that acceleration can still work for "nontrivial" connections, this paper will have impacts. However, the authors seem only using the concept in differential geometry and not exploring it in depth.  The introduction section is not state of the art and seems misleading. As [1] showed, the modern formulation of estimation sequence has strong power to explain acceleration.    [1] Diakonikolas, Jelena, and Lorenzo Orecchia. "The approximate duality gap technique: A unified theory of first-order methods." SIAM Journal on Optimization 29.1 (2019): 660-689.  [2] Su, Weijie, Stephen Boyd, and Emmanuel Candes. "A differential equation for modeling Nesterovâ€™s accelerated gradient method: Theory and insights." Advances in Neural Information Processing Systems. 2014.  [3] Krichene, Walid, Alexandre Bayen, and Peter L. Bartlett. "Accelerated mirror descent in continuous and discrete time." Advances in neural information processing systems. 2015. 