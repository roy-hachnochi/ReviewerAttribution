The paper proposes an original idea, which is brining the upper bound confidence estimate, typically used in bandits algorithms, to improve exploration in Actor-Critic methods.   The claim that 33 "If the lower bound has a spurious maximum, the covariance of the policy will decrease" is not sustained or explained.  The authors mentioned the problem with directional uninformedness and how SAC and TD3 sample actions in opposite directions from the mean with equal probability, but their proposal (5) is still a gaussian distribution, which just a different mean and variance, so the samples would be still have opposite directions from a different mean. Please clarify.  The impact of the level of optimism is not analyzed in the experiments.  In proposition 7, equation (7) it is not clear why the covariance matrices are the same.    In Section 4.1, why not use the max{Q^1, Q^2} as the upper confidence bound of Q_{UB} instead of the mean + std?  The claim that OAC improves sample complexity doesn't seem to follow from the experiments or figure 3, can you do a figure comparing steps to reach certain performance, as a way to show that?  Minor comments: - In line 73 do you mean "Actor-Critic" instead of "Policy gradient"?  - 80 \hat{Q}^1_{LB} -> \hat{Q}_{LB}  - 162 There is not formula (12) - In Figure 3, add Humanoid-v2 1 training steps graph to the 4 training steps to make the comparison easier. - The paper uses a weird format for references that makes it harder to navigate them.  ------------- POST FEEDBACK --------------- Thanks for to the authors for the explanations and for adding extra ablation studies and measuring the effect of the directionality. I have updated my overall score to reflect it.