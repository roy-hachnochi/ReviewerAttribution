The paper is on the active version of a learning model where the learner receives a ``logged'' data set, meaning that the label of each example x is observed with a given logging probability $Q_0(x)$ (known to the learner). In the active version there is also an additional data set, where the learner can decide whether to request the label of an example.  The paper proves various generalization error bounds in this model. This model has been studied in the recent paper [22].   The approach of the paper is closely related to the approach of [22], and modifies the learning algorithm in several respects.   Section 5.3 gives a detailed quantitative comparison of the label complexities of several versions of the algorithm, and the algorithm of [22]. The label complexities depend on several parameters involved and therefore the comparisons are not straightforward.   This is a solid technical paper. The main problem seems to be that no effort is made to justify the relevance and significance of the results, both in terms of the model and of the significance of the improvements over previous work.   The model is a recent one, and therefore explanation of its relevance would be necessary. For example, the assumption that the learner knows the logging policy seems to require some explanation. Also, the results are only meaningful if $q_0$ is not too small, and that could also deserve some discussion. For active learning, the usual assumption is that  there are many unlabeled examples. Here it is mentioned at the end of Section 3 that the number of unlabeled examples is at most the size of logged data set. No justification or comment is provided as to why this assumption is reasonable, or even why it is technically necessary.  Comment on response: it would be useful to add the comment on the effect of not knowing the logging policy, possibly with some explanation of how its approximate knowledge enters in the bounds. 