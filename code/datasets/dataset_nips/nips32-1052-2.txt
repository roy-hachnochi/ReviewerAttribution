Overall, this is a well-written paper that describes an interesting approach to continual learning (namely, meta-learning good initial representation to facilitate online learning). The paper draws connections to the literature well and provides interesting analysis of the representations learnt in this fashion. One shortcoming is that the only two datasets/tasks are considered and these are not large-scale enough to completely evaluate the performance of the method. There’s an open question of how this method will scale to larger tasks and the feasibility of this approach. For example, what do you pre-train on if you want to do continual learning on Atari games? Nonetheless it should provide a good starting point for further research. The rest of this review provides detailed comments as well clarifications that are required.     This paper deals with the important issue of continual learning (CL). Broadly, the objective of continual learning is to allow a network to learn a series of tasks online, without forgetting previously learnt tasks (catastrophic forgetting). There are a range of different ways to tackle this problem, and this paper starts by providing a clear motivation and a comprehensive literature review. Most current methods are largery optimisation based, or memory/replay based.   The approach (MRCL) this paper takes is to learn a representation such that when used in a CL setting, it allows for greater transfer and less interference. This is similar to a MAML-style method for meta-learning, where the idea is to find an initialization that allows for rapid adaptation. Instead here, we want a representation that allows for continual learning. (Another work to reference that is missing from the paper is “Memory-based parameter adaptation” that does a similar MAML style update albeit from a memory-buffer).   The method here involves splitting the network into two sets of weights: a representation learning network (RLN, weights W) and a task learning network (TLN, weights T). The TLN will be trained to maximise reward/minimise loss on the task at hand, with fixed representation from the RLN. After doing this for k steps, we will have a set of weights W_k. We will then sample a fresh batch and update the initial W_0 and T_0 wrt to the loss on this current batch with parameters W_k and T_0, to give us W’ and T’. Thus, we meta-learn an initial representation that allows for good online learning. There is good intuition provided as to what such a representation looks like wrt the solution manifold.   * Please clarify in equation (3) what the expectation is taken with respect to.            Further please consider moving the diagram and pseudocode from the            appendix to the main paper for clarity.   The first task considered is an incremental sine wave task where there are K sine waves to be regressed. At pre-training, the RLN is meta-trained in the fashion above, and at the evaluation time is fixed as a feature extractor. This is compared to a pure pre-training method and another SR-NN method.  Please provide more details (possibly in the appendix) for how this sparse SR-NN method works.   We find the MRCL allows less interference than other method. This approach is also repeated for split-omniglot.    The paper also provides good insight into sparsity that results from training in this fashion However, it would be helpful if the authors could provide a concrete definition of “instance sparsity”. This section could also be shortened somewhat.   One question is how more traditional methods like EWC or memory-based methods would do in this regime. This method uses a modified version of the standard CL setup: ie allowing pre training before the tasks. This is evaluated in the last section where MRCL is viewed as orthogonal to these methods and combined as a pre-training method.  Could the authors however please clarify the numbers in Table 1 -- are these average over all tasks? How do these related to figure 4? The numbers for EWC are surprisingly low, to me. Could you provide some intuition as to why? How converged were the models when the fisher matrix is calculated? One possible extension would also be to run this on permuted MNIST (where you pre-train on another set of permutations) and compare to doing standard EWC with/without pre-training. This would allow the numbers to be compared to more standard benchmark tasks in the literature.   Overall, this paper is a very useful contribution to the field and will provide a significant starting point for future research. I recommend acceptance.  