To my knowledge, the idea of coupling perturbations across examples is a new idea, and worthy of additional exploration. This paper makes a nice contribution in that direction. The idea, and algorithmic instantiation, both seem well-done. The paper claims (significantly) state-of-the-art results across a variety of tasks. Currently, the adversarial evaluation seems suspect, but if these holes are addressed, this paper would be a strong contribution in originality, quality, and significance. (I remain unclear whether the proposed algorithm ought to be an improvement over standard adversarial training, and thus, the empirical results are particularly important here.)  My major concerns are whether gradient masking is present in the model, and whether it is tested against the strongest possible attacks. I will focus on CIFAR-10 at eps=8, as this is by far the most competitive benchmark. First, it is suspicious that the black-box attack (transferring from an undefended model) does better than the white-box attack. Against most adversarially robust models, black box attacks are extremely weak. E.g. Madry reports 86% accuracy when transferring from an undefended model. Second, the large gap between PGD and the CW-variant (the paper says this is PGD using the CW loss) is suspicious. If I understand correctly, the only difference is a cross-entropy loss vs. a margin loss. In typical models, e.g. Madry, these converge to similar values by 100 iterations, but here there is an 8% gap (68.6 vs 60.6).  Several comments on clarity, and miscellaneous questions:  I found the description of Algorithm 1 unclear. For estimating grad_x’ D(mu, nu), my understanding is that the algorithm first estimates the transport matrix T (using e.g. Sinkhorn), and then computes the gradient treating T as fixed, ignoring the dependence of T on x’. On first read, it was unclear how this gradient was estimated, and this seemed the most sensible to me, but please correct me if this interpretation is mistaken.  How are u_i and v_i defined? I couldn’t find this in the paper. The natural choice seems to be 1/N (uniform over the dataset), but why introduce u_i and v_i in this case? e.g. the description in equation (7) would be more straightforward.  Minor: I would mention Sinkhorn and IPOT sooner. Otherwise the reader is wondering how the max in Eq. 6 is solved, until the end of the experimental section.  Minor: Feature scattering seems to combine two distinct ideas: first, using an unsupervised adversary operating on distances between activations (rather than labels) and second, coupling perturbations across examples. It would be nice to decouple the impacts of these — looking at the Identity matching scheme is nice (as this isolates the first idea without the second), and the comparison here could be developed further.  Why pick label smoothing lambda=0.5? In the supplement, lambda=0.8 seems significantly better (65% against strongest adversary vs. 60%).  It’s nice that only 1 attack iteration is necessary for the reported results. I think this is worth emphasizing earlier.  Finally - I’m glad to see the authors note that they intend to open source their model and evaluation code. I believe this is a great practice for the adversarial robustness community.  __________________  Update:   I have changed my score from a 5 to an 7, largely in response to the noticed bug regarding reported black-box numbers in fact corresponding to a white-box evaluation (as well as additional convergence plots, and stronger attacks).  With the reconciled results, the paper proposes a promising idea (computing perturbations which are coupled across examples), and a significant improvement over SOTA on a competitive benchmark (CIFAR-10 at eps=8, white-box).  I am somewhat hesitant after the initial mistake. In particular, this seems like a sanity check which the authors should have noticed before the submission, and the fact that it was unnoticed is somewhat concerning. The fact that the code is being open-sourced, and so that it will be relatively easy for the community to verify the claims made in the paper is a significant contributing factor to my updated score, and not dwelling too much on this oversight.  I would also encourage the authors to include the additional adversarial evaluations (with fixes as suggested by other reviewers) and ablation studies (over label smoothing parameter) in the final version.