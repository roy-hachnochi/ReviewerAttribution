Update after reading authors response:  I changed my score to accept after reading author's response because authors addressed most of the comments from reviewers and they also explain that initial black box results [which created impression of gradient masking] were erroneously entered.  Still my concern about gradient free attack not fully addressed. Authors did provide result on gradient free attack, however chosen method (http://www.shivakasiviswanathan.com/CVPR17W.pdf) seem to be weak attack. So I would highly encourage authors to perform experiments using few additional gradient free attacks (ex: https://arxiv.org/pdf/1802.05666.pdf ) and report it in the final version of the paper.  ----------------------------------------------- Original review:  Originality: The paper proposes a novel technique to improve model robustness against L-infinity adversarial attacks. The technique takes into account similarity between features in the minibatch.  Quality: There are several issues with the paper which makes me lean towards rejection:  1) One big question which seems to be not address is computational complexity of the proposed method. With the same number of inner iterations T, the proposed method requires more compute compared to PGD adversarial training. So assuming that number of iterations T were the same in the experiments, proposed method is actually given some advantage in terms of extra compute.  2) I could not find information about number of iterations T of inner optimization which was used in experiments. I assume that it was the same for PGD adversarial training and feature scattering, but it’s not clear from the text.  3) One of the issues which feature scattering claim to be addressing is label leaking. However as authors mentioned it could be addressed by other means, like guessing labels. So I wonder how proposed method would compare with PGD adversarial training which does not use true labels.  4) There are issues with evaluation of proposed defenses: 4a) According to table 3, proposed method reaches 68.6 accuracy against PGD100 attack and 60.6 accuracy against CW100 attack in white box case. At the same time according to table in the section 5.3, it has lower accuracy against same attacks in black box case. The fact that white box accuracy is lower compared to black box accuracy is usually an indication of gradient masking. 4b) Authors only limit their evaluation to no more than 100 iterations of PGD attack. Also it’s not clear whether they do random restarts and how many. And from the data tables is could be observed that accuracy is decreasing with increase of number of iterations. This means that attack which was used for evaluation might be too weak. 4c) No attempt to use gradient free attacks to evaluate robustness.   Clarity: Paper is reasonably well structured. The section about feature scattering is somewhat harder to read. I would encourage authors to expand their intuitive explanation on what happening during feature scattering. Also it would be useful to include intuitive explanation and/or illustration on what happening in feature scattering adversarial training.  Significance: If all issues with evaluation are addressed and method still shows improvement over baseline then I would say it has moderate significant: it does not solve the problem of adversarial examples completely, but it does provide an interesting idea and improved metrics. 