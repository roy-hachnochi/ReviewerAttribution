****** Update ******* Like other reviewers, I'm happy to see that there was a good explanation for why the black-box setting was broken.  There are two additional points, I'd like to make:  1) The authors either explain the multiple restarts setting wrongly or apply it wrongly. Instead of running 5 separate evaluations and picking the worst (min of mean of accuracy under attack), they should repeat for each example in the dataset the attack 5 times and take the worst (mean of min). This is really important and the authors to fix this and add these results to the paper.  2) I encourage the authors to share their model as quickly as possible and before the conference. This will allow the community to make sure that the authors claims are correct. If they'd rather have this evaluation made in private before fully releasing the model, I'm happy for the AC to transmit my contact details.   All in all, I believe the current score is fair if the authors follow-up on their promise to open-source their code (and hopefully model). ***********************  The authors introduce a feature scattering-based adversarial training approach (based on solving an optimal transport problem). The main motivation is avoid label leaking. Hence the authors also use label smoothing.  Overall, the paper is well written. The motivation is sound and the evaluation seems appropriate. In any case, the results are very impressive (beating the previous state-of-the-art method by 4% in absolute terms on CIFAR-10 with a similar evaluation - see TRADES: TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization by Zhang et al.). As always, it is very hard to judge whether the evaluation is done correctly and I would urge the authors to release their models if possible.  Pros: * SOTA results. * Novel method to generate adversarial examples.  Cons: * Some details are missing (e.g., better study of label smoothing). * The black-box attack is stronger than CW100 which is suspicious.  Details: 1) Label leaking seems to be the core of the problem. The authors should define it earlier in the introduction. 2) The transport cost is defined by the cosine distance in logit-space. Can the author motivate this choice? 3) The batch size is likely to be an important factor. Can the author provide results with varying batch sizes? 4) Label smoothing is very large (0.5). On CIFAR-10, I haven't seen such a large smoothing applied to adversarial training. The authors should analyze the effect of such smoothing (or consider removing it). In particular could the author add rows corresponding to 0.0, 0.1 and 0.2 to Table 1 in the supplementary material and also evaluate standard adversarial training (Madry et al.) with such smoothing applied. 5) The table in Section 5.3 (black-box attack) is slightly at odds with the other results. In particular, the examples found using the undefended network seem to transfer extremely well (even beating CW100 on the original model) yielding an accuracy under attack of 59.7% (while CW100 yields 60.6%). Maybe I misunderstood what the authors meant by black-box attack.  Minor: a) In Table 1, the notation 0.0 and 0.00 is inconsistent.