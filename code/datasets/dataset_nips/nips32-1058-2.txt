-- Post author-feedback comments --  I would like to thank the authors for their response to my concerns and for clarifications on the methods. The authors seem to have done much work to improve their submission.   In particular, I am glad they have included another dataset to evaluate their algorithm. I still do think they should explore other choices for the kernel definition, in particular, the values they choose for $K((\tau, i), (\nu, j))$ when $\tau \ne \nu$ and $i \ne j$. There is a full spectrum of possibilities between regular PHATE and M-PHATE in terms of what the structure of $K$ can be. I do recognize it is a non-trivial choice, and perhaps useful to include in your future work.  As such, I raised my score for the submission to 6.  -- Initial review --  The main contribution of this paper is an extension to the PHATE algorithm by Moon et al. [14] -- here coined: the Multislice PHATE (M-PHATE). The algorithm relies heavily on a proposed a multislice kernel that quantifies similarities between different units of a NN within the same epoch as well as between the same unit but at different training timepoints.  Originality: ------------ M-PHATE slightly modifies PHATE by defining a heuristic multislice kernel over all the hidden units and all epochs of training. The proposed kernel is composed of a varying-bandwidth Gaussian kernel for entries corresponding to different units for at the same epoch (intraslice), and a fixed-bandwidth Gaussian kernel for same units across different epochs (interslice), zero value is assigned to all other entries. Gaussian kernels are a standard choice for quantifying pairwise affinities; the paper justifies the selection of the fixed bandwidth for the interslice part, but provides no reasoning as to why adaptive bandwidths should be used for the intraslice part. The new coordinates for visualization are computed following the same procedure as in the existing PHATE [14] based on MDS over the information distance ($\|\log P^t(i) - \log P^t(j)\|$) defined for the row-normalized multislice kernel ($P = D^{-1}K$).  Novelty of this paper lies more in new applications than new technical methodology. The authors use M-PHATE to visualize the training process with different regularization techniques or the continual learning task with different optimization training schemes.   Quality: -------- M-PHATE uses k-th nearest neighbor adaptive bandwidth for the Gaussian kernel, which can be very unstable and sensitive to outliers. Perhaps an average over the first 1 to k nearest neighbors, or an entropy equalizing based procedure (like the one used in t-SNE) is a more robust strategy.  Apart from that, using the varying bandwidth means that method stretches and contracts different unit neighborhoods at different rates. It is not clear why the authors utilized a fixed bandwidth kernel for ``temporal" (iterslice) but not for ``spatial" (intraslice) variation between units.  There can also be interactions between neighboring units at different epochs, and assigning zero entries corresponding to these combinations ($K((\tau, i), (\nu, j))$ for $\tau \ne \nu$ and $i \ne j$) night not be adequate. With the current choice of the multislice kernel, with no interactions allowed between different units across different epochs, it is somewhat unsurprising that the resulting visualizations mainly show temporal changes and unit trajectories over training time.  The experiments included in the paper were reasonable and helpful for understanding the objectives of the paper. However, the quantitative   as well as visual results do not conclusively show M-PHATE's superiority over other methods. The effectiveness of visualizations was evaluated only with the neighbor preservation, which cannot capture the quality large scale or global structures representation. M-PHATE also does not outperform t-SNE at inteslice neighbor-preservation, which is concerning as the method was intended for displaying the NN dynamics. The parameter selection for t-SNE was not reported, raising a question of whether t-SNE would performed even better if e.g. perplexity setting was adequately tuned.  The visual results show that M-PHATE seems to produce visualizations that have a more coherent global structure consistent with the training evolution over epochs. However, it seems that the M-PHATE show only the dynamics, and the intraslice variation and organization of the units within the same epoch cannot be easily perceived due to crowding of the datapoints.   Clarity ------- Overall, the paper is clearly written, and it's easy to understand the ideas proposed. However, there are some parts that require clarification, e.g. the paper does not explicitly state what is the ``standard kernel''. I assumed that the kernel was simply a Gaussian kernel over the z-score activations, but it is not clear if adaptive bandwidth or fixed were used.  Minor typo: line 111 I think should read ``(...) j-th unit at epoch $\tau$ (...)'', not ``(...) j-th unit at epoch t (...)''.   Significance ------------- Developing visualization tools to improve the understanding of the behavioud of the NNs throughout the training process is important, and and the authors of this paper made a good effort to tackle this task.  Even though, the choices in the paper might have not been optimal or fully tested and explored, the paper makes a step forward by explaining how hidden units can be embedded, if a suitable similarity kernel can be defined between units and different training epochs.  I liked the idea of comparing different training optimization schemes (here Adagrad, Adam, Naive Rehearsal) applied to continual learning through visualization.  It was also useful to see visualization of NNs trained with different regularization schemes, exhibiting different patterns on the M-PHATE plot, depending on generalization performance ranging on a spectrum from pure memorization to high generalization. Although, it seems the interpretation of M-PHATE plots is still not straightforward and should be caustiously considered.