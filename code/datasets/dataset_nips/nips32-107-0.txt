Clarity: the paper is general clear, and concise regardless of the substantial technicalities (but see below). Paper organization is sensible in that too technical proofs are left to the appendix and the overarching proof strategy is put as a separate section.  Originality: this line of research seems very recent, and this paper appears as a natural and timely continuation of [1], which empirically supports that gradient noise in SGD is heavy tailed.  Quality and significance: I believe theoretical results by themselves are deep enough to judge this work as having high quality. This adds to the empirical validation, which are also insightful. I am only a bit concerned this work might be taken as an excuse for making theory, but losing connection with the broad machine learning community.    [1] https://arxiv.org/abs/1901.06053