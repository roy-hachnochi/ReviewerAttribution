Pruning in CNN models has gained a lot of attention in the recent years and this paper introduces a new dynamic channel pruning technique. The paper introduces a simple and effective dynamic channel pruning technique along with accelerators for ASIC hardware showing actual execution time speedups.  Pros 1. The paper does a good job covering the related work for pruning in CNN models. Static vsndynamic pruning and channel vs parameter pruning are well explained. The channel gating layer proposed by the authors is dynamic and more fine-grained than [8] which is the closest related paper to this work. However, references and work related to sparsity in the parameters is missing. 2. The channel gating layer introduces a gating mechanism designed on the activation function. The channel block is simple and effective. It doesn't add much more compute on top of the existing layer and requires to compute partial sums all of which seem current hardware friendly. 3. The experimental results are significant across various models and datasets. The authors show an improved FLOP reduction while getting better Accuracies than other pruning methods referenced in the paper. Also, using knowledge distillation the FLOP reduction gains are furthered. 4. Real time execution speeds ups on ASIC hardware. The speedup is very close to the theoretical gains validating the results further.  Cons 1. Dynamic channel pruning is explored in multiple related works. The differences seem small between the various techniques. 2. Dynamic pruning doesn't save storage space. It'll be interesting to compare the FLOP reduction of sparse (weight sparsity) models vs channel pruning models to understand the tradeoff between Accuracy and FLOP reduction further. 