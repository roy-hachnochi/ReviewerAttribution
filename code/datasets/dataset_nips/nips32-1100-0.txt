- It seems like the second layer in the model is lower dimensional than the first layer. Is there evidence of a dimensionality reduction from retina and LGN that would match this feature of the model?  - "most artificial systems are obtained through heuristics and hours of painstaking parameter tweaking." => Does not sound like a relevant comparison, because these artificial systems can solve much more complicated tasks than MNIST.  Clarity: - The task on which the network was tested (MNIST) should be mentioned in the abstract. - "The algorithm organizes inter-layer connections to construct a convolutional pooling layer, a key constituent of convolutional neural networks" => The term "convolutional" implies weight tying, but here you can only obtain locally connected units, without weight-tying. Please change language here and everywhere CNNs are mentioned. "Our work on growing artificial systems got us interested in how critical times of different developmental processes are controlled, and whether they were controlled by an internal clock." => Please share your thoughts on this interesting aspect (or remove this sentence).  Originality: looks original, but the authors you be more explicit about how you work is different than refs [25->29]. In particular, can you expand on what these other studies contributed?  Significance: Interesting results for neuroscience. How could this network apply to ML (beyond a slight benefit on MNIST compared to random networks)? Maybe new bio-inspired hardware?