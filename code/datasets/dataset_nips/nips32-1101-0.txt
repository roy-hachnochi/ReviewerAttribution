The method proposed in this paper are novel and the experiments are very solid. In table 2 they show the result evaluated in 5 datasets, 4 different sample rates and 3 different methods combined with the new method. The proposed method performs very well in all situations.   However, I am not convinced by the explanation given in the paper.  First of all, I don't understand what does transfer in 'negative transfer' means? Is it mean to fine tune from a pretrained model? Or to use the feature from a pretrained model? Or to use some technique like L2-SP to preserve the previous feature?  The authors conclude that the negative transfer does exist in 3.2 and Fig 1(a). But from my prospective, fig 1(a) only shows that L2-SP performs worse than L2.  Fig 1(b) shows that lower layers are more transferable than higher layers and the transferability can be somehow indicated by the singular value. Fig 1(c)(d) shows that with more training data, the singular values are smaller, especially in the smaller half of sigma. These are all very interest result, but I cannot see how it related to negative transfer.  