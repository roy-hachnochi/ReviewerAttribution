This paper investigates whether negative transfer exists in fine-tuning deep networks pre-trained on a domain and how to deal with it. Batch Spectral Shrinkage (BSS) is proposed as a regularization to suppress which features that potentially cause negative transfer, indicated by small singular values of SVD(F), where F are the feature matrices.  The main problem addressed by the paper is vivid and well motivated. The proposed regularization to alleviate the problem is intuitive, through the familiar singular value decomposition. The empirical evaluations show the effectiveness of the BSS regularization over a range of datasets -- models with BSS perform on par or better than those without BSS after fine-tuning, especially with limited number of fine-tuning examples.  Improvements: - In my opinion, the fine-tuning step is a special case of continual learning (CL) that only has 1 additional step. It would be interesting if BSS can be incorporated into existing CL methods such as EWC (Kirkpatrick et al. 2017) and/or LwF (Li & Hoiem 2016) as well -- is it even possible to do so? - It would be great if there is a BSS fine-tuning use case other than visual recognition, e.g., text classification with pre-trained word embeddings, that can be evaluated.  ============== After the rebuttal  I thank the authors for providing the response to my concern, by reporting the additional experiment outcomes with EWC and on text classification. My final score is up to 1 level.   Please incorporate the new experiment results into the manuscript / supplemental materials.