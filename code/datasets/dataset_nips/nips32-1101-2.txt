1. The reviewer thinks that the novelty of this paper is not enough. The title of this paper is “Catastrophic Forgetting Meets Negative Transfer”. However, the part that deals with catastrophic forgetting only uses the previous methods, and the formula only extends the proposed BSS regularization to the previous methods. There are also no ablation studies to verify the effectiveness of the two parts, i.e., catastrophic forgetting part and negative transfer part. 2. Line 167 mentioned that “in the higher layers, only eigenvectors corresponding to relatively larger singular values produce small relative angles. So aligning all weight parameters indiscriminately to the initial pre-trained values is risky to negative transfer.” Then why not re-initialize all the high-level parameters and train again? Part of the transfer learning, i.e., “A Survey on Transfer Learning”, only transfer the parameters of lower layers. Are there any experiments to verify the pros and cons of this process? 3. The paper analyzes the influence of network parameters and feature output representation on negative transfer. Why use feature regularization instead of parameter regularization? Are there any experiments to verify? 4. The paper mainly solves the negative transfer phenomenon in fine-tuning. But the comparison methods are all about catastrophic forgetting, and there is no negative transfer method. Why not compare with the state of the art negative transfer methods?  “Characterizing and avoiding negative transfer” 2018; “Deep coral: Correlation alignment for deep domain adaptation” 2016 “Adapting visual category models to new domains” ECCV 2010 “Adversarial discriminative domain adaptation” CVPR 2017 5. Some statements in the paper are repeated, and the format of the reference is very confusing. 