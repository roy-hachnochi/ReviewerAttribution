This paper proposed to reweight samples using a simple one-layer MLP in a meta-learning manner. The proposed method is both theoretically and empirically justified. Theoretically, the convergence of the proposed method is proofed. Empirically, the proposed method is justified in both class imbalance and noisy label problems.   Learning sample-reweighting from data is not a new thing. As introduced in related work section, there are other methods in this line, e.g. MentorNet and Learning to Reweight (L2RW). MentorNet also learns to reweight samples from data with the aid of a neural network (a LSTM). Can you discuss more about how the choice of these explicit reweighting functions influence the results?  In noisy label experiments, the classifiers are trained in only 40 epochs for uniform noise, and 60 epochs for flip noise. My concern is that not all methods can converge in too few epochs. It more be more clearly if the whole tendencies of different methods are compared with more training epochs, e.g. 200 epochs. Without seeing the whole tendencies, we cannot simply say that the proposed method is converged faster than other methods (as claimed in line 101 of supplementary materials).  