From what I understand, the goal g is typically a projection of the state s of the agent. The agent could be in different states s_1 and s_2 at the same goal g, but states s_1 and s_2 could be not easily reachable one from the other. For example, g could be a location on manifold (e.g., a city map) and s_1 and s_2 could be two different states (same position but opposite orientations) of a mobile agent (e.g., a car). To go from s_1 to s_2 is not trivial as it requires changing the orientation of the car (it might imply around a block. If the information that is used for planning corresponds to only pairs of (s, g), would not the difference between states at goal locations be lost? In that case, can we still write that the cumulated rewards for a complex task d(s, g) is the sum of d(l_k, l_{k+1}) for consecutive landmarks? Or does it simply mean that g should not be only a position, but also an orientation, or alternatively, that the planner works on pairs of states, rather than pairs of state-goals?  Minor comments: In section 5.1, explain why the cardinality of the state-goal pairs is O(R^{2d}). The color code on Figure 4 could be consistent between (a) and (b) / (c). 