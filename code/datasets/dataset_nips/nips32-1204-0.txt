EDIT I was pleased with the authors answer about the duality gap. I encourage them to include this paragraph in their revision.  I hope the public implementation of the code will be released for reproducibility. I have updated my grade from 6 to 7. ###################"" The article reads quite well, with numerous examples and explanations of  convex optimization concepts. The methodology is strongly inspired from blended conditional gradient.  The experimental validation is dubious to me as for loops are usually costly in python. The authors should at least use numba and @njit to use just in time compilation.  How are the entries of X sampled ? If the design is near to orthogonal (eg if the entries are iid centered gaussian), then the problem is very easy and the  figure may not reflect reality. Why not use real life datasets, eg from LIBSVM, and simulate only x^* and w (the observations y in these datasets could also be used)?   I disagree with the sentence L15. Sparsity of the **solution** is important, but sparsity of the **iterates** does not affect generalization nor interpretation. Getting the sparsest solution is also important in the case when there are multiple ones, but that is not clear in this sentence.  L35: which algorithm needs RIP? Usually RIP is needed to analyze the quality of the solution to Pb (1), not to analyze the convergence of an algorithm.  Fact 2.1: please provide a reference. L83 I think some hypothesis is missing, f = indicator of {a} is strongly convex, but nabla f (x*) does not exist.   The quantity \phi_t was not detailed enough in my opinion. How is it a dual gap  estimate ? What is the reason for dividing it by tau at every dual step, and why are dual step coined like this?    Finally, the code cannot be run on my machine: 363  364     # if still nothing --> 365     return _, 'FN', _ 366  367 def weak_sep(c, x, phi, kappa, S, D):  NameError: name '_' is not defined   And it seems that dual steps are never taken before: In [1]: %run bmp_code.py                                                         ********** η = 100 **********   Total CPU process time: 30.582956643000003   Number of constrained steps: 73 Number of dual steps: 0 Number of full steps: 39   ********** η = 10 **********   Total CPU process time: 30.691314241999997   Number of constrained steps: 41 Number of dual steps: 0 Number of full steps: 81   ********** η = 5 **********         Very cosmetic: L97 you have two different spelling for Lojasiewicz  Algo 1 L5 L6 the authors may want to define \DeclareMathOperator{\argmin}{\mathrm{arg\,min}} to avoid the space appearing between arg and min because of the long text under.  L52 can't it just be \lambda_j ?   The notation H* L66/ R^n* L102 is confusing and I suggest the authors find a better one.