Paper summary: This paper proposed a novel text-to-image synthesis method with its focus on learning a representation sensitive to different visual attributes and better spatial control from the text. To achieve this goal, a novel generator with word-level spatial and channel-wise attention model has been adopted based on the correlation between words and feature channels. To further encourage the word-level control, a novel correlation loss defined between words and image regions has been explicitly enforced during model learning. Experimental evaluations have been conducted on benchmark datasets including CUB and COCO.  Overall, this paper is an interesting extension to the text-to-image synthesis and multi-modal representation learning. In general, it is clearly written with reasonable formulation and cool qualitative results.   However, reviewer does have a few concerns regarding the current draft. - Current title is ambitious or not very precise as text-to-image generation is often coupled with a learned controllable representation.  - What is the shape of visual features v? As far as reviewer can see, in Equation (1), v indicates the feature map with single image channel; in Equation (2), v indicates the feature map within image sub-region. It would be great if this can be clarified in the rebuttal. - Equation (6): j is defined but not used. - Table 1 and Figure 3: While reviewer is impressed by the results on CUB, results on COCO donâ€™t look good. Also, it seems like the model only learns to be sensitive to the color but not shape or other factors. - Figure 4: On CUB, two images generated by similar text (with one concept difference) look almost identitcal (e.g., same shape, similar background color). On COCO, It is clear that two images generated by similar text (with one concept difference) look a bit different (see last four columns). Reviewer would like to know which loss function enforces such property. Why the same loss completely fails on COCO? 