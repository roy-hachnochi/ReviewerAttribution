[I read the author response; my score remains unchanged]  The writing of the paper is very clear, and the paper does a good job of motivating the method and describing the technical details of the approach. I did not find any issues with the technical derivations for the VAE formulation, and to my knowledge the proposed VHE method is novel. The proposed method seems very practical, especially because it can tolerate incomplete graphs, missing edges, and new nodes that appear at test time.  The authors include their phrase-to-word alignment system among their list of contributions, but that portion of the work appears less novel. There have been a number of methods that rely on mutual attention between two sequences of tokens (see e.g. https://arxiv.org/pdf/1606.02245.pdf ; https://arxiv.org/pdf/1611.01603.pdf), and to the extent that the authors' implementation has incremental differences over the past work there is no indication that these differences affect accuracy. That said, the word representation method is not the key focus of the present paper. [Regarding the response: my initial reaction reading the paper for the first time was that the authors were claiming novelty of the architecture itself; I agree that the application of such an architecture to this specific task is separate and can be considered a contribution of this paper.]   [Thank you for promising to address some of the presentation issues below]  77: Why is it "without loss of generality"? I don't follow.  190: Are K_r/K_c a function of L_j/L_i, or are they fixed hyperparameters? I think "number of filters" generally refers to the channel dimension, but these are sequence length dimensions instead. In general the text in 183-195 isn't as clear as the remainder of the paper.  Grammar and formatting:  20: its -> their  265: What does PWA stand for?  Table 2: Could you say more about the methods? For example, adding columns to indicate whether each method encodes topology, content, both, is a discriminative method, is a generative method. Defining the acronyms might also help.  295: "When VHE works?" -> "When does VHE work?"  344: grammar seems a bit odd at the phrase "are not necessary". Maybe "documents with similar info do not necessarily cite each other", or "documents citing each other do not necessarily have similar info" (depending on what the intended meaning is)  [349: textural -> textual (?)]