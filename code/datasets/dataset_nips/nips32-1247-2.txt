General comments: the paper presents a theoretically sound algorithm which also boosts practical performance of deep RL algorithms. The motivation of the original n-step Q-learning update is to ensure that reward signals can propagate effectively throughout a sampled trajectory, which is aligned with the method proposed in the paper. To address the issue with correlated states and overestimation, the author proposes a diffusion scheme that mediates the learning.   Detailed question: 1. The method seems simple yet effective. I feel like a major issue with the implementation is that sampling a whole episode is more complicated than sampling a fixed length trajectory. Does this occur in practice? 2. In Section 3.3, the author introduces an adaptive beta scheme, which could potentially allow for doing without an additional hyperparameter search. However, my understanding is that in practice, the algorithm trains K networks with varying beta in parallel and picks the best one to proceed. This feels like carrying out hyperparameter search on the fly instead of being 'adaptive'? 3. If the n-step Q-learning has a large n (such that n > T, T the episodic horizon), will this correspond exactly to the episodic backward update? 