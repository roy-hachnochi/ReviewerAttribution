Originality: to the best of my knowledge, the proposed algorithm is new in this area. Unlike prior one-shot distributed learning algorithms whose statistical error rate does not approach 0 as m approaches infinity, the error rate of this algorithm decays with m. The decay rate as a function of m is not very fast when d is large, (m^-1/d), but this matches the lower bound so there is not much we can improve. The design of the algorithm also has some interesting ideas on how to encode the local gradient information of the loss functions and communicate them using a limited number of bits.  Quality: I think the paper is technically sound. The assumptions and main results are stated clearly. One point that I am not very satisfied is that it seems that there is a dimension dependence and it seems this dependence is hidden in the big-O notation. How does the final accuracy (or communication complexity) depend on d? Is it optimal or can we improve it? How do we compare it with prior works?  Clarity: I think the paper is overally clearly written. It is easy to follow and I think I understand most part of it. Some minor comments to improve the clarity: Prior to Section 3.1, it seems that the function is defined in the space [-1, 1]^d. But in Section 3.1, it changed to [0, 1] without an explicit statement. Page 5, line 178, \theta_i -> \theta^i.  Significance: I think there is some significant contribution in this paper. In addition to my prior comments, it would be great to improve the experiment section. Adding real-world datasets could be helpful.  ========================= After author feedback: The authors clarified my concerns on the dimension dependence. I still hope to see some experiments on real-world datasets. I changed my score to 7.