------ update after the rebuttal ------ the following points are revised from the discussion ------  1/ In multiview learning, normally people could collect data from two conditionally independent resources or people could split the current existing data into two disjoint populations which creates multi-view in a cheap way.  The way people split data into two disjoint populations could be thought of as minimising the mutual information between two "representations" of the same data. My point is that the authors shouldn't claim their work as totally independent/different from multi-view learning work in the rebuttal since IMO these two research lines are deeply connected.  2/ Maybe it is just my personal opinion. If the goal is to learn diversified vector representations, the paper needs to thoroughly justify the reason for using information bottleneck and also the whole variational inference, which was mentioned in my first review.  To me, this paper threw variational autoencoders every where possible and didn't even both looking into the derivation and checking whether there were redundancy.  From a probabilistic point of view, given x, y and z are naturally conditionally independent. Since the label is also presented in learning, which diminishes the conditional independence between y and z, the only thing we need to consider is to make sure that the mutual information between y and z is minimised, which could be implemented by a single adversarial classifier. That is the immediate thought I have when I was reading the title. With a certain Lipschitz constraint, people can prove error bounds on this issue.  3/ I couldn't figure out why I(r,t) term was necessary as there are two classifiers on y and z respectively, and the classification on the downstream tasks could rely on these two classifiers.  ----  Variational inference is indeed interesting and also variational autoencoders are a huge win in the deep learning settings as SGD optimisations could be applied. However, we still need to carefully consider and justify why variational inference is necessary here.  ----  ------end of the update -------  1/ The necessity of the information bottleneck objective is doubtful. The goal of the information bottleneck objective is to learn "minimally sufficient statistics to transmit information in X to Y (which is T in this paper)". Given the goal here is to learn diversified representations of the input source, I don't see how the information bottleneck objective is being crucial here.  2/ Learning diversified objective through minimising the mutual information between two or among multiple pieces of information is not new. The performance gain one can get in multi-view learning or consensus maximisation is by ensuring that data representations (raw input representations or distributed representations learnt in neural networks) are conditionally independent given the correct label if they belong to the same class. Therefore, after learning, fusing multiple representations leads to improved performance compared to individual representation. This idea has been established around 30 years ago, and I recommend authors to at least refer to their papers in multi-view learning.  3/ If the main goal is to say that the proposed framework is capable of learning diversified representations and fusing representations gives better performance, then at least, a proper baseline should be, for a given baseline model, train two models with different initialisations and then take the ensemble of them when comparing performance.   4/ Lack of ablation study of the proposed objective. This relates back to my first point that the information bottleneck objective is not necessary and also the proposed paper didn't show why it was crucial to have it. Also, the objective function itself seems to be obese. For example, maximising I(z,t|\phi_z) + I(y,t|\phi_y) is a sufficient condition for maximising I(r,t|\phi) given that r is a concatenation of z and y. I hope the authors could critically evaluate their proposed method.