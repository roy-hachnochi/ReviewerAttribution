Originality This paper presents a new way to use GANs in hypothesis testing. It was very interesting to use GANs to construct a null distribution that adapts to the dataset without strong assumptions. The proposed method can be used for feature selection and explainable neural networks.  Quality Authors support their framework with theoretical justification and empirical results. The quantitative experimental results are limited to synthetic data but comprehensive and expected behaviours of the GCIT are observed in synthetic experiments. The only experimental result with real data is shown but it is hard to tell which result is more accurate and powerful. This issue will be raised when GCIT is applied to real-world applications.   Clarity The method is clearly described and sufficient theoretical analysis was done. Exchangeability of samples and statistics were checked. The paper is well-written so that any machine learning scientists appreciate the main contribution and their intuition.  Significance The main concern of this work boils down to the robustness of the method. As the authors have shown, the method dependent on hyperparameters (e.g., Lambda, the architecture of GANs) and quality of parameters of neural networks. Especially, in some academic fields, getting a p-value less than 0.05 is crucial to get the paper published. In this case, it is doubtful that the proposed method can be accepted in that community since by training models with bigger lambda or different GAN architectures will allow them to boost their p-values. The protocols to avoid overfitting of GANs, and choose hyperparameters and models should be carefully analyzed.