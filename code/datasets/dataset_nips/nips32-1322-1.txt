The authors propose a new attention mechanism where the large amount of parameters that are required compute the affinities between the states and input units can be alleviated by sharing the parameters used to encode the different align-able units. As such, the number of parameters needed to incorporate the attention mechanism reduces substantially, and additionally, results on language modelling and translation seem to indicate that the proposed mechanism generalizes better with fewer parameters.  The paper provides a good self-contained extension the transformer class of models, which is worth publishing. 