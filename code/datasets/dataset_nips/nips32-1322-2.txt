# Clarity   I had difficulty understanding how the authors implemented their solution, so I looked at the code in the supplementary material. This code failed to compile, and had numerous confusing aspects, and the authors did not link to the actual code used in training the model.  *Confusing aspect of code #1* There are some parts of the code that are not valid python. For example: ```  for i in range():            cores_tensor[i][i][i] = self.core[i] ```  *Confusing aspect of code #2* Within the MultiLinearAttention class, the authors provide the following branching logic: ```        if n_head > 1:          output_1 = self.Tattention(q_s, k_s, v_s, mb_size,d_v)          output_2 = self.Tattention(q_s, k_s, v_s, mb_size,d_v)          output = (output_1+output_2)*0.5        else:          ouput = self.Tattention(q_s, k_s, v_s, mb_size,d_v) ``` My understanding is that `self.Tattention` is a pure function, therefore the True branch should always be numerically identical to the False branch, but at 2x the cost.  === # Originality  This result seems motivated by the same problem as "Generating Long Sequences With Sparse Transformers" (https://arxiv.org/abs/1904.10509 April 2019), but I could find no comparison with that work.  === # Significance I think that the results of using 50% fewer parameters is quite significant. However I would also like to see the total flops usage compared to the baseline, as flops are frequently the limiting factor for training and deployment of models. 