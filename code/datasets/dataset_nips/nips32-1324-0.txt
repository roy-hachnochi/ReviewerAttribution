Overall I think this is a nice and very clearly written paper. The authors made effort to make the article both precise and readable. The extension of model comparison using KSD rather than MMD is, I think, significant as KSD can be sometimes a more appropriate choice of divergence than MMD. On the other hand I thought the authors didn't really justify why it was important to consider the more general case in which there were more than 2 models to compare: is this an important problem? While in the case l=2, the first experiment shows RelPSI does not perform better than Rel and loses power.  A couple of minor comments :  (Line) 90 - the kernel does not define an inner product: the RKHS has an inner product since it is a Hilbert space 93 - "we interchangeably write k(x,.) and \phi(x)": why is this justified? You havent specified which feature map \phi is  97 - MMD^2 is not a pseudometric 99 - "MMD defines a metric, i.e., MMD^2(P,Q) = 0 iff P=Q": that s not the definition of a metric 107 - original -> originally 109 - we need p>0 for the score to be defined (and the kernel needs to be differentiable in the next line) 111 - What norm is used on H^d?  115 - u_p is a matrix rather than a scalar since the gradient is a row vector by mathematical convention unless otherwise specified  116 - \hat{KSD}_{u}(P,R) should be just \hat{KSD}_{u}, and the x,x' in the sum are missing their indices 150 - should be argmin instead of min  179-180 - I didnt really understand this sentence 