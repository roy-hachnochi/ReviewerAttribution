I thank the authors for their response, and would like to maintain my overall evaluation.  =====  The paper is generally clear, with minor grammatical blemishes. The paper generalizes the relative goodness-of-fit tests for l = 2 models in [4, 17] to comparing l > 2 models by controlling FDR or FPR under the post-selection inference framework (along the lines of [20, 31]) and is in this sense somewhat incremental. On the other hand, the paper makes some reasonable (albeit not groundbreaking) theoretical contributions, and the experiments are fairly extensive.  Several comments/questions:  - Both of the proposed approaches rely on initially selecting a reference model. But what happens if among the list of candidate models, the chosen reference model (i.e., the model that minimizes the chosen discrepancy measure) is not in fact the model of best fit? This scenario could be quite plausible when the chosen discrepancy measure (e.g., inappropriate kernel bandwidth) or when the sample size is small. In this case, even if the latter testing procedures are consistent, one may never arrive at the correct conclusion. Consider, for instance, the toy example where three candidate models are normal distributions with the different means mu and standard deviation .1: A: mu = .9 B: mu = 1 C: mu = 1.2 and simulate n samples from model B. Then, it may be possible (due to noise in the data when n is small) that the reference model was mistakenly selected as model A rather than model B. In this case, it may be that the subsequent testing procedures would always reject model C since it is significantly different A, but model C might not have been rejected if model B had been selected as the reference model. In short, I feel that this approach of selecting an initial reference model and having all the subsequent testing procedure rely on the correctness of that initial choice seems a bit too risky. It seems that all the theoretical guarantees are also under the assumption that this initial choice is correct. Perhaps an alternative approach would be to retain a small collection of reference models (e.g., by performing individual goodness-of-fit tests for each and retain all models for which the null hypothesis was not rejected, while correcting for multiple testing)?  - It seems that the proposed procedures could apply to both MMD and KSD and the theoretical guarantees could be derived in a fairly similar manner. In this case, could the proposed procedures also be adapted to other kernel-based tests, such as the ME and SCF tests of [6] and the FSSD test of [19] (especially since the relative goodness-of-fit tests of [17] were based on the ME and FSSD tests)? If so, the authors should provide a unified statement of the theoretical results that summarizes the requirements on the test statistic in order for the procedures to hold. If not, it would be helpful to point out the challenges involved.  - In the legend of Figure 1, what does 'complete' and 'linear' indicate? 