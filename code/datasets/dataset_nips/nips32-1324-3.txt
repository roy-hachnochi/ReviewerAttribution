This paper proposes nonparameteric comparison tests for multiple (more than 2) models, and proves that these tests conrol either false positive rate or false discovery  rate. The paper appears to be sound theoretically and has experiments which agree with the theoretical claims. There are some typos throughout the paper which sometimes affect the clarity, but they are not significant. However, I do have two major overarching concerns with the paper which I will detail below.  Firstly, after reading this paper, I am not convinced that a statistical test for nonparametric relative multiple (more than 2) model comparison is something that I should be interested in as a machine learning researcher. Perhaps I should be interested in it, but I do not believe that the authors provide enough motivation to justify why we would need this type of test. If we have several models, isn't it good enough to perform a multiple goodness of fit test? It is even states that "the need for a reference model greatly complicates the formulation of the null hypothesis" - why bother with this new complication? I would be interested to hear the authors' opinion on this, and I would like to see this further motivated in the main text from the perspective of machine learning research. EDIT: Upon reviewing the authors' response, I am more convinced about the need for multiple model comparisons and the improvement that these tests provide over multiple GoF tests. I am confident that the authors will present this motivation in their paper, and thus I will raise my score.  This leads me into my second point, which is that I am not sure NeurIPS is the appropriate venue for this type of paper. The submission appears too condensed to fit the NeurIPS format: notably, the algorithm describing how you *actually perform the statistical test* is shoved into the appendix, and the paper is lacking a conclusion. Furthermore, this type of relative multiple model comparison seems to be quite niche, and would possibly be more appropriate for a statistics journal.  Another point that I was wondering - it is unclear what drives the improved results for the CelebA and Chicago datasets: is it really the multiple relative testing framework that has been introduced? Or is it the use of an improved metric (e.g. MMD vs. FID, or KSD vs. NLL)?   Other miscellaneous points: Line 118 - I would change "one sample from R" to "samples from R" Line 168 - Why does the independence allow us to remove the constraint on Az?  Line 271 - How is H_0: D(P_1, R) < D(P_2, R) true in the mean shift case? Wouldn't D(P_1, R) = D(P_2, R) here? Line 274 - What are the actual definition of P_1 and P_2 in this case? Supposedly they are MoGs that differ locally by rotation, but then what is the reference distribution R?  Line 284 - Isn't it a bad idea to use a Gaussian kernel with the KSD? In goodness-of-fit testing, it was shown to be sometimes be unable to differentiate between two distributions; see (Gorham and Mackey, 2017).  Small Typos Line 44 - Undefined acronyms (ME and SCF) Line 110 - Boldface x is introduced but doesn't appear anywhere else Line 126 - "Notation" -> "Notion" Line 264 - "RelPSI-MMD and RelPSI-MMD" doesn't make sense - I'm assuming one of them is supposed to be "RelPSI-KSD"?   To summarize, I will comment on originality, quality, clarity, and significance: - *Originality*: The paper appears to be quite original. This work is placed in context with related work and the authors' contribution is clear. - *Quality*: The paper proposes a method, proves it to be correct in some sense, and then produces experiments which agree with its claims. I would consider that to be a submission of good quality.  - *Clarity*: The paper is a bit dense and can be a bit hard to understand at times (likely because it was condensed to fit the NeurIPS format). However, the paper is very well organized (although lacking a conclusion) - *Significance*: As stated above, this is really where I think the submission is lacking. However, I am open to changing my mind if I can be convinced in the rebuttal.  Reference: Gorham, Jackson, and Lester Mackey. "Measuring sample quality with kernels." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.