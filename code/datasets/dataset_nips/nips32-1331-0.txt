The proposed architecture consists of an encoder RNN which maps input sequences to low-dimensional representations, a decoder network which maps this latent representation to weights of an RNN (similar to Hypernetwork). This RNN is used to predict labels in the input sequence. The whole network is trained end-to-end. This sort of an RNN-based auto-encoder + hypernetwork combination is novel and interesting.  Results on a synthetic time-series dataset and the BD dataset show that the proposed architecture is indeed able to disentangle and capture factors of the data generating distribution.  Experiments overall, however, are quite limited. It would be great to exhaustively compare to 1) an RNN-based autoencoder without the hypernetwork, 2) ablative experiments with and without the disentanglement and separation losses, 3) within the disentanglement loss, contribution of MMD vs. KL and analysis of the kinds of differences / behavior each induces. This would help tease apart the significance of the proposed architecture.  Also, disentanglement_lib (https://github.com/google-research/disentanglement_lib) is a recent large-scale benchmark and presents several standard metrics for evaluating disentangled representations. It would make for a significantly stronger contribution to evaluate on this and compare against prior work. As things currently stand, the proposed architecture is interesting but hasn't been evaluated against any prior work.