Strengths: * Simplicity. Both the bLVNet and the TAM are simple models, easy to implement and probably fairly straightforward to train. This is a good property. * Paper is well-written and the technical approach is easy to comprehend. * Although the TAM is demonstrated using a frame-based 2D CNN, it is straightforward to extend to 3D CNNs, with potential further gains in accuracy. * Comprehensive evaluation on 3 large-scale video datasets shows the memory/efficiency/accuracy gains enabled by the two proposed schemes (bLVNet and TAM).  Weaknesses: * Technical innovation is fairly limited. The bLVNet is a straightforward extension of bLNet (an image model) to video. The TAM involves the use of 1D temporal convolution and depthwise convolution. Both mechanisms that have been widely leveraged before. On the other hand, the paper does not make bold novelty claims and recognizes the contribution as being more empirical than technical. The TAM shares many similarities with Timeception [Hussein et al., CVPR 19], which was not yet published at the time of this submission and thus does not diminish the value of this work. Nevertheless, given the many analogies between these concurrent approaches, it'd be advisable to discuss their relations in future versions (or the camera-ready version) of the paper. * While the memory/efficiency gains are convincingly demonstrated, they are not substantial enough to be a game-changer in the practice of training video understanding models. Due to the overhead of setting up the proposed framework (even though quite simple), adoption by the community may be fairly limited.   Final rating:  - After having read the other reviews and the author responses, I decide to maintain my initial rating (6). The contribution of this work is mostly empirical. The stronger results compared to more complex models and the promise to release the code imply that this work deserves to be known, even if fairly incremental. 