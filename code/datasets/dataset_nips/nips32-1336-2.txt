1. Originality: WEAK a) I think this is the weakest part of this paper. Almost all of the contributions of this work have been individually explored. For example: b) The idea of parallel pathways for low-res/high-res processing was explored in SlowFast, 2-stream networks etc. Granted, authors use a somewhat different design from the ICLR'19 paper, where the spatial resolution is changed across the pathways, but the core idea is fairly well explored. c) The aggregation layer (TAM) is essentially TSM [4] with learned weights, and gets a few percentage points extra in performance. d) Missing related work: Aggregating context temporally for video representation learning has been explored in many previous works, which would be good to report in the related work. I point some here.    - Aggregating using VLAD/Fisher vectors etc: Action recognition with stacked fisher vectors (ECCV'14), Learnable pooling with Context Gating for video classification (CVPR'17), ActionVLAD (CVPR'17), SeqVLAD (TIP'18) etc   - Aggregation using attention: Attention Clusters (CVPR'18), Video Action Transformer Network (CVPR'19), Long-Term Feature Banks for Detailed Video Understanding (CVPR'19) etc   - Other temporal modeling architectures: Timeception for Complex Action Recognition (CVPR'19), Videos as space-time region graphs (ECCV'18) etc  2. Quality: GOOD I think authors do a good job of doing thorough experiments, and comparing performance of recent works along with computational/memory costs. The ablations are useful as well.  3. Clarity: WEAK Quite a few aspects of the model were not immediately clear to me. I would encourage authors to clarify in the rebuttal: a) Splitting video into odd/even frames, setting odd as big and even as little: This seems very adhoc. Why enforce this rule? Why not just use pairs of frames and use one at lower resolution and other at higher? Is there a reason odd frames in the video must be bigger? b) What is the train-time complexity of the model? Since the aggregation layer has to be trained, and needs at least "r" clips temporally-shifted clips at the same time, it would limit the training batch size (something like TSN would not have that issue). Is that a limiting factor at all? I would like to see more discussion on that aspect in the final version. c) L221: What is "single-crop single-frame" testing? I assume it is done in TSN style -- so for SS-V1 model which uses 32x2 frames, you have 32 segments at test time and use a pair of frames from each segment (odd and even). d) If my understanding in (c) is correct, then what is the "multi-crop" setup used in Kinetics? How many frames are being used in Table 3?  e) I am assuming the "Frames" column in the tables reports the *TOTAL* frames used in inference, including multiple crops etc. Is that correct?  4. Significance: MODERATE While the work doesn't significantly improve on the state of the art, it does seem to propose a cheaper alternative. That can be very useful for research groups with limited resources to work on related areas, if the code is made available. However it's not clear from the paper if the code for reproducing the reported results be released?   Final rating ======== I have looked through the other reviews and author feedback. I appreciate authors efforts in responding to my concerns, and clarifying parts of the paper. As all reviewers note, the technical novelty of the work is limited, though the good performance on standard benchmarks with lower computation might be valuable. Given the newer results in rebuttal and the promise to release code, I am upgrading my rating to 6. However, I still think the writing and presentation at least needs quite a bit more work to explain their approach and setup clearly.