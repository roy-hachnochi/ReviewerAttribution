The idea of combining RVM and SVM is interesting, especially in the context of active learning, since it encourages selecting points both near the decision boundary and exploring the input space well.  The paper is well written. The experiments seem to demonstrate advantages over baselines.   The usage of the word "generative" in this paper might be inappropriate. I think RVM is a probabilistic method, but not generative since it doesn't model P(x | y). See wikipedia for several common definitions of "generative" and "discriminative" models.   In Figure 4, why in some of the plots (e.g., for Yeast and Reuters datasets), the learning curves started from different accuracy? Not all active learning methods use the KMC learning algorithm? If it's not, then this raises the question of whether the advantage of the proposed method is due to the AL policy or the learning algorithm. If it is the same learning algorithm, then all learning curves should start from the same accuracy.   Are the experiments averaged over multiple repeats or just a single repeat? It's not mentioned the experiments are repeated, so I'm concerned about the statistical significance of the results.   How large is S in the experiments on the real datasets (Figure 4)?  Table captions should be on top of the tables.   [update]:  from the author's rebuttal, it seems the proposed KMC algorithm appear to outperform SVM or RVM. As different active learning policies are coupled with different learning algorithms, given the reported better performance of the proposed KMC on passive learning setting, it becomes unclear what contributed to the better performance of active learning: is it the policy or the learning algorithm? The new learning algorithm itself could still be a nice contribution. But as an active learning paper, I think the authors should more analysis to make this clear. 