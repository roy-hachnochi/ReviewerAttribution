This paper studies algorithms for average-cost MDPs with discrete states and actions, known transition dynamics, bounded adversarial rewards, and exponentially fast mixing. The problem setting and initial regret bound were described in the work of [15]. Their work and follow-up papers use an experts algorithm in each state, with losses corresponding to Q-functions. The algorithm proposed in this paper is different in that works in the dual: it optimizes the state-action occupancy measure subject to dynamics constraints, using FTRL. Compared to [15], the obtained regret bound has an improved dependence on mixing time, but it includes an extra ln|S| term.   For MDPs with a large state-space, the authors approximate the occupancy measure using a linear function. This requires several changes: relaxing constraints and adding them as a penalty, and generating stochastic  subgradient in order to reduce computational complexity to be polynomial in the number of features rather than |S| and |A|. The corresponding regret bound is with respect to a restricted class of policies, for which the occupancy measure can be expressed as a linear function of the same features.   Originality: The proposed algorithm and analysis follow multiple ideas from previous work. In the tabular case, the approach is quite similar to [13]. The authors say that there are some gaps in the proof of Lemma 1 in [13]. I looked at the paper [13] and also contacted the authors, and it seems like this problem is in fact just a typo. Thus I think a clearer statement of contribution in the context of the work of [13] is needed. The computationally efficient version with large MDPs and linear function approximation is more novel.   Quality: The paper seems technically correct; I did not verify all of the proofs.  Clarity:  The paper is well-written and well-organized. However, the authors should clarify the difference between this work and [13], in terms of both the algorithm and analysis. It would also be useful to clarify the  benefits, if any, of working in the dual (i.e. optimizing the state-occupancy measure).  In the tabular MDP,  it adds a dependence on ln|S|. In the large MDP, presumably one could similarly use a linear Q-function, and not have to deal with dynamics constraints.  As an aside, in the LQR problem with adversarial costs and known dynamics (https://arxiv.org/abs/1806.07104), there exist some advantages of working in the dual SDP: in the primal, feasible suboptimal solutions do not necessarily correspond to stabilizing controllers.  After reading the rebuttal: My overall opinion has not changed much. Yes, the error in [13] is about |S| vs |S||A|.  Given that the motivation for the paper is "a setting often encountered in practice, where the state space of the MDP is too large to allow for exact solutions", I agree with the other reviewers that it would be useful to at least provide an example of such a problem, as well as some numerical simulations. Some of the practical examples the authors mention in the rebuttal and list in the introduction *do not* correspond to the studied setting. For example, in queueing networks, the cost function is not unknown/adversarial, but in fact known, and a deterministic function of the state (it's the number of jobs in the system).   