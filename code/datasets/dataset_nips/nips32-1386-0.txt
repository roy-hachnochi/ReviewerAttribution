*** The author rebuttal has sufficiently clarified my questions about the model assumptions, and the statements of A3 and theorem 1. ***      Originality: The paper considers a model that combines censored feedback with contextual information and non-parametric residuals; the combination of all three has not been studied. They suggest a new algorithm and prove that it achieves logarithmic regret.         Quality: Overall the results in the paper seem technically correct, but I had some concerns about the model/assumptions/results:     - it is unclear to me how restrictive the parametric assumption that the expected log of the valuation is linear in the covariates. Although authors claim that predictor variables can be arbitrarily transformed, the task of designing the appropriate transformation, ie. "feature engineering", is a non-trivial task itself. Additionally it is unclear how realistic/flexible the logarithm/exponential relationship is, as it imposes that the valuations are exponentially far in relationship to the weighted covariate distances. If the purpose of the logarithmic form is to keep the valuation to be non-negative, there are other options for enforcing non-negativity as well.     - Is there a typo in Assumption A3? the first inequality implies that if I were to take z = z^*, then r(z^*, \theta_0) = r(z^*, \theta) even for any \theta \neq \theta_0? Similarly the second inequality implies that if I were to take \theta = \theta_0, then r(z^*, \theta_0) = r(z, \theta_0) for any z \neq z^*? Is this a typo, or is this assumption as stated required and potentially restrictive?    - In theorem 1, does the analysis assume a particular choice of the parameter \gamma, or does it hold for any nonnegative \gamma? As \gamma is used in the UCB/LCB, should we expect an expression similar to log(T)? I presume that if \gamma were chosen to be too small, then something in the regret bound would break so that you do not get logarithmic regret?        Clarity: The paper reads clearly.         Significance: The combination of censored feedback with contextual information and non-parametric residuals seems to be a practically relevant setup, although it is unclear how realistic/flexible the specific choice of the parametric model is. However the algorithm seems to be quite general and easily extendable to other parametric models as well.        Misc question:     - In your algorithm, you point out that each price selection checks one or more cells such that you get free exploration. In fact, you get additional information from knowing that if the user rejected the item at price p, then the user would have rejected the item at all prices higher than p. Therefore you get a datapoint even for cells that would have recommended higher prices than the chosen p_t. Symmetrically, if the user ended up buying the item at price p, then that means this user would have bought the item at all prices lower than p. Therefore we get a datapoint for cells that would have recommended lower prices than the chosen p_t. Do you imagine that the algorithm could be improved by further aggregating the data in this way? This may not be a good suggestion though as the inclusion of the datapoint to the estimate would be dependent on the the outcome of Y_t, which may cause biases.        