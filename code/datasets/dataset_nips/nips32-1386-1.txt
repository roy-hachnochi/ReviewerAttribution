I think the results presented represent solid progress on the problem and the results seem new, though there are a number of ways in which the results could be improved.  * A major drawback of the DEEP-C algorithm is that it has exponential running time. Given that the authors work in a well-specified linear model, this is a bit surprising. Is there reason to believe that computational intractable should be expected for this setting? This would be good to discuss in the paper. If this is indeed the case, it would be nice to prove a computational lower bound.  * For the regret bound in Theorem 1, the authors mention that regret scaling as $O(\sqrt{n})$ is necessary, so the bound is near-optimal in terms of dependence on $n$. However, the bound depends on a number of other parameters, and it would be good to discuss whether their dependence can be improved. Even better, it would be great to include a lower bound. EG, is regret scaling as $d^{11/4}$ to be expected?  * Given that the authors are proposing a new model, they should spend a bit more time justifying the model and related assumptions and showing why it is natural, otherwise the whole paper feels like it not much more than a technical exercise. The fact that the model generalizes the non-contextual setup of Kleinberg and Leighton (2003) is a plus.  Beyond this, the paper is fairly well-written and easy to follow.  Some additional notes: * You may want to consider citing related work on semiparametric contextual bandits (Greenewald et al. 2017, Krishamurthy et al. 2018). * There are multiple places in the paper where the authors use the phrase "we conjecture..." in passing. If these are serious conjectures you should spend a bit more time to elaborate on these points and perhaps include an open problem section, otherwise I would recommend removing.