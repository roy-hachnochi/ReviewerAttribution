Originality: I think the proposed initialization is very interesting and seems to work well for fully-connected network with Relu. Could it work well for Conv-nets and perform comparable to ResNet (on cifar10 and imageNet)? Also, several theorems in this paper seems to be known in previous works and it is better to phrase them as lemmas: Thm 2 is standard and has been widely used in recent mean field/overparameterization papers; Thm 3 seems to be similar to Cor 1 page 8 of [HR]; Equations 9 and 10 of Thm 4 seem to be known in [CS].  Quality: the biggest question I have is: can the framework of this paper (theorem  1 and 4) tell us new insight we cannot obtain from recent mean field papers and [HR], [H] and etc.? Also Section 3 and 4 seem to be loosely related to the theory part, i.e. Section 2. They are mostly about dynamical isometry. Could you explain a tighter connection?   Clarity: the paper is well-written.    Significance: the impact of the paper could be improved if the authors could: 1. using theorem 1 and 4 to obtain new insights about finite width networks; 2. illustrate the success of the initialization method on cifar10 and imagenet.    [HR] Boris Hanin and David Rolnick: How to Start Training: The Effect of Initialization and Architecture [H] Boris Hanin Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients? [CS]:Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, 2009.