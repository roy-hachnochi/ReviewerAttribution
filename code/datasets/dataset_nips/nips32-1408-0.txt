The Hyvarinen scoring function seems to be a special case of the Stein score function (see Eq. (1) in Liu, Lee & Michael Jordan. "A kernelized Stein discrepancy for goodness-of-fit tests." ICML 2016. and set f(x) to be the gradient). As is pointed out in the ICML 2016 paper, it would be better to allow f(x) to be selected from a set of smooth functions (e.g. functions from the reproducing kernel Hilbert Schdmit space) so that minimizing the resulting discrepancy measure can push the density function to match the empirical data distribution. Would the proposed extensions still hold for the Stein score function?  Could the author elaborate on the statement that "compared with Shannon differential entropy of a random variable that measures it descriptive complexity, the entropy and conditional entropy defined in Definition 1 measure the uncertainty in prediction complexity"? Wouldn't the Fano's inequality also characterize the prediction complexity?  Overall, this work can be improved by adding more discussions on existing works on the Stein score function listed below:  [1] Liu, Qiang, Jason Lee, and Michael Jordan. "A kernelized Stein discrepancy for goodness-of-fit tests." ICML 2016. [2] Chwialkowski, Kacper, Heiko Strathmann, and Arthur Gretton. "A kernel test of goodness of fit." ICML 2016. [3] Gorham, Jackson, and Lester Mackey. "Measuring sample quality with kernels." ICML 2017. [4] Liu, Qiang, and Dilin Wang. "Stein variational gradient descent: A general purpose bayesian inference algorithm." NIPS 2016.