This paper develops new information quantities. These quantities are derived from the Hyvarinen loss and are shown to be related to Fisher divergence. The behavior of some of these quantities is similar in part to classical information quantities such as entropy, mutual information, KL divergence etc. The former quantities are said to be faster to compute and more robust than the latter. In this paper, these quantities are derived, studied and used to derive several learning algorithms, most notably a version of the Chow-Liu algorithm based on these quantities. The resulting algorithm is used for structure learning, classification and community detection.  The contribution is imho rather original. Learning graphical models over continuous variables receive some attention, works often limit themselves to Gaussian variables.   I think the paper is well written, though it should be proofread to remove all remaining typos. It is also well organized and the contributions are clear. I think the authors did a great job moving the proofs and details to the appendix.   I think the paper is moderately significant. Learning graphical models over continuous variables has always been challenging. The proposed information quantities have the potential to provide a well-founded and computationally interesting solution to this problem, which sounds very exciting to me. The experiment also clearly illustrates the computational advantages of the proposed quantities.  However and as far as I understand, the paper only discusses pairs of variables and the authors postpone extension to more variables for later works, so I was a bit disappointed. Computational challenges typically arise when going beyond pairwise relationships.   The experiment cover various topics but cover none in depth, which I think is the right call for this type of contributions. That being said, I think the paper would benefit from an empirical comparison between algorithms based on the proposed quantities and algorithms based on classical information quantities. At the moment the paper only compares run-time. Ideally, before using an alternative, faster algorithm, I'd like to have an idea how much the answer is going to differ.  On a related note, it would be interesting for these experiments to illustrate the stability and robustness of the proposed quantities.  In the synthetic data experiment with Gaussian vectors, what is the value of p and what is the family of density distributions used in the CL tree?  Definition 2: Let e be an standard Gaussian random variable independent of Y --> Shouldn't e be capitalized?  in equation (2), there is no "p" in the right hand side. Shouldn't it be p(y) in the second term?  typos: l196 an standard l200 of the its counterpart   ----------- I would like to thank the authors for their feedback. I think going beyond two variables would increase the interest of the work very much and I encourage them to include their additional results in the paper. I will keep my score unchanged.