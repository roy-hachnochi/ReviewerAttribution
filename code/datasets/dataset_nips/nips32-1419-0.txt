- Originality: The proposed method is simple and maybe novel. I haven't seen papers that propose to do a simple uniform quantization in the preactivations just to save memory during training. But I could be over looking some of the network quantization literature.  - Quality: The experiment is very thorough and solid. It shows that the proposed method is able to save memory while maintaining the same accuracy on a selection of networks on CIFAR and ImageNet.  - Clarity: The paper is clearly written. I was able to understand the core contribution and Figure 2 is very nicely designed.  I think it would make it clearer, if the text can explain Eq. 9 better. In my understanding, I think this step is to uniformly quantize the activations within 3 standard deviations, but this requires some extra thinking and is not immediately clear.  - Significance: My major concern with the paper is regarding to the significance of this work. It is acknowledged that the proposed method can be readily implemented in lots of network architecture, thus it has good significance in terms of applications. However, the literature covered in this paper mainly focuses on memory saving, with a little on quantization. In [1] (full reference at the bottom), which is a paper published at NeurIPS last year, the authors show that it is possible to do 8-bit training and quantization together. The difference of outcome, in my opinion, is that while that work does quantization on the forward pass, the weights, and the backward pass of activations, this paper does quantization only on the backward pass of activations. Although memory saving was not a major selling point in [1], it does seem like a by-product. If I am correct, what makes this paper a separate contribution rather than a simplified version of [1]? I could be wrong so I would like to see more how this proposed method is compared to [1]. An experiment on the proposed method vs. the range BN and angle quantization scheme proposed in [1] in a more equal & controlled setting.  Overall, I think this is a solid paper, but we need to see more comparison and discussion on the network quantization literature to evaluate how significant the contributions are. Therefore my overall score is 5 (marginally below).  Reference: [1] Ron Banner, Itay Hubara, Elad Hoffer, Daniel Soudry. Scalable Methods for 8-bit Training of Neural Networks. NeurIPS 2018.  --- Update reading the rebuttal and discussion with other reviewers, I decided to change the score to 6 to reflect the merits of the paper in terms of its simplicity and a strong experimental section.