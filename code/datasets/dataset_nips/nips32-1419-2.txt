Update after authors' feedback:  Thank you for taking the time to answer our reviews and comments.  I feel that there will be interesting bits in the updated version. Nonetheless I still think the experimental part, as well as the limitation to one type of architecture is to light for this paper to be considered as a top 50% one. I'll keep my score of 7 as it looks like an easy and efficient method to save memory in many CNN architectures.  -------------------------------------------------------------------------------------------  This paper is extremely easy to read and to follow. The presented method is at the same time very simple and clever, and seems to work pretty well. It is nice to see that kind of paper, that are not overly complicated but present interesting contribution to an actual and relevant problem. The figures are a nice addition to the paper, they're clear and self-explaining (nb. I did not see Fig. 1 referenced anywhere in the text though).   However, it is a shame that this method is only limited to architectures made only of convolutions, batch normalization and ReLU layers. Even though it covers a large family of models, and the authors point it out in the paper, it would have been interesting to see different architectures or applications. The experimental section is clear and interesting, the experiments seem well-designed and the results are good. An empirical comparison to other methods for saving memory, or to state-of-the-art results would have been quite valuable. That section is a bit disappointing and merely present the parameters of the models and describes the result figures and tables.  Nonetheless I believe this is a solid paper and a good NeuriPS contribution.    Minor points:   - Beginning of Sec. 3.1: "A neural network is composition of linear" -> is a composition   - end of p6 "back-ward pass", "backward-pass" -> backward pass   - end of 3.4: "K = 8,4 bits" : might be clearer as "K = 4 or 8 bits"