In this paper, the authors introduce a method to train deep image estimators in an unsupervised fashion. The method is based on the same ideas as the recently introduced noise2noise method (Lehtinen et al, 2018) but is generalized to deal with a degradation model that produces linear noisy observations from the latent clean image that one wants to recover.  The training avoids using ground-truth images by using two observations of the same latent image under different linear operators (i.e., compressive measurements or motion blur) and different noise realizations. The paper first introduces a non-blind training scheme where the linear operators are known. Then, for blind training the linear operators are estimated, using a second network (subnetwork), and used as a proxy to emulate the non-blind scheme. The whole system is trained by carefully avoiding updating all the parameters together (to avoid a vicious loop). Experimental evidence on two (synthetic) problems shows that the unsupervised (non-blind and blind) training scheme produces similar results as the fully supervised training. Training neural networks to reconstruct images without having access to ground truth data is a major challenge. Recent work (Soltanayev and Chun,2018; Lehtinen et al, 2018)  have shown that this is possible in the particular setting of image denoising. This work generalizes the ideas behind Lehtinen et al, 2018 to cope with a linear model (where denoising can be seen as a particular case). This is an interesting paper that proposes several ideas to avoid the restrictions imposed in the Lehtinen et al. procedure. The paper is generally well-written, but there are a few sections that could be improved with more discussion. In fact, the major weakness of the paper is the lack of analysis.   -- Swap Loss (Eq (3) and Eq (4)).  The performance of the method strongly depends on the matrix Q, but there's no analysis on this. In particular, I can imagine that in the case where the linear operator is formed using random orthogonal matrices, Q will be close to the identity. But, what about other cases? For example, in the case of motion blur, the Q matrix will be related to the power spectrum of the motion kernel process (Q is the autocorrelation). Since it is essentially a random walk, I can imagine that it will have a linear decay with the frequency. This implies that the swap loss is not penalizing high-frequency errors so, the training scheme won't help to recover high-frequency details.   The paper does not discuss any of this but claims that if the measurements are different/complementary enough then this matrix will be full rank. More analysis is needed regarding Q. For example, authors could plot the spectrum of this matrix in the given experiments. Also, this matrix Q is probably one major limitation of the method. This is not discussed in the paper.  -- Self Loss (Eq (5)).  To complement the swap loss, the authors introduce a "self loss" that enforces self-consistency. This is not analyzed and it is not clearly motivated. The justification is just that it produces better results. For instance, in the particular scenario where the linear operator is the identity (i.e., denoising), this loss will enforce that the network f is close to the identity so learns to do nothing (keep the noise!). I understand that this is a particular case of the more general model, but this particular case is covered within the proposed framework. I would like to have more information regarding this loss, and in general what is this loss doing. For instance, the paper could provide results when this self-loss is not used.  -- Prox:\theta Loss (Eq (7)) The authors propose to cope with the blind case by learning the linear operator (degradation) using a convnet  (a separate branch of the network). I find it quite surprising how well this works (according to the experiments). I would like to see more analysis regarding the estimation of the blur (or more generally, the linear operator), in particular, a comparison to other works doing this (e.g., motion blur estimation using deep learning). This is not mentioned much, but this is a very difficult problem in itself.  -- Experiments Deblurring Face images. Why is the method trained for deblurring face images? Would it work if trained in a more complex distribution, e.g., natural images? Noise. The performance gap between supervised and unsupervised training seems to increase with the level of noise. Could you elaborate on this? Also, in the experiment regarding deblurring face images, the level of noise is 2/255, which is pretty low. Have you tried with higher noise levels?  Loss. When deblurring face images, all adopted losses are L1. In this case, the mathematical analysis in Eq (4) doesn't hold. Could you comment on this? Also, why did you choose L1 for this case? Are results with L2 norm much worse? Compressive measurements. The shifted partitions are generated in a very particular way. (Figure 8 in supplementary material). Is this really important? How sensitive are the results to this pattern? This is related to the matrix Q. Other comments: It could be interesting to compare the proposed loss with the losses used in multi-image variational deblurring, for example, see: Zhang, H., Wipf, D. and Zhang, Y., 2013. Multi-image blind deblurring using a coupled adaptive sparse prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1051-1058)  -------- After rebuttal. I believe that the manuscript will improve with the changes that the authors have committed to do. Additionally, I would like the authors to clearly list the limitations of the current approach and briefly discuss them (see e.g., in rebuttal document, l42 to l44, estimation of \ theta, motion kernel estimation; ). This will allow defining clearer future research directions for those who are willing to pursue this line of work.   Since the authors have carefully addressed most of the questions and comments I am therefore updating my score to 7. I would like to see this paper presented at this venue!