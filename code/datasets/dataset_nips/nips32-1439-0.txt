In general, the total least-squares (TLS) problem, also its regularized variant, is important for machine learning. For large-scale problems using methods that can exploit the sparsity of the inputs is crucial, thus the solution suggested by the authors is relevant. The ideas look original and the paper is fairly clearly written, though there are some presentational issues: for example, I do not see the point of having an "informal" version of the main theorem, as Theorem 1 is not much less complicated than Theorem 3.10. The informal discussion of the result should not be called a "theorem". The proposed solution is interesting and the theoretical and empirical results are promising; however, it is not elegant that the approximation is only provided with a fixed 0.9 probability, instead of a user-chosen arbitrary confidence probability. This is probably the most restrictive property of the solution. The 0.9 should ideally be generalized to an arbitrary confidence probability. If this could be done, then it would highly increase the significance of the proposed method.  Minor comments: - The "poly(n/epsilon) d" part is outside the \tilde{O} notation in the abstract, but it is inside in Theorem 3.10. - If the "informal" version of the theorem is kept, it should not have "with high probability", but "with probability 0.9" - Was there really 4TB RAM in the server used for testing, or is it a typo (TB vs GB)?  Post rebuttal comments: thank you for your reply, I am satisfied with your answers and hence I have raised my score accordingly. Please, do include the extension to arbitrary confidence probabilities in the next version of your paper (as discussed in your rebuttal).