Contributions:  The paper uses a combination of GNN and RL techniques to learn to explore graph-structured environments. The method is used in a synthetic maze exploration experiment, and for program testing to improve test coverage.  Key contributions are: Framing of the graph exploration problem using a model that uses (1) a graph memory representing the visited state, (2) incorporating the exploration history in the prediction of the next action, and (3) different weights per time step. Framing program testing (including app testing) as a graph-exploration problem. Ablation studies exploring the overall importance of weight sharing (per RL time step), graph structure, exploration history, and different methods of encoding the program in the program testing.  Comments:  Originality: There are prior works where GNN + RL are combined that has a similar flavor (e.g. where the states are represented as a GNN, or where actions are represented by nodes in a GNN). This work combines those ideas with the goal of learning to explore. In particular, I don’t recall seeing the idea of using an evolving graph memory.  https://openreview.net/pdf?id=HJgd1nAqFX https://arxiv.org/pdf/1906.05370.pdf https://graphreason.github.io/papers/7.pdf  Clarity: The paper is well-written and easy to follow. I’m curious about:  - In Table 2 and Figure 2, where the “randomness” in the “random next-step selection” in RandDFS comes from. In particular, how much worse is RandDFS compared to actual DFS? - In Figure 2, it would be useful & interesting to see the actual trajectory taken by the various agents, or at least the initial position of the agents. - In Figure 4, I’m not sure what “# inputs generated” means. (At first I thought that means T = 5 in these experiments, but that doesn’t really make sense?) - Also in figure 4, what does the y-axis label “joint coverage” mean?  Quality: The submission appears technically sound, with experiments that are relatively different from each other.  The maze exploration task with the ablation study shows the performance gain in using full graph structure and using the overall history. Reporting the standard deviation over the 100 held-out mazes would be helpful in analyzing the proposed approach. That using the overall history performs better is a bit unintuitive to me.  The program testing tasks also have ablation studies that show how the encoding of the program being tested affect coverage. However, it is unclear whether using the overall history would be beneficial at all. The author also does not compare against any fuzzing or neural fuzzing approaches.   The app-testing task shares similarities with the maze task, but in a more realistic scenario. It would be helpful to provide an intuition about what a real app traversal graph looks like (not the generated ER graph) for comparison against the maze task. 