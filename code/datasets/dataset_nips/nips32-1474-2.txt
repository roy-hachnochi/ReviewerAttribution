The paper proposes a method to learn a differential functional graph for two multi-output functions X and Y. An edge in the graph indicates the difference of the conditional dependence of X and of Y for the pair of the nodes (i.e., variables) the edge connects. In order to do so, the paper introduces finite approximation of the random functions, where the bases are first estimated from eigen-decomposition over a kernel matrix on finite observations, and the estimates of the coefficients are obtained through integration. The coefficient estimates for each function are then used to construct a sample covariance matrix, based on which an objective function is proposed to estimate the prevision of the differential graph. The sparsity is induced from the group-lasso penalty. Several theoretical properties are given. The evaluation is conducted on simulation and neural science applications.  Overall, the problem of estimating a differential functional graph sounds interesting and meaningful. However, the proposed approach seems a straightforward extension of the work by Qiao et. al. 2019 [12]. The paper lacks justification of the proposed objective. The notations and description of the method are messy. These issues hinder me from accepting this paper. Listed are detailed comments.   (1) Limited novelty. The core techniques used in the work --- functional principled component analysis to obtain a finite random function approximation, using coefficients of the bases functions to build sample covariance matrices and learn the structure via a graphical lasso based optimization --- are exactly the same as the Qiao et. al. 2019 [12]. The only difference is the objective. To me, at least from the algorithmic point of view, the approach is too incremental. (2) A key component of this work is the proposed objective in eq. (2.7). However, the justification/intuition of the objective is unclear. Obviously, you cannot use graphical lasso objective. But how to justify the proposed objective? What does the first term of L(\Delta) imply? Although the authors try to give some intuition, the explanation is very vague. Why is the expectation of gradient of L zero? Why is \Delta^M the “unique” minimizer? The authors should explain them clearly, because this is the major contribution. (3) The presentation of the proposed method is really messy. The authors never differentiate scalars, vectors and matrices. The authors often confound the notations for the function X and the observation for X, say, X_i. For example, (2.1) defines the covariance between j-th function of X at s and l-th function of X at t. Obviously, the subscript “i" should be dropped. Such kinds of mixed usage of symbols are everywhere. It is really confusing to read. I have to double check [12] from time to time for confirmation.  