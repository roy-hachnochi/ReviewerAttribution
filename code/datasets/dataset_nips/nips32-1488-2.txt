The paper presents a framework for learning by demonstration using third person videos. The method is based on decoupling the intended task from the controller, by learning a hierarchical setup where the high-level module generates goals conditioned on the third-person video demonstration for the low-level controller. Due to its modularity, the proposed approach is more sample efficient than other end-to-end approaches, and the learned low-level controller is more general.  The paper is well written and well structured, it includes insightful figures and diagrams, and fair ablations and comparisons.   Originality: the paper presents an interesting approach to use third-person views as demonstrations for a robot; learning from demonstrations, including from videos, is not a novel contribution, as well as learning modular controllers in the form of an inverse model. Despite limited novelty, the approach presented in this paper is neat and clear. It would be interesting to see comparisons for example with methods that explicitly find correspondences between the demonstrator and the robot, and with methods based on trajectory-based demonstrations.  Quality: the submission is technically sound, and the approach explained in a clear way; the results (including those shown in the video) suggest that there is still room for improvement in terms of succeeding in completing the different tasks (e.g. the pouring policy execution seems wobbly and lucking robustness). Some discussion about the limitations of the proposed method would help evaluating the overall results.  Clarity: the paper is overall clearly written and well organized; a discussion around the limitations of the proposed approach could be added.  Significance: the paper provides an interesting way to address learning by third-person view demonstrations in robotics; this is a challenging and important field and this contribution is interesting with respect to more classical approaches based on hand-crafted models or task-specific solutions.