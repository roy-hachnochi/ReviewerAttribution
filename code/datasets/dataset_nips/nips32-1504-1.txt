The paper applies CVaR-based policy gradient method on robust option learning. The authors interpret the CVaR constrained objective as an unconstrained objective of a new MDP. It is interesting to see the connection and how that can simplify the understanding of the new policy gradient method. Experiments are done to understand the three questions mentioned in Section 4. However, correct me if I misunderstood, the difference between OC3 and Algorithm 1 in Chow and Ghavamzadeh[6] is just that OC3 are updating a set of \theta - {\theta_{\pi_omega}, \theta_{\beta_\omega}}_{\omega \in \Omega}, \theta_{\pi_\Omega} - yet Chow and Ghavamzadeh is updating one \theta. Other than that, the two algorithms are almost the same. Meanwhile, the MDP defined in section 3.2 is also quite similar to the augmented MDP mentioned in Section 5.1 of Chow and Ghavamzadeh [6]. Although applying existing robust MDP methods to robust option learning is important, it is unclear to me what the originality is of the proposed algorithm.   Re: Author Response Thanks for clarifying my concerns. I have adapted my score accordingly.  Typo: - L149: the expected loss R'' can be written as...