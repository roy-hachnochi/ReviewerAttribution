For neural network compression, it is common to train sparse models with L0 projections, as well as quantized models.  It is significant to analyze and propose new algorithms for such training.  The algorithms are limited in practice because the learning rate is constant.  The practical fix of increasing the batch size throughout training is significantly less convenient than decreasing the learning rate.  For the convergence analysis, the main trick is to exploit the optimality conditions from the proximal update, and I found the proof sketch in Section 2.1 instructive.  *Update*  I have read the author response and other reviews, and I am keeping my review the same.  I think my point about the practicality is still true, but this is only a minor comment.  The authors plan to define the practical algorithm in a way that is more clear.