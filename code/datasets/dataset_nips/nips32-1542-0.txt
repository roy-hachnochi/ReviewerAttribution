This paper explores an important direction in NAS. Different from previous work that builds upon well-designed building blocks, this paper tries to learn connections between channels. Therefore, such methods can potentially find better basic building blocks that are not discovered by human. I really appreciate efforts in this direction.   However, regarding the search space proposed in this paper, I suspect such irregular connections between channels are very unfriendly to hardware. Even the FLOPs of the searched model is close to human-designed models, the real hardware efficiency (e.g., latency) may be much worse. Can the authors also include the latency in Table 3 for reference? If it is the case, do authors have any solution to the challenge?  The experiment results are promising in very low FLOPs regime. However, results in high FLOPs (200M+, 300M+, 600M, etc.) regime are not provided due to the efficiency issue of the training algorithm.   I am very curious about the learned connection patterns. Can the authors provide some visualizations of their learned architectures? Additionally, I find that the source code is not provided. Do the authors have any plan about releasing the code?  [Post-Author-Response Update] I am satisfied with the author response. Therefore, I increase the overall score from 6 to 7.