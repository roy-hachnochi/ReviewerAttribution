Originality: The VCR task is a novel task (proposed by Zellers et al, CVPR19). The proposed HGL framework for this interesting task is novel and interesting. The paper applies the HGL framework on top of the baseline model (R2C from Zellers et al., CVPR19) and shows significant improvements. The paper compares other existing graph learning approaches. The main difference between the proposed approach and other graph learning approaches is the heterogeneous nature (across domains – vision and language) of the graph learning framework.  Quality: The paper does a good job of evaluating the propsed approach and its ablations. The visualizations reported in Fig. 4 are also useful. However, I have some concerns -- 1. Fig. 4 a,b,e,f – the intersity of the visualizations is counter-intuitive for some cases, such as “feeling” and “anger” for Fig. 4(a), “anger” and “angry” for Fig. 4(e), “person5” and “witness” for Fig. 4(b), “person1” and angry for Fig. 4(e). Could authors please comment on this? 2. In Fig. 5(a), the arrow is pointing at sky, instead of pointing at water in the water body. It’s not clear how CVM is helpful here. Could authors please comment on this? 3. It is becoming concerning in vision and language literature that models rely on priors (although language) more than grounding their evidence on the visual context. Could authors please comment on whether the use of Contextual Voting Module to incorporate priors could lead to models relying on context more than the true visual evidence? It could be useful to analyze the instances where CVM helps and see if any such undesired prior learning is happening. 4. Why is Y_o concatenated with X_m in Eq. 5 when Y_o is getting combined with X_middle anyways in Eq. 6? How much does the concatenation in Eq. 5 affect the final results?  Clarity: The paper is clear for the most part. I have a few clarification questions / comments -- 1. The concept of information isolated island on L41-42 is not explained. 2. There are some typos in L140-153, such as “shown in Eq. 5” à “shown in Eq. 4”, “f_v” à “f”, inconsistent notations for psi in Eq. 6 and L152. 3. Not all the notations in Eq.8 and Eq.9 are explained. Also should it be W_{oa} and W_{qa}, instead of W_oa and W_qa? 4. What is y_{i}^{rl} in Eq. 12. Specifically, it’s not clear what “r” refers to and how is that related with y_{i}^{l}. 5. It would be good for this paper to describe briefly the three tasks: Q à A, QA àR, Q à AR. 6. L247 – how is the information about tags associated with the bounding boxes in the image, such as “person1” tag associated with red box, “person5” tag associated with brown box fed to the model?  Significance: The task of VCR is an interesting, relatively new and useful task. The paper proposed an interesting, interpretable, novel approach for VCR that also significantly advances the state-of-the-art. I think the proposed work could be of high significance for pushing the research in VCR.  --- Post-rebuttal comments ---  The authors have responded to all my concerns. Most of the responses are satisfactory, except the following two --  The concern about visualizations in Fig 4 was that the intensities are too low for the instances pointed out in the review. The two words in the each of the instance should be aligned with each other (as also pointed out by the authors in the rebuttal), however the intensities are otherwise. It would be good for the authors to look more into this and clarify this in the camera-ready version.  Regarding the CVM – I see the usefulness of it in situations where context is needed. But I am worried about situations when context is not needed but CVM can potentially hurt because it is easier for the model to predict the answer by the prior knowledge based on the context. For instance, predicting “cooking” for “what is the person doing” because the task is being performed by a woman and CVM is using the context of woman.  Given the novelty and significance of the proposed approach, I would recommend acceptance of this paper.  