*Originality*  The approach is novel and an interesting way to combine structured information in the sentence and the image. It could be of use for other vision and language tasks, such as visual reasoning.  *Quality*  The description of the model looks sound. However, it’s unclear whether the graphs are latent or provided during training. Clarifying the inputs and outputs of the model during training and during inference would be useful.  A discussion of this approach’s possible applications to other vision and language tasks would be appreciated.  A more comprehensive qualitative analysis, e.g., more (larger) examples would improve the paper, especially examples and analyses that demonstrate the interpretability aspect of this approach.  *Clarity*  The abstract and introduction assume understanding of highly specific terms, e.g., heterogenous vs. homogenous graphs, intra- vs. inter-graph, and several acronyms that might not be familiar to those not working on VCR. However, most of it was made clearer in the approach section. But to get readers interested, this should be edited.  There seems to be an issue with the citations (some peoples’ first names are being cited instead of their last, like Jacob et al. in L24).  *Significance*   This approach could be of use to researchers working on other vision and language tasks. It achieves SOTA on all three tasks of VCR, and provides interpretable results. 