This paper is well written, and I especially liked the introduction section and conclusions. But, not convinced by the proposed approach and its experimental study.  It seems to me that the proposed approach makes a neural network model even more complicated. Although, one may define variables for particular purpose in an introduced model, it doesn't ensure that is what the variables accomplish when employed in complicated settings.   The prcedure of extracting features, and detecting a component doesn't seem simple to me, and more like a black box. What is the motivation for using Siamese network for detecting a component? The paragraph, "Extracting the decomposition plan", could benefit from an expansion, possibly supplemented by a figure explaining the architecture that is explained in the paragraph.   As for the types of reasonings are concerned, it doesn't have to be of the three kinds mentioned in the paper. Models like decision trees have more sophisticated reasonsings, yet interpretable. (A relevant paper: Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation, ICML 2017).  Most of the experiments section is devoted to MNIST dataset. This is a dataset for which components detection is relatively simpler, corresponding to even a single pixel. Even simple supervised models, and unsupervised models, work perform fine on these datasets. As for interpretability is considered, I don't see how the proposed approach is relevant for datsets like CIFAR-10, GTSRB. I don't think Fig. 4 is demonstrating the value of the model, in the sense of performing reasoning over components.  Why not have large number of components? The number of components is chosen to be close to the number of classes? This doesn't seem intuitive. What is the explanation behind it? In fact, there is a hierarchy of components required for many computer vision tasks, as accomplished by CNNs.  The proposed models seems a like a wrapper around existing neural approaches, for the namesake of interpretability.  Should there not be a comparison w.r.t. other models claimed to be interpretable? There are many techniques just for interpreting a so called black box neural networks, for instance attention based techniques.  Section 4.1.2 is too lengthy and complex to understand; the model should be such that it leads to interpretability; as per the section, there is a reliance of older techniques to understand the functioning of the model.  It would have been more interesting if the proposed model is learned on a relatively less studied datasets, and identifies components interpretable to experts in that domain. The datasets considered in the paper, especially MNIST, have been studied in thousands of papers, with to many architectures explored for the datasets. So it becomes difficult to judge interpretability capabilities of a model, based upon evaluation of such a dataset, as it is highly subjective. Of course, it is good to use the dataset for a preliminary validation.  ------------------------------------------------- I have taken the rebuttal into account as well as other reviews. I am fine with acceptance of the paper, assuming the authors would revised the draft as promised in the rebuttal draft. 