Summary: This paper studies the convergence of incremental and stochastic Expectation-Maximization (EM) algorithms. Authors establish non-asymptotic bound for the averaged EM iterates by building on the framework developed by Mairal 2015. Authors use the term 'global convergence' yet the rates are given for a first-order critical point and assuming that there is a unique minimum.  ** Major comments:  - Under H4, the problem already has a unique global minimum with positive definite Hessian.  Any first-order critical point will be a global minimum. The result is only global because of this assumption.  - Authors use the MISO framework; yet no description of MISO is provided in the paper.  - In the theorems, it is not clear what the expectations are over. For example in E[\bar{L}(\hat\theta_0)], which random variable is this expectation over?  - The bound is given for E[\nabla L (\theta^K)] where K is a uniform random variable  between 0 and the last iteration K_max.  This means that the bound is simply on 1/K_max \sum_k E[\nabla L (\theta^k)].   Using their theorems authors can give an upper bound on the averaged iterates  or by using the inequality  1/K_max \sum_k E[\nabla L (\theta^k)] \geq min_k E[\nabla L (\theta^k)] the best iterate. The current way of writing the bound seems a bit opaque. At least a discussion or a brief remark on the implications of the result is needed.  - Theorem 1 of Paper 1610 is a special case of Theorem 1 in Paper 1613.  ** Minor comments: - line 1, algorithm -> algorithms - line 20, comma after g - line 65, to find to - line 156, tailored   %%%%%%%%%%%% I thank the authors for answering my questions. Authors clarified my concern on assumption H4. But my concern on Theorem 1 of submission 1613 still remains. Based on this I am updating my score to 5.