General comment: It is a nice addition to the previous paper [14] in that it actually counts the number of activation regions other than just the n-k volumes of the discontinuity points of the gradient (which these results use in the proof).  Compared to previous works of counting linear regions, this paper looks at the number when randomly drawing the weights and biases (not necessarily independent). It is well written  However it could benefit from some more intuitive explanations at times. For example, I have some comments that would be nice to see addressed in a revised version of the paper:  - It is not very clearly stated in the paper (terms like "strongly suggests" are used without being alluded to after the rigorous theorem statement) how much training can you actually do so that the bound still holds and under which circumstances would it break down? By definition of a density C_bias seems to be <= 1, so that it wouldn't matter in the grand scheme of things but potentially get a tighter bound? Perhaps the authors are already working on this, but I'm curious: What are the challenges to actually show how the number of regions evolves over time with gradient descent? - Related to this point: In the experiments, the actual number of regions is higher than predicted but you don't offer an explanation. d/n < 1, thus C_grad is probably < 1 given the result in [12], and as C_bias should be <= 1, what does make the number grow so much during training? I find it nice however that at initialization, the number of regions pretty much matches the theoretical bound (could draw a horizontal line for theoretical prediction in the plot perhaps). - I am not sure why Lemma 6 i.e. invariance of the count to uniform bias scaling would actually imply invariance to non-uniform bias scaling which is in turn equivalent to uniform scaling of weights? You do mention that you don't prove it, but why should that be true intuitively? In general, scaling of the randomized weights seems to have rather big impacts of generalization properties of the final neural networks which is what makes me feel uncertain about this claim. - I think it is important to reveal the dependence on d/n through C_grad, C_bias - i.e. that they may grow with d/n / correlation but are "constant" in terms of number of neurons. You later state in 3.3 that "depth does not increase the local density of activation regions" - this sentence is misleading as this is perhaps approximately true for wide networks but strictly speaking not in general  Minor comments for improvement: - It would be useful to explicitly state the results of previous papers on the max number of linear regions so that the reader can see the difference directly. - A figure illustrating bent hyperplanes would be very useful - Maybe it would be nice to define "fan-in" somewhere? - in 3.2. you write I and sometimes [a,b]