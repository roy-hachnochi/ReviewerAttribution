After reading the authors response, I still think the authors could have done a better job in providing experiments that support their main claims in the paper, and there is much room for more analysis of the proposed approach.   However, in the authors' response, the authors provide new results that address some of the above comments. Moreover, the response addressed my concerns and made the contribution of this paper clearer. Hence, I updated my score.   ====================================================== This paper demonstrates how one can fool gradient-based neural network method for model interpretability by fine-tuning the model. The authors provide two sets of attacks: (i) Passive fooling; and (ii) Active fooling while demonstrating the efficiency of their attack on three neural network models (VGG19, ResNet50, and DenseNet121).  The idea of evaluating the robustness of interpretation methods for deep learning is an interesting line of research, however, I have a few concerns regarding this paper.   First, the concept of making the model hide its "original" behavior was defined in previous studies, however, not under these specific settings. It is known as backdooring. It would be highly appreciated if the authors would pay attention to it. For instance:   [1] Gu, Tianyu, Brendan Dolan-Gavitt, and Siddharth Garg. "Badnets: Identifying vulnerabilities in the machine learning model supply chain." arXiv preprint arXiv:1708.06733 (2017).  [2] Adi, Yossi, et al. "Turning your weakness into a strength: Watermarking deep neural networks by backdooring." 27th {USENIX} Security Symposium ({USENIX} Security 18). (2018).   Second, since we know that neural networks can contain backdoors, the motivation is a little bit fuzzy. The authors wrote: "...The regulators would mainly check two core criteria before deploying such a system; the predictive accuracy and fairness ... The interpretation method would obviously become an important tool for checking this second criterion. However, suppose a lazy developer finds out that his model contains some bias, and, rather than actually fixing the model to remove the bias, he decides to manipulate the model such that the interpretation can be fooled and hide the bias". In that case, how would the model interpretations look like? would it be suspicious? would it make sense? if so, maybe the lazy developer fixed it? What is the motivation for using Passive attacks?   Generally speaking, it would make the paper much stronger if the authors would provide experiments to support their motivation.  Third, did the authors try to investigate the robustness of such attacks? i.e. to explore how easy is it to remove the attack? for example, if one attacked fine-tune the model using the original objective with the original training set, would the attack still work?  Lastly, there are some spelling mistakes, to name a few:  - "Following summarizes the main contribution of this paper:" - "However, it clear that a model cannot..."