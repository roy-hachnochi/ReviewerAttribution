This work provides a unified framework called lazy training to explain some recent success in deep learning theory. In general, it shows that by proper scaling, many real world machine learning applications including two-layer neural networks enjoys properties of lazy training which makes them easier for training. Experiments backup their theory.   Overall, this is a good submission and I recommend an accept for this paper. My comments are listed as follows.   - It seems that this paper considers both empirical loss and population loss in a general loss function. I suggest the authors to highlight this in their problem setting.  #################################### I have read all the reviews and the authors' response. 