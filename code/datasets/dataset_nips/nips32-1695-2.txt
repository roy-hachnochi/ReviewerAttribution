The paper raises a copula-like variational distribution with rotation. The approach seems to work theoretically but the author should offer more detailed empirical evidence. Here are several major comments: 1. Since the proposed copula-like construction is a composition of multiple components, an ablation analysis is helpful in identifying the source of representative power. I am particularly interested in the performance of copula-like densities without rotation, copula-like densities with (any other) normalizing flow and mean-field Gaussian with rotation under the experimental setting in the paper. 2. What is the role of the transformation H defined in (7)? Since $\delta$ is fixed, H doesnâ€™t improve the representative power of the variational family. 3. The comparison in Table 4 is somehow unfair. The author should provide the prediction errors for a 400*400 networks with a Gaussian prior using copula-like variational approximation. 4. In related work the author mentioned that their technique can be applied in high dimensions. But the experiments show that the rotation trick is not used in Bayesian neural network. The computational complexity of $O(d\log d)$ seems too high to deal with Bayesian neural networks. In addition, Figure 3 is barely readable and (7) seems to lack brackets.