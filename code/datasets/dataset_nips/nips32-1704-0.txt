This paper considers the problem of estimation the mean mu of a univariate Gaussian under local privacy (LDP) constraints. This work improves on previously known bounds in both the minimax rate (by shaving some logarithmic factors and achieving near-optimal rate), the number of rounds of adaptivity (how many times must the center interact with any individual user), and the guarantee itself (pure locally privacy instead of the weaker approximate local privacy). The authors provide results in two different settings: - known variance (non-adaptive and one-round-of-adaptivity protocols) - unknown variance (non-adaptive and one-round-of-adaptivity protocols)  The techniques are interesting, especially the idea of first estimating the mean "bit by bit" (via their "mod 4") trick). The introduction is very clearly written, and provides a good overview (with one big caveat -- see below, about a paper of Duchi and Rogers). I am, however, less convinced by the rest of the paper: while the high-level discussion of the proofs are roughly OK, there is a constant referring to subroutines not introduced or barely discussed (KVRR1,KVRR2,KVAGG1.,KVAGG2...). I would strongly recommend an overhaul of the writeup, to change this (details below).  1. In the preliminaries, remove Definition 2.1. It is irrelevant and confusing (and takes space), as you never use global DP. 2. you give the pseudocode of Algorithms 1 and 2. This is good, but useless, as you don't give any pseudocode or formal description of *all* the subroutines it invokes. What is the point then, just showing there is pseudocode, but not allowing anyone to actually implement it? I'd recommend including these subroutines in the paper, or remove the pseudocode. 3. similarly in the proofs: you refer to those subroutines a *lot*, but never give enough details for someone not familiar with the literature to begin with. 4. remove the lower bound section entirely: indeed, all the lower bounds (unless I am mistaken) are entirely subsumed by Corollary 5 (arXiv version: https://arxiv.org/abs/1902.00582; for d=1) of Duchi-Rogers'19 (COLT'19). Since the techniques are different, it may be good to keep a paragraph pointing to the supplementary material for a different proof using the SDPI as you do, but it should not take one full page of the submission (esp. since Lemma 5.1 is trivial, and Lemma 5.2 is folklore)  Beyond these "big" comments, a few more: - Discuss the Duchi-Rogers'19 (COLT'19) in the introduction. - In all your statements, correct the (IMO, bad) way you define the sufficient condition on n: "n= Omega(f(n))" (i.e., n appears in both the LHS and RHS) is not satisfying. (Theorems 3.1, 3.2, 4.1...) - In Theorem 1.1 and 1.2, drop the "alpha" introduced for "neatness": (i) it doesn't help, as it hides the important dependencies in the parameters, and (ii) it conflicts with your use of alpha in Table 1. - Theorem 1.3: I assume you meant little-Oh (o(alpha))? Otherwise, this seems to contradict the 2-round protocol of Theorem 1.1. - Section 2: " a single i.i.d. draw" doesn't really make sense. What you mean is clear, but you should rephrase it. - Section 3.1.1: Discuss/give intuition as to why you take it mod 4 and not, say, mod 2 (or 3) - Section 3.2 (ll. 163 and 167): give some intuition as to why this is sqrt(log n). Is it because of a tradeoff you are trying to optimize? If so, which one?  Question: your algorithms only appear to use Gaussian concentration. Would they work for any subgaussian distribution? (other question, related: how robust are you algorithms to model misspecification, say if the distribution is only approximately Gaussian?)  UPDATE: I have read the authors' response. Please, make sure to also take into account all the other comments (e.g., intuition about the sqrt(log n), about why the modulo 4, etc.) when revising the paper.