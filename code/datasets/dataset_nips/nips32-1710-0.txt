Originality - There have been rapid progress in the field of image-to-image translation. Notably, methods generalizes the image-to-image translation to handle unpaired data (e.g. CycleGAN), multi-domain (e.g., StarGAN), and multi-modal outputs (e.g., DRIT, MUNIT). This work builds upon all these methods and presents a method that is capable of performing multi-domain, multi-mapping from unpaired training data. While this is a straightforward and expected extension, I think this is a nice contribution.   Quality - The method is technically sound. However, the work integrates existing losses and disentangled representation (e.g., in DRIT, MUNIT) to achieve the multi-domain, multi-output mapping. The technical novelty is somewhat limited.   - The experimental validation is not very convincing.  1) The "diversity" has not not quantified.  2) In Table 2, it seems that the main improvement over prior art is due to implementation details (e.g., projectionD). It is hard to draw conclusion from the Table. Also, as far as I know, the LPIPS scores are "the lower the better". Table 2 seems to treat it the other way around.   3) The paper claims that the method can perform "multi-domain" image-to-image translation. Yet, most of the applications shown in the paper (except the face attribute experiments in the supplementary material) are mapping between "two domains" only. Examples include seasons, weather, and time of day.  Clarity - The paper is well-written. The implementation details are clearly discussed in the paper and the supplementary material.  Significance - I think the paper will stimulate future research in image-to-image translation.