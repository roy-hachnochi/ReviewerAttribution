The paper proposes to boost translation quality of a non-autoregressive (NART) neural machine translation system through a conditional random field (CRF) that is attached to the decoder.  The CRF reduces the translation quality drop compared to autoregressive neural translation systems by imposing a bigram-language model like structure onto the decoder that helps to alleviate the strong independence assumption that NART architectures entail. The CRF is jointly trained with all other parameters of the neural network. Experiments conducted on WMT14 and IWSLT14 En-De and De-En tasks are reported to yield improvements of more than 6 BLEU points over their corresponding baselines.  By augmenting the decoder with a Markov-order 1 CRF, the resulting network is strictly speaking no longer a non-autoregressive system. The CRF has similar qualities as using a bigram-language model, and even if the increase in latency at inference time is small, one may yield similar quality improvements with only marginal latency increase by choosing one of the many other autoregressive components as the last decoder layer. (Even collecting the bag of top-k scored tokens at each target sentence position and conducting a fast beam-search using a trigram language model may already give similar improvements.) The paper does not describe on which hardware the latency measurements were taken, and there is also no explanation of how the rescoring experiments reported in Table 2 were conducted.  L34: The citations on previous studies is incomplete. E.g., the work by Jason Lee, Elman Mansimov and Kyunghyun Cho on "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement" is missing.  L38: In Equation~(2), the variable z is unbound, and its explanation as "a well-designed input z" is insufficient for its understanding.  L40: The examples shown in Table 1 refer to a specific NART model, and the authors present these examples as if any NART model would exhibit the same type of translation errors, which is clearly not the case. For better clarity, the authors should provide (or reference) a description of the specific architecture and hyper-parameter choice of their NART model from which they derived their examples.  L44: The use of the terms "multimodality" and "multimodal distribution" seems inappropriate and somewhat of a misnomer in this context: There is no indication that the target distribution has more modes ("peaks") than can be captured by the network. The root cause merely seems to be the independence assumption. The same goes for L82 L83 and L204. Maybe the authors meant multinomial (?).  L59: It's best to give a reference to those subsections that will describe the low-rank approximation and the beam approximation.  L140: Z(x) is not a normalization *constant* but a normalization factor (aka. as partition function).  L214 The approach to predict the target length T' merely as the source length T offset by a constant C seems implausible: Particularly for a language such as German, where compounds occur more frequently compared to English, one would expect a linear relationship between T and T'. Would it not be more plausible to make T' a constant and use padding tokens to fill up the target sentences?  L222: The authors should make clear whether they compute case-sensitive or case-insensitive BLEU scores. Judging from the examples given in Table 1, I conject that a case-insensitive BLEU score evaluation was used. A case-insensitive evaluation artificially inflates BLEU scores though, whereas the baseline numbers in Table 2 that are cited from literature report case-sensitive BLEU scores and therefore tend to be nominally lower.  The paper does not describe how the rescoring results reported in Table 2 were set up nor what the numbers 10, 100, 9, and 19 refer to.    