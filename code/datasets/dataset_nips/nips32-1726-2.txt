My main concern with this paper is authors calling their approach non-autoregressive. Running forward backward algo in CRF breaks that conditional independence assumption among predicted tokens which makes the proposed approach autoregressive. Although I would agree with authors that decoding with CRF is much more lightweight and is faster compared to decoding with autoregressive Transformer, where output has to be fed back in as an input into a deep sequence model. I would suggest authors using word "fast" neural machine translation instead of "non-autoregressive" neural machine translation in the paper.  Regarding model itself, I would also like to see an ablation study on the effect of vanilla non-autoregressive loss as well use of different target lengths on the final performance in Table 2.  Apart from my points above overall I believe it is a well executed paper that introduces several techniques to make CRF + Deep Neural Nets applicable for Neural Machine Translation and I recommend its acceptance.