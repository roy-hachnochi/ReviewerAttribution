Originality: 7 / 10 This is a novel paper that is well motivated and executed. Admittedly, all of its components are not novel alone -- grid linear mixture for image augmentation [6], meta-learned generator [35], episodical procedure, and standard few-shot classifiers. The proposed pipeline itself is new and does provide insight that end-to-end image-augmentation is feasible with a strong generator initialization. And also, finetuning a GAN towards certain modalities (or observations) are not informatively studied before. Figure 1 and its experiments could serve as a good reference to researchers who want to study image augmentation. ---------- Quality: 8 / 10 The experiments, as well as the pilot study, are in great shapes and considerations. The shown performance gains are consistent and non-trivial compared to its previous works. One thing to note is that the baselines are rather old -- the most contemporary model in the comparison table is RelationNet [29] from CVPR 2018. I believe there are still a lot of missing numbers out there. And what will make this paper even better are experiments on ImageNet itself to see how it will scale to large benchmark. But I am not exactly sure if BigGAN has seen the testing set during its training.  ---------- Clarity: 8 / 10 The writing is good and it was a great pleasure reading it. Notations are consistent and there are very few typos that do not actually interrupt the flow. It is such a pity that authors have not included the code, but I would strongly suggest to opensource as early as possible since people will be following it.  ---------- Significance: 8 / 10 It would be better if there's any intuition or even empirical study on the correlation between the fusing weight and the real/synthesized image pairs. ---------- Other comments: (1) How would your augmented fused image embed on feature space? And how will it affect the final decision of the classification head? Since you are primarily focused on the one-shot learning task, it will be informative and also doable for such illustration. (2) L180, typo, inconsistent $\mathbf{I_g}$, change it to $\mathbf{I}_g$. (3) L114, logistic regression, is it a one-versus-all classifier for each category, and then select the most salient probability from all candidates? There should be more text for description.  (4) L141, $\mathbf{I}_z$, missing definition, this is the first time you use this notation without any reference.