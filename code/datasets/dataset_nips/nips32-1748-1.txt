This paper constructs a framework for performing reinforcement learning where while choosing an action, the environment state can simultaneously change. This framework is inspired by real-world systems where the environment runs asynchronously from the agents and we would like agents to learn and act in "real-time". This is an important topic of study for practical application of RL to physical systems, including robotics. The authors show that the framework can represent existing MDPs. They also introduce a new learning method based on the Soft-Actor Critic for learning in this framework. Empirical results show that the real-time algorithm outperforms the existing SAC algorithm on various continuous control tasks.  Would this approach handle real-world effects like jitter? I can see a scenario where jitter shifts multiple state changes into the same transition. It seems like your formulation only uses the latest state, so if jitter could cause you to miss assigning rewards for important states. Would effects like this be considered a partially observable RTMDP? Or would you just have to set an action rate as low as the lowest expected delta between state observations due to jitter?  Line 78: Is this actually true? I could imagine a scenario where if the agent had taken a lot of small fast actions with t_s < t_\pi it would achieve the desired behavior. But if it waits to act at a slower rate by continuing computation as suggested here, the required action may be outside of the action space of the agent (assuming that its action space is bounded).  Making the code available would make this a stronger, more reproducible paper. Similarly for the car simulation used in some of the experiments.  Overall, this is an interesting area to study with potentially a lot of impact on real-world RL applications. The experiments show that even in environments where the standard MDP model works well, by considering the problem in the real-time framework, there is potential for even faster learning and better final policies.