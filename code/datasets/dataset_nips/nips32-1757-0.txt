Summary =======  The authors propose to use a already known method as a pooling function for time series.   The idea is to leverage an integral transform, the *path signature*, to map a discretized curve on a real valued sequence.  Truncation leads to a vectorized representation which is then used in practice.   The proposed transformation is differentiable and thus can be integrated into models trained via backpropagation in an end-to-end manner.  As the transformation consumes the one dimension, i.e. the time, stacking it requires to reintroduce a time series like structure in the output.  One way of doing so is discussed by the authors.   Finally, the application is evaluated on synthetic datasets in the context of (1) learning a generative model, (2) supervised learning of a system hyper-parameter, (3) reinforcement learning.   Originality ===========  Applying functional transforms as pooling is not a new idea.  However, using path signatures as introduced in this work may be beneficial for certain applications.   Quality =======  The overall notation is good and easy to follow.  Maybe a little more effort could have been spent on introducing the path signature, e.g., the notion of a tensor product is ambiguous and thus should be explicitly defined.   There are two major issues which have to be specifically addressed:   (1) The lack of motivation.  There are many ways of computing signatures of curves, indeed every spectral representation, for example. I do not the *selling point* for the path signature.  Are there striking benefits over other transformations?  Correlated to this, the related work section is stunningly short. Are there indeed no other approaches using some sort of functional transformation as intermediate differentiable function within the deep learning framework?   (2) There is no conclusion! Given that the motivation is somehow assumed to be granted I would at least expect a conclusion summarizing the contributions and insights.    Clarity =======  The overall clarity is good. The figures are well executed and contribute to the readers understanding.   Significance ============  The authors do not outline and discuss the theoretical benefits of the path signature (although emphasizing the rich theoretical body behind it).  In this context the contribution boils down to applying an already known functional transform as intermediate layer in a deep model and showing that it can outperform on not-benchmark synthetic datasets.  From my perspective this sums up to a rather low contribution/significance.  