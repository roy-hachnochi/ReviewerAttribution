-- The rebuttal answered all my questions. -- The problem that this paper addresses is to minimize or maximize an objective by optimizing a policy that is parameterized by a nonlinear function approximation subject to a set of constraints. The general parametrization is nonconvex, so the objective and the constraints are given by nonconvex functions, and additionally they are also stochastic. The authors propose constructing a convex approximation to the nonconvex functions by using a first-order approximation and an additional strong convex term to ensure stability and the convergence of the method. The authors leverage previous results from [34] to prove that their method converges to a stationary point as the number of iterations approaches to infinity. A constrained linear-quadratic regulator problem is used to show the applicability of their method.  Detailed comments:  Introduction:  The authors give a good overview of the state-of-the-art in policy optimization with respect to constraints. Particularly, they compare their approach to Lagrangian methods, which relax the constraint by using a linear penalty function, and then solve a reinforcement learning problem with a given Lagranian multiplier. They also mention a cross-entropy-based optimization method that achieves asymptotic convergence.    Theoretical section:  In line 85, I think the authors mean "maximize (1)" instead of "minimize (1)", given the optimization problem in Equation (1).   In line 94, the authors mention further reducing the variance without giving any prior explanation, and it was confusing to me.   In line 119, the authors replace the objective and the constraint functions by surrogate functions with "nice" properties, and do not explain what is a nice property. The authors state some properties such as strong convexity, and they will use a strong convex function, but do not motivate the properties of the surrogate functions that they have used.  In line 137, it could be useful to define the gradient of the objective and the constraint function, or give a reference to the policy gradient theorem, the gradient of the functions are undefined in the paper at the moment.  In (5) and (6), the authors just state that \tau is positive, but they do mention the value of \tau anywhere in the paper, including in the main theorem or in the supplementary materials.  I am not sure if the expression for the J_truncate after line 173 is correct. For instance, suppose that \gamma is very close to 1, this means that the expected value of the trajectory would be very high, but the truncated value according to the expression would be very small.  For the main theoretical result, I think the Assumption 3 may be too strong, it basically states that any stationary point of the optimization problem is feasible, and makes sure that the procedure can improve on the infeasibility. However, I believe that, the assumption is too strong, and it basically guarantees that the problem is always solvable with any kind of method.  Evaluation:  The authors compare their approach with a Lagrangian method on a constrained linear-quadratic regulator problem, and show that their method achieves feasibility and converges to a stationary point faster than a Lagrangian method. I think the authors should also compare their approach to the cross-entropy method.  The authors state that their approaches work for general constrained Markov decision processes with multiple agents but they give no numerical evaluation for these problems.   No code is provided with the supplementary materials.    