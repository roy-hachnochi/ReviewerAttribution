* First, for a nonsymmetric matrix, the eigenvalues are not related to the SVD by a variational principle. This is a point of confusion throughout the paper. In line 57, you start with `given an input matrix M'. Listing the SVD / QR as a method to compute the the ED is not correct here. Later you clarify that M is considered to be a covariance matrix. You should clarify at the very beginning that the scope of your method is limited and restricted to symmetric matrices!   * It is not clear to me why it is preferable to use power iterations for approximating the gradients. You are saying in line 28 that the power iteration method is numerically not stable if two eigenvalues being close. So what is the advantage of using power iterations compared to using the analytic solution for the gradients?  * Well, you introduce some tricks like ridge regularization later in order to stabilize your computations. But what if you set your epsilon in line 117 to 0? I assume you would gain not much...   * This said, your computational experiments are a bit biased, in a sense that you have introduced a hidden regularization parameter in your network, namely epsilon. I assume that setting epsilon to 10^-4 has quite some effect. What happens if you set epsilon to 10^-12. Further, you compare your method, that uses several practical tricks (Sec 2.5), to a plain implementation of the SVD. This comparison does not seem to be fair, since you can, for instance, also truncate the ordinary SVD. Thus, I think that a fair comparison requires the use of the same practical tricks.  * I am not very impressed by your results. I do not see much benefit of using ZCA or PCA denoising here, in particular not for CIFAR10. Your results for CIFAR100 are somewhat far away from current state-of-the-art performance (correct me if I am wrong here!). Further it would be nice to compare results to [r1].  * Is PCA denoising really a new normalizing strategy for deep network?  * Overall, the quality of the writing is good.  Smaller comments: * I doubt that the eigendecomposition is widely used in deep networks. That is, because there are so many numerical issues. You may want to modify the abstract accordingly.  * You should introduce L in Eq. (3). * Why is Sigma in Eq. (5) not bold?   [r1] Iterative Normalization: Beyond Standardization towards Efficient Whitening. CVPR. 2019. 