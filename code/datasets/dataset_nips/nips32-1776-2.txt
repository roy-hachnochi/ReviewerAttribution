The basic idea is very simple: for a PCA layer in a DNN, both SVD and power iteration methods have drawbacks. To overcome this, we run SVD in the forward pass and power iteration in the backward pass. This leads to an "improper" backprop, but the results are close enough (people do this all the time, for example in batch renorm) or the errors introduced are good enough that it doesn't matter. I think that originality is somewhat limited in this paper (i.e., it combines two well-known elements in a well-known way). But I do think that the superiority of this approach to previous ones speaks to its value.  The experimental evaluation of the main method is sound and convincing. The crucial experiment for me is Table 2 that shows that basically d = 64 works for this method, whereas previous methods could only really handle d=4. A very natural question here is whether we even have to do blocks of d. On the other hand, I'm not totally convinced about Sec 3.2 and the PCA denoising: realistically, it doesn't look any better than batch normalization. I think perhaps trying this on non-residual networks might give better results, or comparing to vanilla no BN networks.  The paper is well written. Even in the face of quite a lot of mathematics, the paper is clear and a pleasure to read. I found only one typo (line 106 'backpropogation'). This is a high quality submission, and the authors have obviously put effort into the writing: thank you!  The significance of this paper is hurt a little by the niche nature of PCA layers, but as above, I think that this paper could be the basis of a lot of new ideas, so overall I think significance is high.  (It's not clear to me that citing Eigenfaces as an application of PCA layers to deep learning is appropriate?).  POST AUTHOR FEEDBACK: Thank you for the the clarifications. My critiques were quite minor before, so nothing significant has changed in the score, leaving as is.