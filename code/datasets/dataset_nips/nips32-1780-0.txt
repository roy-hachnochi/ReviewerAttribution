Originally: Although phoneme duration prediction is widely adopted in conventional TTS systems, jointly training it in a neural TTS model is new. This paper is one of the first works on non-autoregressive text-to-spectrogram modeling.  Quality: This paper seems sound overall, expected for a few issues in the comments below. Some of these issues must be addressed before acceptance.  Clarity: A well written paper. A good reading to me, except for a few comments below.  Significance: The advantages over its autoregressive counterparts are significant, especially for industrial use. It’s likely to be followed by the research community as well as the industry.  Comments:  1. It’s interesting to see the using of 2-head attention for the Transformer block, instead of more popular setting ups such as 8 in the baseline Transformer TTS model. Does it bring benefits?  2. What’s the reason for using the mel-spectrogram generated by the autoregressive model for distillation training, instead of using the groundtruth mel-spectrogram? Intuitively, the groundtruth gives more accurate information.  3. Sec. 4.3 says that the FastSpeech model is partially initialized from the autoregressive Transformer TTS model (phoneme embeddings and FFT blocks) as they share the same architecture. However, the hyperparams given in Appendix A as well as in Sec. 4.2 shows these two models are of different dimensions for these components.  4. The pre-net and post-net of the baseline autoregressive Transformer TTS, as well as the decoder’s final linear layer of FastSpeech seem missing from the hyperparams comparison in Appendix A.  5. Experiment results on inference speedup -- what’s the batch size used for this evaluation?  6. The latency numbers in Table 2 and Figure 2 seem inconsistent. The numbers in Table 2 seem unrealistically fast.  7. Robustness experiment -- Since you have included Tacotron 2 in Table 1, it would be nice to also include Tacotron 2 in Table 3. Tacotron 2 is another widely discussed attention-based model which is considered also suffering from robustness issues due to attention failure. It will be interesting to include such results for comparison.  8. Needs a reference for CMOS evaluation.  ==============  Update: Thanks for authors' response. I updated my score accordingly.