The paper introduces a new algorithm to learn hierarchical clusters based on fitting an ultrametric to the data. This is achieved by transforming an optimization problem over the constrained set of ultrametrics to one over an unconstrained set of weighted graphs, with the ultrametric constraint implicitly satisfied by the cost function. The idea of transforming the ultrametric fitting in a way conducive to gradient based methods is novel. It allows many ideas for regularization to modify hierarchical clusters in a way that standard hierarchical clustering approaches e.g. hierarchical agglomerative clustering does not allow as easily.  The paper then demonstrates the flexibility of this new formulation by showing how different regularizations and data fidelity terms can be incorporated into the cost function. For example, noticing that in a vanilla MSE cost, small clusters appear high up in the dendrogram, the paper proposes penalizing such occurrences to produce more balanced hierarchies. Another example is the ability to allow semi-supervision in the form of specifying triplets. These cost functions are then showed to be approximated by differentiable functions and (in most cases) efficiently computed in a gradient descent framework. Finally, experimental results validate both the framework (using the CUCP algorithm as baseline) for runtime and accuracy and show that the quality of the clusters are comparable to standard algorithms (Ward and semi-supervised-SVM).  The paper is well written and organized. It potentially opens new directions for exploring hierarchical clustering algorithms through the new ultrametric fitting approach.  