The authors theoretically analyze the dynamics of continuous and discrete optimization for one and two layer linear neural networks under square loss. For the continuous case, they improve the results of Saxe et al. by proving the implicit regularization under more reasonable assumptions.   They also analyze the behavior of discrete dynamics for the same networks. To the best of my knowledge, analysis of discrete dynamics is novel.  In theorem 1, I understand that the specific initialization is required in your proof for the results to hold. I wonder to what extent this initialization is necessary? If one initializes the matrices to a matrix with vanishing nuclear norm but different singular vectors, I guess we still should observe the same phenomenon of sequential learning. Can the authors clarify to what extent this initialization form is necessary?  The fact that the authors were able to relax the assumption that the two matrices should commute is very interesting. A question similar to the previous one regarding this assumption: is this assumption that the two matrices should approximately commute just needed for the proof or do the authors think it is necessary?