The paper studies the gradient dynamics of the SGD learning algorithm in the context of the two layer linear network.   The authors consider two scenarios: continuous and discrete gradient dynamics. The former has been studied in Saxe et al 2018 but the authors manage to loosen the assumption and provide a more general result. In particular, under the assumption that both the covariance matrix X' X and correlation matrix X' Y share the join decomposition, SGD updates can be derived to have a close form. Additionally, the paper shows an interesting asymptotic phenomenon: as the initialization of weights tends to go zero, the weight product at any SGD iterate converges to the solution of the reduced rank regression problem.   A more novel result is the formulation of the discrete gradient dynamics where the authors show that under a bit more stringent assumption, the SGD updates can have a close form.    Although the paper shows interesting and novel theoretical results, the main concern is that the assumption 1 is too strong. It is not clear that this assumption is met in practice. Furthermore, the theoretical analysis might not be able to generalize to network of more than 3 layers. For this, the paper is more suitable to be considered as a low rank regression problem, rather than a deep linear neural network.  