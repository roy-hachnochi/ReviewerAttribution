Summary:  This paper studies the implicit regularization of discrete gradient descent algorithm for over-parameterized two-layer neural networks. By making mild assumptions on the data, this paper finds out that the discrete gradient dynamics learn the reduced-rank regression solutions sequentially.  The experimental results support the theoretical results in this paper.  Pros: - The theoretical results, i.e., gradient dynamics sequentially learn the solutions, on the reduced-rank regression problem are novel and interesting. The results can shed lights on representation learning of deep neural networks. - The experiments on both synthetic and real datasets are well-designed, which support the theoretical results as well as validate the assumptions.  Limitation & Questions: - The analysis seems to be specific to two-layer linear neural networks. Could this be possibly extended to deep neural networks?  Typo: - L459-L460, \bm{W}_{1}(t)^{0}.  Other Comments: The code 'M_0 = 10**-3 * 1/np.sqrt(d) * np.random.rand(r,d)' in [18], 'r' is not defined. After replacing r with a number, the code can reproduce the results in Figure 2.  There is an omission in the related work on implicit regularization: https://arxiv.org/abs/1712.09203