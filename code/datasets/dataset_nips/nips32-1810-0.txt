The work is fairly interesting. It addresses set prediction that has been frequently encountered in a range of problems such as object detection where the order-invariant of a collection of predictions is preferred. It is particularly inspiring that the authors leverages the property of an encoding process that is order invariant and reverses the process for set prediction. In the authors' own words, "to decode a feature vector into a set, we can use gradient descent to find a set that encodes to that feature vector."  I have a few questions about the model and the experiments:  1. For the auto-encoder case, "the same set encoder is used in encoding and decoding". Does it mean the bottleneck, z, is of the same dimension as Y, (dxn)?  2. For Algorithm 8, is the loss on line 8 for training the set decoder? If so, this should be made clear in the formulation, so is in Equation 6.  3. The paper mentions that the two optimization processes are nested. However, they seem to be performed in seq from Algorithm 1. Please clarify it.  4. Did the baseline models with MLP use Equation 1 and 2? MLP would benefit from Chamfer and Hungarian matching as well, which I feel is what are typically used in existing solutions. To me, Line 2-7 in Algorithm 1 is a core part of the contribution of the paper, rather than the Eq. 1 and 2.  =========== Thanks for your rebuttal that answers my questions. My rating towards the paper remains positive.  