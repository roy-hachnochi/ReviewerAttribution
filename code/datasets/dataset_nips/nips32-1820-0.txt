This work focuses on sampling determinantal point processes (DPPs) by utilizing deep learning architecture so that the running time for sampling becomes faster than the standard DPP sampling. To avoid complex operations (e.g., matrix inversion or eigendecomposition) used in the standard DPP sampling, authors come up with an attention mechanism where it simply needs to dot-product of features or kernel matrix itself. As far as I know, most of the prior works in deep learning community borrow DPPs and apply them at the top of layer in the network for the purpose of generalization, compression and so on. Unlike the priors, this work utilizes deep learning models in order to simplify the DPP sampling. This can be a great impact for future works of other models whose inputs are a subset of items and guide them to capture their characteristic (e.g., ranking) using deep learning architectures.  Authors also analyze that the proposed network satisfies with the log-submodularity under certain condition, which can justify using the greedy algorithm for approximating the mode. Although the condition is even hard to check in practical, empirical results for the mode take the best performance in practical applications including kernel reconstruction. This paper is also well-written and well-organized. The motivations and contributions are intuitive and straightforward.   But, some descriptions may give confusion to readers not familiar with DPPs. For example, the objective function to train the model is not provided formally. Authors should describe the objective function (or loss) for DPP sampling more concretely. And it is ambiguous how the normalized log-likelihood and negative log-likelihood are different.  As they reported in the experiment, samples of DPP mode from their network can achieve the minimum NLLs. However, it is not fair to compare their DPP mode to other sampling-based methods, but not for mode. It would be better to benchmark the standard DPP mode algorithm (a.k.a.DPP MAP inference).   In addition, even the sampling process is simple and fast, the training time to learn the model is essential. There are also works for boosting DPP sampling algorithm, e.g., tree-based algorithm [3] or Nymstrom approximation [4]. Both can take the sublinear-time with respect to the size of the ground set while the proposals are linear-time, which can be slower for the large-scale setting. It would be also better to compare the running time of the proposal to other fast algorithms.  Although their benchmarks are weak due to a lack of other similar works, their approach is a new concept applying for DPP and valuable for other works related to DPP. Therefore, it is enough to accept.  [3] Jennifer Gillenwater, Alex Kulesza, Zelda Mariet, Sergei Vassilvitskii. “A Tree-Based Method for Fast Repeated Sampling of Determinantal Point Processes”, ICML 2019. [4] Michał Dereziński, Daniele Calandriello: “Exact sampling of determinantal point processes with sublinear time preprocessing”, 2019, arXiv:1905.13476.  ========================================================================================== I read all reviews and author feedback. Authors have replied my comments with clear and promising explanations. The contributions of this paper would be further improved if all comments come up with the final manuscript.