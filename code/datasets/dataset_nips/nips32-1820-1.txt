** Proviso **  I wanted to pre-emptively point out that besides a general familiarity with the standard formulation and use of DPPs, I do not consider myself up-to-date with recent developments in this field. Consequently, I may not give the most critical assessment of the work. However, I will do my best to review the work to the best of my knowledge.  -- Paper Summary --  DPPs are a well-established family of models for obtaining a set of diverse samples from a fixed or varying ground set. However, the widespread use of such models is hampered by their computational complexity, which is tied to the inversion or decomposition of a possibly large kernel matrix. In this work, the authors propose an alternative DPP model which is instead formulated as a neural network. The proposed architecture is shown to satisfy properties that are typically expected of DPPs, while also providing a substantial speed-up in comparison to standard DPP models and more recent alternatives based on MCMC sampling. The effectiveness of this method is showcased using a synthetic experiment for drawing samples from a unit square, a comparison to other techniques on more widely-established datasets, as well as a practical application of DPPs for kernel reconstruction. The scope of the paper covers both theoretical contributions, as well as more practical elements for designing and fine-tuning the architecture of the proposed DPPNet.   -- Writing/Clarity --  The paper is extremely well written and structured. It was a pleasure to read, and I could barely find any typos in the text. For someone who does not follow recent developments on the topic of DPPs, I found that the concepts explained in the paper are well conveyed and easy to follow. The figures are also neat and properly complemented by the associated text. Good job!  Some minor typos:  - L71: ‘k’ is defined in the abstract, but not in the main text. While fairly obvious, reintroducing  it would be better; - L104: Lower case ’n’ hadn’t been used prior to this instance; - L131: Extra ‘a’; L133: ‘with’ -> ‘where’; - L182: Corollary is shortened as ‘Cor.’ elsewhere.   -- Originality and Significance --  Using a neural network for emulating the functionality of a DPP appears to be a novel contribution which has not previously appeared in the literature. Beyond simply explaining a basic model for carrying out this task, the paper also presents several variations including an attention mechanism, an algorithm for greedy sampling, and also points out several workarounds for overcoming potential limitations associated with this method (such as setting a buffer N’ > N in case of having a ground set which can increase in size). To the best of my understanding, these are all noteworthy contributions which go beyond being simply ‘incremental’.  I think there is an appealing depth to this work which balances both theoretical contributions - in terms of showing how DPPNets obey certain properties of DPPs - as well as more practical ‘engineering’ elements which enable the speed-ups highlighted in the experiments. Perhaps the paper slightly struggles in coming up with a convincing real-world problem where the increased scalability of DPPNet is essential, but the speed-ups achieved in relation to other methods without compromising on performance should definitely interest practitioners woking with DPPs. The theoretical aspects of the paper, although interesting, are not explored thoroughly, which the authors put down to the intractability of effectively verifying the provided proofs. Nonetheless, I think this sets up an interesting avenue for future work by encouraging further exploration of this connection between neural networks and DPPs.   -- Technical Quality/Evaluation --  As highlighted in the preface to this review, I cannot vouch for the correctness of certain theoretic aspects of the work presented here. However, the material I was able to verify appeared to be correct and clearly presented.   The experimental evaluation is also well rounded, highlighting the different variations of DPPNet in comparison to competing techniques before ultimately settling upon a single variation (DPPNet Mode) which performs more consistently than the standard DPPNet. The experiments seem fair, with appropriate metrics (including variance) provided for each set-up. The results should also be reproducible since there is sufficient material in the section (and followed up in the appendix) which includes details on how the datasets were preprocessed and how the architectures were constructed.    -- Overall recommendveration --  While re-iterating the proviso to undertaking this review, I do genuinely believe that this to be a very well-rounded paper which ticks most of the criteria I would expect from a high quality submission. The paper is written in an exemplary manner, the concepts are clearly explained, and the experiments supplement the novelty of the proposed method by also showing how its use can lead to both speed-ups and also performance improvements over competing DPP methods.  ** Post-rebuttal update **  Thank you for your reply. I liked this paper from the first read-through, and reading the other reviews and your rebuttal confirmed my original impressions. There are some minor refinements suggested by the other reviewers which could further improve the overall quality of the submission, such as updating the related work section with references to the recently published work indicated in another review, clarifying parts of the exposition, including the additional comparison etc. However, this remains a solid paper through and through._x000C_