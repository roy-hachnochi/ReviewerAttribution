The paper contributes to the overall class of MARL algorithms as another simple communication method that improves performance with reduced communication costs.  - I am a bit worried about the methods narrow application. It was only evaluated on a collection of similar Starcraft II environments. It also only works on cooperative environments. - Line 111 the Q function targets should be optimized over s_{t+1}, not s_{t}. I think this is just a typo and does not reflect in the results. - I do find it odd that MADDPG (Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments) was not referenced in this paper. It is very related and has a form of implicit communication. - The change to the learning loss is simple.  - There is little discussion on the learning hyper parameter introduced and the messaging thresholds. How are these chosen? How sensitive is the method to these values? It is not explicitly said what values are used for the experiments. I assume the same from the figures.  After going over the author response I appreciate the extra analysis put into comparing the method to MADDPG to make sure it is state of the art. It is good to compare these methods across previous benchmarks to show improvement. While the additional hyperparameter analysis is helpful it is a bit obvious of what is normally done. Some discussion on the effects of specific settings might shed more light on how the method works. I have updated my scoring. 