This work employs techniques developed in network science literature, such as latent space model (LSM) and stochastic block model (SBM), to propose a generative model for features X, outputs Y, and graph G, and it uses graph neural networks to approximate the posterior of missing outputs given X, observed Y, and G. This work is a wise combination of recent methods to effectively address the problem of graph-based semi-supervised learning. However, I have some concerns, which are summarized as follows:  - Although the paper proposed a new interesting generative method for graph-based semi-supervised learning, it is not super novel, as it employs the other existing methods as the blocks of their method, like LSM, SBM, GCN, GAT.  - It seems the generative model is only generative for G given X and Y and by factorizing the other part as p(Y,X) = p(Y|X) p(X), for p(Y|X), it is modeled via a multi-layer perceptron, which is a discriminative model. That is why the authors discard X in all the analyses, like any other discriminative model, and say that everything is conditioned on X. I think this makes the proposed model not fully generative. It is only generative for G but not for X and Y. I was wondering what would be the performance if you would assume p(G,Y,X) = p(G|X,Y) p(X|Y) p(Y), which makes the features X generative conditioned on the outputs?   - It is not clarified how the parameters are learned from the ELBO. For example, in SBM, are p_0 and p_1 the only learnable parameters? If yes, how the constraints are taken into account?   - Regarding the approximate posterior model, in part 3.3.2, the authors have used graph neural networks to approximate the posterior of missing Y given X, observed Y, and G. However, as mentioned in the paper, graph neural networks get only X and G as input but not any Y. It seems this is not a reasonable approximation as it is not consistent with the graph generation step, LSM and SBM, which use the label information to generate the graphs. What is the reasoning of using graph neural networks? Could you revise them to handle the labels too, which will make the approximation more realistic?   - Having mediocre performance on Pubmed data might cast a doubt on dependence of the performance of the proposed method on the input data. How could you explain the poor performance on that dataset? Is there other datasets to test on to prove the efficacy of the proposed method?   