**** I'm satisfied to accept this submission in light of the author's rebuttal, and have been inclined to increase my score to 7. ****  Originality This work is an interesting work that unifies multiple perspectives and approaches, drawing from the state-of-the-art in binary neural networks, stochastic neural networks, neuromorphic approaches, deep neural networks, and weight normalization.  Related work is well cited across these fields, and the approach is unique in this reviewer's knowledge.  Quality The quality of this work is adequate, though there are a couple of simple errors in the text (misspelling in Figure 1, missing sections in the supplementary material, lack of explanation of some abbreviations such as W_3 and S2M).  Overall, the text and derivation is done with high quality, and the tricks used in the derivation are called out to adequately describe the steps to the reader.    The conclusions stand on their own, and quality of insight is needed to bridge stochastic neural networks, multiplicative weights, and weight normalization.  The work could use more difficult datasets, though, to emphasize these results.  The stochastic neural network literature uses more simple comparison datasets than are typically used in the deep neural network literature, as more basic questions of stability, trainability, and accuracy are still being addressed in a way that has been solved in other neural net subfields.  Nonetheless, it can be difficult to know if results found on MNIST, CIFAR, and neuromorphic device datasets translate meaningfully to real-world domains.  It can be helpful to try it on a truly challenging task to examine where the method breaks down.  Additionally, there is some forcing in the derivation.  The lack of an effective learning rule is not addressed in this work; smart initialization allows the experiments to run effectively even with a biased learning rule, while a truly well-matched learning rule would be ideal.  Similarly, while one can train on the erf function as an activation function, and it even has a presence in particular neuron types [1], it is a fairly unusual function.  On a positive note, this reviewer wishes to applaud the authors for the analysis of multiple noise types, which is useful as a hardware implementation of NSMs can have a variety of characteristics.  Additionally, the diversity of experiments across datasets and neuromorphic sensor tests to validate the model are indeed substantial.  Clarity This work contains acceptable clarity, going into detail about architectures, the derivation, the datasets, and the results.  However, the derivation could be more clearly motivated by establishing earlier the aim and approach of the neural sampling machine formulation, as there are numerous branches of the derivation (multiplicative vs. additive noise, Gaussian vs. Poisson noise, the addition of an offset parameter to the noise).  Establishing earlier the approach could set that structure up for the reader to more easily follow.  Significance This result is significant to the neuromorphic engineering community, which can build devices that efficiently exploit many properties of neural sampling machines.  Additionally, there are interesting implications for online learning or continual learning, especially in-device, and the work beckons future research to establish a more efficient learning algorithm for this formulation of neuron.  This manuscript has the strong possibility to inspire other future work.  [1] Jug, F., Lengler, J., Krautz, C., & Steger, A. (2012). Spiking networks and their rate-based equivalents: does it make sense to use Siegert neurons?. Swiss Society for Neuroscience.