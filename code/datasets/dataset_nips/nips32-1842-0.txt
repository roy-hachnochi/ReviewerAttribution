Overall opinion This is a good paper. The motivation is clear and the method is simple. The paper is meaningful in that it provides a new method to parameterize and learn a curvature in weight space. The analysis makes connections to second-order methods and also shows how the learned curvature operates in terms of previous gradients. Experiments show the advantages of their method.  +Clearly written. I especially appreciated the crisp introduction to tensor algebra (section 2.1). +Analysis shows connections to Fisher information matrix, and the decomposition of the altered gradient (eq 10) shows how this method implicitly “memorizes” previous train- and val- gradients. +Experiments show that meta-curvature outperforms MAML, meta-SGD, MAML++, and layerLR  -WRN experiments (Table 4) show that meta-curvature is slightly outperformed by LEO, though I believe this can be overlooked as LEO results involved heavy engineering. -The paper did not cite and compare to a relevant previous work with a similar motivation: [1] also learns additional parameters for MAML to learn to alter the gradient, and the learned parameters correspond to a learned curvature for each layer.  Minor comments: -line 75: If I understand correctly, it is n-mode unfolding, matrix multiplication, and then reverse(?) n-mode unfolding. Otherwise, the result would be a 2-order rather than an N-order tensor. -section 3.2.2: I personally found this, along with figure 1, harder to follow than section 3.2.1.   [1] Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace, ICML 2018  ------------------------ post-rebuttal ---------------------------- The authors have addressed all of my concerns, and the additional experiments highlight the advantage of meta-curvature even more. I maintain my score.