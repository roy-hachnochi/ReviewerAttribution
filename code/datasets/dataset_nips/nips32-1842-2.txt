Originality:  The paper is quite original, I am not aware of similar studies.  Clarity:  Overall, the paper is written well, especially the most challenging sections 3.2 and 4. I have a number of targeted comments thought that I think should be addressed before publication:  1. Line 34: “They compute local curvatures with training losses and move along the curvatures as far as possible.”  I am not sure what authors mean here, perhaps some clarification would be helpful.  2. Line 130: Why is the layer parametrised in this way? If it is a convolutional layer, I would expect a 4rd dimension, the number of output channels. Do authors also consider fully-connected layers? What kind of curvature parametrisation is supposed to be used in that case? 3. Can authors provide more comments on the representational power of their _parametrizations_ (not gradients), e.g. with comparison to other tensor methods, especially Tensor-Train decomposition?  4. Does the batch normalization anyhow affect the analysis provided in section 4 due to gradient sharing? Can we get rid of the batch norm at all as the learned curvature can learn even a better normalisation scheme? 5. I have got a bit confused with the “train”, “validation” and “test” sets used in section 4,  in the standard MAML setup is the meta-update computed on the “train” set and the initialisation is updated based on the loss on the “validation” set (using the paper’s terminology)? If so, I am not sure if the “validation” is the best term to use.  Quality:  The proposed method seems to be solid. The experimental comparison is also broad enough to claim an empirical contribution.  Significance:  The problem of generalisation in few-shot learning is very actual and the paper addresses this problem in a novel, interesting way. Learned curvatures might find applications in other problems too. 