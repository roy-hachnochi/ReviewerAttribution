# Update after rebuttal  I thank the authors for their rebuttal, which addressed my questions. I am confident that this would make a good submission; I am thus raising my score to '8'.  # Summary of the review  This is a very good paper, which I enjoyed reading. The write-up is very dense and technical, but the main message is very clear. I particularly enjoyed the unifying view on the two QAP that is given by the paper. It is a good use of kernel theory.  I have a number of questions and suggestions to further improve the paper, though, as well as some comments about the clarity of the proposed method.  # Review  ## Method (clarity, technical correctness)  - In l. 25, the affinity matrix is introduced as having a dimensionality   of $n^2 \times n^2$. At this point, it is not clear why that should be   the case; ideally, the introduction should lay the ground for giving a   good explanation about the computational complexity.  - While the paper makes it clear later on, the use of the term 'array'   struck me as unclear when first reading it. This should be explained   at the beginning, ideally.  - Mentioning the 'local maxima issue' should be explained a little bit;   it becomes clear later on in the paper but since I am not an expert in   QAP (my background being pure mathematics), it took me some time to   get that concept.  - The node and edge similarity functions in l. 76 should be briefly   introduced. Later on, they are taken to be kernels; could this not   already be done here? I do not see a need for assumption 1; I think   the paper could just introduce the node and edge similarity   measurements as kernel functions directly.  - The explanation of kernel functions in Section 2.2 could be improved.   Why not use the explanation of positive definite matrices here?  - In l. 118--120, I am not sure about the terminology: should the   equality in the equation not be the inner product in   $F_{\mathcal{H}_k}$ instead of $F_{\mathcal{H}}$  - In Eq. 5, what is $K^P$? The notation looks somewhat similar to   $\mathcal{P}$, so I would assume that the matrix 'belongs' to that   space as well.  - A notational suggestion: would it be easier to write $\Psi^{(1)}$   instead of $\Psi^1$? While it should be clear from the context, one   might confuse the second notation with an exponential/multiplication   operation.  - The two relaxations in Eq. 6 and Eq. 7 should be explained better.   Why not briefly prove convexity and concavity, respectively?  - The explanation of the computational complexity (and the improvements   obtained by the new strategy) should be made more clear.  - For the scalability comparison in l. 239ff., would it be possible to   use a normalised comparison, for example by dividing the times of each   method by the respective time of the smallest method? I would like   some more details here because it is not clear whether it is justified   to compare different codes that might not be optimised for a certain   architecture.  - The analysis of parameter sensitivity seems to indicate that there is   a larger variance for $\lambda$. Ideally, I would like to see more   data points in Figure 3 to demonstrate the behaviour with respect to   this parameter.  - Why are the experiments from Section 6.2 not repeated multiple times?   I would be interested in knowing the standard deviations or variances.  - How have the parameters in Section 6.3 for the heat kernel signature   been selected? Would it matter if the values are changed there?  ## Experimental setup  The experimental setup seems to be sufficient for me. I already listed my concerns about doing a more thorough parameter analysis. Other than that, I have no further comments---I should stress that I am not super familiar with competitor methods.  ## Language & style  The paper is well written for the most part, but certain small oversights make it harder to understand at times. I would suggest another detailed pass.  - l. 27: 'based a very' --> 'based on a very' - l. 30: 'two arrays in reproducing kernel Hilbert space' --> 'two arrays in a reproducing kernel Hilbert space' - l. 32: 'develop path-following strategy' --> 'develop a path-following strategy' - l. 38: 'years, myriad graph' --> 'years, a myriad of graph' - l. 49: 'the author' --> 'the authors' - l. 86: 'there exist a' --> 'there exists a' - l. 121: 'Laweler's QAP' --> 'Lawler's QAP' - l. 146: 'small value of $D$' --> 'small values of $D$' - l. 158: 'may need to call the Hungarian' --> 'may require calling the Hungarian'  A small 'pet peeve' of mine: I would refrain from using citations as nouns. Instead of writing 'In [2, 14, 21]', I would maybe write 'Previous work [2, 14, 21]'.  Moreover, I would suggest making sure that citations are always sorted upon citing them: In L. 43, for example, the citation '[29, 28]' should be written as '[28, 29]'.  In l. 116, the citations '[45], [19]' should be merged into one citation.  One issue I have is the length of the paper, or rather the length of the paper and the supplementary materials. Ideally, the paper would give a small idea about relevant proofs. At present, to fully understand the paper, one has to read all the supplementary materials and the main text. Ideally, the paper could give at least some proof ideas.  ## Bibliography  To my understanding, all relevant related work has been cited. Nonetheless, in the interest of improving this paper even further, I would suggest improving the consistency:  - Make sure that the capitalisation of names of conferences and journals   is consistent. For example, 'International journal of pattern   recognition...' should be called 'International Journal of Pattern   Recognition', and so on.  - Make sure that authors names are used consistently. For example, in   item 12, the author names look very incorrect.