EDIT: My comments about notation and figure were indeed cosmetic, hence their position at the end of the reviews. Other points have been clarified in the rebuttal, I expect the authors to include these clarifications in their manuscript despite the lack of space mentioned. ######################  The proximal gradient algorithm allows to efficiently minimize the sum of a smooth term and a non smooth regularizer. Plug and play and regularization by denoising (RED) approaches mimicks the updates of proximal gradient, using a "denoising function" instead of an explicit prox (for some cases of denoising functions, PnP and RED correspond to proximal gradient).  The authors adapt this method to the case where only a subset of the parameter vector is updated at each iteration (generalizing proximal block coordinate descent in some cases)  The goal is to benefit from the known speed-ups of BCD compared to full gradient.   The assumptions made for convergence analysis in Thm1 sound reasonable (smoothness of the fidelity term, non expansiveness of the denoiser), but Assumption 4 is quite strong in my opinion. Thm 1 is somehow disappointing as it does not chow convergence, but only that some points get close to the zero set of G. I don't think the sentence L220 is correct, as Thm 1 does not show convergence of the iterates. I don't understand how (11) is a *generic* denoiser (L175), since it has the form of a proximal operator. ALso, in Thm 2, why not take tau going to infinity ? It seems that some constant blows up with tau, but this is not specified.  The article reads well, although the reference list is quite long and often not cited very specificly, eg [5-14]. Specificities of each paper could be highlighted better in the literature review.  Similar approaches have been developped in the imaging community, where the prox step (or its equivalent in ADMM) is replaced by the application of a predefined function (see eg Image deblurring by augmented Lagrangian with BM3D frame prior, Danyelan et al). The connection with these methods from another field is somehow lacking in the literature review.  It is not clear for readers not familiar with PnP and RED if the algorithm corresponds to the minimization of a functional, which should be better explained.  The seminal work of Paul Tseng on coordinate descent is somehow missing in the 24-27 part of the bibliography.  The use of a special font for U and G is confusing in the case of U, as it looks like an union. Can't the authors use normal U ?  Fig 1 is not very informative in my opinion: denoising problems are standard and the role of the function D is not particularly detailed.  The sentence L17 is not completely correct to me: the true prior can be known (eg the L0 penalization for sparse vectors), but using it is intractable.