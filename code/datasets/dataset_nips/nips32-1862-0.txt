============= After Author Response ==================== I have read the authors' rebuttal, my evaluation remains unchanged. =================================================== This paper proposed lazily aggregated quantized gradient methods for communication-efficient distributed learning. The main idea is to combine lazily aggregated gradient and quantized gradient into a single framework to further save communication cost in distributed learning problems.   The proposed method looks reasonable, and convergence analysis is provided showing that linear convergence is achieved under certain conditions for strongly convex and smooth functions. Empirical results were provided showing that the proposed approach improves both quantized gradient methods or lazily aggregated methods.  My main concern on the theoretical analysis, which is done only on strongly convex and smooth functions, also the dependencies on the condition numbers (smoothness and strongly convex) are hidden in the main paper. By looking at the supplemental materials, it appears that the condition number dependency are quite bad and being much worse than standard gradient methods. 