The paper extends the lazily aggregated gradient (LAG) approach by applying quantization to further reduce communication. In the original LAG approach, workers only communicate their gradient with the central coordinator if it is significantly different from its previous one. In this paper, the gradients are compressed using quantization and workers skip communication if their quantized gradient does not differ substantially from previous ones. For strongly convex objectives, the paper proves linear convergence.  The paper is very well written and the approach is clearly motivated, easy to understand, and discussed in the context of related work. Although the proposed approach is a straight-forward extension of the LAG approach, the idea is sound and theoretically evaluated. The theoretical analysis is performed using Lyapunov functions that measure not only the loss difference between an iterate and the optimal model, but also include past gradient differences and the quantization error. The relation of this measure to the simple (expected) risk should be discussed in more detail. Also, this analysis does not apply to neural networks (for the proofs, the authors assume strong convexity).   The empirical analysis shows a substantial communication reduction over gradient descent, quantized gradient descent and LAG, while retaining the same test accuracy. My only concern is that the experiments have been only conducted on MNIST. There, comparable accuracies can be reached easily (e.g., by averaging aggregates only once at the end, as in [1]). Thus, it is unclear how the approach performs in practice.   Overall the paper is a nice extension to the LAG approach with a solid theoretical analysis. Despite my concerns about the empirical evaluation, I think the paper is a nice contribution to the conference.  Detailed comments: - the approach is motivated by the need to reduce the number of worker-to-server uplink communication, because of the latency through sequential uploads. However, in [4] it was shown that asynchronous updates work in practice. It would be interesting to discuss this approach with respect to quantization, since this might lead to more collisions and thus render [4] infeasible. - it would be very interesting to see how LAQ compares to randomly selecting workers that upload (as in [2]). In this light, it would be also interesting to discuss the advantages of LAQ over approaches that dynamically silence all workers (as in [3]). - the proof of theorem 1 was quite difficult to understand and the Lyapunov-functions are not very intuitive to me. Thus it would be great if the authors could add more intuitive explanations to the proof in the supplementary material.  - the authors claim in the reproducibility checklist that their code is available but I couldn't find a link to it in the paper. - typo in line 66: "see also"  [1] Zinkevich, et al., "Parallel Stochastic Gradient Descent", NIPS 2010 [2] McMahan et al. "Communication-Efficient Learning of Deep Networks from Decentralized Data", AISTATS 2017 [3] Kamp, et al. "Efficient Decentralized Deep Learning by Dynamic Model Averaging", ECMLPKDD 2018 [4] Recht, et al., "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", NIPS 2011  The authors responded to all my comments and I remain convinced that this paper would be a good contribution to the conference. 