The authors consider the problem of reducing the amount of communication for learning in a parameter-server setting. The approach is based on a combination of quantization of new gradient information and skipping updates when innovations are not considered informative enough. In particular, it combines the idea of lazily aggregating gradients (NeurIPS 2018) with quantization of the gradient components.Â   The combination of quantization of elements and skipping updates is interesting and potentially useful. However, both the paper and the scheme itself has some limitations which would be nice to have addressed.  1. The scheme requires additional memory, both at the server (to compute update direction in absence of new information) and at the workers (e.g. to evaluate the transmission triggering conditions). Please quantify and comment on these clearly in the paper.  2. Many critical tuning parameters, such as the step-size alpha and the Lyapunov parameters xi are quite complex to tune. Although the authors provide simple parameter selections that satisfy the complex formulas, these parameters are not used in the numerical experiments. Please use the theoretically justified parameters in your experiments (at least as a complement to the present one) and comment on whether the parameters used in the numerical experiments satisfy the theoretical bounds. It would also be nice with some intuition of how you select D. In addition, it would be useful to have some discussion about how sensitive your numerical results are to the different non-trivial parameter choices.  3. It would be nice to discuss the increase in total iteration counts (as a surrogate of wall-clock time) vs the decrease in total communication load. Although this is difficult in general, with your simplified step-size expression, you should also get a simple value for the contraction modulus. You could then compare this one with the one we get for (compressed or uncompressed gradient descent).  4. In Theorem 1, you prove that V decays, but what about f or gradient norm? Can you say something at all about these?  5. Since your quantization scheme is memory-based, I think that you should also compare it against error-compensated gradient quantization schemes (e.g. Alistarh et al., NeurIPS 2018).   6. Only the strongly convex case is analyzed, so the results do not in general apply to Neural Network training. Could you also analyze the non-convex case? If not, I am not sure that it makes much sense to have the simple one-layer ReLU network in the numerical experiments.  7. Some equation references point to supplementary (this is probably a simple LaTeX compilation error, but please revise!)  ** Update after rebuttals **. Thank you for your reply. You have addressed most of my concerns and I believe that the numerical experiments consistent with the theory and your refined theoretical analysis will make the final version of the manuscript even better.   