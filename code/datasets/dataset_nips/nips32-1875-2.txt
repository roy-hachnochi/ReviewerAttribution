This ms provides an explanation of RNN, CNN  from the kernel perspective. It defines a hidden variable h_t that is dependent on previous time points. The hidden variable h_t depends on the current observation x_t and previous states h_{t-1} through an unknown function f. The predicting variable y_t is depends on h_t by a product of a time invariant factor load matrix A and a dynamic factor matrix E. The author then assumes h_t lives in a Hilbert space, as well as the rows of the dynamic factor matrix E. This assumption makes it possible to represent e_i as the same parametric form of h_t. As a result we can define a kernel for h_t, so we can operate computations in the space of x_t and h_{t-1}. As h_{t-1} and h_t lives in the same space, the kernel is calculated recursively, as shown in equation (6). If we repeat the calculation long enough, q_{\theta}(C_{t-N}) becomes constant. In this recursive process, it is shown that the kernel can be calculated in the space of x.  Based on this framework, the ms make some extensions, and provides interpretation to LSTM and CNN as special cases from the recurrent kernel machine perspective.The proposed method achieves comparable results with current state of the art methods in several experiments and improves the performance in a LFP task.   I find this ms is enjoyable in general. I have two concerns: 1. is it reasonable to assume e_i lives in the same Hilbert space? A proof of the existence might be helpful. 2. It is said that q_{\theta}(C_{t-N}) can be seen as a vector of biases. What are the conditions that guarantee its convergence? 