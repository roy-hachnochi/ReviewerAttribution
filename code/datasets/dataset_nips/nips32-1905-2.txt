This paper is tough to review. On one hand, it's well written and carefully thought out. But I don't come away from the paper with a clear idea of what I should do with xAUC, or why I should prefer it over other measures. On occasion, the authors describe xAUC as measuring misranking, which -- if this is indeed what it measures -- would suggest an immediate intervention to remedy the misranking. However, the authors caution against efforts to adjust the scores to equalize xAUC. At other times xAUC is described as a diagnostic, but even the Bayes-optimal predictor could produce significant xAUC disparities, while a miscalibrated score (where there is clear misranking -- one group's risk is being consistently over or under-estimated) could produce no xAUC disparities. So it's unclear how one should interpret xAUC differences.  The fairness literature is awash with fairness metrics, and I think the burden is on those proposing new metrics to make a compelling case for why we should prefer their metric to the existing alternatives. This paper has not convinced me that xAUC provides significant value over existing metrics.  Response to author feedback:  It's not the case that there are "no metrics specifically for disparate impacts of continuous risk probability scores." For example, in their paper "Risk, Race, & Recidivism: Predictive Bias and Disparate Impact," Skeem & Lowenkamp measure bias in continuous risk scores by fitting logistic regression curves to the score-outcome relationship for each group. Following the American Psychological Association's "Principles for the Validation and Use of Personnel Selection Procedures" they interpret significant differences in either slopes or intercepts between groups as evidence of bias in the risk scores (in other words, they require sufficiency to hold). This has been the APA's recommended way to measure bias in risk scores since at least 2003.   COMPAS provides an illustrative example. xAUC suggests bias, while the APA's approach (applied by Flores et al. in their rejoinder to the ProPublica analysis titled "False Positives, False Negatives, and False Analyses") finds none. Which result should we believe? This paper doesn't make a compelling case for why the well-established approach should be discarded in favor of xAUC.