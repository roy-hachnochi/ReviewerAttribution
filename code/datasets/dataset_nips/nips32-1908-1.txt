This paper introduces a framework to evaluate the perceptual realism of samples from generative models. The framework, HYPE- Human Eye Perceptual Evaluation, is based on psychophysics methods. Two different metrics are proposed. The first one, HYPE_time, measures the amount of time a human needs before distinguishing a real from a fake. The metric is clearly defined and very well founded on psychophysics. The second one, HYPE_infinite, measures the error rate of a human when wrongly classifying fake images and real images (given unlimited time). This second metric is much simpler, faster and cheaper (in terms of human labor) while maintaining the reliability of the first one. The paper is very well written, it is based on psychophysical theory, the methodology is meticulously detailed and the experimental results and conclusions are quite interesting. The work will clearly contribute to the research and development of better generative models, an issue of major importance in machine learning.  I am undoubtedly in favor of acceptance. Nevertheless, in what follows I list a few comments I have: -- The two proposed metrics allow ranking models according to realism, but despite this, their absolute value or even their relative difference does not mean anything. The same amount of numerical change in  HYPEx value does not correspond to the same amount of visually perceived change in realism. This implies that the proposed metrics do not inform how much better a model is, but only produce a ranking. I would like the authors to comment on this. -- The proposed metrics only measure sample realism. But another very important property of generative models is diversity. This is openly stated as a limitation of the method. Notwithstanding, I think it would be good to discuss how a measure of diversity could be incorporated into the framework (either from human evaluation or from automatic measurements of the generated samples). A ranking of generative models should contemplate both realism and diversity at least.  -- Comparison to automatic metrics. In the end, evaluating a single generative model is rather costly so the only way out seems to be to compute automatic measurements. The authors compare to FID, KID, and F1/8 but there is not too much discussion regarding this. It would be interesting to improve this section by trying to draw conclusions a little more interesting beyond saying whether or not the metrics are correlated. Also, the section would look better if a figure with all the FID/HYPE/XX data points was shown not only the correlation coefficients. -- Regarding reproducibility. It would be good if the authors made available all the necessary data to recalculate the metrics shown in the paper. In particular, the evaluations of each human on the generated images that are used. This would allow other researchers to fully reproduce the results, and also facilitate to continue the research in different lines (e.g., can the humans be clustered in terms of realism perception? - do all humans behave more or less the same? Are some better correlated with any of the automatic metrics?) Other minor comments: -- During the evaluation procedure, when presenting images to the experts, in many cases, you mention that half of the images are fake and half are real (e.g., line 135 - 50real / 50fake). Knowing this proportion would bias the expert. Could this be a problem? -- Hyper-realism is hard to understand (the generator produces images that look more real than real ones).  Maybe this is associated with poor quality image datasets (e.g., CIFAR 10) where the images might look a little artificial. Could you add a short comment on this? -- Table 1 (pag 5). There seems to be a typo in the first and second model since the HYPEtime values are out of the respective 95% CIs.  ------ After rebuttal. I appreciate the answers and comments of the authors. I think this is a very interesting work that clearly deserves to be published in this venue.