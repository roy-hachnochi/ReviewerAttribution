In this work, two new benchmarks are introduced for better evaluation of generative models. The first metric is HYPE_time, which computes the minimum amount of time it takes a person to distinguish an image as real or fake. The second metric, HYPE_infinity, measures the errors of people given unlimited time, and is much faster to compute and more cost-effective. The authors test these two metrics on multiple datasets using many models and show that HYPE is reproducible and can be used to separate the efficacy of different models. Further, HYPE was shown to be a more reliable predictor than alternative automated measures. This metric can be used to more consistently compare different generative approaches and provide a foundation for research in this area.  Strengths: - A benchmark for generative models can be very useful for the community to enable more consistent evaluation of new methods. - The authors provided a thorough set of experiments for the two metrics, HYPE_time and HYPE_infinity, across different models and datasets.  Weaknesses: - There was no extended discussion on related work, even though there are other metrics that have been proposed for evaluating generative models. The authors should justify why their metrics are more effective and how they differ from other benchmarks.  Originality: There are a few related works on developing benchmarks for generative models, as included below. It would be great to get a justification for how the proposed metrics are preferred.  Xu, Qiantong, et al. "An empirical study on evaluation metrics of generative adversarial networks." arXiv preprint arXiv:1806.07755 (2018). Wang, Zhengwei, et al. "Neuroscore: A Brain-inspired Evaluation Metric for Generative Adversarial Networks." arXiv preprint arXiv:1905.04243 (2019).  Quality: The quality of the work is high. The measures were based on prior research, which motivates the choice of HYPE. The authors tested a variety of models with many datasets and showed the consistency in the metrics’ prediction of model performance.  Clarity: The paper was well-written. All of the included details about the datasets and models were helpful in understanding the evaluation of HYPE. There were a couple of typos: - Pg 5: the results of HYPE_infinity approximates the those from → the results of HYPE_infinity approximate those from - Pg 7: one for each of object classes → one for each of the object classes  Significance: The addition of a benchmark for evaluating generative models can be very useful to the field, as having standardized metrics allow for more consistent comparison of models. Having reproducible measures are incredibly important for advancing research in this space. ----------------------- I read the author response and am satisfied with the authors' discussion about how the work differs from prior literature.