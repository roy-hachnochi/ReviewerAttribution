Simple case - squared loss and identity covariance.   The key ingredient in the proof is establishing for the problem settings of interest, that the vector \hat{\beta} - \eta lie in a well behaving set T with benign Rademacher complexity.   I am only giving this paper a weak accept because:  (a) - The tools and techniques used in the proofs are pretty standard, and do not contain novel ideas.   (b) - Even though the authors provide some motivating examples and problem settings, I am unable to imagine more general applications of their results in Machine Learning. It seems that for any new problem of interest, we will need to think from scratch to quantifying the set T and controlling its rademacher complexity. Further the estimator in (4) may not be computationally easier. I am quite flexible on this front and am willing to change my opinion based on the author feedback.       (c)  Writing: The paper is not very well written, and I strongly recommend the authors to refine the paper further. Some typos I observed are listed below:  1. Line 47, definition of euclidean norms.  2. Line 74, missing a K in the expression on the LHS.  3. Line 101, the Sigma should be K as per definition (9)