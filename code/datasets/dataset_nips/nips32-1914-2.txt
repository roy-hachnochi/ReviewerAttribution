Summary: -------- The paper introduces a new approach to boosting variational inference. The standard approach is based on optimizing the KL-divergence which can lead to degenerate optimization problems. The paper proposes a boosting approach that is based on the Hellinger distance. The new approach improves the properties of the optimization problem and the experiments suggest that it can lead to better posterior approximation.   Comments: -------  - I appreciate that you discuss the advantages of the Hellinger distance. However, a complete discussion should also point out its disadvantages. Why isn't the Hellinger distance used very often in variational inference? Is it because its less know? Or does the Hellinger distance have computational disadvantages or are there settings where it leads to worse approximations?  - Can an efficient implementation of your approach also be obtained for mixture families other than exponential family mixtures?  - What is the complexity of the algorithm? Especially, how does it scale in high dimensions?  Experiments - More details on the methods should be provided. What is BBVI (I haven't seen a definition). Do you mean boosting variational inference (BVI)? If yes, is BVI using the same family for constructing the mixture?  - The target models/distributions seem to be quite simple (logistic regression + simple target distributions) and are sufficient as toy experiments.  But 1-2 more challenging real-world target models should be included.  - Also, the datasets seem to be too simplistic. If I see it correctly, the highest dimension considered is 10. Does your method also work in much higher dimensions?  - Also the number of data points seem to be quite small. Does your method scale to big datasets?  - Since the methods optimize different distances/divergences, a direct comparison of other performance measures (e.g. predictive performance) would be interesting.   Conclusion: ----------- The paper is well written and the math seems to be sound. The new approach to boosting variational inference based on the Hellinger-distance seems to have some advantages over standard KL-divergence based BVI. My main concern is that the empirical evaluation focuses on too simple settings and it is unclear how significant this work is. 