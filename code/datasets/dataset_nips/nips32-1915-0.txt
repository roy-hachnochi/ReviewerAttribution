This paper empirically shows that SGD learns functions of increasing complexity through experiments on real and synthetic datasets. Specifically, in the initial phase, the function learned can be almost explained by a linear function; in the later phase, the accuracy increases but the correlation between the learned function and the linear function does not decrease (so the linear function is not "forgot"). The measure used in quantifying these correlations is based on mutual information. This observation is extended beyond linear functions, to the case of CNNs with increasing depths, although the phenomenon is less clear there. Finally, the paper provides a theoretical result for a linear model, where overfitting doesn't hurt generalization.  Originality: Measuring the function complexity using mutual information appears to be new in the study of deep learning.  Quality: The experimental result is solid and the phenomenon is evident from the plots. The theoretical result appears to be correct, although I did not check the proof in appendix.  Clarity: The paper is well written and easy to follow.  Significance: Understanding the implicit regularization in training neural networks is an important problem. A key step is how to formulate the notion of implicit regularization, i.e., how to identify regularization as something concrete instead of "promoting generalization". This paper uses mutual information which is able to capture the interesting property of SGD identified in the paper. I think this observation is interesting and could be useful for further research in this topic.  The downside is that the paper is purely empirical (the theoretical result in Sec 5 is only a toy example and not very relevant to other parts of the paper). There isn't any intuitive explanation of why the observed phenomenon might have happened.  ==== update ==== I have read authors' feedback, and am going to keep my evaluation.