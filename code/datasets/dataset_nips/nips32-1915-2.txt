The main claim of the paper is that SGD learns, when training a deep network, a function fully explainable initially by a linear classifier. This, and other observations, are based on a metric that captures how similar are predictions of two models.  The paper on the whole is very clear and well written. Importantly, the topic of the implicit bias induced by SGD and other optimizers is of great interest and significance to the community. However, the first two claims of the paper are not novel based on my understanding of the field. I also have some theoretical issues with the developed metric.   More detailed comments:  1. Novelty of the first two claims of the paper are not clear to me. The third claim seems novel and intruiging.   1a) The first (main) claim does not seem novel. "Understanding training and generalization in deep learning by fourier analysis" and the cited "On the spectral bias of neural networks" try to theoretically and empirically show that SGD training a deep network learns sequentially from the lowest frequencies to the highest frequencies in the input space. More precisely, if one decomposes function of the network using Fourier transformation, then coefficients of the lowest frequency component will be learned first. This to me seems to undermine novelty of the paper because if the claim of these papers is correct, then it implies that initially a network is similar to a linear model.   1b) The claim that network learns function of increasing complexity is not very much formalized. Considering the level to which it is formalized it does not seem novel. See for instance https://arxiv.org/pdf/1610.01644.pdf. This makes outcome of the experiment not surprising.  2. I have two issues with the metric used.   2a) Some novelty is attributed to the proposed metric. But are there any benefit of using the proposed metric compared to a straightforward mutual information between network predictions (F) and linear classifier (L), i.e. I(F, L)? Perhaps it is trivial (sorry then if I missed it), but in any case it should be discussed. It seems to me that if I(F, L) = 0, then the proposed metric is also 0.  2b) While I, roughly speaking, agree with the main claim, I also wonder if the metric actually proves the main claim that initial performance is fully explainable away by a linear model. Consider a hypothetical scenario. Let's say the model initially learns to classify correctly all the examples that *are not* correctly classified by the linear model. Isn't the metric invariant to such a situation, i.e. wouldnt \mu curve look similar? If it is, then we cannot claim based solely on it that model initially learns the same things as a linear model.  Other comments:   3. There is a group of highly related papers on the analysis of deep linear models. Therein it is relatively easy to show that network learns from simplest correlations between input and output to the highest (see for instance "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"), and later work has shown theoretically that this implies that SGD learns increasingly complex functions https://arxiv.org/abs/1904.13262. I think it would be nice to cite these papers. Another group of related papers that might be worth citing shows that DCNNs are highly sensitive to perturbations in the fourier domain of the image, e.g. https://arxiv.org/pdf/1809.04098.pdf. This implies that there is a direction in the input space that models reacts linearly to.   4. I would like to mention again that I found very intriguing the claim that the network is sensitive to things learned in the beginning of training. It might be worth emphasizing in the revised version. There is also a related work I am aware of that has discussed this phenomenon: https://openreview.net/forum?id=HJepJh0qKX.  5. I am not sure the claims are sufficiently formal to put them as claims. This might be personal taste, but I feel that paper clarity would benefit from a cleaner separation of formal and informal arguments.  Update  Thank you for the well written rebuttal. I am very much convinced by your explanations of the metric properties, as well as your contextualization of the linear probe paper. Indeed the metric is better than mutual information. It might be useful to include this discussion in the paper? I would like also to clarify that I was wrong in the review to say that Fourier based analysis of training of deep networks (such as "On the spectral (...)" paper) explains, strictly speaking, your empirical observations.   Having said that, I am still concerned regarding novelty of the empirical observations. Novelty of empirical observations is key, because the theoretical contribution of the submission is arguably limited (as also mentioned by another reviewer). To add evidence that empirical observations are of limited novelty, see Figure 1 from https://arxiv.org/pdf/1807.01251.pdf that shows that low frequencies on MNIST and CIFAR-10 are learned first. This implies that a model close to linear (in some strict sense) exists in the beginning of training. Based on this I would like to keep my score.