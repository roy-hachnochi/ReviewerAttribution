The paper contains many results, which is, unfortunately, counts as a weakness due to its poor organization. For example, there's  an intermission on "bounded range composition", which introduces a strengthening of pure DP.  Epsilon-range-boundedness ends up being a factor-2 approximation of pure-DP, and it is then shown to admit a slightly better (not even by a factor of 2) advanced composition theorem for a special kind of exponential mechanism. By itself, this is a useful observation (which appears already as a remark in the original McSherry & Talwar work) but does it really strengthen the paper?  Overall the paper has several interesting ideas. First, it is the relaxation of the top-k objective. Second, it is the use of the Gumbel distribution whose connection to the exponential mechanism has not be fully explored. Third, it is the pay-what-you-get accounting mechanism. Everything else adds additional and unnecessary layers of complexity.  It would interesting to frame the paper's approach using the language of privacy odometers and filters (https://arxiv.org/abs/1605.08294). The "Odometers" paper seemingly warns against the type of composition that the submission pursues; it'd be worthwhile to identify reasons why the submission avoids the problems presented by odometers.  Another possible connection with previous work is "Scalable Private Learning with PATE" (https://arxiv.org/abs/1802.08908). The key primitive of that paper is top-1 query, and its main insight is that there are significant savings in the privacy budget can be extracted if the querying mechanism does not answer (returns a bottom) when there's no clear favorite.