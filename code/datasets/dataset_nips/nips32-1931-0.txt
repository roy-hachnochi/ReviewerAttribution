Originality: This work builds on recent work on adapting deep networks for use with instrumental variables (DeepIV [Hartford et al 2017] & Adversarial GMM (AGMM) [Lewis & Syrgkanis 2018]) but adapts the optimally weighted GMM [Hansen 1982] (OWGMM) for the task. AGMM is probably most similar in that it is also an adversarial loss, but the variational reformulation presented in this paper results in a far simpler algorithm.  Quality: I thought this was great paper. The variational reformulation of OWGMM leads to a far simpler objective function that neatly leverages the explosion of recent work in adversarial learning (GANs, etc.) by replacing a large number of moment conditions with a single adversarial network.   That said, given that the method appears useful in practice, I would have liked to see more detailed experiments on the practical considerations. For example - I don't follow the GAN literature closely, but my impression is that adversarial losses can be finicky to train - is this a problem in practice or does OAdam lead to reliable convergence (what do typical learning curves look like?)? How well does the validation loss correlate with the loss under an interventional distribution? How sensitive is the procedure to the choice of \tilde{\theta}?   Finally - I found the discussion of the low dimensional experiments very misleading. GMM+NN is described as having "performed well" and outperforming AGMM but it only did so significantly on 1 / 4 experiments (it's beaten on linear and step and abs don't have significant differences). Then the subsequent paragraph attributes DeepIV's performance relative to Poly2SLS the "forbidden regression". But it significantly outperformed all other neural network methods aside from this new approach. The discussion is particularly misleading because the "forbidden regression" argument in [Angrist & Pischke 2008] doesn't consider cross validation. This is a 1-D Gaussian that DeepIV is approximating with a mixture of Gaussians - assuming you're validating correctly there's no reason for misspecification to be an issue. A far more likely explanation is Poly2SLS is properly tuned (the polynomial degrees were selected by cross-validation) while DeepIV wasn't fully trained. I'm not at all surprised that tuned Poly2SLS would perform well on a 1D problem that's well-approximated by a polynomial.   Clarity: I found the paper to be generally very well-written and clear, with only the experimental section missing important details. In particular:   - what were the chosen hyper parameters for the deep networks? Number of layers / units per layer / regularization used / etc?  - how did you choose hyper parameters for your baselines? You mention cross-validation for Poly2SLS - did you do anything similar for AGMM and DeepIV?  - the DGP description looks like it doesn't line up with the figure 2. If X = 0.5 Z + 0.5 e then I'd expect X to have a range \approx [-2, 2], so [-4, 4] seems wrong.  - How is test MSE computed? Is it on the intervention distribution (i.e. is drawn from an unconfounded distribution at test time)? Is it MSE relative to the true response? 