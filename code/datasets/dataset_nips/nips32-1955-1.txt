Originality: EBMs are a relatively less popular class of generative models when compared to other model families such as VAEs or GANs. Although EBMs themselves are not new, the authors proposed the use of Langevin dynamics for training as well as a sample replay buffer for stabilization, which was an original contribution. The authorsâ€™ insights into tips/tricks for stabilizing training was also a nice combination of existing techniques.  Quality: The extensiveness of the empirical results demonstrated the high quality of the paper. The authors showed that their approach outperformed or were comparable to state of the art GAN and autoregressive models (e.g. SNGAN) based on FID/IS, achieved SOTA on adversarial robustness, were able to perform trajectory modeling, did well on an online learning task, and showed experiments with compositional generation.  Clarity: The paper was relatively clear and easy to follow, barring minor typos/grammatical errors (a few of which I have listed in the Improvements section).   Significance: I expect that the contributions of this paper will be very beneficial to the generative modeling community and will open up a new avenue for research in exploring the uses of EBMs in generative modeling, so this paper has high significance.  ------------------------------- UPDATE: I appreciate the authors' responses to my questions, particularly with regards to highlighting the role of the online learning experiments in the paper and tying everything together with a better discussion. I will keep my decision to accept.