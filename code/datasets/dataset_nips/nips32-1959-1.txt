 1. Problem setting: The problem of predicting political affiliation from news media articles is relevant and important. I am not convinced that the assumption of not having text at test time is a necessary one or even a good one. This assumption is not well motivated by the authors and strongly influences and limits the approach. I view this paper as tackling a real world problem (fairly applied) but unfortunately making strong and unnecessary assumptions to solve it which result in both poor performance (Table 1 shows a gap of 9 points because of this assumption) and an unnecessary two stage approach (Figure 2). A real world application should not throw away information or entire modalities without good reason. 2. Dataset: The dataset collected in this work is original and I do not know of a large dataset containing news media articles and affiliations. From the few qualitative examples of images in the paper, the dataset seems to have a lot of visual variety. 3. Approach: I have made my reservations about the problem setting above. I think the assumption of not having text at test time strongly influences the approach. In the first stage, a model is trained using paired images and text. This uses a ResNet to extract image features, and a Doc2vec model to extract text features. The two features undergo late fusion and are then input to a classifier. In the second stage, a linear classifier is trained only on the fixed ResNet features. The first stage training approach seems to be a standard late fusion method. The second stage, according to me, seems unncessary. Also, training a linear classifier on top of fixed ConvNet features is not uncommon.   Questions 1. In L253, the authors say that the JOO method is trained on the closeup of politicians, and thus performs weakest in the 'broader dataset' collected by the authors. In Table 2, however, the JOO method seems to perform the best on "No people". Both these statements don't seem to agree with one another. To add to the confusion, Table 2 also shows that the JOO method's performance on "Closeup" is the methods worst performance (compared to symbols, text etc.). 2. What is the performance of the first stage model? Is it the one denoted by Ours (GT) in Table 1? The results for the Ours (GT) model have not been reported in Table 2. 3. If the assumption of not having text at test time is necessary, the authors should show why their particular style of modeling (two stage) is necessary. How about an approach like DeVISE (Frome et al.)? The ConvNet takes the image as input and has two heads, one to predict the Doc2Vec and the other to predict the political affiliation. Or simply take the top N words in the corpus and then ask the ConvNet to predict those words (along with the political affiliation). 4. What is the "fusion" used in Figure 2? It is never mentioned in the paper. Do you concatenate the features?