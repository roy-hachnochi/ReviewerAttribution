# update after author feedback I have read all other reviews and author feedback. The feedback partly satisfies my request for greater clarity regarding asymptotics notation; on line 178 I now understand the subtlety and would recommend either clarifying or not attempting to push this as an important claim. The new formulation of thm 4.1 makes more sense now. It's good to have the experimental results requested by R1 and R2 as they support the conclusions.  I second R1 in that the name you give to your method sounds like a misnomer to me. It might be worth finding a better name if this is to be published. I don't think I can increase my score still as the next notch is quite demanding (top 15% of accepted contributions), instead I stay firm on my current score.  # original review ## originality The main algorithmic contribution (a new recalibration procedure) is a composition of two existing methods (Platt scaling, histogram binning). The combination is not necessarily original, but happens to bring substantial sample efficiency advantages which justify its importance.  ## quality The analysis is exceptionally thorough. This paper summarises a large body of work.   ## clarity The paper is exceptionally clear. The setup sec2 is careful and clear, also covers the multiclass case accurately, and well illustrated by the simple example in fig1 which is used throughout.  The use of a "mathematical presention" with theorems, lemmas, definitions etc is not merely an artifice, but serves to clarify.  ## significance As argued in the analysis of contributions, I believe this paper is of high practical significance.