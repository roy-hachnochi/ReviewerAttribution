1. The contributions of this paper (summarized above) are original, useful and clearly presented.  2. The new 'variance-reduced' calibration approach is an interesting proposal. It uses a traditional 'scaling' approach to obtain recalibrated forecasts, groups these forecasts in to bins, and then replaces each forecast with the average of the forecasts in the same bin. This new approach is worse than scaling alone in two important ways: the overall performance (as measured by mean squared error) is slightly degraded (appendix D.2), and the new approach is less efficient (as measured by the number of samples needed to achieve a given level of calibration error: lines 174 and 193). The advantage of the new approach is that its calibration error can be estimated more accurately. This advantage is useful if forecasts are chosen to optimize overall performance subject to a limit on the calibration error (appendix G). This criterion reflects the commonly stated goal of probability forecasting to optimize performance subject to calibration but I've not seen the criterion used in practice, except in this paper. Much more common, in my experience, is for forecasts to be chosen simply to optimize overall performance, and this might favour scaling alone over the new approach. The paper would benefit from adding a stronger argument for optimizing performance subject to a calibration budget. For example, it would be useful to discuss at greater length why a calibration budget might be important and how it might be set in practice.  3. The paper criticizes scaling methods because their calibration error is typically under-estimated. The method used to estimate the calibration error in Figure 2 uses a simple 'plug-in' estimator. Another 'debiased' estimator is discussed in section 5. This latter estimator is designed for discrete probability forecasts, but I wonder what results it would give if it were applied to the binned forecasts used to produce Figure 2. If the resulting estimates of calibration error were less sensitive to the number of bins then this might encourage the use of scaling approaches even when a calibration budget is imposed. The paper would be stronger if it investigated this point.  Minor comments  Equation 2: Define M(X)_j.  Equation 3: Replace P(j) with P(k) and replace the second expectation with probability.  Definition 2.4: Define P(j) more precisely. Is it the probability of class j occurring among the labels?  Definition 2.4: The decision to weight the calibration error for each class by P(k) seems like a reasonable choice to me, but I think it is still a choice that deserves a (brief) justification.  Line 122: Replace g(z_i) with z_i?  Line 244: As the mean squared error is better for the same calibration error, this must mean that the sharpness (line 83) is better. Consider adding a (brief) note explaining how the variance-reduced approach achieves that.