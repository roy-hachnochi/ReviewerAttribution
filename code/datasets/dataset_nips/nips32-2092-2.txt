SUMMARY  This paper proposes an action-conditional video prediction approach based on unsupervised keypoint detection. Given an input image and an action category label, the approach takes three steps to generate an output video: It (1) detects keypoints from an input image; (2) generates a sequence of future keypoints of the given action category; and (3) translates each keypoint frame into the target RGB image. Each step uses a dedicated neural network. The authors design a two-stage training strategy: First, the keypoint detector and the translator are trained on pairs of RGB images (in an unsupervised manner). The motion generator is then trained on video samples while fixing the other two networks. The authors demonstrate their approach on three datasets, Penn Action, UvA-NEMO and MGIF, comparing with three recently proposed approaches [12, 17, 19].   ORIGINALITY  The proposed approach eliminates the need for keypoint-labeled datasets through unsupervised learning. This makes it generalizable to a variety of different scenarios where the ground-truth keypoints are not easy to obtain. This is conceptually attractive and will likely to encourage more research in this direction.  The authors make two modifications to [20] for their keypoint detection network: (1) Instead of using keypoints of the target frame only, use keypoints of both input and target images; (2) Instead of predicting the target RGB image directly, predict a foreground image and a mask image, and then blended them with the input image to produce the target image. While the modifications are reasonable, it is unclear whether they actually improves the quality of keypoint detection results compared to [20]. Also, mask-based image generation is not entirely new; several existing approaches already adapt the same idea, e.g., [2] and vid2vid [Wang et al., 2018].   Overall, the proposed approach is a combination of well-known techniques for a novel scenario. The technical novelty seems somewhat incremental.  QUALITY  The design of the proposed approach is reasonable. However, it is unclear why the modifications to [20] were necessary; the authors do not show ablation results to justify their modifications.  The approach is demonstrated only qualitatively using a limited number of examples and through a human evaluation. While the provided examples in the main paper and the supplementary look promising, it would've been far more convincing if the authors have provided more examples of different scenarios, e.g., providing the same input image with different class labels, varying the magnitude of the random vector z to show the network generating different videos, etc. At its current form, the results are not that much convincing.   The UvA-NEMO dataset contains videos of smiling people with subtle differences, e.g., spontaneous vs. posed. Therefore, it is difficult to see if the results actually model the correct action class. Why not use other datasets that provide facial expressions with more dramatic differences across classes? For example, the MUG dataset contains videos from 6 categories of emotion (anger, disgust, fear, happy, sad, surprise).  CLARITY  The paper reads well overall, although there are a few typos and grammatical errors.   I think Figures 9 and 10 are not so much informative. It would've been better if they were moved to the supplementary (with accompanying demo videos) and instead the authors showed results from an ablation study.  SIGNIFICANCE  Keypoint-based video prediction has become popular in the literature. This work contributes to the literature by showing the potential of unsupervised keypoint learning, eliminating the need for keypoint-labeled datasets. I like the direction of this paper and the proposed approach sounds reasonable. However, I have some concerns on insufficient experimental results. 