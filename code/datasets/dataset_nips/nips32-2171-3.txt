In a weight-quantized network, the quantization function is usually non-differentiable. However, many methods need to use full-precision weight for update. Previous methods usually use heuristic methods to transform the gradient w.r.t. the quantized weights to full-precision weight. This paper proposes to learn a meta network to predict this transform. The authors also propose three ways to parameterize the meta network.  The paper is overall easy to follow. My main concerns are in the experimental settings and results.  1. In line 203, the authors said that they report the "best test accuracy". This is not fair.  2.  It is not clear that the proposed MetaQuant works under what kind of conditions. From the experiment results in Appendix A in the supplementary material. The proposed MetaQuant-FC sometimes has very poor performance. On the other hand, the previous STE methods, though might perform worse sometimes, can get relative stable performance for all the reported tasks.  3. In Figure 3 (b), when Adam is used, the STE method has similar or even better convergence behavior as the proposed MetaQuant at the early stage of training. However, it is run for a smaller number of iterations than the proposed MetaQuant. Thus it is hard to draw a comparison between these two kinds of methods. For fair comparison, all the competing methods should be run for the same number of iterations. It is also not clear whether the numbers reported in Tables 1-4 are run with the same number of iterations.  