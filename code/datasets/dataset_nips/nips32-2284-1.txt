The paper is well-written. The introduction section puts well in perspective the previous work. The difficulty current saliency methods have to achieve both local and global attribution is clearly outlined.  The authors propose a simple method that satisfies both global and local properties. It extends gradient x input by viewing the bias as additional input variables that also participate to the gradient x input explanation.  Although the biases do not carry any spatial information (they are constant across the feature map), the gradient of the bias does, which implies that the proposed method can be used to further refine the spatial redistribution.  The method works surprisingly well empirically, and I think it will be a useful reference method for future developments of explanation techniques.  There are however a few minor issues that would be good to address:  Batch norm layers can be merged with the adjacent linear/convolution layers. Could the authors verify that the explanation does not change after performing this merging?  Definition 2 does not seem to be a sufficient characterization for completeness. Just choose Phi(S(x),x) = f(x), and any saliency map is then complete.  In l. 166, there seems to be a "\odot x" missing. The authors should go through the paper to check for potential notation inconsistencies (b vs. x_b, etc.).  Example 1 does not seem to be a continuous function. An example involving a continuous function (like deep ReLU networks) would be more convincing.  It would be good if the authors mention if their method also satisfy other axioms that have been proposed, e.g. continuity, implementation invariance. I assume it does not.  In Figure 1, the bias x gradient at layer 7 seems to be higher resolution than at layer 5. It is counter intuitive as deeper layers typically have less spatial resolution. VGG-16 has many more layer. Why stopping at layer 7?   The displayed explanations in Fig. 1 seem to be in absolute value terms. Can the authors confirm that the aggregating has been done on signed heatmaps (which would make most sense) ?  Experiments are only on VGG-16. Considering at least one other architecture, e.g. ResNet would be useful.