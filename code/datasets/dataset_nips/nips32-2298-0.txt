This paper formalizes the problem of inverse reinforcement learning in which the learner’s goal is not only to imitate the teacher’s demonstration, but also to satisfy her own preferences and constraints. It analyzes the suboptimality of learner-agnostic teaching, where the teacher gives demonstrations without considering the learner’s preferences. It then proposes a learner-aware teaching algorithm, where the teacher selects demonstrations while accounting for the learner’s preferences. It considers different types of learner models with hard or soft preference constraints. It also develops learner-aware teaching methods for both cases where the teacher has full knowledge of the learner’s constraints or does not know it. The experimental results show that learner-aware teaching can achieve significantly better performance than learner-agnostic teaching when the learner needs to take into account her own preferences in a simple grid-world domain.    I think the problem setting considered by the paper is quite interesting and well formalized. I like Figure 1 that clearly shows the problem of learner-agnostic teaching when the learner has preferences and constraints. To the best of my knowledge, the algorithm presented is original. It builds on previous work, but comes up with a new idea for adaptively teaching the learner assuming the teacher does not know the learner model in the RL setting. The paper is generally clear and well-written. It is a kind of notation-heavy. While I think the authors have done a good job at explaining most things, there are still some places that could be improved. Here are my questions and suggestions:  - In equation 1, what is exactly $\delta_r^{hard}$ and $\delta_r^{soft}?$ What is $m$? Is it $d_c$? - In equation 2, what is $\delta_r^{soft, low}$ and $\delta_r^{soft, up}? How do you balance the values of $C_{r}$ and $C_{c}$ when using soft preference constraints? It would be interesting to see how different combinations of these two parameters affect the experimental results.  - In all the experiments, how are the teacher demonstration data generated? When the human teachers provide demonstrations, they can be suboptimal. Will this affect the result significantly? - Why does AWARE-BIL perform worse as the learner’s constraints increase (shown in Table 1)? Any hypothesis about this? Does this mean the algorithm might not be able to generalize to cases where the learner has a lot of preference constraints? - When evaluating the learner-aware teaching algorithm under unknown constraints, only the simplest setting (two preference features) similar to Figure 2(a) is considered. I would be curious to know how well the proposed method can perform in settings with more preference features such as L3-L5 (or maybe provide some analysis about it in the paper). - In Figure 3(b), what are the standard errors? Are the reward differences between different algorithms significant? - While the paper focuses on enabling the teacher to optimize the demonstrations for the learner when the learner preferences are known or unknown to the teacher, it would be interesting to address the problem from the learner’s perspective too (e.g., how to learn more efficiently from the given set of demonstrations when the learner has preferences?).   UPDATE: Thanks for the author's response! It addresses most of my questions well. I have decided to maintain the same score as it is already relatively high and the rebuttal does not address the significance and scalability issues well enough to cause me to increase my score. 