The paper presents an unsupervised domain adaptation approach for semantic segmentation that uses category centroids (anchors) for category-wise feature alignment, trying to keep features from the same category nearby while keeping features from other categories far apart. Pseudo-labeling is used on target samples when sufficiently close to a source anchor (active samples).  Pros: + To my knowledge, this is the first UDA approach for semantic segmentation that combines pseudo-labeling with feature alignment.  Works applying similar ideas exist for classification [3,17,37], but the differences are sufficient and well acknowledged by the authors.  + The presented model is sound and well described. Fig. 1 is helpful to understand the intuition behind the idea (b-c) as well as the actual architecture (a). + Sensible stage-wise training procedure to guarantee good initial anchors, although it makes the training more cumbersome. Also, it seems that it is not saturated in stage 3, would the results improve if trained for more stages? + Convincing results, especially for small classes in which CA-based PLA clearly seems to be crucial in providing good pseudo-labels. Moreover, the authors provide a possible explanation of the limitation of their method with respect to style-transfer for stuff classes. As a suggestion for possible future work, one could think in combining CAG with a style-transfer module to address stuff classes as well.  Cons: - Since pseudo-labeling is done at the pixel level, the pseudo-labels are not necessarily very smooth. The shown pseudo-labels seem relatively smooth (fig. 2 and suppl.), but the method could benefit from enforcing local smoothness to increase the robustness of ATI or PLA. - It would be clearer to add the explicit definition of the loss in L208.  - Missing related work (although for classification): "Unsupervised Domain Adaptation with Similarity Learning", Pinheiro, CVPR2018.  Minor things - Citation missing in L98 for Li et al. - In eq. 4, and if x is defined as the image like in L128, the input of f_D should be something like Enc(x) (i.e. the encoded features) for coherence with Fig.1a. I understand this is a notation abuse for clarity, but this should be mentioned somewhere - A few typos: SYNTHIA in Tab.2's caption, outperforms in L253, etc.   The authors addressed most of the reviewers's concerns in the rebuttal and thus I keep my acceptance score. 