There are some interesting novel concepts introduced in the UDA framework proposed by the authors. Overall the paper is clearly written.  I feel the paper could be improved by demonstrating the effect of using active samples, as these seem to be a prominent elements in the optimisation framework. The ablation results in Table 3 also indicate that +L_CE^tP and L_CE^t are both significant contributors to the final performance. I'd like to see more elaboration on the setting of Delta_d (used both in Eq.5 and 7), which surely decides the number of active samples in Source and Target domains - how some pseudo-label alignment may contribute/impact the final performance? What is the effect of changing the weights for Eq.11.  Regarding the setting of Delta_d, which is set on the distance differences. Is this ideal? Depending on different datasets, the distance-based threshold can be quite changeable and hard to configure. Would it be more reasonable to assign a threshold in a normalised setting? Also the same threshold is used in both Source and Target anchors, which may not be optimal?   Some minor corrections: The context around Eq.5 can be written as - Mathematically, this can be formulated as follows. We first define the distance between ... and the c^th category anchor as  d_ijc^t =... (5) Then, we sort .... in an ascending order, and compare the shortest d_ijc with the second shortest d_ijc... .... we identify this target sample as [an] active one, ...  It is better to also give the formula for L_CE^tP, given its importance.   The comments right under (7) seem a bit loose. You said "they turn out to be more reliable than ...", without providing any evidence or justification. What do you mean by "they do not depend on the decision boundaries"? 