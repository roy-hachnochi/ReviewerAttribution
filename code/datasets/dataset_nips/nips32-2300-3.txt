Strengths --------- In terms of new material,  - one contribution of the paper is to propose a refined definition of a generalized Gauss-Newton matrix. This could open new convergence analyzes but it is not pursued in the present paper.  - numerous experiments are presented to compare the empirical fisher matrix to the true one in terms of preconditioners for optimization on simple models. These experiments support clearly the use of the true Fisher matrix. The code seems very well written and reusable.  The real strength of the paper is pedagogical: - it is very well-written and illustrated (see Fig. 2 or 4 for example) - it has even more value that a lot of previous authors did not make any difference and made numerous false statements.  Weaknesses ---------- 1. Except the new definition of the Generalized Gauss-Newton matrix (that is not pursued), no other proposition in the paper is original.  2. As the authors point themselves, analyzing the EF as a variance adaptation method would have explained its efficiency and strengthened the paper: "This perspective on the empirical Fisher is currently not well studied. Of course, there are obvious difficulties ahead:" Overcoming these difficulties is what a research paper is about, not only discussing them. 3. The main point of the paper relies in paragraph 3.2. This requires clear and sound propositions such as: for a well-specified model, and a consistent estimator, the empirical fisher matrix converges to the Hessian at a rate ... It is claimed to be specified in Appendix C.3 but there seems to be a referencing problem in the paper. This would highlight both the reasoning of previous papers and the difference with the actual approximation made here.   Minor comments: --------------- Typos:  - Eq. 5 no square for gradient of a _ n - Eq. 8 subscript theta should be under p not log - Replace the occurrences of Appendix A to Appendix C   Conclusion: ---------- Overall I think this is an good lecture on natural gradient and its subtleties, yet not a research paper since almost no new results are demonstrated.  Yet, if the choice has to be made between another paper that uses the empirical Fisher and this one that explains it, I'll advocate for this paper. Therefore I tend to marginally accept this paper though I think its place is in lecture notes (in fact Martens long review of natural gradient [New insights and perspectives on the natural gradient method, Martens 2014] should incorporate it, that is where this paper should be from my opinion.)  After discussion -------------------- After the discussion, I increased my score, I don't think that it is a top paper as it does not have new results but it should clearly be accepted as it would be much more helpful than "another state of the art technique for deep learning" with some misleading approximations like ADAM.   Note that though refining the definition of a generalized gauss-newton method seems to be a detail, I think it could have a real potential for further analysis in optimization. 