The authors tackle the general problem of searching for proofs of polynomial inequalities, which includes many computational problems of interest.  They design a ML agent to search for dynamic semi-algebraic proofs; this appears to be the first successful attempt of its kind. The agent is a DQN whose neural network architecture handles polynomial inequalities in a way that takes into account the symmetries of the problem, and is trained in an unsupervised environment. The authors demonstrate its effectiveness on the maximum stable set problem (NP-hard in general), compared to existing static methods, and based on experiments, argue for the efficiency of searching for a dynamic rather than a static proof. A dynamic proof is also more interpretable, and more similar to a human-constructed proof. This is complementary to work on learning strategies for automated theorem proving.  More precisely, the agent works as follows. It contains a memory of polynomials that are known to be nonnegative, and those known to be zero. At each time step $t$, the agent solves a LP to find the current best bound $\gamma_t$ certified using a linear combination of polynomials in the memory. It uses the DQN to select an action (multiply one of the polynomials in memory by $x_i$ or $1-x_i$). The reward is the relative improvement at each step, which requires no supervision. The architecture impose two symmetries: (1) that the Q-function is invariant to the order of polynomials in memory, and (2) that it is invariant to relabeling of variables.   The authors make a large improvement over previous methods. Personally I find this line of work very promising. The paper also has a clear exposition on both the theoretical and engineering aspects, especially on explaining static vs. dynamic proofs.  Questions and suggestions:  + The maximum stable set problem is the only problem considered in the paper. Have the authors tried their method on other problems and found improvements? (The fact that static proofs of level l cannot certify bounds better than n/l seems to make this problem particularly favorable for showing the success of the dynamic method. What about other problems where there is no clear bound like this?) + How essential is it that the agent uses a neural network? How much of the improvement is due to using the reinforcement learning framework (perhaps with a simple classifier), vs. due to specifically using a neural network? + I would very much like the source code to be made available.  ---  Reply: Thanks for the authors' response. It is good to hear of promising results for other combinatorial problems despite the differences in distribution. 