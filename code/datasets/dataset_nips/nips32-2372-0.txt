UPDATE: I have increased the score to 6 as long as the authors will revise the paper as promised in the responses. ===   This paper has more than one topic being discussed. It at the first part talks mostly about the attention mechanism, and in the second section it introduces a new model ChebyGIN, then in the third section it proposed a weakly-supervised attention training approach. Overall, the paper is not all about its title "Understanding Attention in Graph Neural Networks".  In 2.3 the paper says "the performance of both GCNs and GINs is quite poor and, consequently, it is also hard for the attention subnetwork to learn", thus it proposes ChebyGIN as a stronger model. In the experiments we only see a few results from non-ChebyGIN models. It raise the concern that are most of the statements and observations on attention only work with stronger models? Or, is ChebyGIN the only strong model that works well with attention? To summarize, it is concerning to use a limited number of models to understand attention.  In the section "Why is the variance of some results so high?", the paper raises an interesting issue of attention, which is poorly initialized attention cannot be recovered. It is an important issue as an initialization-sensitive model training is what people would like to avoid. However, there's no further discussion or attempt in solving it. In other deep learning models, for instance CNN, initialization has been studied in several literatures. Different random number distribution may have substantial impact on the initialization.  The proposed weakly-supervised attention supervision borrows the attention coefficients from Eq.6, which uses a model's prediction y_i in the computation. The paper is not clear on, or we may have overlooked, how is the model predicting y_i trained? Is it suggesting we first train a model in an unsupervised manner, and use this first version to computer the attention coefficients, then again train the model with the coefficient in the weakly-supervised manner?  In Table 2 the variance of the results are still high enough to make the improvement insignificant, especially when comparing unsupervised v.s. weakly-supervised. It is concerning to consider if the proposed weakly-supervised training approach is meaningful without a good initialization.