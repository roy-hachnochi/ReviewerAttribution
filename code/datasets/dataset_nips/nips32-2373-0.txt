[Update] I read the authors' reply and the other reviews.   Reviewer 2 raises some important points and I agree with some of the concerns about the clarity of the paper and the contrast to existing other work.   The authors' reply contains some additional experiments, which is great. And I appreciate the clarification of my misunderstanding of the storage of parameters, not gradients.   [Update]  I enjoyed reading the paper, the authors do a great job at motivating their approach and describing the derivation. Data cleansing is an important step in machine learning systems and principled approaches for automating this step is a valuable contribution. The setting considered by the authors covers a substantial fraction of ML algorithms used today. Hence the proposed method has the potential to improve data cleansing for most models being used in practice. Overall the manuscript is written in a clear and concise manner. Also the experimental section is solid for the most part.  I had a few minor comments and some concerns about the experimental settings. The supplementary material is very helpful, in particular it is impressive that the authors provide two implementations, in tensorflow and pytorch.   Originality The method appears to be novel and the approach is well embedded into the related work. What wasn’t really clear to me is how the proposed approach is an improvement over just rerunning the SGD. In the proposed algorithm 1 and 2, it seems the approach requires to store the gradients of every step in the SGD optimisation? This is a lot of data that needs to be stored, more than the actual data set, right (or are the gradients only stored for one epoch)? If that’s the case then running the naive version of leaving one data point gradient out amounts to a mere sum over the gradients, which is a trivial and efficient reduce operation.   Quality I found the quality of the paper to be higher than many papers I’ve reviewed for this venue. The paper is well written and motivated, the supplementary material is extensive and the experimental section demonstrates clearly the advantage of the method compared to other baselines for data cleansing.  There was one thing that stood out in the comparisons. In Section 7.1 the authors compare agains a method that requires convex losses, but the models used in the comparisons have non-convex losses. In order to render the baseline method usable for Hessians with negative eigenvalues, the authors simply add a positive constant of 0.1 to the Hessian. Even though the authors of the baseline method suggest this, they probably suggested this as a remedy for numerical instability under the assumption that the Hessian should actually not have negative eigenvalues - and not as a way to render their method usable for non-convex losses. So I’m wondering how fair a comparison this is. Overall, it seems that in these experiments the proposed method really only shines in the case of the MNIST data, in the other cases the difference is not too large. If one could exclude that this MNIST case was due to a wrongly calibrated offset for the Hessian, this would render the comparisons much stronger, no?  Clarity  Overall the paper is very clear and concise. There are just some minor things I feel that could be explained better.   Line 96: The role and importance of the query vector is not entirely clear to me - is that needed for not having to compute the Hessian explicitly or is it just a common approach?   Line 163: I’m not sure this is obvious, but to me it wasn’t clear, how the Hessian times u is equivalent to the derivative of the inner product of the query vector and the gradient of the loss. This appears to be one of the central contributions and could deserve some more attention in the manuscript?   Significance The problem of data cleansing and its automation is highly relevant and this manuscript appears to be a valuable contribution in that space. The authors provide solid experimental validation of the proposed method in a data cleansing application. 