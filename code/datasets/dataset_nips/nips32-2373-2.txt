****Update**** Thanks to the authors for the clarifications. After reading the other reviews, I still stand by my score. I believe the method and the insights are potentially useful to the community.  ===========================================================   The paper deals with an important problem of estimating influence estimation. Tackling the problem has implications across modern day machine learning community. The proposed influence estimator leverages the recursive SGD formulation to approximate the proposed influence function.     The proposed method is interesting and potentially impactful. The problem is nicely setup and writing is clear. The experimental evaluation is sufficient to show the promise of the proposed method        The error bound of \delta \theta_{-j} for non-convex objectives obviously becomes loose as T increases. The final influence estimates for the DNN experiments are understandably bad, compared to logreg. However, it would be interesting to know how good/bad was the influence approximation as the training proceeded. Could you please comment on this?    