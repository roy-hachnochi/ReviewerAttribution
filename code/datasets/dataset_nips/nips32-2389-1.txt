Summary The authors propose a new method for Bayesian deep learning. The idea is to learn a hierarchy of generative models for the data (using [1]), that can then be converted into discriminative models (using [2]). The authors then assign a posterior value to each of the subnetworks, and sample subnetworks to perform Bayesian model averaging. The authors then conduct a series of experiments on different types of uncertainty estimation with strong results.   I have limited familiarity with probabilistic graphical models, and I am unable to provide feedback on the quality of the methodological contribution in the paper. For this reason, in my review I focus on the empirical evaluation and clarity.  Originality The paper provides a very non-standard approach to Bayesian inference for deep neural networks. The method is very original and interesting. Related work on Bayesian deep learning is cited appropriately.  Clarity I believe some parts of the paper could be clarified: 1. It was unclear to me how exactly is the method applied in the experiments. From Section 3 my understanding was that the method learns a hierarchy of Bayesian neural networks from scratch. How is it combined with given neural network architectures, e.g. in Table 1? Is it applied to learn the last several layers of the neural network? 2. Is it correct that the proposed method proceeds as follows: First, it produces a hierarchy of architectures for some subset of a given neural network architecture (see question 1). Then, it trains the weights in this architecture by sampling sub-networks uniformly and updating the weights by SGD. Then the method performs Bayesian model averaging with one of the two strategies described in the paper. I would recommend to include a high-level outline like this in the camera-ready version. 3. In section 3.2.2, what is c? Is it the score produced by B-RAI for the sub-network? If so, wouldn’t it make more sense to re-weight sub-networks based on the corresponding training loss value in the posterior?  Quality As I mentioned above, I will mostly focus on the empirical evaluation and results. The reported results are generally very promising.  1. Section 4.2: the authors compare the predictive accuracy and likelihoods for the proposed method against deep ensembles, Bayes-by-backprop and Deep Ensembles on MNIST. How exactly were the Deep Ensembles trained here, keeping the model size fixed? 2. While the accuracy and NLL results appear strong, it would also be interesting to see calibration diagrams and ECE reported, as in [3] and [4]. 3. In Section 4.3 the authors apply the proposed method to out-of-distribution (OOD) data detection. The authors achieve performance stronger than other baselines on identifying SVHN examples with a network trained on CIFAR-10. For the dropout baseline, have you tried to apply it before every layer rather than just the last 2 layers? 4. The results in Table D-3 show expected calibration error for BRAINet on a range of modern architectures on image classification problems. Could you please report accuracies or negative likelihoods (or both) at least for a subset of those experiments? While ECE is a good metric for the quality of uncertainty, it is possible to achieve good ECE with poor predictive accuracy, so a combination of the two metrics would give a more complete picture of the results.  Minor issues: Line 56: add whitespace before “In”. Lines 93-94: this sentence is not clear to me. Line 212: “outperforms” -> “outperform”.  Significance The paper proposes a new ingenious approach to Bayesian deep learning. The reported results seem promising. Importantly, the method is applicable to modern large-scale image classification networks. However, the clarity of the paper could be improved.   [1] Bayesian Structure Learning by Recursive Bootstrap; Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Guy Koren, Gal Novik [2] Constructing Deep Neural Networks by Bayesian Network Structure Learning; Raanan Y. Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, Gal Novik [3] A Simple Baseline for Bayesian Uncertainty in Deep Learning; Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, Andrew Gordon Wilson [4] On Calibration of Modern Neural Networks; Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger