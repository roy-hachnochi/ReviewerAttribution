[Update after author feedback: I have read the other reviews and the author feedback and want to thank the authors, particularly for answering my clarifying questions in detail. As the other reviewers pointed out as well, the paper can still be improved in terms of clarity, particularly when addressing the (deep) neural network community as a main target audience. I want to encourage the authors to try and improve clarity as much as possible. To me personally, the paper is promising and interesting, but I find it hard to judge how novel the proposed algorithm is (hence my confidence of 2). I am personally leaning towards acceptance, though I understand the criticism and (as the score of 7 implies) "would not be upset if the paper is rejected".]  The paper proposes to learn the structure (connectivity structure and parameter sharing) of a stochastic ensemble of neural networks. This allows for solutions ranging anywhere from large connectivity variation with full parameter sharing (MC Dropout) to solutions with no connectivity variation and no parameter sharing (“classical” ensemble of networks). To achieve this, the structure of an ensemble of networks is expressed as a prior over parameters in a Bayesian neural network (a common interpretation of both MC Dropout and deep ensembles). Crucially, the paper proposes to learn the structure, i.e. the prior, from the input data (e.g. images in image classification) in a self-supervised fashion. The claim is that such a prior (now actually a conditional distribution over parameters given the input data), if tuned to the generative process of the input data, reduces unwarranted generalization for out-of-distribution data of the “prior” (the conditional distribution) over the structure. To learn a prior that is tuned to the generative process of the data, the paper applies a recently proposed Bayesian structure learning algorithm (B-RAI) that induces a distribution over discriminative network structures that are proved to mimic the generative structure (line 115 to 117), thus yielding a dependency between the structure of an implicit generative model of the data and the structure of the discriminative model (line 127 to 128). The paper explains and illustrates the algorithm (both training and inference) and shows how to use it for estimating predictive uncertainties. Finally the paper evaluates the performance of the proposed algorithm by investigating calibration of uncertainty estimates (on MNIST, as well as CIFAR-10/-100 in the appendix) and out-of-distribution detection on (SVHN and CIFAR-10). The method performs well compared to MC Dropout and Deep Ensembles, and some other state-of-the-art methods for calibration (in the appendix). Combining the method with MC Dropout or Deep Ensembles yields even better performance.  I should point out that I found the paper somewhat hard to follow and I might have missed something important or misunderstood small or large aspects of the paper. Therefore I want to encourage the authors to comment/correct my summary of the paper if necessary. I should also point out that I had reviewed this paper at a previous venue and was happy to see that many of the previous reviewer’s comments were taken into account, particularly w.r.t. to improving clarity and improving the empirical comparisons. Before stating my personal opinion, I would like to ask the authors a series of clarifying questions: 1) The paper claims that the method prevents unwarranted generalization on out-of-distribution data - under what conditions does that claim hold? Does the claim essentially rest on p(\phi|\theta) being “well behaved” for out-of-distribution data? 1a) If yes to the latter, does that mean that for out-of-distribution data p(\phi|\theta) needs to spread probability mass across \phi, i.e. different structures - which corresponds to having high uncertainty over the structure for out-of-distribution data?  1b) If yes to 1a), what are the arguments for why this is likely to hold in practice and can you show some empirical results that support these arguments? 2) Would it be fair to say that preventing unwarranted generalization for out-of-distribution data could in principle also be achieved with a large ensemble of networks of different (connectivity) structure, i.e. no parameter sharing (to prevent unwarranted generalization due to learned parameters) and a large variety of structure (to prevent unwarranted generalization due to a fixed connectivity structure)? 2a) If yes to 2), would the advantage of the proposed approach be that the method captures an (infinite) ensemble of networks of different structure more efficiently (in computational terms) in the form of a Bayesian neural network?  Assuming that my current understanding of the paper is mostly correct (i.e. my summary is mostly correct and most of the questions above are answered positively), I am slightly in favor of accepting the paper, since it addresses two timely issues (well calibrated predictive uncertainties, and a formulation for Bayesian neural networks where the connectivity structure is not fixed but drawn from a learned prior) in a novel and original way. I think that the current experimental section is solid and convincing enough for publication, though it could always be improved of course. To me the paper has reached a point where it would be interesting to see independent replications and discussion as well as extensions of the findings by the wider community. I want to encourage the authors to try and further improve the clarity of the manuscript and presentation of the method - with a particular focus on (Bayesian) deep learning practitioners (see some suggestions for improvement below). Since there is still quite some uncertainty in my current understanding of the paper, I also want to point out that it is quite possible that I’ll change my final verdict quite drastically based on the author feedback and other reveiwer’s comments. Originality: BRAINet structure learning algorithm: medium to low (I find it hard to judge whether the algorithm follows trivially from previous work, or requires major innovation), improving calibration and out-of-distribution detection with the method: medium to high  Quality: Medium - there is no thorough discussion of related work, the paper does not mention (current) limitations and shortcomings of the approach, but there is a large number of empirical evaluations with interesting comparisons against state-of-the-art approaches  Clarity: Low - though clarity has certainly improved since the last version of the manuscript, I still had a hard time to distill the main idea, and the experimental details w.r.t. how exactly BRAINet is applied and trained still remain a bit muddy (without looking at the code).  Significance: Medium - the approach is novel and original and the experiments look promising. To be fully convinced, I’d like to see a thorough discussion of the shortcomings, how well the approach scales potentially and results on larger-scale tasks (I do acknowledge that not all of this is necessarily within the scope of a single paper)    Minor comments:  Tuning the prior to the generative process of the data and then using it as a prior for the discriminative model sounds like a mild version of an Empirical Bayesian method, which are known to break some appealing properties of proper Bayesian methods at least in theory. While I am personally happy with exploring Empirical Bayesian methods (in fact many modern Bayesian neural network algorithms, for instance some Bayesian neural network compression methods based on sparse Bayesian learning, learn the prior and thus also fall into the category of Empirical Bayesian methods), I would not necessarily over-emphasize the Bayesian nature of the proposed approach.  It could potentially be very interesting compare and connect/apply BRAINet to Graph Neural Networks. While a full treatment of this is beyond the scope of this paper, at least some discussion about similarities and differences in the approaches and whether they can easily be combined would be interesting.