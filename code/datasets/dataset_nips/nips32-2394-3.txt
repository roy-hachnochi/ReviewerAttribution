Here is an update of my review.  I am keeping my score but have a concern.  <> Some of your more recent experiments show that models without an RNN can perform well in some instances.  But from my reading, the RNN was an important part of the message of your paper.  You are finding a balance between independent generation (graph VAE) and introducing structural dependence with an RNN.  Please be careful to make sure the message of the entire paper is clear, consistent, and as unified as possible.  Perhaps you can offer some guidance to the reader when an RNN will be useful, and why it may not have been needed in some of your tasks.  I know you mentioned SMILES.  <> Optimization of the GNN.  My question was not about the type and tuning of the optimizer (Adam, learning rate, etc).  Sorry for not making it clear.  My suggestion was to clearly indicate what the final and overall loss function is that you use to train all the weights.  You have a GNN that defines a Bernoulli distribution over edges and another that learns a variational posterior over orderings, as well as an RNN.  Overall, all these components build up a likelihood function that serves as the loss that can be used to _optimize_ the weights.  In my humble opinion the final objective function could be more salient, especially for purposes of implementation in the reader's favorite framework.  For instance, an Algorithm environment with a line that refers to the equations that altogether define the loss.  However I know space can constrain that suggestion.     ====================== This paper focuses on the approach of using an auto-regressive scheme for graph generative models which better captures dependencies among vertices in the graph. The authors note that many existing works leverage domain-specific knowledge rather than learning from the data and scale poorly. A central baseline that addresses these issues is GraphRNN which still suffers from O(N^2) generation steps (N is the number of vertices), may lose long-range dependency between distant vertices in the graph, and only presents one scheme for ordering vertices.  The authors propose generating a graph by focusing on B rows in the lower-triangle of the adjacency matrix at a time (a "block" of size B). To generate a new block of the graph, rows of the already-generated portion of the lower triangle are passed to a GRU. The representations from the GRU are passed to a graph neural network with attention, and the resulting vertex representations can be used to calculate a probability of edge formation (via an MLP). To learn an ordering, the authors take a variational approach with a categorical distribution over orderings as the variational posterior. Notably, this generative scheme, dubbed GRAN, can be seen to address each of the concerns discussed previously. The experiments compare summary statistics of generated graphs to actual graphs in real and simulated data against the baselines of GraphVAE and GraphRNN, demonstrate a speed-quality tradeoff controlled by B, and perform an ablation study. * Pros* - The characterization of existing work and discussion of limitations are clear.   - The stated goals are clear and the scheme developed is clearly motivated by these goals. - This is an important line of work and I am not aware of work that takes the proposed approach. - The authors propose an interesting approach for learning long-range dependencies and mixing over canonical orderings. - The experiments demonstrate that GRAN makes graph generation via an autoregressive approach more computationally efficient if a user is willing to compromise on quality.  *Cons* Issues with the experimental design preclude me from rating this paper above a 6.  A few details in the description of the algorithm/training are also lacking.  In particular: - It appears that one train-test split was chosen to be consistent with a previous experiment. It has been discussed that this practice should be avoided, e.g. “Pitfalls of Graph Neural Network Evaluation” - The experiments do not report confidence intervals or any quantification of the variability in the results (related to above).     - The choice/optimization of weights for the GNN that produces edge probabilities needs to be more clearly discussed.   - If possible, experiments could address whether the proposed model better learns long-range dependencies. - It could be made clearer how the size of the generated graph is determined.