Thanks for the rebuttal and the extra results on two more atari games. I agree that this should be considered a theory paper and I'm keeping my score as is as I still think it's appropriate. ******************************************** The main idea of the paper is to construct a new TD update rule for the posterior over state-action values based on the notion of Wasserstein barycenter. This is an original idea which requires some time to digest. Based on this notion of TD learning, the authors derive a Q-learning algorithm which can take advantage of the estimated posterior for guiding exploration. They prove the efficiency of this algorithm in a specific tabular PAC-MDP setting, and they also attempt to scale the algorithm to situations where function approximation is necessary. The paper is very clearly written.  Further comments: 1. The explanation of PDQN is not very detailed. There should be a separate algorithm box for PDQN. For example, the update rules considered in the paper are "derived" from gradient descent update rules and are directly tied to the notion of Wasserstein barycenter. DQN uses RMSProp update rule and so the connection to the theoretical results and definitions is unclear. 2. Why not show results for other Atari games? Asterix doesn't seem like the natural first choice for evaluating an algorithm. It seems that there are some extra hyperparameters e.g. related to the choice of the prior. The extra flexibility that stems from tuning these parameters could improve performance due to reasons unrelated to having an access to a good posterior.