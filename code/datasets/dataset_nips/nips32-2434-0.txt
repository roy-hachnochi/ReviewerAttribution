This paper proposes a method for enhancing the latent space learned by auto-encoders, so that the learned latent space produces meaningful features useful for downstream tasks. The proposed approach considers interpolations in the latent space and encourages the reconstructions from these interpolations to be similar to the data using adversarial learning. The learned latent space is shown to capture useful feature via experiments on MNIST, KMNIST and SVHN.    The paper presents some promising preliminary experiments. However, there are many issues in the experimental setup Why is the quality of features measured during training ? It is more sensible to learn the features, fix them, and then train the classifier on top of the features. Measuring the classifier accuracy during the training introduces some confounding factors, which makes me question the validity of the results.  Why is one method not consistently better than the other ? For instance, in Table 1 mixup(3) is best on MNIST and mixup(2) is best on KMNIST and SVHN.  It is unclear to me whether the baselines considered here are comprehensive, significant or strong enough. I appreciate the authors presenting some discussion about this in 211-213.   I don’t understand the point of showing the reconstructions from the interpolation (Eg: section 4.3). Many prior work have demonstrated controlling attributes in image generation.   Going beyond MNIST, SVHN and evaluating the approach on more complex/real-world datasets would make the paper more compelling.   The paper serves as a preliminary exploration on some interesting ideas. However, the experiments need to be performed on real-world datasets against strong baselines with systematic evaluation to demonstrate the benefits of the approach.  Questions Is the label based interpolation (section 2.2) used in the quantitative experiments ?  Eq (7): Parts of the AC-GAN loss seem to be missing.   Other remarks 64: The term ARAE has been used in Zhao et al., Adversarially Regularized Autoencoders. I would suggest using a different term.  Make figures 3 and 4 bigger Eq (7) x1, y2 should be x1, y1 ? 151 - 160: I don’t know if the biological motivation needs to be emphasized 