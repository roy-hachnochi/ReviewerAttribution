This paper proposed a new regularization method to learn better representations through auto-encoders. Intuitively, the proposed method encourages mix in the latent representations so that a combination of latent representations is able to produce realistic images (achieved through an adversarial loss). The high-level idea and formulation is similar to [1],  but the proposed method differs in that it is not limited by the specific form of mixing functions, while [1] seems requiring a linear combination of two examples. As also mentioned in the paper, this is a difference between JSGAN and least squares GAN, and the proposed method is able to be freely combined with various mixing functions. In addition to the simplest mixup between two latent representations, this paper proposed and studied two additional mixing functions:  (1). an element-wise mask and sum (which is like selection from features maps), and (2). Mixing with K examples (while the experiment only studies K=3). I think the investigation of different mixing functions is original, interesting, and important.  The method is mainly evaluated from downstream classification task and qualitative interpolation examples (in the supplementary material). The proposed method outperforms the baselines when the bottleneck dimensionality is 32. Particularly in low data regimes mixing with more than 2 examples plays an important role. However, I think there are several weaknesses of the paper: (1) methodologically, the main difference and advantage of the proposed method over ACAI lie in the flexibility of mixing functions, but in the full training data setting (Table 1) it seems the simplest mixup performs the best. This suggests that it may not be necessary to use different mixing functions. While the proposed method still outperforms ACAI in this case, the gain is more from a different GAN variant under the ACAI framework than a different mixing function, which is less novel and interesting. (2) When the bottleneck dimension is 256, it looks ACAI mixup(3) greatly underperforms ACAI numbers from the original paper. Thus I doubt that the authorsâ€™ reimplementation of ACAI is more or less problematic, as mentioned in line 211. Why not include numbers from ACAI mixup(2) from your implementation ? That should be a more fair comparison to the numbers from the original paper to judge whether your re-implementation is correct or not. (3) Given that AMR is much worse than ACAI when dimension is 256 on SVHN dataset, I suspect it might be the same case for other relatively complex datasets. Does this mean the proposed method is inferior when working with high-complexity dataset ? (4) I read all the qualitative examples in the supplementary material, and I cannot tell whether the proposed AMR is better than ACAI or not.   [1] Berthelot et al. Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer. ICLR 2019  After rebuttal: Authors responded to my points 1-3. Regarding 1, the added CIFAR10 experiments and SVHN with mixup(4) mitigate my concern about the usefulness of varying mixup functions. Regarding 2, it was a miscommunication caused by a typo in the submission and it is now clear. Regarding 3, it seems that AMR only outperforms the authors' re-implementation of ACAI while underperforming the quoted ACAI results on complex settings (e.g. SVHN (256) and CIFAR10 (1024)). I agree that it has less confounders under the same codebase for comparison but I think it is still important to figure out why your implementation fails to reproduce the results. There might be important (but seemingly ignorable) details to the implementation. Overall, I think the authors addressed part of my concerns and I will increase my score accordingly.