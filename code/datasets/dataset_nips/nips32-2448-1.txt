Originality: The ideas introduced here are certainly not new, but extending intrinsic rewards to multi-agent RL settings is for sure an interesting research avenue, especially when one is interested in decentralized multi-agent settings were no communication is possible between agents.   Quality: The paper is well written.  Clarity: It is a fairly convoluted method, with many components. A better overview of the algorithm could be useful (perhaps in supplementary materials). Furthermore, not all the details regarding the experimental setup and parameter choice is specified. This information is important for reproducibility reasons, and could also be included in supplementary materials. A few examples include the beta learning rate, was there any parameter search performed for lambda, more in-depth view of the networks' architectures.  Significance:  The method is compared against state-of-the-art methods and does show improvements in the selected scenarios. The authors also perform a short analysis of what intrinsic reward the agents learn and how it affect their behaviour.   ------------------------------------ Post-rebuttal:  I appreciate the authors efforts to answer the raised concerns and I think the additional experiments, analysis and explanations will improve the work. I will maintain my score, given the novelty level of the work.