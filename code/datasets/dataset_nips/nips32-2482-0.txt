The problem of estimating Sinkhorn distance has been well studied. Given two probability distributions p and q, both on the same metric space, the Sinkhorn distance is a regularized notion of how measuring how different the two probability distributions are. In general, calculating the Sinkhorn distance needs O(n^2) time, where n = support of p or q. As the authors discuss, there are efficient algorithms that utilize the structure of the metric space (e.g. it being low-dimensional) and try to make this more efficient.  In this paper, the novel contribution is the following: the authors utilize the connection between Sinkhorn distance and Sinkhorn scaling to give an efficient algorithm. They then use Nystrom method to do Sinkhorn scaling. Again, while Nystrom methods have been used in this setting before, this presents a neat bound in terms of what they call "effective dimension". They are able to prove that their algorithm leads to a guaranteed bound on the Sinkhorn distance. They also perform experiments to demonstrate the efficiency. The experiments seems quite exhaustive in terms of the baseline algorithms compared against.  My overall impression is the following: while a number of blocks that are used in the algorithm are existing in literature, putting them together and proving the theoretical bounds requires substantial insight. The authors also extend their bounds to special cases when the data lie in low-dimensional manifolds.  The paper seems well written too.  I was not entirely sure why we would expect R to appear in the run-time? Any intuition , e.g. whether it is inevitable or is a effect of this analysis, would be useful. 