The dynamically changing alliances mean that the domain of diplomacy presents unique challenges for agents. I agree with the authors that this means that diplomacy is ‘deserving of special attention’, I would consider the full game to be a grand challenge for multi-agent research. With recent progress in large-scale RL focusing on single-agent and 2-player zero sum games, this problem is particularly timely.  This work presents state of the art agents trained with deep learning. To my knowledge this is the first successful application of deep learning to diplomacy. The design of the neural network is given particular attention: Several of the design choices are studied by ablation in the supervised task.  The dataset of games is large, I’m not sure how many online games of diplomacy have been played, but 150,000 seems likely to be a significant proportion of them. Not all the games are No-Press, so there is a domain mis-match for much of the dataset. Maybe performance could be improved by including press rules as a feature for the network, or training more on the no-press games.  I would like to see more details on how the dataset was selected. Sometimes in online games players fail to enter orders, or cheat by creating multiple accounts. Have any preprocessing steps been taken to remove data where these things have occurred?  To evaluate the performance of agents, the SL DipNet played games against 4 rule-based baselines, including previous SOTA, plus the RL DipNet, under the conditions of 1 SL Dipnet vs 6 of the opponent, and 1 of the opponent vs 6 SL DipNet. Trueskill is then calculated based on these games. These 1 vs 6 matches are interesting to see, and clearly demonstrate that the DipNet programs outperform the other agents.  However, we _cannot_ draw conclusions about the relative strength of the RL and SL agents from this tournament. The RL agent has had access to the SL agent to train against by the warm-start procedure, and the experiment never compares the RL DipNet against the baselines. I’d like the Trueskill to be generated by some more varied structure of games: A simple procedure that would be interesting is where each game, the player to play each power is sampled iid uniformly from the 6 different agents.   Then we could understand how the agents perform against varied and heterogeneous opponents. This would test an important question in diplomacy, of whether self-play RL in 3+ agent problems overfits to its own play-style.  The coalition analysis is nice, and I am particularly interested in the analysis around the effect of RL training on effective cross power support.   One point to note is that cross power support in human no-press is likely to be reduced by virtue of signalling actions, where a move that is guaranteed to succeed is nonetheless supported by another player to signal that they want to ally, and help with coordination. I wonder if a metric for this behaviour can be designed, whether the SL network is able to pick up on such signalling moves, and whether RL changes this ability?   Minor Comments:  Line 28: Players can have a maximum of 17 units, because if they have 18+ units, they have already won the game. How was the average of 26 possible actions per unit calculated? Is that the empirical average in a dataset after removing any support/convoy orders that use non-existent units?  Line 135: Did this mean to refer to [36] Learning with Opponent Learning Awareness, rather than the DIAL paper?  Line 175: “there is no weight sharing”: Does this mean that there is no sharing of weights for the convolutions at different nodes of the graph, or just none between different layers? Would be good to clarify.    ======= Comments following Author Response:  Thanks to the authors for their clarification on how Trueskill was calculated. This makes it much easier to understand what the scores mean.  There's some additional information I'd like to see included in the final paper if possible: - Table of results - Gameplay data (i.e. example games) - Report the standard deviations of Trueskill estimates