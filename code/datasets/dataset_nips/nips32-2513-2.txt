Comments after rebuttal and discussion ======================================  Thank you for your rebuttal.  It appears you have foreseen many of the concerns and there simply isn't a great solution for many of them.  It may be helpful to note in the body of the paper that the particular evaluation suffices here because the skill gap is so large in the agent pool. In the future as the pool of agents gets stronger a different evaluation may be necessary.  Original Review ===============  This paper is difficult to evaluate as a whole.  I lean towards acceptance, but primarily because this paper may serve as a catalyst for future work.  DipNet and its evaluation are not as thorough as one would hope for.  The game of Diplomacy is well described and motivated as a challenging multi-agent AI domain.  It is clear that it possess interesting aspects that are not represented in popular games in the literature---specifically the competitive/cooperative aspect as well as its communication structure.  This paper is not the first to consider diplomacy as the authors point out, but past work, like Shapiro et al, was perhaps too early to spur academic interest.  DipNet itself is non-trivial, but it is not incredibly insightful on its own. e.g., the state representation is domain-specific and the LSTM action decoding is heuristic.  Bootstrapping training from human data and tuning with self-play is common practice.  i.e., there are many seemingly important details, none of which are novel.  The quality of this paper's contribution is mixed.  On one hand, the motivation and exposition of Diplomacy as a test domain are excellent.  The development and evaluation of DipNet is lacking, though.  As mentioned above, the DipNet agent is fairly complicated, i.e., numerous non-trivial decisions were involved in its creation, many of which are parametrized.  Typically, these decisions are not thoroughly justified and it is not clear how reliant the agent's performance is on these decisions.  e.g., the state representation is lossy.  The authors state "Based on our empirical findings, orders from the last movement phase are enough to infer the current relationship between the powers", but no evidence or procedure to reproduce this claim is provided.  Similarly, using the no-check version of the game is claimed to be important to enable communication.  This seems intuitive, but no evidence is provided to show this is indeed true or to validate the magnitude of this.  Other decisions, such as the network structure, LSTM action decoding order, training procedure and parameters, reward and reward shaping, require further details and justification to aid in reproducibility.  Perhaps the most important criticism of the paper is its evaluation of DipNet. The 1 vs. 6 head-to-head play demonstrates that DipNet is much stronger than the baselines, but the same approach is likely to fail when evaluating agents that are closer in strength.  In multi-agent scenarios, is often the case that an agent trained with self-play will learn to implicitly collude with itself. e.g., if an agent indeed finds a Nash equilibrium then the solo agent will be at a tremendous disadvantage.  This can even occur with different agents that trained in a similar fashion, e.g., playing 3 copies of agent A vs. 2 copies of agent B vs. 2 copies of agent C may not be informative if A and B are similar. i.e., using overall utility or games won as a performance indicator is incredibly sensitive to the pool of agents, especially in multi-agent scenarios.  A thorough and thoughtful evaluation is of particular importance here as it sets a precedent for future papers.  Again, I appreciate that the authors are willing to release their source code as it enables others to perform a different evaluation in the future should a more suitable one come to light.  Minor comments: 269: metod => method  Please go through the citations to correct capitalization, e.g., [15] nature => Nature