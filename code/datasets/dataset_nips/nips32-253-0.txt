The paper proposes the use of triplet loss in order to achieve more desirable geometric relationships between an example, its adversarial counterpart and examples from the other classes.   This is an intuitive proposal and the triplet loss has proven very useful in other ML contexts.  The paper is generally well written and the triplet loss proposal for adversarial examples is original, to the best of my knowledge.  Experimental results demonstrate the use of the proposed loss is very promising for improving adversarial robustness.  It doesn't seem at all suprising that the latent representation of an adversarial example is shifted towards the false class (or away from the true class).  After all, isn't this the basis of the optimization used to generate adversarial examples in the first place?  Furthermore, using t-SNE to visualize the behaviour of adversarial examples is not a new idea and has been used in a variety of papers in a similar way, see e.g. -Generalizability vs. Robustness: Adversarial Examples for Medical Imaging -Defend Deep Neural Networks Against Adversarial Examples via Fixed andDynamic Quantized Activation Functions -IMPROVING THE GENERALIZATION OF ADVERSARIAL TRAINING WITH DOMAIN ADAPTATION For these reasons, I do not believe this component of the contribution is as significant as the proposal of the new loss function.  A question for the future would the underlying idea be further refined to the popular quadruplet loss, which is often used to extend the triplet loss.