The work starts by highlighting potential limitations of PSRL-inspired model-free exploration methods with intuitive propositions. To overcome these drawbacks a framework for decoupling the reward and transition uncertainties by modelling the reward function via BLR is proposed. This architecture is used along with a TD-successor feature learning with an additional Q-values constraint. They conclude with experiments on tabular MDPs and Atari 2600 games.  Originality - The decoupling of uncertainty via BLR of rewards is an interesting direction for driving exploration.   Quality - The first-half of the paper is very clearly written. But the second half from Section 4 is lacking adequate motivation for the method and structure.  Clarity - The part from Section 4 while a proposal for the algorithm, lacks adequate motivation for why the approach would not suffer from the drawbacks highlighted in part 1. It is unclear whether SU is an interesting exploration algorithm because it overcomes the limitation of Proposition 2, or because it satisfies Definition 2. While the authors acknowledge some limitations of the method in Section 4.4, the two parts of the paper seem rather incongruent. Further  (1) algorithmically it is unclearn how the uncertainties drive exploration — greedily/stochastically? (2) there is a disconnect between the pseudocode in the appendix and the last paragraph of 4.2 (3) while the complete theoretical section in Section 5.2 is wrt Figure 1 is interesting, the failure of BDQN and UBE is surprising — if the constants are high, I do not see how they fail so much. (4) Section 5.3 seems unnecessary and is rather unclearly presented (5) y-axis in Figure 4 — clipped or is-between? (6) Why do the embedding need to satisfy said properties in Section 4.1? (7) Successor Uncertainties seems to be a confusing name considering the proposal models only the uncertainty in the reward function explicitly. How does modelling reward uncertainty compare to modelling transition uncertainty? I understand it is briefly discussed in Section 4.4., but the discussion seems to say "we can benefit from modelling successor uncertainty" - mostly rendering the name a misnomer. (8) Section 5.2 is rather unclear — tied actions == stochastic transitions? (9) the neural network models the action as just another input?  Significance - I think the work is significant in parts but the complete paper can be better organized, and the contributions of Part 2 better placed in the context of Part 1. While modelling the reward uncertainty seems promising for prorogation of uncertainty in a more robust manner, the presentation of the actual algorithm obfuscates a lot of details in the main paper.   PS: I have not reviewed proofs for Section 5.1.  ------ Post-rebuttal: Thank you for your clarifying remarks, and sorry about the confusion regarding the Chain MDP. I have read the rebuttal and have updated my score.