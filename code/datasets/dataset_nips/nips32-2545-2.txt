Summary of the work: This paper proposes a methodology to perform inference on encrypted data using functional evaluation. Authors develop a specific model consisting of private and public execution; the private (cypher-text) execution takes place a 2-layer perceptron with square activation functions in the hidden layer. The output of this 2-layer perceptron is revealed to the server, which runs another ML model to classify the input. Authors provide Functional Encryption tools to efficiently run the private part of the protocol. They also propose a   strong points: - Authors clearly distinguish their work from other private inference scenarios: their target is applications where the client might not be "online" and cannot communicate in an SFE protocol. - The importance of off-line secure inference is well-motivated using examples. - The paper is well-written and different aspects on the work are explained effectively. - The scenario of having some labels to be revealed publicly (e.g., digit value) while keeping some private (e.g., font) is useful in many practical applications.    weak points: - The threat model is not clearly explained. I would like to know the following: - It is not clear which party is the adversary; My assumption is that the server is untrusted. Given that, I cannot convince myself how the adversarial re-training step is meaningful: the server is the party who learns the private 2-layer perceptron using training data. From that point of view, the adversary itself has control over the underlying model and he can avoid adversarial re-training. - If the above scenario is not correct, it would be nice if the authors provide explanations to avoid confusion. Please specify which party trains each part of the ML model. - From the runtime results, the Encryption time (client's processing time) is around 4x the server's evaluation time. Therefore, outsourcing the evaluation to the server does not save much time (compared to the scenario that the client herself evaluates the model in plaintext). If the server is not willing to share the model parameters with the client, then we should assume that the server (the adversary) can train the model himself, which is makes the adversarial training step not so sensible. - Another major issue is the scalability of this approach, and whether it can be generalized to other datasets. The provided (private) model architecture is overly simple and is evaluated on MNIST-like data. I would like to see how the network architecture would perform (in terms of accuracy and runtime) for more complex datasets like CIFAR-10. - Minor: please describe what \theta in Section 4.2 represents.