The paper makes several contributions to the inference of (inverse) Wishart processes. The paper considerers an existing model where the (inverse) D-dimensional Wishart process is constructed by >D independent Gaussian processes and inferred by observing the values at xn of the corresponding correlated variable Y. The first contribution is the scaling of the model to larger datasets N, as this is a well known issue with GPs. For this the authors use a variational inference approximation the Gaussian processes whose parameters they learn by optimizing the ELBO (estimation of noisy gradients required for this). The authors find that regularizing the (inverse) Wishart matrices by a constant pos def matrix (white noise model) helps performance, especially in the case of the Wishart process. Another issue is the computational complexity due to large output dimensions. The authors approach this problem by modeling unregularized Wishart distributed matrices with a lower rank (less GPs), where the rank is a tuning parameters that trades off precision and cost. The authors evaluate on 3 real world datasets, and compare to competitor methods. Experiments show superior performance especially for the inverse regularized Wishart process.  I. Originality:  The paper combines essentially two existing methods. In that sense the originality is low. However, it appears to me that getting all of those components to work smoothly together is not a trivial task. Also the white noise model and its analysis, and the factored model are original and practically useful extensions.   II. Clarity:  The clarity of the paper is high in the sense that most details are included in the derivation, but lower on the other hand for the fact that it is so dense. It might have been an easier read if e.g., the details on the gradient computation in section 3 were moved to the supplements, and instead there would be one or two more plots which showcase the approximations done in section 3 and 5, as well as what the regularizer does in 4. In fact, I find the analysis in 4 very nice because it seems to be a recurring issue (e.g., also in optimization, the hessian) where it is not clear if inferring a matrix or its inverse is the better choice. In that regard it would have been interesting to see how a model that predicts Lambda or Lambda^-1 (learned by iwp-n) for every time step would have performed (i.e, a model that predicts Sigma as being the white noise at every step).  In section 4, l.133 it is mentioned that the Monte Carlo gradient approximation has high variance, but in section 3 l. 113 the opposite is claimed. It appear to me that both statements correspond to the same approximation, so could the authors please clarify which of them is correct.  The experiments show that the model performs well with regard to predicting the covariance Sigma between different time series, i.e., the performance metric is the likelihood of the model for unseen Y. This is nice, but I am unclear about if this is what those models are actually used for. I could imagine that the goal would be to predict the individual time seriesâ€™ values Y, not in the sense that the Y are a probable draw from the predictive distribution, but in an estimator sense (an estimator for Y). Is there a way to accomplish this with the proposed model? And if yes, how does it perform?  Lastly I have a question about the datasets used. I gather that the novel method is proposed because of scaling issues in N and D. However all datasets proposed have N smaller than what a full GP can fairly easily handle. Also the output dimensions do not seem to be that large. Do I misunderstand sth here? I am not sure if the total complexity using a full GP is stated anywhere in the paper. If a full GP is feasible, could the authors please point me to the baseline that used a full GP instead of the variational approximation since this model would be the most interesting to compare to.   I would also appreciate a short sentence about wall-clock runtime of these methods.  [Post-Rebuttal: Thank you for your rebuttal. I will stay with my initial score.]