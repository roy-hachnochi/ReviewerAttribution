Summary: This paper tackles the problem of automatic hyperparameter tuning. The proposed approach differs notably from prior approaches in being more sample efficient, requiring data collected from only a single policy. After collecting a batch of data, candidate policies are generated by sampling a set of hyperparameters and taking a gradient step on the current policy with them. The value of these candidates is evaluated by importance weighting the batch of data collected by the original policy, and the best is chosen as the new policy.  Originality: I am not familiar with all the related work, but the idea seems novel.  Quality: As noted in the paper, the approach only works for tuning parameters that only affect the policy update (for example, you cannot tune the batch size with HOOF). It seems like the most sensitive parameters are often these (things like learning rate or discount), so this limitation does not seem too severe.  This is more of a sanity check for hyperparameter optimization for PG algorithms in general, but I would like to understand if it’s important to find “good” hyperparameters, or simply to change them a lot over the course of training. Can you reference a study of this? For example, what if you took the median in Eq. 4, or even a random update?  One drawback here seems to be the random sampling method for hyperparameter options (Section 3.3) - this search space will grow exponentially as the number of hyperparameters increases.  The method really hinges on the importance sampling estimates being reasonable. Can you elaborate on the statement that “while the estimated policy value can have high variance, the relative ordering of the policies has much lower variance.” Why is this necessarily true? If the values are high variance, then they could easily be out of order?  The results seem significant to me, though I’m not familiar with the state-of-the-art in this area. Can HOOF be used to find hyperparameter schedules that can be used to train another model - first of all on the same environment, and then even on a different one?    Clarity: The paper is mostly well-written and clear. From my understanding,  the main contribution of this paper is its sample rather than computational efficiency, and that should be highlighted. Running PBT with on-policy algorithms on a real system would be totally infeasible, but HOOF would be feasible. Maybe this is what is meant by “within a single training run”, but I think this language can be made much more clear. If you define “training run” as training a population in parallel, then it’s also true of PBT, which is not what you mean.  I think it is misleading to refer to the algorithm as “meta-learning” in the abstract - is this a typo? It occurs nowhere else in the paper. I suggest shortening the introduction considerably - a lot of it is redundant with the related work section. In Figure 1, it would be good to provide some results from runs with suboptimal hyperparameters, to get a sense of the variance in returns from different settings. In Figure 2, it would be good to show TNPG without HOOF, to demonstrate it is indeed worse than TRPO.  Significance: medium - seems like a reasonable and sample efficient approach   -------------- Post-rebuttal ------------------  Thanks for humoring the sanity check with random sampling rather than zero-order optimizing Eq. 4. As a small comment, I think explicitly referring to the method as “zero-order optimization” would be quite helpful in making the method clear at first read. Thanks also for your graphical demonstration of the WIS estimates ordering. My point was more that it I think it is *possible* for them to be out of order, even though I agree they will certainly be in order more often than individual value estimates will be. And this depends on K, does it not? If you only sampled 1 trajectory, you’d just have one estimate per candidate (I know that’s unrealistic for PG algorithms, but just for example).  