After reading the rebuttal I believe the authors can improve the quality of the paper with limited effort. I've updated my rating accordingly. ______________________________________________________ Originality: the paper follows the idea of previous works, but uses Thompson sampling in a novel way (to the best of my knowledge) for efficient task selection in contextual BO. However, the overall contribution of the paper I find less impactful, as it is a combination of known techniques (BO, Thompson sampling and regret minimization) and the context selection approach of previous work. The paper places itself well w.r.t. related work.   Quality: the overall quality of the paper is good with an important exception. The kernel selection technique is well supported, the algorithm derivation is sound and we can also read about theoretical guarantees on minimizing overall task regret. However, as with many BO methods scaling to higher dimensional problems is an important aspect. Yet, the paper does not discusses this problem and the evaluations are rather low dimensional: 1-1 (context-action) dimensional for the synthetic data and 1-2 dimensional for the simulated nuclear fusion problem.   Clarity: the paper overall reads well. However, I don't really understand why the paper stresses the 'offline' learning aspect. Not only the title emphasizes this, but the introduction puts lot of weight on this aspect. Yet, the contribution I feel has nothing to do with this problem. The efficient task selection aspect I think is an important requirement for any computational budget, not only for tasks with long simulation time (1 hour for the nuclear fusion). Based on the evaluation time the resulting BO technique can be applied offline/online, or anywhere in between. On another topic: while the paper describes the proposed algorithm well I don't really understand how the context parameters are represented for multiple tasks: is it a continuous, or a discrete variable? Can the algorithm choose from a fixed amount of contexts, or can it optimize it in a continuous setting? Given the tokamak example in the supplementary material it seems like we have a finite set of contexts to choose from, but the context variable itself is continuous. How do you interpolate?  Significance: the paper at it present form needs some work in terms of evaluations and clarity to improve its significance. The main idea and the algorithmic approach may have some useful messages for practitioners, but the overall impact I find marginal.