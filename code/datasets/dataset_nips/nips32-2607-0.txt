Originality: The methods are not necessarily original - the approach is a pretty straightforward application of triplet embeddings from NLP to time series context. Of course, they did need to map context and positive and negative examples into the time series setting, which they have done. I am curious, though, how the method would perform if the "context" and "positive" examples did not explicitly overlap. For example, if they were simply close to each other in time, either slightly overlapping or adjacent, how would this change the performance. Quality: The paper quality is mediocre overall, though I do like the idea and do want to see it published. Clarity: Can be improved, as the writing is poor at times.  Significance: Embeddings for time series is an important problem. This paper does apply a useful technique to embed time series, which to the best of my knowledge has not been done. In that regard, this paper is significant, and the community does need to see these results. That said, there is only one comparison to state of the art existing unsupervised methods in the main paper, DTW, so it is difficult to know how this performs in comparison to other embedding methods like seq2seq.  Again, I like the ideas in the paper, I think they could be very useful in many applied areas, but I'm not sure the current version of the paper is NeurIPs quality. 