Some questions and comments:  -- In Figure 4, why does changing the temperature of a network that is already trained with label smoothing degrade calibration? Can the authors offer some insight?  -- In Figure 5, why does label smoothing slightly degrade the baseline performance of the student network? Doesn't one expect the student's baseline accuracy to improve by enabling label smoothing?  -- From the visualizations shown in the fourth row of Figure 1, it appears like label smoothing could be particularly useful for generalization on samples from classes that are semantically similar. Does this actually hold? (One can examine the confusion matrix of a classification task to see whether confusion between semantically similar classes is resolved in more cases when label smoothing is applied to the model.)  -- The visualization idea is neat and it reveals how label smoothing forces training examples of the same class into tight clusters and encourages examples from a class to be equidistant from other classes. This uniformly holds for different datasets and model architectures (rows 1-3 in Fig. 1). While interesting, if we were to rank the contributions of this work, I'd rank this last. I would suggest reorganizing the layout of the paper so that this section on visualization appears after the sections on calibration and distillation.  -------  Post rebuttal: Thanks to the authors for addressing my questions. I think this is a strong submission and would really like to see it get accepted. I'm raising my score to an 8.