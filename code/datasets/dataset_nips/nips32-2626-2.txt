**Things to like** I like this paper. 1. Quality. First of all, it is well written and pleasant to read. I couldn’t find any grammatical mistakes. 2. Significance. The authors use their analysis to produce something useful: a heuristic for learning rate selection. Then they provide some experimental validation that this heuristic is useful. I appreciate seeing this in a theory paper. 3. Quality. The technical aspects of the experiments are solid.  **Concerns** Take these concerns with a grain of salt. I set my confidence score to 2, as I have a Sutton & Barto-level knowledge of reinforcement learning theory.  1. Clarity. I am worried about how the amount of mathematical notation affects clarity. The authors are not obfuscating anything; I believe this topic simply involves a lot of math. However, I doubt if the majority of NeurIPS attendees will be able to understand this paper. Since there are previous RL theory papers accepted to NeurIPS with this density of math [2], I don’t believe this is a fatal flaw and I did not take this into account in my overall score.  2. Significance. As a practitioner of RL, I’d like the authors to provide a more convincing argument about why I should care about this analysis. I’m not saying I don’t care (I’m quite interested), but I would love to hear the authors make a case for why and how this work affects RL “experimentalists.”  [1] Dalal, Gal, et al. "Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning." arXiv preprint arXiv:1703.05376 (2017). [2] Hasselt, Hado V. "Double Q-learning." Advances in Neural Information Processing Systems. 2010. 