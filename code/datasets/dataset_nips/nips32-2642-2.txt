The authors develop an unsupervised, probabilistic, and scalable approach for spike localization from MEA recordings, in contrast to previous approaches which either required supervision, did not scale to large datasets, or relied on a simple heuristic (e.g., COM). Though the proposed model is relatively straightforward and the authors use standard approximate inference techniques to learn the desired posterior, the application to this domain and empirical validation of the approach seem to be novel contributions. The work is technically sound, with empirical results that demonstrate improved performance as compared to the COM heuristic for spike localization. However, there is no experimental validation of the proposed data augmentation scheme alone, as it seems that this scheme is used in both the MCMC and VAE approaches which makes it unclear what fraction of the performance improvement over COM is due to data augmentation versus the model-based posterior inference. A comparison to alternative spike localization methods besides COM would also strengthen the work, though I'm not sure if the works cited by the authors would even be feasible for the scale of the datasets being analyzed and therefore don't consider this a major shortcoming of the work.  Though the writing is clear overall, some minor details could be clarified: the description of the model in Section 3.1 describes a procedure for choosing the location prior means (lines 122-123), but the proposed inference methods are stated as using a location prior mean of zero, which seems to be a discrepancy. Section 3.2 describes a bounding box of width W and number of channels L that are used in the data augmentation scheme, but the values for these used in the experiments are not explicitly stated (based on the captions from the figures/tables, it seems like values of W = 3, 5 and therefore L = 4-9, 9-25 were used, but this could be more made more clear). However, these are more minor issues that could easily be fixed.  Update based on author feedback: Having read the authors' response, I feel like my concern about the effect of the data augmentation scheme was properly addressed, particularly if the authors commit to including an empirical analysis of the effect of the data augmentation on overall performance in the appendix as they mention. However, the other reviewers' comments on the lack of comparison to state-of-the-art methods makes me feel that this is more of a shortcoming of the submission than I had initially thought, and I'm not familiar enough with the alternative methods to know if leaving out any comparison to them is justified. Overall, I would still lean more towards accepting the submission but don't feel confident enough to strongly recommend acceptance, and therefore maintain my original score.