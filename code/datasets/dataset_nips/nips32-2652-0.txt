***** Post Rebuttal ****** The authors response did not change my evaluation. I strongly suggest to provide better motivation for the problem, and also discuss alternative approaches (e.g., first-order method)s. *****************************  The paper studies efficient algorithms for solving Kronecker Product Regression problems. These are interesting problems, since explicitly forming the regression problem and solving it requires typically huge amount of memory and runtime, but on the otherhand, the regression matrix is highly-structured and hence there is hope of having efficient solvers which do not require to explicitly "write down" the problem. Concretely, given matrices A_1,...A_q, each of dimension n_ixd_i, we want to solve L_p regression (1<=p<=2) with the matrix being the Kronecker product of A_1,...A_q  (this is a matrix of dimension n_1n_2...n_qxd_1d_2...d_q) and the vector of outcomes b is of the corresponding dimension (n_1n_2...n_q).   A recent result (DSSW18]) showed how to solve the most interesting (to my taste) case of $p=2$ with runtime that depends only on the sum of non-zero entries in the matrices A_1,...A_q (which is naturally unavoidable) and the number of non-zeros in the outcome vector b (which might be avoidable). Since b will typically be dense, this runtime is not ideal because of the huge dimension of b. Thus, the main question answered in the paper is to obtain an improved result of runtime that only depends on the overall number of non-zero entries in the input matrices A_1,...A_q.  The authors provide improved results also for the case p<2 (although the improvement is less dramatic - still depends on nnz(b)) and for some related problems such as the-all-pairs regression problem.  On the technical side, the authros claim to develop and use novel techniques for sampling from Kronecker products to construct a compact sketch and then solve it via standard methods (SVD for p=2).  The authors also conduct empirical tests showing their method improves in runtime over the previous work.  While I am certainly not an expert on subject (e.g., sketching techniques used), I think that from an algorithmic point of view, the problem is indeed interesting and challenging, and it seems the authors advance the state-of-the-art with a solid and clear contribution.  Some issues: 1. from the references provided, it is not clear how much such regression problems are indeed interesting in terms of applications (or are they just theoretically interesting?)  2. In principle we can solve such problems with first-order methods for convex optimization. Is it clear that something like SGD are computing gradients in a "smart way" cannot give something better than explicitly forming the problem? I think such a discussion makes sense.