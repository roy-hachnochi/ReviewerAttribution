This paper gives randomized algorithms for Kronecker product regression problems. The claimed run-time of the method is a slight improvement over the state of the art in randomized algorithms. They consider L2 and Lp regression for p in [1,2]. The method is based on obtaining leverage scores of the Kronecker factors. The key observation is that approximate leverage score distribution of the factors yield an approximate leverage score distribution of the product. The algorithm subsamples the matrix proportional to the leverage scores and solves the smaller linear system via classical methods. The algorithm utilizes input sparsity time leverage score approximation methods based on the work by Clarkson and Woodruff, 2009.  They also extend the result to Lp regression using p-stable random variables. For L2 regression, the run-time of the proposed method does not depend on the number of nonzeros of the right-hand-side vector, which is an improvement over previous run-time bounds when the vector is dense. Although the contributions appear to be sufficiently novel. I have some concerns on the hidden constants and the practicality of the method. Please see below for detailed comments.   (1) In Theorem 3.1 and it's proof, is m=O(1 / (\delta \epsilon^2)) or m=O(d / (\delta \epsilon^2)) ? It looks like a factor of d is missing.  (2) The run-time guarantee of Theorem 3.1 involves an unknown constant in the O(1) term, in O( \sum nnz(A_i) + (dq/\delta\epsilon))^(O(1)). Is there a bound on this term ?  (3) The authors need to motivate problems involving extremely rectangular, i.e., n>> d, Kronecker product matrices. Otherwise, the run-time involving d^O(1) is not smaller compared to direct methods.  (4) The authors claim that O(nnz(b)) is larger than O(\sum_i nnz(A_i)) to establish a faster run-time. Is this realistic ? This also needs to motivated.  (5) Conjugate Gradient algorithm on well-conditioned linear systems Ax=b run in O(nnz(A)) time. Is there a similar guarantee for well-conditioned Kronecker product systems ?