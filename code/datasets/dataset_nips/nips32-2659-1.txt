This paper studies the problem of learning halfspaces under arbitrary data distributions and when label noise is present in the data. This problem has a rich history and the celebrated result of [BFKV'97] showed that there exists a polynomial time learning algorithm when the label noise is i.i.d., i.e., when each label is flipped independently with probability eta < 1/2. Essentially this is the only noise model for which we know distribution independent learning results. At the other extreme we have the agnostic learning model where we know that learning halfspaces under the uniform/log-concave distributions is easy and there is also evidence that agnostic learning under arbitrary distributions is hard. An intermediate noise model is the Massart/bounded noise model, where the label of each example x is flipped independently with probability p_x < eta < 1/2. Even for this model it has been a longstanding open problem as to whether one can design a learning algorithm that for any eps>0, achieves error OPT + eps, where OPT is the error of the best halfspace. Before this paper it was known how to achieve this only for uniform/log-concave distributions.   The main result of the paper is that for learning halfspaces under arbitrary distributions under Massart noise, one can achieve error eta+ eps in polynomial time. This is a significant advance over the state of the art. The paper shows that one can construct a decision list of halfspaces to achieve this bound. The main insight in achieving this is Lemma 2.5 that states that under Massart noise, if data distribuition has some non-trivial margin, then by minimizing a convex proxy one will end up with a halfspce w that does well on a non-trivial amount of the data distribution. Furthermore, this space can be identified by simply thresholding |w.x| at a certain value T. Once this is proved, one immediately obtains learning algorithm for large margin distributions by simply repeating the process on the distribution that does not fall within the threshold. For the general case, one can use the idea of [BFKV] to preprocess the data so that a large fraction satisfies good margin and then apply lemma 2.5.  I very much enjoyed reading the paper, it makes progress on a long standing open problem and will lead to further theoretical work in the area. This is a very strong submission and I absolutely recommend acceptance. 