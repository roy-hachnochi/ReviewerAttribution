This paper provides an efficient algorithm for distribution-free PAC learning of halfspaces under Massart noise. This resolves a long-standing open question, at least for representations with a bounded bit-complexity.   The paper is very well-written; the techniques are interesting and well explained, and the organization is good. In several points, there are sloppy statements which are incorrect as stated, though they all seem fixable. The detailed comments below list these issues. I request that the authors address these issues in their response, as well as fix the final version of this submission.  The main observation exploited in this work is that while optimizing a convex surrogate cannot achieve the required error in this case (which the authors also prove), there does exist a convex surrogate that achieves a small error on some non-negligible region of the space. Repeatedly minimizing this convex surrogate on the remaining part of the space obtains a low error on the entire space, using improper learning with a decision list of half-spaces. The paper presents the solution to learning half-spaces with a margin and Massart noise, and later generalizes the solution to half-spaces without a margin, using the finite bit-complexity and an additional preprocessing step. The proofs in the body of the paper seem correct, except for some fixable issues listed below.  This paper addresses an important and interesting question, and resolves it with an elegant and well-explained technique. Assuming all the small issues are fixed, I strongly recommend that this paper be accepted.   Detailed comments: ~~~~~~~~~~~~~~~~~~ 1. The dependence of the solution for the zero-margin case in the bit-complexity of the representation is not revealed until section 1.1. Since the paper claims to resolve an open problem, it is important to discuss the relationship between the original statement of the open problem and the actual solution.  2. In Alg 1, there seems to be an assumption that the marginal distribution is completely available (line 3), otherwise an estimation process is needed here. Please explain.  3. Page 6, lines 249-252 ignore the fact that the guarantee of lemma 2.4 is on expectation only. This is then addressed in page 7, lines 276-279 but it is too late, as the paragraph on page 6 is incorrect as stated. Please unite these two paragraphs and put these on page 6.  4. Proof of lemma 2.5. page 7: the last display equation is incorrect. I believe there are several typos there. The conclusion in line 261 is correct though. Please fix this and explain your correction.   5. Page 7, lines 276-279: It is proposed to use Markov's inequality. Please comment on the boundedness conditions that allow you to do that. Also, Markov's inequality cannot actually get the same guarantee as that of the expectation, there will be some constant factor. Finally, when the procedure is repeated, the right w(i) needs to be selected by estimating L(w(i)) from samples. Please address this.   6. Page 7, line 280: (i) seems to require a small enough lambda.   7.  Alg 1, line 7: unclear what is meant by "uniform over the samples". Please rephrase.  8.  Page 3, line 101, it is not clear what "on the unit ball" refers to, though I assume this is an assumption on x. Please rephrase.  9. Page 4, line 185: theta is not used.   