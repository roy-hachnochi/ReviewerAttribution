This paper introduces an interesting and original framework for multi-armed bandits with potential applications. I liked the impossibility result and the global approach of the paper. Yet, in my opinion, the paper still needs some more polishing. It sometimes lacks clarity and contains some typos or proofs that I found hard to follow. Furthermore, the tightness of the results should maybe better discussed.  I have two main comments: - The lower bound and the upper-bound do not really match. In particular, if we apply the reward instance used in the lower bound to the upper bound, the latter becomes infinite (because the Delta_{ij} are zero), am I right? I found the dependence on 1/Delta(K,K^*) quite weak since this quantity may be really small. I would be interested to see a lower-bound with that dependence or at least experiments to see if this is really observed in practice. Furthermore, there is also a gap of a factor K with between the upper-bound and the lower-bound.   - What about the dependence on the delays? The proposed algorithm does not use knowledge of the delays. Would it be possible to improve the results if they are known or not? The impossibility result states that we cannot reach the optimal strategy but can we get closer to it?  Other Remarks: - Distribution-free upper-bound. The quantity Delta(K,K^*) may indeed be very small in many situations, is it possible to state also a distribution-free bound? - Is the factor (1-1/e) tight? The best factor obtained by the lower bound is 3/4, isn't it? - About Prop 3.4:   - The choice epsilon=0 seems to be the best leading to a ratio 3/4. Why do you need epsilon at all?   - As in all the proposition, the statement should be more accurate. In particular, it is assumed epsilon<1 which is not stated in the prop. Furthermore, "for any epsilon" should be before "There exists" since for each epsilon the instance is different.  - About Prop 3.5: I could not follow at all the proof. The greedy algorithm plays the (K+1)th arm if (1+eps)/(K+1)>1 but then its cumulative reward is better than the one stated for the optimal algorithm... What did I miss? - About Thm 4.1: the upper-bound is a bit hard to read. It would be useful to simplify it or to explain the different terms. - In the experiments, it is stated that sometimes, the greedy algorithm gets blocked in a wrong cycle leading to a negative cumulative regret. I would expect that the opposite is also possible, with UCB blocked in a suboptimal cycle. Isn't it possible with small probability? A high-probability regret bound would be useful. Furthermore, would some random perturbation of the prediction be possible to try to fall into a better cycle? - Proof of the examples l.254 would be nice - I do not understand how K^*=20 is possible in the experiments since the delays are all smaller than 20. Are all delays equal?    Typos: - l.72: i<j missing in the definition of Delta otherwise the gap is 0. - l.72: the assumption mu1>mu2>...>muK should be stated before the definition of Delta - l.73: >=1 missing in the definition of K^* - In some results, there are K arms and some other K+1 arms. Please make it uniform to ease the reading. - l.205: note that H is the Riemann zeta function  - l.208 and later: Delta_{ij} -> Delta(i,j) otherwise define it - l.236: ",," - l.237: exploraiton 