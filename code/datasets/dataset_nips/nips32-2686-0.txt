Originality: This paper has a number of original ideas. It clearly points out that many recent forecasting papers with neural networks train on many series but focus forecasting on individual series. Instead, their novel contribution is to blend these two approaches using an attention mechanism. This attention mechanism simply determines if the forecaster should weight the global forecasting from temporal matrix factorization or the local forecasts trained on individual series. The entire thing is trained jointly so the local forecasts are mostly trained on sections where the local forecasts are important etc. The authors also introduce a method for dealing with the scale problem in forecasting by separately forecasting the mean and residuals.   One question I had, which would be nice to see addressed, is why the attention mechanism is only based on the Y series, rather than both the Y series and the latent factors X + F. I could imagine that there could be other info included in X + F about how good global forecasts would be for a specific series. Clarity: good, I found it very easy to follow and understand.  Significance: High. This is important. Many neural network architectures are build for text/audio/images etc but these cannot simply be ported over to high dimensional time series forecasting. The area needs its own original contributions, and this paper does just that. Quality: High. Experiments are well done, the methodology is well explained and well motivated.