Pros: — Paper is overall well-written  — Approach is simple yet effective — Method achieves state of the art (SOTA) performance on standard benchmarks — Overall, this is a solid paper which warrants acceptance  Cons: — Some expositional details should be added (see details) — Some additional ablations would be useful (see details)  Detailed Comments -----------------------  > Exposition  Overall, the paper provides a clear introduction that motivates use of cross-modal information and lays out the high-level overview of the contributions. The experiments are also well structured and presented clearly. That said, some improvements are possible and would be warranted:   — The description of the problem at the beginning of Section 3.1.1 was confusing at first glance. Given the variety of few-shot learning scenarios, such as generalized few-shot learning, use of better notation and formalities in laying out the support and query sets more mathematically (as opposed to descriptively) would be useful.   — A more detailed overview of TADAM, the second model being extended (in addition to ProtoNets), was missing. This should be added.   — The ZSL case is mentioned in multiple places throughout the work, where at times seems unnecessary.   > Novelty  Method is conceptually simple, yet practically effective (as shown by the new SOTA performance achieved on few-shot image classification on the miniImageNet and tieredImageNet datasets). The paper's adaptive convex combination using learned transformation and mixing networks is interesting and relatively novel.   > Significance  The fact that the method is agnostic with respect to which metric-based model it is extending is a positive. As such, the proposed model may have potential of extending uni-modal metric based models using other modes of information, such as class definitions and knowledge graphs. Seemingly it can also potentially be applied more broadly in other few-shot learning tasks.   > Experiments  The experiments on few-shot image classification are reasonable in scope and coverage. The chosen baselines cover the majority of SOTA and near-SOTA methods previously proposed with good coverage across meta-learners, metric-based methods, space aligners, variational methods and other approaches. The CUB-200 results, although mainly included in the supplemental material, also support the same conclusion and the reported results show the superior performance of the proposed method on a variety of k-shot 5-way tasks.  — It would have been useful to also report results on MNIST although presumably the bi-modal class embeddings for the digits may have not been as effective.   — Additional ablations should be added. Specifically on the"N-way" behavior of the method. Are similar improvements observed with more classes? Although 5-way classification setting is most widely used, the number of few-shot variables and the behavior of the method with respect to such factor is important.  > POST REBUTTAL   Authors have largely addressed my comments. While simple, the approach appears effective and valuable. I continue to be in favor of paper's acceptance.