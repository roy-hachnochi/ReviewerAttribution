Post Rebuttal: Thank you to the authors for their comments. My concern was primarily about not having labels for classes in the test-episodes, since few-shot learning models can be applied to learn about arbitrary classes at evaluation without requiring any syntactic labels. With regard to the author's comments on simplicity of the model and being applicable to any metric-based method, I have increased my score.  Before Rebuttal: This paper proposes a modification to the few-shot learning image classification scenario, which involves using additional cross-modal semantic information in the form of word embeddings for class labels. The idea is that the extra semantic information can be helpful in a lot of cases where discriminating between visually-similar classes can be difficult, especially when given few examples. The authors propose an extension of prototypical networks for this setting where a prototype is the convex combination between the usual prototype computed from support item features and a learned transformation of the word embedding of the class label. The coefficient given to each part of the sum is a learned function of the word embedding. The method is evaluated on two few-shot learning benchmarks: Mini-Imagenet and Tiered-Imagenet. Additional studies are conducted to study the benefit of the extra semantic features as a function of increasing number of examples for few-shot learning.  Strengths  1. Paper is well-written - motivates and describes idea well. 2. Extensive comparison of relevant baselines.  Weaknesses  1. Proposed method is relatively simple extension - involves using typical prototype for class in addition to transformation of class word-embeddings. 2. The benefit of incorporating semantic information is largely in the 5-class, 1-shot learning case (3.5% Mini-Imagenet and 2.75% Tiered-Imagenet accuracy gain compared to state-of-the-art LEO applied to regular few-shot learning scenario) and there seems to be very little gain beyond that number of shots.  Comments The proposed setting and method requires that word embeddings are known for all train and test classes. Is it a more realistic scenario for few-shot learning that word-embeddings are only available for train classes, as this removes requirement that model that can only be used to learn about concepts that we already have word-embeddings for? Is semantic information as defined in paper applicable to few-shot settings beyond image-classification?