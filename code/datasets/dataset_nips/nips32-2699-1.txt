General Comments: The paper is very well written although very dense. I enjoyed the supplementary material, it helped me cover some background knowledge that I needed to understand the paper. The material is very original and very useful since it improves the original goal of hyperbolic neural networks (model graph and tree-like data). The best way is to actually combine it with Graph NNs that we already know perform really well. The experiments are extensive and convincing line 27:” due to the fact that the number of elements/nodes increases exponentially with hierarchy, but the area in Euclidean space grows linearly with distance.” Which area exactly? The area of a disk grows quadratically with respect to the radious not linearly. Also, this argument is fundamentaly wrong as it compares an infinite space with a finite space. The number of the leafs of a hierarchical space grows exponentially but remains finite. The area of a disk contains infinite points. Technically speaking the unit circle can fit the universe. The reason why euclidean space is different and it is described very well on this paper: As described in this paper https://papers.nips.cc/paper/5971-space-time-local-embeddings.pdf The maximum number of points which can share a common nearest neighbor is limited 2 for 1-dimensional spaces,  5 for 2-dimensional spaces while such centralized structures do exist in real data d-dimensional spaces can at most embed (d + 1) points with uniform pair-wise similarities. See the references:  K. Zeger and A. Gersho. How many points in Euclidean space can have a common nearest neighbor? In International Symposium on Information Theory, page 109, 1994. L. van der Maaten and G. E. Hinton. Visualizing non-metric similarities in multiple maps. Machine Learning, 87(1):33–55, 2012.  I suggest reading this blog https://networkscience.wordpress.com/2011/08/09/dating-sites-and-the-split-complex-numbers/ and also this paper: https://dl.acm.org/citation.cfm?id=2365942                     line 91: “In contrast to previous work, we derive core neural network operations in a more stable model” That statement is not backed. It is a bit in the air. You have to be more specific.   One of the weaknesses of this paper is that it doesn’t stress the differences with Reference [10] Hyperbolic Neural Networks. Especially on the derived operators. From my understanding, the contribution is on aggregation layers and attention mechanisms. It also seems to me that another difference is that you work on the Lorentz manifold (which you call hyperboloid, it would be better to use Lorentz, since it is more widely known as that in the ML community).   