Originality: The work builds on the SCAL algorithm, but contains some novel aspects. In particular, the technique used to prove optimism (i.e. that the gain used to compute a policy is an upper bound on the true optimal gain) is different from UCRL and SCAL. While the latter solve an expanded version of the MDP, this paper shows that the value update operator T_c^+ (similar to the Bellman operator, but additionally projecting to a span constraint) is optimistic wrt its application to the optimal bias function. This is tighter by a \sqrt{\Gamma} factor, though the \sqrt{\Gamma} factor remains in the regret bound.  The authors comment that it is an open question whether the \sqrt{\Gamma} dependency can be removed from the regret. This term is not present in the Politex algorithm (http://proceedings.mlr.press/v97/lazic19a/lazic19a.pdf) in tabular MDPs, though the dependence on T is suboptimal there.  The analysis of SCCAL+ relies on similar assumptions and discretization as UCCRL [2], but has a practical implementation.   Quality: The technical content of the paper seems correct, though I did not read the supplementary material. The experiments are small-scale, and SCAL+ is not always better than SCAL or UCRL. However it does perform best in a setting with large Gamma, which is to be expected based on the theory.  Clarity: The paper is well-written, and the proof sketches and explanations make sense. It would be useful to explain the continuous state contributions better, including placing them in the context of previous work, and explaining why UCCRL cannot be implemented.  Significance: While there is no improvement in regret results, the proposed algorithms seem practical and the new technique for showing optimism may be useful in future works.   After reading the rebuttal, I am still in favor of acceptance, and my score remains the same.