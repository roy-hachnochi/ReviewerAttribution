== high-level overview ==  A recent series of papers culminating in [1] have shown that taking the ensemble prediction of any classifier over additive Gaussian corruptions of its input yields a new classifier that is certifiably robust in L2 norm.  However, it has remained unclear whether other randomization schemes (besides additive Gaussian corruption) would yield natural robustness guarantees under other distance metrics (besides L2).  This paper proposes a new randomization scheme which yields a natural robustness guarantee under the Hamming distance (a.k.a the L0 norm).  Using this method, the authors obtain certified robustness guarantees in L0 norm on ImageNet (and binarized MNIST).   I recommend accepting this paper.  Originality: this paper is the first to give a randomization scheme for randomized smoothing which confers robustness in Hamming distance.  Quality: the technical work is of high quality. Clarity: the writing is confusing in places, and I would not be surprised if some of the other reviewers have trouble understanding parts of the paper. Significance: the paper shows how to obtain neural network-based classifiers that are certifiably robust in Hamming distance, which is a significant result.  == detailed overview (this may be helpful to the other reviewers) ==  Consider the problem of classifying strings "x" of characters from some discrete alphabet, and consider the following randomization scheme "\phi": for each letter x_i, with probability (1 - \alpha), replace that letter with a new character chosen uniformly at random from the alphabet, and with probability \alpha keep that letter in place.  For example, if the alphabet is the English alphabet, and x is the string "quickbrownfox", and \alpha = 0.8, then here are five draws of the random variable \phi(x): qufckbvownfox, buiczbruwnfoo, qucckbqopnfox, qucckbrownfox, qzqckbgpwnfox.  Ok, now let  "f" be a base classifier which maps from strings to classes, and define the corresponding smoothed classifier "g" as g(x) := argmax_{class c} Pr[ f( \phi(x) ) = c ], i.e. g(x) returns the most probable prediction by f of the random variable \phi(x).  This paper proves that g is certifiably robust under the Hamming distance.  That is, if we measure p := Pr[ f( \phi(x) ) = y ], where y := g(x) is the top class, then we can certify that for any input x' within some L0 ball around x, g(x') = y.   The paper applies this guarantee to the domain of image classification, where the "alphabet" consists of the 256 integers 0, 1, 2, ..., 255, and each ImageNet image can be viewed as a "string" of length 224 x 224 x 3.  That is, to sample from \phi(x), you consider each pixel x_i, and w.p. (1 - \alpha) you swap out that pixel value for another pixel value chosen uniformly at random from the range 0, 1,2, ..., 255.  You can see examples of these corruptions in Figure 5 in the appendix.    Ok, here's how you prove the robustness guarantee.   Let x be the original input and let x' be some hypothetical perturbed version.  To prove that g(x') = y, it suffices to prove that Pr [ f ( \phi(x') ) = y  ] > 0.5.  However, all we know about f is p :=Pr [ f ( \phi(x) ) = y  ].   This suggests the following optimization problem over functions: what is the minimal value of Pr [ f ( \phi(x') ) = y  ] subject to the constraint that Pr [ f ( \phi(x) ) = y  ] = p?  In the Gaussian case studied in [1], the function f* which attains this minimum is the function f* which returns y on a superlevel set of the likelihood ratio function, i.e. a set of the form {z:  \phi(z | x)  / \phi(z | x')  >= c } for some c chosen so that P[ f* ( \phi(x) ) = y] = p].  (I am using \phi(z | x) to denote the probability of z under the random variable \phi(x).)  In the discrete case studied here, however, it is impossible to choose some c for which P[ f* ( \phi(x) ) = y] = p] is exactly equal to c.  Therefore, the optimal f* is actually a *randomized* classifier which *always* returns y on a set of the form {z:  \phi(z | x)  / \phi(z | x')  > c } and *sometimes* returns y on a set of the form  {z:  \phi(z | x)  / \phi(z | x')  = c }, where the probability in the "sometimes" is carefully tuned to ensure that  P[ f* ( \phi(x) ) = y] = p.  This analogous to how the Neyman-Pearson UMP test is sometimes a randomized test.    Under the randomization scheme \phi(x) proposed in this paper, the likelihood ratio function \phi(z | x) / \phi(z | x') is a function of just ||z - x||_0 and ||z - x'||_0, i.e. the Hamming distances between z and x, and between z and x'.   However, to determine what the threshold "c" should be, you need to face the hairy question of computing the size of each likelihood ratio region, i.e the cardinality of the set of images/strings that are hamming distance "u" away from x, and hamming distance "v" away from x'.  Sadly, there is apparently no way to do this computation in closed form; it needs to be done on a computer.  This takes four days (!!) for ImageNet.  But the silver lining is that by symmetry, you don't need to do this separately for each image pair (x, x'), you just need to do it once for each Hamming radius (Lemma 5).  The experiments were thorough.  The authors made sure to compare against relevant baselines, e.g. using Gaussian randomized smoothing to certify a Hamming ball.   == feedback to the authors ==  -- The main contribution of this paper is the Hamming distance guarantee in section 4.2, and the associated algorithms / experiments.  I think the submission currently under-emphasizes this contribution, and over-emphasizes the significance of both Lemma 1, and the uniform distribution analysis in section 4.1.  Overall, I would recommend re-working the paper so as to put the Hamming robustness guarantee front-and-center.  Why do I say that section 4.1 is over-emphasized?  Because the L1 and L-inf guarantees that one can derive for the uniform distribution are very bad.  (That's presumably why this section is pitched as a "warm-up.")  For example, on 224x224x3 ImageNet, with \gamma = 0.5 (which is already a huge amount of noise), if the probability of the top class is p=0.99, then your bound in Proposition 4 will certify a L-infinity radius of only 0.001139 / 255, which is tiny.  So I'm honestly not sure if it's even worth putting this in the main paper.  Why do I say that Lemma 1 is over-emphasized?   Because (a) Lemma 1 is a straightforward extension of the analysis from [1], and (b) I'm not sure if there are any good applications of Lemma 1 besides the Hamming distance guarantee.  That is: I assume that the reason why you factor out Lemma 1 into a separate statement from the Hamming guarantee is so that others can build on it independently, but I'm just not sure if it's possible to do so.  Additionally, I think that it makes your paper needlessly confusing to first present the abstract theorem first, and to then later instantiate that theorem in the form of the Hamming guarantee.  I recommend the reverse order: you should first present the Hamming guarantee, and *then* note that the analysis can be generalized into Lemma 1 (which could live in the appendix).  -- I think you should emphasize that the Hamming guarantee is *tight* w.r.t all measurable classifiers (just like the Gaussian / L2 guarantee from [1]).  That is: any perturbation outside the certified Hamming radius is adversarial in the worst case over the base classifier.  This suggests that your randomization scheme is naturally suited for Hamming distance robustness.  -- Could you publicly release \rho^{-1}_r (0.5) for standard datasets like ImageNet so that others do not have to run the 4-day computation?  -- I am ambivalent about the decision tree certificates in Section 4.2.  On the one hand, it nicely demonstrates that making assumptions on the base classifier beyond just "p", one can obtain better certificates.  On the other hand:      (1) The main selling point of randomized smoothing is that it applies to large neural nets.  There are perhaps better ways than randomized smoothing to certify the robustness of decision tree-based classifiers.      (2) The assumption that the tree can only use each feature once seems restrictive?      (3) Since both the smoothed classifier's prediction and the worst-case adversarial perturbation can be computed exactly using dynamic programming, I have a suspicion that a smoothed decision tree can be equivalently re-interpreted as a different kind of certifiably robust classifier altogether.  (Though that isn't necessarily a _bad_ thing.)  -- I recommend deleting Remark 2.  The reference [35], which assumes countability, is 69 years old.  Today, the extension of the NP lemma to randomized tests in both countable or uncountable domains is extremely common knowledge: see http://www.math.mcgill.ca/dstephens/OldCourses/557-2008/Handouts/Math557-07-HypTesting-Examples.pdf, or https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/lec20.pdf for example. -- I think you should make clearer that the four-day computation will precompute \rho for *every* ImageNet image.  You don't need to do the four-day computation separately for each image.  I was confused about this for a while.   --  The legend of Figure 3(a) makes it seem like the number of distinct likelihood ratio regions depends on the dataset, whereas in reality it just depends on the dimension of the dataset. -- Throughout the paper, you call your noise distribution / randomization scheme a "perturbation."  I recommend using the term "noise distribution" or "randomization scheme" instead.  Usually, in the adversarial robustness literature, a "perturbation" refers to a specific adversarially-chosen perturbation vector. -- Throughout the paper, you call a specific perturbation vector an "adversary."  Usually, in the adversarial robustness literature, an adversary is an algorithm for generating a perturbation vector (like PGD, or FGSM, or Carlini-Wagner), rather than the adversarial perturbation itself. -- The caption to Table 1 states that "the first two rows refer to the same model with certificates computed via different methods."  Just to be clear, it's the same *base classifier* in both rows, but the model whose robustness you are actually certifying (i.e. the smoothed classifier) is different between the two rows. -- The use of the L1 norm in Lemma 5 is confusing.  [1] https://arxiv.org/abs/1902.02918