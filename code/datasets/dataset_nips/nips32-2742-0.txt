In the prior literature, they cited the low dimensional embedding methods is the reason of the poor performance of the embedding based methods. In this paper, the author proposed that the final score vector for the labels actually generated by highly non-linear transformation such as thresholding the scores. Thus it is not clear if  the low-rank structure of the score vectors directly cause the low-rank on the label vectors. Furthermore, the author uses a simple neural network to mimic the low-dimensional embedding can attain near-perfect training accuracy but generalize poorly and suggesting that overfitting is the root cause of the poor performance of the embedding based methods. This is the first contribution of the paper which breaks the glass ceiling of embedding based methods.   In order to address the overfitting problem, it highlights the need of regularization techniques. We know that most feature vectors associated with only very few true labels, thus the label embedding matrix should have near-orthogonal structure, which is exactly what the Spread-out regularizer did. But the drawback is that it ignores the frequently co-occur labels correlation, thus it is not desirable.   In order to address the lack of  modeling co-occurrences of labels bring by spread-out regularizer, the author estimates the degree of occurrences between labels from the training data. The author built a co-occurrence label matrix and encode the conditional frequencies between labels into the loss function, which is the novel framework of GLaS Regularization. This method defines the degree of label correlation as the arithmetic mean of conditional frequencies between labels. In order to get rid of the expensive computation, the author chooses to compute  stochastic loss on a specific batch.   The GLaS regularizer’s name actually comes from the sum of graph Laplacian regularizer and the spread-out regularizer. Graph Laplacian regularizer tends to assign similar embeddings to labels that co-occur frequently, but the drawback is that it doesn’t penalize the similar embeddings of the non-related labels. This drawback can be solved by spread-out regularizer which encourages all label embeddings to be orthogonal regardless of any correlation. Thus, summing the graph Laplacian regularizer and the spread-out regularizer, the author proposes  their regularizer which captures the advantages of both regularizer.   Through the experiments on several widely used extreme (millions of labels) multi-label classification datasets, the proposed regularizer method matches or outperforms with all previous work in terms of precision on most dataset.   The main contribution of this paper is to point out the bottleneck of the inferior performance of embedding-based method is not limited to the low-dimensional embedding problem, instead, the author proves the poor performance is because of the overfitting issue, and proposes their novel regularizer which is the sum of graph Laplacian regularizer and the spread-out regularizer and evaluate the effectiveness on several widely used datasets.   From this point of view, research can focus on finding the other better regularizer and it is feasible to use low-dimensional embedding based methods even with simple linear classifiers to reach competitive result.   