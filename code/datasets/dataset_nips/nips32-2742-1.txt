After the rebuttal:  - Thanks for the plot. That is useful to see and in favor of the paper. - Note that representability based on universal approximation theorem does not imply learnability. Finding those representative weights, can still be NP hard. ------------- This paper addresses the problem of extreme multi-label classification. The paper proves that the embedding model is expressive. It then provides an example  of experiment on data where the embedding-based model performed poorly as the result of overfitting.  Once authors diagnose the problem as overfitting, they propose regularizers to constrain the model and improve the situation.  Strengths: - The paper addresses an important and relevant research question: is extreme multi-label classification learnable by embedding models? - The expressiveness theory (Section 2.2) is useful.  Weaknesses: - Quality: Results of Section 2.1, which builds the main motivation of the paper, is demonstrated on a very limited settings and examples. It does not convince the reader that overfitting is the general reason for potential poor performance of the models under study. - Soundness: While expressiveness is useful, it does not mean that the optimal weights are learnable. The paper seem to not pay attention to this issue. - Clarity: Related work could be improved. Some related  works are mainly named but  their differences are not described enough. - Organization could be improved. Currently the paper is dependent on appendix (eg the algorithms).  Also the contents of tables are too small.  Overall, I do not think the quality of the paper is high enough and I vote for it to be rejected.