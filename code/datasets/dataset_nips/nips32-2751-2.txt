1. This work seems to be the first to propose a convergent RL algorithm for MFG that is able to approximately find the mean-field equilibrium. Moreover, the explanation of why the naive application of Q=learning fails seems convincing.   2. The paper is well-written overall. However, the presentation can be further improved. For examples, in Section 2.3, it would be better to directly state the problem of the repeated auction as an MFG by specifying what $L$ is and how the reward function and transition probability depends on the mean-field distribution. Currently, the reward function in (3) seems to have a lot of notations and it is unclear how to write it as a function of s, a, and L. Another example is in Theorem 2, it would be better to carefully define each quantity instead of deferring them to the appendix.   3. In Section 4 the authors focus on the stationary solution which is time-independent. However, the uniqueness and existence theorem only guarantee a solution which might depend on time. Is it possible to directly show that the MFG solution is time-independent?  4. Given L, since the optimal policy of an MDP can be non-unique. In this case, the left-hand side of (5) might be large. Is there any intuition why this assumption holds?  5. My major concern is on the epsilon-net. Since the space of all distribution over S\times A is large, the size of this epsilon-net might be huge. In practice, how to construct this epsilon-net? In addition, the construction of this epsilon-net yields a gap $phi(\epsilon)$, which appears in the parameter $c$. In practice, how to choose parameter $c$?  6. The number of iteration in Theorem 12 seems complicated. Is there an easy way to see the order of iteration complexity?  7. Existence of mean-field equilibrium in discrete-time finite MFG is studied in the literature. See ``Markov--Nash Equilibria in Mean-Field Games with Discounted Costâ€œ