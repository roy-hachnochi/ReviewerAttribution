SUMMARY  The paper tackles a few-shot generalization problem for video-to-video translation, where the goal is to synthesize videos of an unseen domain using few example images of the target domain. To this end the authors extend vid2vid with two modifications. First, they replace the image synthesis module with the recently proposed SPADE [41]. Second, they introduce an adaptive weight generation module which provides weight parameters to the SPADE model. This new module is implemented with a soft-attention mechanism to aggregate statistics from a few example images of the target domain. The proposed approach is evaluated on two tasks, motion retargeting and street scene style transfer. The authors compare with three baselines (implemented by the authors) as well as two existing work [1, 46]. The authors report both quantitative and qualitative results to demonstrate their approach.  ORIGINALITY  - The proposed adaptive weight generation module could be considered new. The idea of using a soft-attention mechanism for dynamic weight generation -- in particular, using the embedding a_t from the current input semantic image s_t as a key vector to retrieve relevant appearance statistics from example images e_{1:K} --  is intuitive and reasonable. Although attention-based encoding of image sets is not new, it is well used in this work.     - It is unclear why the soft-attention based weight generation was necessary instead of, e.g., the AdaIN-based approach, as was done in FUNIT [31]. This needs to be better motivated -- Is AdaIN not applicable to the proposed approach, and if so, why? Why is it necessary to propose the attention-based method when existing approaches have already demonstrated the effectiveness of AdaIN in a similar setup (for images)?  QUALITY  - The quantitative results clearly show improvement over the compared baselines. The results on motion retargeting is particularly good, both qualitatively and quantitatively.   - The authors do not discuss limitations/weaknesses of their method. It would've been nice if there was a discussion on this.  CLARITY  - The paper reads well overall. But some of important details are missing (see my comments below).   SIGNIFICANCE  - As mentioned above, the problem is of wide interest to this community and the proposed technique generalizes to different domains. The visual quality on motion retargeting is promising, and the code will be released. Given these, I think this work will likely to have a reasonable impact and could encourage follow ups. 