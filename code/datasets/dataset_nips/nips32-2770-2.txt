#1. The problem tackled in this paper is quite interesting, in which I’ve never seen such work to switch inner product into more general metric. More interestingly, convolutional neural network with generalized inner product with a bilinear matrix is superior to the baseline with the same amount of parameters.   #2. I’m very impressed that Dynamic NSN achieves the better few-shot classification accuracy than LEO, even without using residual networks.  #3. It's very well-written and easy to follow most of parts in the manuscript.  == Updates after the authors’ rebuttal ==  After reading the rebuttal and having a full discussion, my final recommendation is to accept this paper. Below is a summary of justification to the final score.  [Novelty] Though inner product-based convolution is mostly adopted, dynamic neural similarity has some potential to improve the performance of CNN further. Specifically, such generalization seems to be well-suited to few-shot classification, because NSL is theoretically connected to nuclear norm regularization, briefly discussed in the rebuttal.  [Experiments] In the response, the authors included some additional experiments on few-shot classification, showing that Dynamic NSN really improves the classification performance on both prototypical network and MAML.