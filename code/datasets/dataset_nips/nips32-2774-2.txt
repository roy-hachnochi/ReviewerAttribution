<Strength of this paper> - It is hard to say this work has its own originality since it borrows core ideas from other methods like mixup or entropy-based methods. But this paper proposes a unified framework, which shows very good performance for semi-supervised learning, thus it provides a good baseline for further research. - The paper is well written and easy to understand.  - The experiments are good. The proposed method was evaluated on various datasets with good ablation studies.  - They provide source codes to understand and reproduce this work more easily.   <Weakness of this paper> - The implementation details are a little bit unclear. For the example of line 159-164, it would be better to explain the detail of 'exponential moving average of parameters' and why the authors used this technique.  - What happens when K (the number of data augmentations) is larger than 2?  - The weight decay 0.02 seems to be much larger than the standard settings as 0.0025 in [1]. It would be better if the authors provide the reason for the large weight decay.  - It would be better if the authors apply another regularizer (for example, cutmix [2], which is similar to implement as mixup, but shows better performance.) for the MixMatch framework.   Overall, this paper is well written, the proposed framework makes sense, and the experimental results are good.  So I want to give an accept for this paper.   [1] Oliver, Avital, et al. "Realistic evaluation of deep semi-supervised learning algorithms." Advances in Neural Information Processing Systems. 2018. [2] Yun, Sangdoo, et al. "Cutmix: Regularization strategy to train strong classifiers with localizable features." arXiv preprint arXiv:1905.04899 (2019).   -----------------------------------------------  The authors clarified some confusing things through the rebuttal.  I want to keep my original rating (7).     