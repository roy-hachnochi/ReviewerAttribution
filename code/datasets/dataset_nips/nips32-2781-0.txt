The authors propose a relaxation of submodularity, called differential submodularity, where the marginal gains can be bounded by two submodular functions.  They use this concept to provide approximation guarantees for a parallel algorithm, namely adaptive sampling, for maximizing weak submodular functions, and show its applicability to parallel feature selection and experimental design.  Overall the paper is well written and the problem is well motivated. The main motivation for parallelized algorithms is their applicability to large datasets. Although we see some speedup for relatively small datasets in the experiments, my main concern is that due to the large number of rounds in the worst case and large sample complexity, the algorithm may not scale to large datasets, especially in the actual distributed setting, (e.g. MapReduce).  Here are my questions:  - What is the sample complexity of line 5 of Algorithm 1 (theoretically)? In experiments, the authors mention they implemented DASH with 5 samples at every round, but later they say that sampling could be computationally intensive, and that’s why DASH is slower than SDS_MA for small k.  - Is the proposed algorithm scalable to large datasets, and is it applicable to actual distributed setting (e.g. MapReduce)? The number of rounds log_{1+\eps/2}(n) become prohibitive even for moderate size problems, and the experiments are done on relatively small datasets. So, the algorithm may not be actually used for large datasets that is main motivation for parallelization?  - How is SDS_MA parallelized? This is not explained in the paper or appendix, and it’s wired that the parallel version is slower than the centralized one, especially in the multi-processor case and not the actual distributed setting.  - Can other distributed frameworks like “A new framework for distributed submodular maximization“, "Fast greedy algorithms in mapreduce and streaming", or “Distributed submodular maximization: Identifying representative elements in massive data” be used to provide guarantees for maximizing weak submodular functions in the distributed setting (with or without differential submodularity)?  ---------------------- update: I'm not very convinced by authors' answers about the fundamental difference between the current setup and MapReduce. I believe in MapReduce, the ground set can be partitioned to several machines, while each machine filters elements from its local data and samples from it, and the results can be communicated to the central machine.   I also agree with other reviewers that introducing further applications could make the introduced notion of differential submodularity more interesting.