Authors analyze the behavior of principal component regression estimates when p>n. They provide theory on the asymptotic value of the L2 error with the sample size n growing to infinity and fixed ratios between the number of features and n and the number of PCs and n.  While the work is interesting and focuses on the unintuitive phenomenon, in the current form it's practical to use is not obvious. Results obtained under the assumption of polynomial decay of eigenvalues are solid. However, asymptotics there is not that interesting since at some point, the number of required eigenvectors is clearly much smaller than the number of features and the problem becomes easy even if p>n.  From my understanding, the decay of eigenvalues and the growth of p is the key component making the theory hold true. It would be interesting to see where the intuition breaks or what happens when new features are equally important (i.e. have equally large eigenvalues) -- what if all eigenvalues are separated from zero.  Despite a catchy title suggesting applied work, in the current version, authors do not answer explicitly the question they ask in the title. While clearly, the paper discusses the choice of p, under a set of assumptions introduced in theorems there is no clear guideline on the choice of p.  The paper is well-written and easy to read. My two main concerns are: - it seems that it solves a very particular case of a much bigger problem. - it's not actionable despite a suggestive title.