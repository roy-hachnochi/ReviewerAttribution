This work demonstrates how current learning algorithms perform when paired with human teammates on collaborative tasks. The authors introduce a collaborative environment based on the game Overcooked in which a team has to cook and serve meals. Current techniques that use self-play and population-based training trains agents to perform well against similar agents, but this training does not necessarily transfer well to a team that includes a human, who may not play optimally or similarly. The authors first collected human-human gameplay on the game and used a subset of the data to train a human model using behavior cloning. Results on multiple layouts of the game showed that agents trained with an approximate human model performed much better than when trained with other agents. A user study further confirmed that an agent trained with a model of the human performs better with real users.  Strengths: - Incorporating models of humans into AI learning systems is an important direction for the field. - The game Overcooked can be a testbed for others interested in collaborative environments, if released to the public. The qualitative descriptions of the strategies a model learns through self-play is interesting and valuable for understanding why these models fail with humans.  Weaknesses: - Demonstrating that an agent trained with a human model performs better than an agent assuming an optimal human is not necessarily a new idea and is quite well-studied in HRI and human-AI collaboration. While the work considers the idea from the perspective of techniques, such as self-play and population-based training, the authors need to justify how this is significantly different from prior work. - The idea and execution is simple. The model of the human is basic, which is fine if the idea itself is very novel, but there are many works on incorporating human models into AI learning systems.  Originality: While the work is set in the context of more recent algorithms, the idea of modelling humans and not assuming humans are optimal in training is not a new concept. There are several works in a similar area, so it would be important to differentiate the work with many prior works. - Koppula, Hema S., Ashesh Jain, and Ashutosh Saxena. "Anticipatory planning for human-robot teams." Experimental Robotics. Springer, Cham, 2016. - Nikolaidis, Stefanos, et al. "Efficient model learning from joint-action demonstrations for human-robot collaborative tasks." Proceedings of the tenth annual ACM/IEEE international conference on human-robot interaction. ACM, 2015. - Freedman, Richard G., and Shlomo Zilberstein. "Integration of planning with recognition for responsive interaction using classical planners." Thirty-First AAAI Conference on Artificial Intelligence. 2017.  Quality: The paper had overall high quality. The authors paid attention to details about the approach and included them in the text, which helped to understand the full procedure. It was unclear what the imitation learning condition was. Is that an agent that acts exactly as if it were a human based on the trained human BC model? If so, it seems like an inappropriate baseline since the premise of the work is that an agent is collaborating with a human rather than acting like the human acts.   Clarity: The paper was written clearly. In terms of terminology: It seemed like BC and H_proxy were both trained using behavior cloning, which made the names a bit of a misnomer. In the Figure 3 caption, the hyphens made the explanation confusing. There were a few typos, included below, but overall, the approach and results were explained well. - Pg 6: taking the huaman into account → taking the human into account  - Pg 6: but this times → but this time - Pg 7: simulat failure modes → similar failure modes  Significance: Modelling humans when training AI systems is an important topic for the community, as many of our trained models will have to work with people while current algorithms do not always handle this. So, the general idea is definitely significant. The main concern is the originality of the work compared to prior work on modelling humans in collaborative tasks for better team performance.  Other comments: - What does the planning baseline add to the story? - Was the data collection for the 5 layouts randomized? It sounds like the data was always collected in the same order, which means there may be learning effects across the different layouts. - How did you pick 400 timesteps? ----------------------- I have read the author response, and the authors make good points about how the work's contributions still provide value to the HRI and related communities. Specifically, the authors discuss the importance of considering humans in more recent deep learning frameworks and how this provides new value compared to prior works that focus on modelling humans in planning-based frameworks, which is reasonable. I additionally appreciate the experiment that the authors conduct in order to compare their method to a noisy optimality condition used in prior work.