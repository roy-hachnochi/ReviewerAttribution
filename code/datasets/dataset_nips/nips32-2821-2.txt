The problem considered in this paper is well motivated, and the constraint of delay is of practical interest. Also, it looks like this is the first paper that extends existing literature into the stochastic parametric bandit setting.  Two things I would like to see, but do not appear are:  1. A lower bound for learning under stochastic delay. Although I understand that the regret bounds provided here collapse to the optimal regret bound when no delay exists, it would still be great if one can fully characterize the hardness of the problem.  2. A more interesting DTS analysis would go into the frequentist framework similar to [1]. Even for the Bayes setting described here, it would be helpful if the authors could describe in details what is the unique difficulty of extending the results from DUCB to DTS if one simply wants to adopt the reduction framework proposed in [2].  [1] S. Agrawal, N. Goyal, "Thompson Sampling for contextual bandits with linear payoffs". In ICML 2013.  [2] Russo D, Van Roy B, "Learning to optimize via posterior sampling". in Mathematics of Operations Research 2014.