This work asks the following fundamental question about ML classifiers and data distributions: how close is the nearest in-distribution error? Test error, as measured by a random iid sample from the data distribution provides an estimate of the volume of errors in that distribution. Given that this volume exists there must be a nearest error to a given input within that volume. Counter-intuitively, for many high dimensional data distributions the nearest error is expected to be significantly closer than the nearest sampled error in the test set. For several synthetic distributions, rigorous bounds can be proved which imply that small adversarial perturbations must exist for any model with a non-vanishing error rate on the test set---a phenomenon known as concentration of measure. A fundamental open question since the publication of https://arxiv.org/abs/1801.02774, is whether or not similar bounds can be expected to hold on real-world data distributions. This work takes the first steps towards proving such bounds.  Proving such a bound is extremely difficult for traditional ML datasets because the underlying p(x) is unknown, one only has access to a finite sample. Remarkably, the authors have managed to develop a general theory which works with just a finite sample and any metric on the input space. Summarizing the proof technique is difficult as it is somewhat involved and requires many careful definitions. To be honest, in the limited time I had to review this paper I was not able to check the proofs in detail. However, the definitions are clear, and the overall proof strategy is natural yet very clever. While the result is general, it requires  constructing a sequence of subset collections on the input space which match certain properties. Verifying that these properties hold is computationally prohibitive, and so the authors provide an approximate algorithm, which means the resulting bounds may also be approximate. The authors use their theory to prove bounds on adversarial robustness on the MNIST and CIFAR-10 datasets.  Perhaps the strongest bound they prove is that a 6% error rate on CIFAR-10 implies that l infinity 16/255 robustness cannot exceed 71%. This result is quite remarkable, and I think the authors have undersold the significance. The most recent claimed SOTA on CIFAR-10 is 57% robustness (8/255) at 13% natural error rate (https://arxiv.org/pdf/1901.09960.pdf). While there still is a gap, this is suggestive that recent work may be approaching fundamental limits on adversarial robustness in terms of imperfect generalization. It is important to note that the authors make no assumption about the shape of the error set, it seems likely that by making additional assumptions about the error set (given that it must arise from an ML classifier) that one could close the gap even further. Additionally, l_p perturbations are only a toy threat model with little to no real world significance. Actual adversaries rarely restrict themselves to tiny perturbations of an input, see for example this documented attack on YouTube's ContentID system https://qz.com/721615/smart-pirates-are-fooling-youtubes-copyright-bots-by-hiding-movies-in-360-degree-videos/. With all of this in mind, this work does raise questions as to why the research community has invested so much time and effort specifically on the problem of l_p perturbations when there is still much work to be done improving model generalization (and in particular o.o.d robustness https://arxiv.org/abs/1901.10513).  Specific comments/questions:  It is worth including a discussion on https://arxiv.org/abs/1805.12152, which many researchers point to as refuting the concentration of measure hypothesis. Empirically, it is the case that adversarial training does degrade generalization while improving l_p robustness. However, this is not at odds with concentration of measure. While there is a sizeable gap between naturally trained models and any fundamental bound on robustness based on concentration of measure, this theory may explain why adversarially trained models still lack robustness at larger epsilon.  I would also be a bit more specific in the introduction that lp perturbations are a toy threat model that is not intended to model how actual adversaries choose to break systems. The popularity of lp-robustness has, in my opinion, distracted the research community from focusing on the more pressing issue: distribution shift (https://arxiv.org/abs/1807.01697, https://arxiv.org/abs/1907.07174).   Statement of Theorem 3.3 (and other places): The notation for product measure was not defined.  Definition 2.1: Any reason the authors defined an adversarial example as the nearest input which is classified differently, and not the nearest error? While subtle, the second definition is more general and will become necessary as robustness improves. For example, on the concentric spheres distribution it was observed that in some settings the nearest error was actually farther than the nearest input of the other class. Many adversarial defense papers declare success when the optimization algorithm starts producing inputs that visually look like the other class, but it is usually the case that if one searches differently, adversarial perturbations can still be found. See for example the background attack in Figure 6 of https://arxiv.org/pdf/1807.06732.pdf.   Table 1: It's worth noting that MNIST may be a degenerate case with respect to the l_infinity metric. In particular, a trivial defense is to first threshold the inputs about .5 and classify the resulting binary image. Because of this, I would not expect any meaningful bounds to hold for this dataset and metric.