I think the method is interesting, since the use of dilated distance functions it allows interpreting the Mirror Descent type iteration as one of minimizing local regret (at the cost of growing the state space). In addition, a theoretical result of strong convexity is obtained in the case of the Euclidean norm. As far as I can see, the mathematical arguments seem correct.  However, I have two questions, regarding the results: -I do not understand where is used the flexibility given by the choice of the $m^t$. This is important, since part of the theoretical work consists in adapting some known results to that case. What kind of vector did you use in these cases? Without further use, it seems that this have been done because it was technically feasible.   -The fact that OOMD-type methods do not work in deep games (but those of the OFTRL type do) implies that decomposition as local minimization does not seem to have an important experimental effect. So what could be the advantage of such idea in the general case?   %%%%% After Rebuttal  After reading the authors' responses and looking at the others reviewers comments and discussions, I agree to change my score. This is a good submission. 