The paper is a rather straightforward extension of the well known transformer network for time series forecasting. However, it precisely targets two major limitations of the original algorithm, and the proposed improvements handle them effectively. As a result, it shows a significant improvement over the state-of-the-art methods such as DeepAR, especially for datasets which require long term dependency modeling.  The paper is clearly written and the quality is high in all aspects. Readers can understand the benefit of each proposed component, thanks to carefully designed experiments. I think it is a significant contribution for the community, demonstrating the potential of transformer networks for time series forecasting.  Some questions:  Could you provide the dimension of covariate vectors x for each of the experiments with some details?  Which positional encoding scheme was used? Exactly the same as the formula in p6 of the original transformer paper?  It seems the performance of the proposed algorithm for electricity-f_1d and traffic-f_1d in Table 3 and 4 do not match. Is the kernel size different?  What was the window size for electricity-c and traffic-c experiments? The full history length? Could you provide more details?  Would the performance with k = 1 in Table 2 be almost the same as the one of the original transformer network? I assume a sparse attention was used in Table 2, but the performance should be equivalent or better with full attention model according to Table 3. Do you have exact numbers for the original transformer network? 