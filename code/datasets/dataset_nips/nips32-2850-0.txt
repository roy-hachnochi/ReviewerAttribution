The paper is nicely written, motivates incremental few-shot learning problem, and makes a couple of interesting architectural/algorithmic contributions. The results are interesting/convincing, and the ablation study helps to better understand the properties of the prosed method. I believe the paper would be of interest to the community but more clarifications are necessary.   === Comments and questions:  - Inconsistencies in softmaxes. From 3.1, it looks like the softmaxes computed used for training fast and slow weights have different normalization constants (see line 135 and line 139), and hence the logits for b-classes might have entirely different scales than logits for a-classes. I feel that the reason why you need the proposed attractor-based regularization in the first place is to compensate for this scaling issue. Alternatively, you can use the same softmax from line 139 when computing the loss in eq (1) to avoid this discrepancy (since the base classes are available, W_a is pretrained and fixed, this should be possible). Why not do that? I would like to see a comparison with the vanilla architecture (i.e., no attractor-based regularization) that uses a consistent softmax normalization.  - Computational considerations. RBP requires the inverse of the Jacobian wich scales cubically. What is the computational overhead for this method? How would it scale with the increased number of classes? Analysis and discussion of this are necessary.  - Results. The authors mention that tiered-ImageNet is a harder task (which intuitively makes sense), but somehow results on that dataset are better than on the mini-ImageNet. How would you explain that?  - Will the code for reproducing results be released?   === Minor: - line 124: I believe \theta_E denotes parameters of the feature extractor, but it has not been properly defined.