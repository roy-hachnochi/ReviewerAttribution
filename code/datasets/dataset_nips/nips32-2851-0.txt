UPDATE after reading response --  Thank you to the authors for the detailed response.  It addresses most of my concerns.  I hope the authors do include a discussion of effect sizes as they suggest in the response, since effect sizes are perhaps the most important thing to assess for a problem like this.  I now see I misunderstood the importance of the assignment mchanism's confound in experimental design, compared to simple random assignment analysis; thank you for that clarification.  The paper would still be strengthened if it related the problem to how it's addressed in the causal inference and experimental design literature, but the work is still a worthwhile contribution on its own.  ================== Contributions  1. A simulation illustration of possible false positive errors in a widely known analysis of a WSDM double blind peer reviewing experiment. 2. Theoretical results about the challenges in FP errors when analyzing randomized reviewing experiments. 3. A proposed approach for analyzing single versus double-blind peer review, based on paired permutation tests, that has better FP error rates.  ==================================  The authors examine the problem of analyzing the effects of double blind review, in comparison to single blind review.  Much of this work focuses on Tomkins et al., PNAS 2017, which analyzed the results of a randomized experiment for WSDM reviewing that randomly assigned reviewers to single and double blind reviewing.  The authors point out major difficulties in this sort of test, and in particular, a number of flaws in Tomkins' analysis approach, and conduct a simulation showing it could have high positive error rates (much higher than the nominal p-values).  The authors main contribution, I think, is a paired permutation test for assessing the statistical significance of a possible difference in review scores between the conditions (Avg Treatment Effect), conditional on a property of the paper, such as whether the first author is female; such properties constitute important substantive properties for assessing the efficacy, fairness, etc., of double-blind review.  This paper also delineates some of the major issues in peer review analysis, concluding that it is difficult to solve the assignmnt problem (which I think is, the degree of control by reviewers over which papers they review).  I can't tell the significance of this work.  I do like the problem.  I think the case for it to be in scope is because reviewing practices are of interest to the NeurIPS community.  The scholarship is very weak.  The theoretical investigation of the size and power of various tests feels like there should be a very large literature from the statistics and social sciences that's rlevant -- experimental design, null hypothesis tests for GLMs, etc.  The authors make a few dismissive citations about regression, but little other discussion.  And there are tons of randomized field experiments in the social sciences with some similarities to SB vs. DB review.  Most obviously, the famous "audit" studies in economics, where names of (fake) job applicants, in their resumes, were randomly manipulated to test for ethnic and gender bias (e.g. by Mullainathan, but there's many more?).  Did all those social scientists have flawed analyses too?  What's so special about the SB/DB problem that social scientists haven't addressed in other contexts?  In economics they have all sorts of ways of addressing correlated error terms, for example.  The originality seems a little weak.  The main proposed approach for testing bias, with a permutation test, is good, but perhaps a little obvious, at least if you know what a non-parametric permutation test is?  (There are no citations about nonparameric testing, AFAICT.)  The better contributions, in terms of originality and quality, are the theoretical analysis of issues like correlated measurements and confounded review selection.  The paper would be much stronger if it adopted clearer theoretical principles from the causal inference literature.  For example, the notion of what is confounding really relates to what the idealized random assignment experiment is; I'm not sure that reviewer assignment is necessarily a confound in that light, since presumably topical expertise will still be a factor in any future reviewing organization approach; perhaps this should be more clearly discussed?   I have not read, and did not read, the Tomkins paper.  Going by this paper's presentation of Tomkins' analysis method, I found the arguments about its flaws convincing.  Averaging the scores for only one of the conditions indeed seems egregious.  The authors' approach of comparing scores of individual reviews against one another is much better grounded.  I'm sold that some of the methods in this paper are better than Tomkins' approach to the data analysis, but I'm not sure that means it's a big advance -- maybe Tomkins is just weak.  If PNAS did crappy reviewing of their own papers (likely), that's their problem, not NeurIPS's.  (The authors might want to submit this to PNAS as a response to Tomkins.  That's a whole other can of worms, though.)  I don't think the permutation approach in this paper is the only way to do it -- you could also imagine a logistic regression approach, using every individual review (both SB and DB) as a data point with latent quality variables and random effects, that overcomes the major issues in Tomkins' approach.  I think this would be the most popular approach in relevant areas of the quantitative social sciences.  It would have an advantage over the appraoch here of reporting effect sizes, as well as p-values; effect sizes, of course, are of great interest.  I personally believe the approach of only caring about p-values or Type-I error rates is insufficient and sometimes dangerous (see, for example, the ASA's statement on p-values several years ago, or a zillion review articles in the psychology methods literature on this right now).  The authors do not make clear what their proposed analysis approach is; they don't really mention effect sizes, so I have to assume they are missing them, which is a weakness of the proposal.  To be more generous, maybe the authors intend their approach to provide p-values alongside simple empirical estimates of the effect sizes?  Unfortunately, a fleshed-out analysis method isn't really presented.  In sum, there's something interesting here, but it doesn't feel ready to be published. 