This paper works on an issue of the testing framework used in Tomkins et al. for evaluating reviewer bias in peer review. It points out the issue of false positives with measurement error, model mismatch, reviewer calibration, and reviewer assignment. It is shocking to observe the extent of false alarm probability. It then proceeds to develop a novel model-free algorithm that is robust to the first three potential issues.  Strength  * This paper is well written and clearly present the issue at question.  * The proposed algorithm is simple and intuitive, with strong guarantee, and is also validated in empirical simulations.  * This proposed framework is general and can be applied in other settings such as hiring and admission.  * Although it is a negative result, the perspective raised in 5.2 is quite intriguing and addresses a question that I had during reading the paper, i.e., SB and DB can be different simply based on the assignment into these two groups.  Weakness  * All the experiments are done by simulation.  * It would be useful to develop a quantitative understanding of the influence on power.  Additional questions  * I was curious whether the matching process in the algorithm could introduce any bias similar to paper assignment process, or whether there is anyway to examine the quality of matching.  Minor presentation related suggestions:     "control for false alarm probability" sounds strange to me. "guarantee" seems more appropriate at most places.