This paper proposes a new method to learn robust representation for person re-identification problems by separating features for human identity and the others via learning generators of human images. Probably it learns GAN-based models to generate generalized human images to be robust in variations of pose and occlusion. The idea is very inspiring for applying it to other similar visual surveillance applications such as of view-point invariance or outfit-invariance.  This paper is well-written and concisely focused on the main goal.  However, it needs more detailed explanations for reproducibility. Specifically, the part of domain discriminators is not clear. What is the meaning of the sentence 'we add convolutional and fully connected layers for the class discriminator."? How to configure patchGAN for them?  In subsection 'visual analysis for disentangled features', it would be helpful to show shuffling of two people with different colors and styles of dress as general cases for a deeper understanding of the proposed methods.  I'm curious what kinds of effects are shown about camera angles, viewpoints, or outfits if possible.  Here are minor comments: There are some typos and grammatical errors.  It would be helpful to proofread by native speakers. In Figure 4, the box boundaries in green or red are too thin to be seen clearly.  It would be better to make them thicker a little.  line 26: focussed -> focused line 38: argumentation -> augmentation  POST-REBUTTAL: The authors addressed all of my concerns. I've raised my score 1 higher. I am asking the authors to merge the materials of the rebuttal into the manuscript.