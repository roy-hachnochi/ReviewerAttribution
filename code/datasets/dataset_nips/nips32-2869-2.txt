Summary This paper proposes CFCNet, a method to conduct sparse depth completion from sparse depth and the corresponding RGB, leveraging the relationship between them. The authors first demonstrate the structure of CFCNet. The basic idea is to use convnets to extract features from corresponding RGB and depth, minimizing the channelwise 2D^2CCA loss. A transformer network is introduced to map RGB features to depth features, and it is then used to transform the RGB image into depth. The authors then introduce 2D^2CCA, which is an extension of CCA when the input size is of high-dimensional. Finally, experiments on a bunch of datasets (both indoor and outdoor scenes) show that the proposed method achieves the state-of-the-art performance.  Strengths -The depth completion task is important, and the paper provides a method with good performance. -The method is novel. The authors map RGB and depth to two latent spaces where they are highly correlated, so that mapping between the two latent space becomes easier. -The visual results look impressive. As shown in figure 6 and the supplementary, even when testing on real images, results from CFCNet looks impressive, contains much more details compared to previous methods.  Minor issues -There are many “directional”s in this paper, e.g., two-directional, and I still do not understand what they mean. -May reorganize all the notations. For example, F_{s_I} is introduced in line 148 but not in figure 2. Putting the notations into figure 2 might help readers to understand section 3.2 easier.  Comments after the rebuttal ************************************ Thank the authors for the rebuttal. The visualized results look great and still have space for improvement. I agree with two other reviewers that this is overall a good paper, and my overall score remains the same.  