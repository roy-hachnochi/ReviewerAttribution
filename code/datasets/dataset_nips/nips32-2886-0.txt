Main Ideas The high-level motivation of this work is to consider alternatives to learning good forward models, which may not be a desirable solution in all cases. The hypothesis is that a predictive model may arise as an emergent property if such prediction were useful for the agent. The authors test this hypothesis by constraining the agent to only observe states at certain timesteps, requiring a model to learn to fill in the gaps. The model was not trained with a forward prediction objective.  Connection to Prior Work. The method introduced in this work seem novel in the context of other literature that train forward models. Other work have also attempted to overcome the difficulties with training forward models, such as by using inverse models [1]. The predictron [2] also learns an implicit model as the submission does. Would the authors be able to include a discussion comparing their work with the two above types of approaches ((1) inverse models and (2) implicit models) in the related work section?  Quality     - strengths: the authors are careful and honest about evluating the strengths and weaknesses of their work. They evaluated their idea on simple easy-to-analyze tasks and also demonstrated the generality of their method on various domains.  Clarity     - strengths: the paper is very well written and motivated  Originality     - strengths: the framing of the problem and the method the authors use seems novel  Significance     - strengths: this work is a proof-of-concept that training an implicit model with observational dropout in some cases is sufficient for learning policies     - weakness: one of the appeals of learning a world model is that such models help facilitate generalization to different tasks in the same domain. For example, one task could be to train on car-racing going forwards and test on car-racing going backwards, where a forward model that is trained to predict the next state given the current state and action could presumably handle. However, the implicit model studied in this submission is inherently tied to the training task, and it is unclear whether such implicit models would help with such a generalization. Would the authors be able to provide a thorough experiment analyzing the limits and capabilities of how their implicit model facilitates generalization to unseen tasks?  Overall, I like the perspective of this paper and I think it is well written, well-motivated, and thorough. The key drawback I see is the lack of analysis on how well the proposed method fares on generalizing to unseen tasks. This analysis in my opinion is crucial because a large motivation for learning models in the first place is to facilitate such generalization.  [1] Pathak, Deepak, et al. "Zero-shot visual imitation." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018. [2] Silver, David, et al. "The predictron: End-to-end learning and planning." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.  UPDATE AFTER REBUTTAL I appreciated that the authors have conducted the experiment comparing their proposed method with a model-based baseline. I think a more thorough analysis of generalization would make the paper much stronger and I believe the significance of this work is more of a preliminary flavor. The paper is executed carefully and the results were consistent with the author's claims, but this is only the first step. I keep my original score, but would agree with Reviewer 3's comment about motivation and urge the authors to provide a more compelling argument through empirical evaluation for why one would want to consider the proposed idea in the camera-ready.