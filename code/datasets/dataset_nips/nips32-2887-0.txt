Update: I have carefully read the author's rebuttal.  I maintain my rating (good submission, promising ideas, would be better and more compelling if the empirical results were stronger).  Overall this is well-written and makes interest theoretical and conceptual contributions.  It seems likely to spark future work along these lines.  It also seems of some practical significant in that variance reduction techniques have generally not previously been shown to have much success in improving the training of deep learning methods.  Whereas, Figure 3 indicates that IGT was able to significant improve the training of ResNet for CIFAR10, for example.  Curiously, the authors only examined ITA-Adam for IMDb and MAML, and not also for CIFAR10 and ImageNet.  Especially given they are suggesting that ITA is a good plug-replacement for gradients, it would be interesting to see whether ITA generally improved not only Heavy Ball (classical momentum) but also Adam, across all the tasks they examined.  The absence of Adam-ITA for the conv net tasks (CIFAR and ImageNet) makes readers wonder if ITA does not work well for Adam for those cases, and if so why not.  If Adam-ITA does not perform well for them, that would be interesting to examine, to better understand limitations of ITA, which are currently not examined very well in this work as is. That is, ITA seems to always be at least as good or better than the non-ITA base optimizers, but it is not clear if there is fundamental theoretical reasons that will always be the case -- especially since the impact of violating of the assumption of equal Hessians (even when addressed with anytime tail averaging) in practice is not clear or bounded in general.   