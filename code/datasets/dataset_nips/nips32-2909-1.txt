Originality: As stated in the contributions, the paper presents an approach to OPE that is a departure from classic approaches. As far as I am aware this is a very novel way to address this issue  Clarity: I found the paper to somewhat lack structure,  which hurts readability and clarity:  - The description of the positive unlabeled learning and its application to OPE (section 3) is rather convoluted and not always easy to follow. This section could use some intuitive examples and a more detailed step-by-step development.  - Overall structure: Section 3 ends with short disconnected section 3.3 on metrics, then moves on to a largely unrelated section 4 motivating applicability, before section 5 discusses related work and more metrics.  Significance & Quality: The paper addresses an important problem using a novel approach. It will hopefully inspire new approaches to the problem. The method as presented here do have some important limitations: - The method is restricted to deterministic systems with a single binary success reward per episode. As noted by the authors this setting is quite restrictive. Even though many problems may be interpreted as binary success problems, the inability to deal with stochasticity in the problem may have a large impact on applicability. It would also be good to test how important deterministic dynamics are to the performance of the method in practice. - The approach seems to simply ignore one of the key issues in OPE, i.e. the distribution mismatch in the data. In section 3.1, the authors note that they simply assume that the classification error is a good enough proxy and that it seems to be robust in their evaluations. This assumption is not sufficiently tested. It would be good to explicitly evaluate robustness in the Tree example and explicitly evaluate how / if the method degrades when the data is generated by policies that are not very broad/random or far away from the target policy.  - Unlike existing OPE approaches, the method does not seem to offer strong theoretical guarantees / bounds on performance. - It was not clear to me how data efficient the method is. Lack of data is a key issue in the settings considered in the paper (i.e. where no simulation for the real test problem is possible), so it would be good to see how the performance depends on the amount of data. 