Originality: The work seems novel, introducing both a new metric for OPPE, and a new way to benchmark deep RL algorithms Quality: Given the assumptions provided, the proofs all seem correct, and the method seems to work in the settings advertised. Clarity: The proofs and arguments are all easy to follow. It was hard for me to understand how the correlation coefficient, and I hope I reached the right understanding. Perhaps a more detailed series of steps as I outline below would help clarify the steps needed. Significance: If this is more generalizable to less restrictive assumptions, it could be a very novel way to do OPPE, and might advance the whole domain.  L72: Why does the \gamma = 1 restriction exist? It doesn't seem to be used in the proofs, and the experiments seem to have bene done with \gamma != 1.  The assumption that the transition matrix P being deterministic does not seem like a good assumption for many real world problems, although it's likely not a bad one for the specific robot grasping task. For example: due to imprecision in hardware, the same motor command in the same state twice will probably lead to a set of states. Specifically in the tasks chosen, there is very little stochasticity - the robotic hand would probably end up in a close enough position to disregard the stochasticity. On complex real world tasks, you might see a significant amount of stochasticity, which hurts the argument of this paper. The restriction is therefore quite strong and seems somewhat core to the proofs. I do not see a simple way to extend the work to include stochastic transition matrices.  It seems that the metric changes according to how the Q functions were trained. If I understand correctly, the measurement is this: - A few Q functions are trained according to different hyperparameter sets - They are all evaluated on policy to obtain the "true average return" - They are also evaluated on some off-policy data to obtain the metric values - The correlation between the true average return and metric values are obtained. This implies the evaluation of novel techniques in this framework depend heavily on the set of Q functions trained. Thus, it's hard for new works to reproduce the results here, since if the choice of algorithm changes, then so might the ranking among algorithms.  I'm confused to why some of the baselines have a strong negative correlation with the true average return. I wonder if the authors can analyze this.