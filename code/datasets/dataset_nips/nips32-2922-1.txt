** Update **  Thanks to the authors for their elaborate response. I'm excited to see that the construction of the IC neural network is automatic, and I'm looking forward to seeing an example that illustrates how this is done, as well as other examples that give more intuition about the system.  With respect to lines 18--20 of the rebuttal: I appreciate the intention to open source the system and provide URLs in the revised version of the paper. However, it is also important to provide enough details about the system in the paper itself (this also has the advantage of being a form of documentation of the system that you can link to).  In addition to the points made in the review previously, I also agree with the comments made by Reviewer 3, and I hope that the revised version of the paper (for this or another conference) will include a clearer description of the PyTorch PPL, and of the work that needs to be done by the user when connecting an existing simulator to the system.  ************  I am excited to see probabilistic programming applied at such a scale, and I'm looking forward to it hopefully being more widely adopted in the sciences. I didn't find any obvious mistakes in terms of technical details. My main issue with the paper, which is also why the score I give is not higher, is that it is quite difficult to read and quite abstract, leaving a lot to the reader to figure out themselves.   Firstly, the paper can hugely benefit from an example of the PPL it mentions. Figure 1 provides a good intuition of how to overall framework works, but it is too abstract on its own. It is very confusing to me what the purpose of the PyTorch PPL is, as my impression was that already written simulator code is used as the probabilistic model. Other parts of the paper can also make use of some examples, e.g. the way the addressing scheme works, the rejection sampling problem, etc.  Secondly, the paper gives a very good explanation of the inference compilation (IC) technique that it uses for efficient inference. However, it was not clear to me how much of it is left to the user to implement, and how much of it is automatic. For example, does the user have to build or tune the IC neural network themselves? Do they have to adjust the training procedure when "prior inflation" is needed (lines 198--208), or in the presence of rejection sampling code (lines 247--259)? If this is not automatic, what tools does the framework provide to assist with this?  In general, if I have some C++ simulation code I want to use with the framework, how much extra work do I have to put in to make the cross-platform execution possible?   The paper describes a probabilistic programming framework but it applies it to a single example (albeit an impressive one). Without a clear description of the system it is difficult to judge how general the framework is.    ** Minor suggestions for improvement** * The elements, and especially the text of Figure 2 are too small to read.  * Line 93: the PPL using TensorFlow Probability is Edward2 (Tran, 2018), which is perhaps the more accurate name and reference to use here. * Line 97: "IC" -> "inference compilation (IC)". * Reference [37]: A more up-to-date reference here is Carpenter (2017). * For papers published at a peer reviewed venue, consider citing the published paper instead of the arXiv one (e.g. [66]).  * Some references are not capitalised correctly (e.g. [39]: "gibbs sampling" -> "Gibbs sampling").  Tran, Dustin, et al. "Simple, distributed, and accelerated probabilistic programming." *Advances in Neural Information Processing Systems*. 2018.  Carpenter, Bob, et al. "Stan: A probabilistic programming language." *Journal of statistical software* 76.1 (2017). 