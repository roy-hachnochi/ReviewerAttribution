The working topic on evaluating paper reproducibility is important for an increasing research community. This paper conducts an empirical study on what impacts the reproducibility of research papers. However:    1) The definition to reproducibility is vague. They say that a paper is reproducible if they managed to reproduce its main claims. What does it mean exactly? Come close to reported performance scores? Beat competitors?   2) How is it possible that you reimplement the codes of 255 papers independently without looking authorsâ€™ code? This appears as an enormous labor effort.   The paper is mostly clear and has good quality. I am not entirely sure about its significance for NIPS as the topic is not directly concerned with machine learning, but rather with a meta-introspection. One might think that a workshop like  https://sites.google.com/view/icml-reproducibility-workshop/home could be a suitable venue as well. 