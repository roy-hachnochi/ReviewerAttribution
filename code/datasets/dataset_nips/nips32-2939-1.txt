I found this paper quite interesting and I think the contribution is quite original and appealing to the community. The paper is nicely written, easy to follow and it is evaluated in a fair number of challenging scenarios and multiple methods.       Mi main criticism is the lack of comparison of previous "local" Bayesian optimization methods. Bayesian optimization with a dual (local and global) strategy, or with a locally-biased strategy has been previously explored in the past by several authors. Just to give some examples:       -K. P. Wabersich and M. Toussaint: Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and Length-Scale Cool Down. NIPS Workshop on Bayesian Optimization, Preprint at arxiv.org/abs/1612.03117, 2016.       -Martinez-Cantin R. Funneled Bayesian optimization for design, tuning and control of autonomous systems. IEEE transactions on cybernetics. 2018 Feb 27(99):1-2.       -Acerbi, L. and Ma, W. J. (2017). Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search. Proc. Advances in Neural Information Processing Systems 30 (NeurIPS â€™17), Long Beach, USA.       -Akrour R, Sorokin D, Peters J, Neumann G. Local Bayesian optimization of motor skills. InProceedings of the 34th International Conference on Machine Learning-Volume 70 2017 Aug 6 (pp. 41-50). JMLR. org.       -McLeod, M., Roberts, S. and Osborne, M.A.. (2018). Optimization, fast and slow: optimally switching between local and Bayesian optimization. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:3443-3452       The most related to this papers are the works of Wabersich and Toussaint; and Martinez-Cantin which also splits the model and the resources in a local and global region. Wabersich even computes the local region in terms of the accuracy of the quadratic approximation, similar to a trust region for quadratic algorithms, such as BOBYQA.        Also, in the introduction, it is mentioned that "... note that the commonly used global Gaussian process models implicitly suppose that characteristic lengthscales and outputscales are the function of constants in the search space...". That is also not the case in some previous works on nonstationarity in Bayesian optimization. For example, there are nonstationary kernels like the previous work from Martinez-Cantin; warping spaces like the work of Snoek et at.; or treed GPs like the works of Taddy et al. and Assael et al.       - Snoek J, Swersky K, Zemel R, Adams R. Input warping for bayesian optimization of non-stationary functions. InInternational Conference on Machine Learning 2014 Jan 27 (pp. 1674-1682).       - Taddy MA, Lee HK, Gray GA, Griffin JD. Bayesian guided pattern search for robust local optimization. Technometrics. 2009 Nov 1;51(4):389-401.       - Assael JA, Wang Z, Shahriari B, de Freitas N. Heteroscedastic treed bayesian optimisation. arXiv preprint arXiv:1410.7172. 2014 Oct 27.       In fact, the whole discussion about RL having sparse rewards and therefore requiring a nonstationay process is also the motivation of Martinez-Cantin's paper.        Instead, the authors focus on the parallelization of the algorithm, which seems secondary, distract from the main point of the paper and leads to some questionable decisions in the experimental process. For example, while I praise the choice of "non-standard" algorithms for comparison, the fact that they replace the EI acquisition in BOHAMIAN and BOCK for TS, which is known to be less effective in the sequential case. Furthermore, most of the experiments presented are actually sequential, such as the robot pushing (one would probably have a single robot) or the rover planing (those plans are computed in limited onboard computers). Following with the experimental section, there are a couple of minor comments:       -The results of EBO for the robot experiments are quite different from the results on the original paper, specially the variance of the rover planning.       -Given that the objective of BO is sample efficiency, it would be interesting to see the results of a standard GP+EI, maybe limiting the results to few hundreds of evaluation. In theory, the 60D problem should be intractable, but the 12-14D problems could be solved with a standard GP.       -Why using COBYLA instead of BOBYQA from the same author? BOBYQA should be faster as it uses quadratic functions. It assumes that the function is twice differentiable, but so does the Matern 5/2 kernel.       -In all the problems. TuRBO gets a different number of initial samples. For example, in the rover case, all methods gets 200 while TuRBO-30 gets 3000, which is one order of magnitude more. This seems unfair.        Regarding the method, the only comment I have is with respect to the bandit equation, which is purely based on the function sample and not the information/uncertainty on that region. That might result in a lack of exploration and poor global convergence, specially because the sample is based only on the local GP. Wouldn't be better to express the bandit equation in a exploration/exploitation dependent function such as a global acquisition function? Maybe that could explain why TuRBO requires such a large initial set.        Despite all my comments, the results are quite impressive.              ---- Update: Most of my concerns have been properly addressed by the authors.