Authors propose the model-parallel gradient descent algorithm for the purposes of speeding up training of Transformer based language models. Overall the paper is well written and experiments are convincing in demonstration of validity of the approach.  My main question is whether authors have tried training much larger Transformer models that don't fit into one GPU using their algorithm