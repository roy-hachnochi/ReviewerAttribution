This paper proposes a theoretical framework which combines Semi-Supervised Learning (SSL) and Distributionally Robust Learning (DRL). The authors added an entropy regularizer on the unlabeled data, and analyzes the generalization error of the solution to an Lagrangian relaxation. The generalization bound is based on two new complexity measures, semi-supervised monge (SSM) complexity and minimun supervision ratio. The authors showed the connection of SSM and VC dimension, which is nice. The proposed algorithms hows a comparable performance to those of the state-of-the-art on a number of real-world benchmark datasets.  Major Comments: It is unclear why the proposed SSM measure can explain the help from the unlabeled data. Consider a purely supervised algorithm using only n*eta labeled data from the dataset and just optimizes the supervised part of the loss. We can use the standard Rademacher complexity to derive a generalization bound. How does this bound compare to the SSM-based generalization bound? Under what condition does unlabeled data help generalization?   Minor Comments: 1. It seems possible to define SSM complexity without the distribution robustness (so it can be applied to general semi-supervised learning). Is there any existing work about this?  2. The minimum supervision ratio has a very implicit dependence on lambda and zeta, so I am not sure about how useful is this complexity measure. Is there a way to know MSR empirically? 