The paper presents an interesting idea to learn embeddings of hierarchies and ontologies based on quantum logic. I find the proposed formulation to be novel and a positive contribution.  However, I do not think that the experiments fully bear out the utility of the proposed approach. The loss function (10), if solved exactly, would capture the logical structure of hierarchies. However, because it is a non-convex problem, the paper solves it via stochastic gradient descent. Hence, there is no assurance that an optimal solution is always found, and thus the proposed approach suffers from the same problem (perhaps to a lesser degree) as other embedding approaches, i.e., "no guarantee that embeddings maintain the sanctity of the logical structure" (line 35-36). Because of this, I think it behooves the paper to compare itself against other embedding approaches that attempt to incorporate hierarchical structure (in addition to non-hierarchical approaches like TransE and ComplEx that were used as baselines in the paper). The citations on line 326 (i.e., [32, 33, 34, 35, 10]) provide a list of possible hierarchical embeddings approaches that could be compared against. Two additional ones are:   (a) PoincarÃ© Embeddings for Learning Hierarchical Representations. Maximilian Nickel, Douwe Kiela. NIPS, 2017.   (b) Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry. Maximilian Nickel, Douwe Kiela. ICML, 2018.  I think it would have been instructive if the paper has compared against these "hierarchy-aware" approaches, because the comparisons would show how much better the paper's *approximate* quantum-logical constraints are versus other ways of embedding hierarchies.  I think a running illustrative example of the components of equation 10 on page 4 and 5 would greatly improve the comprehensibility of the paper.   Questions: 1. How does the paper's approach compare against (a) and (b) above?  2. Why did the paper not compare against the "hierarchy-aware" approaches mentioned above ([32, 33, 34, 35, 10])?  3. Page 6, line 222-227: By sampling invalid entities, isn't the paper implicitly also making the closed-world assumption, a shortcoming that the paper critiques on line 86-87?  4. Page 6, line 233: How does the paper "[find] all those entities"? By enumeration?  5. Page 6, line 242: How does confidence/probability correspond to projection length? There is no information about this in the paper or the supplementary material.  6. Page 7, Table 1: What exactly constitutes the "reasoning task" on LUBM? What must be inferred? How many steps of reasoning are required? How big is the training and test data? As it stands, this experiment is hard to replicate.   7. Page 7, Table 1: Why is the paper's approach not performing well on WN18?  8. Page 7, line 266, "better convergence": Why would you get better convergence with 3 negative entities per positive entity? What's a negative entity?  9. Page 8, line 291-293, "projecting each entity ... non-fitment score": Could the author elaborate on what this sentence means?   10. Page 9, line 315, "their black box nature": The paper's embedding is also a black-box, no? What interpretability does its embedding offer?   Nits: * Page 4 line 134: implies following -> implies the following (same mistake present elsewhere in paper) * Page 5 line 189: sufficiency -> sufficient * Page 8 line 310: bridges -> bridge  UPDATE: The authors' feedback addressed my concerns to a large extent (though I still feel more details won't hurt the reproducibility of the LUBM experiments). Hence, I've upgraded my overall score to "6: Marginally above the acceptance threshold". 