## Summary This paper consider linear-combination-of-features approximation to the Q-value function in a finite horizon reinforcement learning problem. While the linear approximation requires few parameters and scales efficiently with the state and action space dimension, when Q-valuate iteration is applied to it, the combination of least-squares projection and Bellman operator applications may be expansive leading to divergence. The paper proposes to use the linear approximation to the Q-function at a finite set of anchor states and linearly interpolate to other state-action pairs. This balances the compactness of a linear approximation with reduction in error amplification after Bellman updates due to the interpolation. The authors prove a bound on the error of the proposed Q-function interpolator with respect to the best linear linear-combination-of-features approximation to the Q-value function.    ## Recommendation The paper is well written and easy to follow. The idea of trading off linear approximation and extrapolation in linear-combination-of-features function approximation is interesting. The theoretical intuition about error scaling and control of the error amplification due to Bellman iterates is a good and novel contribution. A weakness of the proposed method is that its effectiveness still depends critically on the availability of good features \phi. The paper could be strengthened if ideas of how interpolation can be applied to other value function approximators were provided. The experimental evaluation is insufficient as only simple problems are considered and not nearly enough experimentation is provided to show empirically the error scaling, the effect of the anchor point selection, the advantage over averagers, or the dependence on the choice of \phi. More results should have been provided on the dependence between the anchor number and location optimization and the choice of features \phi. The theoretical derivation of the error bounds is illuminating in terms of the structure of error scaling but the bounds themselves do not seem practically useful.     ## Major Comments  + The introduction and preliminaries sections are very well written, giving the right context and a formal problem definition.    + The specification of the necessary number of samples to minimize the error between the interpolant and the best linear Q-function approximation in Lemma 2 is nice. The greedy approach to adding anchor points to meet a specified amplification target is a nice contribution.    - The greedy heuristic to construct a good set of anchor points before the learning process is a good addition to the proposed algorithm but is not explored in sufficient detail. A discussion on how complex the optimization problem max_{s,a} ||\theta^{\phi(s,a)}||_1 is should be added. Examples of the numbers, positions of anchor points, and the resulting extrapolation coefficient for common choices of \phi would have been nice to see.     - The experiments are similar and somewhat simple. It would have been interesting to evaluate LAVIER on a real scale problem in addition to the toy problems vs LS-AVI. For example, it would have been great to see how LAVIER performs on a reinforcement learning problem where a good choice of features is not known but polynomial, RBF, or kitchen sink features are used. A comparison versus averagers and/or OPPQ-Learning [15] would have been interesting as well. It would be have been good to provide more variations on the Chain MDP evaluation, in terms of different feature choices and different planning horizon choices.     ## Minor Comments:  - Typos: "Whenever the ineherent Bellman error is unbounded,"  - It would be cleaner to explicitly write the dependence on s_i and a_i in eq. (1)  - The meaning of \delta should be introduced more clearly in Lemma 2  - In Fig. 3 middle, what value of C is achieved by the shown anchor points?  - The notation is sloppy at places, switching from arguments in parentheses to subscripts for functions or from having two to one subscript (e.g., Proof of Proposition 3, around eq. (2)) 