 - - - - - - - - after rebuttal  - - - - - - - -  from reading the other reviews, I find that my concerns were similar to R3. The rebuttal clears up some missing parts on how to compute the matching and the influence of lambda. The intuition that the topological loss puts more weight on "difficult" regions is plausible. And the dilated baseline is a good addition to the experiments.  Overall, I am satisfied and increase my rating under the assumption that these points will be added to the final version of the paper.    - - - - - - original review  - - - - - - -   Overall the paper proposes and interesting loss, that is well motivated and which shows good performance on the evaluated datasets.   Weaknesses: W1 - Clarity/Reproducibility.  While the mathematical part of the method description is understandable and easy to follow, it is not clear how exactly the proposed loss can be implemented since many details are missing from the description. How is the optimal mapping gamma* computed? Exhaustively or with a smarter method? How is Dgm(f) computed? By sorting all probabilities and thresholding iteratively? How are small noisy components dealt with? These are important details that need to be understood to be able to reproduce the method. Since the evaluation is carried out patch-wise: how is the prediction across patches aggregated for the final full result?  Further missing detail: Figure 6a is missing a unit for time.  W2 - Evaluation. The hyperparameter lambda could be ablated to understand what the influence of the balance between the losses is. From the qualitative results it seems to me that it mostly makes the prediction “thicker” to keep the topology in tact. Can the same result be achieved by training with a dilated ground truth mask? Along the same lines, lambda=0 should also be evaluated to have a clean baseline of model performance without the proposed loss.  The reproducibility evaluation states that clearly defined error bars are included. However I cannot find any error bars in any of the plots and the tables do not report statistics such as std. dev. The would be valuable to understand the significance of the improvement.  W3 - Usefulness of the proof. I am not fully sure that there is much benefit in the proof. For example when you minimize the cross-entropy loss the topology between prediction and ground truth is also the same as prediction and ground truth will be identical in that case. In that sense it is not immediately clear what the benefit of the loss or the proof is. Further, line 40 states that the model has “guaranteed topological correctness”, which I don’t think is true - the Betti-error would be 0 in that case which it is not.  