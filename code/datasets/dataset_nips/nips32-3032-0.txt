Originality: The paper proposes a novel model for the recently introduced VCR task. The main novelty of the proposed model lies in the component GraphVLAD and directional GCN modules. The paper describes that one of the closest works to this work is that of Narsimhan et al., NeurIPS 2018 that used GCN to infer answers in VQA, however that work constructs an undirected graph, ignoring the directional information between the graph nodes. This paper uses directed graph instead and shows the usefulness of incorporating directional information.  It would be good for this paper to include more related work on GraphVLAD front.   Quality: The paper evaluates the proposed approach on the VCR dataset and compares with the baselines and previous state-of-the-art, demonstrating how the proposed work improves the previous best performance significantly. The paper also reports ablation studies ablation each individual contributed module of the proposed CCN model, demonstrating the usefulness of each component.   It would be useful if the paper could throw some light on the failure modes of the proposed model.   Clarity: I would like the authors to clarify the following -- 1. Based on the description in Sec 3.1, Graph VLAD needs the query LSTM representation as an input. However, this is not consistent with Fig. 2. Can authors please rectify the Fig. 2? 2. In Eq. 3, it seems like it should be b_j’, instead of b_k’ in the denominator. Can authors please comment on this and rectify accordingly? 3. Sec 3.1 – how are conditional centers (z1, …., zK) initialized? 4. L149 – can authors provide further clarification on how object features are extracted. What kind of pre-trained network is used and how is that different from the network used for extracting the feature map X for the image which is used in GraphVLAD? 5. How is the query representation Y define in L100 different from that Q~ defined in L154?   Significance: The proposed model is novel and interesting for a novel and useful task. The idea of GraphVLAD module and directional reasoning seem to be impactful and could be used for other vision and language tasks as well. The experiments demonstrate that the proposed model improves the state-of-the-art on VCR significantly.  --- Post-rebuttal comments ----  The authors have responded to and addressed most of my clarification questions. It would have been nice to see some related work on GraphVLAD front in the rebuttal too (very briefly) but I am trusting the authors to do justice to this in the final version of the paper.  Regarding the concerns from my fellow reviewers --  1. Connections to brain -- I don't feel too strongly about this.  2. Results on VQA-CP / VQA -- I treat VCR as a different task from VQA / VQA-CP as it focusses more on commonsense reasoning which is not there in VQA / VQA-CP enough. It would be nice to test the proposed model on VQA / VQA-CP as well (which authors have done (only on VQA-CP), however their VQA-CP results are not beating the state-of-the-art (47.70 on VQA-CP v2 by Selvaraju et al., ICCV 2019 and 41.17 on VQA-CP v2 by Ramakrishnan et al., NeurIPS 2018)); however I do not consider lack of beating state-of-art on VQA / VQA-CP to be a reason to reject this paper.  3. Glove in language model -- the proposed model uses BERT and beats the previous state-of-art using BERT (R2C). Zellers et al. already show that VQA models using Glove perform much worse than VQA models using BERT on VCR. So given that it has been established that BERT is much better than Glove for VCR, I am not sure why it is not enough to just keep using and comparing with BERT and not show results on Glove (assuming fair comparison with previous state-of-art). 