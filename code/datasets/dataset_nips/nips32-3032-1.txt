The paper proposes a new model for VQA, and explores it in the context of the VCR dataset for visual commonsense reasoning. The model uses two main components: first NetVLAD [1] and then Graph Convolution Network [2] in order to propagate contextual information across the visual objects, which is interesting and different from prior work in VQA (especially the use of NetVLAD). Experimental results are good and The paper is also generally well-written and structured clearly (except some part of the model description as discussed below).   Model - The paper motivates the new model mainly by comparing it to the neuronal connectivity within the brain. I feel that in this case the comparison is not very justified/convincing. While certainly relational reasoning is important for tasks of visual question answering, I would be happy if more evidence could be presented and discussed in the paper to establish the proposed connection between the relational model in the paper and the operation of the brain. - NetVLAD: Throughout the model description section, the discussion assumes familiarity of the readers with VLAD and NetVLAD - I think it could be helpful to discuss them at least briefly either in the Related Work section or in the beginning of the subsection about “The Computation of Conditional Centers” (Page 4): e.g. to explain in high-level what the method does and provide a bit more detail on how the new model uses it.  Experiments - The paper provides experiments only over a new dataset for commonsense reasoning called VCR [3]. It could thus be really useful if experiments would be provided also for more standard datasets, such as VQA1/2 or the more recent VQA-CP, to allow better comparison to many existing approaches for visual question answering. - Most VQA models so far tend to use word embedding + LSTM to represent language - Could you please perform an ablation experiment for your model using such approach? While it’s very likely that BERT helps improve performance, and indeed baselines for VCR use BERT, I believe it is important to also have results based on the more currently common approach to again allow better comparison to most existing VQA approaches. - (Page 8) The t-SNE visualization is explained kind of briefly and it is not totally clear to me what is the main insight/conclusion that could be derived from that: we see that for one image (blue) the centers are more concentrated and for the other (green) they are spread further apart from each other - it would be good if the authors could discuss in more detail why such a thing might happen or what aspects of the image impact the density of the centers, or how the density affects the downstream reasoning performance.  Clarity - (page 2) The second paragraph that gives an overview of the model is quite long and hard to follow. I think it would be good to make this part more structured: maybe splitting the paragraph into multiple ones that present each of the stages the model goes through.  [1] Arandjelovic, Relja, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. "NetVLAD: CNN architecture for weakly supervised place recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5297-5307. 2016. [2] Kipf, Thomas N., and Max Welling. "Semi-supervised classification with graph convolutional networks." arXiv preprint arXiv:1609.02907 (2016). 