I am on the fence for this paper. I like that the approach is simple. I like that it was tested in different domains. I like that several different views of performance are included. Nevertheless I have many questions, that need resolution before I can increase my score. As it stands I am leaning to reject.  The basic idea is to learn a separate exploration policy with an independent RL agent. The system does not learn when to explore, but how to explore assuming the main controller requests an exploratory action. There is surprising little work on learning separate exploration policies beyond the older work such as Simsek and Barto. I expected this paper to discuss Simseks work a bit, even if it was not deigned for function approximation.  The motivation of learning an exploration policy from a lifetime is a good one. However, the paper goes on to assume tasks are stochastically drawn from a distribution and training only multiple tasks occurs in parallel. This is both unrealistic, and it also seems like it would limit the main idea. That training on an increasingly difficult sequence of tasks would result in better and better exploration. That one view of this approach.  Another view is that by learning from a set of tasks, the agent would learn a policy that would be useful to explore in new domains. Unfortunately the paper does not directly test this. As far as I understand no experiment trains the architecture on a set of training tasks, then fixes the exploration policy, and then evaluates the system on a totally new task. In addition it would be nice to see some more analysis of the learned exploration behavior. Section 6.2 investigates this a bit, checking if the learned policy is simply an exploitative policy. What about investing how the advisor policy changes with time?  I would like to see a bit more leg work to rule out other effects and better discussion of the results. Why not include a strong exploration method as a baseline, like Osband's Randomized Value Functions or [12]. That would give us a baseline to understand the results. Why not compare against a PPO agent that is not reset after each task---show that transfer fails! For instance, the issue described in section 6.1---that in animat some actuator combinations are not useful---could be discovered by a transfer agent.   The results presentation could be improved. None of the figures have figure numbers. Some plots have error bars others don't. The plots with the most noisy lines and the biggest questions about significance have no error bars. In the animate results the range of the y-axis is very small. There are error bars but they are not the same size so it's difficult to conclude that the new method is clearly better. In table 1 some results are not statistically significant, but still bolded. Why is there no training progress plot for Animat like there was for pole balancing?  In addition, important details are missing. What was the function approximator used in pole balancing? Why only 15 runs in pole-balancing? What are the state variables in Animat? Why is there an additional +100 at the goal in Animat---if this is a cost to goal problem this is not needed? How were any of the hyper-parameters tuned in any of the experiments. How do we know the comparisons agains PPO are fair without a clear description of how these critical performance parameters were set?  I look forward to the author response!  ================================ Update based on author response:  Thank you for clarifying about the hold-out test experiment. There are still missing baselines, and open questions that prevent limit the work and were not directly addressed in the author response (but I understand space is limited). The reviewers had a nice discussion of what the results mean, and how they should be communicated. The other reviewer will summarize, but in short please be more clear describing how the experiments described in table 1. Please describe how significance was calculated and spend more time describing the results.   This is a general comment for the presentation in the paper. Describe error bars when included. Explain why the results matter (sometimes the axis range on the graphs are tiny and large). Explain how baselines and your approach were tuned. Identify sources of bias. Help the reader understand the details of your results and why they are important---after all, the contribution of the paper is evaluated entirely empirically here!  Nice to have: more analysis of the exploration policy and how it evolves over time. Something more contextual than action frequency. For example, could you design an interesting gridworld and visualize the exploration policies at the beginning and end of learning? Can you think of gridworlds that should cause the exploration policies to evolve differently during training?     Finally, the way you generate different tasks is reminiscent of the work of Whiteson et al: "Protecting against evaluation overfitting in empirical reinforcement learning" 