The idea to predict the parameters of the convolutions is interesting, however I cannot really understand the motivation behind the proposed method. If I understand correctly, the  V layer is only smaller than a full convolutional layer by a factor of D, (which is 3?). I see the real bottleneck in the fact that you need to predict H*W kernels although you know the semantic content at each pixel. To me it would make much more sense to predict DxCxKxKxL channels, where L is the number of labels. if you would use the conditional weight prediction, you would still be able to use context as input.   With respect to the discriminators it is difficult to understand what the authors developed and what has been taken from other papers. Although they reference other papers, they do not state, what they have contributed.   It is really hard to judge performance. The number of parameters must be high, especially when including attention as well. Especially since the network details are not listed anywhere (number of parameters etc), it is hard to compare to SPARSE. So many different adjustments are made that it is hard to determine which one is the most important one. The Ablation study is going in the right direction but is lacking some parts. For example, CondConv pred C/O FP + MsPatch. Only when you do not use the feature pyramid and no attention, you can actually assess whether the parameter learning is better than SPADE. Also, it would be good to see results with SPADE + FP and SPADE + FP+SE.    Originality: medium Quality: medium Clarity: medium - without having read SPADE, the reader does not know what how to compare these approaches. Significance: low-medium