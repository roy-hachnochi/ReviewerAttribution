Originality: The architecture is novel compare to recent lines of language model work, which all used variation of BERT or GPT (SciBERT, MT-DNN, MASS and etc).  The example ("New York is a city" one) makes sense, but considering the permutation is random when computing the objective function, I still couldn't get why it works better than sequential order because human speaks/writes in sequential order. Could you add more intuitions in paper? Or have you tried predicting n-gram, compare to permutation?  Quality:  Very high considering they did extensive of studies on multiple benchmarks, also the ablation study is nicely done as well. However, the comparison is little bit of unfair since BERT only predicts sub-tokens originally. So I hope you could compare to BERT whole word masking (released around end of May) under the same training corpus, if possible.   Clarity: The paper is well written and organized.  Significance: Achieving first place on multiple benchmarks and more than 3500 stars on their repo explains the significance of this work. I hardly believe there will be lots of follows up on this work because probably only small number of people can afford to train this. But people will use it as the base architecture and fine-tune on top of it, which still benefits the whole NLP community.  I am very satisfied with their new ablation studies and will increase my score. 