Updated after authors' response ------------------------------------------- Thanks to the authors for their well-written response.  In my original review, I failed to fully appreciate the novelty and significance of the result that mouse cortical areas are not organized hierarchically. In addition, I appreciate the added background on mouse cortical areas (as well as the pointer about VISp, I apologize for missing that detail in my original reading!).  In addition, I appreciate the response to my comments about audience. Comparisons between artificial networks and biological data are of growing importance, as such NeurIPS is definitely an appropriate venue for this work.  After reading their response, and the other reviews, I have decided to increase my score to a 7.  Original review -------------------- Originality: The work is largely original. I would point the authors to a (recent) paper that addresses a related question (comparing different methods for measuring representational similarity): Similarity of neural network representations revisited. Kornblith et. al. ICML 2019. Kornblith et. al. propose a similar test for assessing the quality of a similarity metric (comparing all of the layers of a network to a given layer of a network with the same architecture). One key difference is that the Kornblith et. al. paper compared networks with the same architecture trained using different random initializations, whereas in this paper the authors use the exact same network (pre-trained).  Quality: The work is fairly thorough. I appreciated the comparisons to two popular metrics (SSM and SV-CCA), as well as providing a sense of the spread in the results due to random sampling by adding shaded error bars (side note: are these standard error or standard deviation, or something else?). I would like to have seen comparisons with multiple CNN architectures (not just VGG), as well as comparisons within the same architecture but trained with different random seeds. The authors note this potential caveat of their findings in the conclusion, namely that their results are only for comparisons to VGG itself. I think this caveat is significant enough that it should be addressed in this paper. Regarding the second half, I wish the authors could have also compared with mouse V1, rather than just the higher cortical areas. Mouse V1 would provide a nice baseline, as we would expect it to be most similar to early layers of convolutional networks. (note: I am not sure if this type of mouse V1 data is available in the Allen dataset, so this suggestion may be difficult to try).  Clarity: The paper is largely well written and clear. I would have appreciated more discussion and interpretation of the scientific findings. In particular, there was little to no background on what the different mouse cortical areas are, and what is currently known about them. Due to this lack of background, the scientific motivation driving the second half of the paper is weak, and it is hard to know what to make of those results. Finally, I would have appreciated some discussion on similarities/differences of the two metrics (SSM vs SVCCA). Do the authors prefer one over the other, given their findings?  Significance: The first part of the paper addresses an important question, relevant for the NeurIPS audience (when should we trust comparisons between network representations?). However, some of the methodological limitations (only studying VGG, not re-training with different random initializations) reduce the overall significance of the work. The second part of the paper tries to address a scientific question (how do representations in mouse visual cortex compare to VGG16?) but this question is not well motivated, and there is a lack of background material presented to put these findings in context. For example, it is unclear to me if the finding that 'mouse visual cortical areas are relatively high order representations in a broad, more parallel organization' is all that new or surprising. I think these limitations reduce the significance of the second half of the paper.