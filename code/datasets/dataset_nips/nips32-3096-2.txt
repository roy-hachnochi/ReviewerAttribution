This paper studied an off-line TD algorithms via min-max optimization and provided a non-asymptotic analysis of the convergence rate. I do not recommend to accept the paper due to the following reasons.  - In my opinion, the paper analyzed only a generic finite-sum min-max optimization problem, not a real analysis for TD algorithm. Here is my reasoning. Although the paper motivates the problem by TD learning, its actual formulation of the problem starts by assuming that a trajectory of state-action sequence is given, and is fixed and deterministic throughout the analysis. Then the problem becomes a typical finite-sum min-max optimization. Throughout the analysis, the randomness of the state and action variables due to their generation via a Markov chain does not play a role, because these variables are treated as fixed (the authors can clarify this in the rebuttal process). Hence, the analysis and results do not reflect any special property of TD algorithms but only min-max minimization.   - I only view that paper provides the convergence analysis for the finite-sum min-max problem. For this, the paper proposed variance reduced gradient type of algorithm and characterized the convergence rate. While I do think this makes a contribution, the proof mainly follows from the standard techniques. And such a contribution is not sufficient for publication at NeurIPS.  --------------------- After authors' feedback The authors' response did not provide a good answer to my question about their contributions on TD learning. The paper solves only a finite-sum min-max optimization problem, but the claim of contributions (in the title, abstract, Section 1 of introduction, Section 2 of problem setup, etc) was on policy evaluation via nonlinear function approximation in reinforcement learning, which is a very different and much more challenging problem. This is quite misleading for readers to understand the true contribution here. The way that this paper addresses the TD learning problem (see more detail below) does not fit it into the state of the art of TD learning analysis.   For the possible interest of the authors, I explain below why I think that the paper does not address the TD learning problem. TD learning has the goal of learning a value function for a policy, and the paper starts from a valid population (in expectation) objective function eq (8) and eq (10) to achieve such a goal. It has been shown (by existing literature) that the solution of such an objective function (eq (8) or eq (10)) does provide desired value function (a good enough approximation to the true value function in the function space). However, the paper does not solve eq (10), but in fact only solves a finite-sum problem eq (16), which is based on a FIXED state-action sequence. The convergence guarantee is established to show the algorithm converges to the saddle point of the finite-sum objective eq (16). Clearly we wonder whether such a solution is desirable for TD learning, i.e., how well such a solution approximates the true value function. There is no such an answer in the paper. One would naturally think that this can be argued by bridging the solution of eq (16) and solution of the original TD objective eq (10), but the paper does not fill this gap. This can be clearly seen from the fact that the analysis of the paper does not even exploit the statistical distribution of state-action and without incorporating this I don't see a way to connect eq (10) and eq (16). Consequently, the paper does not solve the real challenge in the nonlinear TD learning, where we wonder whether we obtain a good enough approximation to the true value function or just a local optimal solution for the chosen objective function.   