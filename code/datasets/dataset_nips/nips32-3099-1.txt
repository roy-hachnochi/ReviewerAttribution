This papers proposes to use a discrepancy measure where an hypothesis set and a loss function are part of the discrepancy definition. In this very well written paper, the Authors make a compelling case for the potential benefits of a discrepancy measure capable of distinguishing distributions which would be considered as close by the Wasserstein Distance. The supplemental material is also a great addition to the flow of the paper allowing the Authors to clearly demonstrate their points. The DGAN derivations allow for a closed form solutions which can be generalized to a more complex hypothesis set if an embedding network is employed first.  The paper is very convincing at presenting the potential advantages if one knows which loss and has a good enough set of hypotheses (from the embedding network).  However, there is a bit of a sense of let down in the experimental section. The results for GAN training gets only a short treatment in 4.1 when it could have been a very strong point for the use of discrepancy. The results (7.02 IS, 32.7 FID) are reasonable but not a clear separator from other GAN methods. It would have been nice to have a thorough set of experiments there, instead of for ensembling.   The EDGAN approach is working. However it is not as compelling a task as pure Generation. The model basically learns to ignore high discrepancy models and uses only output of a few low discrepancy models (as seen in weights in Table 4 in Supplemental) which is pretty much what intuition would dictate if you hand-picked the interpolation weights. You basically estimate your whole technique based on picking only 5 linear weights... If ensembling is the key to comparing your technique, some other methods out there (wasserstein model ensembling in ICLR'19) are also showing some decent results beyond model averaging.  Overall a very good paper, experimental results could have focused more on DGAN than EDGAN.  Note: line 205, should 'H' be the same font of the other hypothesis sets elsewhere in the paper?   --------------- Thanks for the authors' rebuttal. I am maintaining my score of 7.