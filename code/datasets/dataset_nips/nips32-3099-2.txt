This paper considers learning generative adversarial networks (GAN)  with the proposed generalized discrepancy between the data distribution P and generative model distribution Q. The discrepancy is novel in the sense that it takes the hypothesis set and the loss function into consideration. The proposed discrepancy subsumes Wasserstein distance and MMD as a special case by setting a proper class of hypothesis set and loss function. The author then proposed DGAN and EDGAN algorithms where they consider the set of linear functions with a bounded norm and loss function using square loss. Due to the restricted class of hypothesis set, the discrepancy has a closed-form simple estimation, which is very similar to matching covariance matrix of the data. For the EDGAN algorithm, they propose to learn the combination weights of a mixture of pretrained generators, which is a simple convex optimization problem. In short, the overall writing is clear and the setup is novel with the solid theoretical analysis. 