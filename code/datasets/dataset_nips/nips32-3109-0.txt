# Strong points * The paper is very well written and easy to follow * The observation that inference for GANs is hampered by local minima is important and might have important consequence, e.g. regarding experiments that examine the mode coverage of GANs by inferring latent codes for test data. * The proposed solution is quite easy to implement as it is based on standard HMC * The authors attach code in the supplementary material  # Weak points * It was not clear to me if HMC is really necessary or if noisy gradient descent where the noise is gradually annealed would work just as well. It would be nice to have this as a baseline (this baselines is even easier to implement than the method proposed in this paper). * While inpainting and super resolution are certainly interesting tasks, I'm not sure if these are really the most compelling examples, as it is relatively easy to generate data for a fully supervised method in these cases (e.g. for inpainting just remove random rectangles). Other interesting examples might include ill-posed problems like inverting a Radon transform for medical data or even doing multiple things at once with one model. * It would be nice to have a better ablation study / study of hyperparameters (annealing schedule, acceptance rate, leapfrog updates, step size). * It would be nice to have FID [1] as a second, more global metric, besides MSE and SSIM. * I think AIS is misnomer in this context, since there is no "importance sampling" involved (A better name might just be "annealed HMC").  # Other comments / question * Fig. 4 is very small and it would be good to replace it with a table. It would also be nice, if Fig.4 was referenced much earlier in Sec. 4.2 * "The complex distribution challenges exploration." (l. 261-262): How is the complexity of the distribution related to exploration issues. It would be nice if the authors could elaborate on this point. Are there simple experiments that show this? * "We show MSSIM and MSE between the ground truth and the final output" (l. 272-273): is this just one image or a full test set? How as the train / val / test split done?  (for all datasets) * Why is gradient descent in latent space always called "SGD" and not just "GD"? What is stochastic about it? * It would be nice to have error bars for Fig. 4, i.e. standard deviation over the different test images  # Overall rating While the idea in this paper is relatively simple, the overall observation that gradient descent in latent space for GANs might lead to local minima is interesting and the proposed solution is elegant and easy to implement. While I do not recommend acceptance at this stage, I would be willing to upgrade my rating if the authors can successfully address my remaining concerns and the other reviewers argue for acceptance.  [1] Heusel, Martin, et al. "Gans trained by a two time-scale update rule converge to a local nash equilibrium." Advances in Neural Information Processing Systems. 2017.  === UPDATE AFTER REBUTTAL === I think the authors addressed most of my original concerns (simple baseline, ablations study). Although they did not address my concern about alternative metrics, the naming of the method (I still don't see why there should be "importance sampling" in the name) and additional tasks, I don't think these concerns are major blockers. However, I think not using a test set for these kinds of experiments is very misleading. Given that the other reviewers were very positive and the paper is mostly about the optimization method, I will increase my overall score to "6 - Marginally above the acceptance threshold" . However, I  strongly encourage the authors to rerun their experiments with a separate test set or at least clearly state in the paper that they did not use a test set.