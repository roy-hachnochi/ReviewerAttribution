Originality: This is a very interesting algorithmic contribution. The introduced method gets state-of-the-art results under reasonable computation resources. I was reviewing a former version of this paper for some other conference and have to admit that the new version is significantly improved, mainly because the authors have succeeded to decrease the computational costs of the attention-based deep network by using the probabilistic label trees.   Quality: The method is sound and the empirical analysis is of high quality. The paper does not have any theoretical contribution, but it is unnecessary for this kind of contribution.   Clarity: The paper in general is clearly written. However, the authors could make a better job in description of the methods: - The tree building method seems to be very simple, but I am not sure whether I have understood all the details. A pseudocode would help a lot.  - Similarly, it is not clear enough how the underlying idea of probabilistic label trees have been finally implemented (there is neither a pseudocode in the paper nor the code attached to the submission). It seems that the learning follows a kind of beam search, but this is not clearly stated. Let me underline that this is not necessary for probabilistic label trees. It should be enough to use a given training example only in those nodes for which the parent node is positive (this is what the conditioning on z_{Pa(i) = 1} in (1) says). The solution used by the authors looks like a kind of additional negative sampling. A careful discussion should be given here.   - It is also not entirely clear from Subsection 2.3 how training and prediction are performed. Are the models for each level trained sequentially from top to the bottom levels or they are trained all at once? How does batch training work in this case? Is the C parameter the same for training and inference using beam search? - Notation used by the authors is not systematically introduced (some symbols are defined in captions of figures). This makes the paper not pleasant to read.    On the other hand, the details of conducted experiments (results, parameters, hardware, ablation analysis) are well-described. It is only surprising that the authors did not include the results of extremeText. This is a shallow network based on PLTs that significantly outperforms XML-CNN. Indeed, it gets results inferior to Parabel or Bonsai, but this is an online method methodologically more similar to the algorithm introduced by the authors.  Minor comments concerning clarity are given below:  - word(where => word (where - regraded => regarded  - Do the outputs of AttentionXML \hat{y} correspond to the node variables z_n? - XML-CNN uses binary cross-entropy loss (not the cross-entropy loss function as suggested by the authors) which is theoretically well-justified as it leads to estimation of marginal probabilities (see, for example, "On label dependence and loss minimization in multi-label classification", MLJ 2012, and the extremeText paper).  Significance: AttentionXML achieves better results in terms of precision@k than other state-of-the-art algorithms (including 1-vs-all approaches like DiSMEC, which are very hard to beat) on popular XMLC benchmark datasets. It is worth to underline that the method improves the results significantly not only on precision@1, but also on precision@5. While the proposed approach is still quite costly in training and the approach has many additional hyper-parameters that seem to affect results in a significant way, one should not ignore these outstanding results. The presentation of the paper should be improved, but the contribution deserves publication at NeurIPS. 