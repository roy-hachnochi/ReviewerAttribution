 ====================== Thanks to the authors for the rebuttal, I have updated my score for the paper. It will be helpful to include the additional experiments done as part of the rebuttal to the final version, along with the references. ================  The paper presents a deep learning method based on attention mechanism for Extreme multi-label Text classification (XMTC) problem. The proposed solution is scalable to datasets with upto 3M labels, and is claimed to achieve state of the art results on precision@k and ndcg@k metrics.  Originality - The paper has two main parts - (i) using the attention mechanism for XMTC and (ii) shallow tree for scaling up to large datasets. Even though using the attention mechanism is new for this domain, but the idea of shallow trees is somewhat related to a recently proposed method (Bonsai, [10] in the paper). However one of the main concerns is the following : (a) The process of making the tree shallow by node compression is related to similar ideas in hierarchical classification [1,2] below. In this respect, the reference to these works is missing, and should be included.  Quality - Though the paper shows that using shallow trees along with the attention mechanism, state-of-the-art performance is achieved.   (a) It is not clear how attention mechanism work as standalone i.e. how does it perform on small datasets when there are no scalability issues, and one does not need the shallow trees. It is, therefore, important to evaluate the attention effect separately without the tree structure, and then study the impact of using that in a shallow tree architecture. Does the accuracy improve upon using the shallow tree? How is it work with deep tree?  (b) In comparing on propensity metrics, comparison with another state-of-the-art method [3] (ProXML) is missed out. On some of the datasets, the performance of ProXML is relatively better on these metrics. This comparison should therefore be included in the paper.  Clarity - Even though the paper is clear in terms of writing, it would be really beneficial if the authors also provided the code.  Significance - Since the results of the paper are significantly better than most state-of-the-art methods, it would be of interest to the community.  [1] On Flat versus Hierarchical Classification in Large-Scale Taxonomies, NIPS 2013 [2] Learning taxonomy adaptation in large-scale classification, JMLR 2016 [3] Data scarcity, robustness and extreme multi-label classification, Machine Learning Journal, 2019