Originality: The authors make clear the distinction from related work. They are not the first to integrate GANs for generating goals in RL, but do so in a new and interesting way.   Quality: The comparison with baselines is thorough, showing the benefit of this approach for these domains.   However, page 7 claims that HALGAN RL agents need fewer samples than standard RL, and yet in fact HALGAN must be exposed to enough samples of successful trajectories to be able to effectively hallucinate goal states. Are the 1000-samples used to train the HALGAN shown in Figure 3(f) 1000 examples of /goal/ states, or just states in general. How long does the agent have to explore before a HALGAN can be trained? This discussion needs to be made more clearly and carefully.  A second question is why the generator of HALGAN does not input the s_t that it is trying to modify. Without seeing the original state, how can HALGAN generate a goal if the goal is potentially occluded by something in the state. If this can never happen, are these environments realistic? Will the HALGAN approach work in more complex environments?  Clarity: Overall the paper is very clear, the diagram in Figure 2 and the rest of the figures contribute to ease of understanding the contributions. The distinction from prior work is clear, as is the motivation. Minor point: there is a missing period at the end of the paragraph which starts section 5 (ending with "hallucinations of the goal are discussed next").  Significance: The idea makes sense and could be useful, however it's not clear how often real-world tasks have visible goals which must be added to the state. Further discussion of examples where this is the case in the real world could help bolster the significance of the paper. 