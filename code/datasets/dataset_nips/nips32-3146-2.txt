The paper follows the work of Tsipras et. al., and use simple data distributions to theoretically study the robustness trade-offs. The hypothesis is that adversarially-trained models tend to focus on different features to achieve robustness to a particular attack.   The paper shows, through analysis of the activations of the network, that robustness to l_\infty leads to gradient masking for other types of adversaries on MNIST. Looking at Table 5., this does not seem to be the case for CIFAR. Furthermore, the trade-off seems to be less significant on this dataset (e.g., comparing OPT(R^{avg}) and R^{avg}). This suggests that some of the results may be artifacts of MNIST, and more generally, the hypothesis in Tsipras et. al. on explaining the phenomenon of adversarial examples may not be sufficient. 