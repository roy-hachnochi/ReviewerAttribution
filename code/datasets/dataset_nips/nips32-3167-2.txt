Given the Glow and flow++ papers, it seems the biggest contribution of this work is in the masked convolution coupling layers, which on its own, improves upon Glow but falls behind flow++ for both uniform and variational dequantization.  Given the paper's abstract, I would've expected performance to exceed flow++ but this seems to not be the case for either uniform or variational dequantization. I would've liked to see more comparable experiments with flow++ (by simply augmenting the flow++ architecture with masked convolutions since their code is publicly available) especially since MaCow has the exact same motivations (judging from the abstract) and also uses variational dequantization. I found the current comparisons a bit messy and difficult to understand.  Given that Glow had to use 1 batch per GPU and gradient checkpointing in order to train on CelebA-HQ, can the authors comment on how MaCow compares?   The paper also dedicates a page to discussing dequantization; however, it isn't clear to me how this is different from flow++'s ELBO. It seems the main equations are simply re-deriving this lower bound on the log density. I did not attribute this to be a contribution of the paper, but it could be that I simply did not understand the message here.  Note that the temperature trick for sampling used in Glow is only applicable when the change in log density is constant wrt. the sample, so they only applied it for additive coupling layers. Can the authors verify whether their experiments satisfy this property?  On another note, please be careful with link submissions and author identity. Many of the items on the reproducibility checklist seems wrong, e.g. how hyperparameters were chosen was not specified, the number of evaluations runs is not specified, and I did not see a single error bar or standard deviation in the paper?  ---  I thank the authors for providing wallclock time for sampling. I have one comment being that the authors should clarify in the paper how their section on variational dequantization is different from flow++'s exposition, or I would recommend moving it to a background section and instead expanding the sections on the properties of masked convolutions for a clearer narrative.