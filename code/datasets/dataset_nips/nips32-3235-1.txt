In this paper, authors study the graph convolution networks.  Especially, the authors propose a new filter to approximate the true Fourier transformation, in the same spirit of Chebyshev filters and others.  The proposed filter is based on the ARMA filter of Eq. (6).  This equation is computationally heavy, so the proposed feedback-looped filter approximates the equation as in Eq.(7).  Then the actual implementation of Graph Neural Network is formulated as in Eq.(12).   Experimental performance is measured with Cora/Citeseer/Pubmed scholarship networks and NELL knowledge graph.  Accuracies in the semi-supervised node classification surpass the several standard GCN models.   Since I am not familiar with signal processing literature, I have difficulty in understanding the technical contribution.  There are a few barriers I encountered.  (1) Why the rational polynomial coefficients are preferable for convolution filter development?  Or, why the RPC is less sensitive in an underlying graph structure, compared to Chebyshev?  (2) I read the ref. [16, 17], but cannot understand why the Eq.(7) serves as a (valid?) approximation for Eq.(6).  Thus I cannot judge whether this is a technically reasonable, or interesting, approximation of the ARMA (Eq.(6)).   In Eq. (12), a regularization term \mu is introduced for the layer construction.  So, strictly speaking, the spectral convolution layer is not an exact implementation of the feedback-lopped filter, right?  Then theoretical discussions in Sec 3.4 is not directly applicable for spectral convolution layers. Is this correct?   Also, I expect an ablation study for the additional \mu terms to verify the effect of this regularizer on several experimental performances.   I also  have a concern about the experimental design.  There is no description of how the authors tune the hyperparameters.  In Figure 2, a pair of (p,q) = (5, 3) is the best for the "test" dataset, and the authors used this pair of (p, q) for the main result, table 4.  Does this mean the authors tuned hyperparameters to maximize the test score?  Or, the best (p,q) for the validation dataset is the same as the (p.q) for the test dataset?  Please clarify how the hyperparameters are tuned during the training.   + Experimental scores seem good.  - It is difficult (at least for me) to fully understand the derivation of the feedback-looped filter.  - Theoretical discussion is not for the actual convolutional layer (Eq.12), but its ideal(?) formulation (Eq.7) - The effect of \mu regularizer is not assessed.  - Unclear hyperparameter optimization procedure.   I really expect the author feedback to fully understand the contribution of the paper.   ### After author feedback ### I'm happy to see the detailed author feedback. It definitely helps me understand the value of the submission more correctly.  So I raised my review score upward.   However, the manuscript is still somewhat difficult for some groups of readers. Further explanations and reference pointers will enlarge the potential audience. 