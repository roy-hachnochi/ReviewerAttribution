The paper demonstrates a shortcmoing in PPO algorithm where a suboptimal policy that prefers suboptimal actions can diverge over time due to the constant clipping mechanism. Instead, the paper proposes an adaptive clipping mechansim that selects clipping range based on current policy, giving more chance of exploration to actions that are not preferred by the current policy. The paper demonstrates that the proposed method achieves better objective lower bound than PPO while maintaining the same KL divergence between policies. Experimental results compares the proposed method to  several baselines in different settings.  The paper provides elegant treatment of the problem at hand and has the potential to guide future research. The results are promising but are not very impressive unless the training time is included.  I have a number of comments though. My important concerns are regarding evaluation: - Is there a reason that you are choosing average top 10 rewards? It seems more natural to use average rewards over all episodes.  A related question is why you are choosing only 7 random seeds for Figure 3? - Please compare with adaptive regularization/constraint baselines such as the adaptive KL regularization version of PPO[20]. - Please add more details (perhaps in the supplementary) about your policy class as well as baseline hyper-parameters to make the results reproducible.  Other comments: - Introduction: Lines 23-24 are not very accurate since there is a version of PPO that uses KL penalty without clipping[20]. - L113: Lemma 2 is not clear to me. Are you sure the LHS is not E_{\pi_{t+1}}[\pi_{t+1}(a) | \pi_0] - Algorithm 1: It seems that t <- t+1 should be in the end. - L166: while gets --> while getting. - L235: How can we guarantee that \pi^{new} exists given that it has to satisfy multiple constraints?  ======================================================== I thank the authors for their response. Based on the rebuttal, I have changed my score to 7.