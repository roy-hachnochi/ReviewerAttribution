The paper proposes to adapt the clipping procedure of Proximal Policy Optimization (PPO) such that the lower and upper bounds are no longer constant for all states. The authors show that constant bounds cause convergence to suboptimal policies if the initial policy is initialized poorly (e.g. the probability of choosing optimal actions is small). As an alternative, the authors propose to compute state-action specific lower and upper bounds that are inside the trust region with respect to the previous policy. If the previous policy assigns a small probability to a given action, the lower and upper bounds do not need to be very tight, allowing for less agressive clipping. The adapted version of PPO, which the authors call TRGPPO, has provably better performance bounds than PPO and is validated empirically in several experiments.  Since PPO is frequently used in deep reinforcement learning, correcting the suboptimal behavior of PPO seems like a relevant contribution. I think the paper properly motivates the theory, and the derivations appear correct, with the exception of one derivation that I do not follow (see below). The empirical validation is mostly expected, but there are cases for which the approximate version of PPO outperforms TRGPPO, which is a bit puzzling.  Page 3: "We now give a formal illustration": This sentence appears misplaced, especially since it is immediately followed by Algorithm 1.  In the supplementary material, Equation (4) correctly states the equations on the Lagrange multipliers, but I fail to see how these equations are transformed into Equation (5).  "Table 1(b) lists the averaged top 10 episode rewards": this is a very crude presentation of the empirical results that does not account for variation in performance during learning. A given algorithm may have a performance peak early during learning and then suffer from catastrophic forgetting. I would much prefer to see the learning curves of the different algorithms, to help infer whether learning was stable over time.  Do you have any ideas wht TRGPPO performs worse than PPO in Humanoid? This seems to contradict your theoretical results.  POST-REBUTTAL:  The rebuttal mostly confirmed my initial understanding: the proposed version of PPO is guaranteed to improve over the original PPO algorithm, but still does not come with any convergence guarantees, not even to an approximately optimal solution. I appreciate the effort to explain how Eq (4) is transformed into Eq (5).