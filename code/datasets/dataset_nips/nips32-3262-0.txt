This paper explores a very interesting idea, that unfortunately is marred by rushed writing. The paper is also filled with typos, that make understanding it and judging the quality of the work extremely hard. Exploring the central idea of the paper, that of a generator playing vs a discriminator and a classifier, and giving more intuition about this approach and why the authors expect it to work better than other traditional GAN based methods was extremely necessary. Instead the authors just mention it in passing just before section 3. This has the potential to be a good paper in a future version if all the writing issues are addressed and the paper is planned out a bit more.   Nevertheless, I outline a few questions I had to the authors of the paper.  1. Why does the RHS of min max U(C,G,D) in the main equation (10) not depend on G? Same question for equation 14.  2. Is there a inf_{\lambda > 0} missing from the second term of the LHS of equation 4 and equation 5? If yes, how does that change things?  3. I understand trying to bound the error for the worst-case distribution in an \epsilon Wasserstein ball during the theoretical result, as that gives an upper bound for all other distributions. But why train on the worst-case distribution? What does the framework achieve from this?  4. Given that the generator approximates a worst-case distribution in the \epsilon ball, one would expect that for good data (without noise added) just the classifier would achieve better performance than this framework. Why isn't this the case in the MNIST set within the Imbalance 1 column and SVHN set under the Normal column? 