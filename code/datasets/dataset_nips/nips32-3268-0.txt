The paper studies primarily studies the problem of robust mean-estimation, when a certain fraction of the data is corrupted adversarially. Then, in this setting, the authors propose a nearly-linear time estimator(which is also practical), which achieves the information-theoretic optimal rate. The authors back their claims by conducting experiments on CIFAR, word embeddings etc. These are some of the first experiments to be done at this scale in the robust statistics community.  The paper is well written and the authors provide a lot of intuition for their algorithms. The authors are very rigorous in their theoretical claims for robust mean estimation.   A minor criticism is the handling of the "outlier detection component". I agree with the authors that prima-facie it is not clear, when the outlier detection problem is meaningful. But, then, what is the the output of the algorithm 2 giving?   From my understanding, I can see that if the outlier distribution is such that operator norm of the covariance of the mixture increases, then the weighting given by algorithm 2, can be used to identify the outlier points -- is this correct? But is this an if and only if?   For example, say the outlier distribution N, is a point-mass at the true mean of D, then what would algorithm 2 give?   Minor Question: --In step 3 of algorithm 2, is there a centering step missing?