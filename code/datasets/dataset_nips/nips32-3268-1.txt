This work considers the problem of identifying adversarial outliers, with applications to robust mean estimation. The work proposes to use a "quantum entropy" regularization which can be solved quickly using a matrix  multiplicative weights algorithm. This leads to a near-linear time algorithm. The paper has a nice high-level discussion of how the entropy term improves upon the previous, spectral filtering methods and obtains the improved running time: intuitively, whereas the spectral filtering corrects a single direction at a time, the entropy term encourages filtering many directions simultaneously. Obtaining a (near-)linear time algorithm is pretty significant, as the time complexity significantly impacts the breadth of settings in which such methods may be applied (and of course, we can't hope to beat linear time). Large data sets demand linear time algorithms.  The paper also includes experiments demonstrating that the method is effective. The experiments do convincingly show that the method is effective at removing  outliers compared to several simple methods. The experiments are pretty well constructed, considering some natural models of contamination in two real data sets (CIFAR and word embeddings) and a synthetic data set. Compared to the  simple (fast) baseline methods considered here, the new method does show significant improvement at identifying outliers. I have a few small criticisms here, though. First, even though the polynomial-time algorithms proposed in prior works are not near-linear time, is it infeasible to run them on these data sets? Since the running time guarantee was the main theoretical focus of the work, I would have liked to see some evaluation of the running times in addition to the accuracy; conversely, it would have been interesting to see the AUC scores achieved by these slower polynomial-time methods. If they are the same, then the new method should be a clear "win," but even if they achieved a better AUC in practice, it is desirable to understand what the trade-off is. I realize that the baseline methods have been chosen because they are fast, but they seem a bit weak, and it would be good to include a strong-but-slow baseline to understand if the new method gives anything up in practice. Finally, the  plots for the experiments are so small that they are unreadable unless one zooms in or uses a magnifying glass. There is no point to including them in a print  version. As nice as the extensive plots are, it would be better to choose a  (representative) selection and just note that the others are similar. 