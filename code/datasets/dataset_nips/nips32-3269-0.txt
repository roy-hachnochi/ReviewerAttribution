The paper considers the problem of mixed linear regression: in this problem, an algorithm is given access to n data samples (x_i, y_i) with possibly corrupted labels, where each y_i is one of the m possible linear functions of x_i, i.e., y_i = x_i^T theta_j for j in {1,...,m} (but the algorithm does not know which one). The goal of the algorithm is to determine vectors theta_1,..., theta_m.  A straightforward but computationally inefficient (the complexity is exponential in d) approach to solving this problem is by using Least Trimmed Squares (LTS), which tries to identify the best fit vector (in terms of least squares) over all possible subsets of the data points of a particular, predefined size. To address this issue, the paper proposes using an alternative, simple, algorithm called Iterative Least Trimmed Squares (ILTS), which is similar to other algorithms that have been used for related problems in the literature, as acknowledged in the paper. The algorithm is essentially alternating minimization: it alternates between (1) finding the best set of a given size tau * n, given the least squares solution from the previous iteration and (2) solving least squares over the set determined in (1). This is a natural and very easy to implement algorithm, and I can see it being used if its convergence properties and ways in which to set the parameters are better understood.  The main result of the paper provides sufficient conditions (in terms of the number of samples n, number of ground truth vectors m, a notion of separation of the data) for linear convergence of ILTS, under adversarial corruptions of the labels. These results are mainly meaningful in the setting where the feature vectors come from an isotropic Gaussian distribution and can be generalized to subgaussian distributions. This result requires that the algorithm is initialized close to a ground truth vector, which makes this convergence result local. The paper then provides a global version of the algorithm for isotropic Gaussian feature vectors. If I understood this part correctly, this result leverages robust PCA to find a subspace spanned by the ground truth vectors, constructs an eps-net over this subspace, and then applies ILTS to each point from the subspace.  Overall, I find the topic of the paper to be interesting, and there are some neat ideas in the analysis, but I have a few questions/concern that would need be addressed before I could consider increasing the score: -- How important is it to know tau? How would one go about setting this parameter in practice? How does it relate to the fraction of corruptions? -- Looking at Table 1, compared to [14], the main difference is in the trade-off between sample and computational complexity -- for global convergence results, one pays more in the computation to have a lower sample complexity. [Is this trade-off inherent?] I think this should be explicitly discussed in the paper, for a fair comparison to related work. -- I believe that the claims about nearly-optimal computation need to be toned down, since the computation is really high for global guarantees. Stating that this is nearly optimal would need to be supported with the appropriate lower bounds for the trade-off between sample-efficiency and computation. -- Computation of the epsilon-net to get global guarantees seems like an overkill, which makes me doubt the usefulness of the main result (ILTS and its local analysis). -- I had a very hard time following Section 3.1 (Preliminaries) and in particular Definition 3 and the text surrounding it. Please consider revising this part. -- There are too many different constants in the statements of technical results in Section 4, which makes it very hard to understand the settings in which the results are useful. I suggest (at the very least) including some examples.  While the problem is well-motivated, clearly introduced, and most of the introduction is well-written, I found the technical sections to be hard to follow. There is a lot of notation, that is often inconsistent (e.g., * in the superscript is used to define quantities corresponding to both corrupted and uncorrupted samples). I had a hard time understanding some of the assumptions (e.g., about the separation between the feature vectors). I also feel the language can be improved.  Other specific and minor comments: -- Throughout: computation (complexity/lower bound) -> computational -- Line 61: tau -> tau *n -- Table 1: Compare -> Comparison -- Sigma_(j) in Table 1 is not defined at this point -- What is the meaning of the dash in Table 1 for the number of samples in the global case? -- Please add adequate references to the literature in Lines 81-82 -- Line 108: "of any previous work study" -- Line 109: "fine analysis" -> fine-grained analysis? -- Line 151: it is not clear at this point what it means for an algorithm to succeed -- Line 155: use either "for all" or \forall -- using both is superfluous -- Please use different patterns/markers in Figure 1 -- the figure is hard to read on a printout and would also probably be hard to read for colorblind people -- Definition 2: sigma_max and sigma_min are not defined -- Line 167-168: what does it mean that the prediction error is large due to the X? -- c_j is introduced in Theorem 7 but not used 