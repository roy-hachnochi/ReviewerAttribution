This is an exciting paper! The non-stationarity problem has plagued the iBCI field for a long time, and there's only been one other paper that I'm aware of that has attempted to address it in a non-heuristic way. Despite the shortcomings of this work, for the subcommunity that works with this type of data, this type of thinking is a big step forward, and I recommend publication.  For the results, 20 neurons are available for decoding and DyEnsemble's candidates use either 15 or 18 neurons for decoding. With 18 neurons/candidate the model can ignore up to 2 noisy neurons, and with 15 neurons/candidate the model can ignore up to 5 noisy neurons. Unfortunately, baseline performance declines significantly when including fewer neurons per candidate, giving a tradeoff between noise robustness and baseline performance. Fortunately, I don't think this particular problem would pose as much of an issue in practice because modern electrode arrays have hundreds of channels or more - so even if we just take a subset of the neurons we would still have enough neurons for good performance in the large electrode arrays typical of clinical applications. Unfortunately, as the number of channels grow, the likelihood of having all noisy neurons excluded by a given randomly generated candidate model declines. So while this work improves robustness in a compelling way, there's still more work to be done.  Update: After reviewing other reviewers' comments and the author feedback, am continuing to recommend acceptance.