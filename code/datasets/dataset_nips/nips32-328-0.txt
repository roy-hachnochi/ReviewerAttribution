============================================================== After authors' response: the correction in Eq.(5) makes sense to me. I'm not very familiar with the mini-batch analysis in SVRG. It seems to me after reading the response that although the analysis for "arbitrary sampling" may not be that  innovative, the analysis of mini-batch is new and different from previous works. From this perspective, I do agree the paper has its unique contribution. ============================================================== The paper is written in a clean way, and easy to read. From these perspective, I enjoyed reading this paper a lot. My major concerns are as stated in the contribution section: 1. Results and technics are not super innovative, similar analysis for other sampling scheme or non-averaging inner loop are done in [1, 10] and many other papers. Although they are on different settings, but the idea and proof shares a lot similarity. In this sense, I do not view the theoretical contribution of this paper as significant. 2. As this paper is targeted toward closing the gap of theory and practice, the slight weakness of theoretical results would not be a problem if this paper indeed performs practical experiment and showing the advantages of the changes. However, it's only on UCI dataset, on strongly convex loss functions. Some observations in experiments are not explained clearly.  For instance, why standard SVRG (blue curve) would get stuck in a bad position for such a long time? It seems to be it's only because a very simple reason the choice of m is too large, so that standard SVRG use a very bad estimate of gradient for a very long time. However, this should not be an issue in practice, if practitioner tune the size of inner loop size, instead of blindly follow the conservative suggestion of theory.  Finally, I feel the suggestion on the optimal minibatch size b (equation 5) a bit confusing... In fact, when m=n, and n<< L/mu, choosing b=1 gives complexity n + L_max/mu while choosing b=n gives complexity nL/mu. I'm wondering why the optimal choice of b is n in this case, especially when L_max = L? Do authors implicitly consider the setting L_max >> L?