### post-rebuttal ### The authors did not indicate whether they will allude to related work mentioned. The papers mentioned indeed are similar (granted, the paper deals with finite sums whereas the references consider the streaming variant of the question). As a final note, the second reference does consider the precise question of what is the best batch size on every instance of a streaming least squares problem for mini-batch SGD - something that is similar to issues considered in this paper in the context of SVRG, aside from considering parallel/distributed SGD methods. ################# I like this paper's motivation, message and result. Aside from the comments below, I request the authors to go over typos/bugs (if any).  The paper misses out on some references that have considered similar issues: [1]  The work of Streaming SVRG - Frostig et al. indicates that the inner loop of SVRG needs to be run for a condition number of steps - where the notion of condition number is similar to one described in this paper. [2] The work of Jain et al (2018) "parallelizing stochastic gradient for least squares regression: mini-batching, averaging and model misspecification", JMLR. This paper studies mini-batching in the case of least squares regression and SGD. They have similar notions of expected smoothness, the best batch size etc., specialized to the case of least squares, which I think have been discussed in a very thorough manner. 