The work is novel and the derivation appears potentially quite interesting and amenable to follow-up investigation.  The clarity of presentation could be improved. In the derivation I found it sometimes hard to keep track of what results were novel and which were prior work, and when a prior result is being extended vs. just quoted.  In the empirical results the Bregman divergence is argued to improve exploration. The intuition behind this is unclear to me. By encouraging policy updates to not deviate too far from the existing state action density, it appears similar to trust region methods (i.e. improve stability and avoid local optima). Perhaps the authors could discuss how this can also improve exploration (maybe just by avoiding local minima)?  The empirical results are only over 2 hours of training time, so it is difficult to compare asymptotic performance with prior work, particular IMPALA. It would be helpful, for at least a few games, to run for longer to understand how this approach performs asymptotically.  Additionally, although it is difficult to compare the x-axis (since the ones here are time, while the original PPO paper is frames), many of the PPO baselines seem weaker and less stable than in the original PPO paper (e.g. ice hockey, video pinball). 