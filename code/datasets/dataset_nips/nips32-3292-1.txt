The authors use a recently proposed intrinsic dimensionality (ID) estimator to track the ID of representations across deep network layers and architectures.  Overall, the results, text, and figures were very clear. The paper makes empirical observations that could be used to constrain theoretical models of computation. This method for estimating ID seems generally useful in representation learning research.  Comments/questions Is there consistent variability across layers if you estimate ID per class? Fig. 3 shows the variability at each layer, but it would also be helpful to know whether the fluctuations are random or consistent across layers.  In the section on classification performance, the correlation is shown across network architectures. Would you also find this correlation across networks with the same architecture but retrained with different random seeds? Does this correlation show up in the training error or training/testing loss? Is this true within one network during the course of training? It would be helpful to have a better understanding of where the ID variations are coming from.  I would suggest avoiding red and green as category colors as they are not red-green colorblind friendly.