Originality: To the best of my knowledge the results are new. The methods used on the other hand are simple and to my understanding relatively standard in the field. Yet, the fact that they work for a broad class of loss functions is interesting.  Quality: The statements of the results and proofs appear to be correct. The motivation to study more general functions is clear and the experiment helps significantly with it (I would even discuss about the experiment earlier in the paper). Yet, I dislike the  use of ``necessary" conditions for the approx. monotonicity and approx. triangle inequality. Indeed, at the complete absence of any of the two conditions the authors provide counterexamples. To my understanding that is not enough for a necessary characterization of the error losses: the condition is necessary only if any function (and not at least one function) which does not satisfy this property cannot be approximated in the appropriate sense. Also, I am not sure what is the zero-one law the authors are referring at the title. It will be great if the authors offer an explanation for it.  Clarity: For the most part, the paper is well-written and the arguments relatively clean. One comment is that I consider important is to explain for which choice of r Algorithm 1 works for the general theorem (the initialization step says that r is of order log n which is rather confusing: can I choose r to be 0 for example?) Also I consider important the authors to explain the use of Cramer's rule in the beginning of Subsection 2.1.  Significance: The paper points to an interesting direction of classifying the loss functions for which an approximation algorithm with appropriate guarantees can be constructed. They provide certain natural sufficient assumptions under which this is happening, which generalize much beyond the standard \ell_p norms. Their direction can definitely impact future theoretical and potentially empirical research.