The paper tackles the questions of how neural networks are able to generalize despite having much more parameters than training samples and how stochastic gradient descent is able to provide a good network despite the highly nonconvex optimization. The authors do this by showing how neural networks are able to perform almost as well as the best performer in a broad concept class.   Overall, the theorems are powerful since they do not pose as stringent constraints as other works: no assumption is made on the data distribution and the target concept class is broad (involving nonlinear functions). Further, the paper provides theoretical insight on three-layer networks and show their power over two-layer networks. Thus, the insights are significant and provide new insights to the research field.   Finally, the paper is clearly written, providing a clear discussion of how this work relates to others in the field and providing some intuition into the technical results.