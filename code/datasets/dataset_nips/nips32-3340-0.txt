POST-REBUTTAL  I thank the authors for their detailed response. My main concern was the level of experimental detail provided in the submission, and I'm pleased that the authors have committed to including more of the details implicitly contained within the code in the paper itself. My overall recommendation remains the same; I think the paper should be published, and the strong Atari results will be of interest fairly widely. However, there were a few parts of the response I wasn't convinced by: (1) "(D) Inefficient Hyperparameter": I don't agree with the authors' claim that e.g. QR-DQN requires more hyperparameters than FQF (it seems to me that both algorithmically require the number of quantiles, and the standard hyperparameters associated with network architecture and training beyond that). (2) The IQN vs. FQF discussion. In particular:   (2.i) "It is not difficult to extend FQF to support arbitrary number of optimal quantiles as    in IQN: we can modify the quantile network into a recurrent network so that it generates a single τ each time and    takes state, action and previously outputted τ s as input to generate next τ".    This seems non-trivial as a modification to the architecture, and it's not clear to me that this proposed solution would work in practice.    (2.ii) "Thus, intuitively the quantiles generated in FQF should be better than sampled quantiles (as in IQN) in terms of quantile function approximation in most cases."    This seems speculative, and I don't think there's anything in the paper (beyond the Atari results) to back this up.  --------------------  HIGH-LEVEL COMMENTS  The paper contributes a new distributional RL algorithm, which parametrizes return distributions as mixtures of Dirac deltas, and learns both the locations of the Dirac deltas, and their probability masses, extending the parametrisations used in QR-DQN and C51. This adjustment is explained reasonably clearly, and the empirical evaluations on Atari are strong, and improve considerably on related earlier algorithms. I have checked the proofs in the appendix. Based on these results, I believe the paper should be accepted.  However, the paper is lacking some experimental detail, such as precise architectures/optimizers used, hyperparameter sweeps undertaken, etc.. Whilst some of this can be discerned from the attached code, not all of it can, and it any case it would be clearer if it were presented an appendix of the paper. The code itself could do with better documentation; at the moment, several “TODO” notes are left, and blocks of unused code commented out etc.  In Section 4, the following claim is made “We also set a new record on the number of 222 games where RL agent performs better than human.”, with Table 1 showing that FQF beats human performance in 44 Atari games. However, the agent in (Kapturowski et al., 2019) attains super-human performance on 52 of the 57 Atari games - can the authors comment on this?   DETAILED COMMENTS   Lines 44-54: I think it’s inaccurate (or at least unclear) to suggest that the distribution can be better approximated by FQF than IQN. If I understand correctly, the distributions FQF can express are restricted to the parametric form in Eqn (1), whereas IQN (at test time) can sample many more quantiles than were used in each stochastic gradient computation in training, and therefore is not limited to a particular parametric family of return distributions (although clearly it is limited by the expressivity of the network).  Lines 104-108: I don't agree with this statement - there will be variance in the IQN gradient introduced by using a finite number of samples in each gradient estimate, but this does not affect the approximation error of the converged system. This is similar to the variance that may occur in general distributional RL updates due to using sampled transitions rather than integrating over the dynamics of the MDP.  Lines 136-138: The reason Eqn (2) cannot be minimized with SGD is that it is not possible to obtain an unbiased gradient.  Line 175: It’s not clear to me what these two sentences mean. In general, the Bellman target and the fitted distribution will not be equal, due to the approximation error incurred due to the network using a parametric family of distributions.  Algorithm 1: there are several minor things here that could do with neatening up: Q(s’,a’) appears outside the loop over a’; Z’ and Z are not defined.  Figure 2: the error bars/mean looks off for DoubleDunk?  Line 204: It is unfortunate that results for Defender and Surround were not included; however, the authors have based their experiments on a commonly-used framework, so I do not count this against the submission.  The experimental results the authors report are very strong. Can the authors clarify whether the shaded regions in Figure 2 are standard deviations across the seeds run?  Lines 233-238: It is unclear what the hyperparameters that the authors refer to are. If I understand correctly, their method requires selection of the hyperparameter prescribing the number of atoms to be represented (as with several other distributional RL agents), and additionally requires tuning the training hyperparameters of an additional network, since the probabilities are output by a separate network. I also disagree with the claim that it has been *theoretically* shown that the algorithm proposed in this paper achieves better performance than IQN. As I understand it, the distributions expressible by IQN and FQF are completely different: IQN can express any distribution for which the quantile function is expressible by the network. In particular, all such distributions will be absolutely continuous. In contrast, any distribution that FQF can express must necessarily be finitely-supported.  Appendix: main derivations: “W_1/\partial\tau_i”->”\partial W_1/\parital\tau_i”. 2nd line: in the second integral, the lower limit should be \hat{\tau}_{i-1}.  Minor typos “Dopamine” should be capitalized throughout. Line 78: “A’ \sim \pi(\cdot|x’)” ->“A’ \sim \pi(\cdot|X’)” Line 253: “Motezuma” -> “Montezuma”  References Kapturowski et al.. Recurrent Experience Replay in Distributed Reinforcement Learning. ICLR 2019.