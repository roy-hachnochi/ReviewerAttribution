The proposed approach of making the discriminator's task progressively harder to regularize GAN training appears novel, and makes sense.   I have a relatively large reservation regarding datasets; all use small images by today's standards, CELEBA-HQ128 being the only one at its resolution and the rest in smaller resolutions. Given the success of progressive growing (Karras18) in pushing to high resolutions, I feel we're not seeing the entire picture here. Direct comparison to progressive GAN – and perhaps a combination of the two – would also be interesting. I realize large resolutions mean higher computational demands, but the comparison could be made also in smaller resolutions.  Improvements over dropout appear modest, but to the authors' credit they clearly state they searched for and used the best-performing variant. It would be interesting to know how variable the results are based on which layer dropout is applied to.  Overall, I think this is a solid contribution, but one that does not appear to bring dramatic new capability or understanding.