This paper introduces a novel regularization method (e.g. progressive augmentation) of the original GANs to avoid the overshooting of discriminators and improve the stability of GAN training. Instead of weakening or regularizing the discriminator, the idea is to augment the data samples or features with random bits to increase the discrimination task difficulty. In this way, it could prevent the discriminator from being overconfident and maintain a healthy competition, which would enable the generator to be continuously optimized. The augmentation could be progressively levelled up during the training by evaluating the kernel inception distance between synthetic samples and training data samples. The proposed method has been demonstrated on different datasets and compared with other regularization techniques. The results show a performance  improvement of the progressive augmentation (though there is no noticeable increase in visual quality). The paper also shows the flexibility of the proposed method. The progressive augmentation could be used with other regularizers and the combination could have good performance. The future work focuses on the implementation in semi-supervised learning, generative latent modelling and transfer learning.  Overall, the content of this paper is complete and rich and has good technical quality. The results are well analysed and evaluated, and the claims of the paper are supported. The clarity is good but could be better.  The author's response has been taken into account.