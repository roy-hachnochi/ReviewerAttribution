The motivation of this paper is clear. That is reducing the evaluation cost on the hyper-parameter tuning tasks. Some previous works have been proposed to tackle this issue, such as warm-starting optimization processes, multi-fidelity optimization, etc. This paper proposed to build a meta-model among problems that are from a problem distribution. By applying the Gaussian processing to capture the relations among problems (it likes the surrogate function in Bayesian optimization), the evaluations in the new problems are cheaper. However, this paper is poorly-written and poorly-organized. There are full of typos and grammatical issues in this paper. Some detail comments are listed below: 1. In Figure 1, how much hyper-parameters are you selected in the experiments? Random search is hard to beat GP-BO when hyper-parameters are more than 2 in common sense. 2. What is your algorithm framework of the proposed method? Can you show the pseudo-code of the proposed method? 3. What is the conclusion we can get from the Figure 3? Can you explain it? 