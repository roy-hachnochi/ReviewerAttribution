Originality: The work proposes an interesting approach with a factorized policy to perform RL by performing iterative improvement over the current solution until convergence is achieved. Using RL for combinatorial optimization is not new, even though the specific idea that the authors propose differs from previous works that I am aware of. Quality: I believe the work is technically sound. It uses standard policy gradient theorems and the well-known actor-critic framework. Clarity: The paper is clearly written with sufficient experimental details. On the other hand, I would like to see more about connection to local search and some further improvements (see point 5 below). Significance: Given that (1) solving NP-hard combinatorial problems is a very important task and (2) the experimental results outperform standard solvers, I think that the current work is fairly significant due to its practicability.   UPDATE I have read both the rebuttal and the other reviews. I appreciate the authors' feedback, particularly with regard to adding more information on the connections to local search, and the wider applicability of this work. For this reason, I am willing to increase my score to 7. I would just encourage the authors to ensure that, if accepted, the camera-ready will clarify all points raised in the rebuttal.