The paper addresses the problem of lacking global context on the target side when generating output from left to right. It starts by pointing out that other methods which do decoding in more than one step (by e.g. first generating the sequence right to left) are inefficient computationally. The work proposes a soft prototype, which combines multiple potential outputs on the target side and is more efficient than previous work.  The motivation is very clear and the paper is, for the most part, well written and explained. The approach is not ground-breaking, however it is a good approach that makes sense for addressing the problem considered and if more efficient than previous work. Better MT quality improvements would have made a stronger case, however the method is clearly better than previous work and the improvements are stable across different settings.   Specific questions/comments:  - Section 3.2: how does this become non-autoregressive? By not modeling any dependency between the rows of G_y (i.e. between gx(i))? I did not find the discussion at the end of 3.1 to be very helpful to understanding 3.2 - 3.2 seems the simpler formulation. - Table 1: Does the number of params/inference time refer to the French or German model? The different vocabulary sizes have to lead to different number of parameters.  - Minor: Not sure I fully understand the choice of term “prototype” in this work - the term makes more sense when the data is retrieved (citations 4 and 5 in the paper). 