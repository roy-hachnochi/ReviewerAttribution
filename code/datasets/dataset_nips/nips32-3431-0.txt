[review updated in Improvements section] The paper is very well written.  The paper tackles the problem of sampling for Active learning such that a mini-batch of examples is diverse. It proposes a Bayesian approach as a solution.  In order to resolve non-tractability of the original problem, the authors take expectation of outcomes w.r.t. the current predictive posterior distribution, and Bayesian core-sets (which calculates total expectation by constructing a sparse subset approximation).  A tricky part of the approach in practive is to construct a suitable inner product, the authors suggest some variants for generalized linear models, and random projections for non-linear models.  In the experiments on synthetic and real data from UCI, MNIST, cifar10, SVHN, and fashion MNIST, the authors demonstrate that the method achives what it was purposed to achieve (diversity, competitiveness, scalability).  I think the paper is a good contribution. It solves an important problem, suggests a novel algorithm, and has thorough evaluations of the most important aspects of the algorith.  I also noticed the authors haven't referenced one of the recent relevant works (I think it was on arxiv only), "Diverse mini-batch Active Learning", which might add to their baselines.