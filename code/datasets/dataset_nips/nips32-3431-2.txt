The paper considers the problem of selecting an optimal batch of samples for evaluation to increase the predictive performance of the model over the entire dataset, aka active learning. The paper considers a Bayesian perspective, where the goal is to select a number of points (fixed budget) to minimize the difference between the resulting posterior and the posterior one could have had if all the points were labeled. The problem is formulated as a sparse data approximation problem and corresponding algorithms are presented. Some derivations and interpretations are made in simple models (linear, logistic). Also the algorithm is extended to deal with larger datasets through the use of random projections. A collection of experimental results for regression and classification on publicly available datasets are presented.  Originality: As far as I know the algorithmic development, the interpretations and the experimental results are original.  Quality: The material appears to be sound, although some derivations are presented in the supplementary material which I didn’t check. I would like to have a few things clarified: There has been other work on Bayesian active learning that seems to fit well the batch setting that has not mentioned: “Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach” by Krause et al. Although the focus is on GP, I find the discussion and derivations very relevant to the general Bayesian setting. What is the relationship with this work? If this is not suitable for batch AL, why? If yes, how is it different? What is the relationship between the objective proposed in this paper and one of the objectives discussed there? Please provide a careful derivation of Eq.4, as in its current form it is not clear where the first equality is coming from  Clarity: The paper is well written overall. However the motivation for the batch AL setting can be improved. Typically in AL the cost of a label is significantly more expensive than computation time. Yet in this case the argument is that the label cost is “cheaper” then updating a model, leading to grouping of samples. Indeed, what are the real applications for this scenario?  Significance: Given concerns about the motivation I find this work of somewhat limited significance. Evaluations on the MNIST datasets, although useful, do not relieve those concerns. In other words, experiments on a dataset where labeling costs are “on par” with model update time would be ideal.   Post rebuttal feedback: Thank you for clarifying some of the concerns I raised, including those in the paper will certainly improve it. That being said, the point raised by R2 about the simple way to construct the batch through imputation and model update is still valid. The paper would have been much better if either: experimental results comparing with this method are done; or discussion with sufficient details is provided regarding the cases where model update is too expensive to run. So far I can think of this issue possibly happening in the parallel simulation setting only, but none of the other motivational examples mentioned. As a result, my score remains the same.