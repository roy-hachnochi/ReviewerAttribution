*Originality and Significance*  I think the contribution of the current paper is quite incremental over [2,20] and other works on similar topic, some of which the authors have not cited (such as the ATOMO algorithm which provides a method for handling sparse vectors). But more than this, I feel the authors have failed to appreciate a few points:  1. The authors have allowed small interaction between the parties and even downlink communication from the centre to the parties. This communication is critical for normalising the coordinates and using a quantiser with a fixed dynamic range. However, one of the main points in [20] was to rotate randomly so that this range can be determined by \ell_2 norms, which was assumed to be bounded and need not be described. Authors have a rather simple scheme, but it still requires coordination between the parties by the centre. I feel that random rotations such as randomised Hadamard transform should have been used to avoid this.   2. The lower bound seems to only consider SMPs and doesn't allow for the downlink communication by the centre. In this sense, I feel that the optimality of the proposed scheme has not been established in a strict sense. The authors continue with the running assumption of [2,20] that the norm can be described using a fixed number of bits that can be ignored. I find this assumption a bit absurd for theoretical contributions, but am now accustomed to seeing it in papers on this literature. However, the authors should clearly specify the scope of their lower bounds and allowed communication protocols.   3. There is not much innovation in the scheme or the lower bound. Both seem to follow the template of [20]. The main contribution is the choice of normalising parameter F_1/C in the scheme which allows the authors to exploit sparsity. I like authors choice of Hoyer's measure of sparsity since it will seamlessly bridge between sparse and non-sparse data and will not require a separate evaluation of sparsity before deciding which scheme to use. But the technical contribution beyond prior work is minimal and doesn't suffice for publication at NeurIPS.   *Clarity*  I think the presentation is rathe clear. Personally I would have preferred more exhaustive set of experiments with applications in distributed optimisation, such as those in [2], but I am happy that the authors made an effort to compare their results with those in [20] and repeated the same experiments. Also, the reported improvement shows that indeed the data in these experiments comprises sparse observations, a point which was raised in some earlier work as well. Even the SGD updates for training CNNs on MNIST are sparse.     [Update after reading the rebuttal]  I have to admit that my first reading of the paper was less thorough than I wanted it to be, I will blame the short review cycle for it. I read a few sections again in order to fix the "factual errors" in my review. Here are my additional comments:  1. Lower bound: For some reason, I saw the distribution and information complexity popping-in, and just assumed that there is a Fano bound hidden somewhere. Indeed, this was a gross oversight on my part, apologies. Looks like the main idea  was to use Lemma 3.4 to relate MSE to the conditional entropy H(X|\Pi) for a specific distribution. It is an interesting argument, but its relation to existing lower bounds in distributed Gaussian mean estimation has not been clarified. Since the proof doesn't look difficult, I can't comment on the innovation here.   2. The scheme: I think my evaluation of the scheme was not a "major factual error." It is indeed very simple extension of prior work: The main difference is the choice of F_1 to normalize the values so that the clients can use appropriate interval for uniform quantization. But this F_1 had to be computed based on norms of all the vectors and prior work was not at all looking at this case. The authors have clarified how their goal was to capture "local sparsity" and sort of global sparsity at the same time. That's interesting.  Overall, I think the paper is acceptable for NeurIPS.