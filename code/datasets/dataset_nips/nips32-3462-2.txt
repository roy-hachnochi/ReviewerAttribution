This paper is well written and properly cited. The interplay between batch normalization and the behavior of Fisher information is interesting. Though, it is questionable whether this calculation explained batch normalization helps optimization. The authors calculated the FIM of training loss at initialization, and it is not the same as the Hessian of training loss. Therefore, it is questionable to say FIM reflects the "landscape" of the training loss function. Also, whether suppressing the extreme eigenvalues of FIM can also make the Hessian (at initialization, and along the whole trajectory) well-conditioned is not clear. Hence, there is a gap in showing batch normalization providing a larger learning rate. Is it due to technical restriction that one cannot calculate the behavior of eigenvalues of Hessian at initialization?   In terms of technical contribution, is there some new ingredients in the calculations of batch normalization settings, or it is just a simply re-application of calculations in [1]?   [1] Ryo Karakida, Shotaro Akaho, and Shunichi Amari. Universal statistics of fisher information in deep neural networks: Mean field approach.   --------  I am satisfied with the authors' response to my questions. I am also glad to see the authors did more experiments in the response. I think I can increase my score to 7.   Though, I think the "landscape" terminology used in this paper is not accurate. The FIM is dual to NTK. By the NTK point of view, the dynamics of neural networks is roughly linear, and what matters the convergence is the condition number of the NTK (sharing same eigenvalues as the FIM). The "local shape of landscape" terminology may confuse the readers. I think the authors could explain this more clear in the paper. 