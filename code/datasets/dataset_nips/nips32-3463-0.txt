This work presents ways to obtain estimates of the aleatoric and epistemic uncertainties of deep neural networks. The aleatoric uncertainty is estimated by learning the quantiles of the target variable via Simultaneous Quantile Regression (SQR); it minimizes the pinball loss where the target quantile is randomly sampled in every training iteration. The epistemic uncertainty is implicitly estimated by Orthonormal Certificates (OCs); these are functions that are trained to map in-distribution examples to zero whereas out-of-distribution examples to non-zero values. The authors also provide tail bounds for the OCs in the case of Gaussian input data, which does provide some intuition about the behaviour. Simplicity is a benefit of these estimators and the authors demonstrate their performance on regression and classification tasks.  Uncertainty estimation for neural networks is an important topic and relevant for Neurips. The paper itself is well written, and conveys the main ideas in an effective manner. The ideas themselves are interesting and, as far as I am aware, novel. They seem to be simple to implement, a fact that gives them a nice bonus for practical applications. The experimental evaluation is relatively thorough as there is a comparison to a variety of other methods.   My main point of feedback is that the authors tend to exaggerate their results, as they claim SOTA performance.  - For the evaluation of the aleatoric uncertainty they experiment on the regression task from [1] and their results show that SQR is usually within the standard deviation of the much simpler Conditional Gaussian baseline, thus offering no real improvement. Granted that the predictive distributions might not be very complicated to begin with in those datasets (as a single Gaussian works just as well), the authors should find a different task to make the effectiveness of SQR more convincing. Furthermore, as SQR promises more flexible predictive distributions (e.g. asymmetric, multimodal etc.), I believe that the authors should also compare with a mixture of Gaussians likelihood, another simple alternative that can facilitate for such predictive distributions. - For the evaluation of the epistemic uncertainty the authors compare against various methods on an out-of-distribution detection task. Similarly to the previous point, the performance of OCs is usually within the standard deviation of the baselines thus denoting that OCs do not provide a (consistent) significant improvement. Furthermore, for the svhn dataset it seems that the bold number should instead go to “largest” and “entropy” rather than OCs? They both have a 0.93+-0.01 compared to the 0.91+-0.00 of OCs.  As for other comments for general improvement of the manuscript: - For the regularization of OCs the authors seem to employ a soft constraint for orthonormality; while this is simple to implement it introduces one extra hyperparameter that needs tuning, the regularizations strength \lambda. How sensitive are OCs to the setting of \lambda? A way to bypass this hyperparameter is via a hard constraint. Did you experiment with such a constraint, e.g. [2] and related, for the certificates? - What was the network architecture that you used for the aleatoric uncertainty experiment and what was RMSE for all of the methods? Furthermore, what was the dropout rate that you employed and did you optimize over it (e.g. with cross-validation)? Regression tasks are usually more sensitive to the dropout rate than classification ones. - What was the choice of the loss for the OCs in the epistemic uncertainty experiment? It is a bit unclear to get this information from the code as there are two possible choices.  - Why did you select a part of the training set as an out-of-distribution dataset? While I do agree that it is more challenging, it also makes the comparison against other works more difficult, as this is not the standard evaluation. Furthermore, I believe that it will be worthwhile to have at least one experiment where you compare out-of-distribution detection across-datasets. For example, you can take the model trained on “CIFAR-5” and then evaluate the detection from the baselines and OC’s on SVHN (similarly for MNIST and Fashion-MNIST). This will demonstrate whether each method generalizes across the peculiarities between datasets.  Overall, I believe that this is good work and, if the authors address my comments, I will recommend for acceptance.   [1] Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks [2] Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections  # Update after the authors rebuttal I am happy to increase my score, since the authors adequately addressed my comments in their response.