Originality/Significance: Although the proposed losses are established in other application areas/problem settings, and although their use in this application is kind of “obvious in retrospect”, these losses were not previously used in DG setting together with meta-learning. So it's good to highlight their efficacy in this problem setting. But otherwise the novelty is limited as the same meta-learning pipeline proposed for few-shot in MAML [10], and extended to domain generalisation in MLDG [23] is used.   Engineering Issues: (i) Many hyper parameters are introduced (e.g. Algo 1). Tuning these is not straightforward in DG problems. (ii) L_global and L_local both trigger second order gradients, which makes training slow. (iii) L_global seems to compute all pairs of available meta-train and meta-test, which is slow and scales badly with number of domains.   Empirical: (i) Recent DG papers [1] used ResNet rather than out of date AlexNet. If this paper is accepted, ResNet experiments should be included to avoid making the paper be of out-of-date relevance before its even published.   Minor: -  Some recent methods like [28] missing from comparison table.  - L.219 highes -> highest     Overall assessment: It’s quite a “vision style” paper. Not really a fundamental machine learning development. However the motivation, explanation, ablation, and numerical performance are all quite good, and the real medical application is an icing on the cake. So it could be acceptable for NIPS.  -----  Update. I have read the author feedback and other reviews. Besides the somewhat vision style, I don't see any real flaws. The updated addition of ResNet experiments will benefit the longevity and relevance of the paper.  Hopefully the authors will also share code so others can build on it. 