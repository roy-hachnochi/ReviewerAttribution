I have read the rebuttal and the other reviews and maintain my score.  I look forward to discussion of the (apparently since improved) training time in the main text.  ========  The submission concisely presents a simple but powerful idea that will have impact. The DEQ model is a sensible model that draws on neural ODE-type ideas but stands on its own. (To my knowledge this is the first application of implicit depth ideas to sequence modeling). The derived algorithm is sensible, clearly explained, and the proofs appear correct (I checked them). The experiments seem reasonable and rigorous. The paper is well-written and exceptionally easy to follow. Overall I strongly recommend accepting this paper.  A few questions: - in the paper, the authors state (and empirically show in the Appendix) that they can get away with a large \eps tolerance at inference time. How did they decide upon the \eps=1e-6, 1e-8 choices for training? (which one would expect may change the quality of the learned model) - How were the models initialized? I would expect this has substantial influence on the stability of training, since it may affect e.g. whether f is non-expansive. (From the code it looks like the initialization is pretty standard but this should be discussed in the main text) - the current version of the paper defers all discussion of runtime to the Appendix. Since the (slightly) slower runtime is a drawback of the model, it would be more fair to state this in the main text.