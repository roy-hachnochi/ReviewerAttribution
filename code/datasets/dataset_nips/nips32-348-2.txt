I have read the author response and am satisfied with it. As the other reviewers have pointed out, it would be good to move the discussion on training time to the main text.  -----------------------------------------------------------------------------------------------------------------------------  This paper proposes a new approach to modeling sequential data called Deep Equilibrium Models (DEQ) whereby instead of using a weight tied deep neural network its fixed point is directly computed. Results show that this approach still gives the same perplexity as the deep neural network but has much lesser memory footprint. However the inference and training cost could be 2-4x times higher. The paper is written and organized nicely and is easy to follow. The idea itself seems novel and addresses an important problem of reducing the memory footprint of deep neural networks. However I am not sure what happens when the fixed point is unstable? Nevertheless the authors have done extensive experimentation both on synthetic and real-world datasets and have shown that DEQ reduces the memory footprint by more than 80% while giving the same perplexity.  