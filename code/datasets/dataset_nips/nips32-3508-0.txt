The paper's writing and figures are of very high clarity and quality. The method is novel and the basic innovation is in the new objective function, which has encoder-decoder dynamics that are intriguing. The area of research is tackling the difficult problem of trying to reconstruct images from human brain activity with recent machine learning and neural network techniques, which is a strong fit for the NeurIPS conference. The results in Figure 4e) are impressive and look like a convincing improvement over Shen et al. 2019 as they do not need a generative model prior at all, but train an end-to-end architecture. The only ImageNet statistics in their network are pretrained low-level AlexNet features (thus also further lowering the potential influence of category set statistics).  The fact that I am singling out the results in Fig. 4 e) as being impressive however brings me to the major shortcoming of this work:  Target test data was used for training the decoder. I'm not convinced that the fact that there was no clear association between target fMRI and image resolves this problem. The reasons for this are that:  * The test sets of the two used data sets consist of 50 and 120 images respectively. The L^DE loss being computed on test could essentially lead to learning to reconstruct the tiny set of test images from fMRI and fMRI from the reconstructed test images. It is unclear what is really being learned here.  * Strong evidence for the truth of this statement is that 4e) has visibly diminished quality from the reconstructions where target test was included. It is likely that the model simply has learned by heart to encode(decode(testBOLD)).  * The absolute magnitude of the two image losses will be very different from the L^DE loss. I wouldn't be surprised if this loss is dominating the image losses, further increasing the influence of learning to reconstruct the target test data.  Also, the underlying motivation for tackling the problem of different train and test statistics in reconstruction data sets is difficult to grasp. Most of the data sets available in the field use test sets that are averaged over multiple repetitions to increase SNR, which indeed leaves the test set with different statistics, which indeed can have a diminishing influence when the aim is reconstruction.  This has to be done for different reasons: Partly because the problem is too difficult with currently available brain imaging methodology without artificially reducing noise; partly because the aim of studies like Nishimoto 2011 was testing a certain hypothesis for brain representations (with the reconstruction's aim showing that the model is indeed powerful), needing high-SNR data to lead to correct results. (By the way, to readers unfamiliar with the field the paper should optimally explain why the field is using higher-SNR test data at all.)  A general-purpose visual system decoder, which may be the ultimate aim of this research line should adapt to or not expect different statistics in training data and the data that should be reconstructed. So it is unclear whether adapting to different test set statistics is the most important problem to solve.