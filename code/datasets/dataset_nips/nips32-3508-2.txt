 There are many choices of hyper parameters, such as alpha, or the choice of the architecture. How to safegard against overfit of the test metric? Given that the test sets are of limited size, it risks happening fast.  With regards to the encoding architecture, starting from the images, I would be curious to see how unsupervised training compares to classic pretrained architectures.  In equation 3, it is not obvious to me how to balance the three terms of the loss: how it is done in practice, and what are the guiding principles. 