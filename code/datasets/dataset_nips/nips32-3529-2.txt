The authors propose variational autoencoder where inference is performed bidirectionally (top -> bottom and bottom -> top) with the intention of enhance the flow of information and avoid inactive units. This is achieved via multi-layered stochastic variables and a deterministic backbone network. The proposed inference model is akin to ladder VAE but with stochastic layers. The proposed model does not contain autorregressive elements. The authors present extensive results on image datasets and also consider semisupervised classification and outlier detection tasks.  It is not clear why p_\theta(x|z) is conditioned on the entire set of latent variables rather than z_1 as in Figure 1a, unless off course this is the case when accounting for the (deterministic network) skip connections in Figure 1d. In either case, it needs to be clarified. Similarly, why is q(z_i^BU|x,z_<i^BU) dependent on x considering Figure 1b and q(z_i^TD|x,z_<i^TD,z>i^BU,z_>i^TD) dependent on x considering Figure 1c? From the description in Section 3.2 it seems clear that the model does not explicitly use skip connections as in Figure 1d but the deterministic path implicitly acts as skip connection (and conditioning on x) serving as backbone for all the layers. That being said, Figure 1 does not seem to help explain the concept behind the model.  I enjoyed reading the paper (minus the graphical model), it is well written, well motivated and the experiments are well thought, extensive and convincing.  Post-rebuttal:  The changes to the model description and graphical model, as well as the error bars on the results are welcome additions to the revision. 