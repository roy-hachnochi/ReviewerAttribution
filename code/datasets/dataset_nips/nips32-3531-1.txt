This paper proposes to train simultaneously two models one for code generation and one for code summarization. The main observation is that these two tasks are dual and they can be constrained when trained simultaneously to take advantage of the duality. The constraints are related to the probability correlation and the symmetry of the attention of the two models. The models are based on traditional Seq2Seq models. The probabilistic correlation of the two models is used to define a regularization term, while the symmetry of the two tasks are reflected in regularization terms based on the attention weights matrices.  The paper compares this approach with state-of-the-art approaches for two different datasets (Java and Python) and show improvements in the 1-2% ranges for scores such as bleu, meteor and rouge. While the duality observation is interesting, the improvement in performance is limited.  The ablation studies in the paper show that each constraint based on the two dualities brings a tad bit of performance improvement.  The paper is well written and straightforward to follow (some suggestions for improvement below).   =============== UPDATE: Dual-task training for CS/CG is neat and in the light of the clarifications in the authors' feedback, I'm increasing my score 6->7. Could you please include in the paper the following (from the authors' feedback): the discussion on grammars as additional input, the results on valid code, the clarification on how the language model is used, how the warm state for the two models is achieved, the diagram with the architecture. I think all these clarifications will increase the quality of the paper.  