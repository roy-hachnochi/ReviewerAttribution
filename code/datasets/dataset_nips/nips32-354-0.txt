This paper addresses the problem of training hard-attention mechanisms on image classification. To do so, it introduces a new hard-attention layer (called a Saccader cell) with a pretraining procedure that improves performance. More importantly, they show that the approch is more interpretable requiring fewer glimpses than other methods while outperforming other similar approches and being close in performance to non-intepretable models such as ResNet.  Originality: The proposed Saccader model is original and compares favorably to state of the art works in term of performance and also, more importantly, interpretability. Related work has been cited adequately. However, it is not clear from the paper what are the main technical difference(s) between Saccader and its main competitor DRAM.   Quality: Experimental results show how the Saccader model outperforms comparable and state-of-the-art models. Indeed, figures allow us to see the differences in accuracy and in image coverage. The latter is quite informative for the interpretability claims of the paper. However, no weaknesses of the work have been noted. Indeed, while the results are important, no ablation study has been made with the Saccader cell and the attention network. These two components contain several sub-components tied together and it is not clear that they are all necessary. Furthermore, it is not clear why the attention network is needed at all. Could the increase of parameters of the attention network result in the increase performance of the Saccader model in comparison to the DRAM model?  Clarity: The paper was well organized. Sections follow the usual order of NIPS papers. Small comments: In Section 3.1, item 2., it is not clear why is the “what” and “where” features are called this way. In Section 3.1, item 3., at that point in the paper, it is not clear why there is the concept of time $t$. One or two sentences grossly explaining the reinforcement learning part of the paper at that point might make the paper clearer.   Significance: In addition to what has been noted in the Contributions section, while the Saccader cell and its pretraining procedure have been designed for convolutional networks, there is a safe bet that this cell can and will be used beyond computer vision tasks such as NLP and few-short learning.