The major issues of this paper are related to the motivation. Though it claims in L50 that this is the first end-to-end trainable algorithm that learns the instance segmentation model using bounding box annotations, this does not explain well the value of such problem. If the motivation of using bounding box annotation for training instance segmentation is that such bounding box is cheaper than boundary annotation, then there should be a study of performance versus annotation effort, e.g., in terms of annotation expense or total annotation time. This will answer if weakly supervised instance segmentation achieves better performance than fully supervised on given the same amount of annotation time/money. It may also be possible that given the same amount of time/money, the fine pixel annotation is better than coarse bounding box annotation in terms of training. [R1] performs such a study as reference in terms of semantic segmentation.  [R1] On the Importance of Label Quality for Semantic Segmentation, CVPR, 2018   Line85: This is a misleading statement. For some real-world applications, the pixel-level annotation is required. Moreover, it should be noted that the non-proposal based instance segmentation can be exclusively applied to some other more challenging tasks, like C. elegans segmentation which are more deformable and often cuddle with each other for instance segmentation. In such cases, proposal based methods cannot handle well, though they perform well for segmenting oval-shape objects.  Eq.4 How to set epsilon? Why using Eq. 4 helps enlarge the segment size? Why not training to classify all positive patches as positive labels? Why must it use MIL loss given that all the positive patches are positive in some sense?  Line100: There is no support for "efficiency" of the proposed method over [16].  In general, the paper about using MIL for weakly supervised instance segmentation is not persuasive. It does not explain well why MIL works so well -- on some metrics it even outperforms the fully-supervised Mask RCNN. Given the results, an in-depth analysis is required to explain the advantage.   ------------------------- The rebuttal provides answers to most of my questions. There are still a few concerns --  1) I still have difficulty in understanding why the MIL works so well with multiple sampled patches as positive/negative bags. From Fig. 1 and Eq. 3, I don't know how the MIL forces the model to choose what patch for the positive bag. Perhaps a visual demonstration may demonstrate this. The authors partially answer this in "The four questions about Eq. 4".  2) The answer provided in the rebuttal is very important that studies performance vs. annotation effort (time, money). That's one of the main motivation why weakly supervised learning is important -- if one has infinite money, then annotation for fully supervised learning is no problem. If this paper is finally accepted, I strongly suggest authors include this in the main paper especially given that the paper is submitted to machine learning venue and includes little theories.   Another minor concern is the claim about L85 "in general, the applicability of the fully supervised methods “may” be limited in the real world because of the high annotation cost". This really has ambiguity in defining the "general application" -- production oriented application in companies, or numerous applications in-need in biology/medical science, or others. That's why I would rather see the performance vs. annotation cost to motivate weakly supervised learning instead of saying this vague statement.