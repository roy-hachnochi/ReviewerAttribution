Originality? This work is the first to concretely show that random features alone cannot be used to explain why overparameterized networks work well.  This work is introduces a healthy amount of skepticism in employing vague intuitions for explaining "why neural networks work." This is neither the first nor the last paper with such a premise, but it is a valuable addition to the community.  Quality? The mathematical foundations of the paper appear solid. I did not check the math thoroughly.  Clarity? I find this work moderately unclear. Explicitly introducing the key theorems, their consequences, and their limitations early on would go a long way to framing the rest of the paper.  Significance? I find the significance of this work low yet valuable. This paper illustrates a particular example dataset that is difficult to learn with random features. The existence of such a dataset doesn't necessarily imply the "random features" interpretaion of neural networks is invalid; simply that it is insufficient on its own. 