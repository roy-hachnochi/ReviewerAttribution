In this article, the authors studied the limitation of the random feature-based single-hidden-layer neural network model. By considering a standard d-dimensional Gaussian model for the input data, the authors showed that, for a large class of activation function f, the random feature transformation of the type u' f(W x) for (the rows of) W uniformly distributed on the unit sphere cannot well approximate a single ReLU neuron output, unless either the number of random features or the amplitude of the second layer weight u are allowed to grow exponentially with respect to d.  This article brings novel understanding into random feature-based techniques, and perhaps more importantly, provides the critical analysis arguing that, despite their nice theoretical properties, it seems unlikely that random feature techniques are behind the many success of deep neural networks today. I am pleased to recommend it for publication.  **After rebuttal**: I have read the author response and my score remains the same.