The main lower bound on random features approximating RELU is quite nice and elegant. It builds on the idea that random features cannot approximate certain oscillating functions (which is implicit in some previous work) which are approximable by RELUs. The authors then use an elegant symmetrization argument to say that if your random features are coming from a nice distribution, then you should not be able to approximate even a single RELU. The argument to rule out more general classes of random features is similar in spirit.   In comparison, it is known that a single RELU (under the distribution class for which the above lower bound holds) can be learned as a RELU by standard gradient descent methods.  Regarding significance, the paper presents a strong case that the line of thought based on random features and over-parametrization in several recent works needs new ideas (at a conceptual level) to shed light on the effectiveness of deep learning.   Comments on writeup: By and large the paper is quite well-written but there were a few statements that seemed vague. It will be helpful to clarify these parts more. 1. Line 63/64 - the sentence needs rephrasing (starts with random features and then has random or deterministic in parentheses). 2. Line 72: 'vanilla neural networks' is too vague at this point. 3. 75,76: 'easily learnable' is also a bit vague. It is indeed early in the paper so perhaps adding a pointer to a later remark explaining this would be helpful. 4. 172-176: It would be very helpful for readers (in assessing the literature) if you can point out which of the previous works fall into which category (of the four you listed). Else, chasing them down for clarity would be difficult.   