Response to the rebuttal:  Thanks to the authors for responding to my questions. I still feel the experiments of the paper can benefit (especially from the rebuttal) from more work but keeping the theoretical contributions I have decided to increase the score to 7. ==========  The authors propose a new sampling algorithm, called Stochastic Proximal Langevin Algorithm, and analyze it for sampling from log-concave distributions where the log density is a sum of smooth and differentiable function along with a sum of non-smooth convex functions. They establish the rates of convergence for the algorithm, linear when the smooth part is strongly convex and sub-linear otherwise (the rates are not surprising).  The authors provide a numerical comparison of their algorithm with a few other algorithms for trend filtering on large graphs, demonstrate that for this set-up their algorithm is useful when the full proximal operator is computationally prohibitive and in general it performs faster than  Stochastic Subgradient Langevin Algorithm. On technical aspects, their analysis appears to be borrowing heavily from [17] (as the authors have correctly acknowledged).  I would like to remark that the paper is well-written and easy to read.   I have a few remarks: — Given the introduction of the new algorithm, I believe the work can be motivated better if the authors provide several concrete set-ups/examples where their algorithm can be potentially useful (with some rough gains that one can obtain). Even for the example, they considered, the significance of sampling in comparison to optimization (simply finding the MAP) is not clearly spelled out (other than noting that it might be useful). — It is not clear to me when is SPLA faster than SSLA? and vice-versa? — There should be some trade-off of splitting the non-smooth part into an arbitrary sum. In simple words given f+g, I should not gain by treating g as g/2 + g/2. — I would recommend the authors to provide some clear explanations for computational trade-offs when such splitting is beneficial, and some intuition as to?  Moreover, do we gain/lose if we use different noise draws for each proximal split?  Is the gain simply analogous to the gains of SGD over GD when the number of samples is large (or is there something more to it)? A paragraph providing intuitions about the new algorithm would make the paper more insightful.