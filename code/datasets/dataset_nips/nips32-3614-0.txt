[Post Author-Response Update] I believe that my concerns about the lack of random baselines in the original submission are partially addressed by the new experiments provided in the rebuttal. While I feel that the new random baselines significantly strengthen the paper's results on CIFAR-100, random baselines are not provided for CIFAR-10, SVHN, or ImageNet.  I've updated my score from a 6 to an 7, based on the random baselines for CIFAR-100 and the authors' promise to clarify their evaluation measure in the final submission.  [Originality] The search space used to derive data augmentation policies is reused from previous work (Cubuk et al.'s AutoAugment), with appropriate citations. However, Cubuk et al.'s original algorithm is extremely resource-intensive. The main contribution of this paper is an algorithm that can operate on the same search space and come up with data augmentation schemes orders of magnitude more efficiently. The most closely related work I'm aware of is Population Based Augmentation (ICML 2019), which tries to solve the same problem in a different way.  It seems like there's not yet a large body of work in this area, and the submission's solution seems novel. Related work appears to be adequately cited.  [Quality] Empirical evaluations of the proposed algorithm are conducted on four different datasets, and the results appear to be technically sound. On three of the four datasets, empirical results are roughly on par with existing results from AutoAugment and Population Based Augmentation (PBA). On the fourth dataset (CIFAR-100), results are slightly worse than AutoAugment/PBA in some cases, and the submission provides a discussion of these results. This is an indication that "authors are careful and honest about evaluation both the strengths and weaknesses of their work." (Quote taken from the scoring rubric.)  The submission also provides experiments showing how the empirical results change when two hyper-parameters are varied: (i) the number of data augmentation sub-policies used to train the final model, and (ii) the size of the dataset used to select augmentation sub-policies. These are likely to be of practical interest for anyone who wants to build on the proposed algorithm.  On the negative side, without random baselines, it was difficult for me to tell what fraction of the quality improvements came from the search space vs. the proposed algorithm. It would be very helpful to add the accuracies of models trained on 25, 50, or 100 sub-policies selected uniformly at random from the AutoAugment search space. Basically: is the proposed algorithm able to outperform random sampling?  [Clarity] For the most part, the paper is clear, well-organized, and polished. However, I struggled to understand the exact criterion that was used to select and score augmentation sub-policies from the AutoAugment search space. Even after reading the "Search Strategy" section of the paper for a third time, I'm not entirely sure that I understand the proposed criterion. My current impression (based on Equation 3 in Section 3.2.2) is that we first train a model from scratch without data augmentation, then select an augmentation rule that minimizes the model's loss on augmented images from a held-out validation set. If this is correct, does it cause problems for a data augmentation sub-policy like CutOut regularization that makes the model's job harder by removing useful information from the input image? (I might be missing something, but it seems like the model would have a high loss if we evaluated it on a batch of input images augmented using CutOut, and therefore CutOut would never be selected as a sub-policy.)  [Significance] If the paper's results hold up, they are likely to be of broad interest for people who want to find better data augmentation policies on new problems and new domains.  [Notes on reproducibility checklist] I'm a bit confused about why the authors responded "yes" to the reproducibility checklist item "A description of results with central tendency (e.g. mean) & variation (e.g. stddev)." I might've missed something, but I didn't see variance/stddev numbers reported in the paper (e.g., in Tables 2, 3, 4, or 5).  The reproducibility checklist indicates that source code is (or will be made) available. However, I couldn't find any source code attached to the submission.