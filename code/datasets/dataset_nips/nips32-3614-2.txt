*** After reading Author Feedback ***  After reading the Author Response, I reconsider my assessment of the paper and increase my score from a 3 to a 4. Below, I reiterate my main reasons for the low score, and how the authors have addressed them.  1. Lack of comparisons against the RandomSearch baselines.  The authors provided comparisons against two random baselines, namely Randomly pre-selected augmentations (RPSA) and Random augmentations (RA), and hence fulfilled my request. However, these numbers (in error rates, RPSA-25: 17.42, RPSA-50: 17.50, RA: 17.60, FAA: 17.15), clearly confirmed my point that that FAA is not *much* better than the random baselines. Note that while the authors wrote in their Author Feedback that each experiment is repeated 20 times, they did not provide the standard deviations of these numbers. Furthermore, their FAA error rate is now 17.15, while in the submitted paper (Table 3), it was 17.30, suggesting that the variance of these experiments can be large. Taking all these into account, I am *not* convinced that FAA is better than random search baselines.  Last but not least, the provided comparisons against the random search baselines is only provided for CIFAR-100. How about CIFAR-10 and SVHN? I can sympathize with the authors that they could not finish these baselines for ImageNet within the 1 week allowed for Author Feedback (still, a comparison should be done), but provided the improvements on CIFAR-100 is not that significant, I think the comparison should be carried out for CIFAR-10 and SVHN as well. Also, standard deviations should be reported.  2. Impractical implementation.  While the authors have provided some running times in the Author Feedback, my concern of the training time remains unaddressed. Specifically, in the Author Feedback, the authors provided the training time for CIFAR-10 and CIFAR-100, but not for ImageNet.  I am personally quite familiar with the implementations of the policies from AutoAugment, and I have the same observation with the authors, ie. the overhead for CIFAR-10/100 and SVHN is not too bad. However, the real concern is with ImageNet, where the image size is 224x224, which makes the preprocessing time much longer than that of CIFAR-10/100 and SVHN, where the image size is 32x32. If we take this overhead into account, then the improvement that FAA delivers, in *training time*, is probably negligible.  That said, since the authors have (partially) provided the comparisons against the baseline, I think it's fair for me to increase my score to 4.  Strengths. This paper targets a crucial weakness of AutoAugment, namely, the computational resources required to find the desired augmentation policy. The method Fast AutoAugment introduced in this paper indeed reduces the required resources, whilst achieving similar *accuracy* to AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet. Weaknesses. This paper has many problems. I identify the following.  The comparisons against AutoAugment are not apple-to-apple. Specifically, the number of total policies for Fast AutoAugment and for AutoAugment are different. From Algorithm 1 of Fast AutoAugment (FAA), FAA ultimately returns N*K sub-policies. From Lines 168-170, N=10 and K=5, and hence FAA returns a policy that has 50 sub-policies. From the open-sourced code of AutoAugment (AA), AA uses only 25 sub-policies. Using more sub-policies, especially when combined with more training epochs, can make a difference in accuracy.  A missing baseline is (uniformly) random search. In the original AutoAugment paper, Cubuk et al (2019) showed that random search is not much worse than AA. I’m not convinced that random search is much worse that FAA. AA’s contributions include the design of the search space of operations, but FAA’s contribution is *a search algorithm*, so FAA should include this baseline.  In fact, an obvious random search baseline is to train one model from scratch, and at each training step, for each image in a minibatch, a sub-policy is uniformly randomly sampled from the search space and applied to that image, independently. I believe FAA will not beat this baseline, if this baseline is trained for multiple epochs. While I am aware that “training for more epochs” is an unfair comparison, in the end, what we care about is the time required for a model to get to an accuracy, making this baseline very relevant.  Last but not least, I want to mention that the FAA method (as well as the AA method, which FAA relies on a lot), is *impractical* to implement. Judging from the released source code of FAA, the augmented images are generated online during the training process. I suspect this is extremely slow, perhaps slow enough to render FAA’s policies not useful for subsequent works. I am willing to change my opinion about this point, should the authors provide training time for the policies found by FAA.