This work studies optimization of $\ell_2$-loss for training multilayer nonlinear recurrent neural network. The paper is well-written, well-presented, and easy to follow. When each layer of the neural network is highly over-parameterized, with several additional assumptions, it proves that the (stochastic) gradient descents converges linearly to zero training loss. The overall theoretical result is impressive, and several new results have been developed recently based on this work. The major concern is the practicality and generality of the result for real applications. Below are some more detailed comments:  1. However, the major concern is the practicality of the assumptions and hence the results. The theory seems to suggest that random initialization is already ``good’’ enough, that one only needs to make small adjustment of the weights to obtain zero training loss. Recent work   [L] Lenaic Chizat, Edouard Oyallon, Francis Bach, ``On Lazy Training in Differentiable Programming’’, 2018. [G] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. arXiv preprint arXiv:1904.00687, 2019.  explains that the regime (This is a regime where small changes of the weights result in large change of function values.) of the model the authors considered in this submission tends to behave like linear models (where they call lazy training), which seems to be not yet sufficient to explain the success of deep neural network in practice (demonstrated by experimental results on deep CNN).  The authors should at least cite these results, and provide a detailed discussion on this.  2. The original idea of this line of work is coming from the neuron tangent kernel. This is from infinite width to finite width (asymptotic to non-asymptotic). The authors need to cite and recognize their work  Arthur Jacot, Franck Gabriel, Clément Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks.  3. It seems quite strange to the reviewer that the matrix A and B can be set to their random initialization without optimization, and the training loss can be optimized to 0. Under the considered setting here, it seems that random initializations are close to the optimal solution. In the result, it seems that gradient descent on the weights W (given the stepsize eta very small in the theory) makes very little adjustment (the properties in Theorem 3 and Theorem 4 only hold when W is very close to the initialization),  but producing zero training loss.   4. In the main theorem, the authors hide the degree of polynomial of the overparameterization in m. The degree of the polynomial seems to be very high. Can the author give us a sense how loose are the bounds, and what is the conjecture dependence for the result to hold here?  5.  References. It would be great if the authors can make the citations in order of appearance and abbreviate them (e.g., page 1, change [58,19] to [1-2], and [7,49,54,31,15,18,36,60,59] should be abbreviated [3-11], etc).  ====== After Rebuttal =======   The rebuttal clarifies and addresses most of my concerns, especially the relation of this work to [L], [G], and [J]. Albeit its practicality (it requires significant overparameterization), I agree with other reviewers that this is a solid theory paper that has triggered a lot interest in recent theoretical understandings of deep neural networks (e.g., SGD on the training loss of multilayer ReLU network). It also provides some explanation how SGD works in the early stages of training neural network.  