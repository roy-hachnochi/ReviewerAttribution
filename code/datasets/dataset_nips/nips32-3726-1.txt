The overall idea is interesting and experiments show that it constantly improves the performance of two major integrated tasks, image captioning and visual question answering.  The presentation of MIA is clearly described, and it is understandable to use the method for pretraining of integrated features.  However, when it is applied to image captioning where the inputs are only images, it is a bit difficult how those features are used.  It is because Figure 3 and its explanation are too brief.  It is preferable the architecture of LSTM-A3 is explained or illustrated in some detail.