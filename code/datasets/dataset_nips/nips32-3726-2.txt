-- What is the neural structure of the component that processes textual concepts? Bag-of-words, bag-of-embeddings or RNN encoder without order?  -- How is the textual concept extractor carried out? Is it basically the extractor proposed by Fang et al. (2015) or Wu et al. (2016)? What are the datasets used for training the textual concepts?  --Line 107. Typo, "... from the self-attention in that in the self-attention, the query matrix" -> "... from the self-attention where the query matrix"  Citations:  Lu, Jiasen, et al. "Hierarchical question-image co-attention for visual question answering." Advances In Neural Information Processing Systems. 2016.  Xiong, Caiming, Victor Zhong, and Richard Socher. "Dynamic coattention networks for question answering." arXiv preprint arXiv:1611.01604 (2016). 