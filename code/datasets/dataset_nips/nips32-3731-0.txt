The main contribution of the paper is to devise a bound that only requires data to be i.i.d. with the condition that the activation functions are sign functions. This contrasts with previous approaches who had non-obvious assumptions on e.g. the margin. I'm not an expert in PAC-Bayes but this definitely looks like a step in the right direction.  The authors do a good job of providing the relevant background about general PAC-Bayes and the application to linear classifiers, which provide the backbone of the paper. The proposed learning method is arguably not that useful for practitioners given the computational complexity and the performance, but it provides a good testbed for the theory. Given this, I would prefer to see more space devoted to discussing the implications of the theory rather than to the experimental results. Table 1 should contain some type of error bars, given the stochastic nature of the optimization. In fact, the authors state in the reproducibility checklist that they provide error bars, so this should be addressed. Nits: * The citation in line 137 feels a bit weird, since SGD has been the standard approach for learning deep networks for much longer.o * Few typos (e.g. line 163 "leaning" -> "learning")