###  I thank the authors for their feedback, in particular for clarification on notation and explaining that large minibatches should be estimated only every O(1/eps^2) steps, which is indeed rare. The other reviewers also convinced me that despite not having the right assumptions for the mention applications, the work might still be useful in other applications. I request the authors to remove the applications mentioned in the introduction or to explicitly write that their assumptions are not satisfied for them. Based on this points, I increase my score from 4 to 6.   Let me also clarify on why I believe having the right assumption is important and what I dislike about the theory. SARAH is an interesting method as it does not require bounded gradients and, at the same time, there are settings where the its known complexity is better than that of SGD. However, here the requirements are quite restrictive and do not address applications mentioned in the introduction. The authors argue in the rebuttal that on the optimization trajectory the assumption should still hold, but I also have seen this argument for the method in [1], which was pointed out in [2] to provably diverge (with exponential rate!) on simple bilinear problems.  Another metaphor is proximal gradient. Why do we need it if subgradient converges under bounded gradient assumption? It turns out that the assumptions actually matter and proximal gradient converges faster. So I believe that having reasonable assumption related to the problem motivation is important.  [1] Solving variational inequalities with stochastic mirror-prox algorithm, Anatoli Juditsky, Arkadii S. Nemirovskii, Claire Tauvel [2] Reducing Noise in GAN Training with Variance Reduced Extragradient, Tatjana Chavdarova, Gauthier Gidel, FranÃ§ois Fleuret, Simon Lacoste-Julien  ###   1. Table 1 is confusing because in Wang 2017a function g is allowed to be nonsmooth and in the current work it is assumed to be smooth (so the comparison is not fair). Would the authors agree to add columns to table 1 to show the difference in the assumptions?  2. It is very confusing that the authors write \partial g_j(x) instead of \nabla g_j(x) since they assume that g_j is smooth (equation 12).  3. Assumption 4 is not satisfied for function f in equation (4). For instance take w_1=w_2=...=w_N=0 and let w_{N+1}->+\infty. Then ||\nabla f||->+\infty. Similarly, it is not satisfied for f in line 335. Moreover, it seems that Assumption 9 is not satisfied for g_j in Risk management problem because g_j(x) - g(x) includes <x, r_j - 1/n\sum_{l=1}^n r_l>, which is not bounded and goes to infinity for x->\infty.  4. I like the guarantees for the finite sum case, but your minibatch size is 1/eps^2 in the online setting, which probably means that it's not going to be practical.  5. I see that you put a lot of efforts in deriving the guarantees, but it's unclear whether your work is going to help other researchers. Due to the high number of assumptions, it's more like a technical exercise of extending SARAH by assuming that every iteration is very close to a full gradient step. SARAH itself, in contrast, is very interesting because it works under very general assumptions. I didn't enjoy reading the proofs and feel that they give no new insight about optimization.  Typos: line 26: it should be "at the same time" In equation (4) square is missing in the second term. line 77: "it adopt" should be "it adopts"