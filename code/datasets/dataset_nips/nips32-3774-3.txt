The authors propose a framework for reconstruction tasks (as PCA). The idea is to add weights to all the reconstruction-error (measured point-wise) so when minimizing we will get small weights when the reconstruction-error is large. Also, keep only the top k points whos reconstruction-error  is smallest (and hence their weights are the largest). This way the algorithm is less sensitive to outliers, as they will be assigned with small weight and later be chopped out.  Then they apply this to the PCA reconstruction function   \begin{array} { l } { \min _ { \mathbf { m } , \mathbf { v } _ { i } , \mathbf { p } , \mathbf { w } } \sum _ { i = 1 } ^ { N } p _ { i } \left\| \mathbf { x } _ { i } - \mathbf { m } - \mathbf { W } \left( \mathbf { v } ^ { i } \right) ^ { T } \right\| _ { 2 } ^ { 2 } + \lambda p _ { i } ^ { 2 } } \\ { \text {s.t. } \mathbf { p } \geq \mathbf { 0 } , \mathbf { p } ^ { T } \mathbf { 1 } = 1 , \mathbf { W } ^ { T } \mathbf { W } = \mathbf { I } } \end{array}  and then construct an algorithm which optimize for p and for W,m alternately.  The algorithm is tested empirically using 4 photo data-sets, where it outperforms all the other robust-PCA methods on all the tests.  Although it is concise it seems that the idea is interesting and well treated and analyzed. The main point which bother me is the empirical results. On line 222 - Maybe the pivot point has to do with the polluted ratio being 20%? If so, doesn't that mean that the choice of the noise and the parameter k gave you algorithm a better result then it might have for different k or noise ratio?  Besides that I have a few questions:  - How is this idea/framework relates to Core-Sets for PCA? - On line 146 - p_i should be r_i? - In the empirical reconstruction error test wonâ€™t the plain PCA be the best? Here the whole idea was to ignore noisy points hence accounting them in the measure is problematic. - On line 216 - What do you mean by k_{rate}? Didn't you say (line 164) that k=0.85N? 