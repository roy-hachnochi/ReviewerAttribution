This paper studies the learning dynamics of two-layer neural networks in the teacher-student scenario under the assumptions that the input is i.i.d. from zero-mean unit-variance Gaussian and its dimension is very large. The dynamics is set to be the online algorithm or the stochastic gradient descent (SGD) with mini-batch of single sample, and the dataset size is also assumed to be sufficiently large so that the parameters have no correlation with forthcoming samples. Thanks to these assumptions, the dynamics is governed only by the covariances of connections of the student and teacher, and the closed-form macroscopic dynamics of those covariances can be derived from the SGD dynamics itself. Using this macroscopic dynamics, the generalization error which is also characterized by the covariances only, can be accurately calculated. In the soft committee machine (SCM) setting (i.e. weight from the hidden layer to the output is constant), when only the first layer of the student is leant, it is shown that the overparameterization (the student has more nodes in the hidden layer than the teacher) generally degrades the generalization ability irrespectively of the choice of activation functions (here sigmoid (erf), linear, and ReLu are treated). Meanwhile when both layers of the student are leant, the generalization ability strongly depends on the choice of the activation function: For the sigmoid activation, the generalization error ``decreases'' as the overparameterization level ``increases'' while for the other activations the generalization error almost stays constant with respect to it. A further interesting observation is that there exist another fixed point of SGD exhibiting better generalization but cannot be found from random initialization for the linear and ReLu activations. These observations reveal the complicated interplay among the activation function, the network structure, and the algorithm, which largely affects the generalization ability. This study is thus important because it reveals the need for a more careful consideration about the generalization and the learning dynamics.  Originality: Analysis of two-layer neural networks is recently drawing much attentions of the researchers in the community, but this type of analysis has never been done and the problem setting is interesting. The novelty is clear.  Quality: The submission is technically sound and the theoretical prediction is well supported by experiments. The quality is high enough.  Clarity: The paper is well organized and equipped with nice appendices well summarizing the detailed computations and experiments, though I think the introduction is slightly redundant. No need of large modifications. I only give the list of typos and uneasy-to-understand expressions which I found: Line 36: they have too -> they have to Line 48: a a surge -> a surge Line 56: kernel regime -> kernel regimes Line 94: (x^{\mu},y^{\mu}_{B}). What the subscript B? Line 609: (S34ff) -> (S34) Equations S26-S30: These expressions are for sigmoid activation, but it is not explicitly mentioned in Appendix B. This point should be addressed.  Significance: I think this work is an important one because it provides an insight about the learning dynamics and the generalization. Explicit demonstration of the superiority of the overparameterization in the generalization error in the sigmoid activation is interesting and suggesting.    Additional comments after feedback: Given the author feedback, I am impressed with the result for larger K and the difference from the mean-field result in the infinite width limit, and accordingly I increased the score. I think the fact that the authors found the different convergent point of SGD from that oft the mean-field theory is very interesting and should be stressed in the Discussion section more in the revised manuscript.      