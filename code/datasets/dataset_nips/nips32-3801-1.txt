Summary:  The paper developed an active learning that selects a batch of images that jointly maximizes the mutual information and hence improves the accuracy of the image classifier. This is an extension of Bayesian Active Learning by Disagreement (BALD) acquisition function that computes a mutual information between a set of points and model parameters. This approach outperforms BALD in three datasets: MNIST, RepeteadMNIST and EMNIST.  Originality:  The paper proposes an interesting idea of selecting samples to annotate jointly by maximizing the mutual information between a batch of samples and model weights instead of selecting them independently for each batch. Basically, the main contribution is the extension of the BALD acquisition function on image data to jointly score samples that would minimize the number of annotations while maximize the performance of the model; thus it extends the mutual information between model prediction and model parameters to between a joint of multiple model predictions. Despite proposing this novel batch selection acquisition function exploiting mutual information between multiple data points the paper should include some related work on batch selection [1,2,3], particularly regarding a discriminative batch model that also exploits the uncertainty of unlabeled data [1], selection of the best samples that match a distribution [2] and exploiting diversity in a mini batch using K-means clustering[3], and include recent literature in active learning [4].  Significance:  This paper seems to be a useful contribution to the literature on Batch Mode Active Learning, showing that using the mutual information between a joint set of multiple data points and model predictions outperforms independent selection of samples using BALD and Random selection on three MNIST-based datasets: Repeated MNIST, MNIST and EMNIST, and VarRatios and Mean STD on Repetead MNIST. Moreover, the paper shows that the proposed approach requires less interactions with the oracle (batch size>1) to achieve the same performance when the oracle is asked at each iteration (batch size =1) (upper bound) although the same amount is annotated; hence it reduces the number of times the model is retrained and the number of requests to annotate the data.   However, the main weakness of the paper in my view are: (a) the proposed approach should be also tested on more difficult datasets like CIFAR-10, CIFAR-100, CINIC-10 or ImageNet that requires the annotation of large batches in order to have significant improvement on the performance of the model due to the difficulty of the task. Hence, it would demonstrate that usability of the approach on real world problems. Moreover, (b) a comparison with the most current related work on active learning like CoreSet, Variational Adversarial Active Learning, DeepFool Active Learning, or Expected Gradient Length would be interesting to show the need of using batch mode selection. Finally, (c) it would be interesting to add an ablation study where the performance of BatchBALD and BALD is compared with different sizes of the subsampled dataset pool because the performance of BALD depends also a lot on this hyperparameter and it would reduce the chances of having repeated images on RepeatedMNIST.   Quality:  The idea of having a mutual information between a joint set of multiple data points and parameter and of how it is optimized by applying two approximations (greedy algorithm that selects one sample each time per batch and Montecarlo sampling to avoid exact computation) are certainly good for joint selection of data points while reduces computational time albeit some careless notation mistakes (symbol in Eq. (2), Lemma 2 and line 370 in supplemental material). It would be interesting to show empirically that these two approximations are negligible to the exact solution besides the theoretical proof in Appendix A.   Clarity:  The paper is clearly written and structured clearly, and the code help me on clarifying an important point in the paper (see below). The section from 3.3 to 3.4.1 could be improved by describing firstly the computation of BatchBALD and then introduce the greedy approximation and the efficient implementation for solving the NP-hard problem. It would be present smoothly the proposed approach and the contributions.   However, the paragraphs from line 127 and 131 that describes the MonteCarlo sampling to avoid exact computation should be described how it is computed clearly because it could confuse since given x_1:n-1 fixed the predictions y_1:n-1 are also fixed. Checking the provided code I figured out how it is calculated the P_1:n-1 for the joint mutual information.   References: [1] Yuhong Guo and Dale Schuurmans. Discriminative Batch Mode Active Learning. NIPS 2008.  [2] Javad Azimi, Alan Fern, Xiaoli Z. Fern, Glencora Borradaile, and Brent Heeringa. 2012. Batch active learning via coordinated matching. ICML12 [3] Fedor Zhdanov. Diverse mini-batch Active Learning. https://arxiv.org/pdf/1901.05954.pdf [4] Samarth Sinha, Sayna Ebrahimi and Trevor Darrell. Variational Adversarial Active Learning. https://arxiv.org/pdf/1904.00370.pdf  ------------------------------------------------------------------------------------------------------------------  First of all, I appreciate the authorâ€™s effort for addressing all our concerns and improvements, specially adding extra experiments during this short time. Therefore, I upgrade my recommendation as a good submission and voting for his acceptance.   Moreover, I would encourage the authors to also include an ablation study of how the subsampling size w.r.t. the unlabeled pool affects BALD and BatchBALD. Based on my experience using BALD, the size of the subsampled samples from the unlabeled pool affects on the final performance of the model; hence I recommend adding this ablation study to improve the quality of the paper. 