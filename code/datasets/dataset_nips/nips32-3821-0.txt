This paper uses three techniques for incorporating multi-lingual (rather than just mono-lingual) information for pretraining contextualised representations: (i) autoregressive language modelling objective (e.g. left-to-right or right-to-left language model), (ii) masked language modelling (similar to the BERT loss, but trained on multiple languages based on a shared BPE vocabulary), and (iii) translation language modelling (extending the BERT loss to the case where parallel data are available in more than 1 language). The methods are evaluated on four tasks: (i) cross-lingual classification (XNLI), (ii) unsupervised machine translation, (iii) supervised machine translation, and (iv) low-resourcce language modelling.   Strengths: - Very substantial improvements on XNLI, unsupervised machine translation, and supervised machine translation. These results are important as they showcase the strong benefit of multi-lingual (rather than just mono-lingual) pretraining for multiple important downstream tasks, and achieve new state of the art. - The methodology and the evaluation tasks are explained in sufficient technical details, which would help facilitate reproducibility. - Fairly comprehensive review of related work in Section 2.  Weaknesses: see section 5 ("improvements") below.  Originality: while the methods are not particularly novel (autoregressive and masked language modelling pretraining have both been used before for ELMo and BERT; this work extends these objectives to the multi-lingual case), the performance gains on all four tasks are still very impressive.  -  Quality: This paper's contributions are mostly empirical. The empirical results are strong, and the methodology is sound and explained in sufficient technical details.  - Clarity: The paper is well-written, makes the connections with the relevant earlier work, and includes important details that can facilitate reproducibility (e.g. the learning rate, number of layers, etc.).  - Significance: The empirical results constitute a new state of the art and are important to drive progress in the field.  ---------- Update after authors' response: the response clearly addressed most of my concerns. I look forward to the addition of supervised MT experiments on other languages (beyond the relatively small Romanian-English dataset) on subsequent versions of the paper. I maintain my initial assessment that this is a strong submission with impressive empirical results, which would be useful for the community. I maintain my final recommendation of "8".