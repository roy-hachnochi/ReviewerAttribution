The authors provide a formulation of equilibrium propagation that is applicable to discrete time models that usually use backpropagation-through-time. The main theoretical result of the paper is that the updates from EP are equal on a step-by-step level with the updates of BPTT when the transition function of the RNN derives from the ``primitive function'' (which is similar to the energy function used in Scellier and Bengio 2019), and the RNN converges to a steady state over time. The authors further demonstrate this in practice, also for standard RNNs where this condition is not met.   Originality: The  results of this work seem to be  original.  Quality: The quality of the paper is high. The main theoretical results are explained in a clear and intuitive way, and the experiments are well-motivated.  Clarity: The clarity of the paper is high. The setup, assumptions and theoretical results are clearly explained. The experiments are also well explained.  The figures could benefit from having titles/legends or text in the caption clarifying each setup -- example in fig. 3, unless I read the main text, it's not clear what each column is.   Another minor fix for clarity would be to explicitly specify what the weight updates are for each of the experiment, and when the weights are updated.  The theory section could benefit from a brief verbal outline of the proof.  Significance:  The results of the paper are significant, since it elucidates how and when equilibrium prop algorithm would work for the discrete time case and for standard RNN models. The fact that the performance comes so close to BPTT on MNIST contributes to the significance. Some factors that could affect general applicability of this result is the use of symmetric weights, and the requirement of the network to converge quickly to a steady state.  Further comments: - It would be interesting to see the behaviour of the algorithm when the weights are not symmetric. Is it expected to work? What would be the trajectory of the weights? - Quantitative data of how the performance degrades with more layers could help define the limits of the algorithm in terms of what the deepest networks this algorithm could train.