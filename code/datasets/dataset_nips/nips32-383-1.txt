The paper proposes a new method "Neural Architecture Transformer" (NAT) for adapting existing neural architecture such that they perform favourably in terms of the resource_consumption-performance trade-off (by replacing existing operations with skip-connections or removing them or by adding skip connections).  * Originality: The paper proposes a new search space and a novel search method. Both contributions are original. However, the paper misses closely related work such as, e.g. Cao et al., "Learnable Embedding Space for Efficient Neural Architecture Compression"  * Quality: The empirical results show that the proposed combination of search-space and search method allows to improve existing architectures such as VGG16, ResNet20,ENASNet or DartsNet in terms of the resource_efficiency-performance trade-off. However, it does not disentangle the effects of search space and search method; in particular, it remains unclear if the proposed and relatively complex search method  (policy-gradient + graph-convolutional neural networks) would outperform simpler baselines such as random search on the same search space. Moreover, the method is only applied to neural architectures that were not optimized for being resource-efficient; it remains unclear if NAT would also improve architectures such as, e.g., the MobileNet family or MnasNet.  * Clarity: The paper is well written and structured. It also contains sufficient details for being able to replicate the paper.  * Significance: The paper's three main contributions are potentially significant, in particular for practitioners. The proposed method could be seen as a post-processing step for NAS, which allows increasing the methods performance with the same or even less resources required (since the search space is very simple, a search method is more likely to find optimal configuration in it compared to the significantly larger typical NAS search spaces). However, to really show the significance of the proposed method, it would have to show that it outperforms simpler baselines such as random search and is also applicable to cells that were already optimized for being resource-efficient.  In summary, the paper proposes a promising method for "post-processing" neural architectures. However, because of the shortcomings listed above, I think it is too premature for acceptance in the current form.