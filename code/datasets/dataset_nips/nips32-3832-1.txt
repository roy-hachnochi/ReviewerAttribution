Although the paper is overly technical, it addresses an important problem in deep learning with graph data. In terms of results, the novelty of the paper is in proving the universality for the equivariant case in a limited setting; this limitation to the setting where the output is a function over “nodes of the (hyper) graph” should be clarified in the abstract. For the invariant case, the advantage of the proposed technique over the proof of Maron et al’19 is that it shows that uniform approximation is possible to a function over graphs of varying size. The disadvantage is that it provides no bound on the order of equivariant tensors produced in the hidden layers.   I have not checked the proofs in the appendix and cannot comment on the significance of the techniques. However, I wonder if the authors can comment on the viability of similar techniques to show the universality of equivariant networks for a broader class of discrete group actions (e.g., when the feed-forward layer is uniquely equivariant to a group action)? 