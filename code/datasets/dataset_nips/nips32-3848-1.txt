The submission improves the theoretical supports for the IRL problem in Ng & Russel (2000), by providing a sample complexity and a sufficient condition for the correctness of the IRL problem. The results are mainly based on a geometric analysis on the IRL problem which contributes in two aspects: 1. provides the sufficient condition; 2. transforms the problem to a L_1-regularized SVM problem, whose theoretical characteristics have already been thoroughly studied.   Therefore, to better evaluate the contributions of the submission, I have following two questions:   1. How strong is the definition 4.1 for the IRL problem. (While I admit definition 4.1 is weak enough for L_1 regularized methods, why this is true for IRL problems?) In other words, by only considering the Regime 3 case, how much do we lose?  2. It seems that the formulation here is now a kind of constrained formulation, especially compared with maximum entropy IRL methods [1], which are more often practically used. Therefore, can the analysis in the submission here contribute or generalized to more more practical methods?    [1] Finn, Chelsea, Sergey Levine, and Pieter Abbeel. "Guided cost learning: Deep inverse optimal control via policy optimization." International Conference on Machine Learning. 2016.      =================================================== The rebuttal of the authors is clear and convincing. I would like to increase my score to 6. 