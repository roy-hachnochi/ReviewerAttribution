The article casts the control problem as a probabilistic inference one in a novel formulation based on a Boltzmann policy that uses the residual error as temperature. Since this policy has an intractable normalization constant variational inference is used through introducing another variational policy. The authors derive actor-critic algorithms through an expectation-maximization strategy applied to the variational objective.  The authors offer extensive proofs for several useful properties of their objective: convergence under some assumptions, recovery of deterministic policies, etc/ The current work is also very well integrated with existing literature being motivated by the limitations of existing variational frameworks (the limited expressivity of the variational policy over trajectories and the difficulty to recover deterministic policies in maximum entropy approaches; and the risk-seeking policies pseudo-likelihood methods arrive at). The proposed method addresses all this limitations.  Experiments with derived algorithms validate the approach by achieving state-of-the-art on a couple of continuous RL tasks. Baselines are relevant: a state-of-the-art algorithm (SAC), and another algorithm that naturally discovers deterministic policies (DDPG), the latter being closely related to one of the main claims in the article: that in the limit of the maximization of the objective the learned policy is deterministic.  Considering the originality of the proposed objective, the strong theoretical treatment, the empirical validation, and also the nice exposition that places the article among related works, I propose for this paper to be accepted.  Quality I consider the current article to be a high-quality presentation of a solid research effort. It does a good job in covering both theoretical and practical aspects (e.g. convergence proofs make some strong assumptions (2,4) that might be hard to meet in real setups, but discuss relaxations in the supplementary material).   Originality The article builds on prior work as it starts with addressing some problems of the existing variational frameworks for RL, but it proposes an original Boltzmann policy that uses the residual error as an adaptive temperature. This strategy permits the derivation of a strategy for exploration until convergence based on the uncertainty in the optimality of the state-action value function. To the best of my knowledge, this is an original approach.  Clarity The article is an excellent example of scientific writing. It does a good job in balancing the formal aspects (supported by detailed proofs in the supplementary material) with the intuition behind the different choices, and the connections with previous work (pseudo-likelihood and maximum entropy approaches). I think that in the current state one needs to have the supplementary material close in order to understand the proposed algorithms. I suggest moving into the article details such as the practical simplifications in appendix F3 (not with full detail, only enumerated in section 5).  Section 5 mentions a claim from section 3.3 regarding soft value functions harming performance, but there is no such claim there. It is mentioned in section 2.2 though.  Significance I consider the work to be important in the landscape of variational approaches to reinforcement learning as it solves known limitations of previous approaches and itâ€™s both theoretically and empirically validated. Also, the empirical results show that algorithms might outperform existing algorithms on high dimensional inputs. 