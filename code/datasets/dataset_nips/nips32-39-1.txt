The problem this paper studies, determining whether or not to deploy AI tools, is a vital and under-studied problem, and it's a pleasure to see here.  I agree that understanding current attitudes is an important part of the equation, although I'm not particularly familiar with the task delegability literature in general.  It's hard to tell how these survey responses are influenced by participants' assumptions about both the type/quality of AI assumed and who built the AI.  I worry what this survey is actually measuring is people's assumptions about who is doing the building, and not what they are building.  If they are assuming this is Google building the AI and imbuing it with its values rather than, say, the doctors doing the diagnosis, I imagine this should affect their ratings of both risk and trust.  It's harder to hold Google accountable than your doctor, for example.  This is particularly pronounced in value alignment, where it depends on the participants' assumptions about how or even if the AI was imbued with values.  Do the respondents assume that, contrary to reality, AI is value neutral?  In addition, while this work concludes that trust is the most important factor, I am concerned that this was in part driven by phrasing like "I trust the AI agent's ability to reliably complete the task," which could easily be mistaken for just whether the task should be delegated.  'Ability' could easily be interpreted to include the difficulty of the task.  This would explain the high correlation between the decisions and the trust component.  Finally, one component that appears to be missing is how much a time or effort the *participant* spends on the task.  If they're not annoyed by having to do the task themselves, then there's less incentive for them to want to automate it.  Other concerns: -192:  Why not just ask the actual experts, instead of asking people for their guesses of what experts are capable of? -Table 2:  How are the p-values being computed?  This is an example of multiple-hypothesis testing, and claims of significance should of course reflect that. -265: typo  Edit: I acknowledge the response to my review and while I still believe that this work would be made stronger if it had attempted to understand the kinds of assumptions that people make about AI, e.g. how they're built, it's fine if that's future work.