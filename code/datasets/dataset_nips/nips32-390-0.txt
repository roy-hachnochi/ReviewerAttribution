Originality: Using a differentiable architecture search for pruning a network is new to me and it makes sense. Also using interpolation for fusing different channel sizes makes sense and it can possibly be used for other applications too. Quality: The contribution of the paper seems clear and the proposed methodology makes sense. Experimental results show that the proposed pruning can lead to a good trade-off between computational cost and accuracy. Clarity: The presentation of the paper should be improved. For instance channel wise interpolation is abbreviated sometimes with (CWI) and other times with (CHI), making the reading very confusing. Also, the actual way that CHI is performed is not very clear. It can be intuited by Fig. 2 but a proper formulation is missing.  Significance: In my opinion the contribution of this paper is important because it show that is possible to cast network pruning in terms of differentiable architecture search and results place the method among the most promising pruning approaches.  Additional comments: - From tab.1, the use of knowledge distillation (KD) seems to be important for good results. However, KD can be used to improve any pruning approach. Thus in this sense it is not clear if the good performance of the proposed method are due to the network architecture optimization or the knowledge distillation. If it's the second case, then the contributions of the paper will be reduced. - Fig.2 helped me to fully understand the contribution. However, there should be a clearer formulation of the approach too. - In l.50 the authors talk about optimizing the number of channels but do not talk about the number of layers. This is a bit confusing because it is not clear what is the final aim of the paper. This kind of problems can be found in several points in the paper. It seems like multiple people with different understanding wrote different parts of the paper. - Authors should compare with other approaches also in terms of training time. This method seems computationally quite intense during training.  - From my understanding, in this work channels are grouped together based just on their order. This means that there can be other combination of channels that can outperform those predefined. One could argue that this constraint is maintained during training and it can induce to group channels in a meaning way. However, this is not true in case of starting the optimisation from a pre-trained network as it seem to be the case.   Final decision: I read other reviews and rebuttal.  Some of the answers to my questions are not fully satisfactory: - Q2.1: I saw the comparison with and without KD in figure 1, however, I wanted to see the influence of this factor in the final results. That is, the proposed method without KD would still be superior to others? The authors show results for a specif configuration in tab.2 and it seems that for the other methods using KD produces worse results. Is there any reason for that or it is a typo?  - Q2.2: In this question I wanted to see more convincing results than a single experiment. However authors did not include any new experiment.  Globally I still consider the paper in a positive way, however, I would like to see those two points clarified in a final version.