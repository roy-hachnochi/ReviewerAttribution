EDIT: I have read the other reviews and the rebuttal. Thanks to the authors for clarifying my questions. I am happy to raise my score, and urge the authors to make sure the clarity concerns are addressed in the draft.  Clarity: I struggled with parts of the draft regarding clarity. Some specific points that I may have misunderstood:  - I could not find a clear definition of CHI. Please define this clearly in the draft. If it has many possible interpretations, then specify the goal. Is CHI == CWI? If so, please explain this in a bit more detail as well. An example may be helpful.  -  How was the cost of the network estimated? In particular was is the difference between E_cost(A) and F(A) and how is this made into a differentiable loss? Please specify.  - The role of distillation could have been made clearer. In particular, the authors could specify whether the new network inherits (1) weights, (2) architectures from the networks trained earlier in the TAS procedure.  - The train / test / validation splits need to be clarified in the experimental section. This is especially important, since TAS trains on validation data.  Quality: There are serious concerns regarding the quality of the experiments.  - TAS uses validation data to optimize the architecture of the networks. Even though the validation data is not used directly to optimize the weights, it can still have a very significant influence on the learned weights via the bilevel optimization scheme. Therefore, it is correct to see the validation data as part of the training set for the purposes of evaluation of overfitting. I could not tell from the current draft whether the reported results are evaluated on the test or training sets. Moreoever, given that the validation set was not specified (from what I could tell!), it is hard to be sure whether the reported numbers reflect overfitting or not.  - In the experiment on the effect of strategies to differentiate alpha, in was unclear why the choice was made to not constrain the computation cost. This seems like the least informative choice, and the fact that the chosen method is the only one that succeeds in this context doesn't make it obvious why it should succeed when you aim to constrain the computational cost. The authors could make this more convincing by including experiments with constrained costs.  Significance: Given the concerns with the evaluation, it is hard to assess the significance of the work. Even so, the improvements were modest over competitor methods and raise concerns about the impact of the methods moving forward.   Originality: The method is original, although not completely distinct from previous works, see Louizos et al. 2018. This would not be a major concern, if the experimental results were more interpretable and robust.