The paper proposed a novel zeroth-order optimization algorithm inspired by the success of adaptive gradient methods (such as ADAM, NADAM, AMSGRAD, etc) in first-order optimzation. It is the first work leveraging this into block-box optimization which is a very important topic in machine learning and I believe this work has potential for a high impact. The theoretical analysis shows a convergence rate which is poly(d) worse than first-order AdaMM methods, which is acceptable for zeroth-order algorithms. The paper is well and clearly written.