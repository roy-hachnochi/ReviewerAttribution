This paper looks at the problem of archetypal analysis -- which effectively is a low-rank  representation of the data that perhaps has more interpretablility. Instead of finding a low-rank  subspace to represent the data, we try to represent each data point as a projection to  a convex hull of k-points, where the k points themselves are convex combinations of the original data.    The authors present a sampling based method to create coresets for this problem. The main intuition is that the objective function is close to (in fact upper bounded by) a k-means objective, just the "query set" has changed. Given the strong coreset guarantee, a restriction of the query set means that the existing guarantees carry over. They use a modification of an existing efficient sampling technique and obtain an additive guarantee.  Overall, the connection is nice, though perhaps not very surprising to someone familiar with the kmeans coreset guarantees. The experiments do definite improvement over uniform sampling and some improvement over the existing "lightweight coreset". I wonder why the original kmeans coreset was not used in experiments-- the datasets are small enough that k passes is not too expensive.      Few editorial comments:   The dimension of B is stated wrongly, it should be k X n. Correspondingly that of A.   I did not understand the comment that "removing \eps/2 \phi_X(Q) causes problem in clustering"-- what the authors probably mean is that we cannot really select a subset of points that satisfies the condition without this term on the RHS, for all centers Q.  Also, the statement "In contrast to k-means, we assume that the mean .." is not clear to me. Either this is implied by the nature of the queries in this optimization problem (which is what I suspect),  or it is an important assumption that must be stated clearly.   Just noting that the RHS of the bound 7 implies that we get an additive, and not multiplicative approximation-- most coresets go for a multiplicative guarantee.   I wonder if Proposition 1 is really accurate as stated? If we take the original multiplicative error definition of coreset, then I do not see an obvious argument, in spite of the lower bound in Lemma 1. It seems true if we defining coresets in terms of absolute error, which is what is used here (Definition 3). It will be nice to clarify/present a proof in supplementary.   Overall, making the connection to the k-means objective is not very surprising. The fact that the lightweight k-means (and a slight modification) gives as additive error for coresets is neat.  