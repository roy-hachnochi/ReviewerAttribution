Originality:  Training of optimal decision trees is clearly a problem that has seen a lot of prior work. A distinguishing feature of this submission is that it focuses on optimal *sparse* decision trees for binary variables, and that the approach seems to be feasible in practice, which is achieved by a combination of analytical bounds that reduce the search space as well as efficient implementation techniques. The work builds upon the CORLES algorithm and its approach to creating optimal decision lists. However, the authors extend this approach to decision trees in a non-trivial manner that adds substantial novelty.  Quality: The claims of the paper are very well supported by theoretical analysis as well as experiments. Besides a quantitative comparison, the experiments also give some insight into what optimal decision trees look like, and where non-optimal approaches fail.  Clarity: The manuscript is very well-written and organized. The main manuscript is very condensed, which can make it hard to read at times; but the supplementary includes all relevant material at great detail.  Significance: The paper mainly focuses on improved training accuracy. To further underline the practical relevance of this algorithm, the authors would need to demonstrate convincingly that improved training accuracy also translates into improved test set generalization. This can be harder to demonstrate than training accuracy, because it depends heavily on the data, the number of leaves, etc. The paper has no claims in this direction, instead focusing on the training problem. The supplementary material does provides numbers for accuracy on the test set for a number of problems, but the results are less conclusive than for training. 