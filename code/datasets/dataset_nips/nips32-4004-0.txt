I think building the connection between the norm of weight matrices and gradient is original, but using normalization to improve the accuracy loss caused by quantization is not so impressive. My detailed concerns are listed as follows:  1. In Eq. (4), the condition of gradient exploding is lambda_1>1. Although Table 1 shows the matrix norm, it is not a direct way. Can you clearly visualize the lambda_1 values in several typical LSTMs, before and after quantization without normalization?  2. In the three normalizations, the trainable scaling factors (g) will still affect the value of lambda_1 (thus affects the exploding condition), but the authors mentioned little on this. So can you present the g and lambda_1 values in typical LSTMs before and after quantization with normalization?  3. In sequential MNIST task, the batch normalization (shared) method totally failed. Could you explain why the behavior on this task is such different from others?  4. I noted that the quantization with scaling factors (e.g. BWN, TWN, alternating LSTM) can also improve the accuracy. But I did not see these results in Table 4. Could you please provide them for better comparison?  5. Do your conclusions still hold in GRU? You can also conduct the similar chain rule analysis during back propagation.  6. How about in multilayer LSTMs?