While in recent years a number of extreme low-precision quantization techniques were developed for DNNs, they were not directly applicable to recurrent architectures. In recent work [1] an extreme low-precision quantization method was proposed that utilizes batch normalization and compresses recurrent neural networks without a large drop in accuracy achieving state-of-the-art performance. In this paper, the authors proposed a theoretical explanation of the difficulties of training LSTMs with low-precision weights and practically explored a combination of different normalization techniques with different quantization schemes. The authors experimentally showed that simple introduction of weight or layer normalization allows applying many standard quantization techniques without modifications. Comments, suggestions, and questions: Figure 1 is quite difficult to read. I would suggest using a logarithmic scale. I would also suggest adding a reference to footnote 3 in the caption of the figure in order to increase clarity  Line 183 “On text8, we use a”  -> “On Text8, we use a” Do I understand correctly that simple application of batch normalization works better or similar to SBN? In [1] the authors claim that the size of their network is 5 KBytes, while in the Table4, the SBN size is 5526KBytes? Can the authors please clarify?  Overall, the propositions on upper bounds are quite straight-forward. The experimental results are simple combinations of previous works. Nonetheless, it is quite interesting that the simple usage of different normalization techniques makes different quantization schemes applicable again. [1] A. Ardakani, Z. Ji, S. C. Smithson, B. H. Meyer, and W. J. Gross. Learning recurrent binary/ternary weights. In International Conference on Learning Representations, 2019.  I would like to thanks the authors for their comments. I updated my score since the answers to my questions are mostly positive.