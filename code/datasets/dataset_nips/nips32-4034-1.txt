This paper considers the problem of linear regression when the majority of the data may be adversarial "outliers", i.e., when there is a small set of interest for which we would like estimates of the regression parameters. The paper makes the key assumptions that (1) the inlier distribution is sufficiently anti-concentrated (as many natural distributions are) and (2) the set has at  least constant (alpha) measure. Then they propose a polynomial time algorithm (exponential in alpha) that is guaranteed to produce a list of ~ 1/alpha candidate parameters that includes an accurate estimate of the regression  parameters for the unknown alpha-dense subset.  I note that while the work of Charikar-Steinhardt-Valiant proposed a general convex optimization algorithm for learning in this setting, the guarantee they obtain for their algorithm is sufficiently weak that a trivial 0 solution would essentially always meet their guarantee, and indeed the algorithm does not  estimate regression parameters in practice. On the other hand, work by  Kothari-Steinhardt-Steurer from last year on some simpler problems in the  list-decodable learning model considered using an assumption that Poincare  inequalities were known to hold in the sum-of-squares framework; the assertion  of an anticoncentration inequality is along these lines, but I do not think that this specific assumption was considered previously, and indeed even the  formulation as a sum-of-squares statement could be considered a contribution.  But, there is at least one work that solves list-decodable regression in the alpha << 1/2 regime, albeit with a very different dependence on the parameters: in AISTATS this year, Hainline et al. obtained a guarantee with a polynomial dependence on 1/alpha, but an exponential dependence on the number of nonzero coefficients in the regression parameters; their list size is also polynomial, and not the (near-)optimal ~1/alpha list size obtained here. Indeed, note that Remark 1.7 (on the exponential dependence on 1/alpha) only applies when the list size is small, and the lower bound in Theorem 6.1 showing the necessity of anti-concentration only applies when the list size is at most the dimension, but I do not believe that using a polynomial-size list trivializes the problem.  The techniques of Hainline et al. are very different, using a RANSAC-like  algorithm, that indeed also does not rely on an anti-concentration property for  the inlier distribution. I view these works as complementary, especially as  Hainline et al. scaled exponentially with the number of coefficients, so while the present work is still a significant contribution, this prior work does  reduce the novelty of the guarantee slightly.  In any case, the guarantees are interesting and the paper is overall very clear. The interesting part is that anti-concentration guarantees identifiability for linear regression, which is very clearly explained. A small note to the authors: v is not specified in Definition 1.2 (although 2.1 corrects this omission), so  it doesn't make sense as stated.  ========  Regarding the comparison to Hainline et al.: you are correct that the problem that is the focus of that work is not the same. But, nevertheless, a connection to the list-decodable learning model is discussed in that paper. Their analysis shows that for an arbitrary set of inliers, a good approximation (under the desired l_p loss) to the optimal regression coefficients on that set is found on some iteration of the main loop. So, if one simply outputs the list of solutions obtained across iterations of the loop, this algorithm solves the list-decodable sparse linear regression problem.   You are correct that the loop is a search over tuples of points, thus suffering an exponential dependence on the number of nonzero regression coefficients. I certainly agree that it is interesting that the algorithm proposed in your work has a polynomial dependence on the dimension, and that the techniques are different and interesting. But, again, note that the "brute-force" algorithm avoids the exponential dependence on 1/alpha your algorithm suffers. This is perhaps most interesting in contrast to the lower bounds you discuss; note that such an enumeration of tuples of points cannot be captured in the standard SQ model, either. It thus illustrates the weakness/limitations of these lower bounds.