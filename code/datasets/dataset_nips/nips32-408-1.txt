Low-rank metric learning optimizes a metric matrix subject to low-rank constraints, preserving the intrinsic low-rank structure of the data. However, it still encounters scalability problem when handling large data. This work gives a new formulation that learns the low-rank cosine similarity metric by embedding the triplet constraints into a matrix to further reduce the complexity and the size of involved matrices. The idea of embedding the evaluation of loss functions into matrices is interesting. For Stiefel manifolds, rather than following the projection and retraction convention, it adopts the optimization algorithm proposed by Wen et al. (Ref. [3]). Generally, this paper is well-written with promising results.  Here are some concerns: 1) The upper bound is set to 3000, which means the dimension $r$ is truncated regardless of the intrinsic value of very large and high-dimensional data. Is there any theoretical analysis or just an empirical value?  2) To make the gradient of $f(P)$ continuous, the smoothing function $\mu(x)$ is adopted for $max(0, x)$, what about the approximation loss between them? If not using such smoothing, how to optimize the problem in Eq. (6)?  3) The SVD pre-processing still needs expensive cost which cannot be avoided. It proposed mini-batch version which requires $O(D{n_I}^2 + Ddn_I d)$, i. e., calculating a descent direction from each mini-batch of data and updating the transformation matrix $L$ at a decreasing ratio. It would be nice to provide some theoretical analysis of this strategy.   4) Some parameter sensitivity examinations are expected, e.g., the rank constraint for $M$ is set to $d$=100, the number of triplet constraints, and the size of mini-batch.  Minor issue: Line 93, the parameter "m" which is about the margin is undefined and some analysis is required here.  ================ In the rebuttal, authors have well addressed most of the raised points.  I vote for acceptance.