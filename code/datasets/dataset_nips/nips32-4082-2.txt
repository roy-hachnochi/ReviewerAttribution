General comment: This paper provides a nice principled way to compute a theoretical lower bound for robust accuracy based on a distance metric between two classes in binary classification. This makes rigorous the intuition that the more separated (in the sense of the perturbation neighborhood the attacker can search in), the lower the optimal robust loss should be - bounded below by the optimal standard loss. Although I believe this type of result could have also been shown outside the optimal transport framework with perhaps less notation, it is one justified way to look at the problem.  The paper is written in a coherent manner, the theory is clear as are the experiments. I am not very aware of similar work - if indeed it is the first to prove lower bounds for adversarial robustness using a distributional distance metric I would highly recommended this paper for publication at Neurips.   Here are some questions that remain, from high to low level: - How could this framework be extended to multiclass classification? And how would the method scale with the number of classes? - For the CIFAR plots you should add robust classifier losses as well even though I understand they might not look that great, that is what we have at the moment! - For Figure 3, aparently there are training issues above certain beta. What is the adversarial training accuracy in those cases? I suspect that it is below 100%. In this case, choosing a sufficiently expressive model with the right parameters should mitigate this issue? - In proof of theorem 3: It should read Pr(V \in ...) on the RHS.