The paper addresses the problem of replacing convolutions with self-attention layers in vision models. This is done by devising a new stand-alone self-attention layer, which borrows ideas from both convolution and self-attention. Like convolutions, it works on a neighborhood of the image, but replaces dot operations with self-attention operations. Unlike convolutions, this layers features a significant reduction in the number of parameters and computational complexity, plus the parameter count is independent on the size of the spatial extent. As in sequence modelling, they employ relative position embeddings, on both rows and columns of the neighborhood.  Experiments are carried out on (almost) state of the art classification and detection architectures, where the authors replace convolutional blocks with their self-attention layer, and use average pooling with stride to do the spatial down sampling. As they have experimental findings that self-attention is not effective in replacing the convolution stem, they also build a replacement for stem layers which is devised by injecting distance-based information in the computation of the values of the attention. The rest of the experimental evaluation ablates the architectures by considering the role of the conv stem, network width, modifying the set of layers which are replaced with self-attention, and varying the spatial extent of the self-attention. Overall, the paper introduces a novel and significant idea. Investigating the role of self-attention as a stand-alone layer is a fundamental research question given the recent advances in both vision and sequence processing. The paper is also well written and clear. On the other side, a deeper investigation of what self-attention can learn with respect to convolution would have been useful, but is left for future works. The provided experiments underline that the new layer can significantly reduce the number of parameters and computational complexity, therefore the result is definitely interesting by itself and opens possibilities for future research. 