As opposed to update the real-valued network as usual and quantize the learned one for feedforward, the inertia concept is useful as it can explain the behavior of the gradient descent-based updates. Eventually, it's natural to understand that a large magnitude in the corresponding real-valued network somehow represents the "confidence" about the sign of that parameter, which will be used as a binary variable in the BNN. I agree with this view.  What I'm wondering though is the fact that it is already known that the gradient for a particular weight is not defined by its current value, but by the input values and the backpropagation error associated with it (the multiplication of the two). In that regard, it is obvious that some kind of momentum terms will help improve the speed of convergence, as already known by various momentum-based optimization techniques. The proposed algorithm, Bop, to me sounds like one of those variants, while the authors claim that it is the first optimizer specifically for BNN.   More specifically, the update rule in eq (5) sounds familiar to me, as it is equivalent to the accumulated gradients as in the original definition of momentum. Slight difference would be that in the proposed algorithm the accumulated gradients replaces the magnitudes while in the original definition of the momentum method it replaces the gradient update. I wonder what's the main difference between the proposed method and a regular momentum-based approach then. Meanwhile, the experimental results show marginal improvement.