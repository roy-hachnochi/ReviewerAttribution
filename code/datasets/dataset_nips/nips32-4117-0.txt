The paper studies a form of constrained reinforcement learning in which the constraints are bounds on the value functions for auxiliary rewards. This allows a more expressive formulation than the common approach of defining the reward as a linear combination of multiple objectives. The authors show that under certain conditions, the constraint optimization problem has zero duality gap, implying that a solution can be found by solving the dual optimization problem, which is convex. The authors also extend this analysis to the case for which the policy is parameterized.  Theorem 1 assumes that Slater's condition holds, which is problematic for two reasons. Slater's condition is usually defined for convex constraints, but the authors specifically state that the constraints in PI are non-convex. Moreover, according to the common definition, the duality gap is zero when Slater's condition holds, which is precisely what the theorem claims to prove. I think there are important details missing from the assumption that the authors need to spell out explicitly.  In fact, I think a more interesting contribution would be to show precisely for which reward functions that Slater's condition holds. As it stands I have poor intuition regarding the types of reward functions that will satisfy Slater's condition.  The decision problem PI considers a special case in which constraints are all formulated as a bound on a given value function. What would happen if you allow other types of constraints as well? To what degree does the zero duality gap depend on this specific form of constraint?  Top of page 4: "Let [...] P(xi) be the optimal value of ~PI for the perturbation xi" -> should this not be PI rather than ~PI? Same question at the top of page 5: "linear program equivalent to ~PI".  I cannot find any mistakes in the derivations of Sections 4 and 5. However, I think it is an exaggeration to say that the parameterization comes at almost no price. First of all, the discount factor is usually close to 1, so the quotient epsilon / ( 1 - gamma ) could grow quite large. Second, although the result holds in theory, it is well known that the bound from the universal approximation theorem is hard to achieve in practice, even if the representation is expressive enough.  How is the projection onto R_+^m done? Just taking the maximum of 0 and x?  Assumption 1 does not appear realistic to me, for the same reason as outlined above: it is difficult to achieve hard performance bounds using e.g. deep learning.   POST-REBUTTAL:  I appreciate the authors' effort to clarify the underlying assumptions. The assumption regarding the existence of a feasible policy is still strong, since presumably one is allowed to freely define the reward specifications s_i. However, I agree that it is non-trivial to determine that a zero duality gap holds when the assumption holds. Moreover, the bound on the total variational norm is indeed milder than that for the uniform norm, but the bound is still worst-case over the states. The title of Section 4 gives an impression that in my view is too good to be true. Even the unconstrained RL problem has few theoretical guarantees when combined with function approximation.  As a side note, I think your definition of the value function in the derivation of the proof of Theorem 1 (in terms of occupation measures) is similar to that of Lewis and Puterman in "Bias Optimality". Also I just noticed that you use "s_i" both for states and reward specifications which is confusing.