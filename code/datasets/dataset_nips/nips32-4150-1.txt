Main reason for "accept" decision - addressing the problem on realistic, large scale indoor scene dataset; nice theoretical contribution on the losses, and explaining the decisions made.  Good points: - tackles novel, challenging, large scale problem of synthesizing views for indoor scenes; - works on a large dataset of realistic indoor scenes. - introduces reprojection consistency loss and style consistency loss, which is a nice theoretical contribution - this work is relevant in the context of indoor localization and navigation applications, where inpainting is necessary, e.g. for completing meshes, reconstructing views for which information is not available, such as holes due to occluders.  Not so good points: - Evaluates only on one dataset, while other (larger) scale indoor scene dataset exists (Matterport3D, Gibson). If not suitable, explain why.  Abstract and figure 1 -- omission of the mention that the input should be RGBD. It is not clear from the evaluation how the method performs on synthetic, smaller scenes.  Abstract: mention that as few as 4 RGB”D” views are needed. The mention of reference views taken with a hand-held camera is a bit misleading, making the readers expect just RGB.  Figure 1: mention that the input should contain depth as well. The reference input views are captured at the same time as the desired output views. Might be a good idea to emphasize that as few as 4 reference views   In related work (L72), GQN has not been tested in real world setup -- it would have been valuable to add this experiment for comparison. Since the method is capable of browsing simple synthetic experiments, it would be worth to check how it performs on realistic   Figure 1 and 2 are not referenced in the text.  It is unclear later on whether the 3D Conv Net[34] (L72) mentioned in table 1 and later L234-240 is a contribution of the current work or it was proposed in [34].  3. L90-91: what is the range for d_u (depth) ? what are the units?  L102 -- the differentiable point tracer is suitable to be a part of the main paper. Is it mandatory or important for the point tracer to be differentiable? 3.2 L154-158: how are the layers pre-selected for adding residuals? 4. Experiments  L206: -- Matterport3D (https://arxiv.org/pdf/1709.06158.pdf) is larger indoor scene dataset; they provide, among others, RGBD + camera annotations. How would the proposed method perform on MP3D? (Optional) evaluate on Gibson dataset (http://gibsonenv.stanford.edu/database/). L209, 210 → are the 8 views used for testing, i.e., reference views  L213: how are the views clustered? Is this a manual step? if not, what features were used? How would the current method perform on ShapeNet? How difficult would it be to compare with methods that evaluate on ShapeNet, e.g. Dosovitskyi et al → how would it perform on SceneNet? There is no clear comparison showing why the proposed method   For ablation: what is the contribution of individual loss components? e.g. style consistency vs reprojection consistency? Bring the dataset statistics closer to the beginning of the section defining the evaluation protocol (# of samples, #train, #test); It is not clear how the train / test were split, and whether there is a validation set.  In Figure 4 - please add the color and depth measures for the selected pictures (e.g. similar to Table 1).  In discussion -- It is understandable from the text and the table 1 that the authors are comparing against a very strong baseline (ablation). How much does 0.02 in PSNR (color), or 0.03 in LPIPS affect perception? For someone not familiar with these measures, how could one understand the improvement? The meaning of these differences should be explained in the discussion of the results. 