Clarity: While the paper is readable, there are certainly rooms for improvements in the clarity of the paper. For example, the details of the algorithm in Section 4 is not straight forward and easy to follow, especially lines 146-182.   Originality: The paper considers the MARL problem in which each agent has its own local observations, takes a local action and receive a joint reward, and the goal is to find the optimal action-value function. During the training each agent is allowed to access the action-observation of all agents, and the full state.  It is shown that VDN and QMIX cannot represent the true optimal action-value function in all cases. In addition, for a fixed episode length $T$, it is proved that with increasing the exploration rate, decreases the probability of learning an optimal solution. Considering this, it is assumed that the lack of good exploration coupled with the representational limitations resulted to the sub-optimality of QMIX. To address this issues, an algorithm, multi-agent variational exploration (MAVEN), is proposed to resolve the limitation of monotonicity on QMIX's exploration.  In this order a latent variable $z$ is introduced which is based on a stochastic variable $x \sim p(x)$ which $p$ is uniform or normal probability distribution. Function $g_\theta(x,s_0)$ returns $z$ and another neural network, called, hyper-net map $g_{\phi}(z,a)$, returns $W_{z,a}$.  In parallel, to get the Q-function of each agent, a MLP gets $(o_i^t, a, u^{t-1}_i)$, pass the results in a GRU and the results of the GRU is mixed with $W_{z,a}$ to get $Q(\tau_i; z)$. The whole this block introduces parameters $\eta$. Then, the mixer network with parameters $\si$ obtains $Q_{tot}$. Also, a mutual information (MI) objective (what does "objective" mean here?) is also added into the model to encourage more exploration.    Significance: There are several things to like about this paper: - The problem of safe RL is very important, of great interest to the community. - The way that the exploration is added in the model might be interesting for other to use in future.  However,  - I found the paper as a whole a little hard to follow specially in the algorithm side.  - The experiments do not support the claim of the paper about the significance of the exploration.  