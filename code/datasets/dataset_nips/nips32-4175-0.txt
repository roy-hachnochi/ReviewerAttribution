Regular Hopfield nets of N nodes can only store O(N) patterns. These patterns act as local minima to denoise inputs by moving them towards the stored patterns.  The novel idea in this paper is to link this idea to that of ECCs.   In ECCs, we have N bits of which M are the message and P are parity checksums, with N=M+P.  (For example, M=7, P=3).    ECCs have a similar effect to Hopfield nets in that the 2^M possible messages each have unqiue checksums, and act as attractors to noisy input versions. But unlike Hopfield, they have exponential (2^M) stored patterns rather than linear.  This motivates an exploration of the question, how can ECCs have this larger capacity than Hopfields, and how can Hopfields be modified to be equavilent to ECCs.    The first attempt uses modified Hopfield nets with 4-factors replacing pairwise factors.   This can then be converted into an equivilent RBM-like bipartite structure with pairwise weights.  I would have liked the links to RBM to be made more explicit -- mostly just as a cultural thing, most people in the community are familiar with RBMs due to the deep learning bandwagon so it's a standard concept to hang the new ideas off. e.g. Fig 1c has RBM structure.  This first step is found not to work, because local minisiation doesn't always take us to the correct place.   But another new idea, use of expander network toplogoes, is introduced, which encourages the minimisation to find the correct solutions.      The paper is very well written and has clearly been through good internal review already. (Unusually I can't find any typos to report.)  It's an interesting and novel paper which makes a new link between two previously unconnected areas and I think it could inspire more new thinking between them.  Maybe it will do for Hopfield nets what Turbocodes did for Bayes nets.   Accept.