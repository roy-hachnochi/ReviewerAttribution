[As a general note to the meta-reviewer, the other reviewers, and the authors, this review was a last-minute emergency review and so unfortunately does not consider the extensive information in the supplementary]  This work asks whether “it is possible to build simple, Hopfield-like neural networks that can represent exponentially many well-separated states using linearly many neurons, and perform error-correction or decoding on these states in response to errors on a finite fraction of all neurons.” As the authors note, capacity is often balanced against robustness; since code redundancy is needed to enable recovery from noise, capacity is necessarily reduced because of the redundancy, making this an especially difficult problem.   The authors rise to this challenge and claim to produce a network that exhibits “exponential capacity, robustness to large errors, and self decoding or clean-up of these errors”. There network takes the structure of a restricted Boltzmann machine wherein the hidden units are comprised of clusters of neurons that laterally inhibit each other, each with the same connectivity to the input units but with different weights. The authors motivate their network using inspirations and ideas from expander graphs, error correcting codes, and Hopfield Networks.  The proposed solution is straightforward and well-motivated, and all the design and algorithm choices seem quite sensible. Unfortunately, as a non-expert in this area (and given the short time available for review) I could not properly assess the significance or novelty of the approach in the context of the broader literature.  On a general note, the manuscript is wonderfully written, and as a non-expert in this area it was easy to grasp the significance of the problem as well as the solution proposed. The authors are commended for putting together a clear, crisp, and easy to understand  piece of work. Of particular note are the intuitions and insights scattered throughout the text, which help the reader grasp the logic of the proposed solution.  I have the following questions that I hope the authors can comment on:  - While acknowledging that this work is mainly conceptual, I am left wondering about the approach’s sensitivity to the type or form of data. My hunch is that the approach should be relatively robust, in which case I am wondering about the author’s decision to not experiment with other types of data (and potentially compare results to any relevant baselines, if they exist).  - Building off the previous question, one constraint seems to be the requirement for binary inputs. Do the authors imagine a possible extension of this work to handle continuous valued data, or is this approach limited in this regard? - Can the authors provide further comments on the steep transition between error recovery and non-recovery as a function of noise (see figure 2C). Is there a reason for this fast transition, and/or would less severe slope be desirable? 