Originality:  This work proposes a model which jointly predicts the label and its position to insert it. It is formulated as a variant of ordered set prediction, but slightly differs in that the set grows as we predict a label together with the position to insert it given the current set. The sampling based training procedure is based on the prior work but it sounds quite natural to me.  Quality:  The proposed approach is well-motivated and technically sound to me, in which the prediction is split into two, position and token sub-models. Experiments are well-designed and its computation complexity for inference sounds correct to me. The analysis is quite interesting in that the generation order looks intuitive and the results are analyzed further in terms of the generation order differentiated by POSs.  Clarity:  It is clearly written. However, it is a little bit unclear about the notion of positions, whether they are global positions, i.e., sentence positions, or local positions, i.e., positions of the currently generated set. It would be good to introduce notations so that each \tau_t is dependent on the length of the generated labels at time t.  Significance:  It is an interesting model and might be of interest to many researchers. However, I'd rather expect more experiments on standard data set, e.g., WMT en-de, for comparison with prior work.  