This paper shows introduces a way to combine a markov (logic) network with knowledge graph embeddings. In particular, the approach uses EM to train the weights of a Markov Logic Network in the M-step while inferring latent triple states using a KG embedding model as variational distribution in the E-step. Results on various standard benchmarks are convincing.   I think this paper is well written and relatively clear. The idea is straightforward but in a good way (the kind of thing I thought people would have tried much earlier but haven't). The results are convincing and reasonably ablated. There are various approximations/heuristics that the authors use to make this tractable (e.g. sampling the markov blanket before calculating the expectation). These have fewer theoretical groundings but the empirical results justify them.   The paper could do a better job in discussing recent related work such as "Learning Explanatory Rules from Noisy Data", "End-to-End Differentiable Proving" and "Adversarial Sets for Regularising Neural Link Predictors" that are either related or very related (the last one for example).      