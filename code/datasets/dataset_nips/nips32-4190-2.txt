Post-rebuttal: The authors have promised to incorporate an exposition of the sampler in the revised paper, I believe that will make the paper a more self-contained read. I maintain my rating of strong accept (8).  --------------------------------- The family of log-concave distributions is a powerful and general family of distributions that are often used in machine learning (they include most of the common distribution families). I think this paper makes very nice contributions to the fundamental question of estimating the MLE distribution given a bunch of observations.   I think the key contributions can be broken up into two key parts: - A bunch of simple but elegant structural results for the MLE distribution in terms of 'tent distributions' -- distributions such that its log-density is piecewise linear, and is supported over subdivisions of the convex hull of the datapoints. This allows them to write a convex program for optimizing over tent distributions. This program captures the MLE distribution. - In order to apply SGD (or other optimization methods) to optimize this program, we need a (sub)gradient oracle. Constructing such an oracle is the other key contribution of this paper. This part is technically involved and the authors build an algorithm to approximately sample from the current distribution.   The authors give the first efficient algorithm for MLE estimation problem over a natural and important class of distributions. I think the paper merits acceptance on the strength of the results, and the techniques necessary.