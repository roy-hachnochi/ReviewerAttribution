Originality  The paper seems quite original, with new algorithms based on a novel principle of similarity between graphs.  Quality  In general, the developed framework is not sufficiently analyzed.   This paper contains no error bounds on the similarity between the reduced and original pseudoinverse of Laplacian, nor any results in this direction. There is also no formal reasoning as to why minimizing ||\Delta L^{-1}||_F should particularly result in preservation of the top eigenvectors. There is no sense given of how sparse the graph can be made while keeping ||\Delta L^{-1}||_F small.   There are many hyperparameters to Algorithms 1 and 2, and their choice is only loosely discussed as a purely empirical matter.  The experiments are well flushed out and strong. However, there is no empirical evaluation of simultaneous reduction of nodes and edges, a main advertisement of the paper.   Clarity  Although the writing flowed well, I found this paper relatively confusing.   It is not made sufficiently clear / precise why preservation (in Frobenius norm) of the psuedoinverse of the Laplacian is desired. For instance:  [75 - 76] Spectral sparsification preserves the solution x* of Lx = b in a certain sense (see e.g. the monograph “Lx = b”). Does preservation of L^{-1} in the Frobenius norm imply this same property? Does it imply preservation in a different sense?   [81 - 82] Can the statement that the 2nd eigenvector will be “preferentially preserved” be elaborated on? This feels to me like an interesting statement, and it comes up again in the experiments, but is discussed / justified in broad terms only.   The motivation for the paper is also somewhat unclear. It is not clear whether preservation of L^{-1} is a natural framework in which to consider graph coarsening/sparsification, or whether it is merely a convenient framework. E.g. why is this considered as opposed to the “ ‘restricted’ Laplacian quadratic form” mentioned in [55-57]?  I am also confused about the desire for unbiasedness in section 3, i.e. the requirement that E(L_{tilde{G}})^{-1}) = E(L_G^{-1}). This seems to be a somewhat arbitrary desire, as opposed to data reduction and preservation of ||\Delta L^{-1}||_F. Is it made purely to simplify the derivations (9) - (11), or is there a claim that it is truly a desirable property?  Additionally, I think 3.4 could be reworked. It is not clear that there is a strong takeaway, and in the absence of this, this section contains a lot of implicit mathematical manipulations without much message. It is also confusing, for instance what is B_{1d} (or B_{1c}, or B_2) when one is interested in reducing both the number of nodes and edges? Similarly, 3.5 seems unnecessary.   It is also not stated explicitly how the multi-step scheme eliminates the ‘undesirable behavior’ of the single-step algorithm .  Significance  The paper seems to tackle a relevant issue (graph compression) in a novel way, with strong empirical results. However, without more of a theoretical grounding, and more details on how to implement the given algorithm (for instance, how to choose a stop criterion in Algorithm 2), it is difficult to see it being of high practical significance.  