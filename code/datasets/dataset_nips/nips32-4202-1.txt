Thanks to the authors for responding to my questions. I am content with the submission and have kept my score at 8.  =========  This work proposes a Stochastic Runge Kutta discretization of the overdamped Langevin Monte Carlo chain and establishes a d/eps^{2/3} mixing rate (upto eps accuracy in Wasserstein distance) for d-dimensional strongly log concave distributions which is 1/eps^{1/3} faster than the previously best known rate of order d/eps for the unadjusted Langevin algorithm. They also establish speed-ups for low dimensional non-convex cases when the Hessian operator norm grows as  c+sqrt{||x||}. In fact their result in Theorem 1 is general enough to apply to general discretization schemes that satisfy certain uniform (in iteration) local deviation order.  The authors note that establishing this condition uniformly is non-trivial in general and in their case, they proceed by bounding the Markov chain moments carefully.  Overall, the paper is pretty well written with clearly stated results, contributions, and highlights about the key ideas.   In their numerical experiments, they use same step size for ULA and SRK and show that SRK-LD converges to a smaller asymptotic bias (which is consistent with their results).   I have three remarks: — If would be nice if the authors could clearly point out the condition number dependency in their theorem 2. — What are some other possible ways to establish a uniform deviation condition? — Do we expect any benefit with such higher-order discretization? (it would have to be in dimension dependency since the mixing rates already depend only logarithmically in epsilon, e.g., Theorem 2 of https://arxiv.org/pdf/1905.12247v1.pdf).