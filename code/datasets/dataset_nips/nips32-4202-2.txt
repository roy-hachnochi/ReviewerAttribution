After Rebuttal:  Thank you for the responses. I that believe the paper will be even stronger with the inclusion of the stochastic gradient-variant.   ###########################################################   Originality:  Given an integrator with some local deviation orders, the paper provides a generic theorem to obtain finite-time Wasserstein bounds. This is a very valuable theorem, which will be useful for other theoreticians working in this field. On the other hand, to the best of my knowledge, this is the first paper that uses a stochastic Runge-Kutta integrator for sampling from strongly log-concave densities with explicit guarantees. The authors further show that their proposed numerical scheme improves upon the existing guarantees when applied to the overdamped Langevin dynamics.   Quality:  On the theoretical side, I have gone through the proofs; however, I didn’t go over them line by line. The proofs are overall clearly written and the roadmap is clear.   On the experimental side, I think the experiments are sufficient since the main contributions of this paper are theoretical. However, I must admit that I don’t believe that the experimental results are charming so that a practitioner would decide to use this integrator in their applications.   In this line of research, I think the authors should mention the following paper: “Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo”, Durmus et al, Neurips 2016, where the authors aim at obtaining a higher-order integration scheme for the overdamped Langevin equation.    Clarity:  The paper is very-well written and organized. I enjoyed reading it.   Significance:  Sampling methods based on diffusions have been a major subject in large-scale Bayesian machine learning and computational statistics. In this context the paper is very well-placed and is of great interest to the community. I don’t believe that the paper proposes a practical algorithm that could replace the existing simpler methods, but it has strong theoretical contributions, which should be sufficient for this conference proceeding.   A small criticism in terms of significance could be the following. What makes diffusion-based sampling techniques interesting to the ML community is their ability to scale up to large problems since they enable the possibility of replacing full gradients with stochastic gradients. In that respect, an algorithm which requires full gradients is not very interesting to a large portion of the ML community. In the current version of this paper, it is not clear at all what would happen if we apply the same ideas to SGLD. To make the algorithm more appealing for the ML community, I suggest the authors to at least add a discussion for the case where the full gradients are replaced with stochastic ones.  