The main contribution of the paper is to characterize the implicit bias of the adagrad on linear classification problems using logistic loss (and other losses). They show that the adagrad converges to a direction which is a solution of a quadratic optimization problem depending on the initial conditions, hyperparameters and the data itself unlike gradient descent which converges to the max margin direction. They also give a few toy examples to demonstrate the properties of the adagrad solution and the difference as compared to the gradient descent direction.   Significance: It is an important problem to understand the implicit bias of various optimization algorithms on the solution and there have been several recent works along this direction. The motivation comes from understanding the generalization abilities of overparametrized deep neural networks. People have observed that the generalization abilities for deep neural networks with adagrad are not as good as with gradient descent and hence, understanding the exact form of implicit bias for adagrad is an interesting and important problem.  Originality:  This paper is the first to characterize the explicit direction to which adagrad converges for linear classification problems with logisitic or exponential loss. The framework and the assumptions and some parts of proofs are borrowed from Soudry et. al. However, overall the proof has a lot of new components as compared to Soudry et. al and they proceed via a different argument where they do not characterize the convergence rate just an estimate on the direction of convergence. [1] also talks about the convergence direction of adagrad depending on the initial conditions and the hyperparameters. Can the authors please discuss more on that in the related work section?  Quality:  Overall, I find the paper is well written. It would be nice to see more intuition of the proof of the convergence.   A minor comment: 1) Appendix Page 4: line 43: \lambda is not defined and \eta seems to be missing here?  [1] Gunasekar, Suriya, et al. "Characterizing implicit bias in terms of optimization geometry."  ---------- I have read the authors' response and thank them for their response. It would have been nice to include some empirical results as a part of the paper on somewhat larger datasets than are currently in the feedback document. 