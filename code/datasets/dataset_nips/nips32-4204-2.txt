Originality: The results are original to the best of my knowledge.  Quality: Mostly looks OK. I only have an issue with Proposition 3.1. It is claimed that with positive probability property (i) of Lemma A.6 holds for any absolutely continuous distribution I don't understand why this is true. To the best of my understanding, such distributions are simply distributions with a density function. However, such distributions can have finite support (e.g., a uniform distribution over [0,1]), and therefore property (i) may have zero probability.  Clarity:  Mostly OK, but I feel the authors tend to focus on special cases, an don't give the complete picture, even when it's easy to explain/draw.  For example, I was confused by the presentation of Example 3.1. It took me a while to understand the full solution:  \tilde{w} is pointing in the (sign(cos(\theta),sin(\theta))) direction - except when \theta is pointing in the one of the axis directions, then the solution is non-unique. If the authors add (or draw) this general result, it will be much  easier to understand what is going on in section 3.3.2.  Significance: Personally, the results do not seem very surprising to me, but  it is also important to prove results which are not surprising, and perhaps the proof method has some novelty.  Minor comments: 1) line 81: "Lemma" -> "Theorem" 2) line 94: I guess it is assumed that y_n=0 w.l.o.g from this point on? Later y_n appears (e.g before line 146)  again and should be noted why. 3) line 118: what is "decisive" part? 4) Example 3.1: Why use two points? Isn't one point enough? 5) line 151: what does "irrelevant to x_1" mean?  %%% Edited after rebuttal %%% First, I thank the authors for clarifying Proposition 3.1, and I, therefore, increased my score as promised. The revised version seems correct now, but I think the authors overstate its significance in the rebuttal. First, it is not clear from the proposition, and the discussion around it, how small is this positive probability. The proof suggests that to me that "generically" this probability is exponentially vanishing with the number of dimensions and data points. Such a rarely occurring result does not seem to be very strong. Second, I think it is also somewhat inaccurate to say that "It negates the claim 'the implicit bias of AdaGrad does indeed depend on the initial conditions, including initialization and step size' on Page 8, Gunasekar, Suriya, et al. [2018a].". Specifically, I think the claim in Gunasekar et al. was not made for every dataset (as clearly, it not true with d=1), but that such dependency can exist, in contrast to gradient descent (and, indeed this seems to be the generic case from this paper).  Still, I still think it is a borderline paper, as the final results are somewhat incremental (there are no surprises or new exciting theoretical directions), the characterization of the implicit bias seems incomplete (it still depends on h_{\infty}, which we don't know), and the writing still requires polishing (as admitted by the authors).   