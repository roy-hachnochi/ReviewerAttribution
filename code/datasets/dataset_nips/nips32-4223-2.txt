The paper addresses the problem of training spiking neural networks (SNNs), in particular recurrent SNNs, with a new form of backpropagation (BP). BP on SNNs is hard because of temporal effects and the discontinuous nature of spikes. The topic of deriving BP rules for SNNs has received increasing attention in the previous years, and the most relevant works are referenced. The present paper is valuable in extending the scope to recurrent networks, and giving more attention to temporal effects, which are neglected in other approaches. So the topic itself is not original, but it is a valuable extension of the state-of-the-art.   The main contribution is the ST-RSBP learning rule, which backpropagates over spike trains, rather than unrolling recurrent networks completely over time. Spike-train level Post-synaptic potentials (S-PSPs) are introduced, which accumulate the contributions of the pre-synaptic neuron to the PSP right before the spike-time of the post-synaptic neuron. From this, an approximation of the spike count is derived, which in turned is used as the basis for the backpropagation derivation.  The following remains unclear to me: - The BP algorithm requires a desired output firing count y (in (3)). How is this determined? Is there a constant firing rate for the correct output assumed, and if yes, what is it? - When is the BP update performed? Is it a batch-mode update, or can online learning be performed as well? - How suitable is the learning rule for hardware implementations of SNNs in neuromorphic or digital platforms? - How are firing thresholds set? Section 4.1 says it is "depending on the layer", but does not give more results.  Experiments are performed on 4 datasets, where in every case an improvement over previous BP-based SNN training methods is shown. There is also a comparison to non-spiking approaches, but I am not sure how the authors picked the references. For example for Fashion MNIST there are much better solutions available than the ones reported here, there are even multiple tutorials that show how to reach 93% or more. - I strongly recommend also reporting the non-spiking state of the art for each example to give a fair comparison and not over-sell the SNN results. - I am not sure why a recurrent model was chosen for the (static) Fashion MNIST dataset.  Overall I think this is a good contribution, and the presentation is OK, although a lot of the derivations can only be understood with the help of the supplementary material. Recurrent SNN training is not a major topic at NeurIPS, but is of interest to the community.  As a final suggestion I would recommend re-working the abstract, because it tells a lot of very generic facts before discussing the actual contributions of the paper.  ===================  Update after author feedback: I want to thank the authors for addressing my and the other reviewers' questions in their author feedback. My main concerns have been addressed, and I think this paper should be accepted, accordingly I will raise my score to 8. Still, I think there are some improvements to be made, as addressed by the reviewer concerns and the author feedback: - please make explicitly clear that you are not addressing CNN-type networks - I think it would be great if you can include an outlook on the hardware implementation on FPGAs, because I think this is an important point for this rule - Please make the original contributions compared to existing spiking backprop variants clearer - Please include all hyperparameters (thresholds, learning rates, ...) also in the supplementary material, and not just in the code.