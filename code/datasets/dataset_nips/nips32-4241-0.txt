The work addresses the issue of neural networks’ overfitting to test sets on classification tasks due to widespread reuse of the same datasets throughout the community, and how that affects the credibility of reported test error rates, which should reflect performance on ‘truly new’ data from the same distribution.  The proposed test statistic does not affect the training procedure, and is simple in theory:  if the (importance-reweighted) empirical risk and the empirical risk of adversarially-perturbed examples differs by more than a certain threshold (given by concentration bounds), the null hypothesis that the classifier and the test data are independent is rejected.   My main concern is that the type of adversarial examples used, bounded translational shifts (for image data), is very limited and likely to be unrealistic.  Effectively shifting the frame of a CIFAR image is quite different from swapping items in a scene; it is less subtle and less ‘insidious’, unless perhaps a “7” is converted via truncation into a “1”.  It would have been nice to see example adversarial images for a sense of how they compare to the ones typically discussed in the literature, particularly as a selling point of the work is the use of adversarial examples.  Finally, while the writing is fluid, it comes across as rather too conversational and verbose.  It would have been nice to have the math separated out from the text, in the form of definitions, propositions, etc., and for the text to have been generally more terse.  *** [UPDATE] It appears that the authors misunderstood my examples of adversarial examples.  By "swapping items in a scene", I meant repositioning or replacing irrelevant items; this is comparable to [1], which studied the addition of irrelevant items (eg, stickers to a stop sign, which happens often enough in real life, if not in the exact form shown in the paper).  This was meant to be an example of a realistic adversarial example--and one not achievable by translational attacks.  By "truncation of a '7' into a '1'", I meant an example that *could* be achieved by the authors' translational approach, but which is not subtle enough to really be considered an adversarial example in the classic sense.  Conversely, as the authors concede, translations that fall short of such an extreme are rather weaker than standard ones.  These include pixel-by-pixel perturbation (ie, adding a random mask to an image), one of the most widely studied cases, and where differences are essentially undetectable to human observers.  This is a very intriguing line of work, but unless more compelling sorts of adversarial examples are used (finding ways of dealing with the induced distribution shift, as mentioned), it seems a bit premature.  Hence my score remains the same.  [1] Physical Adversarial Examples for Object Detectors Eykholt et al., 2018