The authors present a method called Conditional Neural adaptive Processes (CNAPs) able to efficiently solve new multi-class classification problems after an initial pre-training phase. The proposed approach, based on Conditional Neural Processes[1], adapts a small number of task-specific parameters for each new task encountered at test time. These parameters are conditioned on a set of training examples for the new task, don't require any additional tuning and adapt both the final classification layer and the feature extraction process, allowing to handle different input distribution.  While being very close to CNP, this work focuses on the image classification task and makes several addition to the original method. These additions (FiLM layers, auto-regressive feature adapter, usage of deep sets) are clearly justified and their individual contributions are explored in the different experiments.  The major negative point of this paper is its similarity with CNPs. The authors compare the two approaches in section 2 (lines 67-70), but this argument is not convincing at all, the adapted parameters can also be seen as a simple vector. I think the article would gain from putting a bigger emphasize on the auto-regressive way of dynamically adapting the parameters, which is an interesting and novel contribution.  The article is very well written. While the approach is complex, the authors did a good job at progressively presenting the different components used, with clear explanations and corresponding references to justify each choice they made.   [1] Conditional neural processes. Garnelo et. al. 2018