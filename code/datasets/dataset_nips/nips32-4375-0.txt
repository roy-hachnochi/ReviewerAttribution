 This paper considers the problem of privately estimating the "fingerprint" of a dataset. A dataset over some universe consists of (x,n_x) pairs, where x is a label, and n_x is the number of times that label x occurs. The fingerprint of this dataset is the histogram over the n_x's: it discards the labels, and, for each r >= 0, it counts the number of labels which were observed exactly r times. This is equivalent to approximating the histogram of the distribution up to a permutation.   Under some appropriate distance measure (the minimum L1 distance over datasets which may correspond to the given fingerprints), this paper gives an algorithm which estimates the distribution up to accuracy ~O(sqrt(n/epsilon)) (in the high-privacy regime -- in the low-privacy regime, there is also a O_epsilon(sqrt(n)) upper bound). There is a known lower bound of Omega(sqrt(n/epsilon)) in the high-privacy regime, showing that the upper bound here is optimal up to a log(1/epsilon) factor. There is a new lower bound in this paper for the low-privacy regime, which is close to tight (up to a constant in the exponent multiplying epsilon -- as such, the use of "near-optimal" is slightly imprecise in line 119-120, as I usually read this as optimal up to polylogarithmic factors). The authors also provide a corollary of their main upper bound, to estimating symmetric properties of distributions.  The results seem quite strong. It is impressive that this paper manages to essentially settle the problem in both the high and low privacy regimes. (Very) naive methods would give error of O(n/eps) (the authors should provide a comparison with the sample complexity for the problem of estimating the histogram itself, which I believe is this same value of n/eps), this paper improves the accuracy quadratically in both n and eps. The algorithm itself is fairly "elementary" -- there are a lot of steps, but pretty much all of them are simple operations, which may result in some adaptation of this method being practically realizable. There is a rather careful "partitioning" method into high and low count items in order to achieve the sqrt(n) error bound, rather than the naive n.  My biggest criticism with this work is perhaps the clarity of the actual method. While a prefix of the paper is rather well-written and provides a nice exposition, unfortunately, I found the technical core of the actual result difficult to follow. The pseudocode itself (page 8) is rather inscrutable by itself, so one must consult the accompanying prose description (Section 4). However, the critical portions of this (Sections 4.2 and 4.3) are a bit terse, and marred by typos and poor language in some crucial places (I point out some of these instances below). I think this paper would benefit greatly from a more thorough and carefully written technical overview of the algorithm, potentially even in the supplementary material. I would be slightly hesitant to accept the paper in its present state, since I feel a small improvement here would greatly reduce the burden placed on any future reader. Thus, I would consider improving my score with a firm promise to address this issue in the final version.  It would have been nice to see some concrete instantiations of Corollary 1, even in the supplementary material. What are the sample complexities for these problems, and how do they compare with Acharya et al.'s work? What type of properties can you handle that they do not (I think distance to uniformity?)? I think it would be worth expanding on the implications of this result. Also, wouldn't entropy depend doubly-logarithmically on the domain size, since f_max can be as large as log k? This is contrary to what is stated in line 173-174.  Some typos I discovered (not comprehensive): The crucial definition in line 180 appears to be incorrect at first glance, as the s in the summation is never used. Line 227 typo (deferentially). Line 237 has poor grammar (and generally, this sentence could be expanded a bit -- why does a large L1 distance imply a large sensitivity in the histogram (and what does the sensitivity of a histogram even mean?)). Superscript in line 236: h^2_2 should be h^s_2. Missing period line 243. Algorithm PrivHist has Z ~ G(e^-eps_1) -- should be Z^a. Also, Z^b should have eps_2 instead of eps_1. Input in PrivHist-LowPrivacy H^\ell is missing a c in the superscript.  EDIT: Based on the promise of the authors to improve the explanation of the main algorithm, I have raised my score.