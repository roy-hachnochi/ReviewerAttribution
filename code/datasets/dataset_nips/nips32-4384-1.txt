1) Ablation studies. It would be really interesting which algorithmic choices were necessary to get the reported results eg:  What about longer unrolls of the graph net T>2? How important is it to hard code that the policy with p=1/2 flips random variables of unsatisfied clauses? 2) Details on curriculum. I did not find the details to what criterion was used to decide when to ratchet up the task difficulty in curriculum learning. Could the authors clarify (or did I miss it somewhere)? 3) No value function? I’m surprised to see that the authors used straight-up REINFORCE without even a value-baseline (as explained in the appendix), bc in my experience this almost always makes a big difference in learning speed. Have the authors tried just adding a different “head” to the graph net torso that predicts expected return and use that as a baseline? 4) Figure 4: It would be interesting to see the walk-SAT baseline results here in the graph. 5) l228-231: Why is the policy trained on multiple episode on the same problem? Is this to reduce computational overhead due to initial checking for satisfiability with MiniSAT? I’m asking because attempting to solve the same problem multiple times can cause strong correlation in policy gradient. It might give a significant boost to write feasible instances into a replay buffer and then sample random formulas afresh for each gradient update. 