i)Originality :  Introduction of the notion of “local switching cost” which extends the notion switching cost in online learning to RL. They also present (two flavours of) a Q-learning algorithm that achieve the regret matching the previous work however with the added benefit of having lower local switching cost.   ii)Quality : The motivation behind considering the problem of RL with low switching cost is clearly explained. They provide a lower bound on the switching cost of any algorithm with sublienar regret.  The proposed algorithm follows a conservative approach to change policies, as is the requirement for low switching cost, however their analysis manages to recover the same regret bound as the previous work. They also show applications of their algorithms in concurrent RL.   iii)Clarity: The paper is well written. Both the text and math are clear except few minor issues: a. Line 128: “...,so that the agent needs only play” b. Line 149: “Our algorithm maintains wo sets…” b. Line 254: “…,and it is a priori possible whether such a mismatch will blow up the propagation of error.” b. line 274: “.., at most off by a factor of O(H^2 log K)”  iv)Significance : This paper addresses an important problem in RL which has strong practical motivations. They provide algorithms which are provably better at the considered problem than the previous work.   