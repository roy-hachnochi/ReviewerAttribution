1. Technically, this paper is interesting but I just fail to understand the practical relevance of the problem. And since it puts together a bunch of well-understood ideas, it seems rather incremental.  2. Switching cost in online learning was introduced in the late 1980s but in a  general way as opposed to a very specific cost function here. Furthermore, finite horizon MDPs are considered (as opposed to infinite horizon) which kind of negates the argument that it can be costly to switch policies. This is because the optimal policies in the finite horizon are non-stationary anyway. So, it may hardly matter that these change due to exploration.  In my opinion, that problem will be much more interesting in a infinite horizon setting and with the both the local and the global switching cost functions.  3. In Theorem 2, it would be better to present exact bound on regret (if it is known) instead of just in O(.). I presume that from their analysis, the authors are able to achieve it.  4. I am not able to appreciate the application to Concurrent setting. Perhaps the authors can elaborate. We already know asynchronous QL converges under suitable recurrence conditions. So what is new here.  5. Why is LB in Section 7 interesting since it holds only when the switching cost is constrained.