After rebuttal:  I read the author response and other reviews. I'll keep my assessment. ---------------------------  This paper studies the Q-learning problem with linear function approximation and proposes the first provably efficient algorithm DMQ for general stochastic setting. Melo and Ribeiro [2007] and Zou et al. [2019] are most related work but assumes the exploration policy is fixed. This paper also introduces a DESC oracle, which examines whether all functions perform well on both distribution D1 and D2. In the DMQ algorithm, the learned predictor will be near optimal when the DESC oracle returns False. In addition, DESC oracle will only return true at most polynomial number of times thus avoiding exponential complexity. The DESC oracle is novel and has a nice property. Overall, this paper is clearly written and theoretically sound.  Some comments:  The proposed DESC oracle has an interesting property to me. By definition, it tests whether all functions in the function class work well under both distribution D1 and D2. In the DMQ algorithm, it will guarantee the learned \hat{\theta} is close to the true \theta.  One limitation of this work is the assumption that the true Q-function is exactly linear, which will not hold in general. Is it possible to extend to the approximate case? Will the algorithm also return near optimal policy when the best linear approximation is close to the true Q-function?   The sample complexity is polynomial in terms the 1/\gamma, where \gamma is the suboptimality gap. If few state-action pairs have 0 or quite small gap, but most state-action pairs have large gaps, can the sample complexity be improved?  In Algorithm 2, the agent needs to collect on-the-go reward y, which has higher computation complexity than collecting reward r since the agent need to roll out the timestep H.  The proof of Theorem 6.1 is a little unclear to me. In line 401 and 402, why label ys in D_h^a have bias bounded by \epsilon? Is it because the result hold for h'=h+1,...,H so that the induction can be applied?  The \gamma notation denotes the suboptimality gap in this paper. Although it will not lead to confusion, I think it would be better to change to another notation since \gamma is usually reserved for the discount factor.