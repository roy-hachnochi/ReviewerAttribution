After rebuttal:  I have carefully read the comments from other reviewers and the feedback from the authors. My main concern was the generalization ability of NGD, but the experiments in the feedback are a bit confused to me because GD doesn't seem to achieve zero training loss but NGD converges to 0 very quickly in MNIST regression. I would suggest the authors provide more details about that experiment setting, e.g., how do you select the hyperparameter. Thus, I would like to keep my score unchanged. ---------------------------------------------------- The authors analyzed the dynamics of Natural Gradient Descent(NGD) with a two-layer over-parametrized ReLU neural network and quadratic loss. The framework for the proof follows the recent line of work about over-parametrization, e.g., the papers written by Du et al, Li and Liang, and Allen-Zhu et al., the core of which is the Gram matrix. The main intuition of this framework is that if the Gram matrix is positive definite, then the gradient will be lower bounded by the minimum eigenvalue of the Gram matrix and the value of objective function (loss function), i.e., the gradient cannot be small unless the objective is small, so we will get a linear convergence rate. Moreover, due to this linear convergence rate, the movement of the weights will be small, making the Gram matrix always positive definite. These two things are in some sense equivalent to the two conditions mentioned in this paper, and the authors proved that with NGD and a two-layer over-parametrized ReLU neural network, one will satisfy these conditions with high probability. The interesting thing about this result is that it achieves a better convergence rate using the second-order optimization algorithm and better tolerance for the step size.  The authors also provided analysis for K-FAC in a very similar setting, and still uses a similar framework for the proof. The converge rate and requirement for the width of the network is a bit worse than NGD, but acceptable. The authors analyzed the generalization performance for NGD as well.  At several places, the authors made some simplifications for the setting, e.g., fix all the weights on the second layer. This is acceptable because otherwise the movement of the second layer will still be small and doesn't significantly influence the result.  This paper is generally well-written and very easy to read.  The authors also bounded \lambda_0 in a very simple setting, which is not mentioned in the paper of Du et al., but it would be better to analyze it more carefully.  Despite the advantages mentioned above, I have some concerns about this paper, which are listed below:  1. For the generalization result, getting similar generalization bound doesn't necessarily mean that the two algorithms will have a similar ability to generalize. Thus, to make the authors' claim about generalization more convincing, I would suggest the authors do some experiments on some real-world datasets and compare the generalization performance.  2. The authors gave a lower bound for \lambda_0 but doesn't lower bound \lambda_S, so we don't know the difference in width requirements of NGD and K-FAC. The authors can say something about the lower bound of \lambda_S or do some experiments to convince the readers that the requirement of K-FAC is not much worse.  3. The authors are simplifying some settings like fixing the second layer weights, but I think the authors should provide some intuition about why it works for the general setting instead of directly saying "it's easy to ...".