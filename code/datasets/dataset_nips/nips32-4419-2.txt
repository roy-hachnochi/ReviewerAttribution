After reading authors' response: I now understand that there might be subtle issues with preprint [3]. However, I maintain the original comment that the continuous-time convergence of KL under LSI isn't surprising and the discretization analysis seems novel.   I now see that the generalization to a weaker Poincare ineq. condition is a good addition (which the authors claim to be one of three key contributions in their response). However, it is slightly confusing that they have not included any description regarding this contribution in the abstract of the original submission.   - Originality There has been a resurgent interest in ULA and its derivatives in recent years partly due to the empirical success of SGLD [1] and strong theoretical guarantees for its performance in non-convex optimization [2]. Most results were proven with the rather stringent assumption of strong convexity, however, recent results were able to replace this with the log-Sobolev condition [2,3] or distant dissipativity [4,5] (based on switching between synchronous and reflection couplings).   The first contribution of the paper, which proves a convergence bound in the KL divergence without assuming strong convexity is not surprising. Most of the techniques are known or bear a resemblance to existing works in the literature (see e.g. [2,6]), and the authors are upfront about this. Nevertheless, the discretization analysis using arguments based on the Fokker-Planck PDE for the density seems novel. The overall analysis is clean and is a good addition to the current literature on sampling methods based on the Langevin diffusion.   The second part of the paper, where a generalized convergence bound in the Renyi divergence is shown, is novel.   - Quality The paper is technically sound with generally well-written proofs provided. Although I did not read over every detail in every proof, I followed most of them (proofs of the main theorems 2 and 4), and they seem correct.   A potential weakness of the second part of the paper is the rather strong assumption on the (modified) biased target measure. It seems that to verify LSI for the perturbed biased target, we may need to first infer some property of it, which seems much more difficult than merely bounding its error from the true target.   Additionally, the second part of the paper would be more convincing if ample motivation was given as to why the Renyi divergence would be an interesting distance measure to consider for sampling applications.   - Clarity While the paper is generally pretty readable, the overall clarity can be further improved. It seems that the introduction is quite long and contains the description of some less relevant information, i.e. the weaker Poincare inequality condition.   Specifically regarding the introduction, one of the other issues is that some of the important concepts are not formally defined. This is generally fine for informal high-level theorems/proofs, but it is the usage of less standard notation and terminology that might confuse people. For instance, the first theorem in the intro assumes the target measure \nu (instead of the negative gradient of log-density) is L-smooth, where only until section 2.1 was it mentioned that this actually means the gradient of -\log \nu is Lipschitz (or the Hessian is upper bounded).   Additionally, I'm a little bit curious as to why Theorem 1 and 3 don't assume Lipschitz drift coefficients (or in this case just smoothness of the gradient of potential), since this is usually required for the existence and uniqueness of the solution to the SDE.   - Significance See part 1 of the review.  References [1] Welling, Max, and Yee W. Teh. "Bayesian learning via stochastic gradient Langevin dynamics." Proceedings of the 28th international conference on machine learning (ICML-11). 2011. [2] Raginsky, Maxim, Alexander Rakhlin, and Matus Telgarsky. "Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis." arXiv preprint arXiv:1702.03849 (2017). [3] Ma, Yi-An, et al. "Is There an Analog of Nesterov Acceleration for MCMC?." arXiv preprint arXiv:1902.00996 (2019). [4] Gorham, Jackson, and Lester Mackey. "Measuring sample quality with kernels." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. [5] Eberle, Andreas, Arnaud Guillin, and Raphael Zimmer. "Couplings and quantitative contraction rates for Langevin dynamics." arXiv preprint arXiv:1703.01617 (2017). [6] Cheng, Xiang, and Peter Bartlett. "Convergence of Langevin MCMC in KL-divergence." arXiv preprint arXiv:1705.09048 (2017).