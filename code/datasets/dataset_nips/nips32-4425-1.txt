This paper considers learning the distribution generated by one-layer ReLU functions, with input from Gaussian distribution. The high level idea is simple. First step is to estimate the bias term and the norm of each row of the weight vector. Second step is based on a well known fact on the covariance of two ReLU functions. The first step is shown via a maximum likelihood approach from an earlier work (DGTZ18).  Overall, the paper is well presented and the proof looks correct to me. One limitation is that the proposed estimation method is somewhat restricted to the specific setting studied here. This makes the connection between this paper to the motivating examples (GANs, VAEs) somewhat weak.   Other questions:  *) Do similar results extend to more general input distributions? What are the boundaries beyond which estimation becomes hard? It may be worth discussing a little bit.  *) In Sec 4.2: estimating the norm of W(i,:) and b(i) boils down to a single dimensional problem. I wonder if there is a simpler way to estimate these quantities. 