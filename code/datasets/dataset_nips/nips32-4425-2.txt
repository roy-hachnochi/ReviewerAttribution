The paper studies the problem of learning the parameters of a one-layer ReLU network by observing its outputs on a latent standard Gaussian input. This problem is different from the supervised learning problem typically studied for deep networks where we see both the input z to the network and the output y and wish to learn the parameters of the network theta. Here we know the input z is coming from a standard Gaussian and we only see the output y and we wish to learn the parameters of the network (which would translate to learning the rectified Gaussian distribution) The paper presents an exponential sample lower bound for learning if the bias is allowed to be arbitrary. This is because when the bias is allowed to be negative most of the realizations of the latent variable z can map to 0 in the output space due to rectification and hence we will see very few effective samples. Then the authors proceed to restrict the bias to be non-negative and using recent results on learning a Gaussian from truncated samples give an algorithm for learning the network when the weight matrix is well-conditioned. The main algorithmic tool used is projected gradient descent.  The estimation proceeds in three steps. First we realize that we can have identifiable recovery only for WW^T. To recover WW^T, it suffices to have the two norms of each row of W and the angles between all pairs of rows. To recover the two norms of each row of W and the bias vector, the algorithm for learning from truncated samples is applied. Then to estimate the angle between rows, the paper uses a simple observation wherein the sign of inner products of two vectors u,v with a random Gaussian vector z is used to infer the angle between u,v. This estimation however carries the noise in the estimation from the previous step and one needs to show that the noise doesnâ€™t accumulate by too much.  Overall, the paper is well written and has clarity. I am not sure how novel the algorithm is given that is it heavily based on prior work but the idea of applying it to this setting carries merit.  ========== POST AUTHOR-FEEDBACK: Thanks to the authors for their response where they explained the difficulties of extending the results to neural networks with larger number of layers. I will be staying with my current score.