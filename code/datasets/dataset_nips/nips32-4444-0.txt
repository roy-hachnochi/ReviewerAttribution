The claim of this paper is clear and easy to read. Although the composition of CNTK is straightforward and its exact computation in ReLU networks seems to be a bit trivial, this is a natural direction to be explored and some researchers will be interested in experimental results reported in this paper. The non-asymptotic proof is also a novel contribution and will be helpful to further develop the theory of NTK.  Basically, I believe that this paper is worth to be published.   I think that clarifying the following points will further increase the significance of this paper.  - In table 1, CNN-V (or CNN-GAP) trained by SGD achieved the highest performance compared to CNTKs. When the number of channels increase, does the performance of CNN-V (or CNN-GAP) converge to that of CNTKs? Because Authors argue the equivalence between the sufficiently wide net and kernel regression in the former part of the manuscript, some readers will be interested in experimental justification on such equivalence in CNNs. It will be also informative to empirically demonstrate in what finite number of channels the trained CNNs perform better than the kernel regression with the CNTKs.   - In main theorems, the width m is evaluated by using the notation of poly(…), i.e., m >= poly(…). It seems to be almost enough to show that the lower bound is polynomial, it will be also helpful to show the specific rate of the polynomial order (I mean, when m>=poly(n), what is k satisfying m>=O(n^k)?). Such rates are explicitly shown in a part of lemmas and theorems (ex. Theorem E.1 and E.2), it is hard to follow them in the proofs of main theorems.  It seems to be better to add a brief description of the rate of the polynomial order to Supplementary Material.   - It may be better to remark on locally-connected network (LCN; CNN without weight sharing). [Novak et al 2019] claims that the kernel matrix (corresponding to weakly-trained nets) without global average pooling is equivalent between CNN and LCN. I am curious as to whether this equivalence holds in the case of NTKs. As shown in Table 1, the CNTK-GAP performed better than CNTK-V. This might be because the pooling makes correlations between different patches non-negligible and the advantage of weight sharing appear.  -  Lemma E.3 : I cannot fully understand the meaning of "random vector F=w^{\top}G".  Does this mean that F is a fixed random vector independent of w and w is generated under the constraint of F=w^{\top}G after fixing G and F? It would be better to enrich description on its definitions.  Typo:  In Lemma F.9:  m >= poly(\omega) ->   m >= poly(1/\omega)  --- After the rebuttal --- I have been almost satisfied with the authors' response and keep my score. I hope that Authors can improve the polynomial rate of the bounds in follow-up works. 