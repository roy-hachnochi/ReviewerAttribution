A growing body of literature has shown that building symmetries into neural networks through equivariant layers is an effective means of improving results, especially in the face of limited data and even when data augmentation is used. This paper continues that trend by showing that equivariance to chirality transformations consistently improves results on pose regression tasks.   The paper is well written and easy to follow. Related work is discussed in a mostly adequate and balanced manner. The work fits in existing theoretical frameworks when considering that the group acts in a linear way (though not via permutations). Such networks are covered by the theory of Shawe-Taylor and colleagues (e.g. "Representation theory and invariant neural networks") as well as recent work by Kondor, Cohen, and others. This paper however focuses on the practical aspects of implementing chirality-equivariant layers, rather than mathematical theory, and as such makes a very useful contribution.  It is shown that the equivariant layers reduce the number of parameters and FLOPS. A very solid experimental validation is performed, showing consistent improvements over recent state of the art methods for this task. The improvements are not very large, but this is not to be expected from such a small (2 element) symmetry group.  Overall, this is a nice paper with a simple, well executed idea.  Typo on line 75: equvariant  >>>> Post rebuttal comments I have read the other reviews and the rebuttal. The reviewers seem to agree that this paper makes a useful contribution and should be accepted. Since I did not raise any major concerns in my initial review, the rebuttal was mainly addressed at the other reviewers, and so did not change my judgement significantly. 