This paper studies how the critical batch size changes based on properties of the optimization algorithm, including SGD, momentum, and preconditioning. Theoretically, the authors analyzed the effect of batch size and learning rate via a simple quadratic model. Empirically, the authors investigated deep learning experiments and confirmed their theoretical findings for the simplified model.   The paper is clearly written. The theoretical developments are based on a simple diagonalized quadratic model.   1. Regarding the proposed quadratic model, as the Hessian H is diagonal, the loss can be decomposed dimension-wise and therefore the optimization in each dimension evolves independently. These formulations are very restricted and far from the practice of deep learning models. Can the author comment on the generalizability of the analysis to non-diagonal quadratic models or even simple neural network models?  2. Regarding the optimizers, the author adds some Gaussian noise to the gradients to model the effect of stochastic sampling. Such a noise model is fixed throughout optimization and has diagonal covariance matrix, which is different from SGD whose stochastic noise also evolves and the covariance matrix can be non-diagonal. Also, the author claims that Adam can be viewed as a preconditioned SGD, but the preconditioning matrix considered takes a very special form of H^p. While all these simplifications can lead to an analytical understanding of the optimization, they do not necessarily cover the practice in training deep models.  3. Overall I think the theoretical contribution of this paper is limited. The author tries to explain and understand the optimization mechanism of deep learning by studying a simplified quadratic model. The deep learning scenario violates many of the assumptions in this paper, e.g., diagonal Hessian, independent optimization among dimensions, fixed noise, etc. While the theoretical results of the quadratic model fit (to some extent) the empirical observations in training deep models, there is no good justification for a solid connection between these two models. It would be better if the authors can justify (to some extent) the motivation of simplifying deep learning scenarios into such a simple one.      I have read the authors' response. It addresses my concerns on the diagonal assumption of the Hessian of NQM. Overall, I think this is an interesting paper that tries to model the relationship among the optimal choices of hyper-parameters for different optimizers in training neural networks. I am still a bit concerned about the use of a convex quadratic model and its generality. I raised my score to be 6 marginally above the threshold.