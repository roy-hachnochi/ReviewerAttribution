This paper proposed a new algorithm for max-inner-product-search, a widely encountered problem in all kinds of applications. Though seemingly similar to ANN problem, MIPS is different in terms of theory and algorithm design, so that the massive amount of KNN methods cannot apply directly. The authors extend the well-known Delaunay graph type of methods in ANN to MIPS and provide both theoretical discussion and experimental evidence to show the advantage of the proposed method. I find this paper to be interesting, and would like the authors to consider my following comments:  1. For assumption 1, I'm a little confused. It is not clear to me why this assumption is important. In general, the data points can be shifted without causing problems, so that conical hull should be the whole space as long as the data points are in general position.  2. Line 126 "dataset approximately follow the normal distribution" is not generally true. The datasets can easily be multi-Gaussian or more complicated, but of course this won't affect the paper's assumption that 0 is an interior point of the dataset. Maybe a better way of expression is to generally check this assumption in the datasets used in the experiments. Also, it would be nice to see how the algorithm is affected if assumption 1 does not hold.  3. A trend of the ANN and MIPS community is to involve GPU for acceleration. Though it's a matter of hardware and implementation, nowadays the algorithm design is deeply influenced so that an algorithm with a little higher complexity but more compatible with GPU is preferred. In some cases, GPU+exact search is already as fast as approximate search. The scale of some of the datasets used in this paper already fall into this scenario. The authors may consider to involve some of the related literatures like [1-3] or discuss the possibility of making the algorithm easier for parallelization or compatible with GPU.  4. The experiments in section 5. I'm confused about the running time experiments. I only find QPS test but not preprocessing time. Since approximate type of search methods usually require pre-processing and the overhead is generally large, it is better to also demonstrate the latency comparison for that part.  5. The paper's contribution might be a concern since it is an extension from exiting algorithms. But at least the experiments show positive results so it might still be meaningful and it is indeed an interesting work.  [1] Johnson, J., Douze, M. and Jégou, H., 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data.  [2] Wang, C., Tang, L., Bian, S., Zhang, D., Zhang, Z. and Wu, Y., 2019, May. Reference Product Search. In Companion Proceedings of The 2019 World Wide Web Conference (pp. 404-410). ACM.  [3] Li, S. and Amenta, N., 2015, October. Brute-force k-nearest neighbors search on the GPU. In International Conference on Similarity Search and Applications (pp. 259-270). Springer, Cham.   ---------------- Update:  Thanks to the authors for the response. I think the rebuttal has cleared my confusions and some issues mentioned in my previous comments. I decide to change my rating to 7. One further comment regarding assumption 1: you only need to perturb the dataset a little if you have a zero vector in the dataset (which is usually the case in practice). Or more directly, if you add (0, 0, ..., 0, +-\epsilon, 0, ..., 0) to the original dataset, it won't affect the MIPS task since the added vectors have very small norm, but then assumption 1 becomes true automatically. I'm just trying to understand what is actually needed here for the theories to hold.