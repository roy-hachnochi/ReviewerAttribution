The central idea of this work, that it is easier to define individual fairness when individuals receive many decisions instead of a single classification is a solid idea, but it is not exactly new.  The earliest paper I can find now that proposes a notion of individual fairness based on this idea is Zhang and Shah 2014 (http://papers.nips.cc/paper/5588-fairness-in-multi-agent-sequential-decision-making.pdf), but even the work by Joseph et al. (2016, 2018) cited is loosely an example of this.  The motivation actually given is, however, less than satisfactory.  The example cited Kearns et al. (2018) is best addressed not by this work, but by that paper cited (and on this subject, I don't love using two real genders but two fake races).  The real reason is not that you forget about a sensitive attribute and therefore harm that population, but because the guarantees become trivial as the complexity of the set of attributes go to infinity.  Here, on the other hand, guarantees need not do this, as the utility the individuals receive is not binary.  So, I really see the proposed definition as closer to group fairness with group sizes of one than the individual fairness of Dwork et al. (2012).  This outlook can help to solve the following issue, which is that I'm highly skeptical of introducing a new fairness definition for computational conveniences/ assumptions, such as realizability assumptions cited as a reason for this new fairness definition.  It should therefore be pointed out that the ethical motivation here is (basically) the same as in group fairness, in a setting that allows for a simpler formulation.  Nonetheless, this is a particularly clean formulation of this basic idea.  It's also a well-written paper and while I did not have a chance to check the proofs, the results make sense.  Edit: I am satisfied with the response to this review.