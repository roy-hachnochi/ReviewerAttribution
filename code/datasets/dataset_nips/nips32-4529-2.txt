This is a nice paper that establishes convergence of actor-critic algorithm in the special case of LQR. The paper entirely addresses theoretical analysis, and continues the line of analysis of RL for LQR. While certain pieces of analysis are built on a recent paper on analysis of policy gradient methods for LQR (Fazel et al ICML 2018), analysis of actor-critic style algorithms seems more challenging due to the interaction during learning between actor and critic. The analysis techniques departed from the traditional ODE-based approach for actor-critic algorithms. The paper is well-written for the most part. Overall, I believe this would make a nice addition to NeurIPS.   I do think that the bilevel optimization perspective, while interesting, does not really contribute much to the exposition of either the algorithm, or the analysis. It appears that the paper did not leverage much from bilevel optimization literature. Going the other direction, i.e. using what is presented as contribution to bilevel optimization seems unclear to me.   Equation 3.18: it is stated (line 215) that the objective in equation 3.18 can be estimated unbiasedly using two consecutive state-action pairs. Iâ€™m not sure if I understand why that is the case. Would you come up against the double sampling problem for policy evaluation here?  Some minor comments: - Equation (3.16), the symbols are used before being introduced in line 210 - Looks like there is a missing constant on line 294 (for some constant C is what you meant) - The analysis in the paper is for the on policy version of actor-critic. So  algorithm 3 does not really come with the same guarantees. Perhaps the main body of the paper should make this clear. 