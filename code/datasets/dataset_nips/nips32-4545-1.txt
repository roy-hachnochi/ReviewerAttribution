The authors study the inductive bias of relu networks in dimension 1. They distinguish between the active and kernel regime, which is a very timely question in deep learning.  This paper gives a very satisfactory and precise answer for the kernel regime. Theorem 4 shows that in 1 dimension the kernel method interpolates according to a cubic spline.  Unfortunately , the result for the adaptive regime are not clear. Theorem 3 simply states some dynamics and then remarks below that if c^2>a^2+b^2 then the dynamics follow the reduced dynamics. However I do not see that they formally proved it follows an adaptive linear splines.   I have read the responses and the authors seem to address some of my comments. I'll keep my score at "marginally above"