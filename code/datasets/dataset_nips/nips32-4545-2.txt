 Although the result in this paper only applies to one-hidden-layer networks with one dimension input, it is an extremely important direction of understanding the difference between neural network learning and kernel learning. Especially under the current hype about neural tangent kernel (NTK).   The main concern for me about this work is the paper is written in a non-rigorous way, which makes it hard to understand the technical contribution of the work. For example, the main claim "in adaptive learning regime, the learning favors linear splines" (as in abstract) is never formally proved anywhere. The only Lemma in this adaptive regime is Lemma 1, however, the Lemma is more an "intuitive statement" than a formal theorem: the \rho in the Lemma is the \rho at some iteration, and it is not clear what is the picture about the full dynamic when all the iterations are taking into consideration. (for example, a point can be attractor in one iteration and repulsor for another). It is possible that in the long ran all these attractor/repulsors get canceled and the network does not adapt to the data eventually.  The smaller concern is that the paper is missing some proper citations about previous works in learning under over-parameterization, mostly the kernel regime:  Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data  A Convergence Theory for Deep Learning via Over-Parameterization  Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers   Especially the second work actually proves that the neural network learns the solution of the best kernel in the "kernel learning regime", even for deep networks. The authors cited some follow-up papers of these works but the original ones are not properly addressed.    After Rebuttal:  The authors have promised to improve their paper in the next revision. Their promises make sense and addressed my concerns. I am willing to higher my score. 