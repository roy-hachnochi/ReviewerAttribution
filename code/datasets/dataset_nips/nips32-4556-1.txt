Summary: The objective of this paper is to produce an RLfD algorithm that can both increase learning speed vs. sparse-rewards from scratch, and exceed expert performance.   The main assumption is that all expert trajectories go through the same sequence of abstract steps, which are internal to the expert and implicit in the demos.  By training classifiers to segment these trajectories into a sequence of monotonically increasing "goals", the goals can be used as sparse shaping cues.  Abstracting over states in this fashion prevents the shaping signal from fitting the idiosyncrasies of the expert, and allows the experimenter to control the granularity of expert's state-density distribution to follow during training.  The main contribution is an EM-style algorithm for learning goal labels from expert trajectories.  It works by alternating between (a) assigning a goal-label to all states in the dataset such that goals are monotonically increasing over the demos, and (b) fitting an n-way classifier to maximize log-prob of the labels. The authors also use an explicit anomaly detector to avoid shaping when far from the expert data, which showed a positive effect in their 2 most challenging tasks.  Comments: "252 The poor performance of the ValueReward and AggreVaTeD can be attributed to the imperfect value 253 function learned with a limited number of trajectories. Specifically, with an increase in the trajectory 254 length, the variations in cumulative reward in the initial set of states are quite high. This introduces a 255 considerable amount of error in the estimated value function in the initial states, which in turn traps 256 the agent in some local optima when such value functions are used to guide the learning process." > This argument should apply to the proposed method as well.  The proposed method is effectively learning a value-like-function on a discretized state-space, and using it in the same potential-based way the regular value function would be.  How do the authors explain the robustness to local-optima vs. the baseline methods?  These tasks are not especially complex, and I find it suspicious that the authors were unable to get VBRS and aggrevated to work at all.  "285 We also see that our method performs better than even the optimal expert (as it is only optimal w.r.t. some cost function) used in AntMaze" > Aren't you evaluating on that same cost function?  If so it sounds like an under-fitting issue,  and if not the difference in cost function should be discussed.  Fig 3) all baselines fail utterly in tasks 1 & 3, which is suspicious .  See comment below Fig 4a) why does performance drop as  nd 2->5?  Why not show n=1?  It seems that shaping may not be needed for this task.   Fig 4b) no effect of ng -> suggests shaping isn't really necessary here and that the main effect comes from pretraining. Both 4a and 4b makes a weak case for the proposed method, since the goal sparsity has little effect on learning speed or final performance.  "Although the equipartition labels in Eqn. 2 may have a many-to-one mapping from state to sub-goals, the learned network modeling πφ provides a one-to-one mapping."   > How can a mapping from Rn -> Nk be one-to-one?  Would be interesting to see if expert performance can be exceeded when training solely from expert demos, ie without any environment reward at all.  There’s reason to believe this is possible, but it wasn’t discussed explicitly   Final comment: The main approach to goal-labeling works by iteratively assigns labels to states in ascending order over trajectories, and training a classifier to minimize a cross-entropy loss with respect to these labels.  It seems the root issues are the need to preserve sequential ordering of class-labels over time, and the need to preserve the balance of goal-labels across the dataset.  The narrative and algorithm might benefit from expressing these criteria more explicitly, e.g. by using Ordinal-Regression to capture the sequentiality of discrete labels by construction in the model, and using a prior (e.g. uniform) or KL-regularizer on labels to preserve entropy.   This comment is merely a suggestion, and did not affect the review.