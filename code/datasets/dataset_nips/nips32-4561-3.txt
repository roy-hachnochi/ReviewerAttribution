The paper proposes to enable single-example normalization that, for each layer in expectation, is equivalent to batchnorm. For this purpose, the paper observes that the normalization statistics can be estimated as moving averages, and the gradients through them could be estimated as moving averages E[g] (g is the backprop input) for the mean subtraction and E[gx] for the divisive normalization. The proposed approach adds to this the division by the total norm of the activations (akin to layer normalization), ostensibly to combat the explosion / vanishing of activations. Positive results are shown on several image tasks with ResNets (for classification) and U-Net (segmentation), as well as RNN and LSTM on PTB.  Overall the paper makes sense, and the results are encouraging. However, there are several significant issues that I think make the paper less strong than it could be:  - All the image results are shown using models that, as has been shown, can be trained without any normalization. Specifically, the ResNet architecture has been hypothesized to enable better gradient propagation, could be initialized in a way that makes the normalization unnecessary, and by this token could be more forgiving to the choice of normalization scheme used. I think the results would be stronger if they also used deep non-residual architectures, such as Inception, to demonstrate that the proposed scheme performs well even in the absence of skip connections.  - The layer scaling step in Eqn. (9) means that the proposed architecture requires inference-time normalization (similar to LN and GN). I think this should be made more explicit. Regarding the reason for this step, the paper suggests that it is to combat the vanishing / exploding activations. I would like to see this statement substantiated. Specifically, I would like to know that indeed this is to bound the activations, rather than to e.g. reduce the bias / variance of gradients. For instance, could it be that the proposed scheme without the layer scaling would likely produce gradients w.r.t. the weights that are not orthogonal to the weights, and the layer scaling fixes the issue? A simple way to verify this would be to consider other activation-bounding techniques, such as clipping the activations.   Some more-minor comments:  - The paper claims to compute unbiased gradient estimates -- I am not sure that they are unbiased. The computation at any given layer may be unbiased, but the product of these may not be since the Jacobians of different layers are not independent.  - I did not understand the "freezing" part in lines 93-98. What is meant by this?  - In Eqns (11b) and (12b), is there a multiplier \alpha_g missing before the \epsilon on the RHS?  - Lines 205-207, I am not sure that the gradient scale invariance can be invoked here for the cases where the normalization occurs in one of several parallel branches (which happens in e.g. ResNets).   - Overall, the paper is rather clear. However, I did not find Figure 1 to contribute to this clarity -- I did not make a significant effort to understand it though, as I assume it illustrates what is already very clear in the text.  - The projections performed when backpropping through batchnorm are well known -- e.g. second half of sec. 3 of https://arxiv.org/pdf/1702.03275.pdf.  - I would love to see the optimal values of \alpha_f and \alpha_g for at least some of the tasks in the paper text, rather than the appendix.  Overall I think the paper as it is could be of value to practitioners, but it could be made more so.