This paper proposes to use stochastic logic to approximate the activation function of LSTM. Binarization of non-linear units in deep neural networks is an interesting topic that can be relevant for low-resources computing.   The main contribution of the paper was the application of stochastic logic to approximate activation functions(e.g. tanh). The authors applied the technique to a variant of LSTM model on PTB. Given that the technique is not really tied to the LSTM model, it would be more interesting to evaluate more model architectures(e.g. transformers), and compare them with the models that needs the non-stochastic logic versions. How would the approach compare to things like lookup table based the transformations? (given that we are already accumulating).  Given that the PTB is a small dataset, which makes the result favorable for compression, would the approach generalize to bigger dataset?(e.g. wikipedia?).   Direction of improvements: Having a dedicated evaluation of stochastic logic based activation will enhance the paper and allow the technique to be applied to a broader range of applications. From a practical point of view, implementing the method on a real hw(e.g. FPGA) will make the results more convincing, as the cost might goes beyond the number of gates.  Finally, a major part of the paper is about designing the FSM for the stochastic logic activation. While this could be a contribution of the paper, it might be less relevant to the NeurIPS audiences. 