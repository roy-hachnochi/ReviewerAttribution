[Originality] This paper proposes a novel method for learning unsupervised representation for molecules. This is critical because most of the molecule datasets are small. Learning an unsupervised representation allows the model to better generalize and potentially utilize unlabeled data in a semi-supervised setting. Currently there are few methods working on learning unsupervised molecular representation and therefore I think this paper is original.  [Quality]: The paper is technically sound. The paper provides theoretical analysis characterizing the model's representation power and generalization bound, which is important for understanding the model. It would be good to see the average sparsity of c(n) on some molecule datasets. The paper performed extensive empirical comparison against a wide range of baselines, therefore I believe the experimental results support the claims.  [Clarity]: The submission is mostly clear. Due to the space limit, the paper is very dense and most of the details are provided in the supplementary. If this paper gets accepted, I think the author should reorganize the paper properly to move some parts of the appendix into the main paper to improve its readability.  [Significance] As I mentioned, the paper conducted experiments on standard benchmarks and compared against many baselines. The results are significant and I believe this paper will encourage many researchers to design unsupervised / pretraining methods for molecules. As an extension, the authors can test the model in a semi-supervised scenario, using unlabeled molecules to derive the graph embeddings. Ideally the method should work even better when unlabeled molecules are incorporated.  =============================================== Upon reading other reviewer's comments, I found that MPNN baselines are missing on QM9 and delaney, which outperforms the proposed method. Despite that, I think the proposed method is still novel. Therefore, I am keeping the original score but lowering my confidence due to missing MPNN baseline.