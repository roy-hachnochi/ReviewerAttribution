*** Thanks. I've read the responses, which answered my questions. ***  This paper is clearly written and the results are insightful. The theoretical results rely on non-trivial combinations of several recent techniques. I enjoy reading the paper and appreciate the contribution. However, I must admit that I am not particularly familiar with the field on RL+LQR, so I cannot comment on the details of the derivations. Below are some questions I had after reading the paper.   1. Projection step  Algorithm 1 and 2 both use a Proj_\mu to project the Q matrix onto pd matrices. I am wondering if this is truly necessary or could potentially hurt performance. It seems from (2.10), we do not need Q to be pd. Though the optimal Q is pd, how do we estimate \mu in practice?  2. Adaptive algorithm  First, LSPlv2 is not clearly defined in the main text, though it can be understood from the appendix. My concern is that, it is unclear whether K^{i+1} will remain to be stable in this online setup. If not, wouldn't it violate the stable behavior policy assumption? The proof in appendix appear to rely on Assumption 1, which seems pretty strong, because wouldn't that imply the optimal policy is almost solved in one online iteration?  3. Practicality Though this paper is theoretically focused, but from a practical perspective, I am wondering if the model-free setting is a reasonable approach to LQR. This is not a criticism, but as the paper states, it has been shown model-based is superb both theoretically and empirically. I am guessing, when would model-free approaches start to shine. For example, would model-free approaches start to make more sense, when the state is high dimensional and without low-rank structure (or even infinite dimensional)? while the action space is low-dimensional. The experiments considered are fairly simple. I think having some motivating problems would help readers to better appreciate technical contributions here.  Minor:  In line 73, K_eval is not defined, though it can be understood as the feedback gain from the later context.   