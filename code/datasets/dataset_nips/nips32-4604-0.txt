**********After author response***************  I appreciated that the authors took the time to try to relax their noise assumption. The idea seems promising but I cannot change my score without seeing a real and complete proof. Yet the fact remains that, even if I will not fight for this paper, I think that it deserves a submission.   ***************************************************   Originality: the aim of the paper is very general but very important: how to accelerate SGD ? The algorithm is based on the accelerated gradient descent from Nesterov together with a multistage approach that does not seem new neither. Yet they reach optimal rates for the algorithm which is the principal aim of the paper.  Quality and clarity: The aim of the paper is clear and as robustness of the acceleration of gradient descent with respect to noise is important, the result stated is quite strong. Moreover, even if the topic is quite technical and several attempts in the past have been made, the topic is well reviewed and documented. However, the paper lacks from some clarity, especially in Sections 2 and 3, where this is rather a succession of lemmas and theorems than a clear discussion about the difference with the previous works or the method employed.   Significance: as I said in the previous Section (contribution), the result was already proven in more specific settings, and the significance of this paper may be more methodological than theoretical or experimental.