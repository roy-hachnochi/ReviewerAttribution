Originality: This paper provides a clear and deep analysis of a multi-stage accelerated SGD algorithm. The results show that the expected function value gap is bounded by an exponential decay term plus a sublinear decay term related to noise. They recover the deterministic case in the single stage and zero noise special case, while reaching the lower bound O(\sigma^2/n) in the noise term. The paper contains sufficient novel results and is competitive comparing with related work. In particular, the main results reveal how to choose the right time to switch from constant stepsize to decaying stepsize, a crucial choice for the overall performance of stochastic algorithms.   Quality/Clarity: The paper is written with care. The proofs are correct, clear and concise. The results are well presented and highlighted.  A detailed comparison is made with related papers. For example, the authors showed that AC-SA can be seen as M-SAG with specific varying stepsize rule. The technique of exponential decaying stepsize allows to avoid the requirement on the knowledge of \sigma.    Significance:  The paper brings new algorithm and new results in accelerated stochastic gradient methods. The results match the lower bound without requiring the knowledge of noise level and provide insight into parameter choice of the algorithm. The new algorithm also perform asymptotically the best in the reported experiments.  Comments: 1, Should the function value gap in 132 be changed to the Lyapunov function? Also, where is Corollary 3.2 needed in the whole paper? 2, Typo in Equation (32).   I read the response.