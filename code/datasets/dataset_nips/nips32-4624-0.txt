UPDATE: Authors answered my questions, I would like to keep my score unchanged and suggest to focus on clarity of the final version. Perhaps, this is the case when I would really be interested in looking at the source code.  Originality: the paper borrows the general idea of product keys from the database community, however the application to fast retrieval in neural memory systems seems quite novel to me.  Quality: The core ideas of the paper are sound, however more I would appreciate more rigor in both conceptual and experimental comparison with other approaches incorporating memory to Transformer (see e.g. [1]).  Another suggestion would be to discuss more the issue of potential non-uniformity of the query distribution, which indeed seems to be quite relevant. There is a recent work [2] that also uses distributed sparse memory and uses a seemingly more advanced way of improving on this than a simple batch norm.  Clarity: On this front the paper can be improved, and it is my main critique of the paper.   Authors do not explain clearly how are the keys and values computed. Presumably those are the same as in the original Transformer network, produced on the full sequence processed to the moment, but I could not find any confirmation of this.  Expressions such as "x = x + FFN(x)" (line 160) look a bit odd.  I find the modification of the multi-head attention (section 3.3) unusual and requiring some explanation. If output of the heads is summed and not concatenated, how does this modify the original Transformer?  Significance: I think it is a quite neat technique that is easy to implement and it will find applications in the community.  [1] Chen, Mia Xu, et al. "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation." Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018. [2] Rae, Jack, Sergey Bartunov, and Timothy Lillicrap. "Meta-Learning Neural Bloom Filters." International Conference on Machine Learning. 2019.