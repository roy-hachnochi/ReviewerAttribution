They introduce an algorithm for inferring "probabilistic deterministic finite automatons" from a probabilistic language model oracle that can compute P(next_symbol | string_so_far), which is exactly what a autoregressive neural language model, like a rnn, would give you. The paper is well-written, the problem is well motivated, and the algorithm appears to be better than the prior work, as measured by the experiments. However I am not qualified to adequately review this paper, because I'm not familiar with any of the related work. For this reason, I lean toward accepting the paper, but I have very little confidence in my assessment.  The only problems I have with this paper are with the experiments. They evaluate on SPICE data sets, which appear to be standard data sets for finite state language model learning. However, in looking at the cited SPICE paper, there are actually 16 SPICE data sets, and they only evaluate on 4 of them. Why only these four? Are the other twelve too hard? It would be okay if your algorithm cannot handle the other twelve, provided that the prior work also does poorly on this test cases.  Relatedly, what would happen if you apply your algorithm to a language model trained on a large natural language corpus? I imagine that this is an ultimate intended use case of distilling finite state machines from neural language models, so that for example you could use the (finite-state) language model on low power or embedded devices where floating-point operations are at a premium. In a similar vein, state-of-the-art language models for natural language are based on transformers, and as far as I can tell your algorithm would work equally well with a transformer model, or indeed any autoregressive model. This might be an interesting experiment to do, or at least mention as a possibility.  "Probabilistic deterministic" finite state machines sounds like an oxymoron, but really what is meant are probabilistic symbol emissions and deterministic state transitions. For readers like me that are not familiar with this literature, a quick sentence in the introduction clarifying this would be helpful (the third paragraph and lines 112-115 make the meaning clear, but it would be helpful if it came a bit earlier).  The equation on line 125 might have a typo in it: $\sigma$ appears as a free variable in the left-hand side, but does not occur in the right hand side. Did you mean $... = \frac{P^p(w\sigma)}{P^p(w)}$ ? This would make sense for calculating the probability of $\sigma$ following $w$.