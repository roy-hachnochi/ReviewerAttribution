The paper shows that the dynamics of gradient descent for optimizing an infinite width neural network can be explained by the first-order Taylor expansion of the network around its initial parameters. Furthermore, it shows that when the loss function is squared loss, the dynamics admits a closed-form solution. It also provides a learning rate threshold such that whenever the learner rate is smaller than that threshold, the trajectory of gradient descent is in a neighborhood of the trajectory of gradient descent on the linearized neural net, under the condition that the neural net is sufficiently wide. It also shows that the prediction of a neural network can be described by Gaussian Process when the width goes towards infinity.  The paper is written well and the insight is very interesting. I enjoy reading the paper and I also think the contributions might be significant.  Q1: The theorem requires that \Theta is full rank. Does it hold in practice? Is it a strong assumption? It looks like the authors did not check this assumption in the experiments.  Q2: (line S84) Can you explain how to use Theorem G.3 to get (S84)?  Typo: (S77) \Theta_t <- \hat{\Theta_t}  (S96) Should there be a distance factor \| \theta - \tilde{\theta} \| ?