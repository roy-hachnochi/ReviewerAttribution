This paper show that for the task of learning the best possible ReLU to fit some labeled data (whose unlabeled points are from a spherical Gaussian) (i) assuming hardness of the (well-studied) "Learning Parities with Noise" problem, no algorithm can have error OPT + o(1) (where OPT is the error of the best ReLU fit) in polynomial time (in the dimension n) (ui) unconditionally, no SQ (statistical query; includes in particular gradient descent and most practically used algorithms) algorithm can have error OPT + o(1) (where OPT is the error of the best ReLU fit) in polynomial time (in the dimension n)  The authors also show that, while OPT+o(1) is not achievable, it is possible to get error OPT^{2/3} + o(1) in polynomial time.   Both upper and lower bound proceed by (careful) reductions, one to the Parity with Noise problem, the other to the problem of (agnostic) learning linear threshold functions in the Gaussian space.   The paper is very clear, and though I am not an expert I found it interesting and well written. I recommend acceptance, modulo the few comments below.  ========== - ll. 38-41: this is confusing, as "Corollary 1" is not a corollary of Theorem 1. I strongly recommend you detail a bit these two lines to make this clear: Theorem 1 and Corollary 1 both rely on the same reduction, but Theorem 1 (for all algorithms) relies on Assumption 1 as base case, while Corollary 1 (for SQ algorithms only) is unconditional based on the hardness of learning parity with noise for SQ algorithms. - everywhere: "hermite" needs to be capitalized, it's a name - ll. 266-267: I didn't quite get the distinction and short discussion about biased ReLU. Can you clarify?   UPDATE: I have read the author's response, and am satisfied with it.