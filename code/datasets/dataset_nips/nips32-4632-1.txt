The paper investigates the relationship between BERT and syntactic structure. The idea is based on Manning paper as the author pointed out. The overall readability is OK. Here are some points the author could do better: 1. The visualization tool is useful. However, a comprehensive quantitative evidence would be more convincing. The figures shown in the paper (like parse tree embedding) are just representing very 1 or 2 instances. How does this idea apply to all sentences in the corpus? 2. The attention probe part (binary and multiclass) show some accuracy number. But are they good? There lacks comparison against using other features. 85.8% could be good in some binary classification tasks but very poor in others. So the authors need to establish this evidence. 3. Theorem 1 is interesting. But it only proves that for ONE tree, or ONE sentence, there's a power-2 embedding. This embedding will definitely be useless if you use the same words but in a different sentence syntax. How can you prove that for all sentences, there can be an approximately good power-2 embeddings, which is the case from Manning's result?