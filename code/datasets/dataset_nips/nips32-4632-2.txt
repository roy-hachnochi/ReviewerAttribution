Originality: This submission uses existing techniques to analyze how syntax and semantics are represented in BERT. The authors do a good job of contextualizing the work in terms of previous work, for instance similar analyses for other models (like Word2Vec). They also build off of the work of Hewitt and Manning and provide new theoretical justification for Hewitt and Manning’s empirical findings.  Quality: Their mathematical arguments are sound, but the authors could add more rigor to the conclusions they draw in the remarks following Theorem 1.  The empirical studies show some interesting results. In particular, many of the visualizations are quite nice. They provide convincing experimental results that syntactic information is encoded in the attention matrices of BERT and that the context embeddings well represent word sense.  However, some of the experiments seem incomplete relative to the conclusions they draw from them. For instance, the authors conjecture that syntax and semantics are encoded in complementary subspaces in the vector representations produced by BERT. There isn’t that much evidence to suggest this, except for their experiment showing that word sense information may be contained in a lower dimensional space. Further, in Section 4.1, the authors provide a visualization of word senses. In particular they note that within one of the clusters for the word “die” there is a type of quantitative scale. A further exploration of this directionality would have been interesting and more convincing.   Clarity: The paper is clear and very well written.  Significance:  While their mathematical justification is interesting in relation to previous work, it’s not particularly novel or interesting in and of itself. It leads to a better understanding of the geometry of BERT, and may provide inspiration for future work. The empirical findings are interesting. They are perhaps not particularly surprising, but to my knowledge no one has done this analysis before.