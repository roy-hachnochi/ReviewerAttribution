In general, I like the question this paper asked, i.e., whether or not it is necessary to impose a large deviation from the model parameters in order to attack distributed learning. Most of the research in Byzantine tolerant distributed learning, including Krum, Bulyan, and Trimmed Mean, uses some statistically "robust aggregation" instead of simple mean at the PS to mitigate the effects of adversaries. By the nature of robust statistics, all of those methods takes positive answer to the above question as granted, which serves as a cornerstone for their correctness. Thus, the fact that this paper gives a negative answer is inspiring and may force researchers to rethink about whether or not robust aggregation is  enough for Byzantine tolerant machine learning. However, the author seems not aware of DRACO (listed below), which is very different from the baselines considered in this paper.   L.Chen et al Draco: Byzantine-resilient distributed training via redundant gradients, ICML 2018.  The key property of DRACO is that it ensures black-box convergence, i.e., it does not assume anything about the attack Byzantine workers use to achieve convergence. Thus, the "common assumption" is not made by DRACO,  and it is NOT reasonable to claim " all existing defenses for distributed learning [ 5 , 10 , 27 , 28 ] work under the assumption".   While this paper's idea is creative, it does not seem to be fully developed. The proposed attack is shown to break quite some existing defense methods empirically. Yet, does it break DRACO? If not, does it mean that DRACO is "the right solution" to the proposed attack? More discussion is in need. In addition,  theoretically, when would the proposed attack break certain defense (depending on the parameters chosen, datasets, etc)? Is there a simple rule to decide how to break a certain defense? The experiments were also oversimplified. Only three models on small datasets with a fixed distributed system setup are far away from  enough to validate an attack empirically.   The main idea of this paper is clear, but the writing itself does not seem to be polished. A lot of typos exist, and some sentences are hard to understand. The use of math notations is messy, too. See below for a few examples.  1) Line 4, "are (a) omniscient (know the data of all other participants), and  (b) introduce ...": "are (a)" should be "(a) are"; 2) Line 10, "high enough even for simple models such as MNIST": What model is MNIST? To the best of my knowledge, MNIST is a dataset, NOT a model (such issues exist in the experimental section as well); 3) Line 45, " Likewise, all existing defenses for distributed learning [ 5 , 10 , 27 , 28 ] work under the assumption that changes which are upper-bounded by an order of the variance of the correct workers cannot satisfy a malicious objective": This is hard to understand. Rephrasing is needed. 4) Algorithm 3, Line 2: It is not clear what objective the optimization problem maximizes. My guess is that the authors want to maximize z, which should be written as $\max z s.t. \phi(z) \leq \frac{n- m-s}{n-m}$; 5) Line 326, "state of the art" -> state-of-the-art.  ================================================= Thank the authors for their explanation. 