Coding theory addresses both stochastic and adverarial noise. Minimum distance, one of the simplest and most familiar metrics of a code, measure the ability of a code to correct adversarial errors.  I don't understand the argument in Section 2.1 about the volume of the "uncertain region" in the space of logits. Why is the Euclidian volume relevant? Any continuous function with the same limiting behavior will have this behavior. The relevant way to measure the size of the uncertain region is using the distribution of the inputs to the softmax layer. If the variance of these inputs is too large then the outputs will be overconfident and if the variance is too small they will be underconfident. A persuasive argument that the shape of the softmax function is responsible for overconfident classification must be more nuanced than the one made here.  Section 2.2 describes a "correlation-based decoder". This is a logistic activation layer followed by a linear layed defined by the code followed by a ReLU layer followed by normalization to a probability distribution. Why were these two choices for the nonlinear activations made? Doesn't the ReLU throw away information that could be useful for calibrated confidence levels?  Why is the alpha hyperparameter introduced? It is not used in the experiments and the framework does not seem to depend on it.  In Section 2.3, the code described is equivalent to a repetition code. This is a trivial code. Why should it offer any benefit? The Hamming distance between codewords is increased, but on the other hand, the operator norm of the final layer is larger than that of an identity layer. This would seem to cancel out the benefit of the code. In general, the problem of adversarial examples exists even with only two classes, but there are no interesting codes in this case.  Theorem 1 is a variation on the well known Plotkin bound.