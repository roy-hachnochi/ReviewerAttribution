This paper deals with an important problem in theoretical reinforcement learning (RL), that is, finite-time analysis of on-policy RL algorithms such as SARSA. If the analysis techniques, as well as proofs, were correct and concrete, this work may have a broad impact on analyzing related stochastic approximation/RL algorithms.  Although important and interesting, the present submission contains several major concerns, that have limited the contributions and even brought into question the practical usefulness of the reported theoretical results. These concerns are listed as follows.  1. To facilitate analysis, a number of the assumptions adopted in this work are strong and impractical. For example, i) The projection step in (1) is impractical. In addition, if one notices, later in the proof of Lemma 2 (see eq. (27) in the Appendix), another assumption requires the norm of feature vectors to be less than 1. Making these two assumptions simultaneously, does not necessarily guarantee that the true Q-function can be approximated by the inner product of $\theta$ and $\phi$.   ii) Later, it is claimed that without the projection, the method is still useful, and one just needs to set $R$ to possibly a very large value; however, it can be seen from the right-hand side of equations (3), (4), and (11), that the second and third power of $R$ shows up through $G$, and thus the bounds will possibly be very loose.    iii) Besides the projection step to bound the gradient norm, another related concern was also raised by [Chen et. al. in Finite-Time Analysis of Q-Learning with Linear Function Approximation, arXiv:905.11425v1, May 2019]. It is claimed in [Chen etal'19] that the results (Theorem 1) of [F. S. Melo et. al. Analysis of reinforcement learning with function approximation, ICML2008], that this work builds on, cannot be verified. This concern further weakens the contributions of this work.   iv) Assumption 2 requires $C$ to be small enough to guarantee a negative eigenvalue. This assumption is vague, in the sense that there is no clear characterization of $w_s$, and in practice, how one can guarantee a "feasible" or "meaningful" policy for which it is possible to have such a small $C$.    2. It is also recommended to provide some numerical tests to verify the theoretical results. 