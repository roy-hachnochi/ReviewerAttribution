This is a very succinct paper and I think that it serves as a nice theoretical contribution. One exciting feature is that the reduction builds on the online boosting framework of Beygelzimer, Kale, and Luo - to my knowledge this is the first theory result that uses their technique as a subroutine. The reduction from oblivious to adversarial adversaries seems new as well.  One comment I have would be to spend a bit more time discussing *why* the techniques work. In particular, I found the result of Lemma 9 rather counterintuitive: It essentially says that if we run any pure DP pac learning algorithm a constant number of times on any arbitrary dataset, one of the outputs will have nontrivial classification accuracy on an unrelated target distribution with constant probability. If this hasn't been noted anywhere before it seems worth mentioning as a structural result.  Some misc typos/comments: * Proof of lemma 9: $m$ and $m_{0}$ seem to be confused in various places/not defined. * 173: Shouldn't this be $m(1/4, 1/4)$ rather than $m(1/4, 1/2)$? * between line 181 and 182: Should this be "with probability at least 1-1/4" rather than "with probability at least 1/4"? * Where is $\mathcal{H}(\mathcal{D},c)$ defined?