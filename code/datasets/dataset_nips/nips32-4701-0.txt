 [Updated after the author feedback]  =Summary= The goal of the paper is to devise a mechanism to predict wind speed based on the 2-second video snippets on a checkered flag. A resnet-18 is used as a feature extractor for each frame. The timeseries of features is then fed to an LSTM to predict the wind speed. A specialized dataset is also collected for the aforementioned task.  The paper attempts an interesting problem. The idea of using visual signals to predict wind speed is indeed quite clever. However, in the current state, the paper needs more work before it is ready for publication. On the plus side, the collected dataset is quite useful for the boarder ML community. On the negative side, the paper neither introduces a theoretical or a methodological novelty, nor does it show striking empirical evidence. The empirical results are somewhat underwhelming and certain choices and intuitions are not explained very well.  Overall, this is a promising research direction, and the authors are encouraged to pursue it by refining the writing and expanding the experimental setup. Please see the suggestions for improvements below.  =Originality= Medium. The paper applies well-known models to a specific application. The gathered dataset however can prove to be quite useful.  =Quality= Low. The model choices could be made in a more careful manner. For example, it is not clear why only recurrent networks would need to be used here. Please see detailed comments below.  =Clarity= Medium. The content is generally accessible.  =Significance= Low-Medium. The data collection and curation procedure used in the paper is quite thorough and could serve as a nice guideline for ML practitioners. However, on the negative side, the paper is very application specific and the results are somewhat underwhelming. Given the limited scope of the empirical validation, the paper is unlikely to have significant impact on the broader ML community.  = Specific comments and suggestions for improvement =  - The rebuttal was helpful in clarifying the operational mode of the proposed method, that is, once the method training is done, one can use the pre-existing structures e.g., trees and flags (as opposed to planting custom flags at each point of interest) to measure wind speed. This is a pretty clever idea and this reviewer highly recommends explicitly mentioning it in the intro. - In relation to the previous point, the performance on test sets (tunnel test set and adjacent flag test set) is a bit underwhelming. Looks like the method is overfitting to the main flag. Perhaps, it would help to have a separate validation flag and fine tune the model  hyperparameters (architecture, early stopping) based on performance on this separate validation flag?  Most importantly, it would be great to extend the experimental setup to other (non-checkered) flags/trees to empirically show that the idea has the promise to generalize. In summary, the paper can greatly benefit from a more comprehensive empirical section.  - The intro mentions that the paper leverages both "physics and machine learning to predict wind speeds". However, the method is completely data-driven (CNN + LSTM). - It is not clear why the LSTM needs to be used in this setting. It is true that LSTMs are the first model that one thinks of when modeling timeseries data, however, for data with fixed length and relatively small (30 timesteps) trajectories, a CNN might work equally well. Additionally, it seems that given the nature of the problem (motion of an object over time), it seems that the self-attention mechanisms might be well suited for this task (https://arxiv.org/pdf/1711.07971.pdf). Regarding the answer in rebuttal to this point, this reviewer is not convinced that once can use the LSTM trained on 15 fps to make predictions on 30 fps. - It section 3, it seems like bins containing disproportionately large data are down-sampled. Rather than discarding data, why not upsample the sparse bins? - While this reviewer is not an expert on wind speed prediction, it sounds like taking 1-minute speed averages might be too noisy, that is, wind speed might change a lot during this time. It might be possible to check this variance from the data. The question that remains after the rebuttal is that how precisely was the number of 1-minute chosen? How would the takeaways change if one considered 30-second averages? - How were the training and validation sets separated? If one considers a 1-minute interval, then there are thirty consecutive 2-second  intervals in this time period. However, all of these intervals might potentially have the same wind speed value (since it is a one minute average). Giving this overlap, would it make sense to split the training and validation data such that it does not overlap in these 1-minute intervals? - It would be great to mention if / how the analysis of 5.1.1 gets affected by the fact that the ground truth wind speeds considered in the paper are 1-minute averages.  