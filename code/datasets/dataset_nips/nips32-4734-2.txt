This paper provides a very compelling reframing of the common understanding that fairness and accuracy must be traded-off in any fairness-aware algorithm.  Instead, this paper convincingly argues that fairness and accuracy can be aligned depending on assumptions about skew in labels and training data selection.  These assumptions are clearly articulated, and experiments are provided for varying amounts of skew.  This first contribution - the reframing of the fairness / accuracy tradeoff - is a critical one for the fair-ML literature and has the potential to be highly impactful within the subfield, especially given the strength of the articulation of the problem in the first few sections of this paper.  Specifically, the paper argues that label bias and selection bias in the training set can lead to unfairness in a way that simultaneously decreases accuracy.  The second main contribution of the paper is a fair semi-supervised learning approach.  This approach builds on the discussion of the tradeoff by assuming that the labels should be distributed according to a “we’re all equal” assumption.  Given this assumption, the SSL technique is a fairly straightforward addition of a term to the loss function.  Experiments are then included based on synthetic data generated via a Bayesian network (see Supp Info) to allow for control of the amount of label and selection bias in the resulting data.  The resulting experiments show that in the case of label bias, the traditionally accepted fairness / accuracy tradeoff does not apply, and increasing fairness increases accuracy under the stated assumptions.  The semi-supervised approach does a good job of achieving both fairness and accuracy.