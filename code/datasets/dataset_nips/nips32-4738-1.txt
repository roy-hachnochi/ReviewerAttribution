Overall, the paper has some new nice ideas for fitting a parametric model when density may only be known in an unnormalized fashion. The idea of using Stein's method to estimate the ratio function is a neat one, as it skirts the need to constrain the integral of the ratio to be 1.  However, after reading the paper, I'm still unsure how much better this is than other methods people use to fit models when the normalizing constant is intractable. The experiments compared the new method to score matching and something called "KSD^2", but it didn't compare to other methods like Contrastive Divergence or Noise Contrastive Estimation. I would be more convinced if I could how much better DLE performed on more non-toy examples.  Another large question I had after reading this paper was how a practitioner could select the Stein features to use. On one hand, the paper states one must be careful from selecting too many features, as then the optimization might be too pessimistic when discriminating the data from the model. On the other hand, the paper also demonstrates that the variance of the parameter estimates achieve the Rao-Cramer lower bound when the score function is in the span of the Stein features. Given these conflicting forces, how does one go about choosing the Stein features? How does one even know if the parameters being estimated are close to the optimal parameters minimizing the KL-divergence of q and p?   L78: its actually the expectation of the log ratio. L98-99: Why does correlation complicate things? It doesn't appear any moment other than the first are used in the derivation of KLIEP. L179: I think the lambda is missing a subscript? Also what does it mean to hold "with high probability?" L243-250: This part is a bit confusing. Given we are trying to fit a parametric model, why would we be worried about picking up spurious patterns in the dataset? If its also a concern that we could pick too many features, how can one choose them in a principled way? This seems discordant with Corollary 4.  Originality: The paper has some nice novel ideas for estimating parameters in models where the normalization constant may not be known. Using Stein's method for estimating the ratio between p and q is a neat one and worth exploring more.  Quality: The paper appears to be correct. Some of the technical assumptions are quite specific and I imagine hard to check, but they do posit quantities that represent the asymptotic variance of their estimators.  Clarity: The paper is well written. There are some details left out (e.g. what's KSD^2?) and some of the bounds in Assumption 2 could be written a bit more cleanly. It would also be helpful to flush out in words what these assumptions mean conceptually.  Significance: The paper would be a nice addition to the set of approaches one can take for unnormalized density models. However, I'm a bit concerned on how one would apply these ideas in a general setting.  APPENDUM:  I would say that my original two critiques still stand. I think this paper has some interesting ideas [like using Stein's method for the density ratio estimation problem and also the asymptotic analysis accompanying their estimator] but I'm still not sure how useful this estimator is in the wild. In the author rebuttal, they say  """ Contrastive divergence was commonly used for restricted Boltzman Machine (RBM) where Gibbs sampling can be efficiently done. However, we consider a much wider family of models. In our MNIST experiment, the model is more omplicated than an RBM and Gibbs sampling is hard.  """  It's really not difficult to wire up an RBM to the MNIST dataset, in fact, it's a natural model to try. Hence I'm not convinced by their argument and think their paper would greatly benefit from a comparison. I've personally tried to use Stein's method for parameter estimation and found it harder to beat out other methods like NCE or Contrastive divergence.  I also didn't get exactly the answer I was looking for regarding the choice of Stein features. I understand they have worked out a criteria for selecting them from a set, but I was curious how they choice the basis functions in the first place. Are they polynomials of varying degrees? Do they need to be centered near the mode of the distribution? I think there's more in the details they haven't flushed out in this paper.  So overall, I'm still a weak reject, as I find the empirical results a bit lacking.