For complicated probabilistic models such as Bayesian neural networks, approximate inference is inevitable and theoretical understanding of the ramification when using it in reinforcement learning are unknown. Hence this is a very relevant and interesting question.  One should also point out that the recent approach of garbage-in reward-out [Kveton (2019)] solves the issue in a different manner and is clearly related to the problem authors are trying to tackle.   The paper stylistics can be significantly improved. It contains duplicate sentences such as in 156-157 and includes repetitive paragraphs such as 129-143 and 194-222.   The major issue I have with this paper is that as far as I understand the mapping f in e.g. Theorem 1 is the approximate inference mapping. i.e. it takes the true posterior to the approximate posterior. I do not see how existence of a bad mapping can imply that no regret is never possible. An interesting results would point out that no mapping that corresponding to approximate inference scheme can lead to no regret with the example given. The statement, as given, in my opinion cannot imply the conclusion made in the statement.   Also, the posterior approximate might be close in different alphas. So this should be specified in the statement in the theorems.   Simulations argue in favour of the authors argument and are nice but their presentation can be vastly improved by making them more concise.   I believe that this work is a nice direction but requires significant reworking and better explanation of the theoretical statements if authors believe they are correct.   Upon feedback I realize the authors aim is to show a worst case example is possible and sufficiently close in one metric is not enough to guarantee a no-regret algorithm. I believe these results are interesting for the research community but I still maintain the paper needs significant polishing.  