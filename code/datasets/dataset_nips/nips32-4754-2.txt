Detailed comments: The paper considers policy learning for cost-sensitive classification and contextual bandits problems. The focus is on arguing that directly minimizing the empirical risk is difficult if the policy takes the specific form of \pi(x) = f(q(x)) where x is the context, q an unconstrained model and f the softmax function. To reduce the difficulty, surrogate losses are designed which have the “calibrated” property, which means efficient minimization of the surrogate functions implies efficient minimization of the risk. In the cost-sensitive case, the surrogate is convex.  Originality: to the best of my knowledge, the two surrogate functions are original. They connect the cost-sensitive classification and contextual bandit which provide insights to both classes of problems.   Theorem 1 is a nice result extending the understanding of the difficulty of optimizing empirical risk.  Quality: The main contribution of this paper is theoretical and it is of high quality. One technical question is what the role of the baseline v plays in the theoretical result. It is shown empirically beneficial to have such a shift, could the authors provide some theoretical intuition on why this is the case? Empirically, how is v chosen?  Clarity: The paper is mostly clear to read. One thing that can be improved is that the F function, first appeared in Proposition 2, is not defined. Later it is defined in the context for KL divergence but it is not clear whether in Proposition 2 it refers to the KL divergence version or not.  Significance: I think the contribution is significant to the community which suggests new surrogate objective for both cost-sensitive classification and contextual bandits.   Minor comments: Equation (8) has a typo, the middle expression, inside the parenthesis, should be a minus sign instead of a plus sign.  Line 127: \hat{R}(\pi) does not yield the highest test error in MNIST. Am I missing something here? 