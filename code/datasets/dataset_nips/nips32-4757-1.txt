This paper proposes a mixture of asymmetric laplacians (ALD) to model the distribution of an output variable for regression in deep learning.  > The paper lacks clarity which is not helped by the dispersed typographic errors. The overuse of "uncountable mixtures" to refer to the simple process of marginalizing a 1D random variable, which is fed as input to a quantile regression network is confusing.  > This work lacks novelty as the only contribution is quite simply a Monte Carlo average of a fixed number of ALDs.  > The evaluation of the uncertainty measures crucially omits the concept of calibration and respective metrics.  > This work is missing standard regression benchmarks such as the UCI datasets [1] or the benchmarks used in [2] (as well as prior work). I believe that the expressivity of this framework should allow for adequate performance on homogeneous distributions just as well as heterogeneous    [1] Asuncion, Arthur, and David Newman. "UCI machine learning repository." (2007).  [2] Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. "Simple and scalable predictive uncertainty estimation using deep ensembles." Advances in Neural Information Processing Systems. 2017.  ============  Having read the rebuttal, I appreciate the extensive experiments that empirically establish the calibration of the uncertainty measurement.   Therefore, I updated my score but kept my comments as is for completeness. 