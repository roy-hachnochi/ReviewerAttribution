This paper introduces a new approach for Neural Architecture Search by utilizing Gumbel-Softmax operator as a trick to overcome the problem of selecting operators for edges which is discrete and a challenge for gradient descent. The authors justify their proposed approach by saying that the existing approaches do not use the exact architecture that is optimized in validation. Given that each edge might need to use more than a single operator, the authors introduce the idea of ensemble Gumbel-Softmax that allows their framework to be used when multiple operators are required for each edge. The paper is well written, easy to follow and the experimental study shows that their framework performs better than many other state of the arts baselines.    One major problem that I have with this work and its motivation is that I am not sure why the operations used in each edge should be mutually exclusive. In general, I guess the gradient descent can determine what combination of operators should be utilized for the best performance. If there is an operator that is not good for an edge, its weight is going to decrease in the optimization. So, not sure what is the practical reason for using one-hot vector  after optimization for methods that do so ([33] and [52]).  So I believe methods in [33] and [52] can be used without using the one-hot vector step. As mentioned in these papers, they have also used the top-k strongest operations (not only single one). So the justification why the current method is better does is not strong enough even though the authors are showing that their introduced framework performs better and faster.   