Quality The paper is thorough in describing the method and supporting the proposed method with experiments  Clarity The paper is well written and easy to follow.  Originality & Significance Although the method is not very novel in light of Paper 1253: RecreateGAN (see more below), the experimental exploration of different settings of the method is thoroughly done and interesting. The idea of matching local image features to word-level embeddings and matching global image features to sentence level embeddings is intuitive and makes sense.    This paper shares significant parts of the method with Paper 1253: RecreateGAN, in particular, the textual-visual embedding loss in (6) of this paper matches the pairwise loss defined in eq (5) of the other paper.  However, this paper uses this component as part of a different method, namely for textual-visual embedding vs an image similarity embedding.   Additionally, the cascade of attentional generators in this papers is very similar between both papers.  Although both papers can be seen as methods to translate something to an image (in this case text, in the other paper, an image) using similar embedding methods to condition a cascade of attentional generators, the details of the methods and tasks still have quite a bit of differences between each other, and it would not be possible to explain both systems in one 8-page paper.  I find the text-to-image paper more compelling, as the task itself makes more sense to me, and the results are state of the art.  However, as there are similarities, I am repeating some comments from my review of the other paper here: 1. The rationale for the attention-weighted similarity between two vectors is unclear to me.  Since all the loss in (6) is symmetric in w and l (and also symmetric in s and g), why not use a symmetric similarity measure? I would also appreciate an exploration of the effect of using a cosine similarity here.  2. I don't understand why the probability of w being matched by l is calculated over the minibatch in the formulation of (3).  Why is it desirable to have p(l | w) depend on the other members of the batch? 