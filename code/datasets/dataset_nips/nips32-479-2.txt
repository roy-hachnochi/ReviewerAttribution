his paper presents a novel text to image (T2I) generation method called LeciaGAN which is inspired by the process humans follow to achieve the same goal. These processes are modelled by decomposing the required tasks into three phases:  the knowledge learning phase, the imagination phase, and the creation phase. Semantic consistency and visual realism is achieved by using adversarial learning and cascaded attentive generator.   The paper provides a good overview of the literature in the related works, and provides a good motivation for the proposed algorithm. Despite the many concepts required to clearly describe the model in few pages, the well-thought structure of the paper makes it easy to follow, while sufficient description is given to highlight the necessary elements.  The metric used to (Inception Score and R-precision for objective measures, and subjective visual measures and human perceptual testing) is sufficient to establish the performance of the proposed algorithms as well as to compare it with already existing methods. I am quite satisfied and impressed by the result obtained and the method used to get there. I believe this paper has a significant contribution to the T2I field.  Some minor comments are as follows: * The descriptions in Text-Image Encoder paragraph in section 3.1, line number 123, are slightly cryptic. I think rewriting it to clearly present the pairwise similarity part would greatly help the readability of this section.    * Equation 7, just below line 141, the second term in the right side of the equation should be log(1-D_modal(s_n)) 