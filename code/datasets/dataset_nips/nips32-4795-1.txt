The idea of learning prototypes to improve interpretation is interesting and the comparison with prototypical cases provides clear explanation of the classification result. The presentation of the paper is easy to follow. However, I still have some concerns/questions regarding the method and the experiments.  1. The choice of prototypes P seems essential for the performance. It would better if some more implementation details can be provided. For example, 1) how are P initialized? 2) How to choose H_1 and W_1, since the similarity is based on the L2 distance, the size of the prototypes may be important. 3) Is the update of P (between line 183 and 184) performed every iteration? 4) How to choose m_k?  I believe some of the above question might be answered by carefully checking the code provided by the authors. Yet I think it would be helpful to summarize these in the paper, maybe using an algorithm summarization, for readers to get a better overview.  2. The comparison between training latent patches and the prototypes, as well as the update of prototypes may increase the computational cost since the algorithm needs to go over all possible patches. What is the time complexity?  3. The interpretation results seem a bit weak to me, as only Figure 4 provides some comparison with related interpretation methods on one example.  ===========================  I like the idea of this paper, and the author response has addressed my concerns in complexity, and comparison with other methods. I have changed my score to 6 accordingly.