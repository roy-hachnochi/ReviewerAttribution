- The paper combines the idea of Adaptive Computation Time (ACT) and multi-head attention build an attention mechanism called Adaptive Attention Time (AAT). Although the two techniques have been well explored individually, this is the first work combining it for attention for image captioning.  - The paper is also clearly written with explanation of all the hyper-parameters used in the paper. This should make reproducing the results easier. - It is not clear what is the contribution of AAT compared to multi-head attention. The base attention model already is doing much better than up-down attention and recent methods like GCN-LSTM and so it’s not clear where the gains are coming from. It’d be good to see AAT applied to traditional single-head attention instead of multi-head attention to convincingly show that AAT helps.  - More analysis is required to find the reason for improvement from recurrent attention model to adaptive attention model. For instance, how does the attention time steps vary with word position in the caption? Does this number change significantly after self-critical training?  - How much does the attention change over multiple attention steps for each word position? From the qualitative results in the supplementary, it’s not clear, how is the attention changing from one attention step to another.  - Is it the case that self-critical training is necessary to fully utilize the potential of AAT. The gains when trained just using Cross-Entropy Loss are minimal. Even for self-critical training, the gains in other metrics (SPICE, METEOR ) are minimal.   Minor Comments:  - In Line 32, the paper says that words at early decoding steps have little access to image information. Why is this the case for traditional models? Doesn’t every time step have access to the same set of visual features?  - Are the ablations in Table 1 done on the same split as Table 2?   Update [Post Rebuttal]: The authors addressed several of my concerns through experiments and answered many of my questions in the rebuttal. They showed that AAT helps even in case of single-head attention; that self-critical training is required to fully optimize the potential of AAT; Fixing attention steps introduces redundant or even misleading information since not all words require visual clues and that increasing the min number of steps reduces performance supporting the claim that adaptive attention time works better than recurrent attention. In light of the rebuttal, I am increasing my rating to 7. 