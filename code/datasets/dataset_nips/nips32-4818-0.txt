Many studies have looked at the ideas of physics simulation as a cognitive model. In such works, physics engines are usually employed as a model of human cognition of physical tasks, with the perception part of the task is often abstracted away. In parallel, data driven model have been frequently used to learn to parse raw visual inputs to detect or locate objects, frequently without using any explicit model of the physical world. This paper tries to bridge these two fields to build a complete model of how humans perceive certain physical scenarios, from raw pixels to expectations over objects. Whereas all of the parts employed in the proposed "pipeline" are based on previous works, their arrangement into this contiguous framework is new, as is the human and modeled results on the new dataset the authors also present.   The paper is clear and well written in general. However, I do think it would benefit by making its goal clearer. Is the purpose of the proposed framework to be a model of human cognition (for example, the results with ellipsoids might suggest that humans might be performing some sort of simple shape processing at early stages of visual processing)? Or is it to propose a possible new route for improvements in computer vision? (And if so, what is the path the authors see for this type of architecture to contribute to better visual models? Or is surprise detection intrinsically valuable?)   Some further clarifications that would be useful: - How fast does the whole pipeline run? How does it compare to the baselines used? - Are the results in Figure 5 different if you calculate accuracy per-person and then average the accuracies? (Instead of averaging scores across people and then calculating accuracies.) - Are there qualitative visualizations (as in Figure 4) for the predictions of the baseline models? Could be interesting in the supplement, if the results are not completely trivial - The GAN model uses a surprise score based on the discriminator. Are the results similar if a L2 loss like the one for the LSTM is used? - Isn't it a bit ad hoc that the values of the surprise for the "rare" events are hardcoded at values seemingly unrelated to their already hand-picked probabilities?(Supplement line 113, r values.) Any specific reason for these values?  In sum, this paper presents an interesting combination of object detection with data-driven models, and probabilistic dynamics modeling to construct a complete pipeline for implausibility detection. Though the type of tasks employed are somewhat specific, and modeling surprise directly does not seem to have immediate practical applications, the framework works points in an interesting direction of more integrated modeling and presents interesting comparison to human results. The paper would probably benefit from more explicit discussion of its modeling results in comparison to the human data, and the possible inferences about human cognition.