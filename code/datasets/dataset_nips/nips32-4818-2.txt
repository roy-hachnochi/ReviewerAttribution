I think this is a high-quality paper. The model proposed is fairly straightforward, and it is unclear whether any decision made represents a particular engineering contribution in which the model was refined by tests on these datasets, but it is cognitively-motivated at many steps. I really appreciate the train-test split design meant to better mimic human subject experiments -- pushing past standards of how we validate in machine learning is a very useful thing to do. The baselines are useful comparisons and I have no sense that these are weak. I do wish that I had a better sense of whether the surprise metrics on the baselines are reasonable -- looking at the supplementary, it was less than clear for me whether there might be some fairer surprise comparison for any of these.  The human subject comparison is careful and a very welcome contribution.  From an originality standpoint, the model builds on a variety of object-centric forward models, but it is clearly different than these in a way that lends it to the sorts of expectation violation experiments that it tests. I should emphasize that I consider this train-test split design to be quite novel as well -- some recent works have moved towards a more dev psych=inspired train/test split, but it is far from the norm and I think a very welcome addition.  The paper is very clear, with very good structure that allows readers to delve into details at different levels. I quickly knew where everything was and could refer back quickly when I needed to.  Put together, I think this is a contribution of some significance. Object-centric representations have matured over the past several years, but this is the first example to my knowledge that actually follows through with an expectation-violation comparison test (with a great train-test split!) like those that have inspired these sorts of models. It contributes to this virtuous cycle between AI and dev psych in a clear way.