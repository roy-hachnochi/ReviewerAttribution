Under the periodic and simultaneous restarting assumption, the Thompson sampling algorithm is quite straightforward. The significance is in the analysis of its regret. Allowing general benchmark policies in the regret analysis is a strength of the result. The writing is clear.   My main concerns are as follows: 1. The simultaneous and periodic restart of all arms is rather limiting. Any motivating applications that give rise to this model?  2. Under such a restarting assumption, the problem is essentially reduced from a restless bandit problem to a problem with i.i.d. processes by viewing each epoch as a super-time-instant. The authors should discuss this view of the problem and its implications in regret analysis (e.g., difficulties in mapping analysis of Thompson sampling in i.i.d. reward processes to this setting under this view of i.i.d. super-time-instants, and how different the current analysis is from the existing ones on Thompson sampling for i.i.d. rewards).  3. The abstract and the introduction, especially the presentation and comparison with existing literature on restless bandits with unknown model, do not give a complete picture. The restrictive assumption of restarting (which also significantly simplifies the regret analysis) was not mentioned. Note that the work by Liu, et. al and by Tekin and Liu allow general Markov chain rather than two-state and do not need restarting assumption. The work by Dai, et. al, although require stochastically identical two-state arms, does not rely on the restarting assumption, either. A more complete picture of the result should be reflected in the title, abstract, and introduction. All these results, including this paper, make restrictive assumptions, largely due to the difficulty of the problem. They are just limited in different aspects.   3. Under known transition probabilities, with the restarting, the problem is a finite-horizon restless bandit problem with a horizon of length L (the period of restarting). In this case, the optimal policy is in general time-dependent. A more clear discussion of this issue and its ramifications in the regret analysis when the benchmark policy is the optimal policy will provide more insight and enhance the paper.  