(1) I have a serious concern about the parameter alpha: In Theorem 1, the prediction error delta needs to be larger than alpha^4. This means that if we require a high accuracy prediction, i.e., delta is small, alpha also needs to be small. Increasing the sample size/number of iterations T does not reduce th error.  In another word, if we require the learnt network to be consistent, i.e., risk->0, then the composite signal in (2.2) needs to diminish. The scaling alpha->0 makes this regime not very reasonable. In contrast, if we consider alpha to be small fix constant, Theorem 1 will not give a meaningful bound.  (2) The authors should highlight the difference  between this paper and Allen-Zhu et al. 2018 for analyzing feedforward NN in terms of the proof techniques, as a large proportion of the proof technique in this paper are adapted from Allen-Zhu et al. 2018. 