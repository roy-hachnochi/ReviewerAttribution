This paper replaces the iterative routing (cluster finding) used in EMCapsules with a transformer-like attention. Since there is no notion of cluster finding and it is similar to transformers, the units do not have organic existances and the capsules only have instantiation parameters. Therefore, to sparsify the network authors design a binary decision block which prunes the connections (existance of the connections) in addition to the attention. For the classification, they improvise a notion of capsule existence by summing sigmoids over elements of its instantiation matrix.  This is an original work which proposes a network for computer vision with grouped activations. Alike CapsuleNetworks, in each layer there are groups of neurons that cooperate with each other to explain incoming signals. In this work in each layer groups of neurons compete with each other for sending their signals (equation 4, sum over i) where as in Capsule Networks capsules compete with each other for receiving a signal (sum over j). The idea behind Capsule Networks is that each unit has a single parent, therefore the normalization should be over j which automatically sparsifies the network. Because some of the upper capsules may receive 0 signal. In the current proposed work a single capsule in layer L can activate all of the capsules in layer L+1 by sending its vote to all of them.   This is an interesting work which is deviates from EMCapsules in several ways. Such as matrix multiplying the weights and poses inspired by affine transformations between part-wholes in EMCapsnet is here replaced with element wise multiply. Expanding on the reasoning behind this change would improve the manuscript. Also finding strong agreements by finding clusters in all bottom groups of EMCapsnet, is here replaced with a pairwise top-down agreement. In the experiments they show that the new proposed model have better accuracy in compare to EMCapsNet and their viewpoint generalization ability does not deteriorate. Expanding on the viewpoint generalization experiments will make this submission stronger. Generalization on rotation of smallNORB and affnist especially.  Room for improvement: The importance of each novel component is not explained, or explored. An ablation study which experiments without the straight-through router would clarify the role of the attention module vs the connection sparsifier. Furthermore, an ablation which does normalization on j rather than i (like EMCapsnet) would make StarCaps closer to EMCaps and therefore more comparable. The role of the specific attention module (with several conv layers) vs a simple linear layer is not clear.   Line 264-280: is inaccurate. EMCapsnet has two threshold beta_a and beta_u which threshold the activation of a capsule based on std and support (how many inputs are assigned) of the cluster. If std of clusters are larger than beta_u or number of capsules is smaller than beta_a the capsule will deactivate. These are proportional to the std of weights and total number of available points (number of capsules in previous layer). The initialization values in Hinton 2018 are for 32 capsules. Changing the initial value proportional to the change of the number of capsules controls the sparsity of EMCapsnet layers. Also in Table 1, authors can report proportion of active capsules, or proportion of routing factors above .5 after 3 routing iteration for EMCapsNet as well. Currently the vanishing pose argument is only backed by the drop in accuracy which given Fig. 2 is expected anyway.  ************************************************* Thank you for providing the ablation study and affnist results. The performance gain with the lack of single parent assumption is interesting. I am increasing my score to 7. I would still urge the authors to provide the smallNORB rotation generalizability as well. Providing the results only on Azimuth makes the submission weaker.