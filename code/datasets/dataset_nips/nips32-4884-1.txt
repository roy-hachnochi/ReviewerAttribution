The analysis shows that the error of RF is always bounded away from zero unless the number of neural N goes to infinity. Both NT and NN achieve zero error if N is greater than or equal to the dimension d. It also shows that NN always achieves smaller errors than NT, because NN learns to fit the most significant direction, while NT can only fit the sub-space that is spanned by random directions.  The results of the paper is quite intuitive, but non-trivial in my perspective. It provides a clear evidence that even for simple target functions, the neural network can hold advantage over random features models. This fact is often easy to be ignored, because for many simple tasks, random features work as well as the neural networks. I think the paper can become a reference point for future work when people want to talk about MLP versus random features.  The comparison between the RF and the NT is not very meaningful. It is true that NT can achieve a zero error with a finite number of neurons while RF cannot, but that only holds for specific target functions (quadratic and mixture of Gaussians), not to mention that NT has much more parameters to learn than RF. That said, the comparison between RF/NT and NN is the main contribution of this paper.