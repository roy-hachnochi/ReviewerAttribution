I enjoyed reading the paper so much. It was well written.  As the author states, the paper presents four original contributions. MMD DRO, its generalization bound, its tractable approximation, and new regularizer for kernel ridge regression. They are all indeed novel and beyond just a combination of existing techniques. The paper would be a food for thought for many researchers.  A concern I have is that the approximation of the bound for a general learning problem might lead to a population distribution not included in the uncertain set. Therefore, to my understanding, the proposed approximation shares the weak point of phi-divergence approach. If so, shouldn't it be stated more clearly?   --------------------------after rebuttal----------------------------------------------------- The authors claim that the MMD DRO solves the problem of the uncertainty set, e.g. line 40. However, at this point it is not practical since the tractable exact reformulation is not presented. Instead only the discrete approximation of the MMD DRO is introduced in Section 5, which might lead to a population distribution not included in the uncertain set. So I personally feel that the authors exaggerated their contribution in the Introduction slightly. I recommend the authors to discuss the limitation of the paper more candidly in the camera ready. 