*********** update after author feedback ***************** The improvements the authors note sound great and I hope this can improve the impact of the paper significantly. I would give an accept score if I were able to have a look at the new version and be happy with it (as is possible in openreview settings for example). However since improving the presentation usually takes a lot of work and it is not possible for me to verify in which way the improvements have actually been implemented, I will bump it to a 5. I do think readability and clarity is key for impact as written in my review, which is the main reason I gave a much lower score than other reviewers, some of whom have worked on exactly this intersection of algebra and G-CNNs themselves and provided valuable feedback on the content from an expert's perspective.   The following comments are based on the reviewer's personal definition of clarity and good quality of presentation: that most of the times when following the paper from start to end it is clear to the reader why each paragraph is written and how it links to the objective of the main results of the paper, here claimed e.g. in the last sentence to be the development of new equivariant network architectures.    The paper is one long lead-up of three pages of definitions of mathematical terms and symbols to the theorems in section 6 on equivariant kernels which represent the core results of the paper. In general, I appreciate rigorous frameworks which generalize existing methods, especially if they provide insight and enable the design of an arbitrary new instance that fits in the framework (in this case transformations on arbitrary fields). However that being said, my main concern with this paper is that I'm not sure whether the latter is actually achieved because of how the paper is presented in the current state. They describe G-CNNs in the language of field theory which is nice, but they do not elaborate on the exact impact of this, albeit interesting, achievement. Furthermore given my personal definition above, the quality in presentation is severely lacking and my main criticism about the paper.  As an ML conference paper, the presentation is not transparent enough 1. for the average researcher to understand the framework to see for which equivariances they could use it 2. to see how exactly they can now design the convolutional layer in their specific instance.  Now the authors could aim for the properly trained and interested mathematicians/physicists only - then, however I'm not sure why pages 3-5 of definitions are at such a prominent place in the paper. It is background knowledge that either the trained reader knows already or won't understand in the condensed way it is presented here. In particular, the definitions are given without the reader knowing why and how to connect it the familiar machine learning models/concepts. If they are experts, then these pages occlude the main take-away of the paper for the connection of these field theoretic concepts to practical G-CNN networks, which is discussed very little on the last page. Since the paper introduces everything starting with symmetry groups (admittedly basic concept) however, it does seems to aim to reach someone not too familiar with group theory. However it is inconceivable to me how they would be able to follow through all the way until induced representations and equivariant kernels in a reasonable amount of time, without actually properly learning the background by working through chapters of a book.