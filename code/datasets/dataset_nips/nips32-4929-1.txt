This study analyses overfitting in machine learning based on a meta-analysis of over hundred Kaggle competitions that comprise a variety of classification tasks. Kaggle provides a leave-out test data set, which is used to publicly rank competing methods; the competing methods can aim to improve their public ranking by improving their method. This can potentially lead to overfitting. The degree of that overfitting can be evaluated since Kaggle also maintains additional leave-out data sets that are used to provide a final ranking for the competing methods. The results indicate that the degree of overfitting is relatively low, and the authors' interpretation is that this demonstrates the robustness of cross-validation as a method development technique.  In terms of the strengths and weaknesses   The overall Quality of the work is high. Whereas the idea and implementation are relatively straightforward at a conceptual level, this work contributes new empirical information on overfitting, a fundamental subject in machine learning. If there are shortcomings, the limitation of the study only to classification tasks is one; but expanding the present work to other machine learning tasks could be relatively straightforward; the paper introduces the new concept, and the clarity of presentation does benefit from a well-defined scope on classification tasks. Source code is also provided, further supporting the overall quality (transparency, access) of the work.  Clarity. The study design and presentation are clear, and the implementation is rigorous. Sufficient empirical analysis and discussion of related work are provided, and potential extensions are discussed (other competition platforms; other modeling tasks; scoring methods to assess overfitting).  Originality. The large-scale analysis of overfitting in machine learning studies, implemented based on public competition platforms, seems to be a new idea. The paper also includes interesting reflections on the types of competitions and data set qualities (money prices, data set size..) and how these are reflected in overfitting. The work does not include substantially new theoretical or methodological ideas; the originality is mainly empirical.  Significance. The study lays a groundwork for extended analysis of overfitting of different types of machine learning models. It has implications for better understanding of the fundamentals in the field. The weakness is that the improved understanding does not readily translate to pragmatic recommendations for analysis, besides bringing increased confidence in cross-validation as a model training technique.  A paper with a similar title has been presented in ICML2019 workshop (https://sites.google.com/view/icml2019-generalization/schedule). The content is not the same but there seems to be notable overlap, and the ICML2019 paper is not cited in the present submission. It would help if the authors can clarify what are the main new contributions of this submission with respect to the ICML2019 workshop paper.