This paper studies a matching problem on correlated random graphs. In this model, a Erdos-Renyi graph $Gt$ is first sampled. Then two sugbraphs are sampled from $G$ (with vertex labels shuffled). This paper considers two problem:   (i) hypothesis testing: in H_0, two independent Erdos-Renyi graphs are given; in H_1, two graphs are sampled from the correlated random graph model. The goal is to design a randomized algorithm to determine whether we are in H_0 and H_1.   (ii) matching: given two graphs sampled from the correlated random graph model, design an algorithm to match the nodes from two subgraphs.   I find the problem well motivated. First, this problem is related to the graph isomorphism problem, which is notoriously difficult in the worst case. Thus, it is natural and important to study "average case" behaviors (i.e., design algorithms under random graph models). Second, the authors mentioned a number of settings (i.e., de-anonymize social graphs), in which the new algorithmic techniques developed in the papers can be potentially applicable.  The new algorithmic techniques developed by the authors are also very interesting. As mentioned by the authors, "simple statistics" (e.g., degree distribution, eigenvalues/eigenvectors) of a random graph is usually highly concentrated so they are unsuitable for hypothesis testing. The authors propose that they can design a family of subgraphs so that these subgraphs are unlikely to be encountered in a random graph (and even less likely to appear in two independent random graphs). Then by counting the numbers of these subgraphs in the input graphs, we can determine whether H_0 needs to be rejected.   The technique can be generalized to design a matching algorithm.   Designing the family of subgraphs for counting seems to be highly non-trivial for a number of reasons: (i) the graphs need to be "unnatural" so that they appear with low probability; indeed, the authors find an interesting way to build paths on top of random graphs to make them "unnatural"; and (ii) the size of these subgraphs need to be a constant (to make sure the running time is polynomial). It is usually harder to manipulate constant size random graphs to get "high probability" results.   In summary, I find this paper studies a very interesting theoretical model and proposes a new algorithmic technique that can potentially have practical impact. It is a typical theory paper in first tier ML conferences such as Neurips/COLT. 