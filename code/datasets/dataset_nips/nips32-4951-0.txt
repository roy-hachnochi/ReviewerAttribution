Author feedback: I thank the authors for the feedback. The feedback was of high quality and satisfied my concerns. I suggest that, perhaps a compressed version, of "Explaining limitations of our work" from the author feedback, which I enjoyed reading, will be added to the final version of the paper.   The paper "Sampling Networks and Aggregate Simulation for Online POMDP Planning" proposes a new solution to computing policies for large POMDP problems that is based on factorizing the belief distribution using a mean field approximation during planning and execution and extending aggregate simulation to POMDPs. In short, the proposed POMDP planner projects factorized beliefs forward in time forming at the same time a computational graph and then computes gradients backwards in time over the graph to improve the policy.   Overall, the approach seems promising and outperforms state-of-the-art in large problems where the approximations do not seem to degrade performance.  The approach relies on a factorized approximation which is not accurate contrary to particle based approximation which are accurate in the limit. Due to this the experiments should be extended with more classic POMDP problems, for example the RockSample problem where the factorization is suspected to not work well. This would give a more complete picture of the strengths and weaknesses of the approach. I would like to emphasize that even though the proposed approach may not work in all tasks it provides a novel approach to POMDP solving and could be beneficial in many practical problems.   EXPERIMENTS:  "Here we test all algorithms without domain knowledge." It is well known that MCTS based algorithms (such as POMCP) only work well with a suitable rollout heuristic (the heuristic can also be learned, see e.g. Alpha Go Zero). Also results with a heuristic (domain knowledge) even if with a simple one should be provided. It can be also considered "fair" to provide some domain knowledge to MCTS approaches in the form of a rollout heuristic since adding domain knowledge is straightforward compared to other approaches.  Figures 3 and 4 should include confidence intervals.   RELATED WORK:  Using factored beliefs in POMDP planning is not new. For example, [Pajarinen et al., 2010] uses factored beliefs to solve large POMDP problems. These results also give hints on when a factored belief may be problematic. For example, in the classic RockSample problem, where movements are deterministic, a factored probability distribution does not seem to be a good approximation.  Pajarinen, J., Peltonen, J., Hottinen, A., & Uusitalo, M. A. (2010). Efficient planning in large POMDPs through policy graph based factorized approximations. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 1-16). Springer, Berlin, Heidelberg.   LANGUAGE:  "A MDP" -> "An MDP"  "each of these is specific by a" -> "each of these is specified by a"  "it is important to noice" -> "it is important to notice"  "which rolls out out" -> "which rolls out"  "shows the potential of sample sample sizes to yield good performance" ??  In Figure 1, there is the text "SOGBOA". Should this be "SOGBOFA"?