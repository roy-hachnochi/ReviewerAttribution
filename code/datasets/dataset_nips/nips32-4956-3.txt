This paper proposes to provide a model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model by providing an explanation in the form of a small subgraph of the input graph together with a small subset of the most influential node features. The authors validate their proposed approach through several experiments   This paper reads well and the results appear sound. I personally find the methodology and the provided techniques to overcome the challenges in each step very interesting. Furthermore, the provided experiments support their intuition and arguments.  As for the drawbacks of this paper, I find the relationship to the prior works partly unclear. Furthermore, the related work study is missing some of the most recent works on explaining the learning methods on graphs [1,2,3]. Moreover, It would be nice if the authors could also provide some ideas for future research directions, such as the prospects of using their approach for improving node classification and link prediction tasks.  [1] Dai, Hanjun, et al. "Adversarial attack on graph structured data.". [2] Zügner, Daniel, Amir Akbarnejad, and Stephan Günnemann. "Adversarial attacks on neural networks for graph data.". [3] Pezeshkpour, Pouya, Yifan Tian, and Sameer Singh. "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.".