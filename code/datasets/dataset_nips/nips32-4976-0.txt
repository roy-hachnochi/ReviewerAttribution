The paper formulates a budgeted Markov decision process (BMDP) able to deal with large search spaces.  The problem is framed as a multi-objective MDP considering the rewards and cost functions. This creates value functions for rewards and costs, and the framework searches for a minimum cost policy, within the maximum reward policies that satisfies the budget restrictions.  The paper experimentally shows the performance of the proposed approach in three domains: corridor, spoken dialogue, and autonomous driving.  The framework searches for a minimum cost policy given a maximum reward policy satisfying budget restrictions. In principle it could be framed the other way around searching for a maximum reward policy given minimum cost. Add a note of why this is not a good idea.  When creating batch samples what happens when the budget is zero? The episodes ends with a negative reward?  The proposed framework is not a contraction so in principle is not guaranteed to converge. The authors provide some possible explanation of why this was not an issue in the selected domains. In which cases should we not use the proposed approach?  The convex hull policy algorithm is not very clearly described. How can the budget lie between two Q_c, is it not the case that all of them should respect the budget constraint?  Typos: - provided Appendix A  