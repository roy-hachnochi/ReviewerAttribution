In light of the new experiments which mostly validate the approaches capability of improving performance on the main task - I have increased my score from a 6 to a 7. It would have been nice if the new results considered more than one random seed (had confidence intervals etc...).   -----------------------------------------------------------------------------------------------------  Originality   The paper is well framed, the authors adequately cite related work on GVFs, auxiliary tasks, and meta gradients. Combining meta-gradients and GVFs for auxiliary task discovery is fairly novel. The closest related work used a myopic objective which the authors improve upon with their multi-step version.  Quality  The multi-step meta-gradient is a simple yet effective way (as the authors show in the experiments) to improve the learning of a representation. On that note, it would have been nice to have a similar ablation of the multi-step objective on more complicated domains such as pacman.   The authors are direct in acknowledging some of the limitations of the approach, as it requires maintaining k copies (where k is the unroll length) of the network parameters. Perhaps some additional discussion about whether or not this is harmful in practice (running out of gpu memory) would be useful.   Unfortunately, based on the current set of experiments I’m not entirely convinced that there is a concrete scenario where you would actually want / need the proposed method. Since the ultimate goal is to perform better (either in terms of sample efficiency, final performance, or perhaps generalization since their method could lead to a better from generalizable representation) it is striking that the baseline A2C agent outperforms the proposed method across experiments.   On that note, not allowing the gradients from the main task to flow to the representation layers is definitely one way to isolate the contributions of the auxiliary tasks.  However, it feels more like an ablation study then a main result. I think it is critical that the authors add some experiments where they allow the gradients to flow from the main task and highlight gains with respect to some metric (whether that be sample efficiency, generalization, etc…).  Clarity   The paper is a very clearly written and well polished paper. All the necessary hyperparameters and experimental setup is provided in either the main text or the supplementary.   Significance   Overall, the automatic discovery of auxiliary tasks through the proposed multi-step objective would be of great interest to the Deep RL community and despite not necessarily achieving any state of the art results (which would have put this paper over the top) is still an interesting contribution.  Small notes:  Figures 7/8 in appendix - colors in legend don’t line up (e.g blue doesn’t appear in the legend but does in the figure)  Regarding the multi-step meta gradient, I’m wondering if a td lambda style version of this would perform better, where you put more weight to short term losses (since there’s variance in using distant losses).  Optimizing hyperparameters for A2C and then using that set of hyperparameters for all other algorithms is obviously not the ideal setup  (the ideal setup being doing an individual search for each method) but understandable under hardware constraints. This could explain to some degree why the A2C baseline is the best performing agent in all experiments.   