This paper suggests an extension to MAML that focuses on integrating model-based meta-learners and gradient-based meta learners.     The method recognizes the task "mode" and adapts an initialization that can be learned through a few gradient steps.      A practical optimization algorithm is presented for learning the proposed framework.     The paper is technically sound. MMAML is tested on three different tasks, including regression on synthetic data, image classification on challenging datasets, and RL on standard datasets. In the image classification tests, for the two-mode case mini-imagenet has been used, while for the 3-mode case and 5-mode case, the additional datasets are not very challenging.   The paper claims that the gap between the proposed method and MAML is larger when there are more modes, suggesting that the impact of the method is better seen in cases with more modes.  However,  it is not clear whether the improvement in performance is due to the ability to better handle higher number of modes or it is because the additional modes are simple tasks and don't have much different distributions.  The Two-digit MNIST and three-digit MNIST datasets added on the 5-mode case are both easy tasks and have relatively similar distributions.  While it is expected to have lower accuracies when more modes are present, perhaps because MNIST-based datasets are too easy, the five-mode experiments have very high accuracies as compared to 2-mode tests. Some other classification datasets such as CIFAR might be a better choice to evaluation.  The paper is generally well-written and structured clearly.  The idea of the paper has a good practical impact and solves a limitation of MAML.    Update after authors feedback: Authors have addressed the concerns. I would like to update my overall score to 8.