This paper studies meta-learning in the context of a multi-modal task distribution (e.g. few-shot image classification where input-output pairs come from entirely different datasets). The authors first note that MAML—which finds a single initialization of the parameters and updates those parameters to the task via standard gradient updates— is not well-suited to this setup because the diversity of the tasks likely requires substantially different parameters. Motivated by this observation, this paper proposes an extension to MAML, called multi-modal MAML (MMAML), which is designed to capture the multi-modality in the parameter space. More specifically, this paper uses a separate modulation network that adapts the task parameters through an affine feature transformation. The modulated task parameters then undergo the usual MAML gradient update.   The paper evaluates the proposed extension on three tasks: a synthetic few-shot regression problem, a few-shot image classification problem,  and a meta-RL problem. They compare their method against two baselines: MAML and multi-MAML, a variant of MAML which has access to the ground-truth task mode label. For the regression task, they show that 1) MMAML significantly outperforms MAML (especially when the number of modes increases), and that 2) surprisingly MMAML also outperforms multi-MAML, and that 2) the modulation step is doing the most of the work, already resulting in decent performance on the regression tasks without using the gradient steps. More or less similar findings are reported for the image classification problem.   Strengths: 1. The paper is well-written and the core idea is well-motivated and easy-to-follow 2. The method is quite versatile and is evaluated on a diverse set of tasks (ranging from synthetic regression to image classification and reinforcement learning) 3. I believe the baselines are sensible (though the paper would benefit from comparison to stronger methods, see weaknesses)    Weaknesses: 1. I believe the paper would be much stronger if it compares against stronger baselines like prototypical nets, proto-MAML, bayesian MAML and TADAM. Though the authors mention these methods in the related work, they do not directly compare against them.  In the supplementary material, they say the following about bayesian MAML:  “We believe the model distribution is still unimodal (with a Gaussian prior), which is not well-designed to address multimodal task distributions (similar to MAML).” Such claims should be backed by empirical evaluations.  2. Although the authors show through t-SNE visualizations that the modulation network successfully separates the modes of the task distribution, I believe more can be done to investigate the learned modulation step. For example, one of the premises of the proposed method is that the pure gradient step of MAML can’t bring you far enough in parameter space to obtain good performance across nodes. I think it will be informative if you check the norm of the modulation step vs the norm of the gradient updates, and compare this to the norm of MAML updates.   Typos: L83. Those   UPDATE: After reading the rebuttal I have updated my score to 7. 