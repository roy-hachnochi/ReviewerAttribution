Originality: - The authors claim this paradigm towards specifying ML models is novel. It is somewhat difficult for me to assess the originality of this work as it's not exactly my area, but I am inclined to agree that their approach seems new and interesting.  Quality: - Section 3.3: It's quite generous to call these "key properties" of the model, as really they refer to the results of this particular instantiation of slice-based learning on this toy dataset. It's definitely nice to see that the approach works on a toy dataset, but I would strongly consider reframing this section. - The authors point out three challenges in building their slice-based learning framework: coping with noise (in the slice labeling functions), stable improvement of the model, scalability. The latter two are adequately addressed in the paper and experiments, but the noise aspect was not really addressed. It'd be nice to see an experiment where you add noise to the labeling functions to see how model performance varies in the amount of noise, as a lot of interesting slice labeling functions would likely be noisy in text and vision. Relatedly, is Figure 3c missing? - In the experiments, the proposed approach gets modest gains (some well within a standard deviation of other baselines), and in the appendix there are some counterintuitive trends on some of the slices where the propsed approach improves where other baselines hurt and vice versa. I don't think this is fatal to the paper as it's still interesting work and seems somewhat effective, but it'd be nice to have some explanation/speculation of why this is.  Clarity: - The exposition of the framework in Section 3 is nice and very readable; the figure and lettered model components do a lot of work here. - Section 4.1: This section seems to be missing important details. (1) what do you mean by "we created our own data splits"? Did you collect new data or re-use existing labeled data? (2) What are the slices? This seems quite important and interesting to know, even as a brief, general statement (e.g., "We create slices for CoLA based on the presence of various wh- question words."). The information is in the appendix, but definitely should be in the main paper. - Section 4.2 and generally: It would really be nice to have slightly more informative descriptions of the baselines, e.g. what is data-programming? (This might just be me not being that familiar with this area, but if it is an "emerging class of programming models", it would seem like not that many people know about it and worth explaining!) - Section 4.3: Related to the above, it is very odd not to show performance per slice in the main table. I know it's in the appendix, but a main research question is improving performance on the slice w/o hurting performance overall, and the table as presented does not show that. There's also two entire experiments contained in S4.3, lines 283-292; that is not enough time or space for readers to get a since of what the experiment is. - L214: "numebr"  Significance: Overall, this work feels quite significant to me, though again, it is somewhat orthogonal to my area. The proposed approach does seem competitive with the "state-of-the-art" (mixture of experts) while being significantly less resource intensive. I believe that industry ML practitioners will likely use this approach and other academic groups in this area will take inspiration from the ideas and try to build on them.