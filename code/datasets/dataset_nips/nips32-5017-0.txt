I believe the proposed method, HAL (Hierarchical Abstraction with Language), is an interesting approach for HRL. The authors adapt Hindsight Experience Replay for instructions (called Hindsight Instruction Relabelling).   I have some concerns about the experimental setup and empirical evaluation of the proposed method: - The motivation behind introducing a new environment is unclear. There are a lot of similar existing environments such as crafting environment used by [1], compositional and relational navigation environment in [2]. Introducing a new environment (unless its necessary) hinders proper comparison and benchmarking. It seems to me that the environment was specifically designed to highlight the strengths of the proposed method. One of the most important motivations behind studying HRL methods is solving complex sparse reward tasks. I would have liked to see the proposed method applied to some of the most popular sparse rewards tasks such as Montezuma's Revenge, as it helps in gauging the significance of the proposed method as compared to several published methods evaluated on these tasks. If the proposed method can not be applied to standard HRL tasks, then it is a limitation of the method which should be discussed.  - I believe the proposed method is similar to [1] and [3]. The authors should position their work with respect to [1] and [3] which would also serve as better baselines in my opinion.  - Two HRL methods used as baselines in the experiments completely fail in the new environment proposed by the authors. Some explanation behind this result would be helpful.  - The authors state that the high-level policy always  (even in the diverse setting) uses ground-truth state as input, namely position and one-hot encoded colors and shapes. I think the ground-truth state consists of compositional features which make it very easy for the high-level policy to learn to output instructions, while the baselines can not leverage this. I believe this is an unfair comparison and allows raises concerns about the effectiveness of high-level policy with high-dimensional state space, especially because it is not tested on any standard environment.  - The appendix is full of typos. For example, line 675, "Due tot", line 705 "do to time constraint". Section B.1, which is referenced several times in the main paper, seems to be incomplete.   [1] Jacob Andreas, Dan Klein, and Sergey Levine. 2017. Modular multitask reinforcement learning with policy sketches. ICML-17  [2] Yu, H., Zhang, H., & Xu, W. (2018). Interactive grounded language acquisition and generalization in a 2d world. ICLR-18  [3] Oh, Junhyuk, et al. "Zero-shot task generalization with multi-task deep reinforcement learning." ICML-2017.   ---- Updated after author response:  After reading the author response and other reviews, I maintain my rating. This is due to the following reasons:  1) It seems to me that HAL is specifically designed for the proposed environment and not a general HRL method. The author response confirms that the proposed method is not general enough to be applied to any environment ("environments like Montezuma’s don’t have labeled data or infrastructures for complex language captioning"), however the introduction claims that HAL is a general HRL method, the first contribution stated is "a framework for using language abstractions in HRL, with which we find that the structure and flexibility of language enables agents to solve challenging long-horizon control problems". Specifically, the environment needs to provide whether each language statement is satisfied or not by the current world state. I believe the crafting environment implemented by the authors also provides this information. This is a very strong assumption and severely limits the applicability of an HRL method. These limitations are not acknowledged in the submission. I suggest reframing the introduction to introduce the task/environment and the challenges associated with it, and propose a solution for the specific task.  2) Based on the author response, I believe most of the gains over the baselines are coming from Hindsight Instruction Relabelling (as the authors also mention "DDQN is able to solve only 2 of the 3 tasks, likely due to the sparse reward" in Section 6.2, and in the rebuttal authors say "HAL significantly outperforms policy sketch because it is off-policy and leverages hindsight relabeling"). In my opinion, HIR is an adaptation of HER in the proposed environment and not very original.   3) The above also raises concerns about fairness in comparison with other methods. HIR requires specific information from the environment about whether each language statement is satisfied or not by the current world state. This makes the comparison unfair because baselines do not leverage this information from the environment.  4) I also agree with Reviewer 3's concern about high-level policy only chooses from a fixed instruction subset and therefore does not learn or output anything compositional. The additional results provided in the author response are significantly different from the original submission and require additional details.