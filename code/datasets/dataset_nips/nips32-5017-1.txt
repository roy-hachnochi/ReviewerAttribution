Originality and Significance: The idea is natural and intuitive. Now that the authors have shown this idea works, there's a direct avenue for incorporating ideas from other work (i.e., generalization in visual QA) to improving RL. The authors did a great job finding the right setting (a reasonably compositional one) to showcase language's promise in RL (highlighted by the systematic generalization results). I know that several others have been thinking about this idea in general for a while (using language as an abstraction in HRL) - for example, see concurrent/later work "Hierarchical Decision Making by Generating and Following Natural Language Instructions" (https://arxiv.org/pdf/1906.00744.pdf). Regardless, it is great to see this idea actually work in a pretty challenging / sparse reward RL setting. One drawback of the implemented agent is that the high-level policy treats each instruction distinctly, which takes away from some of the story of aiding RL by exploiting the compositionality in language. Decoding instructions in a compositional manner would be fit better with the authors' aim; full autoregressive decoding would be impressive (but challenging), but it would even just be interesting to factor the action space compositionally (e.g., first predict the instruction template, then predict the key nouns/adjectives in the template, or perhaps using hierarchical softmax to decode actions). Right now, as I understand, the low-level policy treats the goal input as compositional, but the high-level policy does not treat actions as compositional.  Quality: The work is well-executed. Task-design and model-design decisions are simple, clear, and well-motivated. The instructions themselves could be more diverse; I would've been more interested in seeing the authors experiment on highly compositional/diverse instructions (i.e., at the level of language complexity/compositionality of CLEVR questions) on the state-based environment rather than experimenting with simpler language instructions from pixel-based observations (since the paper's focus is on language).  Clarity: The writing was quite clear overall. In general, I felt like the paper made many distinct points about how language could be useful; it would've been helpful to frame the intro/discussion/paper as focusing on 1-2 of these (i.e., compositional generalization), as well as being concrete about how language can help. For example, it seems that compositional generalization through language is a relatively unique/strong aspect of this work, while using language instructions to specify vague goals is a property of instruction-following tasks in general, not specific to using language in HRL. I only understood around Page 7 (experiments) why the authors concretely expected language to help with generalization (when the authors describe the explicitly non-compositional approaches); even then, I would've liked more explanation for why policies did generalize compositionally, in contrast with the expectation (Page 7): "From a pure statistical learning theoretical perspective, the agent should not do better than chance on such a test set."   Minor writing comments: * "Fortunately, while the the size" -> "Fortunately, while the size" * "arrnage 4 objects around an central object" -> "arrange 4 objects around a central object" * Figure 4b: Maybe order the keys in the figure by the number of instructions / the performance in the graph (more intuitive/easier to read). And/or spell out "12k" -> "12000" so faster to tell what's going on (initially I was confused reading the legend) * Figure 5: The legend is pretty small * The appendix has a few typos as well