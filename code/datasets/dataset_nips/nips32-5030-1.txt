         # Originality         I am not an expert in the literature in online learning with kernels,         but it seems [12, 13] are indeed the most relevant prior work, which         the paper cites and builds upon.          I would be surprised if the idea of using Taylor approximations for         kernels is completely new; please consider adding relevant references.           # Quality         I have not verified the details of all the proofs, but the math in         general does seem to add up: The main claims do seem to follow correctly         from the central Theorem 9 and the other relevant results. So does         Theorem 3, which is central to the third contribution which I find to be         the most interesting.          I believe, however, that the claims in Section 3.1. regarding Gaussian         kernels have to be taken with caution. In particular, the computational         and storage complexity does not scale with the effective dimension as         claimed, apparently, but only with a quantity that is an upper-bound on         the effective dimension. As such, it cannot be claimed that the         algorithm is more efficient than the $O(n^2)$ ones (and it is not a         really useful property to be more efficient that $O(n^2)$ whenever         $n=O(e^d)$).         Having said that, the authors do provide experimental results to show         that the algorithm with taylor approximation is fast.           # Clarity         The paper is clearly written and easy to follow, though it certainly         requires a proof-reading as there are many typos, gramatical errors,         etc.          The proofs also seems cleanly written and easy to follow.           # Significance         I have commented on the significance in the previous question.         In general, this is a clear paper and a nice addition to the literature,         hence I am recommending acceptance.         However, I have some questions about the third contribution (see below),         and the first and second contributions do not seem enough on their own.           # Questions for the authors:         - Is there any other benefit to your use of AWV other than removing         the bound on $||f||$? Wouldn't Pros-N-KONS obtain the same regret         bound as PKAWV if you apply Theorem 9 (with the difference that the         first two terms ("regret terms") in the bound are handled by the KONS         regret analysis rather than the K-AWV analysis)? In particular, is a         version of Corollary 8 possible for KONS?         - What is the key property that makes Corollary 8 possible? I realize         that the approximation terms in Theorem 9 capture the change in the         projection operators, but shouldn't this be compensated for by the         adaptivity of the projections? Also, why is the $m$ term not there in         the projection error term bound of Pros-N-KONS (why is it only in the         regret bound)?                  #######################################         After the author response:         #######################################         After reading the other reviews as well as the author response, I would like to         increase my score. The author response does answer my questions for the most         part. I can also agree with the authors that there are $(n,d)$ regimes in which         this is applicable and useful, and the authors also correctly note that the         result is more general, and less computation is possible at the expense of more         regret.       