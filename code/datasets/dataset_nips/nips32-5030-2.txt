While the paper makes some good theoretical contributions, I do feel that the contributions may be rather niche, and of limited impact. Moreover, the exposition was hard to follow. Several undefined notations and grammar mistakes make the reading unpleasant.    Originality: I would rate the ideas low on originality, as the idea of using Taylor expansion and Nystrom approximation has been extensively explored for online learning with kernels (and in general learning with kernels).   Quality and Clarity: - There are several grammatical and semantic/notation errors which disrupt a smooth reading (e.g. we aims” at; little “considers” non-parametric function; AWV seems like an arbitrary term for denoting an algorithm – and I could not find what AWV stands for; What is B and d_eff in Eq 2?; Line 122 – is not “The on the space…”;  and many more such instances throughout the paper ) - First paragraph in the introduction makes several strong claims about the research directions in general - not properly validated through references or other evidence - What are the pros and cons of considering squared loss over a Lipschitz loss? - What was the motivation of choosing the specific basis function for the Gaussian kernel? How does this approach differ in principle from other projection methods (e.g. Fourier projection) - Since the dictionary for Nystrom method is fixed, would that make the method sensitive to noise? - The experiments show algorithm as PKRR, while the algorithm has been denoted as PKAWV in most of the paper. Experiment section stats that M = 2,3,4 are tried, but only results for 2 and 3 are shown - I am not sure why the proposed approaches are faster than FOGD, as all of them are essentially linear projection algorithms. All algorithms should grow linearly in time (except KRR), but from experiments do not seem to indicate this for the baselines. Time out of 5 minutes (and later stated as 10 minutes) seems to be very small and arbitrary. Some of the learning curves also seem to be a bit strange. How significant is the performance gain of the proposed methods? The log scale graphs make it hard to see – perhaps a table with standard deviations gives a better picture.  - Were the classification experiments done using a squared loss too? Would that make the comparison slightly unfair to favour the proposed method?   Significance: While the work in general is good from a pure theoretical perspective, I think that, the audience for this work might be a bit too narrow (focusing on one specific type of loss function, and primarily improving the speed and maintaining the same regret bounds), limiting its significance. 