Originality/related work: The paper feels incremental as it combines existing techniques/ideas in a (perhaps) novel fashion (perhaps the main novelty is bringing in the Vovk-Azoury-Warmuth forecaster). The main claim is that this is the first method that achieves subquadratic per-round complexity (for reasonable classes of kernels). In relation to this, it would be great if the authors could comment on the relation of the present work and the papers [1,2,3]. In particular, the recent paper by Zhang and Liao seems to be doing something very close to the present paper, though they consider general convex losses (unlike the present paper which considers squared loss), hence their regret results will be different. However, the essence of both papers seem to be the same (this paper is also close to [16], cited in the paper). Calandriello et al's COLT paper [2] considers optimization with bandit feedback, but otherwise the ideas are very close. The same applies to [3], which I would have also expected to be cited and compared with.  Quality/clarity: The paper is generally well written. It is a bit hard to make out the algorithms though based on the text of the main paper. Some minor comments are listed below.  Significance: The method looks promising; the main theoretical contribution is perhaps Theorem 3 (or, its more advanced version, Theorem 9).   Minor comments and some questions: * abstract mentions adversarial datasets; this seems like a nonstandard terminology that is perhaps not needed * e^d seems large..  * Throughout the paper, paper numbers are used with no names. This makes the paper unnecessarily hard to read. Please write citations in a way that allows mortal humans to follow them. * After Eq (2) it is mentioned that the bound in Eq (2) is "essentially optimal" and the next section will explain this; but I could not really find where this is explained. Please be more precise with cross-references. I guess this refers to minimax lower bounds by Zhang-Duchi-Wainwright; I'd appreciate if the appendix had the precise statement (reformulated in the formalism of this paper). * Page 2, line 50: aims->aim * Page 3, line 88: player->learner * line 99: what about larger gamma?? * line 109: "rough inequality" -> "approximate inequality"? Comma missing after Finally. * line 119: f = sum alpha_i phi(x_i) is not formally correct * line 120: K_pp has not been introduced yet * page 3, line 122: First sentence is weird. * page 4, line 167: C is the covariance operator: which one? * line 168: Euclidean projection wrt the RKHS norm, right?  * page 6, line 202: So what is the range of the grid? Can you add more details? citing the CBL book does not help much here * line 209: "works"->"work". What does "It" refer to? Consider rephrasing. * line 213: So where is that algorithm? * line 235: classical assumption: perhaps "common setting"? (this is not really an assumption; more like a setting or an example) * page 8, Fig 2 (also in appendix): Figures seem to use labels that don't match the main body.   References --------------- [1] Zhang, X., & Liao, S. (2019, May). Incremental Randomized Sketching for Online Kernel Learning. In International Conference on Machine Learning (pp. 7394-7403). [2] Calandriello, D., Carratino, L., Lazaric, A., Valko, M., & Rosasco, L. (2019). Gaussian process optimization with adaptive sketching: Scalable and no regret. arXiv preprint arXiv:1903.05594. (also at COLT 2019) [3] Rudi, A., Calandriello, D., Carratino, L., & Rosasco, L. (2018). On fast leverage score sampling and optimal learning. In Advances in Neural Information Processing Systems (pp. 5672-5682).