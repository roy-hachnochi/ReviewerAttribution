I have read the authors' response and believe it adequately addresses all reviewers comments.  -------------- Original Comments:  The authors present a regularity condition with proof that it guarantees linear convergence of projected Grassmannian subgradient descent. The regularity condition is intuitive and useful in that it only requires characterization of the Riemannian subgradient in the neighborhood of the desired point, and potentially applies to a wide class of functions. In addition, they show that their analysis holds for two problems of modern interest (Dual Principal Component Pursuit and Orthogonal Dictionary Learning), leading to new convergence results for these problems.  For my part, I have verified the correctness of Proposition 1, and Theorem 1.    Specific Notes:  Line 84: I found the definition a bit off-putting at first for the obvious reason that we usually think of the projection of B onto A as an operation on B. I recognize that in this setting if A and B are elements of O(c, D) then the definition is equivalent. I just think it may be nice to include a sentence stating this for clarity.  Line 116: neighborhood  Line 118: Is it more appropriate to say this gives  abound on the sum of the cosines of the principal angles?  Section 3.2: Although it is clear what you mean, it may be better to explicitly state that by projection onto the Grassmannian you mean orthonormalization. Alternatively (and perhaps preferably), you could formally define projection onto the Grassmannian.  General Notes:  It seems to me that the convergence results should hold even if the steps are taken along a geodesic. I have not explored this idea very much, but it may be straight forward to extend similar results to non-projected Grassmannian gradient descent.   