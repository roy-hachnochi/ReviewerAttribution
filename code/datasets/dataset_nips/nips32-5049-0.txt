In this paper, the authors propose a distributed Newton method for gradient-norm optimization. The method does not impose any specific form on the underlying objective function. The authors present convergence analysis for the method and illustrate the performance of the method on a convex problem (in the main paper).  Originality: The topic of the paper, in my opinion, is very interesting. The paper presents an efficient Newton method that is motivated via the optimization of the norm of the gradient. As a result, no assumptions are made on the objective function. This is a key differentiating feature of the method as compared to other such methods.   Quality: The overall quality of the paper is very good. The motivation is clear, the theory is well thought out and presented, and the numerical results show the superiority of the method (on a convex problem). I state my minor comments/questions/concerns below.  Clarity: The paper is well-written and motivated. Below I state a few minor issues and concerns, as well as a few suggestions to improve the clarity and presentation of the work.  Significance: In my opinion such distributed Newton methods are of interest to both the optimization and machine learning communities. This paper attempts to alleviate some of the issues (communication vs computation balance, number of hyper-parameters and sensitivity to hyper-parameters) of existing works. I believe that this paper, after minor corrections and modifications, should be accepted for publication at NeurIPS.  Issues/Questions/Comments/Suggestions: - “increasingly more frequently”: is this phrase not redundant? - “regardless of the selected hyper-parameters”: The theory shows that this is indeed the case for the method. Of course, the method was designed, in a certain sense, such that that claim would be true. Nevertheless, it could be a bit misleading. Although, strict reduction is guaranteed with any hyper-parameter setting, this reduction could be very small if the hyper-parameters are chosen poorly. The authors should comment about this in the manuscript. - Related Work and Contribution paragraph 1: the authors should more clearly state which disadvantages apply to each method. - Line 66: “derived by optimization of the”: missing word? - Deriving DINGO from he optimization of the gradient norm is interesting and has many advantages as stated by the authors (e.g., no restrictions on the functions). However, does this approach have any limitations/drawbacks? For example, convergence to a local maximum or saddle point? The authors should add a comment about this in the main paper. - The discussion about the hyper-parameters is interesting, and shows the strength of the proposed approach. I suggest that the authors present this information in a table. For each method, the authors could clearly show the hyper-parameters associated. Moreover, in the table the authors could clearly state the per iteration cost (in terms of communications) of each method. - The drawback of the current analysis of DINGO is that it requires exact solutions to the sub-problems. The authors clearly state this. What is the limitation? How would the analysis change to account for inexact solves? The authors should comment about this in the manuscript. - Per iteration cost of DINGO: the authors should discuss the per iteration cost of DINGO in the manuscript. Both in terms of communication and computation, and compare with existing methods. If I am not mistaken, in the worst case, DINGO requires 4 rounds of communications per iteration, plus the number of communications associated with satisfying the line search condition. - Line Search: Usually, the Armijo condition does not have the factor of 2. The authors should comment about this in the paper. - DINGO algorithm is complicated: Algorithm 1 is complicated (with the three cases). The authors may want to give a high level description of the method before the present the actual algorithm. - Effect of theta parameter: The theta parameter controls the search direction chosen by the algorithm. Essentially, it controls that the search direction is not orthogonal to the gradient of (4), and that it is a descent direction. The authors should comment about this in the paper and discuss the role of theta. - Assumptions 3-6: The authors should add to the discussion of these assumptions. Why they are realistic? Why they are necessary? - In this experiment presented in the main paper, the function is strongly convex, and thus all iterates fall into Case 1. The authors should discuss the effect of Case 2 and 3 iterates on the performance of the method. - Future Work: What do the authors mean with “more efficient methods”? 