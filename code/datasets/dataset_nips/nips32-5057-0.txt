Significance:  The proposed criterion (SIC) relies on maximizing a discrepancy between a joint distribution and the product of its marginals over a set of test functions. Unlike existing criterions, SIC incorporates a gradient constraint on such test functions which enforces sparsity in the coordinates. This provides a way of selecting the most dependent dimensions for which the gradient of the optimal test function will not vanish. Such a criterion provides with additional information compared to the mutual Information or to the HSIC. In that sense, it has potentially many applications.   Originality: The idea of using a sparsity-inducing penalty on the gradient of the test functions leads to an elegant formulation that provides interpretable scores of dependence on each variable.  Clarity: Very clear presentation of the method and good organization of the paper. I have one minor suggestion: I found, Figure 1 hard to parse at first because it makes it hard to compare the different methods:  TPR and FDR are expected to behave differently  (TPR: highest is better and opposite for FDR) but they are all in the same figures. Would it be better to have one figure for TPR and another one for FDR?   Quality: The paper is technically sound and precise. The experiments seem to support the main claims of the paper which is to learn the dimensions that maximize the dependence between variables. The authors compared the proposed method with other methods: Elastic net, Random forest and simple MSE with an additional gradient penalty. It seems that overall, SIC performs as well as random forests on simple examples but benefits from additional interpretability. It also seems to lead to better False discovery proportions on more complicated datasets compared to GLMs. However, I still have a few questions about the paper:  It is not totally clear to me how the gradient penalty could help more than simply using the formulation (P) which learns a sparse selector gate to detect the most relevant features. I think it would be very beneficial to the paper to include an experimental comparison with this simple formulation (for instance in figure 1).  Also in the convex case, the authors show the existence of the solution but what remains open is the consistency of the estimator obtained in that case: does it recover the true independence structure?   Finally, in section 5, a neural network without bias was used which leads to a class of homogeneous functions. Such class can be very restrictive in that it doesn't have  universall approximation capabilities. How critical is this choice of function for the method?   Overall I think that the paper proposes an interesting and novel method that has many applications.   ================================ After reading the other reviews and the authors' response, I still think this is a good submission and should be accepted. It would be great to incorporate the explanations provided in the response to the final version of paper. 