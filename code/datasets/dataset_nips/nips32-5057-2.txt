 Summary: The paper proposed the Sobolev Independence Criterion (SIC), an interpretable dependency measure between multivariate random variables X (input) and Y (response). It is an integral probability metric between the joint distribution and the product of the marginals, regularised by gradient of the witness function wrt each dimensions of X. For a general function space, the optimisation problem has l1- like penalty on the gradient terms to ensure sparsity and l2-like penalty on the square of the witness function to induce smoothness. This form of the optimisation is not easy to work with: 1) expectation appears after the square root in the gradient penalties (i.e. a non-smooth term) and 2) the expectation inside the nonlinearity introduces a gradient estimation bias (biased expectation estimation). The authors proposed to alleviated these problem by introducing an auxiliary variable \eta through the variational form of the square root. This \eta_j parameter is the normalised importance score for each feature j of X and hence can be used for feature selection through their ranking.   Two classes of functions have been studied in the paper: fixed feature space and deep ReLu networks with biases removed. When the SIC witness function f is restricted to an RKHS which lead to an optimisation problem that is jointly convex in f and \eta, they provide theoretical guarantees on the existence and uniqueness of the solution [Theorem 1], convergence results of the perturbed SIC (i.e. SIC_\epsilon where \epsilon is an regularisation term added inside the square root of the nonlinear sparsity penalty to induce smoothness) to SIC [Theorem 2] and the decomposition of SIC into the sum of contributions from each coordinates [Corollary 1]. Instead of choose a feature map, they propose to use deep ReLu networks to learn them, however the optimisation problem becomes non convex. They utilises existing optimisation algorithms and FDR control methods to establish SIC based feature selection algorithms. The paper concludes with experiments on two synthetic dataset and two real life datasets.  The paper is clearly written however there are a few typos. The structure is logical and explanation is adequate. It utilises the sparsity inducing gradient penalties, that were popular in other areas of machine learning (e.g. double back-propagation, WGAN_GP, optimal transport theory, etc.), as a regularisation term in the traditional integral probability metric. Consequently, one has direct access to the importance score of each feature and this naturally leads to a feature selection scheme. Such formulation is general and one is allowed to consider different function spaces. The work provides an interesting formulation of a dependency measure  and of the problem of feature selection, which is worth exploring further. However, overall I find the experiments section requires much more discussion, or at times, more experiments to illustrate the performance of the proposed procedure.   I have a few questions/concerns that require some clarifications from the authors:   Main concerns: 1) From my understanding, the paper proposed two FDR controlled SIC: HRT-SIC and knockoff SIC. Some more experiments or explanations might be needed since it is not clear to me if the two methods perform similarly? Or are there situations where one clearly is better than the other? Or there are situations where one can not apply one of them?   2) The authors proposed the a kernel SIC and then alleviate the problem of kernel selection by introducing the neural SIC, where the feature maps are learnt. My understanding is that it is highly problem/data dependent when neural networks outperforms kernel methods. Have you tried to use a Gaussian kernel with a fix length-scale (e.g. median heuristic) in the experimental comparison? Is it always better to use the neural SIC? I feel some discussion/experiments are missing here.   3) There is no discussion of the trade-off between the computational time and performance. Can the authors provide some comments on this? I find it hard to judge the gain in the performance without knowing the computational cost of the proposed method.   4) I am a little confused by the beginning of section 4: this seems to be a slightly different formulation from the usual kernel set up. For HSIC (or MMD), the kernel is required to be bounded (i.e. E_z(\sqrt{k(z,z)})< \infty) for the existence of the kernel mean embedding and for the mean embedding to be injective, we further require the kernel to be characteristic. The only constraint on this space of function seems to be through the parameter u, does it mean we can have any feature map \Phi_\omega in the setting presented (e.g. tensor product of feature maps that correspond to non characteristic kernels?) Or does these requirements translates into condition on the parameter u? Or is it in fact not necessary to have any conditions on the feature map? Can the authors please explain these?   5) line 116 on page 4, Theorem 1, (2) why is it important to show that L_\epsilon has compact level sets on the probability simplex? It is also not clear to me how it was show in the proof?   Minor concerns:  1) line 49 on page 2, the authors mentioned that it becomes a generalised definition of Mutual information, has this “generalised” concept been studied somewhere already?  2) line 55 equation, what is this zero norm of w?  3) line 90 on page 4, why can we remove the 1/N term from the last term?  4) The HIV-1Drug Resistance with Knockoffs-SIC experiment, it is not clear to me if it is a single run of the experiments or an average over multiple runs? What does GLM stands for?  5) line 386 on page 13, it is not clear to me how we can take partial derivative of L, we can obtain the matrix on the right? Should it not be g? Also the line before line 387 has some typos.   ========================= Authors comments read. I am happy with the comments provided by the authors, it would be great if the relevant explanations provided could be elaborated in the revision. I have adjusted my score accordingly. 