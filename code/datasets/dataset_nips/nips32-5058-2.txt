The authors apply entropy regularization techniques to Monte-Carlo tree search (MCTS). For this purpose, they first investigate bandit problems with the entropy regularization and introduce a new bandit setting called a stochastic softmax bandit problem. The problem is to optimize the value of the expected value plus the entropy regularization term, instead of the (pure) expected reward. Then an optimal action-selection strategy called E2W is proposed for this problem. By applying E2W to the tree search, a new MCTS method is proposed.   The paper provides theoretical analysis regarding the convergence rate of the proposed methods. The result indicates that the proposed tree-search could improve the state-of-the-art tree-search based on UCT. However, I have a concern as noted in [a] bellow.  The paper also demonstrates the sample efficiency of the proposed method in both synthetic problems and Atari game playing.   Concerns: [a] The authors claim that the proposed method has better convergence rate that UCT. However the optimization problem and condition between them will be different. For example, the existing methods usually consider minimization of the regret or the negative expected reward, while the proposed method consider the expected reward plus the entropy as the objective. Furthermore, the proposed method seems to assume that the reward (and state-transition) are deterministic. I would like to recommend a fair and careful discussion (and experiments) with describing difference of the conditions and the objective functions.  [b] While the experimental results simply show that the proposed method could have better performance than the state of the art, there is no discussion. Some discussion and intuitive explanation will be useful for better understanding of the proposed approach. In other words, in which case should we use the proposed methods instead of UCT or the other methods? Also note that the proposed algorithm has an additional hyperparameter tau, which could cause the over-fitting to the tasks.  [c] The regularization coefficient tau will have big impacts on the performance because it can change the optimal arm and also effect on the convergence rate. Thus, the setting of tau is important to use the proposed methods. However, there is no (empirical) evaluation regarding tau. For example, it should be useful to evaluate the sensitivity of the tau setting.    Minor issues: - The definition of H in eq. (2) is missing. - I feel that the result of UCT in the rightest figure in fig. 3 is strange. Why does the decrease of the error stop?   === * Update after the rebuttal I would like to thank the authors for submitting a response to the reviews and addressing some of my comments. I have the following comments. - I agree that MENTS is directly compared with UCT in the experiments of Figure 3. However, the optimal action a* in Theorem 5, which is the action with largest softmax-value rather than the conventional action-value, could be different with the optimal action in the conventional RL or UCT. If it is true, the difference will be important and should be mentioned. - I would like to have more meaningful discussion about the decrease of error of UCT in Figure 3 (k=8,d=5). I feel that the rebuttal to this point is just a paraphrase of the experimental result. - The assumption of deterministic transition and reward in Line53 will be unnecessary because it can be misleading. 