This paper introduces a neural regularization method for CNN based image classification architectures. The authors hypothesize that biasing the representation of artificial networks towards biological stimulus representations might positively affect their robustness. They present natural images (CIFAR10) to mice and measured the responses of thousands of neurons from cortical visual areas. A prediction model is trained on the collected data (100 oracle images), in order to estimate the neuron responses to a much larger image set (5000 images) and perform denoising. The neural representation similarity is then used to regularize CNN by penalizing intermediate representations that are deviated from neural ones. Experimental results on the CIFAR10 data show that the proposed regularization method can achieve better performance on classifying noisy images, in comparison with several baselines or control models.   (1) The idea of using the similarity of neurophysiological data to regularize the representation of artificial neural networks is interesting and novel. The authors have made good efforts towards bridging the fields of neuroscience and machine learning through such regularization.   (2) Experimental results on the CIFAR10 data demonstrate the effectiveness of the proposed method and provide some validation of the hypothesis.   (3) In the proposed joint training method (Section 3), a selection of layers from the bottom to the top of the architecture is chosen. The final similarity of representations from CNN is a weighted average of the similarity calculated from each layer. The output from different layers can be dramatically different, which lead to quite different final similarity values. The authors need to provide guidelines on how to choose such a selection of layers.   (4) In Section 2.2, a predictive model is used to "denoise neural responses". What is the prediction accuracy (or correlation) in the proposed experiment setup (i.e., use the neural response to 100 oracle images to predict the response to the non-oracle 5000 images)? The scaled model response is defined as $\hat{r}_{ai}=w_{a}v_{a}\hat{rho}_{ai}$. It is not clear which correlation measure is used to compute $v$. Generally speaking, correlation coefficient measures the statistical relationship between two variables. The definition of the scaled model response is not mathematically correct and meaningful. I am a bit concerned about how the performance of the predictive model can affect the final results. The prediction accuracy of a model built on 100 images is likely not high when applied to 5000 images.   (5) The proposed method is only applied to one dataset CIFAR10 and one architecture ResNet18. The authors may want to report the performance on other datasets and architectures as well.   (6) The authors claim that "we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the modelâ€™s predictions". From the current experiment setup, the prediction model is only applied to 5000 non-oracle images, not on million scale.   (7) How can the proposed neural regularization method be used in practice? It is apparently not a trivial effort to collect actual neural responses, build prediction model, and then perform joint training. It would be great if the authors can provide some discussions along this line.   (8) Minor issue: there are also some typos and grammar issues in the paper.   Post-Rebuttal:   I appreciate the authors' response, which helped clarify some important technical details, especially on the use of 100 oracle images, the training of predictive models on 5000 non-oracle images, as well as the scale of training data. Authors also added new expriments on other datasets and architectures. I hope these clarifications and discussions can be included in the final version of the paper. 