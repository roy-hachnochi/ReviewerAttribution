The paper proposes SASA, a method to drop the learning rate by a constant multiplicative factor when a certain criterion is met. The criterion is a statistical test, indicating stationarity of the Markov chain of the optimization parameters, which is performed once per epoch, and computed on easy to evaluate statistics of momentum SGD. The method is evaluated on three real world tasks and shows comparable performance to hand-tuned competitors.  I. Originality Statistical tests for optimization are not new, in that regard the originality of the paper is low. However, I find the approach to test for stationary interesting and useful.  II. Clarity The clarity of the paper is high. Indeed, while it is proposing a novel method, it contains parts similar to a technical report, which is refreshing to read. The authors also anticipate potential questions or concerns the reader might have and try to answer those in the paper already.   Here are my concerns on clarity.   i) The authors describe a “statistical test”, however, I am not sure if the exact test is defined somewhere in a concise way, e.g., what is the null-hypothesis? (Is it “no stationarity”?)   ii) Furthermore, it is still not entirely clear to me what is at stake if the test triggers wrongly, or does not trigger (although it should). As I understand, the same type of test is performed at each epoch, which will make it somewhat likely that both those scenarios happen eventually. I could imagine for instance, the test not triggering, and the optimizer slowly diverging away from a stationary distribution again. On the other hand, non-stationarity seems to be the null-hypothesis, and not rejecting it should be the safe-place of the test.   iii) In my opinion it would be beneficial to spend a bit more time on the interpretation of Yaida’s condition, as this is the condition of the proposed test. Looking closely, Yaida’s condition seems to say that the fraction of the squared norm of the search direction d_k, and the inner product of x_k and its gradient g_k is constant when SGM is in a stationary distribution. This, does not seem trivial at all to me, and I wonder if the authors have some intuition about why this relation holds (and what it means geometrically).   iv) Additionally, from what I gather, the condition holds for “general functions F”; first, it is unclear to me what that means, and second, by assuming that there exists a stationary condition, one might implicitly also assume some smoothness on the function F that makes the stationarity possible. The condition, otherwise, seems very specific.   v) On a practical note, it is unclear to me what the lowest possible number of samples N is that needs to be acquired to make the method work reliably. Especially for smaller datasets, this might be an issue, both because statistics lack after one epoch, and also because the Markov chain did not mix enough.   vi) On another practical note, I am unsure, however, how applicable the proposed method is. For instance, I am unclear if the method is still applicable when the gradients g_k is a biased estimator of the gradient? Biased estimators seem to become popular lately, and the significance would increase or decrease depending on the applicability.  [Post-Rebuttal: Thank you for your rebuttal. I increased my score, as I believe this is an interesting paper.] 