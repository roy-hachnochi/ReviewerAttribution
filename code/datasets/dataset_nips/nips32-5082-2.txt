[Update after author feedback] I did not have any significant concerns with the paper originally, and the author feedback addresses the other reviewer's concerns about scaling to large datasets. I continue to recommend acceptance.  This paper provides a method called zero-shot KT for performing distillation from a teacher model to a smaller student model in the zero-shot setting (i.e. the student has no access to the teacher's training data). An adversarial generator is trained alongside the student and seeks to generate pseudo-examples that maximize the KL-divergence in the class predictive distributions between the teacher and student. The student seeks to minimize this same KL-divergence. The proposed model is shown to be competitive with or outperform distillation methods that have access to significant amounts of training data. In addition, the paper proposes a metric to quantify the mismatch between teacher and student as images from the test dataset are adversarially attacked.    As far as I can tell, this work is original. Rather than relying on auxiliary information about classes, the proposed zero-shot KT approach only relies on the teacher model itself. Related work is detailed enough to clearly understand the novelty of the work and how previous approaches are related.    The quality of this work is high. Figure 1 demonstrating results on a toy problem and figure 3 showing adversarially generated pseudo-examples were helpful inclusions for understanding the method. In Table 1, the authors show the circumstances under which zero-shot KT is outperformed by KD+AT, albeit KD+AT having more examples. Results were compared over a range of different architectures and over 3 initial seeds. The authors also addressed potential concerns head-on, such as issues with the adversary and choosing hyperparameters.    Clarity is excellent. The writing was clear and insightful. The method and hyperparameters were discussed in sufficient detail to reproduce the results.    Significance is high as well. As the authors point out in the introduction, this method is useful for distilling and making accessible pre-trained models for which the dataset may not be released due to intellectual property, privacy, size, or other concerns. Such scenarios will only become more likely in the future. A potential concern is the choice of hyperparameters, but the authors describe how they chose the hyperparameters by using another task and that the hyperparameters do not have a large effect on performance.    The teacher transition curves in Figure 4 was a point of confusion for me. I was under the impression that the teacher network was the one being attacked and so was surprised that the performance of the teacher differed so drastically between the left and right columns. Is there any particular reason why the student model was the one attacked and not the teacher?