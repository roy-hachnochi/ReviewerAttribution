*** update after author feedback *** The author feedback proposes a clearer exposition of the ASA form and being more clear about what happens when the map is nondifferentiable, which would be welcome additions to the submission. With these modifications, and further reflection about the possible significance of this being a user-friendly tool, I have upgraded my assessment of the paper. *** end of update ***  Originality: The paper is of modest originality. It augments the existing idea of differentiating through conic programs by implicitly differentiating the optimality conditions, so that the user can specify a problem in DCP form and still take advantage of this technology.   Clarity:  The paper is, for the most part, very clearly written, including nice devices like running examples to illustrate what is going on. Still a major issue for me was that when some of the key ideas were introduced in section 4, they were not explained clearly and systematically. The example I found most frustrating was the term "jointly DCP", in the sense of "jointly DCP in x (the variable) and theta (the parameter)".  This appears as an essential component of the definition of ASA form, but is not defined itself. It is clearly not the case that this requires joint convexity in x and theta (from the examples) so what exactly does this mean? This really must be explained more clearly.  Similarly, I found statements like "which ensures that the maps C and R are affine; in practice, this means that their derivatives are easy to compute" quite confusing. Does this "ensure that the maps C and R are affine" (which would then imply their derivative are easy to compute)? Or does it mean that C and R could be taken to be non-affine functions, as long as their derivatives are easy to compute? Or does it mean something else entirely?  Quality: The paper appears, overall, to be technically sound.   I found that the work was a little too dismissive of the situation in which the solution mapping is not differentiable. I think dealing with nondifferentiability is far beyond the scope of this work, but I think the authors would be better served to admit that there are many examples of convex optimization problems (e.g., linear programs) where the solution mappings are nondifferentiable functions of the problem data. Having said that, they are differentiable almost everywhere, so whether this is an issue or not depends on how the points at which the derivatives are evaluated are chosen. The citation [4, section 14] is a bit too broad, and doesn't sufficiently explain how the work under review deals with the nondifferentiable case. Does it issue a warning? Does it fail silently?  A much more minor comment is that in a couple of places in the paper the authors asser that the solution of convex optimization problems can be obtained "exactly". (e.g., implicitly in line 82, "we compute it exactly") and in line 382. In these cases it would be reasonable to say that these things can be computed "to high accuracy" or something a little more vague, since "exact computation" is a much stronger statement, and for many convex optimization problems we do not know how to exactly compute the solution, and so would not know how to exactly compute the gradient of the solution mapping.   Significance: From a technical point of view the contribution is not that significant---the ideas behind it are relatively simple, or are extensions of non-trivial ideas that have become "normalized" as they have matured. Having said that, I think this paper is of quite high significance in the sense that if it the implementation is done well and is easy to use then (possibly very many) researchers are likely to make direct use of this work.     