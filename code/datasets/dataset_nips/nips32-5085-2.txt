If I understand correctly, the layer is somehow like an auto-differentiation variant, that represents complex but disciplined convex problems as a tree of the composition of subprograms of bilinear, then apply a cone optimizer to generate the solution and differentiating over parameters to get the whole solution path, which is similar to LARS.  I think the implementation is friendly enough for users like me, and I appreciate the efforts to make it a library.  The authors did not mention the concept of the condition number of the convex problems, therefore I am curious whether the inner-most cone program is better-conditioned than the original program, theoretically or empirically.  Besides, I am curious whether this applies to non-smooth subprograms as well, e.g. with $\ell_1$ norm or ReLU function insides, where subgradients have to be applied.