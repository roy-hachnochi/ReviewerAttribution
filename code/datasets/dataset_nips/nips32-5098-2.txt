The paper describes a differentiable renderer for triangle meshes that provides gradients with respect to geometry, texture, and illumination. As a key innovation, the paper claims the formulation of rasterization as interpolation, saying that "In contrast to standard rendering, where a pixelâ€™s value is assigned from the closest face that covers the pixel, we treat rasterization as an interpolation of vertex attributes". However, the formulation in Equation 1 using barycentric interpolation is textbook material in computer graphics. I do not think using this standard approach in a differentiable renderer can be claimed as a significant novel contribution.  The experimental results on single image 3D reconstruction and textured shape generation using only 2D supervision lead to good quality, but they do not provide a significant improvement over previous work on these problems in my opinion.   I consider the fact that this renderer also supports gradients due to illumination and texture, which some of the other public implementations don't (or they focus on specific cases, like spherical harmonics illumination), more as an engineering detail. The state of the art here is the work by Li et al. on differentiable ray tracing, which even supports gradients due to indirect illumination.  The paper should also discuss "Pix2Vex: Image-to-Geometry Reconstruction using a Smooth Differentiable Renderer" by Petersen et al., and "Unsupervised 3D Shape Learning from Image Collections in the Wild" by Szabo and Favaro. Both use differentiable rendering, for single view shape reconstruction or shape generation using 2D supervision only similar as in this submission.  In summary, the technical contribution in the differentiable renderer does not seem significant enough to me for a NeurIPS paper. Experiments are performed on standard problem statements (single image reconstruction, shape generation from 2D supervision) with good results, but previous work achieves quite similar quality.