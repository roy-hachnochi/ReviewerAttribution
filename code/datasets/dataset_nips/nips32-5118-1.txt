The paper presents an algorithm for finite-horizon off-policy policy evaluation based on a new way of estimating stationary distribution correction ratios called Marginalized Importance Sampling. The paper derives Marginalized Importance Sampling, gives a theoretical analysis of the algorithm's sample complexity (showing it possesses an optimal dependence on horizon), and presents strong results on simple MDPs, time-varying MDPs, and the Mountain Car domain.  I recommend accepting the paper for publication. To my knowledge the method is original, the quality of the analysis seems good, the writing is clear enough, and the work seems significant and likely to be extended.  EDIT: I have read the author's feedback.