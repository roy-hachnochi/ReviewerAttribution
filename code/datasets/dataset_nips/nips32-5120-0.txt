This paper studies the contextual bandits with cross learning. Existing literature which does not assume any structure among the contexts establishes a weak O(\sqrt{CKT}) bound. In this paper, the authors assume that the rewards corresponding to every context is observable and prove a O(\sqrt{KT}) bound under two settings; The setting where the contexts are adversarial, but the rewards are stochastic and the setting where the contexts are stochastic, but the rewards are adversarial. The paper also proves a lower bound of O(\sqrt{CKT}) for the setting when the rewards and contexts are both adversarial arguing that cross learning does not help when rewards and contexts are both adversarially generated.   The paper abstracts a number of interesting applications into a new theoretical problem and presents two different algorithms, UCB1.CL and EXP3.CL along with their theoretical analysis. The paper acknowledges that UCB.1CL and EXP3.CL are respectively minor variations of existing bandit algorithms, UCB-1 and EXP3 and claims that their analysis requires significantly new ideas. However, I think the paper fails to convincingly argue that existing proof techniques (for both UCB-1 and EXP3) do not extend to the problem setting under consideration. The paper fails to argue why the “optimism under uncertainty” proof argument (for example see Section 4 in [1]) do not carry over for proving the regret bound of UCB.1CL. Since we observe the rewards corresponding to all the contexts for any selected arm and hence can construct the corresponding confidence intervals, the preceding argument can be used verbatim to prove the desired bound.   [EDIT] After considering the author response, this reviewer no longer holds the earlier concern about the correctness of the lower bound. The reviewer has changed their score accordingly.     References [1] Agrawal, Shipra, and Nikhil R. Devanur. "Bandits with concave rewards and convex knapsacks." Proceedings of the fifteenth ACM conference on Economics and computation. ACM, 2014.