Summary: The paper studies the generalization and optimization aspects of regularized neural networks, and provide two key contributions: (a)they show that a O(d) sample complexity gap between global minima of regularized loss and the induced kernel method. (b). They also establish that in infinite-width two-layer nets, a variant of gradient descent converges to global minimum with of (weakly) regularized cross entropy loss in poly iterations.  Detailed comments:  1. The paper studies a natural and important problem and makes fundamental contributions in this direction. Recent results in deep learning theory exploits this neural tangent connection to prove optimization and generalization results. In light of this, it is important to study the limitations of this. This highlights that recently popular neural tangent kernel view has its own drawbacks and perhaps(?) insufficient to explain the success of modern machine learning.  2. The paper is very well-written with the problem setting and intuitions clearly explained. In particular, the authors explain the intuition behind their lower bound: why the induced kernel method will require more samples.  3. The technical content is solid and rigorous, with novel contributions of tools which could be useful in general. Moreover, the high level ideas are well succinctly explained, for example, the key idea they exploit for the upper bound is showing that global minimizer of regularized loss converges to max margin solutions. Another important technical contribution they remark is a new technique for proving lower bound of kernel methods which is explained done exploiting the fact that the predictor in the RKHS lies in the span of the data. I skimmed through the proofs of generalization, and the arguments seemed fine and well written to me.  4. A good thing is that the authors remark the generality of the result presented; for example, the sample complexity gap is showed to hold for multi-class classification and regression. This answers natural questions on how general are the ideas to extend. The authors also remark these results can potentially hold even when the only regularization is the implicit regularization of optimization.  5. The only section that required multiple re-reads for me is Section 3. Perturbed Wassertien Flow. Perhaps this is because I am not familiar with relevant literature, but I had a feeling that it was rushed and should have been explained/presented better. For example, there is no intuition provided how eqn 3.1 came about or what it means. Similarly, for eqn 3.2, the authors explain what U is but what about the first term? A few more lines explaining this will help readability. This is important because it provides more accessibility to a broad audience as NeurIPS. Moreover, the authors don't give intuition/details of proof techniques for the optimization result in the main paper. I hope they polish section 3 in the revision.  Minor comment: Please explain somewhere why it is called "weak" regularization.