Summary. Generalization bounds on neural nets, based on Rademacher complexity, use the norm bounds on weights of layers, which gives an exponential dependence on depth. Moreover, existing lower bounds show that this is unavoidable (in general). The goal of the paper is to get bounds polynomial in depth by additionally using properties of training data. However, such data dependent bounds comes with challenges, discussed in the paper. The authors introduce "augmenting" the loss function with desirable properties and present tools to derive covering bounds on augmented loss.   Comments: 1. Data-dependent generalization bounds have recently become popular to derive sharper generalization bounds. This paper contributes to this line of work by considering properties of training data, in particular norms of layers and norms of Jacobians of laters with other layers. They paper presents the (novel) idea of augmenting the loss function with the desirable properties, they then derive generalization bounds on the augmented loss.  2. The paper is technically rigorous and the authors develop a lot of tools, in broad generality, which can be helpful in other related problems.  3. Even though the paper is technically heavy with a lot of notation, I found that it was rather well written, with all the main ideas well presented and explained. The authors present the idea in more generality than perhaps needed (which complicates notation perhaps) but I think it would be difficult to simplify.  3. The authors present the abstraction of computational graph (which computes the loss of network) and show that "augmenting" the graph in a   way corresponds to augmenting the loss. They then develop covering bounds on the augmented loss. The main technical contribution is augmenting the loss functions to enforce desirable properties, and also deriving covering bounds. The technical challenges encountered are well described, although it becomes much clear after reading appendix A and B rather than the main paper (more on this below),  4. The presentation gets perhaps very dense in the technical sections, and unable to put forth the key ideas. In particular, the machinery of computational graph, although connected to neural networks, looked overkill to me at first. I then went through the corresponding sections A and B in the appendix to understand it well. While it is understandable that the authors kept a condensed version due to page restrictions, fleshing out sections 5 and 6 better can help accessibility to a wider audience.  5. The results in the paper apply to smooth activations and therefore not apply to ReLU networks which are (arguably) the widely used ones in practice. It would be nice if the authors could discuss the challenges to extending it to ReLU networks.   6. While the main idea of the paper is reiterated: removing exponential dependence on depth, it would be helpful to see how the bound compares to existing results on simple datasets(?). The only experiment they provide in the appendix is on regularizing with respect to their complexity improves performance. It would be more insightful to compare against existing bounds and show (as claimed) that with increasing depth, the upper bound is more meaningful than previous work.  ------------ I thank the authors for the clarifications ; I have read them and would keep my score as it is.