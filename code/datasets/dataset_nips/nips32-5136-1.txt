This paper studies data-dependent generalization bounds in multi-layer feedforward neural networks with smooth activation functions. The authors give a new Rademacher complexity bound, which is polynomial in the hidden layer norms and the interlayer Jacobian norms. Assuming these empirical quantities are polynomial in the depth, the Rademacher complexity bound obtained is polynomial in the depth, while a naïve Rademacher complexity bound (product of layer matrix norms) is exponential in the depth. In the experiment, the authors explicitly regularize the networks’ Jacobians and improve test error.   In the proof, the authors bound the empirical Rademacher complexity by bounding the covering number of the function class. To exploit the data-dependent properties, the authors augment the original loss by including indicators functions which encode the properties. In order to bound the covering number of the augmented function, the authors develop a general theory to bound the covering number of a computational graph. Basically, the authors show that as long as the release-Lipschitzness condition holds and the internal nodes are bounded, we can cover a family of computational graphs by covering the function class at each vertex. The authors further design an augmentation scheme for any sequential computational graph such that the augmented graph is release-Lipschitz.   Overall this is a good theory paper which studies the data-dependent generalization in neural networks. My only concern is on the assumption that the interlayer Jacobian is small. It might be better to identify some scenarios in which we can prove that these terms are indeed small (polynomial in depth). At least, the authors can include more intuitions to explain why the interlayer Jacobian should be small in practice.   Some minor comment: Line 246: “Let R_V the collection” -> “Let R_V be the collection”  ------------------------------------------------------- I have read the authors' rebuttal and other reviews. I will keep my score. 