The paper addresses an important problem in deep learning. As larger-and-larger models are being trained on increasing amounts of data, on-device memory becomes a bottleneck. Efforts which reduce storage of auxiliary information including gradient moments and activations, allow for larger models and/or batches on same hardware. I endorse the paper for acceptance to NeurIPS. The paper is well-written and the results are impressive. I especially appreciate that the authors tested the algorithm on cutting-edge models such as ImageNet, BERT and Transformer MT, and also had an ablation on hyperparameters such as batch sizes (Figure 3b). There are some ways that the paper can be improved:  1. The algorithm section can be improved a bit. As it stands, the algorithm is written a bit too abstractly/generally. This is impeding readability, especially about the covering sets. I'd highly suggest that the authors reduce the abstraction and (for instance) present the algorithm for simple deep networks and then qualifying how this can be generalized.   2. I would have liked a deeper comparison between Adafactor and SM3. The authors do discuss it at the end of Section 4, however, it is unclear from simply reading that paragraph what the differences would be between the two for simple use-cases such as Transformers, ConvNets or Feed-Forward networks.   3. I was hoping that the code for SM3 was included with the paper. It was not. I would encourage the authors to include or open-source that if possible.   4. It is not totally clear what the strategy is with learning rates. The authors state that their algorithm is unlike Adafactor in that it decays similar to Adagrad rather than being somewhat constant (similar to Adam).  Do you reduce the learning rate as you go along? Do you scale the learning rates linearly with batch sizes? Do you warmup the learning rates?  POST REBUTTAL EDIT :   I have read through the rebuttal and the other reviews and maintain that this paper should be accepted. I thank the authors for clarifying my questions.