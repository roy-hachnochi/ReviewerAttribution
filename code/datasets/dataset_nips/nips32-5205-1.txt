This paper proposed a new algorithm called LSGD for distributed optimization in non-convex settings based on pulling workers to the current best performer among them, rather than their average at each iteration.   Originality: using the leader stochastic gradient descent is interesting. Quality: the theoretical analysis is rigorous. Clarity: the paper is easy to understand including the motivating example, and the problem formulation.  They do not compare the convergence rate with existing methods theoretically, which is important to see the theoretical improvement.