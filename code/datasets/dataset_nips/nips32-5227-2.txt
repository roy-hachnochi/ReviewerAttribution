This paper aims to contribute to the theory of transfer learning by making new divergence measures across domains to develop minimax bounds, bounds on sampling cost and covariate shift correction bounds using instance reweighting.   While it is very useful to have minimax theory on the transfer learning setting (in this case covariate shift), it is difficult to parse significance of the theoretical results in the way the paper is written. In particular, the current results lack sufficient comparison with prior work, for example with transductive bounds for covariate shift, such as in [Cortest et al, 2008, Gretton et al, 2009].   Furthermore, there does not appear to be sufficient treatment of the results as to how they depend on various terms. For example, there is almost no discussion on the implications of Theorem 1, 3, 4, 5 and 6.   However, the new quantities introduced by the authors to measure divergence, and the kinds of bounds being derived can be useful if they are put into better context, and more intuition is provided about the theoretical results presented  References:  Cortes, Corinna, et al. "Sample selection bias correction theory." International conference on algorithmic learning theory. Springer, Berlin, Heidelberg, 2008.  Gretton, Arthur, et al. "Covariate shift by kernel mean matching." Dataset shift in machine learning 3.4 (2009): 5.  