In this paper, the authors propose a hierarchical Bayesian mixture of Hawkes processes with a parameter adaptation mechanism based on a meta-learning technique for modeling multiple short event sequences with graph-like side information. In the proposed model, each sequence is modeled by a mixture of Hawkes processes, whose mixture ratio has relation to the adjacency of the sequence to the other sequences. Moreover, the parameters of the component Hawkes processes are slightly varied among sequences using the mechanism of the model-agnostic meta-learning framework. The authors provide experimental results on synthetic and real-world datasets, which show the superiority of the proposed method.  Overall, the paper is very well written. The technical details are explained in an easy-to-follow way, and the proposed method is clearly positioned in the context of event sequence modeling using Hawkes processes.  I am not completely sure about the motivation to use MAML to adapt the parameters to each sequence. What is the biggest advantage of using MAML instead of just fluctuating $\theta_k^{(i)}$ around $\theta_k$ using some regularization terms? Or in other words, isn't it possible to consider the multi-task learning (using graph information) for mixtures of Hawkes processes?  The figures in Table 1 would become easier to interpret if the colors of the nodes in the right three columns are roughly aligned to those in the leftmost column geometrically.  In Table 2, the performance of HARMLESS with MAML, FOMAML or Reptile greatly differ in some cases. Is there any guideline to choose which meta-learning method should be used in general?  -----  [Update after authors' rebuttal]  Thank you for the rebuttal. I read it. The points I mentioned are minor (they are just about clarification), so I maintained my score to be 7. Good luck!