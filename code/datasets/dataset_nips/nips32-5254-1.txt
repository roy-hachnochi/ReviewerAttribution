I appreciate the authors' response about generalization beyond normalizing flows.  I would encourage the authors to add these generalizations in the conclusion or discussion section to help readers see how these results could be more broadly useful.   ---- Original review ---- The novelty primarily comes from deriving an unbiased log density and log determinant estimator rather than using a fixed truncation of an infinite sum, which can be significantly biased. The tradeoff (which the paper mentions clearly) is variable memory and computation during training.  Overall, I think this is a focused but solid contribution and is explained clearly.   The induced mixed norm section could be shortened to a paragraph and the details left in the appendix.  I appreciate including the somewhat negative result but I think a paragraph of explanation and details in the appendix would do fine.  Overall, I didn't find any major weaknesses in the paper.  One small weakness may be that this primarily builds off of a previous method and improves one particular part of the estimation process.  This is definitely useful but the novelty doesn't open up entirely novel models or algorithms and it is not clear that this can be generalized to other related situations (i.e. can these ideas be used for other models/methods than invertible residual flows).  Could you mention what distribution you used for p(N)?  I might have just missed it but wanted to double check.  Also, since any distribution with support on the positive indices allows for unbiasedness, why not choose one that almost always selects a very small number (i.e. Poisson distribution with lambda close to 0)?  Could you give some intuitions on what are reasonable values for p(N)?  Figure 1 should probably be moved closer to the first mention of it.  I didn't understand reading till near the end of the introduction.