This paper provides a novel model named multi-agent common knowledge reinforcement learning (MACKRL) which aims to learn a hierarchical policy tree. Also, the authors provide pairwise MACKRL as an example of one possible MACKRL architecture.  The paper is well-organized and clearly presented. However, there are several concerns that the authors need to further improve: 1. Model Efficiency: As the authors claim, the possible pairwise partitions is O(n!), all possible partitions including tuple-wise cases may reach O(n^n). As the number of agents increases, the model suffers greatly from exponentially-growing model complexity although the authors restrict themselves to pairwise setting.  2. Ablation Study: As the author state in Algorithm 1, the common knowledge is a random seed and participates in choosing partition, deciding action types and taking action. However, the authors do not verify whether the common knowledge is actually working or ignored as the similar cases in Computer Vision fields. The experiments for ablation study are expected to add.   3. Experiments with HRL: As the authors state, MACKRL uses a hierarchical policy over the joint environment action space. Considering several hierarchical reinforcement learning (HRL) methods also deal with the similar task, the additional experiments for further comparison with these HRL methods like FuN are expected.