Summary: This paper considers few-shot classification and seeks to make use of the unlabeled query data during few-shot classification by training on it with a meta-learned critic loss. The algorithm builds on top of MAML, and has two stages. In the first stage, the model is adapted via gradient descent on the labeled support set. In the second stage, the model is further adapted via a meta-learned critic loss that is a function of a featurization of the model parameters and the unlabeled query set.  Originality: The proposed approach strikes me as quite similar to One-Shot Imitation Learning by Domain-Adaptive Meta-Learning (Yu et al. 2018). In that paper, they similarly learn a critic loss, in their case to adapt an RL agent to a human demonstration in which ground truth actions are not available. This citation should be added to the paper. More distant but still quite related is Evolved Policy Gradients (Houthhooft et al. 2018), which meta-learns a loss function for an RL agent.  Quality:  A few clarifying questions: What is the point of Figure 2 (shows the effect of the critic on output probabilities)? The caption is entirely descriptive of the plotted data. In no case presented does the critic actually change the top prediction!  Are the network backbones the same across all comparisons in Table 2? This is very important to ensure fair comparison (see Chen et al. 2019.) In particular, SCA performs on par to LEO - are the backbones the same here? Incidentally, the idea of meta-learning a loss function is general and could be applied to other meta-learning algorithms besides MAML, I don’t see a need to restrict it to gradient-based in line 122 and other places.  Clarity: The writing is clear for the most part (Sections 3 and 4 are a bit long-winded). Figure 1 is very helpful. It would be good to explain what you *expect* the critic loss to learn, for motivation.   Significance: low/medium - mainly due to concerns about the validity of the results, additionally due to the overlap of the idea with previously published works.  ----------------- Post-rebuttal ------------------  Thank you for clarifying the comparisons in Table 2. I feel confident now that the most important comparison (with regular MAML) is correct. I also appreciate that while it’s good to contextualize the results with respect to non-MAML based methods, this is not critical to prove the point about transduction. I’m satisfied the authors did their best in this regard. However, I would like to say that I strongly disagree with the statement, “It is fair to compare methods on the quote results on the same benchmarks.” Network backbones and training techniques improve over time, therefore it’s unreasonable to compare directly with numbers reported in older few-shot papers that built their algorithms on top of what are *now* antiquated methods. It is important that as a community we do not waste time chasing incremental improvements that are revealed to be an illusion when an older method is ported to modern times.   I still feel that Figure 2 could be improved. Perhaps an analysis of how often the critic changes the predictions, or in what specific cases? “It doesn’t do nothing” is a pretty low bar for your method…  Thanks for agreeing to add the requested citations. I agree that your contribution is distinct. I do think that formulating transductive meta-learning more broadly to include meta-learning besides gradient-based approaches would make the paper more impactful. 