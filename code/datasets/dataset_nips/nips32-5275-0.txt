This review has 2 parts. The first part is my review of the paper as a standalone paper. The second part is a meta-commentary unifying my reviews for both this paper and "Neural Tangent Kernel for Any Architecture".  ++++++++ Part 1 ++++++++++++++   This paper demonstrates that infinitely-wide architectures made from a range of building blocks are Gaussian processes.   Fundamentally, the paper seems to have two core contributions.  (1) The paper collapses a wide range of operations (convolution, pooling, batchnorm, attention, gating, as well as the inner products for the actual GP Kernel computation) into the matrix multiplication / nonlinearity / linear combination framework. (2) The paper presents a mean field theory of tied weights, which allows a rigorous extension to RNNs as well as a rigorous integration of the forward and backward pass.  This paper is a clean, elegant and logical next step in an important research direction. It feels to me almost like a book chapter.  There is only 1 experiment demonstrating that theoretically computed GPKs correspond to practical values. I think you should include experiments that cover all the layer types your theory covers.  I would include LinComb+ in the main paper. This is a significant extension that I believe readers should be aware of as it greatly expands the design space for new layer types.  I think Corrollary 5.3 is unnecessary. At this point in the paper, it is obvious that you express neural networks of standard architecture as Netsor programs. You don't need to spend a third of a page to reiterate this fact. In fact, I would get rid of section 5 altogether and put all the theory into section 7.  I think section 4 and 6 should methodically educate readers on how to a) translate their network into netsor and b) compute the GPK given pairs of inputs and a netsor program. Equation (2) should be included in there as well. I think you should include the Netsor "implementation" of as many building blocks as possible in the main paper, not just batchnorm / convolutions. I believe those transformations are actually the most important contribution of your paper. I also wouldn't title section 6 "More examples" as I think its a severe understatement. The transformations are not just examples, but important insights.  (To avoid repeating myself, I also refer to my comments on the NTK paper. Since I wrote that review first, it is a bit more extensive but many suggestions also apply to this paper.)  ++++++++ Part 2 ++++++++++++++   I believe that this paper and "Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes" are close to being dual submissions. I believe the policy with regards to similar papers by the same author is that each submission must have substantial independent contributions of the other. Formally, let A and B be papers of the same author under submission. Let contribution() be the set of contributions of a paper. Let T be the threshold of contribution magnitude a paper needs to meet to be accepted. Then as far as I can tell the dual submission policy states that paper A can only be accepted if |contribution(A) \ contribution(B)| > T and paper B can only be accepted if |contribution(B) \ contribution(A)| > T.   Contributions that are unique to a single paper are:  - the class of nonlinearities used / the presence of matrix transpose in Netsor - the experiments presented (However, these experiments are very small in number and also substantially similar.)  I believe that this does not meet the threshold. So based on the standard as outlined above, I would reject both papers.  However, I somewhat disagree with the dual submission policy as stated. I believe that paper A should still be accepted if |contribution(A)| > 2T and paper B should still be accepted if |contribution(B)| > 2T, as long as the duplication of material is not gratuitous. And I believe that both papers in this case meet that stadard narrowly. Hence, I will give 6 ratings to both papers.  Finally, let me add that I believe the authors could have done a much better job at adding unique content to each paper. NeurIPS enables authors to submit companion papers in the supplementary material. All the content in the NTK paper that is duplicate in the GP paper could have been summarized as "previous work" that reviewers can access in the companion paper. This would have left space open for unique contributions, such as a much larger number of experiments.     ############ Post-rebuttal comments ##########  Based on reading the other reviews, a consensus has emerged that the NTK paper should be "punished" for overlaps with the GP paper but that the GP paper should be evaluated based on its own merit. Using this standard and taking into account experiments given in the rebuttal, I change my score to an 8.   