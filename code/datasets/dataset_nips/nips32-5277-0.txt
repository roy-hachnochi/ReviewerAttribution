This paper proposes a neural network architecture that allows fast computation dimension-wise derivatives (regardless of dimensionality). Proposed modification in the computation graph leads to diagonal Jacobians (as well as higher order derivatives) that can be computed in a single pass. The model also generalizes a number of recently proposed flow models, which makes it a significant contribution to this line of research. The authors show that their idea can be combined with implicit ODE methods, which results in faster convergence rates in stiff dynamics. Continuous-time normalizing flows is also improved by computing the exact trace thanks to the proposed framework. Finally, DiffOpNet is proven useful for the inference of SDEs, which is known to be a difficult problem.  The paper is very well-written and easy to follow, I really enjoyed reading the paper. The proposed idea seems to be pretty straightforward but also well-thought and developed throughout the paper. In addition to the idea, the paper is also novel in the sense that it brings together the recent advances in automatic differentiation and a neural network construction. I believe that we will be seeing more papers taking advantage of AD graphs in the near future. Hence, this paper, being one of the firsts, should definitely be accepted.  Comments on the results: - Results in Figure1 is intuitive and really impressive but I do wonder how you identified the stiffness of the ODE systems. - I am particularly impressed by the improvement upon CNF in Table1.  - Did you explore the SDE systems having d>2 dimensionality?  - One restriction of the SDE inference is that the diffusion is diagonal. Would your framework still be used if it weren't? - Could you comment on the execution times? - Could you give a reference to your claim that setting k=5000 and computing (13) is the standard way of computing NLL? Replies to the above questions could also be included in the paper.