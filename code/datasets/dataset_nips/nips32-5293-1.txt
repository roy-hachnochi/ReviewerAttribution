The term "latent natural language instructions" appears many times throughout the paper. The problem is that the instructions aren't latent: they're provided as supervised training data, and modeled as such. See the Andreas paper Learning with Latent Language (which you might want to cite) for why the language signal is latent in their approach.  A similar comment applies to the idea of a "latent plan". Since the plans are represented in natural language, and provided as supervised training data, the plans aren't latent either.  I liked the fact that the instructions "contain a number of challenging linguistic phenomena", and the examples provided clearly showcase this aspect of the dataset.  Overall the paper is well-written, but there are some non-native phrasings which could be improved for the next version, eg "If an enemy unit goes out the players visibility"; "embed them in isolation with the 4.1.3." (can't use a section number like this)  It would be good to have more details of the RNN encoder. Is it a vanilla RNN, LSTM?  Overall there were a number of aspects of the experimental setup and the agent architecture where I had to work hard to ascertain the details. One example is the fixed set of instructions, and where this set comes from. The caption in Table 3 says N most frequent. This is a detail that needs stating earlier in the paper and in the body of the text.  As far as I can tell, the RNN is never used to autoregressively generate instructions, eg using a beam search at decoding time: the generative model is only ever used to rank a subset of the instructions from the training data. It would be interesting to see what happens when the agent can generate instructions, especially some not seen in the training data. It would also be good to see some examples of such generated/sampled instructions (perhaps in the appendix).  It would be useful to at least have some idea in the main body of the paper what the rule-based bots are like and how they are implemented.  Section 5.3 has very little content. This space could be used more productively, either to provide some of the details alluded to above, or to provide a more informative analysis.  The suggestion for future work of using RL and generating novel instructions is intriguing. Either would improve the current paper, although the use of RL could reasonably be left for another paper I think. The generation of novel instructions might go in this version, and if not I would be clear early on in the paper that the generative models are only being used to rank the set of instructions from the training set.  Overall this is a promising paper, with an interesting and potentially useful dataset, but which isn't quite ready for a top-tier ML conference such as NeurIPS.  Comment after author response: thanks for reading the reviews and responding accordingly. The authors comment that the language is clearly latent at test time, but this has to be true, otherwise there's nothing to predict (as far as the plan is concerned). My understanding of "latent" is that it typically refers to a random variable which is not observed during training.