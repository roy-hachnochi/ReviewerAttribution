Originality: The main contribution of the paper is to propose a structured representation for video prediction models based on extracting keypoints from images. Models that extract keypoints from images had been proposed before, and here the authors propose an extension of those ideas to video. The paper also has experiments to empirically analyze this representation, which is often lacking in other video prediction papers, despite the fact that learning representations is one of the main motivations for video prediction.  Clarity: The paper is well organized and clearly written.  Quality and significance: The experiments are sound and properly assess some of the points made by the authors.  I believe there are some issues/typos with the model formulation. The likelihood term in equation 2 should not include x_t in the condition (otherwise p(x_t|x_t, other RVs) is trivial. Similarly for equation 4.   I also think that the claims that this model avoids compounding errors and that it has efficient sampling are a bit misleading/not properly supported.  As they are described at the moment they are not particular to using a keypoint representation. Instead, they are caused by the decoupled training of the encoder-decoder and the VRNN dynamics model, but this could be done also with unstructured frame representations.   I also think that there should be an ablation of the different extra losses in the main paper. The experiments show an improvement using the 'best of many' technique. In the supplementary material figure S1 shows that there's a slight improvement for downstream tasks using the different losses. However, this should be quantified for video prediction too. Also note that some of the extra losses (sparsity of the representation) could be adapted to unstructured representations, and I wonder how would this change the results for downstream tasks.  As for the experiments and their results, I think some conclusions extracted by the authors are a bit unjustified. For video prediction (figure 3), in FVD the model has a clear advantage, but not so much in terms of VGG cosine similarity, where it seems that the model's performance has a lot of variance (best in terms of closest sample but worse than baselines in terms of the furthest sample). Note that at sampling time without ground truth having such high variance could mean that some samples are very poor. While the authors argue that this is a sign of increased diversity, this is not trivial nor properly supported. In practice it means that some of the samples generated by the model deviate quite a lot from the ground truth. Furthermore the authors do not compare to other contemporary state of the art methods such as SAVP [13], which obtains significantly better FVD scores than the SVG baseline. It would be interesting to include such comparison.  In general, despite the above issues, I believe they do not significantly alter the conclusions from the experiments and therefore I am in favor of accepting the paper for its novelty and positive results.  Minor notes:  Figure 7 graph 1 has wrong x/y limits, the lines go out of the plot. line 126 typo: 'ideally, the representation should (missing verb) as few keypoints...'  ------------- POST REBUTTAL UPDATE ---------------------- I read the rebuttal and the other reviews. Some of my comments such as the one regarding compounding errors have not been addressed, however I still believe the paper is a good contribution and keep my rating.