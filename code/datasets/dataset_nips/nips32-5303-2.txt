#### EDIT after feedback #### I have read the feedback.  Comparison with L^2 baseline would be a nice addition. The response also helped understand the role of Section 4. ######################### This paper presents LSH schemes where each data point is regarded as a probability distribution (on a discrete random value) and measured by f-divergences.  The LSH for a certain f-divergence is approximated by another f-divergence that gives an LSH family.  In concrete, the paper bounds several divergences with the squared Hellinger distance which gives the L^2-LSH.   My comments and suggestions are * Experiments It is nice to see the empirical validation of the approximation guarantee as well as the performance of k-nearest search.  My concern is that the comparison with existing LSH families lacks in Figure 2.  L^2 LSH would be a reasonable baseline.   * Section 4 This part is rather independent of the rest of the paper.  While mutual information loss is motivated for compression tasks, the described algorithm is not readily connected with that task.  Experiments are not presented either.  What is the strength of this hashing scheme?  * Proposition 1 While Proposition 1 constitutes the core to develop LSH schemes for several f-divergences, the claim is tough to follow at first glance.  If possible, some visualization helps the understanding.  For example, how the bound L \leq f/g \leq U connects (r1, r2, p1, p2)-sensitive family with (Lr1, Ur2, p1, p2) one.   