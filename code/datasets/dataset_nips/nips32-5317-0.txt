Strengths: A well-written paper, with natural flow of reasoning. First, problems with black-box approaches are pointed out. Then it is shown that if the algorithm for well-behaved functions converges, then it converges to the local minimum a.s. Then it’s shown that the algorithm converges a.s. Appendix: boxes with the current goal really help to follow the paper.  Weaknesses: -- Experimental section seems redundant. It doesn’t contain precise data which one can use for comparison with other approaches. Convergence to a local minimum is proved, there is no need to show this experimentally. -- Why is parameter \beta needed? It is always present together with either B or g, and therefore can be removed. -- Section B.2 of the appendix is devoted to the proof that a choice of h_0 can be arbitrary, not necessarily random. It would be helpful to provide a motivation (why is this fact useful), since from practical point of view it’s not a problem to initialize h randomly. -- Line 260: “In a typical zero order approach, one could resort to expensive line searches to determine the appropriate value of the gradient approximation accuracy in each step which guarantees the decrease of f”. A reference or other clarification should be given.  Suggestions:  -- Briefly describe black-box approaches (FPSG, ZPSG) in appendix, before proving Lemmas 6 and 7. -- For proofs and theorems in appendix, which are similar to [JGN+17], it may be potentially better to specify this similarity (e.g. “Similarly to [JGN+17] we establish the following result”) -- Algorithm 1: why special symbol \bot is needed? It can be replaced with x_t. 