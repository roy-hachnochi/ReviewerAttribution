Updated Review: Thanks to the authors for clarifying the stopping criteria for meatier synthesis.  My review for this work remains the same following author response because I believe that the authors have demonstrated this work to be of high quality and relevance to the NeurIPS community.  Originality: Although the algorithms used to synthesize the metamers themselves are nothing new, the work is a novel combination of previous approaches and techniques, and the analysis approach gives these methods a fresh perspective that leads to good insights.  Quality: The work is of high quality and is a complete piece of work that will advance our understanding of the relationships between architecture, task and training in determining representational similarity between networks and humans (as well as between networks).  Clarity: The paper is overall clear, though some details could use a bit of clarifying (what was the threshold for satisfactory termination of synthesis?  How was this determined?)  Significance: This work builds on theoretical and experimental work from neuroscience used to analyze how well models of perceptual systems capture the representation within the human brain by synthesizing stimuli that match the responses of some part of the model completely and using human subjects to validate that the original and matched stimulus are in fact the same.  The authors adapt this test for use in comparing the representations in neural network models trained on complex tasks with human representations helping to bridge the two fields.  This work also builds on a string of recent work attempting to find similarities and differences between our learned model representations and human perceptual representations, and adds a critical analysis tool for understanding network deviations.  The test stated here is actually looser than the original metamer test, which required not that the matched images or sounds were classified the same (which is permissive of some perceptual deviation or distortion), but were perceptually indistinguishable.  Nonetheless, I believe this is a useful tool for comparing representational similarity, both between networks and humans, and between different types of networks and allows the authors to extract meaningful empirical contributions isolating architectural and training modifications that bring the representations “more in line” with human representations (accounting for aliasing in architecture, trained vs untrained networks, and training on specific databases and tasks vs others).  Followup work with this technique could help identify a wider corpus of tasks that lead to metameric similarity between networks and humans and can lead to deep insight about the goals that human perceptual systems are optimized for. 