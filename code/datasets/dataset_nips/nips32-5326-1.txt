Updated after author response ---------------------------------------- Thanks to the authors for their response. In light of their points, and the other reviews, I have increased my score to a 7. Please include the random seed ImageNet results in the final revision!  Original review -------------------- Overall, I enjoyed reading this paper. The authors provide a very thorough set of experiments that investigates the use of metamers for probing artificial neural networks. I appreciated that the authors took the time to carry out the human experiments, generated metamers for networks both in the visual and auditory domains, and tested multiple architectures in each setting.  Major comments: While the methods are solid and the results comprehensive, I have some questions regarding the interpretation of the results.  The main finding seems to be that metamers of deeper layers of artificial networks are not perceptually recognizable by humans. My reading of the paper is that the authors interpret this as a failing of artificial networks as a model of human perception. For example, they state on lines 38-39 that they "leverage the idea that metamers for a valid model of human perception should also be metamers for humans". I disagree with this assessment, for a few different reasons: - First, there may be many purposes for models of human perception, not all of these require that the model match human perception exactly (i.e. all models are wrong, but some are useful). - Second, the authors are comparing networks trained to perform object classification (a specific visual task) or word recognition (a specific auditory task) with the entirety of human visual and auditory perception, which needs to support a multitude of perceptual tasks relevant for behavior. For example, in vision, perhaps a better comparison would be to test whether the metamer is sufficient for just the ventral visual pathway (although I understand this is experimentally infeasible). Regardless, I do not see why we should expect that networks trained on a single task should generate perceptual metamers recognizable by humans; as these networks are only trained to solve a very specific subset of what the human perceptual system is required to do. - Third, I want to offer a different interpretation of the results. Imagine a hypothetical experiment where one is able to perform the same optimization (synthesizing a metamer) for a particular human, by optimizing it to match the activations in some part of the brain (pretend we can access all of the relevant neurons and synaptic weights in that brain). Would the synthesized metamer be recognizable by other humans? I am not so sure--perhaps optimizing the input for a particular brain would make it unrecognizable to other brains. The analogy in artificial networks would be to see if synthesizing a metamer for one network are recognizable by other networks. Indeed, the authors did some of these experiments--and find that they are not! So perhaps expecting metamers to be recognizable across networks (either human or artificial) is too stringent a requirement.  Minor comments: - The first paragraph of the introduction discusses some of the ways in which deep networks have been shown to be different from human perception. The paragraph ends with the statement that "the consequences of these discrepancies ... remains unclear". However, it feels to me that this paper does not address this question, instead, it seems to add more discrepancies (in the form of metamers), but does not take on the question of understanding the implications of those discrepancies. (I would simply suggest rewording the first intro paragraph to focus on what the paper focuses on). - The authors comment on difficulties associated with matching synthesizing metamers for units after ReLU nonlinearities. Instead of the proposed solution, why not simply target the unit activations *before* the nonlinearity?