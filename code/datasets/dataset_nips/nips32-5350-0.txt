---------After Rebuttal--------- After reading through the rebuttal and considering the other reviewer's comments, I have decided to raise my score to lean towards acceptance. In the rebuttal, the authors have run additional experiments demonstrating that their method does outperform augmentation pre-training baselines by a reasonably large margin. However, I still think the reliance on these explicitly defined set of transformations leaves something to be desired. Also, I don't quite understand the last point of their rebuttal "for videos we do not do augmentation" when Figure 5 in their supplementary says "Example of the training data and the augmentation function A for video."  ---------Before Rebuttal--------- The ideas presented in this work are clear and the algorithm is succinct. For the proposed method, one concern is the actual complexity of the components involved in meta-training. The approach of using Auto-Augment within UMTRA would suggest significant computational overhead for a new dataset. Also, the number of potential hyperparameters in this extension of MAML in addition to the original hyperparameters introduced by MAML would make this method unwieldy to use in practice. Both these issues reduce the practicality of this idea and harm the significance. Also, the overall originality is somewhat limited as it is a fairly direct combination of data augmentation with meta-learning.