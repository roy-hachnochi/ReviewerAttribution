======= AFTER REBUTTAL ========= I will raise my score after the rebuttal. I feel that the new dataset, the video results and the precise responses really helped with the clarity.  ========================  Abstract: -> I’d recommend against putting specific method names and comparisons into the abstract because this makes the abstract not age very well as methods change and improve. It also makes the contribution seem fairly incremental, rather than providing a new insight.   Introduction:  -> I would recommend against calling these auxiliary tasks. The meta-training tasks are not what people would typically think of when talking about auxiliary objectives.  -> minor: “same supervised learning format”. Not clear what this means, maybe reword? -> As a reader, mentioning that 2 other methods did the same thing recently doesn’t give me any additional insight in the introduction and doesn’t really add value there. It should be discussed in the related work and perhaps in the method section, but making it so prevalent in the intro and the abstract aren’t adding significant value in your contribution.  -> I think a more detailed discussion of why the right comparison points are unsupervised representation learning algorithms versus other things is going to be helpful.  Related Work:  “We need to point out  however, that the meta-learning phase uses custom algorithms in these approaches as well” -> I don’t understand. I thought the whole point was to not use custom algorithms, and you say so yourself. Why the contradiction?           -> You mention CACTus and AAL. Can you discuss detailed comparisons and relationships in the related work?  -> Related work which was missed from RL: https://arxiv.org/abs/1806.04640 -> The meta-testing performance is going to critically depend on the overlap between the imaginary task distribution and the testing distribution. Would help with clarity if this was mentioned early in the paper.   Method:   “with samples drawn from the same distribution as the target task” -> Would help to formalize this? What does it mean for this to be drawn from the same distribution. Is there a distribution p(x) on images x?  -> Minor: What is $N$ in the method? -> Minor: What is $K$, why do we need $NxK$. These should be clarified with a preliminaries section.   “The exact labels used during the meta-training step are not relevant, as they are discarded during the meta-training phase.” -> Minor: This line should be reworded to be more clear.   -> A preliminaries section would be really useful to clarify the notation. For instance, the term "target learning phase” is used in many places but not clearly defined.   -> I think the choice of augmentation function might be more than just a hyperparameter. It seems very crucial to the performance of the algorithm. For instance, if we were classifying boats, just rotating or translating may not give us the validation we want in general. We may need something that looks significantly more different, for instance a different boat altogether. Can we potentially provide any guarantees on generalization here when the augmentation function is very domain specific?  -> The rationale in 170 seems like a bit confusing to me. Why is it unbiased? Could the authors provide a bit more clarity?  Experiments:  I liked the control hyperparameter setting that the authors use for UMTRA evaluation.  The additional comparison of the effect of UMTRA augmentation range on the performance is pretty interesting.   The improvement on omniglot is pretty striking, and the relatively much smaller improvement on mini imagenet suggests that perhaps the augmentation in one is much easier to do effectively than the other.   I think the overall experiments section is pretty strong, with impressive results on omniglot, and weaker results on mini-imagenet.   I think the algorithm is in many ways somewhat simplistic, and is very strongly dependent on the choice of augmentation. This can be pretty nontrivial for more challenging domains, and a more general discussion of this would be important, as well as perhaps results on another domain to showcase the generality of the augmentation method across domains.  