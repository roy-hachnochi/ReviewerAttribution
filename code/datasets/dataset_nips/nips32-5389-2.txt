The main contribution of this paper is a manually curated dataset of functions determining if a function is vulnerable or benign. The novelty here is that there is no bias introduced by either assuming that most of the data is correct (assumed by anomaly detection works like e.g. [19]) or encoding the bias of an existing static analyzer. The evaluation results on this datasets, however, are not convincing for practical application of the resulting classifier. The training data has similar number of vulnerable and benign graphs, while practical programs have much lower percentage of vulnerable functions than the accuracy of the classifier. Thus, accuracy in the 70-80% range is not practical and likely its output in practice will look like pure noise (if 2 out of 100 functions are vulnerable, a classifier with 70% accuracy will give on average 28-29 false positives and has non-trivial chance to miss a vulnerability). This means that the classifier needs significant changes. Furthermore, the baseline to which the paper should compare is static analysis, not other neural architectures.  Technically the paper is almost a verbatim copy of the architecture already proposed in [19] for prediction of (variable) names in programs. The edge types described in Figure 2 are also quite similar to the ones in [19]. Unfortunately, the writing of the paper makes it appear as if these architectural details are contributions of this work.  The evaluation also shows that the neural network captures program-specific features that do not transfer between programs. The combined accuracy of the classifiers is lower than most of the individual classifiers. This means that collecting more training data will not help, but may actually harm the classifiers. It also jeopardizes the paper claims of capturing semantic features.  minor: pg.6 s/non-CFC/non-VFC/  - UPDATE -  The authors have mostly answered my concerns with the extra experiments. The accuracy metric gives little information about the recall or the precision of the learned code analyzer. It would help to provide precision instead of accuracy in the evaluation.  In terms of differences from [19], partial control flow is also encoded there. Also other papers encode control flow. It would help if the paper focuses more on empirical evaluation (as with the provided extra experiments) and introducing the new task, because the actual graph architecture is hard to differentiate from prior works.