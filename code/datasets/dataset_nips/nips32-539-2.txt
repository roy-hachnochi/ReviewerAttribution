The paper presents impressive experimental results. The promises of high efficiency and good performance of the proposed method are certainly kept. For me, this combination of efficiency and good empirical performance is a main benefit of the proposed approach, which could make the proposed method interesting for a wider audience.   Having said that, I still have some criticism about the paper.  Regarding the experimental evaluation, I am missing ablation studies that would further investigate the effects of different design choices. This includes at least the following: - the effect of normalization vs. no normalization (L146-154) - the effect of the scale of voxelization, i.e. the voxel size before convolutions (L155-160) - devoxelization: the relative performance of hard assignment vs. trilinear interpolation (L166-168) - feature fusion: the detailed effect of residual addition vs. concatenation - the number of convolutions -- do we need the same number of voxel convolutions as previously point convolutions to be maximally effective (since the receptive field size will differ)?  The paper currently only presents one version of the proposed approach, but a better understanding of those parameters and design choices is needed in order to effectively use the method in practice.  In particular, I am not convinced the point cloud normalization will always be beneficial, since the normalization invariably loses information about the metric scale of objects. (Imagine, for example, two scans from the KITTI dataset: one where the recording vehicle stands in front of a wall and only perceives a depth range of 2-5m vs. a standard view along a street with a depth range of 2-50m. When normalizing both point clouds to the range [-1,1] and voxelizing them with a fixed voxel resolution, the voxels will correspond to very different metric sizes).  Also, I would be particularly interested in the relative performance of the different options for devoxelization. While I follow the argument that both voxelization and devoxelization are differentiable, the nearest neighbor assignment version of voxelization/devoxelization will exhibit discontinuities if points are close to voxel boundaries. This could potentially lead to bad gradient flow and negatively affect network training.  Regarding the indoor semantic segmentation experiment, the paper only reports results on S3DIS. In the 3D SemSeg community, S3DIS is considered outdated (and problematic because of differing evaluation protocols between papers). Instead, it is better to use ScanNet2, which allows for better controlled evaluation.   Please comment on the points above in the rebuttal.    Finally, I have some criticism about the writing and paper presentation  - I found the writing style of the beginning of the paper a bit repetitive. By the time I had reached the end of Sec.3, I had the feeling that I had read the same sentences 3 times over.  - The analysis of voxel methods only considers regular voxel grids. What about octree representations? They are far less memory intensive, and they should result in a more regular memory access pattern.   - The experimental evaluation only considers baseline methods with regular voxel convolutions. [7] have shown that submanifold sparse convolutions are extremely effective -- with a variant of [7] currently leading on the ScanNet2 SemSeg benchmark. Yet, sparse convolutions are neither discussed nor compared against.   Update It seems like we're all in line with our assessment of the paper.  In my view, the rebuttal does a good job of clearing up the remaining points. I especially appreciate the detailed ablation studies.  I will increase my score to Accept as a consequence.