I’m not very familiar with recent approaches to explanation methods in ML, therefore my low confidence. 1) I am not entirely convinced that an amortized explanation model is a reasonable thing to consider; this could possibly stem from my incomplete understanding of the major use cases of an explanation model. I imagine it to be most useful in practice to investigate outliers / failure cases of the system in question. If this is correct, the explanation method does not necessarily need to be very fast, as it’s only used in rare failure cases. Furthermore, (under this use case assumption) the explanation model would only be useful if it matches the true model on (catastrophic) miss-classification examples; this is inherently hard to guarantee if this is not taken into account during the design and training of the explanation model. A straight-forward computation of feature importance on the ground truth system would be more appropriate in this case I presume. 2) Eqn (5): This definition of feature importance only considers contributions of features towards the ground truth label. It does not attribute importance to features that potentially massively change how the model goes wrong, as long as the error stays the same; eg on ImageNet it would not assign importance to pixels that change the prediction from one wrong class to another, although intuitively this could be an interesting piece of information to debug a classifier. Could the authors comment on this choice? 3) I might be misunderstanding the evaluation of the feature importance uncertainty in section 4.2. Why can the authors not compute the ground truth feature importance on the test set and check is the predictive uncertainty is well calibrated to these held-out values? Ie why is the rank-based method necessary? 