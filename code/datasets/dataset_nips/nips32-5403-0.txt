This paper is very inspiring. Slow convergence and sample inefficiency remain challenging problems in RL, especially when handling continuous and high-dimensional state spaces. This paper applies a general method that drawn from regularized Anderson acceleration (RAA) to accelerate the convergence or improve the sample efficiency for the model-free, off-policy deep RL.    Overall this paper is well constructed and improves the problem from a novel point of view (they make the observation that RL is closely linked to fixed-point iteration). The results show that their approach substantially improves both the learning speed and final performance of state-of-the-art deep RL algorithms, and the literature review shows that the author is knowledgeable in this field.    Here are my major concerns:   I would suggest adding a description about policy iteration is missing in 3.2 since value iteration is a special variant of policy iteration.   Some minor suggestions: Line 116 to 125: specify m > 1. Line 149: Besides l2-norm, do other methods (like l1-norm or lâˆž-norm) works effectively here? Typo on Appendix ??: line 151, line 191, line 239, line 245, line 286, line 294.