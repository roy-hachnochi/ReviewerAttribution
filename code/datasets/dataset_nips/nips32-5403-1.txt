The main contribution of this paper is to apply Anderson acceleration to the setting of deep reinforcement learning. The authors first propose a regularized form of Anderson acceleration, and then show how it can be applied to two practical deep RL algorithms: DQN and TD3.  Originality: This paper falls under the vein of applying existing techniques to a novel domain. While the idea of introducing Anderson acceleration to the context of RL is not new, as the authors mention, it has not been applied to deep RL methods. While the originality is somewhat limited in this aspect, developing a practical and functional improvement for deep RL algorithms is not trivial.  Quality: The paper is technically sound, and the experimental analysis is fair and supports the main thesis of the paper. I like the fact that introducing RAA results in similar performance gains on both TD3 and Dueling-DQN. This is promising from a reproducibility standpoint, as it is more likely that the performance gain from RAA is real and meaningful, and not simply resolving an odd quirk found in a particular deep RL method.  Clarity: The paper is well-written. The background on Anderson acceleration is clearly explained, as well as the motivation and extension to the deep RL domain.   Significance: Developing good and stable off-policy RL algorithms is an important area of research. This work proposes a well-motivated modification to existing off-policy algorithms which appears to be simple to implement and has the promise of moderate performance gains. I believe this work will be of interest to the deep RL community.  Minor typos and clarifications: - Appendix links are broken (i.e. Lines 191, 239, 286, 294, and more) - It is not immediately clear what value of "m" was used for the experiments. I assume "m" corresponds to the "number of previous estimates" hyperparameter, but this could be made more explicit.  Additionally, there is some informal language used in the paper. I have proposed some minor modifications: Line 35: "this mapping is essentially a kind of fixed-point problem" -> "Iterating this mapping results in a fixed-point problem" Line 173: "may make the iteration get stuck" -> delete?