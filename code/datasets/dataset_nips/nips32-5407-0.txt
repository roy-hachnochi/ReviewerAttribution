The paper presents an extension to the Neural Processes (NP) model for dynamical systems. In more abstract terms, whereas the standard NP is a model of functions which can be defined on arbitrary domains X, in the proposed model the function domain is assumed to contain a dimension which evolves with some sequential model. The authors demonstrate the model on GP functions with dynamic kernels, and on images of dynamic 3D scenes.  The paper presents an interesting idea which can be very useful in practice when modeling data that contains dynamic elements. The presentation of the model and background is written really well.  The main issue in the paper is the lack of details in the experimental section, and in regards to the what the authors call the "transition collapse".  Another issue is that the paper does not discuss the main drawback of their approach, namely that conditioning the model on a sequential domain rather than an arbitrary domain, restricts the predictions of the model to be sequential too.  In contrast to the standard NP the proposed model cannot make "jumpy" predictions far in the future, or prediction of the past given the future. This is OK but needs to be clearly mentioned.  The missing details are: 1. What is the transition model for the kernel in the regression task? 2. How was the NLL in the regression task computed? On held out sets? how many examples were used? 3. How was time used in the regression baseline? Was it part of the context encoding? Without this detail I'm not convinced the comparison is fair. 4. What is the motion model in the 2D images? What is the action policy in the 3D model?  5. The author mention the observations in the 2D data are 64x64 pixels. What is the size of the full image? 6. How were the actions and time encoded in the baseline's context (not only the query)? Appendix c mentions something about a backward RNN but that was not clear to me. 7. The description of the "Performance Metric" is inconsistent. NLL is not equivalent to the MSE of the mean. NLL is intractable but ELBO should be a decent proxy (roughly MSE/var + KL). Figure 3 says NLL and line 279-286 describe recall-MSE - which one is used?  The discussion of the 'transition collapse" is interesting but it's hard to learn something without more empirical demonstrations and analysis: 1. Why is the PD loss annealed in this linear way? I would think a more natural way to implement annealing would be y varying T-tilde. 2. What was the alpha used?  3. What is the crucial difference between the two 3D datasets that makes PD vs. no-PD behave differently? 4. Can a toy example be constructed that shows exactly when "transition collapse" occurs and when dropout becomes usefull?  I realize that it is hard to fit a full analysis of this in an 8 pages paper, so perhaps it would be better to treat this as a training detail (which should still be mentioned and discussed briefly. maybe having some analysis in the appendix), and add more space to describe the  sequential vs. baselines experiments.   Minor comments: Line 53: This is the first time CGQN is mentioned. Perhaps the place to give the full name and citation. Line 68-69: This sentence seems to be reversed. Shouldn't this be "condition the observation model on the context"? Equation 6: If I understand correctly the Q's of the expectations should be the same as in equation 5. i.e. Q_\phi (z_t | z_{<t}, V).  ============== Update: I thank the authors for their response. I am upgrading my score to 7 as all my concerns have been addressed in the response.  I think that incorporating all the experimental details that were missing, and adding more insight about 'transition collapse' as proposed in the response will make this a good paper .