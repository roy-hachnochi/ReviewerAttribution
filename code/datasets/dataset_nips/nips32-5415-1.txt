The proposed method extending hindsight experience replay with k-step inverse dynamics learning is original and significant. While there is a limitation that this paper assumes a deterministic environment and a goal space that is a part of the state space and on which one can define a test function, there exist many challenging scenarios that satisfy these assumptions. For example, this paper and the original hindsight experience replay made a similar assumption but, it showed its potential for challenging continuous control tasks.  The paper is clearly written and the intuition is easy to understand. I have minor questions about the experimental setting, especially about the choice of maximum K. In theory, the maximum K should correspond to the maximum number of steps required to reach the goal, but the experiment in the grid world used maximum K=5 and the maximum K is not specified in the OpenAI Fetcher environment. Since choosing the maximum K seems important for this algorithm, clarifying the used maximum K and explaining how small K still provides improvement might be necessary.  * After author response  I increased my rating after the author response.  The only concern I mentioned in the review was about the effect of K in different scenarios. The author response effectively addressed this concern by 1) providing an intuitive explanation on how a model with small K can still improve performance, and 2) showing an additional experiment controlling K in a continuous control environment. I believe the explanation and experimental result would be useful to understand an important characteristic of the algorithm: the effect of K. Therefore, I would recommend having the results in the final version.