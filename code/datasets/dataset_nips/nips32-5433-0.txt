The paper proposes a natural combination of two methods in FSL and SSL, (namely the (good) MTL and the (basic) self training, respectively), to address the problem of learning a classifier from few labeled and many unlabeled examples. However, the trivial composition of these methods brings almost no gain from using unlabeled samples, so an effort is made to make the self-training more robust to noise by involving an additional few-shot method (namely, the Relation Network, which is basically the Siamese network) and fine-tuning on just the labeled examples (which is allowed by the initial MAML training). The experimental results corroborate the efficiency of this method, so overall a good practical knowledge is shaped and delivered by the paper. In the light of the presented results, I wonder if the proposed soft weighting of the pseudo-labels can be used in the vanilla SSL task, composed with any method based on label propagation. Perhaps authors have some results or thoughts in this direction. One issue that bothers me in Table 2 of performance results is the low accuracy reported for the baseline methods [22] and [37]. These results are lower than the concurrent performance of methods using just the few labeled examples, without the unlabeled ones.  The presented ablation study is satisfying, since the performance of the different versions of the algorithm blocks helps to understand their vitality. The performance reduction due to distracting classes, demonstrated in Table 2, is a good additional 