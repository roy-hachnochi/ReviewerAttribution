The paper proves that RNN trained via SGD when only the weights are trained converges to a point close to the optimum and that the number of examples required for training scales polynomially with the number of training examples.  The result is very interesting and novel. Yet, I have few remarks:  1. The work "Robust Large Margin Deep Neural Networks" provides generalization error guarantees that are independent of the number of neurons, unlike what is written in the paper that there is no such generalization result for neural networks till now.  2. I would appreciate seeing a simulation that demonstrates the result proposed in the paper. I believe it is not hard to train such a network when only one matrix is optimized. Indeed, as only one parameter is optimized the performance will be poor but still, it would demonstrate the result.  3. I would appreciate a discussion as to how the work may be extended to learning of the other parameters. 