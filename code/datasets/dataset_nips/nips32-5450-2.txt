[Upper bound] Every hypothesis class H can be learned up to excess error α by a pure semi- private algorithm whose private sample complexity is (roughly) VC(H)/α^2 and public sample complexity is (roughly) VC(H)/α. For agnostic learning, VC(H)/α2 examples are necessary even without privacy constraints. The idea of algorithm is to use the public data to construct a finite class H′ that forms a “good approximation” of the original class H, then reduce the problem to DP learning of a finite class. It can be captured via the notion of α-covering.  [Lower bound] Assume H has an infinite Littlestone dimension. Then, any approximate semi- private learner for H must have public sample complexity Ω(1/α). The lower bounds boil down to a public-data-reduction lemma which shows that given a semi-private learner whose public sample complexity is << 1/α, transforms it to a completely.   Overall, this paper is clear, written well, and has a good contribution. From my view, it is around the conference threshold. 