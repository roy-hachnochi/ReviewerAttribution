Overall I thought this was an interesting paper which was well written with some nice ideas. I can see the potential for impact of their work. My main concern is that the authors don't give a good intuition for why their idea works and I have doubts that there might be (1) something special about the ZINC dataset or (2) some important knowledge about chemistry and/or the dataset which was necessary for their results.  While the writing was clear I was left with a lot of questions. The most significant ones are marked with **.    Gibbs questions: ** What is the initial state for the gibbs sampler?  Did it come from the real dataset? Was only one initial state used? Does it change the result if you use different initial conditions? How much burn-in was needed? I tried to find answers for these questions, apologies if I missed them. ** Figure 1 seems disjoint from the paper. It seems to describe the chemical embedding process but it refers to (1) attention and (2) message passing, both of which are never mentioned again.   Expert input: I imagine that the results are highly dependent on the corruptor used. This seems like an important part of the paper and I was required to read the appendix to understand your research. I still was left with many questions: Who wrote the corruptor? Were they an expert in Chemistry? Can the authors expand on how the corruptor choice impacts performance? Are there bad corruptors that don't work?    Is chemistry special? I find it surprising that the corruptor, which to me looks basically like a random walk along legal moves, provides a good match for the true distribution of chemical molecules. I would imagine that there are common tropes in chemistry which are not visited often. Perhaps the authors will suggest that its the denoising auto-encoder which is learning about these chemical tropes -- but that is especially suprising.  ** Can the authors explain their success? Is there something special about the ZINC dataset that makes this possible? Would this, for example, work for generating student code-solutions (see code.org/research)?   Baselines: ** What if you just run the corruptor? How well does it do?  ** If an expert is able to define generative legal moves, can they also write a generative grammar? ** Would the expert be able to generate the corruptor if they didn't have access to a ground truth dataset? Is the generative process of chemicals markovian in real life?  The authors cite denoising auto-encoders as the motivation behind their corruption-reconstruction idea. How important was the Gibbs contribution? If you just applied the denoising auto-encoder idea directly as suggested in [2] perhaps you wouldn't necessarily need Gibbs sampling.    Possible confounds: ** How is the frequency of molecules distributed? I imagine some molecules are more common than others -- are they zipfian distributed? If so the results could be dominated by a few common examples.  ** What influences from ZINC could have been used to construct the synthetic dataset? Specifically was any data from ZINC looked at by the researchers or shown to the model?