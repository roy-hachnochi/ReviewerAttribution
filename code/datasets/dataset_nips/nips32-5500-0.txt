#####After Rebuttal#####  I think the authors to clarify my concerns. For the second question, I would like to see more instances of functional attacks, which could be also applied to image classification. Given such results would make this paper stronger. I will keep my original rating.  #####################  This paper proposes a novel class of threat models for crafting adversarial examples. The paper is well-written. A typical type of functional adversarial attacks is realized by changing the color of images as ReColorAdv. The constrains on this attack and the optimization process is clearly illustrated. Below are some minor concerns:  1. Although the proposed functional adversarial attack is novel, it is somewhat relevant to "blind-spot attack" (Zhang et al., "The Limitations of Adversarial Training and the Blind-Spot Attack", ICLR 2019), and "unrestricted adversarial examples" (Song et al., "Constructing Unrestricted Adversarial Examples with Generative Models", NeurIPS 2018). The authors can discuss the connections between the proposed attack and other attacks, and also the differences.  2. Although the definition of functional threat model is general and flexible, this paper only provides one instance of functional attacks. More examples of functional attacks can make this paper more convincing and interesting.  3. From Table 1, C and C-RGB attacks are less powerful than L_\infty attack (D). S+D attack is more powerful than C+D in most cases, and gets similar performance to C+S+D. So the concern is about the effectiveness of the proposed ReColorAdv attack. It seems that ReColorAdv brings little benefit. 