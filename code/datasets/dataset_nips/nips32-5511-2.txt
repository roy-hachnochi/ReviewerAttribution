This paper presents Fenchel Mini-Max Learning formulation for density estimation, with the goal of being able to do inference, parameter estimation, sampling and likelihood evaluation in a tractable manner.  Major comments:   1. I think equation 2 is incorrect and the negative sign on the left side should be removed; if in the conjugacy relationship in line 114, we use t = 1/p(x), then we reach (2), without the negative sign on the left side. This propagates to equation (3) too, and might affect the implementation. please double check.  2. The formulation relies on the importance sampling estimation of the normalization constant (I(x, \psi)), i.e., you are basically using Monte Carlo estimation of 1/p(x).  This is in contrast with what is states in lines 138-140. Also, the quality of proposal distribution then can significantly affect the quality of solutions by FML. I think there exists a lack of discussion on this matter in the current manuscript.   3. In the gradient analysis part, authors resort to Monte Carlo estimate for the inverse likelihood (equation (4)). What is the benefit of this over just getting a Monte Carlo estimate of the likelihood itself?  4. The formulation of FML mostly relies on the Fenchel conjugacy for log(t) function. Can this framework be applied for a general convex function f?