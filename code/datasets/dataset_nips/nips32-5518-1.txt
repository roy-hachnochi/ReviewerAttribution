Originality. The problem of finite-sum minimization is very popular and attains a substantial attention recent years. Despite to the number of successive works in this field, results on both accelerated and variance-reduced methods are still under investigation and required additional research. The authors proposes new accelerated stochastic gradient method with variance reduction with its global complexity analysis. This method achieves the currently known convergence bounds and has a number of nice features (among others): * Tackling both convex and strongly convex problems in an unified way; * Using only one composite (proximal) step per iteration; * Working with general Bregman distances; * Relatively easy in implementation and analysis. To the best of my knowledge, up-to-know there are no accelerated variance-reduced methods having all these advantages simultaneously.   Quality. The paper contains a number of theoretical contributions with proofs. Two algorithms (Varag and Stochastic Varag) are proposed in a clear and self-contained way.  A number of numerical experiments on logistic regression with other state-of-the-art optimization methods are made.   Clarity. The results are written in a well-mannered and clear way.  A small concern from my side would be to make the introduction part more structured. Especially, more concrete examples of the cases, when we can not handle strong convexity of $f$ (but still know the constant) would be interesting. It also would be good to add more discussion and examples to the text, related to "Stochastic finite-sum optimization" (Section 2.2.).  Significance. The results of this work seems important and significant to the area of stochastic variance-reduced accelerated methods.