This paper gives a new algorithm for properly agnostically learning halfspaces with a misclassification error that is epsilon-far from (almost) the optimal gamma-margin error (up to some small approximation factor), with a run time that is poly(d/epsilon)2^(1/gamma^2). This result can be compared to previous work, which gave an algorithm that is poly(d) (1/epsilon)^(1/gamma^2), but gave an algorithm with no approximation factor.  The proposed algorithm uses an interesting iterative approach, wherein in each stage, a projection that is aligned with the difference between the current halfspace and the optimal half space is sought, using the part of the distribution which incurs a low margin under the current half space.  The paper also provides lower bounds on the computational complexity of any proper learning algorithm, under some standard complexity assumptions. These lower bounds show that the dependence on \gamma in the proposed algorithm is essentially tight. They also show that if one removes the approximation factor completely, then it is not possible to obtain a similar result, i.e. one with a polynomial dependence on $d$ and $1/epsilon$.  Overall, the paper is well-written, and the results appear to be important and interesting.    Detailed comments:  - In section 1.4, Chow parameters are mentioned, but they are not subsequently used in the presented analysis, they are only used in the supplementary material. It is not clear how the algorithm and analysis in the supplementary relate to the ones in the paper.  - Page 3, line 113: f and x_i are not defined. - Page 4, line 161: "Ruling out alpha-agnostic learners are slightly more complicated", "are" ==> "is" - Page 5, display equation in Claim 2.2: P[D'] should be P[y<w,x><= gamma/2] - Page 7, line 256: an extra parenthesis in the math formula. - Page 8, line 306: "learners learners"  