Post-rebuttal: Thank you for the responses they have cleared some of the concerns raised. -------  The authors present CARML, an unsupervised method to generate tasks for meta RL. Previous approaches other require the manual definition of task spaces which is especially hard, or by pipelined approaches in which through interactions with a CMP one can yield a task distribution. The authors propose to use a latent variable density model of the meta-learner’s behaviour in order to adapt the task distribution and then meta-learn on those tasks.  The paper is well written, everything is motivated and the choices made clearly explained. They provide results of their proposed algorithms in VizDoom and a Sawyer arm, in which they show that their method requires significantly less samples to learn when compared with baseline approaches.  I really liked reading the paper, I think unsupervised task generation is very important for meta-learning and this paper provides a nice way of doing it. I do wonder however how D is handled. It seems to me that every trajectory is added to the reservoir. This increases the complexity of fitting the task giver q. Don’t some trajectories become obsolete as the meta-learner evolves? Fitting q in D this way could harm performance. I feel that having a smart way of dealing with the ever increasing D is important.  One thing that would have been nice to include is how the approach compares to [22] as it seems to be a motivating paper for this research. 