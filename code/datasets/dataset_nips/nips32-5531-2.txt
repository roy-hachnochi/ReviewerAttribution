This paper presents a method for learning a distribution of tasks to feed to an agent that's learning via meta RL, while simultaneously optimizing the agent to perform better more quickly on tasks sampled from this distribution. The task distribution is trained using an objective that maximizes mutual information between a latent task variable and the trajectories produced by the meta RL agent. The meta RL agent is trained to maximize this mutual information, more or less. The overall optimization relies on some variational lower bounds on mutual information, and on the RL^2 algorithm for meta RL.  Experiments are provided which show that the task distributions and meta RL agents trained in this co-adaptive manner exhibit some potentially useful behaviors, e.g. an improved ability to quickly solve new tasks sampled from an "actual" task distribution -- i.e., a task distribution which is not equal to the one that's co-adapted with the agent.  I think the ideas explored in this paper are reasonably interesting, and may spark some more practical insights in the rest of the research community. The paper was clear and didn't make unnecessary claims. My main criticism is that it's a bit hard to predict the strength and future value of this work in the absence of prior work that it out-performs or a strong argument for why "automatic curriculum learning for unsupervised meta RL" is a conjunction of keywords worth exploring.  Extending that point, there also isn't much effort to explain why this particular approach to this particular problem is particularly worthwhile, which would be helpful in the absence of prior efforts to solve the problem. E.g., as an alternative one could train an agent and task distribution using one of the 10s of DIAYN-like approaches, and then train another agent to solve tasks from this distribution via meta RL using reward functions constructed similarly to the ones used in the proposed method.  Post Rebuttal: I appreciate the new experiments aimed at separating the benefits of improved diversity-based skill learning and co-adaptive vs pipelined approaches to incorporating this in a meta RL setting.  I'd recommend refactoring the presentation a bit as well, to emphasize that these two contributions -- i.e. improved diverse skill learning compared to DIAYN and a co-adaptive approach to using diverse skill learning for meta RL -- are separate but complementary. It may be difficult for people who aren't already familiar with these topics to perform the appropriate factorization if it is not explicit in the technical presentation.