The paper considers a bandit problem where the learner selects a subset of (up to) k>= 2 arms in each round. Subsequently, the learner observes as feedback a rank-ordered list of m >=1 items from the subset, generated probabilistically according to the Pluckett-Luce distribution on rankings based on the multinomial logit choice. That is, each arm is associated with a weight and, at each round,  the online algorithm plays a subset S of at most k arms. It then receives feedback according to one of the following models:  1. Winner Feedback: The environment returns one item from S. The probability of each subset is proportional to its weight. This scheme is known as the ``Placket-Luce probability model" on S.  2. Top-m Ranking Feedback: The environment returns an ordered list of m items sampled without replacement from the Plackett-Luce probability model on S.  The main contribution of the author(s) is to  give algorithms for regret minimization that perform optimally (up to constants) under the assumptions of the  feedback models (1) and (2). The exact bounds on the regret are somewhat involved to state here, but they depend logarithmically on the number of rounds and, interestingly (especially in terms of upper bound),  on the weight distribution over arms.  In terms of techniques, while I am not an expert in the area so I cannot judge their novelty, I did find the proposed algorithms and their analysis interesting and intuitive. A possible criticism is that the setting considered by the author(s) is similar to submitted Paper 396 (indeed, it was flagged as a possible dual submission). That said,  the two papers seem to use sufficiently different techniques.   Overall, I found the paper interesting. My main concern has to do with whether the assumptions of the proposed models are natural enough, and their relevance to practice.   