In this paper, a deep generative model that learns to generate SAT formulae similar to a set of formulae provided as input for the training time is proposed.  The approach is novel as far as I can tell. The paper is well written and provides sufficient level of details. The experimental evaluation is brief, but shows promise. indeed, G2SAT seems to generate CNFs which are similar to those provided as input, and which can be used as additional benchmarks.  In the training, 10 smallest SATLIB/SAT competitions benchmarks are used, resulting in CNFs with 82 to 1122 variables and 327 to 4555 clauses. Would G2SAT scale to using larger instances? In the generation of new instances, are these of similar size to the instances used in the training time? I did not find a mention about this. Furthermore, it would be interesting to know about the the time wise scaling of G2SAT (this is essential to demonstrate the stated efficiency of G2SAT). How long does it take to generate new instances? Does this scale to larger instances?  Pros:  - A way to generate SAT formulas with similar properties as the formulas provided as input the the training of the model  - Experiments suggest that the instances generated indeed resemble the original ones provided as input in terms of several measures  Cons:  - No mention of how does the approach scale   - No access to code during reviewing  ======================== After response period: The author's response clarified many of the concerns raised in the reviews. The additional results provided should definitely be included in the paper. 