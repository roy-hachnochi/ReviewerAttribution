Originality: This is a very original contribution which studies commonly used reductions for multi-label classification. The obtained results are not only surprising, but also very important from the practical point of view.   Quality: All the results are presented in a formal way. The claims are clear and theoretically justified. The empirical illustration is rather limited, but this is not the main focus of the paper (the results for OVA and OVA-N should be given in the Appendix).   Clarity: The paper is clearly written. Nevertheless the list below contains some comments regarding the text: - P(0_L | x) = 0 => I would prefer to not make such assumption. For example, some benchmark datasets contain examples with no labels. - Prec@k and Rec@k: these measures have certainly been used before Lapin et al. 2018. - Lemma 3 => There are undefined quantities in the lemma and typos in its proof. The final result is correct, but readability should be improved (at least the authors should say what y_{\not i} means and that they use the fact that P(y) = P(y_i)P(y_{\not i}|y_i)). Moreover, P(y_i' = 1) should be given in both forms, i.e. as in Lemma 3 and as E_y|x [ y_i/\sum_j y_j ] (the latter form is used often in the proofs of the other results). - binary relevance: the name binary relevance was used before Dembczynski et al 2010, but not sure where for the first time :( - The analysis could mention the label powerset approach which reduces a multi-label problem to a multi-class problem with exponentially many meta-labels. In this approach, however, we do not get easily marginal quantities. - Eq. \ell_{PAN-N} => I suppose that there should be log before the link function in the sum - The analysis in this paper is methodologically similar to the one from "On label dependence and loss minimization in multi-label classification", MLJ 2012. - Appendix: please proof-read the derivations.   Significance: This is a significant contribution containing somehow simple, but surprising results with high practical potential.