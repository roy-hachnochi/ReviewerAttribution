This paper analyzes a relevant machine learning problem, as it intends to provide additional insights into existing methods that have been introduced in an ad-hoc manner. From that perspective, the paper is interesting. However, it is not easy to understand the main results. I have the feeling that the write-up and design of the paper could be substantially improved. Let me give some specific comments to make this point clear.   Up to Section 4 the paper is reasonably clear, but in Section 4 things get a bit messy.  I have some problems to understand the losses in Equations 6 and 7, because the l_BC and l_MC are never defined explicitly. Please give formulas here. Many different ways of expressing the logistic and softmax loss exist, so I would like to have it precise here.   I can’t see the usefulness of Equation 8. Is this loss reasonable? Let’s take a training example with 100 positive labels. Treating each positive label as positive gets a weight of 1/100, while treating positives as negatives gets a weight of 99/100. So, this seems to suggest that false positives should be heavily penalized. Maybe I am missing here something, but this does not make sense to me.   Equation 9 is also a strange variant. Here the denominator in the sum does not depend on i, so it can be moved in front of the sum. As a result, this term just reweighs the importance of an instance, but it does not influence the risk minimizer for that instance. PAL and PAL-N should therefore have the same risk minimizer, but Corollary 7 seems to suggest a different result. I am confused here.   Should Corollary 7 not be named Theorem 7? In mathematics a corollary is a direct implication from a theorem. I don’t see from which theorem the corollary would follow here.   Traditionally, there are two ways to optimize task-based loss functions in machine learning: (a) optimizing a convex and differentiable approximation of the task loss during training, (b) fitting a probabilistic model at training time, followed by a loss-specific inference step at test  time in a decision-theoretic framework. For me, a big point of confusion is that the approaches are somewhat mixed in this paper. Wouldn’t it be easier to analyze the different methods in a classical decision-theoretic framework? In essence this would boil down to using accuracy for l_BC and l_MC.    In a nutshell, this is an interesting paper, but I think the write-up could be improved. In general the results are not very clear and counterintuitive.   ---- After author rebuttal: ----  my main motivation for giving a somewhat-lower score was some technical things that were not clear to me. I am satisfied with the author's response and will raise my score.  