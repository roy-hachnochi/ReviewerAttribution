The authors consider the parameter recovery problem that the data are generated from a ``teacher network''. The goal is to learn the network from the generated data which are from Gaussian distribution and labeled by the ``teacher network'' with some white noises. A simple algorithm is proposed to learn the ground-truth parameters of a non-overlapping CNN, which is shown to converge efficiently with high probability with a small sample complexity bound.  Due to the good properties of Gaussian inputs, the expectation of each update lies in the span of w^t and w^* (v^t and v^* as well). Also, with some properties of a specific ``good set'', as long as we haven't yet achieved optimality, the gradient in each step would be large enough and get closer to w^*. Furthermore, once the initial point lies in the ``good set'', all iterations will still be in that good set to guarantee the high convergence rate.  The high-level idea and proof sketch of the paper is clearly written. The authors design a special but simple gradient descent algorithm that has several nice properties to make the steady improvement over iterations happen. However, it would be much better to write down the proof of the claim in line 209 since it is where the assumptions of ``non-overlapping filter'' and ``Gaussian data'' are required. Moreover, some conditions used in the proof should be clearly specified. First, the proof indicates that $\eta_w$ and $\eta_v$ are both less than 1 but it is not mentioned in the main text. The c_1 and c_2 in the theorem statement is not enough for showing both $\eta_w$ and $\eta_v$ are less than 1 since C and C' can still be chosen to be large enough such that the inequality does not hold. Secondly, it is not clear how to get line 518 and line 528 from assumptions although I believe that there should be some ways to make the proof work.   For the originality and significance of this paper, the authors follow the line of [13] but propose a feasible algorithm that converges linearly. Also, they fix the two-stage convergence guarantee by simply reusing Theorem 4.3 at different time steps. However, this work only applies to non-overlapping and Gaussian input setting, which is far from the cases seen in practice, and it is not clear at all such strong assumptions can be avoided using the approach of this paper.   Typos: (a) The P_r in line 101 should be P_k. (b) The third and the last term of step size \alpha (line 152) might have some mistakes.  * After author response: I have read the response, but it does not change my opinion. I still feel that the assumptions of Gaussian inputs and non-overlapping filters are very strong, so I really have difficulty in judging the potential impact of this work. On the other hand, I would not be upset if the paper were accepted. 