This paper analyzes the non-asymptotic convergence for two time-scale TDC under a non-i.i.d. Markovian sample path and linear function approximation. The results are new and important to the field, and the analysis in this setting seems nontrivial. In addition, the paper also develops a new variant of TDC under a blockwise diminishing stepsize, and proves it asymptotically convergent with an arbitrarily small training error at linear convergence rate.  Extensive experiments demonstrate that the new TDC variant can converge as fast as vanilla TDC with constant stepsize, and at the same time it enjoys comparable accuracy as TDC with diminishing stepsize.   Overall, the paper has both analytical as well as practical value. However, the following issues need to be addressed.   1. The non-asymptotic convergence analysis for other GTD algorithms under a non-i.i.d. Markovian sample path has been studied in e.g., [30,34]. Hence, the new challenges of analyzing TDC relative to GTD in [30, 34] need to be compared and highlighted.   2. The paper generalizes the stagewise stepsize in the conventional (one timescale) optimization e.g., [34] to the considered two-timescale optimization. The new challenges of analyzing algorithms with blockwise diminishing stepsize in this new settings need to be discussed.   3. It is mentioned that the non-asymptotic analysis can be applied to studying other off-policy algorithms such as the actor-critic and the gradient Q-learning algorithms. A comment is due on how the theoretical guarantees can be affected in these settings?