This paper provides finite-time bounds for TD with gradient correction (TDC). While non-asymptotic behavior of TD and asymptotic behavior of TDC have been studied before, non-asymptotic analysis for TDC is new and interesting given the importance of off-policy learning and the challenge of step-size tuning in two time-scale algorithms.  The paper is well-written, the discussion on the impact of the two step-sizes is clear and is supported by experiments.  Questions:  - The plots show the error between \theta and \theta^*. How is \theta^* obtained for these domains?  - How would the worst-case errors predicted by the bound compare to the errors observed empirically in the experiments?  - Besides implications for the choice of step-size, do these bounds provide insight on what properties of the problem, the behavior policy, and the representation affect the rate of convergence?  - The first paragraph says that gradient-based TD algorithms are "more flexible than on-policy learning in practice." What does more flexible mean here? 