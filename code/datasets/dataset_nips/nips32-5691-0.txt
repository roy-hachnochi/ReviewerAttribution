Strengths a. The paper is well-written and the authors are clear about their claims.  b. The idea of critical periods during training with reference to regularization is interesting. If true, this would give a different way to think about generalization.  c. The authors have performed a number of experiments with different configurations. Although, there are deficiencies mentioned below.  Weaknesses Despite the above a I see the following problems. My primary concern is that while a number of results have been presented, they do not necessarily support the conclusions that have been drawn.  a. I think that the claim that regularization biases the training toward a set of solutions (each of which is as good as any) is not sufficiently borne out by the experiments. The 'initial transient' is identified in the experiments as occurring at around the 100 epoch mark after which, it is claimed, regularization does not make a difference to generalization. The conclusion drawn from this is that the transient determines which set of solutions is selected, and once the set has been selected regularization has no further effect. I would suggest that another possible explanation for this is that the solution is already quite close to its optimum at the end of the identified transient. For the case of weight decay this would mean that the regularization term does not have a significant effect (since the weights are already small), although the L2 norm of the weights can still move around a bit if the minimum is flat. I think that to establish the claim it should be shown that at the end of the identified transient, the solution is still quite far from the optimum and simply comparing the norms might not be enough if the minimum is flat. Interestingly, this is recognized by the authors (lines 417-421) but not quite dealt with.  b. Similarly for the case of delayed regularization I would suggest that the claim that after a period it has no effect is not sufficiently established. For example, one possible reason why delayed L2 regularization might have little effect is that weights have already become large before it is triggered. If so, then it might simply require more training time to lower the weights and reach convergence. That this is not ruled out is suggested by Figure 2 (B) where the weight norms are still on the decrease for delayed regularization. This seems to suggest that the models were not trained to convergence.  c. I have another comment on the claim that it is a common belief that smaller L2 norms for weights lead to better generalization. I might be wrong but I don't think that this is commonly held. The cited paper [1] mentions this for linear problems y = Wx, but not as a general claim as far as I can tell.  d. Datasets One weakness I find with the paper is that the experiments are only done for two very similar datasets. I think that more diversity in the datasets is required. The authors have anticipated this question (402-409), but I would counter by saying that one dataset and one network and one optimization procedure do not constitute a phenomenon.  Reference: Wilson et al. The Marginal Value of Adaptive Gradient Methods in Machine Learning 