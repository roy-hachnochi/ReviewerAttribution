Response read. I appreciate the commitment to making the paper clearer. Thank you for that. This is a really good paper! ___________________________ **Summary** They learn in an unsupervised way keypoints (x,y coordinates) corresponding to “relevant” objects using  the novel “Transporter” architecture, which combines an existing architecture, PointNet,  with a novel unsupervised task, “feature transport” (predict the future in pixel-space, but constrain only the features at predicted keypoints to change between two consecutive frames’ feature maps). They also show how accurate their keypoint prediction is with an object tracking task using ground truth coordinates. Lastly, they use these keypoints effectively in two downstream RL tasks: model-free RL (neural fitted q iteration) with keypoint-indexed features as input and sample-efficient exploration by defining an intrinsic reward based on maximizing each keypoints movement in +x,-x,+y,-y directions. (also they find the most controllable keypoint and exploit that at test time!). They use a few games from Atari for each task.    **Strengths** * The transport mechanism takes a minute to get straight, but it makes sense. It seems to be a good inductive bias that makes the pixel-based reconstruction training signal more amenable to learning good object-centric features. * The transport mechanism is also novel and seems intuitively better than the original PointNet approach of just concatenating the heat maps to the feature maps and this is shown as they track the true keypoints better than previous approaches * Also they are the first (to my knowledge) to take this keypoint based work (Jakab, Zhang) and apply to RL and show it works and helps for control and for exploration. * Their new intrinsic rewards for exploration in this new keypoint space make sense and they provably show they are better than random policy * The methods section, introduction, related work are well-written and nicely motivate and explain the transporter architecture. The paper is well organized and the diagrams nice and clear and very helpful * intrinsic reward using keypoints is novel as far I know and intuitive and promising * Results are very important/significant: learning more object-centric, state representations and using those to be more sample efficient and effective is something the field should move toward and I hope more people follow up on  * Addresses exploration in a way I find intuitively better/scalable -> explicitly trying to find objects/keypoints and than operating in the space that moves them around seems like a better approach than previous work that uses pixel-space based losses to make an exploration bonus (pseudo-counts,) or surprisal-based predictive exploration (intrinsic curiosity) or exploits access to simulator (Go-Explore) -> would be cool to see the number comparisons for this though!  **Weaknesses: ** * Clarity. There are parts of this paper that are a bit unclear. The diagram and caption for KeyQN section are very helpful, but the actual text section could be fleshed out more. It would nice if the text could have a little more detail on how the outputs from the transporter are input to the KeyQN architecture and how the whole thing is trained. The exploration section was well explained for most part, but it took a bit of time to understand. Maybe would help to have an algorithm box. Also, the explanation of training process a bit confusing. Maybe a diagram of the architecture and how the transporter feeds into this would help. Also, I am confused a bit about whether the transporter is pretrained and frozen or fine-tuned. One quote from the paper in this regard confused me: “Our transporter model and all control policies simultaneuosly “  so the weights of the Transporter network are not frozen during the downstream task like in KeyQN? * Experiments: They only show these results on a few games (and no error bars), so it would have been nice (but not a dealbreaker)  to see results from more Atari games. They do partially justify this by saying they couldn’t use a random policy on other games, but I’d be curious just to see what happens when they try a couple more games. Would be nice to see comparisons to other exploration methods (they only show results compared to random exploration)  Nitpicks/Questions  * Makes sense to just refer the reader to the PointNet paper instead of re-explaining it, but a short explanation if possible of PointNet (couple sentences) might be helpful, so that one doesn’t have to skim that paper to understand this paper  * The diagram in figure 5 (h_psi) should show a heat map not keypoints superimposed on raw frame right?  * In the appendix “K is handpicked for each game?” How? Validation loss?   * The tracking experiments but the section is a bit unclear. I have a few questions on that front:   * why is there a need to separating precision and recall?    * why not just report overall mean average precision or F1 score? Might be a bit easier for reader to digest one number    * Why bucket into diff sequence lengths? what do the different sequence lengths mean? There is no prediction-in-keypoint space model right? So there is no concept of the performance worsening as the trajectory gets longer. Aren’t the keypoint guesses just the output of the PointNet at each frame, so why would the results from a 200 frame sequence be much different than 100 or something? Why not just report overall precision and recall on the test set?  * In the KeyQN section What is the keypoint mask averaged feature vector?  just multiply each feature map element wise by H_psi? 