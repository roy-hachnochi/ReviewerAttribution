Summary: This work proposed a practical implementation of an effective approach InteractiveRecGan to utilize offline data to build a recommender agents with better policy than the reference one. Specifically, a model-based RL framework is incorporated with adversarial training to better utilize the offline logged data to lean a “optimal” policy.   Using 1 - 5 scale with 5 highest Originality:  3 out of 5  Compared with the recent related works such as [1], the main difference is to use a model-based RL instead of a model-free RL. Meanwhile, compared with model-based RL in recommendation such as [2], the main difference is the adversarial training     Quality: 4 out of 5 This work is technically sound, and the proposed methods are well supported by both theoretical analysis and the empirical analysis  Clarity: 4 out of 5 This paper is very well-written and the empirical results are presented in an easy-to-read way. All the plots are very easy to read  Significance: 3 to 4 out of 5 It is not clear and how promising the proposed approach is to be deployed and served as an online recommender  [1]: Generative Adversarial User Model for Reinforcement Learning Based Recommendation System, in ICML'19 [2]: Model-Based Reinforcement Learning for Whole-Chain Recommendations 