Overall the paper provides new and insightful approach to tackle learning problems with non decomposable performance metrics. The result generalizes some existing approaches to optimizing non decomposable metrics and give provable algorithms for more general objective functions or constraints made of classification rates. The authors also resolve open questions from earlier work of Cotter et al (ALT 2019, JMLR 2019) about using OGD on true and surrogate Lagrangian to achieve convergence as a positive result although receiving a slightly worse convergence rate.) The problem of optimizing non decomposable metrics is significant and this paper provides algorithms with theoretically guarantees for a class of such learning problems and thus the results are themselves significant. 