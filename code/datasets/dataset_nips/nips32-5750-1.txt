After reading the author feedback, I am impressed by the thorough comments and I am increasing my score from 5 to 7. Please incorporate your discussion points to the paper, and thanks for the well-thought-out response!  * * * * * Original review below * * * * *  From a methodological standpoint, this paper combines an ensembling approach that is basically just pasting (Brieman 1999) with nearest neighbor classification and then also with the denoising method of Xue and Kpotufe (2018). We already know that k-NN classification can achieve the minimax optimal rate in fairly general settings (Chaudhuri and Dasgupta 2014). That pasting used in conjunction with k-NN classification also achieves this rate is unsurprising (and the analysis is straightforward), and stitching the result with the theory developed by Xue and Kpotufe is also straightforward. Overall the theoretical contributions are reassuring albeit unsurprising and incremental.  Given the aim of the paper, I think it's extremely important to experimentally compare against recently proposed quantization schemes that are largely about how to scale up nearest neighbors to large datasets: see Kpotufe and Verma 2017, and Kontorovich, Weiss, and Sabato 2017.  Perhaps more discussion/theory on when/why one should use pasting rather than bagging to ensemble k-NN estimators would be helpful. I realize that the authors have mentioned a little bit about known asymptotic results but I think numerical simulations would really be helpful here.  Separately, depending on the dataset (i.e., the feature space and distribution), I would suspect that even just taking 1 subsample without ensembling could yield a good classifier. Perhaps some discussion on understanding how much training data we could get away with (and whether we could just ignore a lot of the data to save on computation) could be helpful. This comment is a bit related to the quantization strategies mentioned above.