This paper treats the theoretical analysis of k-nearest neighbor (kNN) classification when the kNN classification is performed separately in different machines with different data. The information is combined later for final decision.  The paper is clearly written and the experiments support the derived theory well. The derived result is important because the simple methods such as k-NN classification is becoming important for treating really large data, but the data now cannot be treated in a single machine. The results achieve the similar order of convergence rate in a single machine algorithm without loosing the simplicity of k-NN methods. This paper is a nice contribution to nearest neighbor community in NeurIPS.  