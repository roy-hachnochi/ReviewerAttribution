This work considers the relationship between convex surrogate loss and learning problem such as classification and ranking. The authors embed each of the finitely many predictions (e.g. classes) as a point in Rd, assign the original loss values to these points, and convexifies the loss in between to obtain a surrogate. The authors prove that this approach is equivalent, in a strong sense, to working with polyhedral (piecewise linear convex) losses, and give a construction of a link function through  which L is a consistent surrogate for the loss it embeds. Some examples are presented to verify the theoretical analysis.  This is an interesting direction in learning theory, while I have some concerns as follows:  1) What's the motivation of polyhedral losses? The authors should present some real applications and shows its importance, especially for some new learning problems and settings.  2) It would be better that the authors focus on some specific learning setting such as classification, or AUC or classification with rejection. The presented submission focuses on various setting and there is a lack of deep understanding on some specific problems.   3) It would be better to present some intuitive explanation for Definitions 1-3 for better understanding their meanings.  4) There are some known results in Section 5 and the authors should present some new real applications of surrogate loss.  5) It would be better to present some regret bounds on the surrogate loss and polyhedral losses.