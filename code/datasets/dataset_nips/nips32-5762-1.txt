The paper tries to find adversarial samples in binary hashing when we have two modalities (image and text). The idea is to find the min perturbation to the image (text ) that maximizes the Hamming distance between the binary code of the perturbed image (text) and binary codes of the relevant texts (images). They show that adding these adversarial samples to the test massively decrease the performance of the network.  Issues and questions: 1- My main concern is the novelty, as it is well-known that adversarial samples exist in neural network models. How does the current approach make itself different from all previous approaches in finding the adversarial samples? 2- In eq (2), the accuracy is defined based on the argmax of H(). But, H returns a d-dimensional continuous vector and its argmax does not show anything in the hashing literature. This needs more explanation. 3- In eq (3), it is clear that for S_ij=1 (similarity), the objective function finds the perturbation that maximizes the distance. But, what does happen for S_ij=1? 4-  My other main concern is experiments. In table 3, the models are trained by adding adversarial samples to the training set. By comparing the results to the table 1, we can see the accuracy decreased significantly (around 4% in some cases). This is disappointing since we want to keep the accuracy as we make the network more robust.  ------------------ As the authors mentioned in their response, finding adversarial examples in cross-modal learning is something new in the literature and could lead to more robust networks. I changed my score to above the threshold.