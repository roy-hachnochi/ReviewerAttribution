Originality: This work builds a novel solution to an existing challenge of RNNs based on already known techniques with the goal of improving interpretability and expressivity of the machine learning model. The differences to the previous work have been clearly stated, but a related work section is missing.   Quality: The theoretical contributions of the paper are technically sound. The experimental part can be improved, though. Although the toy examples are very clear and advocates for the soundness of the results. More experimental evaluations are needed. Moreover, a large body of related works are missing in the paper; specifically, fundamental attempts on the interpretability of deep networks and specifically recurrent neural networks. Examples include: Attribution (Saliency maps) [Olah et al., Distill 2018]. Dimensionality reduction [Hinton et al. JMLR 2008] Feature visualization [Karpathy et al., ICLR 2016] Pattern matching [Strobelts, et al., IEEE Trans Computer Vision and Graphics 2018] Attention Networks [Bengio et al. ICLR 2015] Dynamical systems analysis of LSTM nets [Hasani et al. ArXiv 2018]  Clarity: The paper is well-written and well-organized. The structure of the paper makes it easy to follow the paper’s storyline. A related work section is missing, and this makes the judgment of the clarity of the results a bit challenging.   Significance: This work takes a step forward towards addressing an issue regarding the interpretability of recurrent cells, the “vanishing saliency problem”.  The theoretical description of the problem is novel and technically sound. I find the investigation of the paper over the “vanishing saliency problem” quite interesting. I have a suggestion/concern, if we have the vanishing saliency problem in RNNs, and the solution is to use an additional attention mechanism at the input of the LSTM networks, what would happen if we do not use an LSTM in this case (both the toy and the real-life examples) and only use an Attention network? I would suggest to include this experiment and provide results. Maybe really “attention is all you need”?   Although it is invaluable to improve performance over a real-life test case, in my opinion, the experimentations performed in the paper could be extended to at least one of the standard benchmarks to compare performance.  It is beneficial to observe the attention weight matrices, where one could reason about how the dependencies to the initial time steps are being established.   Also, it would be interesting to see how the learned forget-gate weights differ in networks of Vanilla LSTM compared to LSTM networks supplied with an attention mechanism. 