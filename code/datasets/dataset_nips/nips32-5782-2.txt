The authors describe the proof for expected 0-1 error of deep ReLU networks trained with SGD using random initialization (from He. et. al.), that results in algorithm dependent generalization error bound that is independent of network width if the data can be classified by the purposed NTRF features with small enough error. The result is generalizable  (two layer and larger function class) and when reduced to two layer setting is sharper. A more general and tighter bound is also derived, similar to Arora [3] to show similarity to neural tangent kernel from Jacot [17]. The purposed NTRF is shown to be richer (contains gradient from first layer) and generalization bound sharper ( compared to 31 and 13). He initialization enables removing exponential dependence on depth in kernel matrix, where the kernel matrix takes all layers into consideration  (as opposed to last hidden layer in [10]). The results are also compared and shown as generalization of [3] with sharper bounds.