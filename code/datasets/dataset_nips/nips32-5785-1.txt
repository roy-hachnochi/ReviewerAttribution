This paper proposes a new randomized optimization method for solving high-dimensional problems. The paper is well written and the authors demonstrate the efficacy of their methods with strong theoretical guarantees and extensive experiments to back up their claims. The authors use standard results from Fenchel duality of the dual problem and its adaptively sketched counterpart to bound the relative error with high probability. The relative error bounds are shown for high dimensional matrix with finite rank, exponentially and polynomially decaying eigenvalues. The adaptive sketching method significantly outperforms its oblivious sketching counterpart for different decay values.  The sketched matrix is computed exactly once and with probability 1 achieves better condition number.  However, computing it involves a matrix inversion step that needs m^3 computations, is there a way to approximate it for problems which might need large m. Moreover, will the approximation still satisfy Proposition 3 upto a certain multiplicative factor? On the contrary, for small m, will dynamically modifying the sketching matrix lead to even tighter bounds?  While the experiments indicate that sketch based methods indeed converge quickly to their optimal solutions, it is unclear if SGD (without sketching) converges to the same solution or if it performs better (albeit after a longer time).   It will be interesting to compare the test accuracy vs wall-clock time of MNIST and CIFAR10 with oblivious sketching and other methods   The authors claim that the sketch method did not work well with SGD, did the authors experiment with relatively large mini-batch sizes which may resolve the issue of not approaching the minimizer accurately.  Post rebuttal: In light of the rebuttal posted by the authors, I would like to keep my current score. 