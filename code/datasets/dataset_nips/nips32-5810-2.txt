# Strengths: - The paper copes with an interesting problem. Both model compression and lifelong learning are active domains of research and proposing an approach which somehow bridges the two problems and proposes a dual view on these problems is great.  - The exposition of the paper is very clear. The paper reads nicely and the problem is very well motivated. - The proposed algorithm is very simple and therefore easy to understand. Its simplicity should definitely be considered as a feature. - The approach is evaluated in multiple experiments, some of them on "controlled" data, and one experiment on a real model (ResNet-18) on real data (CIFAR-10 CIFAR-100). The experiments are quite convincing. - The proposed model compares favorably with previous work (See Fig. 3).  # Weaknesses - When reading papers about catastrophic forgetting, I can't help myself from asking for actual practical applications. All experiments in this paper are on datasets that have been actually made up from standard benchmarks, by concatenating manipulated samples. There are probably real tasks with available data for which it would make sense to use such a model. I would like the authors to comment on that. - What lessons learnt from this work could influence or guide research in model compression?  Overall, I enjoyed reading this paper who's motivations are clear and execution is very good. The tackled problem is interesting and the experiments convincing.  Please note however that I am not an expert in lifelong learning, and may have missed important details or relevant work that could change my opinion. Therefore, I await the author's response, the other reviews and discussion to make my final decision.