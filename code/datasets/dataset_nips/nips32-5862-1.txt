I think the idea is interesting and the motivation for this problem is important. However, there's other meta-model Bayesian methods that I think the authors should consider discussing in their paper:  1.)  Bayesian inference in hierarchical models by combining independent posteriors  (Dutta, Blomstedt, Kaski, 2016) 2.) Meta-analysis of Bayesian analyses (Blomstedt et al., 2019) 3.) Differentially Private Bayesian Learning on Distributed Data (Heikkila et al. 2017)  Also, perhaps a useful baseline comparison to your method would be a simple one level hierarchical extension of sharing features, which should be possible as the features are assumed Gaussian in each of the experiments, and compare the results against your method. This could better illustrate why your method is necessary in terms of computation time, privacy preserving, etc.  I think it's quite a nice idea considering it's designed for structures like mixture models which I think is quite novel for this type of work.