The paper considers various kinds of canonical statistical inference problems which aim to infer properties of an unknown discrete distribution p based on n independent samples from it. The considered problems include property estimation where the goal is to estimate f(p) up to certain accuracy for some function f, distribution estimation where the goal is to estimate the true distribution up to a certain distance, and property testing, where the goal is to determine whether p is close to a known distribution q or not.  These problems have been studied since decades ago and received a lot of interest in both statistics and machine learning community recently in the finite sample regime. Sample optimal estimators have been proposed seperately for different problems recently. This paper approaches these problems based on a universal plug-in estimator using profile maximum likelihood (PML) estimation for the unknown distribution p, which outputs a distribution which maximizes the probability of outputing the profile (histogram of histograms) of the sample sequence. They showed that: 1) The estimator achieves optimal sample complexity for sorted distribution estimation for a wide range of parameters. It also improves upon the dependence on error probability over previous works. This also holds for approximate PML (APML), which can be computed in near-linear time. 2)  The estimator achieves optimal sample complexity up to constant factors for all additive symmetric properties that are 1-Lipschitz with respect to relative earth mover distance, which include Shannon entropy, Renyi entropy, support size, support coverage, distance to uniformity. This also holds for APML. 3) For Renyi entropy estimation, the estimator also matches the current state of the art for non-integer \alpha's, which is optimal up to log factors. It also improves upon state-of-the-art on its dependence on error probability. For integer \alpha's, it achieves optimal dependence on the alphabet size while the dependence on the error parameter is not optimal. The result also holds for APML. 4) For identity testing, an algorithm based on PML estimator can achieve the optimal sample complexity up to log factors. But it is not clear whether this will hold for its approximate version.  The results in this paper are pretty impressive.  As this is the first result which shows the optimality of a single plug-in estimator for a wide range of statistical inference problems. The optimality of PML has been shown before for only certain property estimation problems based on a case-by-case study. This paper extends it to estimating a more general class of 1-Lipschitz additive symmetric properties, estimating Renyi entropy and sorted distribution estimation.   The paper also demonstrates the efficacy of its proposed estimator on distribution estimation and showed it outperforms the current state-of-the-art Good-Turing estimator.  The paper is also nicely written with clearly stated results with detailed comparison to previous results.  Cons:  1. The result on property testing is less impressive since it is neither sample optimal nor near linear-time computable. Various computationally efficient and sample optimal estimators have been proposed before.  2. The experiments are based on MCMC-EM algorithm to compute the PML estimation. It would be nice to see how would the near-linear time approximation of PML would work empirically.  3. The experiments for Shannon entropy estimation in the appendix is not impressive.   4. Detailed comments: 1) The notation for the equation between line 78-79 is not clear. A better way might be \forall p \in set H_i. 2) Theorem 1: it might be better to state all the conditions in the theorem to make it more self-contained. 3) Line 104: APML is not properly defined. There might be confusions on what does it mean by an approximation of PML.  Overall, I think the paper is among the top papers for NeurIPS. I would strongly recommend this paper to be accepted. 