Originality * Their proposed algorithm has little in the way of surprising conceptual insights, but in this case that is a good thing - the parallelism algorithm is simple, intuitive, and achieves nearly linear throughput increase in the number of accelerators used (hard to expect much more). It's surprising that this hasn't been done before, given that it's such a simple trick that gives such a large speed up for training distributed models. * The authors propose a couple smaller tricks to make the parallelism algorithm work or work better (re-computing forward pass activations to reduce memory requirements, doing so while waiting for backward pass gradients). To train their very deep models, the authors also use another few smaller tricks (e.g., clipping logits to mitigate bad gradients)  Significance & Quality * General-purpose model parallelism algorithm: The proposed algorithm is applicable to almost any neural network architecture without modification; the authors demonstrate this feature by scaling up state-of-the-art architectures in both computer vision and NLP settings. * Empirical results with scaled up models: The large-scale models enabled by TensorPipe show empirical gains on highly competitive benchmarks in computer vision (ImageNet) and NLP (machine translation). In machine translation for low-resource languages, these gains seem quite substantial.  Clarity: Clearly written, aided by the simplicity of the algorithm. Figure 2 is a clear, comprehensive overview of the approach. The writing could be made more concise/dense in some places, especially to make more space for the helpful analysis of wall-clock time breakdown referenced in the Appendix (2.2). The authors clearly describe the relation of their work to other parallelism algorithms, plus the broader literature regarding deeper/wider models and generalization. The authors may also wish to relate to the literature in developing model architectures that are explicitly easy to distribute (e.g. "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"). It also may be helpful to briefly overview (or show in a figure) the architectures for AmoebaNet and Transformer. 