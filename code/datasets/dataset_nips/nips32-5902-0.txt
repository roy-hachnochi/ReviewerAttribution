Summary: The authors suggest a new composition rule that brings flow models and autoregressive closer together. The main contribution is a sufficient condition for invertibility and a type of masked convolution design with triangular Jacobian, whose inverse can be computed sequentially in a parallelizable manner. Some concerns remain about novelty.  Pros: There is a dire need for a generalized framework around recent progress in flow-based modeling. The paper aims to provide this.  The empirical results are strong.  Major concerns: A recent paper [1] is neither cited nor discussed. However, the paper contains masked convolutions and discusses relationship to autoregressive models. It also has to compute a sequential, but parallelizable inverse. I am concerned about the novelty of the current paper and its lack of discussion of this work. If the authors can not make a compeling argument how their method is sufficiently more general or different than [1], the current work seems to be too incremental to warrant acceptance to Neurips.  [1] Hoogeboom et al., ICML 2019, Emerging Convolutions for Generative Normalizing Flows.   ==== Post rebuttal ====  The authors addressed my major concern and discussed emerging convolutions in the context of their work. The significantly reduced time-complexity due to the fixed point iteration inverse is a significant improvement, so I will raise my score by one point. 