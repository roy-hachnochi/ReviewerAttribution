The paper studies the problem of data-dependent generalization bounds. Novel abstract results are presented, which extend the existing mutual information and PAC-Bayes bounds, which scale with the mutual information or KL divergence related to a random uniform subsample of the original dataset. Based on this framework, an improved generalization bound for the Langevin algorithm is developed, which achieves $O(1/n)$ rate and depends only on the trace of covariance of stochastic gradients. The idea of proof is simple but very interesting. Mutual information method and PAC-Bayes bounds are both based on Donsker-Varadhan variational formula. Plugging in one of the measures to be based on a random subsample brings the ideas of leave-one-out analysis to this literature. Such type of extension is nontrivial and can potentially bring about better understanding of data-dependent generalization bounds. The examples of Langevin dynamics and SGLD also made solid progress in this direction. That being said, the current presentation impedes readers' understanding of the techniques, and the writing needs a lot of improvement. For example, in Theorem 2.1, it is not clear what the function $\psi_{R_D(\tilde{W}) - \hat{R}_{S_J^C} (\tilde{W})} (\lambda)$ means. The results about Langevin dynamics and SGLD should be formally stated as corollaries or theorems. Besides, the authors should also include more intuition about the meaning of subsample J and auxiliary random variable X.