The algorithm differs from [6] in that instead of averaging the actual eigenvectors, it averages the eigenvectors weighted by the corresponding eigenvalues. The claim is that this modification enables a better concentration result in terms of estimating each eigenvector.  A small notational issue -- it might be useful to state upfront that when used without a subscript (which sometimes has been done), "\| \|" refers to the spectral(?) norm.  The "bias" lemma is quite interesting, and this is clearly the technically most challenging part of the paper. It would be very illustrative if the authors could present a clearer intuition of what "cancellations" enable a better bound here.  I was wondering why the authors do not refer to any of the randomized sketching methods for EVD/SVD? It seems to me that there is a feasible algorithm in which each of the machines either sketches the samples and then combines them to find the actual SVD. It also seems to me that as long as we get a guarantee corresponding to Theorem 1, it is possible to get bounds on individual eigenvalue/vector estimation similar to Corollary 4 & 5. It would be nice to at least compare the results, both theoretical guarantees as well as empirically, with these methods. 