The paper presents an approach to the increasingly popular problem of "Text Attribute Transfer/Text Style Transfer". The authors motivate their approach by pointing to prior work that argues that disentanglement of "content" and "attribute" in the latent representation of a sentence isn't a necessity to achieve attribute transfer. They therefore present an approach that seeks to "edit" an entangled latent representation, via gradient signals from a pre-trained, differentiable, classifier trained on the same latent representations, to alter the sentence's attribute. Entangled latent representations are produced by a simple sequence autoencoder.   Pros: - Well rounded experiments and evaluation (3 automatic metrics that account for different aspects of the problem - BLEU, PPL, classifier accuracy + human evaluation) on 4 datasets. - Good improvements on multiple metrics including human evaluation on multiple benchmarks. - Although I don't find much discussion about this, the approach presented in this paper should be a lot faster to train that unsupervised MT like approaches that rely on backtranslation. The autoencoder and classifiers can be pre-trained independently and the activation maximization step needs to be run only at inference. - Interesting overall approach, investigating the broad viability of latent space editing via activation maximization in NLP and specifically for text style transfer.   Concerns: - Gains over prior work seem quite substantial, but as a baseline, I would have liked to have seen a simpler autoencoder architecture. It isn't clear to me how much of the gains come from this particular choice of architecture. - When presenting how well the autoencoder performs at reconstructing sentences, qualitative examples, seem like a bad choice to demonstrate this behaviour (Table 0.4 in the supplementary section), presenting BLEU scores between input and reconstructions should paint a better picture. - Related work and presentation of the paper make connections to the FGSM adversarial attack, but not to a large body of work on activation maximization, that are more related to this line of work, since they've been successfully applied to producing generative models (see for example [1] Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space by Nguyen et al. (2017) [2] Synthesizing the preferred inputs for neurons in neural networks via deep generator networks by Ngyuyen et al. (2016) [3] Visualizing higher-layer features of a deep network by Erhan et al. (2009).)  Having read the authors' response, I still think this is a good submission and look forward to seeing their ablation studies.