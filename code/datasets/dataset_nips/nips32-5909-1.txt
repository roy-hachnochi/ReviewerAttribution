This problem studies a novel variation of pure-exploration in multi-armed bandits, where the objective is to discover the pair of adjacent arms with the maximum gap of expectations.  The authors provide elimination and UCB-based algorithms and show their upper bounds.  They also provide a minimax lower bound to show the optimality of the proposed algorithms.  They conduct experiments on both synthetic and real world data sets to verify the effectiveness of their algorithms.    The problem studied in this paper is a novel variation of MAB problem and has some meaningful real-world applications.  It provides a preliminary theoretical foundation to efficient learning-to-rank tasks as well as approximate query processing tasks.  The unique challenge in this problem is the unknown order of arms, which introduces a non-trivial twist that differs the problem from traditional top arm identification.  On the other hand, the output only needs to find the maximum gap, which is easier than figuring out the full rank of all the arms.    The proposed algorithms are presented in a clear way and seem quite intuitive.  The algorithms generally maintain the confidence intervals of gaps between arms, as well as an active set of arms which are possibly related to the max gap.  The arms in the active set will be sampled and played.  The upper bound of the proposed algorithms is related to how close “the gap of a pair of arms” is to “the max gap”, as well as the gap between a pair of arms per se.  The authors also provide an intuitive explanation of the upper bound.  A more practical question here is that if there are multiple pairs of arms with similar gap values, the algorithm still needs to take a lot of samples to figure out.  The authors may consider a PAC-setting to prevent this situation.    I have minor concern on the experiment settings.  The theoretical results presented are obtained in a fixed-confidence setting.  However, the experiments shown seem to be evaluating performance in a fixed-budget setting.  Is there a specific reason why a fixed-confidence experiment is not conducted?  Another concern is: do authors have more real-world data set test cases other than the Borda one?  Currently the authors primarily run experiments on only one single test case in real-world data.  However, it would be interesting to see 1) what would be the hardness distribution in a real-world data set.  2) does the algorithm performance align well with the hardness value.  Generally, I think the paper studies an interesting novel problem setting.  The paper is well-written and provides a solid foundation for future studies.  There are some minor issues with experiments, but overall the paper would still be a valuable contribution to the community.    %===After rebuttal===%  Thanks for the authors for their response.  I think the figure illustrating hardness and #samples is useful and could be included in the paper.  I remain my relatively positive opinion.   