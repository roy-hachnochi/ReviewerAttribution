This paper considers a variant of pure-exploration bandit problems where the agent tries to divide the arms into two sets such that the highest mean of one set is smaller than the lowest mean of the other set, and the gap between these means is maximized. Algorithms based on elimination and UCB (with/without LCB) are proposed, and their high-probability sample complexity bounds are derived with a discussion on the lower bound in the fixed-confidence scenario. Simulation results are given for synthetic setting and a one based on a real-world dataset.   One of the main concerns is that the motivation is quite unclear. In the proposed framework, a very large number of samples is required if the largest and the second largest gaps are close to each other, as illustrated in the discussion on the lower bound, even though the objective of the framework is just clustering. Therefore there should be detailed discussion on in what situation such an exact clustering becomes necessary. For example, in the simulation for the Streetview dataset, the resulting “correct” clusters seem quite unbalanced and I don’t understand why finding such clusters is essential. For the same reason, the discussion “Note that finding the safest image (best-arm identification) is hard as we need a lot of human responses to decide the larger mean between the two rightmost distributions” seems quite inappropriate since it seems to needs a lot of human responses to decide the larger gap between the highest and the second highest ones.  Another concern is that the techniques used in this paper in the evaluation of the sample complexity seem to be a naive application of the original analysis of the base algorithms, successive elimination and LUCB algorithms. As a result, the derived bound is only order-optimal (with respect to Delta) and does not evaluate the expected number of samples even though the lower bound is given for the expected number of samples. Furthermore, the sample complexity bound is only correct with probability 1-delta, where delta is pre-specified by the required confidence level and cannot be controlled in the evaluation. There should be more modern analysis on the lower bound and matching upper bound following the line of work by Kaufmann for best arm identification, techniques of which seem also applicable to this setting.  Section 1.1: The considered model must be clarified (Bernoulli, Normal or sub-Gaussian,...) to discuss confidence bounds. Algorithm 2: There seems to be no explanation on the choice of c, even though the algorithm with small c cannot be correct. Experiments: It is strange that the error probabilities are evaluated even though the theoretical framework considers the fixed-confidence setting.  -------- Reply to the rebuttal: - choice of c The rebuttal clarified that c=5 is used in the experiments as in Jamieson+, but, unlike Jamieson+, there is no argument on what choice of c is theoretically guaranteed. In fact, c does not appear even once in the proof of Theorem 1.  - "characterization of the "closest" alternate model does not hold in the MaxGap bandit problem" In general bandit problems, the lower bound is characterized by an optimization problem over a family of alternate models, and the closest model does not have to be explicitly available, though the resulting lower bound becomes not explicitly written. The analysis like those around Figure 8 will be useful (and interesting) to simplify the candidates of alternate models but the current result itself does not seem to be very useful because of the same drawback as LUCB as pointed out in the first review. The characteristic like "other arms may have to be sampled even if its own gap is small" also appears in other extensions such as linear bandits. 