Summary - The proposed approach to image captioning extends two prior works, object-based Up-Down method of [2] and Transformer of [22] (already used for image captioning in [21]). Specifically, the authors integrate spatial relations between objects in the captioning Transformer model, proposing the Object Relation Transformer. The modification amounts to introducing an object relation module [9] into the encoding layer of the Transformer model.  - The authors evaluate the proposed approach on the Karpathy split of MSCOCO dataset, demonstrating state-of-the-art performance on automatic metrics, in particular in CIDEr-D score. (The compared approaches only include Att2All [20] and Up-Down [2].) - A few ablations and baselines are present, including comparison to a standard Transformer model. Tests of statistical significance show that the proposed model outperforms the standard Transformer in terms of CIDEr-D, BLEU-1 and ROUGE-L, while SPICE-attribute breakdown shows improvement for Relation and Count categories. Qualitative results include examples where Object Relation Transformer leads to more correct spatial Relation and Count predictions.  Originality - The proposed approach combines ideas from [2,9,22] in a straightforward manner.  - Several prior works are not cited [A,B,C,D]. [A] proposed context-aware visual policy network, significantly improving over Up-Down. [B] introduced a GCN-LSTM model which integrates semantic and spatial object relationships. [C] further made use of scene graphs to integrate object, attribute, and relationship knowledge. Finally, recently [D] proposed “relational captioning”, a new task where multiple captions need to be generated for an image to summarize different relationships present in the image. Comparison to these works, both conceptual and empirical is necessary.  [A] D. Liu, Z.-J. Zha, H. Zhang, Y. Zhang, and F. Wu. Context-aware visual policy network for sequence-level image captioning. In 2018 ACM Multimedia Conference on Multimedia Conference, pages 1416–1424. ACM, 2018. [B] T. Yao, Y. Pan, Y. Li, and T. Mei. Exploring visual relationship for image captioning. In Computer Vision–ECCV 2018, pages 711–727. Springer, 2018. [C] Yang, Xu, et al. "Auto-encoding scene graphs for image captioning." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. [D] Kim, Dong-Jin, et al. "Dense relational captioning: Triple-stream networks for relationship-based captioning." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.  Quality - The proposed approach appears technically sound, as it extends prior work in a straightforward manner. - The very similar prior work [21], which also utilized Transformer for image captioning, is cited, but no empirical comparison is included. Also, recent results from other works are not included [A,B,C]. - Although comparison to state-of-the-art is incomplete (see above), the authors provide an informative ablation/baseline study, including tests of statistical significance, etc.  - No evaluation on the online MSCOCO test server is included. The online test set is meant to demonstrates models’ performance in a blind scenario, where overfitting is less likely.   - No human evaluation is included, although the authors are aware that some of the automatic metrics poorly reflect human preference (L178); in fact, all automatic metrics fall short of capturing human preference, thus human evaluation is desirable. - It would have been interesting to see some failure cases in additional to the presented (cherry-picked) success cases, or to see some discussion on that.  Clarity - The paper is clearly written and was mostly easy to follow. - I do not find Figure 1 helpful in terms of illustrating the proposed approach, e.g. no explicit visualization of spatial relationships is present. The main idea is not obvious from looking at this figure. - It is somewhat confusing that different tables reflect different settings, e.g. with/without self-critical training, with beam size 5, 2 or 1 (?), and it is not always clear which case is shown and how different numbers in different tables relate to each other.  - It would also be helpful to report the two standard settings, with beam size 5 with/without self-critical training, compared side-by-side, as in [2].  Significance - The idea of harnessing spatial relations for image captioning is not new [B,C]. The proposed approach is not particularly technically innovative, but appears quite effective. The overall improvement over prior work [2] is impressive, but in comparison to more recent works [A,B,C], the results are on par / only marginally better. Thus, I rate the significance of the presented contributions as Medium/Low.  UPDATE The authors did not include empirical comparison to prior work [A,B,C] or human evaluation, promising to do so in the final version. The results on the test server are not put in context with any of the baselines.   With neither a clear comparison to state-of-the-art [A,B,C], nor human evaluation (which is always desirable for tasks like captioning), I am not convinced about the overall significance/impact of the proposed method.