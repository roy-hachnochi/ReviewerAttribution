1) Originality: The authors propose an incremental modification to the transformer network which accounts for spatial relationship between the objects. The ideas of using transformer encoder+decoder architecture, pre-training the network with cross-entropy loss and fine-tuning in self-critical setup using CIDER scores with rollout as reward are borrowed from previous literature.  2) Quality: The proposed modification i.e. the geometric attention weights are supposed to encode geometric relationships between objects. The accuracy or success of this modification is measured via a proxy of better performance on captioning metrics and spice sub-scores for relation/count/size etc. Though the captioning metrics show improved performance and some qualitative results indicate that the network understands spatial relationships, it is hard to tell if the network is learning meaningful spatial relationships in its geometric attention weights. A visualization for the attention weights for an appropriate layer (say for the qualitative examples in Tab. 7) may be useful to demonstrate that the network does indeed learn spatial relationship information. Also, the method does not get a statistically significant improvement for the "size" sub-score of SPICE metric in Tab. 5. The proposed geometric attention weights use relative size of bounding boxes in eq. 6, but this does not translate into performance gain.  3) Clarity: The paper is well written and easy to follow. The contributions are stated clearly and sufficient details of the method are present. The experimental evaluations are properly described with sufficient details.  4) Significance: The paper proposes an incremental modification to the transformer networks for image captioning, which can help boost performance on captioning metrics. The proposed method can be a useful trick in captioning network implementations for small performance gains.  Post-rebuttal comments --  The paper proposes a modification that boosts performance in practice. The author feedback clarifies my concerns regarding experimental setup of the result tables and with this additional explanation, the tables are consistent. I still feel experimentation on different spatial features (the core idea of the method) is missing, the attention visualizations are interesting and should be discussed more in the final version.