Originality: Previous theoretical work on the subject is to quantify the amount of additional labeled data required to attain non trivial robust error whereas Theorem 2 quantifies the additional unlabeled data required. The contribution due to the meta algorithm is minor since classification with L^stab and L^adv has been studied before.   Quality: The theoretical results are sound and the claims are well supported from the experiments  Clarity: The paper is well written for the most part.   The term certified l2/accuracy is not defined.  Line 94 says difficult to learn classifier. For the gaussian model, the classifier must be easy to learn. Isn’t that so? Line 126-128. It is difficult to follow the logic in line “As n grows … goes to 0”. It seems that it is based some unexplained geometry. Same goes for the line with “parameter scaling”.  Significance: The theoretical results are for a simple gaussian model, instead of a more realistic one. The results on real datasets might be quite different. Using more datasets for experiments might be more convincing.   Furthermore the ratio of positives to negatives 1, which is again a special case. What happens when there is class imbalance? 