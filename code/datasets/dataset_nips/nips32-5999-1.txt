Updates: I appreciate the additional experiments and clarifications in the rebuttal. I think this is a good paper and would like to increase the rating.  Overall, the paper is clearly written and easy to follow. It proposes an interesting novel approach to pooling that leads favorable gains in performance.  Although the core mechanism, i.e. estimating pooling parameters using global features, is from the previous GFGP method [1], I think connecting it to probabilistic models is not trivial and can be regarded as a satisfactory technical contribution.  My biggest concern is about its practical usefulness. It says that the proposed pooling method requires additional O(C^2) parameters, which are not negligible. For example, we could simply use more convolution filters with average/max pooling to improve performance.  It would be more convincing if authors can somehow compare methods with roughly the same number of parameters to prove that the improvement is not just about the increase of model capacity in general by adding more parameters.   I'd like to know more details of the derivation of the approximation in Eq.15, particularly how these fixed number are derived. Also, Eq.16 is confusing because this only applies to the case sigma_0 = 0 but not in general.  Authors mention the log-Gaussian as a possible alternative to iSP-Gaussian, but no reported in experiments. Was it totally impossible to train log-Gaussian based model because of instability? 