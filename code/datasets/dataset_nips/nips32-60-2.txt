The paper is well written and well structured.  It clearly references existing work on (this type of) meta-learning, and gives an excellent overview of the area. The concept of "implicit differentiation" probably deserves a slightly better introduction, explanation + more references, given its relative importance to the main contribution. Apart from this slight exception, each section of the paper is well presented, concise, and easy to understand.  I especially appreciated the references + background section, and the interpretation of iMAML relates to, builds on, differs from, and generalises previous work. I wish the paper were a bit clearer on what iMAML "gives up" compared to normal MAML, e.g. by pointing out situations where MAML might succeed and iMAML fila (and vice versa). I also would have wished for more detail + resolution in the experimental section, and perhaps for a slightly more thorough experimental comparison, or a stronger demonstration of superior performance (e.g. a wall-clock measurements of CPU- and memory usage). Nonetheless, it is an excellent paper that I enjoyed reading and would recommend to accept at NeurIPS 2019.