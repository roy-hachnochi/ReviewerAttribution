Originality: The work is original in its modelling of meta architecture search. The approach used builds upon existing methodologies in variational inference (coupled VB) and neural architecture search (bayesian formalism of DARTS search space) (Perhaps also similar to [1] in the use of Gumbel Softmax trick)  Quality: The work is well presented. Relevant baselines have been cited in the comparative study. An empirical analysis to understand different aspects of the proposed model is also provided.  Clarity: This work clearly motivates the problem statement, provides a solution and supports it with an empirical study to understand the learnt posterior distribution of different tasks, the effect of prior and the flexibility of model in few-shot learning setting.  Significance: As NAS methods are often expensive, it is natural to look for meta modelling approaches that can help share certain aspects of NAS across tasks. This work is a useful step in this direction.  Weaknesses:  The tasks considered comprise of different resolutions of the same dataset. It is well known that models learns on ImageNet transfer to different resolutions. Evaluating the approach on a diverse set of tasks would be useful.  General comments A latex typo on Page 5 \texttt{CIFAR-10}   [1] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, Kurt Keutzer: FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search. CoRRabs/1812.03443 (2018)