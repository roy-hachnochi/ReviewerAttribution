The idea seems to be novel to me. I specifically, like the skip-connection part which allows one to compose network with different functioning layers. The only limitation is that the number of such elements are hard-limited to K and it is not permutation-invariant. However, I think the paper still stands without them. The experimental results have been presented elaborately. It is also very interesting to look at the PCA results of the weights and the architecture and how the different datasets are organised with respect to others. cifar10 and imagenet32 are close in the architecture space, but different in the weight space - which makes sense as the image sizes are close. I am not a deep learning person, so I am unable to see the limitation from the real application point of view. But looking from the meta-problem and the Bayesian methodology, I would love to accept this paper.   Minor comments are: 1. The paper needs a good proof reading to fix many type-setting issues e.g. extra 'a' in line 19, extra 'then' in line 53, part of line 66 can be writte better as "...network as a composition of L layers of cells...", extra textit in line 198.  2. The authors also have not spent any effort in giving proper references: sometimes conference names are in long format, somethimes they are short. Even it has duplicate references, [24] and [25]. Please fix them in the later version.