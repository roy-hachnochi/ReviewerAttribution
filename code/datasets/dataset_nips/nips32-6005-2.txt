1) The proposed model might not address the error propagation issue for long-range sequence imputation tasks as effectively as claimed. The imputation process highly relies on the predictions made at the coarsest level (r=1), since the predicted value will be encoded and utilized to update hidden states. The coarse-level imputation tasks, meanwhile, are relatively difficult because of two factors: first, the available information at this stage is the incomplete sequence only, unlike fine-level imputation tasks where we could utilize former predictions; second, the imputation task is formulated to estimate the value of a 'midpoint' and the skipped time-steps at this stage is large from both sides. Thus, the predictions at this level should be considered noisy, and bring negative influences on further imputations that treat equally on the observed and predicted values. The training of the decoder at this level also raises some concerns, which is specified in the next bullet point.    2) The training of multiple decoders might be insufficient. As discussed above, coarse-level decoders (i.e., g^r with smaller r) are of great importance. However, the training of decoders at the coarse levels are not sufficient due to 1) the number of training instances are sparse for a large size gap; and 2) decoders are optimized individually. Also, although the decoded results are further utilized (e.g., g_1 -> x_3 -> h_3 in Fig.1), it is not specified in the paper whether gradients are back-propagated from these paths and it is non-trivial to fulfill this. One interesting potential would be realizing some sharing of parameters between decoders at different levels.    3) There are also some concerns about experiment settings and results analysis. The selection between maximum likelihood and adversarial training object is not clear. It is stated that two objects are for deterministic (Traffic and Billiards datasets) and stochastic (Basketball dataset) settings, respectively. However, it seems straightforward to use L2-loss based training object for the Basketball dataset and vice versa. It would be better to have discussions of training objects selection or experimental comparison.     The evaluation metrics for experiments on Basketball dataset are all indirect measurements. Considering the ground-truth position trajectories are provided in the dataset as (x, y)-coordinates, it's better to include the L2-loss of different approaches.   Besides Figure 6, it will make the experiment results more convincing if various percentages of missing values are studied for more datasets. For experiments on Traffic and Billiards datasets, the presented results are only when a large amount of data is missing (e.g., 122 out of 144, 180 out of 200)   Some minor comments:  - In Algorithm 1, the step of updating mask m_t after imputation is missing, and recursive imputation is not presented clearly. - In Eqn 3, the choice of 'pivots' (at t-n_r, t+n_r) is not aligned with the description, i.e., using observed or predicted values. - What are the input representations of missing values for encoders?   - The plots of error in Fig.4 are not very well readable.  