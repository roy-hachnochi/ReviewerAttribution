This paper studied the non-asymptotic convergence rate of the TD algorithm under overparameterized neural network approximation. For such nonlinear function approximation, TD algorithm may not even converge. The paper exploits the fact that the overparameterized neural network has implicit local linearization, which guarantees the convergence of the TD algorithm to global convergence. The paper also generalizes the analysis to Q-learning and policy gradient (in appendix). This is the first study that made connection to recent breakthrough analysis of overparameterized neural networks, and leveraged the property for establishing the convergence rate for reinforcement learning algorithms. The contribution is solid. The presentation of the paper is clear and easy to follow.   I have the following question for the authors to clarify. The neural TD algorithm adopts the average of the parameter matrix W as the output. It allows the constant stepsize, but requires preset the number of iteration T. Is there any specific reason to adopt such an algorithm? What happens for the simple semi-SGD output without taking the average? This may require diminishing stepsize to converge or constant stepsize to converge to a neighborhood of the global minimum. But in terms of the analysis, will the current approach be applicable?