The paper consists of solid enough efforts to analyze the convergence rate. While I am convinced by the results, I cannot verify the details of the proof.   I have some questions regarding the theory, e.g. I had a difficulty of finding the notion of "one-point monotonicity" in the paper, which probably should be included for self-containing. The assumption 4.3, 5.2, incorporate some constants c1, c3, \mu, however, at least in the main paper, there is no clear explanation of how these constants relate to the convergence rate.   The paper involves a two-layer neural network, but still, it seems to be like a linearly parameterized model, as well as the rate, if all these assumptions are to apply. The implication of theorem 4.4, 4.6 has not made clear how the width of neural network effects the rate, and it seems to be hidden in the big-O notation, so it is hard for me to value the importance of overparameterization, or explicitly, the number of nodes in this two-layer neural network, comparing to number of iterations.