This paper considers the problem of DP reinforcement learning under continuous state space setting. To be specific, this paper requires DP output of the value function approximator, given neighbouring reward functions. The previous algorithms fail to work since the authors are considering a continuous state space setting. This paper incurs Gaussian process mechanism to add functional noise iteratively in the training and give a theoretical analysis on the privacy guarantees. Utility analysis is also given under discrete state space setting since there exists no theoretical guarantees even for the non-private version of the algorithm under continuous setting. This pape is a nice application of Gaussian process mechanism. Although the algorithm is not complex, yet this paper makes contribution to the community of Differential Privacy, since reinforcement learning is a very important problem in DP.  This paper is generally well written although some polishing is preferred. There are some typos and some places are confusing.