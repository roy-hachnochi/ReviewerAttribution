Update: I read authors' response and would like to keep my accept rating   Summary: Authors look at boosting from a different angle that is commonly done. Each new step of boosting tries to build a learner that has highest correlation (they call it an edge) with a modified distribution of the instances labels. The edge is usually an empirical edge, calculated over the available sample of the data. Authors argue that such empirical edge is not always a good unbiased estimate of the true edge, which we really ultimately want to maximize. Then they argue that when memory is limited, we should only store examples with higher weights, since they contribute more to the estimate. So they propose to use weighted sampling to re-populate the sample with more important instances They also draw examples one by one and estimate the loss, once enough examples were drawn to give a good estimate, the best split is chosen. This leads to potentially smaller samples (we don't need to see the whole dataset to decide) and less memory consumption (keep only important samples). The experimental results show that at least on one dataset, the speedup AND generalization performance is much better than that of Xgboost and LightGbm  I found it an interesting and a refreshing way of thinking about boosting. The paper is well written (albeit somewhat loaded, see my suggestions). My main critique is limited number of experiments (2 datasets) which produced mixed results.  Comments/Questions: 1)What authors refer to "early stopping" - eg read instances until you saw enough to choose the best split is not new per se. For example Tensor Forest https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE uses hoeffding bound. But i do admit that i have not seen it applied to boosting before, and the formulation is different and interesting 2) What happened in Splice dataset? What gave an edge to LightGBM there. 3) I think you need to run experiments on more datasets - among two tested, one is impressive and another is not so much  Minor:Line 37 we flushes->we flush 