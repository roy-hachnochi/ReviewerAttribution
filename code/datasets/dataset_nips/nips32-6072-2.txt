This paper focuses on memory efficiency of boosting methods and proposes a new implementation of boosted trees. By combining three techniques, i.e., effective sample size, early stopping, and stratified sampling, the proposed method reduces the number of examples scanned while controlling the estimation error. Empirical results show that the proposed method can run efficiently with memory smaller than the training set and time faster than XGBoost and LightGBM.  Here are several concerns and questions: 1.  The paper should rearrange the algorithm description to make it more clear so that the algorithm can be reproduced. 2.  The paper is well organized, while it needs to give the intuition of the main ideas in the early part which will make the paper more readable. 3.  This paper uses a class imbalance example to explain the intuition of weighted sampling which is confusing. Besides, the two datasets seem to be imbalanced. In consideration of imbalanced classification problem, there are a number of methods reducing computation complexity. Related works about class-imbalance learning should be discussed. LightGBM also has options for imbalanced classification problem which should be discussed.  Updated after reading feedback: I have read the author's feedback and all reviews. The feedback addresses my concerns, and I tend to vote for accepting this paper. 