=Statistical Strength= Throughoutt the paper, you refer to the concept of 'statistical strength' without describing what it actually means. I expect it means that if two things are correlated, you can estimate properties of them with better sample efficiency if you take this correlation into account, since you're effectively getting more data.   Given that two features are correlated, optimization will be improved if you do some sort of preconditioning that accounts for this structure. In other words, given that features are correlated, you want to 'share statistical strength.' However, it is less clear to me why you want to regularize the model such that things become correlated/anti-correlated. Wouldn't you want features to be uncorrelated (covariance is the identity)? Rather than having correlated redundant features, why not just reduce the number of features, since this is a tunable hyperparameter?  I can imagine an argument that it is better to have a model with lots of features where there is redundancy, and you carefully control for this redundancy instead of one where you reduce the number of features (num activations in intermediate layers). This argument would be consistent with recent work on overparamterization and implicit regularization. However, you would need to test this in your experiments.  =Line 46=  I don't understand this statement, and I expect other readers will not too. Why set priors that would seek to mimic the behavior of the prior-less learning method? Why would this lead to better generalization?   =Effect of batch size= I was concerned by how big the effect of the batch size was in figure 2. The difference between 2B and 2A is considerable, for example. This suggests that there is considerable implicit regularization from using different batch sizes, and that the effect of this may be substantially bigger than the effect of Adareg. In particular, the difference between 2B and 2A for a given method seems to be bigger than the difference between methods within either 2A or 2B.   I know there has been work on the connection between mini-batch SGD, natural gradient, and posterior inference in https://arxiv.org/abs/1806.09597  I am not up to date with this literature, and I expect there is follow-on work. It is important to comment on this.    In general, assessing the impact of regularization techniques is difficult because there are so many ways you can regularize. You could also do early stopping, for example. The important regularization technnique that I wish you had discussed more is simply using a model with less capacity (fewer hidden activations). See my comment on 'statistical strength.'   =Minor comments= The claims in the first sentences of the abstract are unnecessarily general. Why make broad statements about 'most previous works'  or explain how all neural networks are designed?  You repeatedly refer to things being 'diverse' in the intro, but don't explain or motivate what that means enough. 