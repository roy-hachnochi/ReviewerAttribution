This paper considers the regret of X-armed bandit problem. They propose a lower bound on the regret of this problem. Then they show their MeDZO algorithm, which has the advantage that need nothing to be the input, and can achieve good performances for any fixed parameters. They also show the theoratical regret upper bound of this algorithm.  The idea of using the empirical distribution before is very interesting, which can reduce the number of arms we needed while keeps the regret in a low level.   The regret upper bound of this policy matches with the lower bound provided in this paper, which means both bounds are tight. I think I have read some papers that use a similar idea, but this one is the first one I know about using this trick on X-armed bandit model.  The proof seems to be correct, but I do not check thoses ones in supplementary file in detail. The writting is also clear for me to understand this paper.   I found there are no experiments in this paper, and I am wondering how much the regret gap will be when comparing the MeDZO policy with other ones who takes \alpha as input and then make the optimization.   After rebuttal:  - I saw the experimental results, and I think add some experiments into the paper is useful for readers to understand the contribution of your papers.  - The similar ideas: for example, in "Reducing Dueling Bandits to Cardinal Bandits", their Doubler and MultiSBM policy both regard an empirical distribution on arms as a generalized arm.