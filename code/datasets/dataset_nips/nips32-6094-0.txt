This paper considers the problem of Non-linear Inductive Matrix Completion (NIMC) in a deep learning formulation. In NIMC one is given a query set, an item set and a few query-item relevance values, and the goal is to learn the query-item relevance function. The main contribution of the paper is to provide theoretical guarantees for using a one hidden layer network to estimate that function via an L2 loss. In particular, this can be thought of as two one-layer networks, one learning the embedding of the queries and the other the embedding of the items, while the relevance function is taken to be the inner product of the outputs of the two networks. The authors prove that for sigmoid and tanh activation functions the objective function is locally strongly convex around the global optimum and that stochastic gradient descent converges linearly if initialized sufficiently well. Interestingly, the authors show that even though ReLu is harder to analyze, locally strong convexity is also possible if one suitably removes some redundancy from the objective function.  The paper is mostly well written (especially the first half of it), with well-informed literature, featuring significant and novel results. Technical correctness was also checked to the best of the reviewer’s ability. However, there are some weaknesses that the authors should address:  1. Even though the analysis contains sufficiently novel elements, it is quite similar in structure     with some of the papers already appearing in the references. I believe it would be fruitful     for the community if the authors actually acknowledge this and in fact describe very carefully      what is standard methodology and what is novel. In that spirit, there is a lot of room for     improvement in section 3.3. 2. Along the same lines, i find the writing in section 3.4 unsatisfactory. It is too cryptic and it is     not essential for a short paper: this type of initialization has already been discussed in references     appearing in the paper and a short mention that this can be applied should be enough. Hence      i suggest reducing section 3.4 to a short sentence.  3. Instead, more experiments are needed: a) please show the convergence rates for sigmoid and      tanh. b) please compare results using “good” and random initialization and c) please show some     more realistic experiments with higher values of d,k,n and real data.  4. Please clarify: removing the redundancy from the objective function requires some knowledge     about the optimal solution, correct? If so, what is the applicability of this result? 