In my opinion the paper is very well written. I particularly like that they explain their theoretical results in intuitive words.  This paper provides theoretical recovery guarantees for non-linear inductive matrix completion (NIMC) for several activation functions such as sigmoid and ReLu. They show that this non-convex problem behaves as convex near the optimum, and give initialization conditions to "fall" near the optimum.  As the authors explain, NIMC tightly related to a one-layer neural network. Providing theoretical guarantees for this type of problems is indeed a very fundamental one, and arguably one of the most important in our field this days. However, the real problem of interest lies in deep networks, as opposed to the single-layer case. In fact, single layers are rather easier to analyze, and have in fact been analyzed quite a bit (e.g., their own references: Zhong et. al, Recovery guarantees for one-hidden-layer neural networks, ICML 2017; Li and Yuan, Convergence Analysis of Two-layer Neural Networks with ReLU Activation, NeurIPS 2017, and more). Hence, while the paper does provide new theory for NIMC, I believe the authors are overselling the importance/novelty of their contribution.