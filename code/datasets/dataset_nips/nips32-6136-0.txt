POST-AUTHOR FEEDBACK: I thank the authors for their explanations, which addresses my concerns. I look forward to seeing the camera-ready version with improved results and the clarifications. =======  The paper is well-written, the technical results are precise and correct as far as I can tell, and the results are novel and interesting. However, I am slightly puzzled over a few points, some of which pertain to the interpretation of the results. I believe the paper would significantly benefit from clarifying the following points:  * The authors refer to mirror descent with (fixed) weighted Euclidean distance as an adaptive gradient algorithm, but this connection is not clear to me. Adaptive gradient algorithms would imply a time variant conditioning matrix A_t, instead of a fixed A. Unless I am missing something, this distinction should be clarified.  * The authors should bring out the intuition behind the importance of the quadratically convex geometry. The notion seems central to the results, but from the main text of the paper it is difficult to understand why it is so. The one place I can see it being used critically is in the proof of Proposition 4, in switching the order of inf and sup. If this is the critical step, this should be emphasized more clearly.  * Am I correct to interpret that the switching of inf and sup in Proposition 4 would mean that an adaptive gradient algorithm (time-variant conditioning matrix A) is equivalent to a fixed one from minimax regret point of view, since the right-hand side allows for the optimization of the weights (\lambda) based on the parameter (\theta)?  * For the results on arbitrary gradient norms, it is not clear whether Corollary 3 follows from Theorem 2 or Corollary 3. The conditions in the Corollary are looser than those in Corollary 2, but the language surrounding the corollary makes it sound like it follows from Corollary 2.  * It is hard to interpret the bounds in Corollary 3. It would be illustrative to compare the upper and lower bounds for specific cases, and demonstrate the highest and lowest possible gaps between them, in order to get a sense of what the result means.  * In the definition of minimax stochastic risk, the authors define F_P(\theta) as the expectation with respect to P, i.e., the data distribution. Right after the definition, they also state that the additional expectation in the definition is with respect to the data, which is distributed by P. This seems confusing to me, F_P(\theta) should already be a deterministic quantity. If this is a typo, it should be fixed, and if it is not, the differences of the two expectations should be clarified.  * On p.3, in the definitions of M_n^S(\Theta, \gamma) and M_n^R(\Theta, \gamma), the supremum over the input set \mathcal{X} is taken *after* the infimum w.r.t. \theta_i. Since this is minimax regret, it is not clear why the supremum is taken before the infimum. 