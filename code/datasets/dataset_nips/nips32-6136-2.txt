============= After Author Response ==================== I have read the authors' rebuttal, my evaluation remains unchanged. =================================================== This paper shows the constraint set and gradient geometry matters a lot in the convergence (regret) of stochastic gradient methods, and provide concrete cases of when to use adaptive gradient methods in terms of constraint set geometry.    The main results is 1) for quadratically convex sets, the adaptive gradient methods are minimax optimal; 2) for non-quadratically convex sets (e.g. Lp norm balls with p in [1,2]), any linear mirror descent methods are far from optimal. Such a sharp characteristics of minimax rates of stochastic gradient methods in terms of constraints geometry provide useful insights in understanding the practical behavior of adaptive gradient methods.    