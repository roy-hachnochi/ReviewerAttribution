UPDATE AFTER READING AUTHOR FEEDBACK ================================================================================  I would like to thank the authors for taking the concerns expressed in my review very seriously. The author feedback addresses my concerns very well. I think with the promised fixes this will be a strong paper with an original idea that could be simple enough to be used in practice. From a theory perspective, the paper might spark new ideas in readers since TI is explained very well and there are probably more connections between TI and VI.   ORIGINAL REVIEW ==================================================================================================  The paper proposes a series of new lower bounds to the model evidence in variational inference that generalize the standard ELBO. The proposal is based on a discretized version of thermodynamic integration (TI). Intuitively, instead of evaluating log p(x) directly by solving the intractable integral $p(x) = \int p(x,z) dz$, one first evaluates log p_0(x) for a reference model p_0(x, z) for which this is easy to do. One then changes the model p_0(x, z) on a continuous path until it is deformed to the target model p(x,z), and one integrates up the changes that this procedure incurs on the evidence log p(x). By choosing how the integral is discretized, one obtains either a lower or an upper bound on the evidence.  I find the proposal convincing and well written. The underlying idea of TI is simple and well explained. A non-obvious way to obtain stochastic gradient estimates is also well explained. Experiments focus on models with discrete latent variables, as it is advertised that the proposed gradient estimator is applicable even to this situation. I am curious if the authors expect similar performance gains for models with continuous latent variables.  My only main comment is that I cannot find a discussion of the variance of the gradient estimator. The proposed bound is tighter than the ELBO, i.e., it is less biased. Usually, bias reduction comes at the cost of an increase in gradient variance (see [Bamler et al., NIPS 2017] and [Rainforth et al, ICML 2018]). Larger gradient variance slows down convergence of gradient descent.  The variance of the proposed gradient estimator would be interesting for another reason. The paper proposes to use a new gradient estimator although the standard REINFORCE method would in principle also work for the proposed bound. As far as I can tell, the only argument against REINFORCE gradients would be to reduce gradient variance (the use of discrete latent variables is only an argument against reparameterization gradients, not against REINFORCE). However, the proposed gradient estimator in Eq. 11 looks like it could suffer from high variance too because one has to estimate a covariance between two quantities. To estimate a covariance, one has to estimate the difference between two quantities that are typically of similar magnitude (see Eq. 12). This is hard to do numerically as absolute values cancel approximately while variances add up.  The "Related Work" section mentions only work related to TI. There has been more work on tighter bounds for black box variational inference, e.g., the above mentioned papers or [Li and Turner, NIPS 2016].