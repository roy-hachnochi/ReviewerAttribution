This paper shows that policy iteration converges to the Nash equilibrium in a LQ zero sum game. The authors also propose several nested gradient algorithms that converge to the equilibirum as a rate which is linear in a neighborhood of the equilibria.  The paper is technical and very specific to the LQ case. None of the arguments used in the paper can be used in a more general case.  The reader is  concerned with  the claim in lines 183-184. it is assumed that the set \Omega contains the Nash equilibrium. What if this is not the case? can the whole approach still be adapted or everything falls apart? Can one give conditions under which this assumption is satisfied? 