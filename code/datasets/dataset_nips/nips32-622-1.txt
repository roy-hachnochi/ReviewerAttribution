This paper considers the problem of reconstructing a random permutation online.  Similar to the secretary problem, n items arrive online and the algorithm is given each item's rank relative to items that arrived before.  The algorithm must then guess the item's absolute rank among all n.  The goal is to minimize the distance to the true ranking, as measured by the total number of inversions.  The authors consider a method based on guessing each item's rank by assuming the ranks seen thus far are uniformly spread (plus some random noise).  They show this method results in O(n^{3/2}) inversions in expectation.  They moreover show that no algorithm can do asymptotically better.  This is an interesting and non-trivial probabilistic exercise, with interesting connections to the theory of random search trees.  The authors do a good job of explaining the technical challenges and how they overcome them.  But I found it hard to get excited about the results.  Since the problem is so difficult, and the total number of inversions is quite high even if the algorithm does "well," it is hard to understand whether this analysis separates good algorithms from bad algorithms.  Indeed, the general approach used here is the first thing one would try.  Is there a different natural approach that does not work as well?  The one especially interesting aspect of the algorithm is the addition of random noise for the analysis, which is useful in other learning contexts as well.  Is this crucial?  I.e., does the algorithm perform worse without using noise, or is it just helpful for the analysis?  Overall, while I appreciate the technical analysis behind this work, I am not sure what insight to take away from the paper.  For that reason, I think the paper is not quite ready for such a competitive venue.  Comments post-rebuttal: I take the authors' point that whether the "total number of inversions is high" depends on the normalization.  But I don't think renormalizing affects the question of whether the proposed approach is the "right" one for any particular application.  I feel as though this could be addressed with some compelling use-cases, to illustrate why "total number of inversions" is the right objective function.  Regardless, I found it interesting that bucketing techniques get only part of the way to the improved performance, and this helped clarify for me what is technically challenging about this analysis.  Post response and discussion, I feel as though this is a cool and clever mathematical exercise, but without a concrete application or a crisp take-away message, I still worry about fit. 