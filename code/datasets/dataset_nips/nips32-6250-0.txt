Summary:   This paper has a very interesting claim: distributional shift in imitation learning settings is primarily caused by causal misidentification of the features by the learning algorithm. An interesting example is that of a self-driving car policy trained on a dataset of paired image-control datapoints collected by an expert human driving the car. If the images contain the turn signal on the dashboard then the supervised learner learns to have very good predictive power by indexing on that feature in the image. Clearly that does not generalize during test time. While this is a pathological example, such behavior is present in most settings where usually the state is blown-up by appending past states and actions. The authors clearly show this phenomena in various settings like mountain-car, hopper and some selected Atari games.  Two methods are proposed as fixes:    Interventional setting: In this setting a mixture policy (trained on a uniform distribution of binary masks over the features and an initial seed dataset of states-expert action pairs) is rolled out and the expert is invoked on a subset of encountered states that have the most disagreement with individual binary masks over the features. Note each binary mask encodes a particular causal graph structure. The loss wrt the expert on this subset of states is recorded and a linear regressor trained to result in an exponential distribution over all graphs. Finally the best graph in this distribution is returned.   Policy execution setting: In the setting where there expert intervention is not possible, it is assumed that access to episodic rewards is at least present and that such rewards are a good proxy for expert behavior. In this setting the graph parameterized policies are rolled out and the probability of that particular graph is proportionally increased or decreased based on the corresponding episode return. Finally the best graph in this distribution is returned.   For image observations as in Atari games a latent space observation via VAE is used to get disentangled observations.  Experiments on MountainCar, Hopper, and selected Atari games like Pong, Enduro and UpNDown show that using the methods proposed, results in achieving reward that is similar to that can be achieved using the 'original' state space features (without the confounding previous actions added on) even if the state space is blown-up ('confounded') by appending previous actions.   Overall comments:  - Pretty well-written paper overall though I have clarification questions below. Overall I find this to be quite an important paper in learning-from-demonstration work and something that will spark quite the discussion at the conference.   - Algorithm 1 \pi_{\textrm{mix}} should be cross-referenced to lines 201-202 and ideally introduced there. I am a bit confused between \pi_G and \pi_{\textrm{mix}}. \pi_G is well defined and clear. What precisely is \pi_{\textrm{mix}}? Once I have trained f_{\phi} which according to lines 201-202 is the mixture policy how do I get \pi_{\textrm{mix}}? Note f_{\phi} has a particular signature which takes the appended graph binary vector G. Is it all ones for \pi_{\textrm{mix}}?  - What is the disagreement really capturing in Algorithm 1? After walking through some toy examples myself my intuition says that states with high disagreement score by definition have lots of different answers depending on which G is compared against. G's which have causal misidentification will have different answers against the rest of the Gs. Does this assume that causally misidentified Gs are the majority of set of Gs and good Gs are one or few? If so do is it possible for all the bad Gs to agree with each other thus achieving high agreement score? Some light on this will be useful.  - How should one set N compared to |G| in Algorithm 1? How many G's were needed in practice in the domains experimented with here?  - Table 2: No disc-intervention?  - Ultimately from all the experiments is the takeaway that one should not do IL on confounded space (blown-up state space with previous actions appended) in the first place? Is this problem mostly because people tack on the previous actions and exacerbate the causal misidentification issue?    Update: ----------- After reading author response I am upgrading my score. An important paper in LfD literature. 