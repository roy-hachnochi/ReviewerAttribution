Compressing word embedding matrices is an important application, useful both for using NLP application on small devices and for generating more efficient (and less polluting) NLP models. The authors present an important contribution in understanding and evaluating different methods for compression. The paper is well written and explained (I liked the survey on sections 2.1 and 2.2). The only real concern I have with this paper is the large amount of content in the appendix (33 pages!). It seems that much of the content, including proofs that are important for the paper's argument, were put in the appendix to save space. I am not sure I have a clear idea on how to save space, but I would certainly encourage the authors at the very least to use the ninth page to bring some of the content back to the paper if accepted.  Questions and comments:  1. The word embedding reconstruction error measure (section 2.2) assumes that X and X~ have the same dimension. But this does not hold for compression, in which k < d. How can this method be used to evaluate compressed models then?  2. How do the authors apply GloVe to do question answering on SQuAD (section 2.3)?  3. Section 3.1.1: a potential way to speed up the score computation is to only consider part of the embedding matrix (i.e., a subset of the vocab, using a smaller n). Did the authors study the effect (in theory or in practice) of this relaxation?  ===================== Thank you for the clarifications in your response.