The authors propose a new metric for evaluating compressed word embeddings that they term as the eigenspace overlap. They prove that calculating the overlap between the left singular vectors of the uncompressed and compressed embeddings is a reliable metric and a good proxy for the performance of the compressed embeddings on downstream tasks.  The paper makes several contributions: First, they display that earlier compression methods do not significantly outperform a simple baseline of uniform quantization. Second, they show that the metrics used to evaluate these embeddings cannot explain this behavior of uniform quantization being better than the other methods.    Motivated by these findings they propose a new metric called eigenspace overlap that is able to not only explain the surprisingly high performance of uniform quantization as compared to the others, but also provides us with an easy to use metric that correlates better than most  previous methods on downstream tasks.   To me, especially striking is the result that uniform quantization does as well as these other methods, highlighting the fact that the previous compression techniques were not thoroughly compared to relevant baselines. This paper performs an average case analysis of the generalization error using compressed vs uncompressed embeddings based on linear regression and show that larger eigenspace overlap results in better expected generalization performance for the compressed embedding. The authors show that it is also a good metric for choosing between different compressed embeddings both in terms of being accurate as well as robust.  Originality: New metric for calculating the quality of the embeddings. Paper distinguishes itself from previous metrics which are adequately described for the purposes of the paper.  Quality: The technical content is sound, with thorough analysis and proofs of proposed methods.  Clarity: Well written paper, very easy to follow.  Significance: Useful for low memory applications 