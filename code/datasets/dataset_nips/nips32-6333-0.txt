This paper proposes an approach to optimally select samples for a small replay buffer to perform continual learning (CL) without forgetting. Like previous work (eg. GEM/A-GEM) the problem is formulated from the perspective of constrained optimisation (minimise loss on current sample subject to loss not increasing on previous ones). Unlike GEM, with clear separation and knowledge of tasks, this approach addresses the general non-stationary learning problem. The paper proposes a theoretical argument for using the variance of gradients to select samples for the buffer. One related work that could also be cited is "Adapting Auxiliary Losses using Gradient Similarity", by Du et al, 2018 (https://arxiv.org/abs/1812.02224)  I think this is a novel idea that is well worth exploring and could be a nice publication, but have a number of questions and concerns that I believe need to be addressed first.  On the technical side: - What if no feasible region exists? Eg. for a binary classification problem, two [almost] identical samples with label noise would be enough for the feasible region C to be the empty set (as the gradients would be exactly opposed). - One concern I have is with the successive approximations made. We want Ctilde to match C as closely as possible, this is then relaxed to Ctilde being as small as possible --> minimising solid angle --> minimising the surrogate objective. Fig 2 shows the accuracy of the last step (and in fact, how well does this work for a bigger range of angle/surrogate values?); but how does this translate to approximation error in the original objective? Which constraints are typically satisfied or violated (ie. are there qualitative differences in the kinds of samples that are remembered and forgotten)? - Given an impetus to minimise "size" how do we stop Ctilde choosing a [close to] empty set (eg. Two opposing constraints from noisy data as suggested above) - The iCARL CIFAR-10 results reported here seems poorer than even the CIFAR-100 result (ie. a much harder task) reported in the original paper - where does this discrepancy arise? - Can you quantify the computational improvement of GSS-greedy over the others? This is just stated in passing in the text in lines 220-222. - A number of the reproducibility checklist options haven't been satisfied - things like error bars / uncertainty, hardware used for experiments, specification of evaluation protocol, etc. Most of these are easy, and I think uncertainty over multiple runs is important (at the very least for external comparisons) - Additional benchmarks would be nice in order to contextualize these results - currently it compares against GEM and iCARL.  On the writing side: - The related work section is extremely brief, and the paper needs to be better positioned, first within CL, and then constraint-based CL. - The intro seems to be quite critical of prior-focused methods and needs more citations, and possibly toned down language (some of it seems emotive, such as "enjoy the beauty of...", "Prior-focused method has bad performances..." - Figure 1 is not immediately clear, I would show the blue constraint on just the left hand plot to indicate it is being removed on the right. (Showing on a plot something that is removed is a bit difficult to follow) - I'm still not clear on what iid online (as opposed to offline) refers to. Is it just training a single model on iid data for the same number of steps as the incremental setting? If so, why not start the curves at zero as well for consistency? - While the intro motivates the approach broadly as relaxing i.i.d assumptions within each task (lines 46-50), the only experiments performed are with a sequences of tasks, each shown i.i.d. As such, I think the motivation / claims need to be narrower, or additional non-stationary scenarios evaluated.  Other minor points: - Avoid conversational or imprecise language, eg. "Way smaller" on line 118, and  "this exact" in 129. - There are also some minor grammar issues throughout (such as "bad performances" in line 52), so please double check for consistency. - Line 151: "as shown in Suppl" doesn't make sense. - Is "disjoint MNIST" not the same as Split MNIST? If so, I'd change it to be consistent   POST-REBUTTAL:  After looking at all of the reviews and the rebuttal, I don't think my position has really changed.  Our concerns seem to be mainly that it would be nice to have more experiments on some other standard benchmarks, as well as some additional discussion and analysis on the high-level picture, particularly in terms of how the different steps and approximations behave and interact.  I feel this is a nice idea, but there's more the authors could do to make it even more convincing. As such, I'm thinking of staying with my original score of 6.