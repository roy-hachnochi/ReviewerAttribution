Overall Comments This paper is reasonably well motivated and provide justifications for the key use of integrated gradients as part of the computing the confidence score. The paper also presents several empirical demonstrations of the algorithm. The key motivation is that one might want to compute calibration scores without retraining like is typical for isotonic regression and platt scaling.  Originality I am not aware of work using integrated gradients for computing calibration scores. However, the literature on interpretability and uncertainty representation is vast. In this regard though, the paper does a pretty comprehensive job covering several recent work.   Quality The work is well motivated and demonstrates that this type of technique is needed. In addition several of the experiments do indeed show that the work captures some sense of uncertainty that is typically reflected in modern neural networks. One piece that is currently missing from this work in my opinion is a clear and solid proof of concept; I'll expound on this later in this review.  Clarity This paper is reasonably well written and I was able to follow several of the clarifications behind this work. A few suggestions, the abstract as it currently stands is quite dense; the authors could easily cut out a few sentences and still retain the overall story. This is a matter of style so feel free to disregard, but the first four sentences of the abstract could be easily cut without any loss in meaning. This is the same for large parts of the introduction. The second paragraph of the introduction could be broken into two and condensed more succinctly.   Sentence starting with "Robust machine learning models ..." on line 97 seems to me to be a matter of opinion. One could easily build a robust model, (not sure what definition of robust you have here), that still places a significant amount of attribution on a lot of inputs dimensions. I'll return to this point later on.  I like section 3; it is clear and a good mathematical motivation of your work.  Significance The motivation of this paper is significant, and as far as I can tell, there are not a lot of works that have proposed this metric before.   Some Questions and Points to address  - Need for a proof or concept or demo. I know reviewers are often accused of asking for extra experiments during rebuttals, which is not often realistic; however, here is one that would better convince me of the utility of this work. Train, say a logistic regression classifier on a dataset where you can reasonably control how well calibrated this classifier is. Now get the 'ground truth' classification with platt scaling or one of the baseline methods that you mention, finally compare this 'ground truth' classification to ones derived with Algorithm 1. This will greatly nail your point home.  - Sparse of Integrated Gradients Attribution. One key motivation in this paper for using integrated gradients is that it is not efficient to sample in the neighborhood of high-dimensional inputs, so one can use the attribution to weight the kinds of inputs that one samples. This is a fine motivation; however, it is not clear to me that integrated gradients attributions should be sparse. For things like MNIST, and Fashion MNIST where the background is black, the attributions are typically sparse, however, in looking at examples of integrated gradients attributions on Imagenet, these don't look sparse, so it is not clear how much the IG attributions will actually help. Can the authors comment on this?   - Too much focus on adversarial examples in demonstration. This paper spends a lot of time showing that algorithm 1 produces low confidence on adversarial examples. Sure, adversarial examples are extremely popular topic these days, however, calibration and uncertainty by themselves are very important topics, and I would encourage the authors to look beyond this specific application. I stress this because, an adversary that takes into account the fact that you are using an IG attribution map can fool your method to produce high confidence scores on adversarial examples. To send this point home, the methods in citation 57 are exactly what one would need to do this.   Update After reading the author rebuttable, I am planning to stick with an accept for this work. The authors have provided baseline comparisons and answers more clarifying questions. I think this is a valuable contribution that could spur future work in this direction.