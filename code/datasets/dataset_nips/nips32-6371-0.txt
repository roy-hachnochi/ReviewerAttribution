The authors propose a new way to use replay to avoid catastrophic forgetting. What the authors propose is after each new example to train on the previous examples that had the loss go up the most after the update. This is a very simple and it is clear why this should be better than replay uniform at random. In order to make their algorithm feasible the authors first select uniformly random subset of C examples, then they have to compute how the loss changed between all those C example due to the parameter update and then they do a training step on the K examples where the loss changed most. As a way to make this more efficient the authors propose to instead do nearest neighbor search in the latent space of an autoencoder to find similar examples. This method reminds me of memory based parameter adaptation by sprechmann et al (https://arxiv.org/abs/1802.10542) where the authors also do continual learning by doing nearest neighbor search in a latent space. I am a bit worried about this approach in so far as when the new task is very dissimilar to the old task the examples retrieved by lookup in autoencoder latent space could well end up being quite meaningless, however the nearest neighbor search will give examples which are not very diverse. In general it seems to me using the nearest neighbor search in autoencoder latent space would easily give examples which are not very diverse. Have the authors observed this in practice? Could you comment on whether you see this as a problem? The experiments seem fairly toyish to me, however I think the idea is nice and should be published