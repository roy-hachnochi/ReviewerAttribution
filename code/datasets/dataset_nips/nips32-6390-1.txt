==================== update after author response & discussion ====================  Thanks for your response to our reviews. I'm happy to recommend acceptance, assuming that the promised tweaks will be made and any comments from the Meta Reviewer taken into account.  ==================== original review ==================== Originality: medium-high Quality: high Clarity: high Significance: medium-high  Although the results are perhaps not astonishing, I think this paper explores an interesting direction, I enjoyed reading it and it made me think more about the problem.    Questions & comments:  - Is the pseudocode example in Figure 1 representative of the granularity of the pseudocode in the whole dataset? In the real-world this level of granularity might be arguably unnecessarily high -- I would at least expect lines 8-10 to be compressed into something like min_i = index of smallest element in A[i..n] On the other hand, line 94 says that "annotators were encouraged to provide high-level descriptions". Could you add a comment on (what you think) the impact of the pseudocode granularity would be on the method's performance?  - (Line 32 and Related work) The idea to not just look at the top-1 prediction seems sensible, and using a neural component to guide a search procedure exploring more possible solutions until one consistent with an input-output specification is found, is an algorithm template that has been used in program synthesis before. I'm familiar with DeepCoder and PCCoder, but maybe you can find better examples. Coincidentally, DeepCoder also looks at a few (very easy) competitive programming problems.  - (Lines 67-70) The requirement that the final source code have the same number of lines as the pseudocode seems artificial (real pseudocode may often require translating a single line into multiple lines of compilable code). Could you comment on the (lack of) importance of this restriction? Would anything break if more source code lines could be produced per pseudocode line, or does the method routinely sidestep this restiction by putting multiple semicolon-separated statements on a single line?  - (Lines 95-96) What is the corresponding pseudocode:code length ratio for the SPoC dataset? Based on the information provided on line 122, would it be roughly 1:1.15 (assuming equal token lengths), which would make it lower than the NAPS synthetic dataset, or are the pseudocode tokens on average shorter than code tokens?  - (Line 102) Could you please clarify what is meant by the sentence saying that consistent granularity of the descriptions is ensured "by design"? It seems that a non-automatic check might be needed to ensure that different codeworkers produce descriptions on an equal granularity level? The existence of the TestW test set seems to also suggest that different crowdworkers might produce systematically different descriptions. The next question is also related.  - When annotating each line of code, are the crowdworkers allowed to take into account the semantics inferred from surrounding lines, or does the annotator have to translate each line of code into pseudocode independently, ignoring its context? For concreteness, given these two lines of code ... 7: prefix_sums[0] = a[0]; 8: for (int i = 1; i < n; i++) prefix_sums[i] = prefix_sums[i - 1] + a[i]; ... would it be valid to describe line 8 as "compute prefix sums of the array `a` into prefix_sums"? This interpretation relies on the fact that prefix_sums[0] was set to a[0] on the preceding line. Or would a more granular translation of line 8 into pseudocode be required?  - Is there any reason except simplicity for the translation seq2seq model to be applied to each line independently, not conditioning on the context (surroundings) of each line of pseudocode, nor on the candidates generated so far?  - (Line 178) Is the LSTM that the L line embeddings ae passed through bi-directional or uni-directional?    Minor comments: [Line 212] "shows" -> "show" [Line 218] "one another" -> "with one another" [Line 219] "have an even" -> "have even" [Line 223] "have least" -> "have at least" [Line 226] And also assuming that it is always possible to combine individually correct candidates together to form a correct program. [Line 270] "Many" -> "Much"