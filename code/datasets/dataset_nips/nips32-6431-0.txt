The paper is well written and even though notation might be heavy when reading the proofs, the authors try to give intuition behind their approach. I have minor questions about some of the presentation in the paper.  On line 167 do the authors assume that the size of the population is |\mathcal{X}| = 10m?  On line 176 shouldnâ€™t the inequality be an equality, otherwise how is the above distribution over \mathcal{X} proper?  While first reading the paper, I had the following confusion about the intuitive explanation between lines 179 and 181: we need the mass on the single point to be very large compared to m, however, in that case it is highly likely that the returned classifier will have small generalization error because it will classify the point with large mass correctly and then the probability to sample every other point upper bounds the generalization error. Maybe the authors can hint at what the size of \mathcal{X} is compared to m?  From my understanding an important part of the lower bound proof is carefully balancing between the distribution which puts large mass on a single point and the distribution which slightly biases one of the labels compared to the other. It still a bit mysterious why exactly these two types of distributions are needed. It seems that the terms appearing in Eq. 3 are the ones governed by the two distributions. Elaborating more on why these terms show up and their relation to the two distributions might make the presentation better.  Overall the contributions in terms of lower bounds are significant and almost match the state of the art known upper bounds. The proofs are non-trivial and to my knowledge are novel.