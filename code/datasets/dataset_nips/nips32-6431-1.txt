This paper provides a lower bound that matches existing margin-based upper bounds for voting classifiers. These existing upper bounds bound the true error of a voting classifier, as generated by boosting algorithms, by the sum of its empirical probability of obtaining a large margin, and a generalization term that depends on this margin. The provided lower bound nearly matches this upper bound, up to log factors and small powers.  The paper presents two theorems. Theorem 2 is a lower bound that can be directly compared to the upper bound. Theorem 1, in contrast, replaces the empirical margin of the voting classifier returned by an algorithm (f_{A,S}) with the empirical margin of another voting classifier (f). This change of classifiers results in a lower bound that does not have a direct bearing to the upper bound, which is phrased in terms of a single voting classifier. The discussion of Theorem 1 (see line 113) doesn't seem to acknowledge this difference or explain why this result can still be considered a matching lower bound.  Theorem 2 does provide a lower bound which is directly comparable to known upper bounds, and is tight up to log factors and small powers. This theorem only holds for sample sizes that are slightly larger than what would be required to avoid triviality, m = (ln N/\theta^2)^(1+\Omega(1)). Strangely, this limitation is completely ignored in the text, although other easier limitations are discussed. The authors should address this limitation explicitly in their description of the result. The notation \Omega(1) in the power of m should be replaced by the actual number: the value here is of importance, since large values would make the result much weaker. It seems, from the supplementary material, that the actual value might be reasonable, however it should be explicitly spelled out.    Almost none of the proofs are provided in the body of the paper. Instead, long explanations describing the proof parts that are based on standard lower-bounding techniques are provided. This is unfortunate, since the supplementary material seems to include some interesting proofs that could have been easily incorporated in the body of the paper instead of long explanations of the standard parts of the proof. The result is a paper which provides almost no interesting technical contribution, although such contribution does seem to exist --- in the supplementary material.   The most interesting result and proof seem to be Lemma 3, which shows the existence of a relatively small set of hypotheses that admit voting classifiers with a given minimum margin if only a small number of the labels are negatively labeled. The proof of this lemma uses an interesting construction technique based on AdaBoost. However, like all other proofs, it is not presented in the body of the paper.   Considering the significance and novelty of the result, I find the provided lower bound mildly interesting, though not surprising. Had the proof of Lemma 3 been provided in the body of the paper, along with some of the more interesting parts of the rest of the proofs, this might have made a more interesting contribution. The current organization leaves most of the actual contribution out of the paper. In fact, very little that can be technically verified is provided in the body of the paper and almost all the derivations are in the supplementary material.   - line 110 "it is always possible" -- this should only be possible with the D whose existence is proved in theorem 1. - Sometimes lg is used, and sometimes ln. Please be consistent. - line 407 in the supplementary: "lebeling"-->"labeling" - Reference SFB+ includes an "et al" which is out of place.  - The mention that proofs are "deferred to the full version of this work" repeats in many places, where in fact the proofs are provided in the supplementary material.   