The minimax regret and the Bayesian regret in linear partial monitoring are considered in this paper. While introducing the connection between OSMD policy and Thompson sampling, this paper first proposes a generalization of Thompson sampling, where the expectation of the action (on a vector space) at each round is equal to the expectation of the optimal (fixed) action with respect to the posterior distribution of the loss. An upper bound of the Bayesian regret is provided, which matches the worst-case regret bound of OSMD. The proposed technique is also applied to the analysis of the worst-case regret of OSMD, which improves the known bounds for the classic k-armed bandit problem, a class of bandit problems with graph feedback, and online linear optimization.  As far as I understood, the framework (such as the Bayesian setting with non-i.i.d. priors) largely follows that in [21]. There seems to be substantial contribution in the theoretical aspect and the explicit construction of the algorithm (MTS), but I'm not sure about how novel and nontrivial the results of this paper are since I'm not familiar with the settings of this paper and [21]. In fact, the paper has too much weight on the theoretical aspect and I wonder that it may need more emphasis on the practical contribution to make it as a NeurIPS paper rather than a COLT paper.  *Remark 5: I do not understand why the MTS matches TS for this case. In MTS, X_{t+1} is determined by the posterior mean of A^*, where not only the posterior distribution but also A^* depends on past observation under the induced filtration. On the other hand, as far as I know, TS depends on the past observation only through the posterior distribution of the loss. Then I want to know why these seemingly different procedures become the same. *Line 80: The description is somewhat misleading around here since it seems from this sentence that linear partial monitoring is just an example of problems considered in this paper while all formal analyses in this paper are given for this problem, although the technique itself seems to be applicable to wider problems. *Equation after Line 133: x seems to be X_t. *Figure 1: The horizontal and vertical axes should be clarified (t and cumulative regret?)  ----- Response to the authors' feedback:  On the practical contribution, the feedback just refers to the description of the linear partial monitoring that is a generalization of some other problems and there is no additional explanation on how the results (possibly potentially) become benefitial in practice. After re-reading of the paper and the other reviews I became more convinced with the originality of the theoretical contribution of the paper. Still, considering that this paper is currently a purely theoretical one, I also think that the requirement on the theoretical contribution is much higher than other papers with practical contributions and I decided to lower my score.  Relation with TS and M-TS: my point does not seem to be caught correctly. For example consider the standard Bayesian setting such that the rewards from each arm is i.i.d. and the prior distribution of the success probability is distributed by, e.g., Beta(1,1), and we have T=11 rounds and have currently observed (5 successes and 0 failure) from arm 1, and (0 success and 5 failures) from arm 2 for the two-armed bandit. Then in the TS that I know, a typical implementation is to generate random numbers theta_1 and theta_2 from Beta(6,1) and Beta(1,6), respectively, and the arm corresponding to the larger theta is chosen at the 11-th round. On the other hand, TS and M-TS that the authors explain, a naive implementation of the algorithm seems to become as follows. - Generate theta_1 and theta_2 from Beta(6,1) and Beta(1,6), respectively. - Generate random numbers (r_{1,1}, r_{1,2},..., r_{1,6}) and (r_{2,1}, r_{2,2},..., r_{2,6}) from Ber(theta_1) and Ber(theta_2), respectively. - If 5+sum_{i=1} r_{1,i} > sum_{i=1}^6 r_{2,i} then pull arm 1, and pull arm 2 otherwise. This algorithm itself seems reasonable since it exploits more when the number of remaining rounds becomes small, but it requires the knowledge of the time horizon and has a weaker connection with the standard TS. Another reviewer informed me that there is a line of works that use this definition of TS, but it is different from TS that most people know and this difference should be clarified.  "Equation after Line 133: x seems to be X_t" in my first review: sorry that "Equation above Line 133" is the correct point.