[Edit after the author feedback]:  I thank the authors for addressing my comments during the author feedback. I have read the authors' response as well as the other reviews. The authors' response addresses my concern regarding the motivation of the proposed method. Overall, I think this submission provides a new 'adversarial' setting, and I update my overall score as "6: Marginally above the acceptance threshold".  ========================================================== Summary:  This paper proposes a training time attack that is able to manipulate the behavior of trained classifiers, including deep models and classical models. The paper develops a heuristic algorithm to solve the expensive bilevel problem in Eq 4. The experimental results demonstrate the effectiveness of the proposed algorithm in this paper.  Pros: - The proposed heuristic algorithm is efficient and able to find good solutions for the expensive bilevel problem. - The adversarial training examples generated by the proposed algorithm can manipulate the predictions of the classifiers and transfer between different types of classifiers. - The generated training time adversarial examples can transfer between different models including deep and non-deep models.  Limitation & Questions: - Is there any comparable baseline method for this training time adversarial attack? - As shown in Figure 5, for the MNIST dataset, the model is 'robust' against the generated training adversarial examples when $\epsilon \leq 0.3$ and the percentage of adversaries is less than 60%. As 60% is already a large percentage, this makes the proposed attack less effective.  Typo: - L230, should be $f_{\theta}(g_{\xi}(x))$.  There is an omission in the related work on the bilevel problem in data poison attack: https://arxiv.org/abs/1811.00741