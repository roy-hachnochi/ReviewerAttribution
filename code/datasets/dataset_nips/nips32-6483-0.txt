Post author feedback:  I have read the comments from the author feedback and I maintain that this paper is well written and makes valuable contributions. I also think that the experimental analysis is sufficient.   ======  The authors investigate ADMM in the context of solving optimization problems which consider generative priors. Convergence guarantees for the problem of inverse imaging under generative priors have been limited and restricted to gradient descent based schemes prior to this work [Bora '17, Hegde '18]. The presentation of the paper is excellent, and reads well; citations to prior work are adequate. The authors highlight three key assumptions required to show convergence of the ADMM algorithm to a O(1/rho) radius of the true solution: restricted strong convexity of loss function L, strong smoothness and near-isometry of the generator G. These assumptions are fairly standard in literature, and papers which have previously used them have been referred to appropriately. This paper also additionally establishes theoretical guarantees for gradient descent when used for the problem setting of the paper.   Experiments in Sections 1 and 2 also highlight faster running times and epoch complexity, of ADMM with respect to conventional Gradient Descent, which is interesting.   Minor: [126] typo "architectures"  [777] !