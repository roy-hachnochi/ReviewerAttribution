 I think the "global optimization" aspect of the main result and the fast (i.e., linear) convergence rate are very interesting, and perhaps also surprising. For example, for the least square problem  min_z ||A G(z) - b||  prior works such as Hand & Voroninski [2017] and Heckel et al [2019] have established the global optimization aspect of simple gradient descent like algorithms. But the result obtained in this paper is much more general, and also applies to formulations with extra nonsmooth terms, with a practical numerical method.  Moreover, general understanding of ADMM applied to nonconvex problems is still very rare. I think this result is definitely a beautiful addition to this line of literature also.  Some minor comments:  * While the generative prior idea is connected to the generator network in GAN, I think it's potentially confusing to emphasize GAN in the subsequent generative-prior induced optimization problems, i.e., saying things such as "This GAN-based optimization problem" (also, "with the explosion of GAN in popularity... and (possibly) non-smooth problems") because this could very likely lead people to think of the min-max problem involved in GAN. So probably after the brief recap of GAN, one can stick to terms like deep generative model to avoid such confusion.  * It's interesting to see how the near-isometry property of G comes into play to yield the result. Suppose R = H = 0, can the main intuition of the proof be given, or maybe milestone steps that reflect how the property of G is used be sketched? I wonder how much the difference is to the convex case.