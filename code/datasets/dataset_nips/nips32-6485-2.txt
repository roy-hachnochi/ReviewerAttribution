Update (after reading the rebuttals):  After reading the rebuttal of authors, I have addressed my concerns on the novelty of the new self-normalized concentration, since the key point is that the coefficient of regularizer is changing.  I indeed appreciate this work. The idea of this paper is natural but there indeed exist technical challenges, and the authors address these issues elegantly. So I think it deserves an acceptance.  Nevertheless, there are still many typos in current verison besides those listed before, for example, in Theorem 2, eq. (6), it should be "\log(1/\delta)" instead of "\log(1/\gamma)". I hope authors could carefully check and revise the final version if accepted :)   ----------------------------------------------------------------- Original Comments:  This paper extends the paper of [12,13] from a sliding-window strategy to the weighted least square method. The idea is natural and well-motivated. The authors give theoretical analysis, which is based on the self-normalized concentration from [1] with a clever choice of the norm.  The analysis is essentially not hard but requires a deep understanding. Particularly, authors propose to consider the $M$-norm, where $M = V\tilde{V}^{-1}V$ in the WLS based linear bandits. This is by contrast with the common $V$-norm that considering in [1] and [12,13]. By considering the concentration over such a norm, authors can still appeal to the self-normalized concentration developed in [1] (with certain modifications) to WLS based linear bandits. Along with some standard techniques and tricks, they finish the argument. I appreciate the elegant idea.  However, there are also some deficiencies in the current version, particularly regarding the paper organization.  First, I would like to remark that the new self-normalized concentration claimed in the paper is essentially a direct extension of Theorem 1 of [1]. Since Proposition 1 is in the norm of \tilde{V} but not the $V$-norm, so authors can directly obtain the result from Theorem 1 of [1]. It seems  unnecessary to restate the supermartingale argument as done in Lemma 1-3. Please clearly state the novelty of this part.  Technical Issues: First, the current order for WLS bandits is O(dB_T^{1/3}T^{2/3}), the dependence in d is worse than that of sliding window linear bandits [12]. This can be improved by choosing \gamma as 1-(B_T/Td^{2/3})^{2/3} in line 470.  line 461: the inequality should hold for the maximum singular value of $M$. How can you guarantee it is the same as the maximum eigenvalue, as $M$ may not be symmetric.  Other minor issues: - line 459: there is a $\lambda$-term in $V$-martix, so is there an extra $\lambda$ missing in the 4-th inequality? - line 483: I do not think it is problematic to apply Theorem 2 of [12] to A_t, because the self-normalized concentration applies for all t>0. Authors are encouraged to add more explainations.  Additionally, the related work in Section 1.2 is not satisfied. Authors are requested to reorganize the related work to make it more informative. - line 67-69, authors first introduce the full-information dynamic regret [6], but it follows by work of linear bandit with static regret [21,32]. The organization should be reconsidered. - line 76-77, authors claim "[2] gives fully problem-dependent dynamic regret bounds in  O(log(T)).", however, it seems impossible to obtain a sublinear dynamic regret independent of the non-stationarity measure; besides, how can $O(log T)$ bound be regarded as "problem-dependent"? - line 78, the rate of [12,13] is not O(B_T^{2/3}T^{1/3}), the correct one should be O(B_T^{1/3}T^{2/3}), authors should carefully check it - line 83-84, I do not think [15] is the first work that considers the switching bandits. The seminal work of [Auer JMLR'02] has already given some results.  [Auer JMLR'02] Auer, Peter. "Using confidence bounds for exploitation-exploration trade-offs." Journal of Machine Learning Research 3.Nov (2002): 397-422.  Other related works on dynamic regret of contextual bandits are missing [Luo et al. COLT'18] and [Chen et al. COLT'19].  [Luo et al. COLT'18] Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in non-stationary worlds. In 31st Annual Conference on Learning Theory (COLT), 2018.  [Chen et al. COLT'19] Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A New Algorithm for Non-stationary Contextual Bandits: Efficient, Optimal, and Parameter-free. In 32rd Annual Conference on Learning Theory (COLT), 2019.  Some of the experimental descriptions are unclear and hard to understand. For example, line 280-282, what doe this sentence mean?  - how to add the noise? - how to calculate the accuracy of the algorithm?