The paper is written well. Overall, the problem considered in this paper is important for today's potentially large-scale applications where instead of the actual function value (that is given as an expectation or as a finite-sum over the whole data) we need to resort to stochastic estimates (i.e. an iid sample from the distribution or randomly chosen mini-batches). While the problem formulation is novel, the main novelty is in providing the unbiased estimates of the gradient of the lovasz extension and the rest follows immediately due to the convexity of the extension. The lower bound is also novel.   A note regarding related work: The concept of  submodular optimization with noisy (unbiased) oracles has been introduced before in two papers: (i) Gradient methods for submodular optimization, (ii) Stochastic Submodular Maximization: The Case of Coverage. To the best of my knowledge these works use SGD albeit in the context of maximization. Also, the work "Submodular Optimization under Noise" considers noisy (worst-case noise) oracles and should be cited as related work.  