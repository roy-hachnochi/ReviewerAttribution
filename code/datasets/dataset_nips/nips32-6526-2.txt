Originality:  I am not an expert on RL systems and so it is somewhat difficult for me to judge this topic. I would largely defer to other reviewers here. However, I really enjoyed the exposition and found the line of research original and interesting.  Quality:  To me, it seemed that the quality of the research was high. I was able to follow the theoretical development and found it compelling. I strongly believe that I could sit down and reproduce the work without much difficulty. One place where some more exposition / details might be useful was in the experimental portion. Could the authors elaborate on exactly how the simulations were performed?  A few more minor questions:  1) Is eq. 5 (especially converting from Q_{t+1} - Q_t) only valid in the \eta\to0 limit? 2) In eq. 8, is the series convergent? Would it be worth analyzing the \Delta x^2 correction to make sure it is subleading? Are there any conditions aside from n, m\to\infty that need to be satisfied here? 3) Eq 9 -> 10. It seems like in eq. 9 there are n-decoupled equations describing the per-agent Q-values. However, I don’t see how to get from there to a single agent independent equation. I.e. if two agents start with different Q values, surely their Q-values will follow different trajectories? Is this just saying that “without a loss of generality we can consider a single agent”? Which is a statement I agree with.     Clarity:  This paper was very clearly written. There are a few places where the language is not idiomatic, but I was very impressed as a non-expert by how easy the paper was to follow.   One minor question, in eq. 4 & 5 should we be considering a_j since x_{j,t} is the probability given to action a_j?   Significance:   Naively, it seems to me that this work could be quite significant given increased interest in ever larger multi-agent games. Having said that, the action / state space considered here is small. It would be nice to include more examples including some that actually have state associated with them.  