The preliminaries section lays out the required mathematical formulations of tensor networks. While the section presented serves its function, it could potentially be more clear if the authors spaced out and typeset the maths (similar to how it is done in Section 3.1).   The visualizations in Figure 2/3 illustrates how a fusion network (and hierarchical network) can be constructed with PTP units. These visualizations clearly communicate how features are pooled across modality and time step. That said, perhaps the descriptions about the HPFN (Section 3.2) are overly verbose. Analogies (such as the analogy connecting ideas to CNN architectures, L128-131) are helpful, as are the details comparing and contrasting connections to ConvACs [5] and DenseNets [11]. The related work section further details comparable architectures and limitations of previous work. Specifically, the authors note that the proposed architecture should be 'more' capable of capturing complicated multimodal correlations than comparable baselines.   The experiments presented in the paper cover the CMU-MOSI dataset [28] and the IEMOCAP dataset [2]. Both of these experiments are sentiment classification from text, acoustic, and/or visual features. For baseline comparison, the authors adopt similar featurization methodologies to previous work.  The model architectures section (L210-225) is not clear. I would like the authors to consider a table which would allow for reference of each architecture and respective details. The results are clearly presented in Table 1. I would like some measure of significance testing reported in the paper. The results are compelling and support the hypothesis that modelling high-order interactions is beneficial toward these classification tasks.  Details on how the models were trained, and the parameters were optimized is notably missing from the paper. Additionally, the empirical computational overhead is unclear. While the authors experimented with a collection of models, they do not compare the number of parameters, training time, or training curves of each of the models. This is likely important results given that the authors are arguing the performance benefits of the PTP units.