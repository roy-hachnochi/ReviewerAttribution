Originality: The idea is not entirely new. L106 gives an interesting explanation of F-Y loss, which seems to be a novel perspective. Basically, the paper focuses on a special case of the F-Y loss [6] for structured prediction; the consistency analysis of excess Bayes surrogate risk is based on the work of [27, 29] using calibration function.    Quality: All proofs look good to me (after a quick check).  The experimental part of this paper is relatively weak: it only shows the proposed F-Y losses work for selective examples. However, there is no comparison with prior works. Classical baselines such as SSVM, CRF, SparseMAP etc should be compared.  Clarity: The paper is generally well written with a clear structure.   Significance: Convex surrogates such as F-Y loss should be interesting to structured prediction community. This paper may attract more people (potentially deep learning folks) exploring better surrogates/pipelines for structured prediction. 