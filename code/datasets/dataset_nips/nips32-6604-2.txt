Originality: The paper provides a new PAC-Bayesian bound based on a notion of stability of the learning algorithm on two subsets of the sample. While the authors discuss links with other PAC-Bayesian bounds including Tolstikhin and Seldin's bound based on a different version of Bernstein's inequality, they fail to cite or compare their approach to   Rivasplata et al. (NeurIPS 2018), PAC-Bayes bounds for stable algorithms with instance-dependent priors.  This paper also combines the PAC-Bayes approach with the stability of the hypothesis learned by a learning algorithm on the sample, and also state that the empirical values can be much smaller than previous state-of-the-art PAC-Bayesian bounds.  While I did not thoroughly compare the proposed bound with Rivasplata's bound, I feel that the paper can not be accepted unless the authors do such a comparison, in terms of theoretical properties and empirical results.  Quality: The paper is complete and seem technically sound - all results are presented with complete proofs (and even a new notation to simplify some theorems and proofs). Empirical results on a toy dataset and some UCI datasets are provided to show the advantage of the bound when compared to other PAC-Bayesian bounds of the literature.  Quality: Sections 1 to 4 are clearly written and are easy to follow even if the subject at hand is complex. However, the following sections leave me confused - there are many back-and-forth discussions and results that I feel should have been introduced earlier in the paper, giving me the impression that this paper was originally shorter with a bigger appendix with complete results. My conclusion on this matter is that I think the paper could be easier to follow if it was more polished (and possibly reorganized).  Significance: The new Bernstein inequality seem significant and could be reused by other researches. However, the lack of comparison with Rivasplata's NeurIPS 2018 paper makes it hard to make a decision on the significance of the new PAC-Bayes bound or the authors.