originality: The regularization term has been developed before in semi-supervised learning on unlabeled data [26] and in adversarial training as a smoothness term [41]. This paper combines the two and conducts adversarial training by applying the regularization term on unlabeled data. The theoretical and empirical analysis are new.   quality: The paper is technically sound overall. However, some crucial points are not well addressed. For example, (1) the proper number of unlabeled data m. It is observed that the proposed method (UAT++) behaves  differently on CIFAR10 and SVHN (Fig.1), wrt m.   On CIFAR10, it outperforms others starting from m>=4k. On the other hand, on SVHN, it performs worse than even the baseline with a small m and performs similar to VAT. Although theorem 1 is provided on the theoretical aspect of m, there is no connections and analysis of it with the empirical observations.   (2) In table 2,  it is observed that the performance could drop with an increasing m. The explanation that the unsupervised data "contains more out-of-distribution images" renders the argument at the beginning of sec 4.2 less effective ("robust to distribution shift", "fully leverage data which is not only unlabeled but also uncurated").   clarity: The paper is clearly written and well organized.    significance: The idea of improving model robustness using unlabeled data is interesting and is likely to inspire more efforts in this direction. The idea has been empirically verified to some extent; however, some crucial aspects as mentioned above might requires more efforts to be better addressed.  ================Updates==================== The rebuttal from the authors has addressed my major concerns on the mis-match between theory and empirical results, as well as the claimed robustness against distribution shift v.s. the actual results. I believe the investigations on using unlabeled data for improving adversarial robustness is of great importance and this work makes a useful step towards it.