This paper considers the interesting question of whether model-based reinforcement learning can achieve good performing by using a myopic policy instead of doing full planning at each step, and presents an affirmative answer for this. Specifically, it is shown that UCRL2 and EULER can be adapted to use a myopic policy that does 1-step planning, preserving the regret and improving the time complexity by a factor of S.  How large are the constants in the complexity notations and how do they compare with their full-planning counterparts? Some experiments comparing the actual runing time of the greedy algorithms with their full-planning counterparts will be helpful.  Overall, the paper is well-written and the results are very interesting.  * Update after rebuttal The authors have clearly answered my questions about the the constants in the complexity notations and promised to provide experimental comparison between the greedy algorithms and their full-planning counterparts. I still vote for acceptance.