In my opinion, the main advantage over existing results such as [Recht et al, Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent] and [Duchi et al, Estimation, Optimization, and Parallelism when Data is Sparse] is the type of performance bound. While the bounds in the cited papers are mainly concerned with sparsity of the data, the bound in Theorem 1 is a generic regret bound of the order O(\sqrt{T}) for the convex case and O(\log T) for the strongly convex case. This is clearly related to delayed feedback results in prediction with expert advice.  The asymptotic behaviour of the regret bound for HedgeHog! is also O(\sqrt{T}).  There is also an advantage in the required conditions. The paper considers the case of convex (rather than strictly convex) target function in combination with a generic convex (rather than box-shaped) domain. All these conditions are well-known in the theory of gradient decent algorithms (and discussed at length in [Bottou et al, Optimization Methods for Large-Scale Machine Learning]), but not in the theory of asynchronous algorithms. The result of [Recht et al, Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent] on asynchronous algorithms is limited to strictly convex functions and the result of [Duchi et al, Estimation, Optimization, and Parallelism when Data is Sparse] to box-shaped domains. 