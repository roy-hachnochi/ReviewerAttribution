The paper touches an important problem of measuring the quality of conditional generative models. The author proposed Classification Accuracy Score -- a metric that is based on a performance of a discriminative model that is trained on samples obtained from the conditional generative model. The paper also discussed pros and cons of the proposed metric. The empirical study shows that a number of sota-level deep generative models fail to match the target distribution.   Pros: While the idea has been proposed before in Shmelkov2018, it was not widely used in the field. The current paper points out some limitations of deep generative models as well as limitations currently used metrics, thus the paper delivers a significant contribution. The paper is clearly written, the experiments look thoughtfully designed.  Cons: The original work Shmelkov2018 has not been cited. The proposed metric has a range of limitations that are listed in section 3 (e.g., the metric does not penalize memorization).  Despite the questionable novelty of the proposed metric, the paper provides a nice and interesting empirical evaluation with sota level generative models, that definitely is of interest to the community. The increasing popularity of the proposed metric also has a significant potential impact. I recommending to accept the paper.  Comments:  0) The original work Shmelkov2018 should definitely be cited.  1) The experimental evaluation suggests that IS/FD tend to over-penalize likelihood based generative models as well as over-penalize nearly ideal reconstructions from VAEish model. While the issue is discussed in section 4.3 (lines 270-280), it is still not clear why does this happen. I feel like there are some reasons for it that might be interesting and useful for the community.  2) The paper may also benefit from adding comparison on MNIST dataset as well as more extensive evaluations on CIFARs. The first, it is really interesting if the proposed metric is already perfect for a simpler dataset (for most generative models perhaps)? The second, a lot of groups have no resources to reproduce the ImageNet experiments, so more MNIST/CIFARs experiments might accelerate future research.  3) The one more case when the proposed metric might produce wrong results is Dataset Distillation (https://arxiv.org/pdf/1811.10959.pdf) however the case looks extremely unlikely.  4) Formatting recommendation: it is common practice to use a vector format for plots e.g., use pylab.savefig('name.pdf').  (Shmelkov2018) "How good is my GAN?." Proceedings of the European Conference on Computer Vision (ECCV), 2018.