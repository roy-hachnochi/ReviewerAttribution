Originality: To the best of my knowledge, this is the first paper which proposes continues neural representation which is compatible with multi view / projective geometry toolbox and at the same time does not require dense 3D ground-truth data.   Quality: The technical content appears to be correct. There are multiple aspects I like about this paper: + continuous representation, memory efficient while capable of operating at high spatial resolution + compatible with multiview/projective geometry + differentiable ray-casting is a contribution on its own + multiple applications + authors will release the source code and have paid a lot of attention to reproducibility and fair evaluation While the paper (resp supplementary) provide discussion about time and memory complexity, it would be good to add some real numbers into the paper. Similarly, it would be interesting to see how many iterations the differentiable raycasting requires, resp how slow/fast it is, and how it compares with standard non-differentiable algorithms in terms of speed (I'm not saying it has to be faster to be interesting and I do get the point that the differentiable one enables training of this model - I just think it would be interesting to put it more into context). Also, most experiments are carried out on very small scenes, typically consisting of a single object. It would be good to discuss how well the proposed could generalize to i) large-scale scenes and ii) unconstrained environments (ie not a single object, rather a room or some other more generic 3D scene); it might be interesting to add such experiments, using e.g. (significantly downsampled) ScanNet dataset.  Clarity: While the paper is pretty readable, there is certainly room for improvements in the clarity of the paper. In particular, the paper should be self-contained - many implementation details are provided only in the supplementary. While I do understand it is difficult to fit the paper into 8 pages, it would be great to move these details into the main paper. Most parts of the paper read well, however, I believe that adding a figure illustrating the whole model (not just raycaster) would help the reader. Figure 1 is pretty clear, however, text/symbols are very small and reader has to zoom in - would be great to fix. First few lines of abstract (1-5) seem to be bit rushed and would be great to rephrase them. Similarly first paragraph of introduction (l. 17-21) do not quite match the quality of writing of the rest of the paper (deleting l. 17-21 would not make the introduction any worse). But in overall, I've enjoyed reading this paper!  Significance: Exploring continuous neural representations is of great interest, as the discrete counterparts often do not scale well, often have significant memory and compute requirements and limited spatial representation. While the proposed approach was tested only on very small-scale and (relatively) simple scenes, it represents an important step forward. 