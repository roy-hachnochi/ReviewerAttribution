**Update after reading the author's feedback With the additional information on connections with existing work and comments and the practical setup, I'm willing to update the score to 7.  *******************************************************  The characterization of the distributed deep learning in the form of pipeline optimization seems to be a novel contribution, and the convergence results, particularly incorporated with randomized smoothing look reasonable. Some comments regarding clarifying the contribution: 1. The relationship between the proposed pipeline parallel optimization setting and existing work is not clear. Does it contain related work as special cases? The authors mentioned in the abstract that the presented study is distributed per-layer instead of per-sample. It could be helpful to give additional comparison along this line. 2. The manuscript seems to be short of details on the distributed computing mechanism. This was briefly touched in Section 2 on asynchronous value/gradient evaluation. Additional discussions such like the distributed framework, scalability etc. could add more practical value to the submission. This part is also unclear in the evaluation section of the paper. The improvement discussed in Section 5 over GPipe shows interesting trade-off, however as the authors mentioned those conditions are seldom seen in practice and the experiment setup seems artificial. 3. In the beginning of Section 4, the authors mentioned acceleration is possible. Whatâ€™s the counterpart that the method is evaluated against?  While the manuscript is an interesting read from the theoretical perspective, the reviewer is interested to see additional evidence on the practical impact such as improvement over state-of-the-art methods on well-studied applications. 