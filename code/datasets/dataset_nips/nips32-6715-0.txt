POST-REBUTTAL UPDATE: I am happy with the authors' response to my main question. I maintain that this is a top paper and it should absolutely be accepted. -----------------  This was the best of the 6 papers I reviewed. Intuitive results, great idea and execution on the main result. The writing is very clear the discussion on related work thorough and detailed, and the paper overall is a joy to read.   I believe that it should be accepted.  Some critiques I had about the limitations of the proposed method (warm start which needs to use an initialization of rank proportional to the ambient dimension) are already discussed honestly in the paper, which also includes an effort to propose a heuristic method to “bootstrap” from a high-rank initialization progressively reducing the rank in a logarithmic number of phases.  My main remaining question is the following:  Your main result is given with high probability, in expectation. Could you elaborate with some discussion on what that guarantee means, and what it does not in terms of the actual convergence of the algorithm? In my opinion that’s the only piece of discussion missing from an otherwise great paper.     Originality: The paper is motivated by a nuance in the known information-theoretic lower bounds for online stochastic PCA. The bounds suggest that the best possible rate is O(1/n), where n is the number of examples, but it critically relies on an assumption on the rank of the data. The authors point out how, for low rank matrices, this lower bound becomes uninformative, leaving the rank-constrain problem wide-open for improvement. The authors go on to do exactly that by proposing an algorithm that achieves exponentially fast convergence. Even though I have done some work in the area, I admit that I was unaware of this nuance in the lower bound. I have not seen other work exploit it in a similar way.  Quality:  The quality of the paper is excellent. From organization, writing, literature review, motivation and presentation of the results it is very well done and polished.   Clarity:  Excellent. I will recommend this paper to people interested in getting in the area and wanting to learn.    Significance:  Given the originality of the contributions 1 and 2 above, I think this paper can have a significant impact on the research community interested in similar problems.   Minor typo, Line 252: “shaper” -> “sharper”  