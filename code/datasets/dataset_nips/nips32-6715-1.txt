Originality: The method is novel and non-obvious. The original Krasulina method is literally SGD on the squared reconstruction error. However the generalization presented appears to be distinct from this form of generalization, and is instead motivated to preserve self-regulating variance decay as the residual decays. The relationship to related work, particularly VR-PCA, is explored in depth and adequately cited.  Quality: The body of the submission is technically sound, and I have no reason to doubt the validity of the proofs in the supplement; the structure is reasonable, though I have not checked the arguments in detail. The authors are careful and honest about addressing the strengths and weaknesses of the method, in particular proposing Algorithm 2 to overcome a weakness in the assumptions in Theorem 1, acknowledging the desirability of extending the theoretical results to effectively low-rank data, and providing a detailed comparison with VR-PCA in theory and practice. However, the significance of this comparison would be strengthened by considering runtime, not just epochs.  Clarity: The writing is very clear and well-organized. The introductory overview of approaches to PCA provides the reader with helpful orientation. There are two natural questions for which Iâ€™d appreciate further clarification:  Significance: The results are highly significant given the centrality of PCA in data science and the modern context of online algorithms on big or streaming data. The method improves upon existing theory and practice.