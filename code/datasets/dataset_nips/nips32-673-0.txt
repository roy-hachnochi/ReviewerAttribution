Update after rebuttal:  I thank the authors for addressing the main points that I brought up in my original review. In particular, I believe that they did a good job in making some of the intuitions underlying their design choices clearer; including those clarifications in the text would make the paper's claims more easily understood by the readers. Overall, I also believe that the rebuttal did a good job at explaining the high-level rationale underlying the proposed method, even though I still feel like the technique doesn't always comes with a solid theory behind it. On the other hand, as R3 mentions, standard TD(lambda) doesn't have particularly solid theory surrounding it either, and that needs to be taken into account when judging how solid the contributions of this paper are. I have kept my original scores for this submission.  -------------------- This paper introduces a technique to allow for the practical use of lambda-returns in combination with replay-based methods. This is an important problem because, while replay methods are important to decorrelate samples, they typically rely on sampling random minibatches of previous experiences, which makes it hard to readily adapt them to perform the calculations required by lambda-return methods. The paper proposes to store short sequences of past experiences into a cache data structure within the replay memory so that adjacent lambda-returns can be rapidly precomputed. Moreover, the stored lambda-returns can be interpreted as stable versions of target TD errors, which allows the authors to build a training architecture that does not rely on the typical target network used in Deep RL.  This is an interesting paper and (as far as I can tell) it does introduce an original contribution to the field. It is, overall, clear and accessible---except for a few parts of the text that I mention below.   I have a few comments and questions:   1) in Section 3.1, the authors argue that computing all of the lambda-returns in the replay memory requires only one Q-value per lambda-return, on average. This is not immediately clear to me. Could you please clarify why this is this case?  2) while I understand that eliminating a target network is an interesting and valuable contribution, the proposed solution requires "refreshing" all lambda-returns in the replay memory every F steps. In practice, other than requiring less memory, what is the main advantage of this method, vs. having to replace the current network with target network every F steps?  3) I do not fully understand the theoretical justification (and implications) of your decision to construct a cache of experience chunks by mixing one copy of the cache that is randomly shuffled and another one that is sorted by TD errors. Why is this needed and what happens if you use data originating from just one type of cache; i.e, only data that is completely random, or just data that sorted by its TD error?   4) regarding Figure 2, it is not clear what is being shown here. What are the x and y axes showing? Also, all curves look extremely similar: what is the conclusion that one should take from this analysis?  5) I do not fully understand the theoretical implications of the oversampling ratio hyperparameter (X) that you propose. This seems like a design decision that is not principled but that may work well in practice. Could you please discuss in which (more general) situations this approach would be appropriate?  6) the authors argue that their approach for efficiently computing lamba-returns is possible while maintaining an acceptably low degree of sampling bias. This is achieved by using a prioritization hyperparameter p, which needs to be properly annealed over time. Could you please discuss the implications of the annealing process (e.g. how fast p goes down to zero) in the transient properties of the trained network? I.e., *during* training, what happens when p is large (close to 1) and what could the negative effects be if we anneal it (say) too slowly or too quickly? 