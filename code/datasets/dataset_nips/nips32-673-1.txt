I've read the other reviews and authors' rebuttal. As the other reviewers, I do appreciate the proposed framework which enables the integration of lambda-return and experience replay at reasonable cost. In addition, the authors’ response seemingly addresses the other reviewers’ concerns on the design rationales. Hence, I’ve increased my assessment on the overall merit by one point, although I still have some concerns that the authors may present only favorable results for the specific choice of new parameters on the main idea of cashing. The authors’ rebuttal provides some useful intuitions on how to tune one of the hyper-parameters, X, but I was expecting concrete justification on the choice of hyper-parameters that are newly introduced in this paper, or study on the impact of each of new parameters. Nevertheless, as R1&3 mentioned, it may be beneficial to widen the spectrum of available/doable RL algorithms for the community, so I’m leaning towards accepting this paper. ---- The presentation is clear and well-organized. However, it is unfortunate that there is no study on the computational complexity of the proposed caching scheme, although reducing computational cost seems the main contribution. In addition, in the experiments, the gain from using lambda-returns compared to the standard 3-step DQN is inconsistent and even negative in some cases. Therefore, it is hard to believe that the proposed framework indeed provides some performance improvement other than gain from just introducing new parameters (e.g., X and lambda) to be optimized.