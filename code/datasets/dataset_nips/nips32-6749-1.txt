# Originality  The problem of incorporating memory in model-free RL is not new, however there is a general lack of qualitative analysis on the problem due to the lack of clear testbeds (since most current ones might have many confounding elements, or different focus) and baselines. This paper attempts at providing both, and thus makes for a good and original contribution to the NeurIPS community.  I also appreciated the focus on testing for generalisation across instances of the tasks, since that is an important metric that is often lacking in published papers in the area.   # Quality  The work presented is overall of high quality. The technical contribution is theoretically sound, as it is a relatively straightforward combination of existing methods. A satisfactory ablation study was provided, and the method was compared against a state of the art distributed RL algorithm, IMPALA.  The authors are mostly careful about their state claims about performance of their method, and they managed to mostly convince me of the quality of the presented testbeds.    # Clarity  The paper is well written, albeit at times a bit too reliant on the presence of supplementary materials. As this is a common (and not easily addressable) problem with work presenting testbed-baselines pairs, this didn't affect the score too heavily, however the exposition would have gained from strongly focusing on any of the two main contributions.   # Significance  The problem of incorporating and utilising memory in model-free agents is a relatively strong focus of the RL community, and this work sets out to provide both testbeds and baselines to work towards tackling this important issue. The paper provides some insights on the usefulness of auxiliary reconstruction losses, which confirm and strengthen previous findings.  Provided the code and the tasks are successfully released, this paper will make for an important baseline towards the quest to solve this general problem.