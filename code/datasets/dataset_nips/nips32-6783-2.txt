Tackling multiple senses of words is important, and this paper makes the first attempt to resolve the problem that each word corresponds to a single (sense) vector at the projection layer in text generation. I appreciate the motivation of this work, and the proposed model improves performance on various text generation tasks.  In terms of method, there's nothing particularly surprising. It adopts a sense embedding matrix instead of word embedding matrix, and uses heuristics to dynamically allocate senses to words. In addition, it employs kernels with a learnable variance in place of inner product. While these techniques are not novel, they may be practical and provide a baseline for future work.