Originality: The problem studied in this paper ("how to make an autonomous agent learn to gain maximal control of its environment") is not novel, with much prior work (see [Klyubin 2005] for a good overview). While the algorithm itself is novel, it seems like a bit of an ad hoc combination of ideas: bandits, curiosity, goal-conditioned RL, planning. Said another way, all sufficiently complex methods are likely to be original, but that in no way guarantees that they will be useful.  Quality: While I laud the authors for including some baselines in their experiments, I think they were remiss to not include any intrinsic motivation baselines or planning baselines. One simple baseline would be to use SAC to maximize reward + surprise. Exploration bonuses such as RND [Burda 2018] and ICM [Pathak 2017] are good candidates as well. Given that the environments tested are relatively simple, I expect planning algorithms such as iLQR or PRMs could work well.   Clarity: In terms of experiments, the method is complex enough that I don't think I'd be able to reproduce the experiments without seeing code. Code was not provided. The writing is vague in a number of places, and contains many typos (see list below).  Significance: I agree with the authors that we must develop control algorithms that work well when provided only limited supervision. However, my impression is that this method actually requires quite a bit of supervision, first in specifying the goal spaces, again in defining the distance metric for each (L121), again in defining the procedure for sampling from each, and finally for tuning hyperparameters. I suspect that a method with 8 components has a large number of hyperparameters.