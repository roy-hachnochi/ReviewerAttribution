This paper introduces an algorithm to learn how to execute tasks that might depend on other tasks being executed first. Each task is a goal-conditioned problem where the agent needs to bring a specific part (coordinates) of the observed state to a given target. The challenging component is that, to modify some coordinates of the state (like the position of a heavy object), the agent first needs to modify other coordinates (move itself to a tool, then bring the tool to the heavy object). Therefore, methods that always set goals in the full state-space like HER or HIRO don’t leverage anyhow the underlying structure and might considerably struggle with this environments. This is an important problem to solve, and the assumptions are reasonable.  Instead, the algorithm the authors propose fully leverages this structure, even if they only assume that the coordinates split is given (not their ordering or relationship).   - First they have a “task selector” that picks which of the coordinate sets (ie, task) to generate a random goal in. This task selector is trained as a bandit with a learning progress reward.  - The goal is then passed to a subtask planner that decides if another task should be executed before. To do so, a directed dependency graph of all task is built, where the weight on the edge between task i and j is the success on executing task i when j was exectued before (in previous attempts). The outgoing edges from node i are normalized, and an epsilon-greedy strategy is used to sample previous tasks. - Once the sub-task to execute is decided, a “goal proposal network” gives what were the previously successfull transition states. This network is regressed on previous rollouts, with some batch balancing to upweight the positive examples. - Finally, the actual goal is passed to  a policy that is trained with SAC or HER.  All points above also use a surprise component that seems to be critical for this setup to perform well. For that, a forward model is learned, and when the prediction error exceeds a fixed threshold the event is considered surprising. I would say that each individual piece is not drastically novel, but this is the first time I see all the components working successfully together. The clarity of the method explanation could be improved though.  Their results are quite impressive, even compared to other hierarchical algorithms like HIRO (albeit none of their baselines leverages all the assumptions on the task structure as they do).  They claim that learning progress is better than prediction error as intrinsic motivation (IM) because “it renders unsolvable tasks uninteresting as soon as progress stalls”. This is not convincing, because even in the cases they describe where an object can’t be moved, a model-error driven IM will stop trying to move it once it understands it can’t be moved. Furthermore, it seems from their results that only using the surprise, and no other type of IM is already almost as good. I would argue that the strength of their method is in the architecture, rather than in one IM or another. It might be better to tune down a bit the claims on which IM is better.  Overall, it is a slightly involved architecture, but that powerfully leverages the given assumptions. The paper needs some serious proof-reading, and the presentation and explanations could also be drastically improved. Still, I think this is a good paper, with good enough experiments, analysis and ablations. It would be interesting to push this algorithm to its limit, by increasing the number of tasks available, making the task graph more challenging than a simple chain, adding more stochasticity to the environment, etc. 