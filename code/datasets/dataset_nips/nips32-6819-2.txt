Note: the website links are switched around ("code" links to video).  This paper proposes a method for intrinsic motivation in hierarchical task environments by combining surprise-based rewards with relational reasoning. The problem that is addressed is an important one, as RL often fails in long horizon, multistep tasks. The idea of "controlling what you can" is an intuitive and appealing approach to thinking about these kinds of environments. CWYC assumes that the state space has been partitioned into different tasks. This assumption is not entirely unreasonable given the power of object detectors and localization.   Method: CWYC has many components that optimize different objectives. The task selector component is maximizing learning progress. A task graph learns which tasks are dependent on which, and selects a path through the graph. Subgoals are generated for each subtask with an attention process over all states, with different parameters per task. Goal conditioned policies for each task.   Originality: No single component in this method is orignal, but their combination is. I found it to be an interesting way to use both intrinsic motivation and object relations. This method seems to provide a very good curriculum for learning complex tasks.   Quality: The experiments are well documented and the method outperforms other methods, but it does not compare to other intrinsic motivation methods.   Clarity: Good.  Significance: Overall, I find the paper significant. While the assumption of a partitioned state space is a lot, it is not unreasonable with tools such as object detectors. The goal proposal structured output should be discussed more in the main text.   Potential Issues: - The task graph is not a function of the particular goal in the final task: this means that it may be unable to choose a good sequence of subtasks if the sequence should depend on the particular goal (if you want the heavy object to go left, you need tool 1, but to move it right, you need tool 2).  - The subgoal attention requires attending over all possible goals, which could be a problem in continuous or high dimensional state spaces.  - The experiments should compare to "Curiosity-driven Exploration by Self-supervised Prediction"