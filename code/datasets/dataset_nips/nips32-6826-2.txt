-Originality: Doing variational inference based on a standard ELBO with reparamterised gradients on the Poincare ball is new. It uses ideas similar to very recent/concurrent work (Ganea et al., 2018; Ovinnikov, 2018; Nagano et al., 2019), but it is made clear how this work differs from related work.  Quality: The submission seems technically sound, with detailed experimental results. The paper empirically compares their approach mostly with their Euclidean counterpart. This is fair, of course, but it would be interesting to see how it compares empirically with the Poincar√© Wasserstein Autoencoder (Ovinnikov, 2019) and the hyperboloid model of Nagano et al. (2019), like do they yield similar latent representations, how are the respective sample qualities?  -Clarity: The paper is polished and well written. The background on Riemannian geometry is to the point, so that the paper is in most parts accessible to readers without training in non-Euclidean geometry. Nevertheless, I feel that readers could benefit from more high-level guidance in Appendix B, like what do we learn from Section B.8 and B.9?  -Significance:  I feel that this is a significant work and others can build on these ideas either methodologically or experimentally. The experiments presented show advantages over Euclidean counterparts in different domains. I also feel that the proposed approach is easier to use for practitioners compared to some related work.  -Some comments/thoughts: I was wondering if instead of using a Gaussian, similar extensions can be made for other spherical distributions such as a Student-t, as they also have a stochastic representation of z=r*\alpha (Fang et al, Symmetric Multivariate and Related Distributions, 1990, Chapter 2)? Also Mallasto et al, Probabilistic Riemannian submanifold learning with wrapped Gaussian process latent variable models, 2019, considered the pushforward of a Gaussian Process by the exponential map. Could something like this be used here for a VAE on the Poincare ball, say with a GP prior (Casale et al, 2018)? Further, the Riemannian Normal distribution seems to be unstable in higher dimensions from Table 3. Are there different limiting distributions of the Wrapped/Riemannian model as the latent state space dimension goes to infinity, and does this provide some guidance for applications?   POST AUTHOR RESPONSE: Having read the rebuttal and the other reviews, I keep my initial score of 7. The authors agreed to improve Appendix B, which I felt lacked some high-level guidance. Their response also indicated that the proposed approach can be extended in different ways. My main complaint was that the paper lacks some empirical comparison with very recent related work (Ovinnikov, 2019, Nagano et al., 2019). However, even without such a comparison, I think it is still a complete and interesting paper.  