*************After author response************  Thank you for the answer. I'll keep my mark and vote for accepting this paper.  ****************************************************   In this paper, the authors provides a clear analysis for Stochastic Gradient Descent for least-square in a non-parametric setting on the interplay of: -the number of passes over the data set -the size of the mini-batches -the size of tail-averages -the step-size of the stochastic gradient descent  Originality: As far as I understand the paper, it does not seem that the analysis of each term of the SGD estimator uses now tools or really new technique, even if the analysis of convergence of the population risk gradient descent iterates to the real minimizer considered with tail-averaging is new. Yet, the techniques for bounding each term seem borrowed and adapted from previous papers analyzing SGD for least-squares problems -related papers are adequately cited.  Quality and clarity : Theoretically speaking, the paper is self-contained and provides proofs of all theorems and a clear discussion on all the assumptions made in the paper. Furthermore, despite the number of parameters concerned with the analysis, the main results (Theorem 1 and Corollary 1) are very clear and clearly compared with the relative work. However, the experimental section may lack of a real dataset where r can be computed and where we could see the difference between tail and uniform averaging. Moreover, when Theorem 1 and Corollary 1 speak about the interplay between 4 different parameters, the experimental section only provides a clear illustration of the difference between tail and uniform averaging (even if this is the main result of the paper).  Significance: The paper is very clear, very well-written, and is a clear summary for SGD for Least-Square in the non-parametric setting but lack from real novel ideas.