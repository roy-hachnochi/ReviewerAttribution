The empirical results were difficult to interpret.  Some of this difficulty was due to ambiguity of the results themselves, but the rest could have been addressed in the discussion.   The lack of consistency in results across tasks, coupled with a weak discussion of these experiments are concerning.  However, that the lambda=1 setting consistently outperforms the baselines and ablations suggests that the general technique is worth trying as a drop-in replacement for uniform sampling in experience-replay UVFA settings.   Analysis includes a proof on the bound of the sub optimality of the sampling strategy (which is nice) but it would have been helpful to include an empirical evaluation what effect sub optimality in sampling actually has on agent performance. Is performance highly dependent on getting sampling just right, or is anything that makes sampling more greedy sufficient?  Relatedly, it would have been interesting to compare with simpler prioritized-replay mechanisms, e.g. directly using the goal-similarity metric in a priority queue, particularly since this is easy to implement. "A diverse subset A of achieved goals encourage the agent to explore new and unseen areas of the environment and learn to reach 146 different goals." > In a HER/UVFA setting it seems that the online choice of goal is the biggest factor in determining exploration, vs. what is backed up offline.  I'd expect that in some cases sampling diverse goals could actually decrease exploration for a given goal by removing delusions in the value function.  Overall I think the connection between prioritized sampling, which this paper focuses on, and the exploration-and-exploitation trade-off, which is typically viewed as an online choice or a reward augmentation, is tenuous and warrants further discussion.     In Fig 3) CHER offers no benefit over HER and HEREBP baselines for tasks b & c, but is significant on tasks a & d.  To what do the authors attribute this difference? I also find it suspicious that the HER and HEREBP traces are nearly identical for all experiments, and even have similar dips in what is typically noise.  In Fig 4) tasks c & d, which are the harder and more interesting tasks, the effect of lambda is quite small, which suggests that the benefit for fig 3d vs DDPG-HER is mainly an effect of <our algorithm> vs <other algorithms>, and not the balance of greedy or diverse sampling. Might this be explained by other parameters or tuning for the baseline implementations?  This result seems particularly surprising since in fig3a, which is a similar task, CHER and DDPG-HER had equivalent performance.  To what can we attribute this inconsistency?   On tasks a & b lambda 100 > 0 and 10 > 0.1, suggesting that 1 is optimal, but that being too greedy is better than being to diverse.   This is an interesting result, but doesnâ€™t seem to hold in c&d.  Plot titles and axis legends difficult to read.  What is the compute cost of the proposed method?   Running an iterative optimization inside the batch-sampling step of a Deep-Rl algo sounds expensive.