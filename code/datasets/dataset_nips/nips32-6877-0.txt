Update: The authors have addressed my questions.  I hope in the camera ready there is a clear discussion on Taylor expansion VS. finite difference (at least in the appendix).  I also second the other reviewer on the importance of comparing in the batch norm case, since the method should be used as a general purpose initialization scheme.    Longer Summary: - Authors introduce the GradientDeviation criterion, which characterizes how much gradient changes after gradient step. Simple and avoids full Hessian (+) - They use meta-learning to learn the scale of initialization such that GradientDeviation is minimized. - They claim meta-learning the scale is mostly architecture-dependent can be done with random data and random labels, without the need of using a specific dataset (+) - They compare with other initializations schemes (DeltaOrthogonal, LSUV) and with Batchnorm. Beats other schemes and competitive with Batchnorm (+)  Shortcomings: - Can be problematic for very large models (-) - Can only learn scale of parameters (but from experience I think the actual distribution does not matter too much anyways for independent initializations)  The authors claim their approach is more general than analytical approaches. Analytical approaches do not account for nonlinearity.  Originality: Combines two-lines of work: learning to initialize and developing analytic initializations (e.g. Xavier, Kaiming).   Quality: Enough experiments, paper well written. Related section seems a bit thin.  Clarity: The paper is very well written and easy to read.  Significance: Their "GradientDeviation" criterion can be reused and explored by others. Their method can be combined with usual initialization schemes.     