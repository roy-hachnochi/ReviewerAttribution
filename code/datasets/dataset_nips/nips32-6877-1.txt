They propose a novel algorithm that automatically finds the good initialization for a neural network by meta-learning methodology even without specific data (i.e. in a domain-independent way). To do that, they propose a new metric that is called gradient deviation, which measures the scale of one step ahead gradient likewise in MAML. They assume that the gradient of good initial parameters is less affected by the curvature near it.   I think It’s nice to bring the idea of meta learning into learning the initial parameters of model and the objective function. But it’s lack of evidence that the method is working in a sense that there is no theoretical proof of experimental results.  Q1. I’m not the expert of optimization theory, so I can’t fully confirm that the hypothesis you made is valid or not. Is there any theory that plain surface in the initial point leads to a better local minima? Even if you have no theoretical proof, there should be an experimental support that Gradient Deviation represents a metric of good initialization (i.e. Gradient Deviation – final performance for various initializations of the model)   Q2. I don’t understand the protocols you used in experiments. Why do you have to remove skip connections and batch normalization layers? Is it natural to compare random init (or other known initialization method) vs. meta-learned init?  