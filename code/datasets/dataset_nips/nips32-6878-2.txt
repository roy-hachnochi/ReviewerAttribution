The paper presents a deep generative model for relational data. There is a considerable recent interest in modeling graoh-structured data, not only in traditional domains, such as social network analysis, but also in domains such as molecular design. Therefore, it is a timely contribution.   As is common witrh deep architectures, the paper assumes that each node in the graph has a deep representation with each layer's representation depending on the above layer's representation of that node (and other "adjacent" nodes), which enables capturing higher order node dependence. The representation in each layer is modeled by a Dirichlet random variable, which in turn is conditioned on node features (if available).  Comments:  - The idea seems to be inspired by recent work (Hu et al, 2017) and properties of Bernoulli-Poisson link function. However, the paper also makes some key improvements over previous work (e.g., higher order node dependencies), which results in better predictive accuracies.   - Inference for the model is done via MCMC sampling which for the proposed model is expensive and fairly complicated (relies on latent variable augmentation techniques that facilitate closed form sampling updates). This somewhat limits the scalabilty to really massive datasets. Although the cost scales in the number of edges in the graph, the method also requires sampling a large number of other latent variables which could potentially make it somewhat expensive for large graphs.  - The paper should also discuss other recent work on deep generative models for graphs, such as  (1) Variational Graph Autoencoder (Kipf and Welling, 2016) (2)  Stochastic Blockmodels meet Graph Neural Networks (Mehta et al, 2019)  These methods use graph convolutional network (GCN) based encoder to learn latent representation in a generative model (latent space model or stochastic blockmodel). It would also be nice to have a discussion /comparison with such models.  - For the experiments, since the proposed methods can use node features, it might be a good idea to evaluate it on "cold-start" problems, i.e., hold out all the link/non-links for some nodes and try to see if the model can predict them well.  - The model is fairly complicated and MCMC is used for inference. In the experiments, 2000 iterations were run. Were these sufficient? Did the MCMC chain seem to converge? If yes, how did you assess that?  - The model is fairly involved (though inspired by previous works) and even though the inference update equations are provided, implementing it from scratch would be non-trivial. Moreover, the experimental settings, hyperparameter etc. need to be carefully selected. It would be nice if the code can be made available (I could not find it with the submission) for easy reproducibility  Overall, the paper is addressing an important problem. The proposed model, although inspired by various recent works, makes some interesting improvements. The empirical evaluation is decent; however, it is limited to small-sized graphs.  ========================================= Post rebuttal comments: Thanks for your response. I still feel the paper, despite having an interesting model and decent experiments, is somewhat borderline, especially since the basic model is largely inspired by HLFM [11] and the MCMC based inference method used in the paper is mostly based on prior work.   Also, although the model is interesting, the paper falls short on making a strong case as to why it would be preferable as compared to various other deep generative models of graphs proposed in the recent couple of years. I am especially talking about generative models that use things like graph convolutional networks (GCN) and recurrent neural networks as encoder. The paper doesn't discuss or compare against any of these methods, which is disappointing since these methods are now very widely used in modeling of graph structured data.  Perhaps one of the most basic of these is the VGAE (variational graph autoencoder) or Kipf and Welling (2016) which should have been a simple baseline. VGAE uses GCN and does incorporate higher order node correlations due to the message-passing scheme of GCN, and it would be nice to show a comparison with such methods. Even if the paper doesn't compare with more advanced variants of VGAE, at least some basic comparison with VGAE should have been provided.  The proposed model does have some nice interpretability properties but the paper doesn't explore those. A nice experiment would be to use the embeddings to discover topics in the graph (for example, as done in HLFM [11]).  In summary, the paper has promising ideas but, in my opinion, it is somewhat found lacking due to the other reasons. I think by incorporating the above suggestions, the paper would be significantly better. As I said, the paper is borderline. If the paper does get accepted, I would encourage the authors to include some comparisons with VGAE based methods (and a discussion of pros/cons with Kipf and Welling (2016) and Mehta et al (2019)). Some qualitative results would also be nice to have. 