[UPDATE]  I will maintain my score (7) because I believe the paper makes a clear and substantial contribution to the CL literature. I still recommend acceptance and I am willing to trust the authors will improve the writing. Below I quickly discuss other concerns.  I agree with R2 and R3 that more analyses were necessary, but I am satisfied with the Author's response for both components of the loss figure (a) and number of exemplars (c). I thank my colleague reviewers for insisting. If you follow closely figure (a), the contributions of each component of the loss increase as more classes are added, which I find quite interesting, I think this really adds to the paper and the authors are most likely going to discuss this in the final manuscript. Overall I am satisfied with the new amount of analysis.  While the conceptual contribution may seem relatively low vis-a-vis PathNets, I would like to argue that using very high capacity models for such problems is actually a novelty on its own which is not covered by PathNets. In fact, I can't think of any CL work out there which managed to generalize using such large networks on the datasets typically used in the field; I think it's safe to say that this paper uses at least 10x more parameters and 10x more computation compared to previous methods. But I believe the paper does a great job of showing that previous CL methods actually do work better at this novel scale and using residual architectures. I believe that not only the combination of previous methods is new, but none of those methods were proven to still be relevant at this scale before.   While the writing seems rushed at times, I am satisfied with the clarifications I received. I also agree with R2 and R3 that some of the claims were not well supported, but those claims were not central to the paper, imho. It's a simple matter to remove claims like superiority to genetic algorithms, which is weak anyway.  I will not increase my score to 8 because the other reviewers are correct that novelty is not earth shattering, but I believe it's substantial enough for acceptance. The combination of existing methods, creative use of resnets at unprecedented scale for continual learning experiments are nicely complemented by strong technical execution. This is an exciting new ground for continual and incremental deep learning.   [OLD REVIEW] Originality: The paper spends excessive amounts of space trying to differentiate itself from previous work at the conceptual level. This is not necessary imho, as the paper is valuable on its technical innovations alone. The novelty comes from a thorough and judicious investigation of much larger and expensive network architectures and specifically hand engineering an incremental learning algorithm for the sequential classifier learning case with a (small) persistent data store.  The model is highly over-parameterized compared to previous works or to the capacity that is required for good generalization in the regular multi-task training case. While, in a sense, the claims of bounded computation and memory footprint are true, they are only true because all the needed capacity is allocated beforehand, so model growth is not needed in the use cases considered; this is however no guarantee for the more general case of sufficiently different tasks, or simply for more tasks in the incremental learning sequence. Claims of computational efficiency are similarly overblown. The model is indeed able to benefit from independent exploration, fully in parallel, of training N different novel paths. But this is a trivial claim, so can every other model, e.g. by exploring different hyperparameters and choosing the best in validation.  The claims of difference to genetic algorithms and reinforcement learning are also overblown. Random path selection *is* a basic type of genetic algorithm. Furthermore, the sequential, balanced task classifier learning setup is one of the simplest examples of continual learning problems; it is not at all clear that random path selection would work as well for much more complex cases, such as multi-agent reinforcement learning or GAN training, both of which are much less well behaved continual learning problems.  The claims of minimal computational overhead over competing approaches are simply ridiculous. Minimal overhead over huge cost still adds up to huge costs, since the  proposed architecture is very expensive compared to even recent works. Not only many paths are explored simultaneously, but even a single path is much more expensive to train and evaluate compared to most other approaches in the table. But this is not necessarily an issue; we want to increase performance and computation is becoming more abundant and cheap. However, such misleading claims are an issue.  Quality: The experiments look thorough, and the improvements look large enough to be significant, although it would be nice to have some error bars. Ablation and FLOPS calculations are clearly interesting.  The discussion of backwards transfer is particularly shallow. It’s not at all clear to me that improvements don’t really come from actually training more on the stored samples rather than some backwards transfer effect from learning new tasks on different data. Some analysis and targeted experiments would be required.  Clarity: writing is a bit silly here and there, especially when over-hyping and comparing to previous work. This only detracts from the otherwise good quality of the paper.  Significance: Perhaps significant beyond incremental classifier learning, although the paper does little to convince us of it.  