 This paper aims at solving the saddle point problem $$\min_{x} \max_{y} g(x, y)$$ where $g(x, \cdot)$ is concave for each $x$ and $g(\cdot, y)$ is either strongly convex or nonconvex for every $y$. The authors introduce a new algorithm called DIAG which combines Mirror-prox and Nesterov's AGD to solve the minimax problem. For the case that $g$ is strongly-convex with respect to $x$, the authors show that  DIAG has a convergence rate of $\mathcal{O}(1/k^2)$ when the suboptimality for a pair of point $(\hat{x},\hat{y})$ is defined as the primal-dual optimality gap $ \max_{y} g(\hat{x},y) - \min_{x}  g(x, \hat{y}) $. For the case that $g$ is nonconvex with respect to $x$, the authors show that DIAG finds an $\epsilon$-first-order stationary point (FOSP) after at most $\mathcal{O}(1/\epsilon^3)$ gradient evaluations.     The convergence criteria considered in this paper for solving a strongly convex-concave problem is interesting. In particular, the discussion in the first two paragraphs of Section 3 is very  illuminating and clarifies why one cannot achieve a convergence rate of $\mathcal{O}(1/k^2)$ by solving the minimization problem very accurately using any linearly convergent method and using AGD for solving the concave maximization problem. Basically, if one does this procedure the resulted algorithm will achieve a sublinear rate of $O(1/k^2)$ for the error of the concave maximization problem, i.e., $h(y)-h(y^*)$ where $h(y):=\min_{x} g(x, y)$, but this result does not necessarily translate to a convergence rate of $O(1/k^2)$ in terms of primal dual optimality gap. The reviewer believes that  this is due to the fact that the function $f(x):=\max_{y} g(x, y)$ may not be smooth in general. I think authors could also add this point to this paragraph.    The main idea used for extending the result for the strongly-convex case to the nonconvex setting is to add a proper regularization to the minimization problem to make its objective function strongly convex and use the fact that a first-order stationary point (FOSP) of the regularized problem is close to an FOSP of the original minimization problem. This technique is not novel and has been already used in many papers including reference [27] in the paper. Indeed, by adding this regularization (proximal term) we are in the setting of strongly convex-concave problems and the DIAG method can be used to solve the resulted problem.    Overall, the ideas in this paper are interesting (in particular the beginning of Section 3) and the authors did a good job in explaining the challenges for achieving a convergence rate of  $\mathcal{O}(1/k^2)$ when $g$ is strongly convex w.r.t. $x$ but concave with respect to $y$. However, the presentation of the algorithm and explanation of the related work are not satisfactory. The reviewer recommends moving the proof for the nonconvex case to the appendix so that the authors will have more space to explain the related work better and present their proposed DIAG method in detail. Below you can find more detailed comments.    1- Section 2.2 of the paper is not well-written. In particular, when the authors talk about the mirror-prox method and the conceptual mirror-prox method (which is also known as Proximal Point method for saddle point problems [R3]) they do not mention that the expressions in (5) and (6) are the special cases of (conceptual) mirror-prox method when we use the Euclidean space. Also, the presentation of sub-routine in (6) for approximately performing (5) is not clear. It would be also helpful to explain that a special case of (6) when one perform two steps of (6) is the Extra-gradient method.    2- In the related work section, the authors mention that the notion of stationarity considered in  [26]  is weaker than the one used in this paper, and it does not imply the results presented, however no explanation for this statement is provided.    3- The main contribution of the paper is proposing the DIAG method for solving strongly-convex concave saddle point problems. However, the authors do not provide any explanation for DIAG and they just simply provide the Pseudocode of this algorithm. How does DIAG relate to Mirror-Prox? What is the intuition behind the subroutine called ``Imp-STEP"? Why do you need to analyze a weighted average of $x$ while you use the last iterate of $y$ in the analysis?    4- Another concern is the comparison benchmark for the strongly convex-concave case. The authors compare the results to the results of [R2], which only studies the case of convex-concave function. The reviewer feels that this comparison is not fair. Also other papers including [R1] which get a linear convergence rate for the strongly convex-concave case, when the coupling is bilinear have not been cited.    5- Many papers which solve similar problems have not been cited in the paper. For example, papers including [R4] and [R5] (and references therein) should be cited.   [R1] Simon S. Du, Wei Hu, ``Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave Saddle Point Problems without Strong Convexit." \textit{AISTATS 2019}    [R2]  Nemirovski, Arkadi. ``Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems." \textit{SIAM Journal on Optimization} 15.1 (2004): 229-251.   [R3] Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. ``A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach." arXiv preprint arXiv:1901.08511 (2019).  [R4] Monteiro, Renato DC, and Benar Fux Svaiter. ``On the complexity of the hybrid proximal extragradient method for the iterates and the ergodic mean." \textit{SIAM Journal on Optimization} 20.6 (2010): 2755-2787.  [R5] Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. ``On the convergence and robustness of training gans with regularized optimal transport". \textit{In Advances in Neural Information Processing Systems, pages} 7091â7101, 2018.  =========== after author's rebuttal ========== I read the comments by the other reviewers as well as the response provided by the authors. The authors did a good job in clarifying the intuition behind their proposed method, and I am fine to recommend acceptance of this paper if the authors include this explanation in the final version. I still think comparing the result in this paper with [[R2] Nemirovski, Arkadi. ``Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems.] is not fair as [R2] only studies convex-concave problems. 