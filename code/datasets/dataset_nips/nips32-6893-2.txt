This paper studies smooth saddle-point problems under two different assumptions on the curvature of the problem: (i) the stongly-convex-concave problem and (ii) the nonconvex-concave problem. Regime (i) is rather classical in the field and there are many (also accelerated) algorithms available achieving the optimal rate of convergence in the primal-dual gap function. The second regime is challenging and it is a very interesting and important question to study accelerated methods for such problems.   Theorem 1 gives the convergence rate for an accelerated algorithm delivering the optimal convergence rate for the smooth strongly convex-concave saddle point problem. The paper  "OPTIMAL PRIMAL-DUAL METHODS FOR A CLASS OF SADDLE POINT PROBLEMS, by YUNMEI CHEN, GUANGHUI LAN, AND YUYUAN OUYANG SIAM J. OPTIM". develops an accelerated primal-dual method for a problem of the form   f(x)+\inner{Ax,y}-g(y)  where f is smooth convex, and g is convex and non-smooth. Can you relate your result more to this paper and compare your algorithm with theirs? In general I have some hard time understanding the details behind Algorithm 1 (made explicit in the list below) and some other remarks the authors should take care of:   -) Line 65: VIs have many more application than just in differential equations. Please provide other references which are more relevant for this audience. -) line 72: Something is missing here when your write liminf S(x). -) The discussion in Section 2.1.2 is a bit misleading. In the end, what you propose here is to search for solving the MinMax problem, so you make a deliberate choice over the MaxMin problem. This is fine, but could be said directly.  -) Definition 4 is not complete. It must hold for all $x$ and $y$.  -) Lemma 1: Why is it important here that X=R^{p}? -) Eq. (5) no bold letters. -) Instead of referring to the Appendix it might be better to refer to the supplementary materials.  -) In Theorem 1 it is not explained what epsilon_step exactly is. Only after the basic complexity bound a specific value for this sequence is provided, but it is still not clear where it appears in the analysis. I guess it is just a tolerance level for executing a subroutine, but this can be explained in the Theorem without having to read the pseudocode.  -) I find the pseudocode representation of Algorithm 1 strange. Why are the assignment arrows used here and not simply equalities?  -) Algorithm 3 is only defined in the Appendix (supplementary material). It might be good to refer to this in the pseudocode of Algorithm 1. -) I don't understand the subroutine Imp-STEP- What is $R$ exactly and how is it chosen? How is $y_{r}$ computed in eq. (7)? How is x_{k+1} determined from this routine?  -) line 196: What do you mean by "the primal-dual gap is unknown". It simply makes no sense to look at it since there is no natural correspondence between a "dual" and a "primal" problem in non-convex optimization.  -)  line 203: In which sense do you have convergence $O(k^{-1/4})$? -) Lemma 5: First line of the proof should be $\tilde{f}$. -) Proof of Lemma 3: It should read $\tilde{g}(x,y)$.  