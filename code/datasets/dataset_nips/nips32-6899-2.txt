I have read the authors rebuttal, and decided to keep my score. I believe that if the heuristic glueing approach can be made precise, this would greatly improve the paper.   ========================  This is an interesting paper and addresses an important problem: namely the conditions under which discrete-time optimization algorithms can be approximated by continuous time dynamics uniformly in time. If this holds, large-time properties of the latter can be used to infer that of the former.  Previous work in this direction relied mostly on constructing Lyapunov functions, but in this work the authors propose an alternative: allowing the initial condition of the continuous-time dynamics to change a little -- thereby establishing uniform error estimates for a larger class of loss landscapes.  Although most theoretical results are based on well-established results from dynamical systems theory, I feel it is nevertheless important to introduce these ideas to the greater machine learning community.  One perceived drawback of this work is the limited setting: most results are on quadratic functions (or perturbations of them). I have the following questions: 1. Can the results for quadratic settings be generalized, especially for the Hyperbolic case, to more general loss functions? e.g Thm C.1 C.2 2. On gluing: from the discussion from line 237, it would appear to me that if you perform local quadratic approximation of the loss function and then glue all pieces (assuming they are hyperbolic, uniformly) together you would naively get a global result. Since this result is not proved, I expect there is some difficulty in this program. I would appreciate an explanation of this difficulty.