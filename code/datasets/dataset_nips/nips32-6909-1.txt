Post rebuttal:  Many thanks for addressing my concerns. I have read the rebuttal and other reviewer's comments. Please elaborate them in more details as needed in the revision.  1. Regarding (8) -> (9) transformation: perhaps there could be a data-dependent way of learning a better prior of \alpha.  2. Regarding my comment that the case where one dimension has scarce data. The scarcity of the data is caused by the naturally smaller intensity function. If one dimension has a harder to learn triggering function, it may propagate onto other dimensions. In prior works, such cases are hard to handle, and I think the method proposed here could be used as a remedy to this issue. I don't think the intuition explanation is a satisfying answer and I believe some additional exploration is warranted here.  Overall I maintain my vote to accept the paper.  --------------------------------------------------------------------------------------  This paper proposes learning the parametric MHP by regularized MLE. The parameters to be learned are \mu and W, and a limitation of the traditional algorithms are that all the dimensions share the same regularization parameter. The authors proposed a Bayesian method and modeled \alpha, the regularization coefficient, as a hidden variable. An EM algorithm follows naturally. Experimental results show that the proposed method improves upon the EM-based learning algorithm MLE-SGLP proposed by Xu et al.  I feel like this is a good submission but it's quite possible I didn't understand the main part of the paper. I think the idea is novel and empirical evidence suggests it would help researchers to select the regularization parameters. In cases where learning samples is scarce, I think this is particularly useful.  Empirical evidence suggests that the method works, but formulation-wise, there are a few places that confused me although I already think it's quite interesting.  1. The first key step in the paper is from (4) to (8), where the regularizer is viewed as \log p_{\alpha}(\mu,W). I would argue that the ignored normalization coefficient here depends on \mu and W itself, and cannot be omitted. Or \alpha has to depend on \mu and W (which is not the case per Appendix). Therefore I suppose this is not an equivalent transformation?  I also think it is this reason that leads to the fact that one cannot optimize \alpha in (4) directly, am I right for this? I didn't understand why one can optimize over (8) (stated in L142) as one can change \alpha arbitrarily and it doesn't affect the choice of the parameters which is determined by the problem.  2. I didn't quite understand the intuition behind the transformation from (8) to (9). In L147 I suppose the upper bound should be the maximum likelihood instead of the likelihood?  3. I saw in appendix several choices of p_{\alpha}. How did you obtain this from (4)? Any reasons why p_{\alpha} should be designed as described in Appendix?  Understanding those along with other reviewer's concerns would help me better appreciate the result of this paper.