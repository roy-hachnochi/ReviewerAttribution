The paper focuses on a broad problem that plagues many researchers today. The size of the models is often limited by the memory of the accelerator (GPU or TPU), the researcher is limited in what they can do. While there have been prior attempts to solve this problem in specific settings, they have been too limited to specific kinds of models and haven't seen broad usage or availability across ML tools and libraries. The paper does a good job of explaining the problem, coming up with a general solution that seems to apply to all kinds of directed graphs (most of deep learning), and empirically demonstrate the wins over existing strategies.  While at first glance the submission seems incremental, the significance of it comes from the fact that it could now be more easily integrated into the underlying ML software and be made available to a lot more researchers.  