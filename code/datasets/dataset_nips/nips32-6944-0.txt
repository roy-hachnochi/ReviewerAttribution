While I personally am not very excited by yet another algorithm-for-group-fairness-in-binary-classification paper, this work does fill in a real gap in the literature.  I did not check proofs carefully, but otherwise this work is pretty well written and appears correct.  This work does perform one unfortunate sleight-of-hand by starting with exact fairness and giving Bayes optimal for this exact case, but then only proving that their algorithm is exactly fair in the limit.  While exact fairness won't be possible on the distribution, it should be on the sample at least.  Relatedly, I would prefer to see the finite sample results they claim to have, but do not show, even if those results are highly dependent on \eta.    Assumption 2.2 seems, regardless of whether it's been used before, difficult to buy as a reasonable assumption.  In practice, there may only be finitely many values of X with non-zero support, and therefore you'll have non-zero probability mass on the events in question.  I would guess that removing the assumption would make the results messier, but it should still be possible to write down without this assumption.  Finally, the experiments left me more puzzled than satisfied, as their claim was that their method was preferable to the other top performer because it was more general and could be applied to RF, which was indeed one of the best algorithms on these datasets for minimizing error.  But RF is clearly more difficult to get to be fair, because on at least some of the data sets, the RF+post-processing methods were not top performers.    Other things: -There are quite a few tyos/grammatical issues, starting with lines 34, 38, 65, 72, 87, 90, 93, etc. -line 88: \mathcal{G} is not a set  Edit after author response:  Thanks for addressing my concerns in detail.  