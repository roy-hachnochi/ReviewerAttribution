Originality: -Although the Hardt paper has suggested the use of this approach, the paper claims that itâ€™s the first to actually show this is indeed possible. It's the first to decouple the risk and fairness in terms of regression function and base rate.  -Use of unlabeled data to help facilitate fairness along with better accuracy is novel.   Quality: -The assumptions made about the model are very well justified. The discussion after each assumption provided the context as to why the assumption makes sense and why the assumption is needed to study their model. These discussion as a result provided very good intuition and set up the stage for the proof. -The results in the appendix are quite strong, too.   Clarity: -Overall, the paper have a very smooth flow, whether it be discussion of their assumptions or their remarks.  -The proof sketch is thorough yet simple enough to understand the gist intuitively. -Once again, the remarks are very helpful. For instance, they expect the natural follow-up questions and answer them very well (e.g. why asymptotic convergence instead of the rate of convergence). Even in the appendix, they provide remarks as to how to read (e.g. make sure to understand some preliminary concepts before proceeding the proof).  -The paper follows the neurips guideline very strictly: transparency in terms of hyper parameters are chosen, confidence intervals for the accuracy and fairnes, etc.  -It's not a big deal, but the figures are somewhat hard to read; one suggestion is to change the highlight yellow color (Adult Yellow) to something else.   Significance: -There's already a huge literature on plug-in approaches. The paper connects the fairness literature with that literature. -The proposed approach of decoupling things into regression function and base rate may be useful for other notions of fairness as well. -Empirically, their methods stay competitive, if not outperform the current state-of-art. -Also, in the appendix, it discusses what the optimal classifier without sensitive feature looks like, although the proof of consistency of the empirical estimator is not provided. Once again, they validate their results in this case empirically.   Question:  -Isn't it pretty common to assume bounded Rademacher complexity (as in Agarwal et al.) to argue about uniform convergence (generalization error) of the classifier?     ***** POST REBUTTAL ******  I am also concerned about the non-trivial changes included in their response, and I'm not too excited about another group fairness paper. However, I think it's still well-written and and techniques are somewhat interesting. Therefore, I'll keep my score at 8.