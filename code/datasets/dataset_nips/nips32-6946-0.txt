* The authors seem unaware of prior work on learning which hyperparameters are most important to tune and which values are most promising. For instance: https://arxiv.org/pdf/1710.04725.pdf learns priors over the hyperparameter space. * The method only works for completely numeric spaces. The numeric and non-numeric subspaces could be tuned separately, but then you can't learn interactions between both kinds of features, while a full Bayesian optimization approach could. * The results show excellent anytime performance: with limited resources, it provides good solutions fast. However, since the search space is restricted, it is possible that the very best configurations fall outside the search space, and that a more extensive search would yield a better model. This is not demonstrated since all experiments were cut off after 50 iterations. * the ellipsoid introduces an extra hyper-hyperparameter. How should it be tuned? * Generalizability is questionable. The toy problem is really small, I don't really see what to take away from these results. The OpenML experiments are larger (30 datasets) and up to 10 hyperparameters, but this is still rather small to other work in the literature, e.g. autosklearn with hundreds of hyperparameters and datasets. The experiments all included very densely sampled problems, it is unclear how accurate this method is if only sparse prior metadata is available. One sign that is troubling is that randomsearch seems to outperform more sample-efficient approaches easily in these experiments. * Why were only 4 meta-features used? Most prior work uses many more. * In is unclear why the hyperparameters were discretized in the neural network experiments.  Update after the author feedback: Most of my concerns have been addressed. I still have some reservations about generalizability but the current experiments are promising. 