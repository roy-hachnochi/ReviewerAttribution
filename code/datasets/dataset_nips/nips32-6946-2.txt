Summary of the main ideas: A really novel methodology to learn search spaces in Bayesian Optimization that is able to build ellipsoidal search spaces  that are sampled through rejection sampling. I like this paper due to the fact that it seems to me after reading several papers about this topic this this methodology is the best, and most practical, to learn the search space in Bayesian Optimization. It has further work in the sense that can not be applied as it is to GPs but I think that using an approach such as for example PESMOC to model the ellipsoid as a constrained solves it easily (it is commented in the paper). I see it so clearly that I would even be willing to collaborate on that idea. The approach gains quality in the sense that it does not need parameters (critical in BO) in proposes several methods and they are simple yet effective. Brilliant paper from my point of view.  Related to:  -> Learning search spaces in BO.  Strengths: Effective and simple methods that solve a popular task in BO more easily than previous methods. Proposes several alternatives. Proposes further work to address the only tasks that are pending. It seems to me that opens a new set of methodogies to be further improved. The paper is very well written.  Weaknesses: We can argue that is not a complete piece of work, but it is veery rigurous. Lack of theoretical content, but it does not matter as the empirical content and the proposed methodologies are solid.  Does this submission add value to the NIPS community? : Yes it does. I miss simple but effective methods for BO and this paper contains them. It contains further work to be done. It is an approach that I would use if I were working in a company, in the sense that it is easy and pragmatic.  Quality: Is this submission technically sound?: Yes it is. Estimating the search space as an optimization problem and with different geometrical shapes in the sense that the paper performs is sound. Are claims well supported by theoretical analysis or experimental results?: Experimental results support the claims. Is this a complete piece of work or work in progress?: It is a work in progress though, GPs are not covered and are the most useful model in BO. But the path is set in this work. Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?: I think so from my humble opinion.  Clarity: Is the submission clearly written?: Yes it is, it is even didactic. Good paper that I would show to my students. Is it well organized?: Yes it is. Does it adequately inform the reader?: Yes it does.  Originality: Are the tasks or methods new?: They are tools used in other problems but their application in BO is sound. Is the work a novel combination of well-known techniques?: Combination of well-known techniques, very well done. Is it clear how this work differs from previous contributions?: Yes it does. Is related work adequately cited?: Yes I think so.  Significance: Are the results important?: I think so, they seem to speed up the optimization. Are others likely to use the ideas or build on them?: Yes, definitely, I would do it and I am thinking in contacting them after publication or rejection is done, I found this piece of work very interesting. Does the submission address a difficult task in a better way than previous work?: Yes absolutely. Does it advance the state of the art in a demonstrable way?: Yes it does. Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?: Yes, experiments provide good results.  Arguments for acceptance: Effective and simple methods that solve a popular task in BO more easily than previous methods. Proposes several alternatives. Proposes further work to address the only tasks that are pending. It seems to me that opens a new set of methodogies to be further improved. The paper is very well written.   Arguments against acceptance: Honestly, I would accept this paper, I know that NIPS has high standards, but I think that it is a good paper.