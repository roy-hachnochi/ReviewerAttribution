This is an interesting paper, and well written. Overall I like the contributions. I have the following comments to consider.  I am not sure "feedforward" is an appropriate prefix for the technique, as it seems to suggest that the approach is feedforward neural networks based. Though, it is completely upto the authors.  If I understand correctly, the proposed approach differs from the prior well known technique, one-coin Dawid-Skene model, in the sense that there is a prior distribution on worker accuracies. I suppose, similar prior model has been considered in other works? In section 3, although references are provided for known concepts used in the proposed technique, it doesn't give a clear picture on the novelty of the technique. May be, there should be a related work section.  It is claimed that the proposed technique differs from the previous works, in terms of employing variational mean field approach, in the sense that KL divergence is minimized between the true and approximate posterior. It would be good to provide expressions for KL divergence, and to show how it becomes tractable to minimize it for the assumed prior distributions (so the posterior) and the mean field approximation of the posterior.  In the previous works, coordinate decent is used for optimization, whereas it is proposed to perform (stochastic) updates, using a simple observation in each step, leading to local analytical optimizations. I suppose, the updates become analytical because of the specified beta distribution for the prior? Is it necessary to perform updates using a single observation? That seems like an extreme scenario even for online settings. How about updating with small sized batches of observations?  I would like to mention it here that, in the recent literature of machine learning, information theoretic objectives like KL-divergence, mutual information have been used for optimization even if not computable analytically. And, it is a standard practice to use stochastic gradient rather than gradient. While it is interesting to see similar concepts being used for Bayesian inference, it should not come as a big surprise in terms of novelty.  A small suggestions: section 3.2 could benefit from a sketch so as to explain the reasoning behind reordering the labels.  In Theorem 1 and 2, the expressions for F(.) are too complicated to have clear interpretations, though useful to compute the upper bounds numerically. The authors do not provide any statements or remarks regarding the interpretability of the theorems. May be, the expressions should be simplified in the original theorems or in the corollaries, just a thought.  In section 5, for synthetic data, Monte Carlo sampling seem to be outperforming the others. Though, its performance is not as good for the real datasets. Any intuitions on why that is the case? Also, I didn't see a reference for MC method.   Is it true that the value of parameters, alpha, beta, are assumed to be known, rather than optimized or tuned?  There are no errors bars, or statistical significance numbers for the experimental results presented in Section 5.  I don't know how impactful the empirical evaluation is. I don't see significant improvement in accuracy numbers.  What does this statement mean: "average accuracy just above 0.5"?  ----------------- The rebuttal clarifies on the questions asked in the review well. I like this paper, and vote for its acceptance. Also, increasing the score from 7 to 8. 