Study of the Neural Tangent Kernel (Jacot et al. 2018) is a compelling approach to understanding the dynamics of learning in neural networks. As acknowledged by the authors, while this kernel determines the dynamics only for very wide networks, better understanding of this simplified regime could be a first step towards a fuller understanding of learning dynamics more generally. In this work, the authors study the smoothness, approximation and stability of this kernel for shallow fully connected and convolutional networks.   It has been recognized for some time that since overparametrized neural networks do not overfit, the choice of architecture and/or optimization algorithm must bias the solutions found towards ones that generalize well. In the past, it has been difficult to understand this implicit bias except in very simple architectures, and studying it in the wide network regime is another step towards a better understanding of this phenomenon.   The presentation is very clear for the most part, and the authors make an effort to distinguish their contributions from previous work. They are also honest about the limitations of the setting of the analysis (the only exception is perhaps the issue of the decomposition in terms of spherical harmonics, see below).  When presenting the Mercer decomposition in terms of spherical harmonics in Proposition 5, isn't one assuming a uniform data distribution over the sphere? If this were not the case, then the RKHS in eq. 11 would take a different form (since one could choose f to be supported on the entire sphere). If this assumption is made elsewhere in the text it might be good to state it clearly in this section. It is also useful to note that in general one would expect realistic data distributions to be supported on low dimensional subsets of the sphere. This limits the applicability of the results in this section.   Given the known stability results for convolutional networks, is it surprising that the convolutional NTK feature map is stable as well? 