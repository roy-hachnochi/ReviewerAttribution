Post author response:  After reading other reviews and author's response, my evaluation still holds. I thank the authors for the thoughtful response and will be looking forward to see future developments in this direction.  ______________________________________ In the infinite width limit, neural network’s gradient flow training dynamics is well captured by Neural Tangent Kernel. The advantage of this picture of understanding neural network is providing angle to study kernel properties to understand deep neural networks which has been a hard challenge theoretically. Although applicability of “lazy training” via NTK to realistic model / dataset is still not clear, it is an important theoretical landmark to have a good understanding of this kernel capturing neural network training in a certain limit.   The authors set out to study inductive bias of Neural Tangent Kernel which is very timely and important contribution.   Authors show that NTK with two-layer ReLU networks are 1) not Lipshitz smooth but satisfy weaker Holder smoothness property. 2) Also studying CNN NTKs stability shows less stability compared to kernels obtained by fixing weight of all layers except the last layer. 3) Using spherical harmonics decomposition show that eigenvalues decay slower than ArcCos kernel that would correspond to ReLU network kernel with fixed weights except for the last layer.   Interesting observation pointed out by the authors is that the finding show tradeoff between stability and approximation where better approximation property captured by NTK is tradedoff by less stable/smooth property.   While few recent concurrent work discuss NTK for convolutional networks, the current submission also provide definition of NTK for CNN, independently, also generalizing to linear operators that has not been considered in other works such as patch extraction and pooling operators.  Few obvious limitation is that the analysis especially looking at the spherical harmonic decomposition assumes that input data is uniform sample from hyper-sphere. For real dataset, it is unclear how kernel spectral properties would be similar or different from simple toy data domain. While all of the work is studying property of kernels theoretically, it is a weakness of the work not showing any empirical support of the inductive bias described in the paper.   Nit:  Line 167 “NTK kernel” repeats `kernel’ twice. For references to GP limit of neural networks, one should also cite [1] along with papers already been cited.  [1] Alexander Matthews et al., Gaussian Process Behaviour in Wide Deep Neural Networks, ICLR 2018