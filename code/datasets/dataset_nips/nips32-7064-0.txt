Originality: The author propose a novel and general architecture that, to the best of my knowledge, has not been described before. Thus the idea of the "shallow" two layer RNN architecture as well as the accompanying theoretical analysis and experimental results are all novel.   Quality: The claims appear correct, although I have some confidence in not having missed important issues only for claim 1 and 2. The experiments are comprehensive and instill confidence in the proposed architecture and theoretical guarantees. The code they provide appears about average for this type of research prototypes.   Clarity. Most of the paper is clear and easy to follow. There are however a few typos and sentences that could be improved with some additional proof reading. (See below for some of the typos I spotted)   Significance. The simplicity of the method combined with the well-motivated use case of embedded devices with constrained resources mean that I see this paper as a useful contribution, from which many are likely to benefit and thus worthy of NeurIPS.     Question and comments:  When running over 5 random seeds, what kind of variance is observed? It would be worth mentioning this at least the supp material, to get a sense of the statistical relevance of the results.   46: ensuring a small model size -> I believe the model size would not be smaller than that of a standard RNN, if so the claim appears a bit misleading   Claim 1 appears correct as stated, but the formulation is a bit convoluted, in the sense that one typically would be given T and w, and can decide on a k; whereas in the current formulation it appears as if you are given a T and q and can pick an arbitrary k based on that, which is not really the case.   Line 199: from this sentence it is not very clear how SRNN is combined with MI-RNN, it would be good to give a little more details given that all results use this model are based on a Shallow extension of MI-RNN. In the same vein the empirical analysis would be a little stronger if the results of SRNN without MI-RNN would be reported too.   Minor: 37: standard -> a standard 81: main contributions -> our main contributions 90: receptive(sliding) -> [space is missing] 135: it looks like s starts at 0 where all other indices start at 1; including line 171 where s starts at 0 137: fairly small constant -> a fairly small constant 138: that is -> which is 139: tiny-devices -> tiny devices 152: I would find it slightly more readable if the first index v^{(2)} was 1 instead of T/k; if you need an index at all at this point 154: should be v^{(1)} not v^{(2)} 159: tru RNN -> a true RNN 159: principal -> principle 172: for integer -> for some integer 240: it's -> its 267: ablation study -> an ablation study latency budget of 120ms -> it's not clear to me where this exact limit comes from is it a limit of the device itself somehow? 318: steps of pionts threby 314: ully -> fully   In the MI-RNN paper [10] they benchmark against GesturePod-6, where the current paper benchmarks against GesturePod-5, are they different? If so in what way?