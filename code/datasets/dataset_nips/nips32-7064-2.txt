Overall this is a well-written paper with proper motivation, clear design, and detailed theoretical and empirical analysis. The authors attempt to improve the inference efficiency of RNN models with limited computational resources while keeping the length of its receptive window. This is achieved by using a 2-layer RNN, whose first layer parallelly processes small bricks of the entire time series and the second layer gathers outputs from all bricks. The authors also extend SRNN in the streaming setting with similar inference complexity.  One concern about the bound in Claim 1 in the streaming setting: In line 137: w is required to be fairly small constant independent of T. In line 166: w = k * q (w is a multiple of k, and thus k needs to be small constant) In line 173: The bound becomes O(\sqrt{qT} * C_1) iff k=\sqrt{T/q}, which is not o(1). Therefore, I was expecting analysis in practical applications with large T and small w.  In SRNN, will the O(T/k) extra memory cost be an issue during inference?  The extension of multi-layer SRNN in Section 3.2 provides at least O(log T) inference complexity. The bound here is too ideal, but it would be great to see empirically how SRNN performs by adding more shallow layers. The empirical improvements over LSTM and MI-RNN on multiple tasks are impressing.   ==== Thanks for your responses. I have read the rebuttal and other reviewers' comments. I am glad to see about the experimental comparisons to CNNs and the refinement of your claims in the rebuttal, and I think including them in the manuscript or supplementary would better clarify and strengthen this paper. Overall this is a relatively simple yet effective solution to edge computing, which would keep becoming more important. 