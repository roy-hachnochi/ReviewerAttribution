Over the past few years, adversarial examples have received a significant amount of attention in the deep learning community. This paper approaches and addresses this important problem in a unique way by disentangling robust and non-robust features in a standard dataset.    I have few queries: By selecting robust or non-robust features for standard or adversarial training, how do you avoid over-fitting of models to these features?  In your proposed method, you randomly sample clean images from the distribution as the starting point of optimisation. Do the obtained images look similar to the source images or target images (images which provide robust features in the optimization)? If they are similar to the source images, dosen't it mean that the robust features are not robust?  Why do the authors use distance in robust feature space as your optimisation objective? Any specific reason or motivation for this?  UPDATE AFTER REBUTTAL: I thank the authors for addressing my comments. They have original contribution, and overall research problem has been nicely presented and addressed. I am increasing my score by 1 for this paper.