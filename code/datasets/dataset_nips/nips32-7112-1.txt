Originality:  The papers build on existing literature to provide a unified framework for stein discrepancies based on a generalized diffusion operator. Although the proposed generalization is expected, it is clearly beneficial to have such complete and rigorous treatment of stein discrepancies in the context of learning.   Significance: The paper exhibits and characterizes cases when score matching would have undesirable behaviors compared to KSD. Such contribution is very valuable to the community. Also, the new generalization provides a whole new class of discrepancies that are more suited for heavy-tailed and non-smooth distributions.   Quality: the proofs are sound, the experiments are simple but convincing which makes it a complete and insightful paper.   Clarity: The structure is very clear and the paper is pleasant to read.  I have two small questions: - In section 4.2, it seems that m was picked to match somehow the expression of the parametric model.  What guides such choice? Can one think of a general guideline to pick such function to improve the convergence properties of the loss (convexify the loss)? - The theory predicts that DSM is more robust than SM, could this be illustrated in a simple example?   ================================ After reading the other reviews and the authors' response, I still think this is a good submission and should be accepted. It would be great to incorporate the explanations provided in the response to the final version of paper.  