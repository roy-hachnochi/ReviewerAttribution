In this paper the authors establish upper bounds on the generalization error of classes of norm-bounded neural networks. There is a long line of literature on this exact question, and this paper claims to resolve an interesting open question in this area (at least when the depth of the network is viewed as a constant).  In particular, the paper considers generalization bounds for a class of fully-connected networks of constant depth and whose matrices are of bounded norm. Work by Bartlett et al. ("Spectrally normalized margin bounds on neural networks", ref [4] in the paper) proved an upper bound on generalization error that contains a factor growing as the (1,2)-matrix norm of any layer. If one further assumes that the depth as well as all the spectral norms are constants, then this is the dominant term (up to logarithmic factors) in their generalization bound. In this regime, the main result of this paper improves this (1,2)-matrix norm to the Frobenius norm (i.e., (2,2)-matrix norm). The authors achieve this by defining a notion of approximate description length of a class of functions, which roughly speaking, describes how many bits one needs to compress a function in this class to in order to be able to approximately recover the network from these bits.   Overall the paper is rather clearly written. One concern I have is that there is not a lot of discussion (e.g., in the introduction) about what in their technique allows them to improve upon the previous work of Bartlett et al, or in their language, to remove the additional factor of d (depth), from sample complexity bounds. This would give more confidence in their result. In particular, something describing the following might be useful: their proof proceeds in a similar pattern to previous covering number-based upper bounds on generalization error of neural networks, except by replacing utilization of covering numbers with approximate description length. The key in their proof that allows them to improve the bound of Bartlett at al. seems to be Lemma 3.6, which allows one to replace the (2,1)-norm in covering number bounds for linear maps with the Frobenius norm (and using approximate description length instead of covering number). At least part of the reason for that improvement seems to result from the fact that they measure the covering number in the target with respect to the L_infty as opposed to L_2 norm (Defn 2.1). If this is the case, I wonder if the proof of the main result can be made to only use the language of covering numbers?  Finally, there is some work appearing after the Bartlett et al. paper [4] establishing generalization bounds for networks with various bounds on norms and parameters (e.g., [LLW+18] below). Some discussion of these papers might be useful.  [LLW+18] Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization bound for deep neural networks: CNNs, Resnets, and beyond. arXiv:1806.05159, 2018.  I did not verify correctness of proofs in the supplementary material.  Overall, this feels like a somewhat significant contribution that introduces some new ideas to resolve an open question.  Miscellaneous comments: In the equation of Theorem 3.5, I think the middle term should be dropped (or flipped with the first term). (L_infty covering numbers are always at least the L_2 covering numbers.)  Line 149: samples -> sampled  Line 159: that that -> that  Line 170, 2nd word: to -> the  Line 186: I think the sigma at the beginning of the line should be rho?  Update: I was already quite positive about the paper, and nothing in the authors' response changed my opinion.