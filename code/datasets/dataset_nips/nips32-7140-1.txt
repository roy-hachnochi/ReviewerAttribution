EDIT: I have read the author feedback and the authors have agreed to revise the writing. This is clearly a good paper that should be accepted. Two more comments regarding the rebuttal: (1) My original comments apply to natural training as well, and I understand this is a very challenging topic. (2) I understand one can reduce the dependence on H, but my concern was that probably more parameters are needed to more precisely characterize the landscape (for both natural and adversarial training). For example, one such parameter is width. --------------------  This is an original paper that develops convergence results (of training error) of adversarial training for extremely wide networks. As far as I know, this is also first such results for adversarial training. The high-level strategy of the proof follows the recent paradigm of analyzing extremely wide networks for natural training (more precisely, through NTK), where the behavior of the networks becomes almost linear, and thus is amenable to precise analysis. This is a good paper, I like it, and vote to accept.  That being said, the current convergence theory is quite unsatisfying (I would say the same thing for the convergence theory we currently have for natural training). Among many things, I want to highlight two things that are particularly unsatisfying to me:  1. Depth of the networks *only* hurts convergence in this theory, and it is unavoidable. Let's start by observing that the current dependence on H is 2^{O(H)} (this is despite of the fact that the required width depends linearly on the number of training points, which is a separate thing), and moreover, as H increases, we will strictly need (exponentially) wider networks. This is very counter-intuitive, because empirically, theoretically (e.g., from the research of circuit complexity), and intuitively, H should at least sometimes help, rather than only hurting the convergence.  Moreover, such a "monotonic dependence on H" is unavoidable because H is the *only* architectural parameter used by the theory to characterize convergence. And it is impossible to get rid of this monotonic dependence without injecting more parameters.  For example, one could imagine the following theory by measuring one more parameter as a complexity measure: Suppose that we try to interpolate and recover certain hidden function f through data, and given depth H, the optimal neural network for representing f has width w_H(f). In this case, one could hope that for some good f, the width requirement for overparameterized networks to converge is a function of w_H(f), instead of H. Note that, w_H(f) could potentially be much smaller than H. For example, if we study f=XOR, then as H gets large enough, w_H(f) = O(1), and is independent of H, and so, even if we have a theory where the overparameterized network is of width 2^{w_H(XOR)}, it is a constant for large H (2^{w_H(f)} = 2^{O(1)} = O(1)), and thus give huge improvement. There are many things that can be explored in such an extended theory -- and I believe that that will lead to a much more reasonable theory.  2. From this theory, what are the "testable hypotheses" for guiding empirical work on neural networks? I think a downside with the current "foundations of deep learning" research is that it is more like "explanation for explanation's purpose", and lacks "predictions". I believe that a good theory should be able to predict something useful and guide the practice (which is critical and common in say a physics theory). Yet, I am not sure what the current theory can say about this ( maybe there is but it is not written in the paper).