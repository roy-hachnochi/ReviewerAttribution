The paper proposes a language for expressing task specifications for reinforcement learning (RL). The language is based on linear temporal logic and is compiled into a finite-state automaton (FSA). An augmented MDP that contains the FSA’s states as a part of its state is then constructed and equipped with a shaped reward function that rewards the agent for being close towards completing  subtasks without violating constraints.   The paper is clearly written. It is very heavy on notation, but this is probably unavoidable in such a paper. All notation is clearly explained, and with a bit of effort, the method can be understood. Empirical evaluation is limited, but it does show big improvements over carefully chosen baselines.   As a non-expert in the topic of task specifications for RL, the reviewer can’t 100% guarantee that the method is novel, but the related work section seems rather compelling. In particular, it seems that the construction of the augmented MDP and reward shaping indeed make the paper sufficiently novel for publication.   A question: using augmented MDP effectively introduces a sort of memory. Could not the same be achieved via using something like an LSTM?  Another question: it is not very clear from the paper what the SPECTRL system is, and it probably should not be mentioned in the bullet list of contributions in the introduction.   UPD: I have read the rebuttal and other reviews, I still think the paper is good. I would highly recommend the authors to release a reference implementation. 