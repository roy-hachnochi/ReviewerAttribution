Post Response update: Thank you for the detailed response. I still believe that a more in depth discussion of the differences or similarities of policy and cumulant based formulations is required to place the paper appropriately in context of prior work.  I think the new results presented by the authors in the response partially address my concerns about comparisons with prior work but not fully. I would still like to see comparison against a policy-based method as per the authors' classification.  I agree that all methods might have negative transfer but it would be ideal to include a discussion of the conditions under which the methods would show positive or negative transfer (something that the authors do) and to place that in context with other methods at least qualitatively (something that the authors dont).  The newer evaluations in the response do satisfy a part of my concerns. I am still not inclined towards a full-fledged accept but do view the paper more favorably. I would recommend the authors to incorporate the changes suggested by the reviewers.    Originality: This paper is largely building off of prior research in hierarchical RL and transfer learning. While the methodology is not very distinct from prior work, this paper does bring them together. The core ideas of this paper are shared by many other works transfer learning to guide hiearchical RL ([1],[2],[3]) and parametrizable options [4].  Significance: The paper demonstrates novel ways of combining hierarchical RL and transfer learning, but with so much other work trying to achieve that (see references), but this is not a unique approach towards that, thus it is hard to gauge significance without direct comparisons.  Quality and clarity: The paper is technically sound and well structured.  I like the idea of using a linear combination of the Q-values (and cumulants) in a generalized policy improvement framework. I also like the clean reformulation of options into generalized policy evaluation and improvement framework through the addition of termination as a primitive action.  Major comments: 1) Coverage of options used: As with most approaches that start with limited number of options or have pre-trained options, the quality of the final solution depends very strongly on the options selected for the 'keyboard'. Thus if all the component actions are poor at the 'test' task, the resulting policy would also have no hope of performing well at it.   2) Distinction from entropy-regularized RL: The author's approach might be seen as a special case of entropy regularized RL as T -> 0 [1].   3) Benefits/drawbacks w.r.t other continuous parametrizations of options: The authors have provided a single way of parametrizing options through linear combination of their Q functions followed by generalized policy evaluation and improvement. However [3] and [4] provide alternative approaches to parametrization of options. How do these methods compare to yours?  4) Clarity on the cumulant used in Foraging domain: The cumulant function used for each of the nutrients as defined in the paper and the supplementary material differ (line 246 in the paper) and (502) in the supplementary. I would request the author to clarify which one it was, and whether the difference was only in the learning curve for the constituent options or was there a difference in the performance of the options keyboard as well.   Minor comments: 1) Behavior of Q-learning with simple options: I am curious as to why simple options showed no learning effects in both the scenarios?   [1] - Haarnoja, Tuomas, et al. "Composable deep reinforcement learning for robotic manipulation." 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.  [2] - Frans, K., Ho, J., Chen, X., Abbeel, P., & Schulman, J. (2017). Meta learning shared hierarchies. arXiv preprint arXiv:1710.09767.  [3] - Gupta, Abhishek, et al. "Meta-reinforcement learning of structured exploration strategies." Advances in Neural Information Processing Systems. 2018.  [4] - Da Silva, B., Konidaris, G., & Barto, A. (2012). Learning parameterized skills. arXiv preprint arXiv:1206.6398.  