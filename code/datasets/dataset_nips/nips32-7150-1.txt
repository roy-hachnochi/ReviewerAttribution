1, There do exist challenges applying pretraining LMs to both NLU and NLG tasks. This paper takes advantage of BERT’s masked LM objective and GPT’s architecture. Combining three types of LM objectives is a straightforward but effective extension.  2, For the second advantage of this paper (line 45-48), there is no experiments to compare pretrained LMs with different objectives. I also have doubts on why single objective LMs will overfit since it is trained on large scale corpus.  3, The generation experiments are not convincing enough because the authors only conduct experiments on summarization and question generation. I would like to see more experimental results on other generation tasks such as machine translation and response generation. 