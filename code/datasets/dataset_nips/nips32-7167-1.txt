High level takeaway from this paper: This is a solid theoretical work with improved theoretical and algorithmic results.  Pros of this paper: - Tighter approximation guarantee while achieving a tight convergence rate - Improved variance reduction technique for estimating differences of gradients which is a critical ingredient in their setting - Lower bounds showing their results are tight - Extension to stochastic submodular maximization  Cons of this work (also includes some questions/clarifications which are not clear in the paper) - Seems like a condensed version of a much longer paper: While this is not uncommon in NeurIPS, it does affect the clarity of the paper. Also all the proofs and main ideas are in the extended version which makes the paper harder to go through. This is not a criticism as such, but just a suggestion to improve the readability of this manuscript - Extension to the discrete setting: While the result seems to follow from the results of the continuous submodular counterpart and the multilinear extension, I do not understand how one would compute the multilinear extension efficiently. One still needs to sample the ML extension which is high polynomial in complexity? I'm not sure how the authors are circumventing this. - Lack of concrete examples: This comment is coming more from a practitioners perspective. It is unclear how to use such an algorithm in practice. What are concrete examples of continuous submodular functions and how do the results in this paper impact them? Lack of empirical results demonstrating the improved convergence: Related to the above. This paper has no empirical results to demonstrate how these carry over in practice. Does the improved convergence results imply improved performance in real world applications? I would like to see this in the main version of the paper.