The paper utilizes the fact that when the support of sparse code is identified, larger step size can be used to speed up the convergence.  If I understand it correctly, as alpha/beta*W = D, then alpha*W in (11) is replaced by beta*D, and the authors claim that only tuning beta (which is replaced to alpha in (14)) is enough. However, in the experiments, there're no results on the effects of different step sizes.   Another issue is that in section 5, I cannot tell why SLISTA is better. And I am curious about why ALISTA is nearly not working at all.   Also, the authors mention that this LISTA style model can be trained unsupervised or supervised, are there any empirical evaluation on that? In fact, the claim that ISTA does not converge for the supervised problem in general seems odd. Is there any support for this claim?  Finally, in comparison to ALISTA, this paper is less general and comprehensively analyzed. For example, the convolutional LISTA and robust version are interesting to consider. 