Although this is an interesting work, the main argument of this work is not so convincing. The authors argue that the even-sized kernel will shift the feature map which results in performance degradation. However, the shifting may not be the key reason here. One popular explanation is, when using 2x2 kernel in downsampling layer with stride=2, information will be lost since no overlapping between adjacent convolution patches. The feature map shifting may not be a key issue here as convolutional operator is invariant to spatial shifts. I am not convinced that asymmetric or symmetric padding will make big difference in DCNN.  ====== Post Author Feedback ====== I read the author feedback. The authors provide more experimental results to show the performance improvement of symmetric padding. I raise my score based on the new numerical results which seem promising. 