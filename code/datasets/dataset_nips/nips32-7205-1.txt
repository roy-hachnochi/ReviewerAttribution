The authors propose sketching gradients to solve the communication overhead of large-scale distributed SGD. In particular, they show a reduction from O(d) to O(log d) where d is number of model  parameters. Count-sketch is used to estimate the top-k elements of the gradient by first identifying the heavy-hitters and then getting their values from the worker nodes.  Extensive experimental results are shown on a wide-variety of tasks showing up to 40x improvement in communication cost.  Comments:  Overall, the paper is well-written with clear remarks to show how it differs from related work such as Stich et al which it heavily relies on. The results are important because SGD is a workhorse for a wide-range of deep learning models and speeding up the communication can help train better models/architectures in a shorter time.