 This is a very good paper. I really enjoyed reading the paper. The result is very strong and the paper is very readable. I strongly recommend accepting the paper, if the proof is correct. (I could not check the supplementary material due to time constraints.)  This paper solves an important open problem in robust estimation. The open problem is described in the beginning: "Is it possible to attain optimal rates of estimation in outlier-robust sparse regression using penalized empirical risk minimization (PERM) with convex loss and convex penalties?" The answer was negative in the past researches, but the present paper gives the answer "YES".  The estimate is very simple, because it is obtained from the minimization of the simple Huber-loss with the $l_1$ penalty with some devised tuning parameter $\lambda_o$, which is an interesting point. To show the optimal convergence rate, we need some additional assumptions that the authors honestly mention on l.26-27. However, they are not strong.   The introduction is very clear. The authors give a clear history of the study of convergence rate in robust estimation. The key sentences are: "this result (which means the result obtained in this paper) is not valid in the most general situation, but we demonstrate its validity under the assumptions that the design matrix satisfies some incoherence condition and only the response is subject to contamination." and "The main goal of the present paper is to show that this sub-optimality is not an intrinsic property of the estimator (3), but rather an artefact of previous proof techniques. By using a refined argument, we prove that $\hat{\beta}$ defined by (3) does attain the optimal rate under very mild assumptions."  Section 2 gives a key theorem. In particular, the authors illustrate why some complicated assumptions are necessary in the first paragraph with interesting examples, and they also give the main point of the proof which is to treat the extra parameter $\theta$ as a nuisance parameter when we obtain the bound, but not in the past.  Section 3 focuses on the Gaussian design.  Section 4 gives a very clear survey on prior work.  The authors give future works very clearly in Section 5, including technical details amazingly.  Section 6 is a numerical experiment, which verifies the order of $o/n$. This is a very small experiment, but this is enough, because the main purpose of this paper is a theoretical one.    Major Comment I have only one comment. I think the largest feature is that the tuning parameter $\lambda_o$ is incorporated into the Huber loss in a different way. The usual $l_1$ penalized Huber loss function with $\lambda_o=1$ will not give the optimal convergence rate. What is the role of $\lambda_o$ on eq.(5)? (although the role is clear on eq.(3).) $\Phi(u)=(1/2)u^2 \cap (|u|-1/2)$. The effect of $\lambda_o$ vanishes for $u^2$, but it remains for (|u|-1/2), which gives the loss function $\lambda_o\sqrt{n} ( (1/n)\sum_{i=1}^n |y_i-X_i^\top \beta| -1/2 )$. When $\lambda_o$ has the order used in Theorem 3, the factor $\lambda_o\sqrt{n}$ has the order $(\log(n))^(1/2)$, which converges to infinity. This implies that the effect of large error of $|y_i-X_i^\top \beta|$ is not admitted at all. As a result, the main loss is the squared error only. Is this an appropriate point of view? If you have a clear point of view on the role of $\lambda_o$ on eq.(5), please give related comments.    -------------------------------------------- Thank you for your reply. 