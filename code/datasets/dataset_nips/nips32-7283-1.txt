 updates after rebuttal ----------------------------  I read the rebuttal and still think more evidences are needed to prove that continuous bernoulli is a right choice. I agree with the other reviewers that this is an interesting and valid question to look into. However, I do find in the current form this paper does not meet the requirements to be published at NeurIPS.  - The simplicity of the approach is not necessarily a problem, but in this case we would like to see more empirical analysis (ideally on real-world architectures used every day and common benchmarks). This is clearly missing from the current paper.  - There is no clear evidence that the proposed continuous Bernoulli distribution outperforms other easy or widely used choices (e.g, Beta, Gaussian, 256-way softmax, discretized logistic). In the newly added experiements on CIFAR10 the proposed CB distribution is outperformed by Beta, which obviously leads to the concern about the motivation.   Several comments: 1. There isn’t much information in section 4.1, 4.3. 2. In section 4.5, it is mentioned that applying the transformation of the mean of continuous Bernoulli can help fix some problem of the VAE trained with improper bernoulli likelihoods. It is pointed out by the authors that the common reasoning is false but interestly, there is still improvements if we do this. Since this is more an analysis paper, it would be more interesting to see the reason. 3. The results on MNIST is good and clearly show improvements. But given the idea is rather simple, I’m expecting more experimental evidence on SOTA architectures. The existing results on CIFAR is not satisfying. 4. The authors show in some experiments that continuous Bernoulli has better performance than Beta distribution, which is a natural choice for continuous distribution over [0, 1]. But there is no explanation for this. Why should we prefer continuous Bernoulli over Beta? 