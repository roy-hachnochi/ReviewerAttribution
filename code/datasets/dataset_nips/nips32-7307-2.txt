The introduction of gossip algorithms to Deep-RL is original. The work is generally clearly presented, but some of the reported baseline results do not match previous published works.  Figure 1: The IMPALA results look completely off, as do the A3C results on pong, and the A3C results in the appendix. There shouldn’t be such a discrepancy between A3C and IMPALA when running with the same hyperparameters (there is a larger discrepancy on some games, but not these ones). I suspect a bug in the IMPALA implementation, or at least an unfair comparison due to all other results using a more recent (and hence more tuned) set of hyperparameters from [Stooke&Abbeel 2018]. It should be noted that individual Atari games are particularly sensitive to individual hyperparameters, and this can easily dominate the difference between algorithms. Comparing on a larger number of games, and using comparable hyperparameters across methods, would alleviate this. In particular, the difference of the unroll length N (5 and 20 for the two sets of hyperparameters) is quite significant. These differences make me doubt the quality of the conclusions presented.  140-144: I can’t help but feel that this definition for an update is arrived at in a backwards manner, when it could be defined more simply.  157: Related works define beta inline. This could too, rather than alluding to it being “related to the spectral radius of the graph sequence”.  Table 1: Evaluation by sampling directly from the policy is more standard, and can sometimes outperform the argmax action.  Table 1: The performance of A2C and GALA-A2C on BeamRider are with in stderr of each other, so A2C should be bolded too. This makes 3 games where GALA-A2C is on par with baseline methods in terms of learning performance, outperforms in 2 games & underperforms in 1 game. On its own, this is a positive but minor result. The main benefits of GALA-A2C over A2C are computational as described later.  202: What other hyperparameters are affected when increasing the number of simulators? Does this change the size of the batches used for training? Please provide more details of this experiment.  203: Convergent at what point in training?  Figure 2b: What does it mean for an A2C agent to have multiple ‘agents’? Do these refer to different shards of the training batch, on different devices, whose gradients are summed using allreduce in [Stooke & Abbeel 2018]?  Figure 3: This is a nice figure demonstrating the computational potential of gossip based algorithms over A2C. Would it be possible to add the other baseline methods to these plots? (A3C & IMPALA).  Supplementary C.2, Figure 4. The results presented for `A3C (Mnih et al)` do not reproduce the published results from that paper, suggesting a bug in the implementation.   ------------- Edits in response to author feedback: Thanks for answering the majority of my questions.  To clarify: IMPALA was trained for 200 million frames (see section 5.3.2. of the paper, third paragraph). Are the rest of your results using `environment_steps  == frames/action_repeats`?  Using `frames` as a way of counting experience is more common - `steps` are clearly ambiguous when using action repeats. I suggest using `frames` throughout the paper.  I'm still unconvinced by your baseline results for IMPALA - but those concerns aside, GALA-A2C demonstrates a computational improvement on top of Stooke's A3C or A2C with a solid theoretical basis, and the comparisons of energy usage and hardware utilisation are nice.  I was perhaps a little ungenerous when writing my review and have increased my score. On reflection, I am actually curious to see how gossip approaches would perform in similar setups in my lab.