This manuscript has strong merits on a theoretical level -- it studies an important and wide-ranging class of models by providing an innovative and creative parameterization of smooth, random functions.  It is not yet clear to me whether this method will be very easy to use in practice, or whether adding appropriate modifications/approximations into Gaussian Process methods will be sufficient for practical purposes.  This paper only explores incorporating a nonnegativity constraint on the function class. However, this doesn't seem to be a serious challenge to methods like pp-GPFA, which pass the GP through an exponential function and use black box variational inference to fit the model. While the author's state that they outperform GPFA, this isn't to surprising since their VAE allows for a nonlinear mapping from the latent space, while GPFA is restricted to a linear mapping. In other words, the improvement could be attributed to other factors that are not the core message of the work. (Perhaps the authors could argue/show that GPs + BB variational inference wouldn't scale to fit a similar VAE?)  The way the authors control for degrees of freedom between their various models in the model comparison section is confusing to me. The bin size seems like it should be optimized on a per-model basis, and the only thing kept constant across models is the dimension of the latent representation. Adding more bins for the spike times does not make the latent representation less interpretable, and I wouldn't expect it to be very computationally costly. So why handicap these baseline models?  Finally, I can think of a couple potential advantages of Gaussian Process methods over the DRS approach. I don't view these as critical shortcomings of the paper; however, it might be useful for the authors to directly address or clarify these points:  1) By specifying the kernel/covariance functions of the GP, practitioners can control the power spectrum and add (for example) periodic structure into the model. Additionally, this property makes GPs very attractive for certain analytic derivations. Controlling the frequency content in DRS would seem more challenging.  2) DRS, like many deep networks, may be challenging to optimize and require substantial hyperparameter tuning for different problems, whereas GPs are not typically combined with deep nets and thus are easier to optimize.