Response to authors' feedback:  I thank the authors for the rebuttal. My score remains the same.   -----------------------------------  The work proposes an initialization for deep linear networks in which the last layer is initialized to 0, and the rest to the identity Id. With this initialization, the networks are shown to converge linearly to zero loss, under conditions (for discrete-time GD) that are different from and perhaps conceptually simpler than previous works. For instance, compared to reference [2] (Arora et al “A convergence analysis of gradient descent for deep linear neural networks”, ICLR 2019), this work removes completely the delta-balanced condition in [2] by showing that this condition actually holds, for most layers, on the GD trajectory (Lemma 4.2 and Eq. (4.6)).  While certain elements have already been seen in previous works (e.g. the property in Lemma 4.2 is similar to the delta-balanced condition in [2], or the requirement of zero initialization for the last layer’s weight has been seen in “fixup initialization” of reference [21] in the context of residual networks), I think the proposed initialization as well as the convergence analysis here deserve credits for novelty. In particular, I appreciate the insight that at the region where symmetry breaks, the velocity is bounded away from zero (line 146 and Eq (4.4)), and hence one ought to initialize accordingly.  Interestingly, from my understanding, some further extensions of this analysis might say more about the landscape on which GD travels. For example: - Firstly, by symmetry, one can actually choose any one weight matrix to be the zero-initialized one, with the rest initialized to Id.  - Secondly, one can modify the ZAS and initialize the last layer as non-zero appropriately (say, 0.1*Id), such that the two identities after line 145 hold.  These points further attest some degree of distinction from “fixup initialization” of [21] for this work. - Thirdly, I think instead of initializing to Id, one can initialize to alpha*Id, for alpha>1, and get an improvement on the convergence rate. In particular — if my mental calculation is correct — that should yield, for continuous-time GD: R(t) \leq \exp(-2\alpha^{2(L-1)}t)R(0). Note that this alpha*Id initialization doesn’t change R(0) as compared to Id initialization, since one of the matrices is 0 at initialization. This improvement is significant for larger L. Now we can see that the convergence rate can be made arbitrarily large!  The above three points, if true, should show that there might be hidden insights from the analysis of this paper.