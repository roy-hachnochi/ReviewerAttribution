I've read the author feedback and appreciate the added experiment, which shows the benefit of the proposed ZAS initialization in training very deep ResNets compared with standard init schemes (the authors tried the Xavier init.) For the moment I am raising my score to 6.  ------  This paper proves a simple theoretical result for optimizing a deep linear network: when all layers are d-by-d, if we initialize the top layer as 0 and all but the top layer by the identity matrix, then the gradient flow (as well as gradient descent with a polynomially small stepsize) enjoys linear convergence to zero risk on the matrix regression problem.  This paper is well presented and the convergence result is particularly neat. However the result is a rather direct consequence of a known alignment property on deep linear networks, and thus might be incremental in its contribution and novelty.   Indeed, the proof follows straightforwardly from the known fact (as the authors noted) that the matrices W_{l+1}^T * W_{l+1} - W_l * W_l^T remains constant on the gradient flow path. This paper comes up with a specific design which utilizes the above fact and implies that W_{(L-1):1} stays well-conditioned, and hence a gradient domination condition.  Compared with existing alignment-type results on deep linear nets (e.g. Ji and Telgarsky 2019), the present result is a bit lacking in interesting messages for real problems, e.g. when nonlinearity is present.