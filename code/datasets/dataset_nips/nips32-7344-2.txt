# Positive aspects  * The toy model of Figure 1 is insightful and effective at providing an intuition behind the potential advantages of the method.   # Post-rebuttal  Authors have responded to some of my criticisms, increasing my score to 6. * I checked the proofs in Page 5 and they seem correct to me.  # Main criticism  The main limitation I see in this paper is that experiments are exclusively done on linear networks. While linear networks might provide a useful tool for the theoretical analysis of initialization and to derive convergence rates, they are not useful machine learning models. In other words, I do not think the development of a new initialization method is useful unless its shown to be competitive on real problems (i.e., beyond deep linear networks).   Furthermore, I have concerns with the experiments the authors show even in this unrealistic scenario. The authors only show _one_ run of their experiments where ZAS outperforms the near-identity initialization. It is hence unclear whether this is a reproducible effect or an exception one that only arises in this very specific example that the  authors hand-picked. It would be much more convincing in any case to show these curves for multiple runs of the same dataset and not just one hand-picked.  # Other comments  The paper is rather poorly written. Take for example the description of related work in L17-23. The authors write "the random orthogonal initialization proposed in analyzing the gradient descent ..." are the authors citing a paper named "analyzing the gradient descent ..." ? are they describing the paper?,  or in the next line "how the knowledge transfer across tasks in transfer learning" [what is knowledge? transfer-> transfers? furthermore, how is this related to the current paper? Almost every paragraph has some example of poor writing like this, making the paper hard to read.  Poor writing is not limited to prose. The mathematical notation is equally sloppy. For example, R is defined in Eq. (2.3) as a function of W_1, ..., W_L. It then appears right below in (2.4), where its instead a function of the iterations t. I guess that R(t) is overloaded to mean R(W_1, ..., W_L), where the W's are taken at the t-th iterate, but I shouldn't have to guess.   # Suggestions  It seems to be that A : B = tr(A B^T), why not use this more standard notation? 