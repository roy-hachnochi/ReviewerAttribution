        The paper is well-written and clear, the different components of       the RKN are introduced in an incremental fashion providing       motivations for their introduction.         The main concern with the paper is a lack of originality and       novelty. The RKN are based on the work in [19], which presents a       much more general framework for deriving neural architectures       for various types of sequences and graph kernels. This work       adapts the neural encoding for sequence kernels by replacing the       exact match between subsequences with a gaussian kernel       accounting for vectorial representations of symbols       (substitution matrices for aminoacids). Note that the original       work itself already mentions the possibility to replace exact       matches with dot products. The multilayer construction is also       taken from [19]. The authors introduce a number of efficiency       refinements taken from the kernel literature, such as Nystrom       approximation, mostly following another publication (ref [4])       which addresses the very problem presenting in this       work.          Overall, I believe the differences with respect to [19] and [4]       are limited and too application specific to justify a       publication in a top machine learning venue.        The most recent competitor presented in the experimental       evaluation, apart from the work in [4], is from 2007. Is it       possible that [4] is the only recent approach dealing with fold       recognition?        AFTER REBUTTAL:       The authors did better clarify the novelty of their contribution with respect to the        papers they build upon. I still believe the scope is a bit limited, but I am not       against the paper being accepted provided the authors clarify these aspects in the revised version.          