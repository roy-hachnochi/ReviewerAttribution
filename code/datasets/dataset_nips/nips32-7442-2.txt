---------------------------- Post rebuttal update ------------------------------- Authors clarified in the rebuttal that they have already been using a variation of a baseline and simulated experimental setup that I was asking for in the review and that misunderstanding arose due to insufficient clarity in some parts of the paper. I believe this paper has some interesting ideas and novel results that are of interest to the community. I am increasing my score, provided authors will include necessary clarifications and discuss missing related works. Iâ€™d also appreciate if the code is publicly released. ------------------------------------------------------------------------------------  I have summarized the positive aspects of this paper in the "contributions" section. Here I will focus on the questions, concerns and suggestions for improvement.  My key concern is the relation to the earlier work [1] not mentioned in this paper. [1] studies the problem of OT under invariances such as rotation and translation and demonstrate applications on unsupervised word translation. Their formulation is essentially the eq. (4) of this paper. This paper differs from [1] as it allows to account for clustering structure via the hierarchical OT formulation. I think [1] should be considered as one of the key baselines to justify the approach presented in this paper. Application to word translation could serve as a an interesting higher dimensional example increasing significance of this paper. Word embeddings are suitable for clustering (empirically) and there is an optimism that HiWA may work well there.  [1] Alvarez-Melis, D., Jegelka, S., & Jaakkola, T. S. (2018). Towards optimal transport with global invariances. arXiv preprint arXiv:1806.09277.  Question regarding synthetic experiment: how is the second dataset generated and how is the true R computed? What is the reason to use a different random subspace per cluster (besides verifying Lemma 4.3). Equations (3) and (4) suggest that there is a single unknown orthogonal matrix miss-aligning two datasets. Why not simply generate data from a mixture of Gaussians, then apply a random rotation to parameters of the mixture and generate second dataset - this seems to better align with the problem formulation.  Minor comments and typos: 132: augmented -> alternating 156-157: links are not working; what is update (1) and update (2)? 177: program -> problem 179: extra "the"