update: I've read the rebuttal and keep my original score. It is good to see new results on a harder environment + how choosing right metric helps in another one. I keep the original score since the theory/math is sufficient for this method, but is not a significant contribution.   ---  The authors propose a novel curriculum generation technique that uses both the observed task goals/initial states and the visited states information to automatically generate a set of achievable but task-relevant goals for better exploration. The technique is combined with HER to achieve substantial improvements on sparse reward domains, especially in the context with adversarial initial-goal states.   The paper is written very cleanly and in good quality. The motivation is well grounded based on the smoothness of value functions. The idea is simple but novel and effective. The illustration in Figure 2 is convincing of the failure of HER in adversarial cases and how this approach can improve exploration. Figure 4â€™s comparison with prior ground-truth success-based curriculum also shows favorable result. This curriculum method is also realistic, as it does not make resettability assumption or Monte Carlo success rate evaluation as done in other works such as reverse curriculum generation. The author also discusses the core limitation of the method as the reliance on distance metric for curriculum and suggests future work in conclusion.   Comments: - extension: for faster adaptation to target goal distribution, authors may even consider changing exploration goal within a rollout (in K=1 setting) - can monotonic improvement guarantee for this curriculum approach be derived? 