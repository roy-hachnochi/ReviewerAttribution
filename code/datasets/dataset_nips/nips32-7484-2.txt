This submission includes a number of original and significant contributions and show-cases them in different, challenging first-person environments and various experiments. Rather than training training forward models in isolation and then using them for control, the authors train them jointly with a model-free policy, using the internal state of its recurrent network as a belief state from which future frames are predicted. Hence, the paper addresses several current research topics of high interest: (1) training powerful environment models, (2) effective memory methods for control in POMDPs and (3) improving the sample complexity of model-free RL algorithms. The authors make a case for the necessity of overshooting during RL. I find the motivation insightful since overshooting is usually motivated by the desire to produce coherent multi-step predictions (for planning) rather than by improving conditioning. The experimental evaluation is clear and insightful but, as mentioned in the conclusion, leaves the reader with a few unanswered questions. In particular, it would have been nice to further investigate the interplay between policy and forward model performance. That being said, the experiments are certainly impressive, especially the ones in the voxel environment that demonstrate data efficiency, and support the main theses of the paper quite well.