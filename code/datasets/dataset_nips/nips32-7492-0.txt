Post-feedback update: Thank you for your update. Your additional results will strengthen this paper, and I still think it should be accepted. ------------------------------------------------------------------------------------------------------------- Originality: The ideas presented in this paper represent somewhat of a synthesis of other ideas. Specifically, it combines the basic overall framework for SPEN training using a reward signal introduced by [1] with the idea of adding in random search to find reward scoring violations, which has been used in the past by various papers (which are cited appropriately in this work). However, this exact combination is novel.  Quality: The motivation behind using random search to augment the generation of labels to use for training the model is sound and verified empirically. Numerous appropriate baselines are included, ranging from beam search-type approaches to more directly comparable approaches such as [1], and the introduced approach outperforms all of them. There are additional results presented that further reinforce some of the ideas and motivations introduced when describing the model: specifically, that some problems use reward signals that are somewhat uninformative in many regions of the search space, and that using solely gradient-based approaches to find new training points can cause the model to get "stuck" in local optima. It would have been interesting to see experiments in a semi-supervised setting to see how much this approach can augment training using a limited amount of fully-labeled training data, but the content in this paper is sufficient to be interesting on its own.  Clarity: The ideas are presented clearly and logically, and there are no problems in understanding the problem, the motivations behind the solution, and how the solution addresses shortcomings of other approaches. The experiments are described in adequate detail and the results are easy to understand.  Significance: The ability to utilize a reward function for training instead of full supervision is appealing, since getting full training labels can be much more expensive than being able to provide a reward function. The presented results indicate that this approach can provide significant improvements over competing approaches that do not use full supervision and thus is worthwhile to use.   [1]Rooshenas, A., Kamath, A., and McCallum, A. Training structured prediction energy networks with indirect supervision. NAACL: HLT, 2018.