===== Update following rebuttal ==== The additional experiments strengthen the submission so I am updating my score to 6. I think there should be a more serious discussion of the accuracy-vs-computation tradeoff for the truncated randomized search (number of steps, margin requirement, etc). Unfortunately, this point was not addressed in the rebuttal. =====  Overview: This paper proposes an approach called Search-Guided SPENs for training SPENs from reward functions defined over a structured output space. The main idea is to refine the prediction of the energy network by searching for another output that improves its reward. If the search is not too expensive, this can speedup training and improve performance. Rather than gradient-based search (previously suggested in R-SPENs), which can get stuck, a truncated randomized search with reward values is proposed here. Experiments suggest that the proposed approach can indeed improve performance on several tasks with relatively cheap computation. Overall, the approach is presented clearly and seems to improve over previous work in experiments. However, I feel that some aspects which are left as future work, such as results in the fully-/semi-supervised settings and investigation on the effect of the search procedure, should actually be included in this work.  Detailed comments: The effectiveness of the randomized search procedure seems central to this approach, but it is discussed in Appendix B. It seems like this should get more attention and be included in the main text. Also, it seems interesting to explore smarter search procedures that exploit domain knowledge and compare them to the randomized one in terms of the computation-accuracy trade-off. This is mentioned as future work, but feels central to understanding the merits of the proposed approach.  Experiments: * In the experiments, it is interesting to add a comparison to a fully supervised baseline that does use ground-truth labels (e.g., vanilla SPENs) in order to get a sense of performance gaps with reward-based training, whenever ground-truth labels are available. In multilabel classification this is especially relevant since the reward function actually depends on the true labels. * An interesting question is whether the proposed approach can improve over supervised training (e.g., vanilla SPENs) in fully-supervised and semi-supervised settings, and not just learn a predefined reward function, but this is not addressed in the paper. * How is the trade-off parameter alpha between energy and reward in eq (2) chosen? Presumably some tuning is required for this hyperparameter which should be considered for the training time comparison. * Are the results reported wrt to some ground-truth or wrt the reward function? I was expecting to see both in Table 1. This distinction can help understand the source of errors (optimization vs mismatch of reward and performance measure).  Minor: Line 120: notice that y_n may not exist if y_s is already near optimal. This is handled later (line 135), but only as failure of the search procedure, and not because of near-optimality. Would be good to clarify. Line 229: “thus we this” Line 301: “an invalid programs” 