============= Update after rebuttal ================ I have read the other reviews and the authors's rebuttal.  I appreciate the additional experiments presented by the authors and will be upgrading my recommendation to a 7 to reflect this. I still think more of these additional experiments and larger-scale experiments would increase the paper's significance by a lot, but I think it does pass the bar for publication in its current state. =============================================  The paper introduces a new twist on the ranked-based approach to training structured prediction energy networks via light supervision (where light supervision means that the learning signal for the energy function learned by the model comes from enforcing that the energy levels are consistent with the levels of a reward function). Instead of picking random samples guided by the energy function, which will often offer the same reward (since this function is mostly uninformative and has wide plateaus), the samples are sampled first through gradient-based inference, and then via local search on the reward function itself, to make sure that there is a difference in value between the two samples. The algorithm is run on 3 small scale structured prediction datasets and is shown to outperform the previous ranked-based SPEN training algorithm.  Originality The main contribution (i.e. the new sampling of the datapoints) is a new twist on an existing algorithm. As such it's not very original, though novel. The related work is extensively cited and the delineation with the contributions of the paper is well done.  Clarity The paper is very well written and easy to read. While probably a bit verbose, it explain in details both the SPEN models, their training, the new proposed training and its relations with the previous work. There are a couple of surprising claims though, which are worth noting (see details below).  Quality While the algorithm is applied on 3 tasks, all of them are very small scale. One cannot really evaluate the promise of the approach on such datasets, so in this sense the paper might not be quite ready yet. Otherwise the paper is technically sound.  Significance Again, while the innovation is fairly minor, it might result in big improvements empirically, but one cannot readily verify it for the lack of large scale task in the experimental section.  Question: This is beyond the scope of the paper (although it would make for a nice addition and help strengthen its originality. In the setup considered, we only use the light supervision of the reward function. On a fully labeled dataset, it is possible to compute many rewards based on the ground truth labels. Would you expect that training with both the supervised loss as well as the lighter supervision of these additional rewards would work better than to train simply with the supervised loss?   All told, this paper is on the fence as regards acceptance. It is very clear and of good quality, but might still be improved with larger scale experiments.  Details l157-159: the claim is a bit surprising, considering gradient descent is notoriously prone to converging to poor stationary points as opposed to the global optimum. l200 & l202: the citation should read Daumé and not Daumé III. The way to do it in a bib file is the following: author = {Daum\'e, III,  Hal and ...} l215, the claim that the models do not have access to the ground truth is misleading at least in the case of multi-label classification where the reward is a direct function of the labels.