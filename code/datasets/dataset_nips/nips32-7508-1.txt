It's an interesting idea supported by a lot of theoretical derivations. Especially on the artificial tasks I could understand that reward redistribution would indeed help the agent learn the task more quickly. On atari games, things are much more complicated, as the scenes are complex and contribution analysis may fail to capture essential scene elements and actions.  Though generally positive, I have a few critical comments:  1. The experimental results were not carefully analyzed. Especially since the results of contribution analysis can be visualized easily (e.g. what frames contribute to most changes of predicted rewards), such visualizations would help us identify when RUDDER works and when it fails. 2. Although theoretically the optimal policies should be unchanged after reward redistribution, RUDDER led to worse performance on a significant portion of atari games. Closer analysis is welcome, for example the authors could use the visualization technique suggested above. 3. The authors should discuss more about the limitations of RUDDER. Apparently it's not for all tasks. There's a trade-off between quicker learning of Q values and errors of contribution analysis. 