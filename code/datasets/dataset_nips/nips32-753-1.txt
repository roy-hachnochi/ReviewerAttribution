Summary: the submission shows that a robustly trained classifier encodes strong visual priors about the world. This is a result of Tsipras [37] method and findings, and well described in L42-43, L47-48, and Figure 2. Hence they were able to show meaningful results for a wide set of class-conditioned image synthesis tasks. Being robustly trained (i.e. using eq 2 rather than eq 1) is indeed crucial for achieving such results.   Elaborated Strengths:  S1) the main point of the submission (a robust classifier encoding strong priors about the visual world) is valid. I found it insightful and would happily read a paper such as this one that would analysis/proves this. Here the main point is proved via experimentally showing the robust classifier could solve a various conditioned image synthesis tasks.  Elaborated Weaknesses  W1) The submission in numerous parts claims to be solving "computer vision" or a big chunk of it and pitches the method as a computer vision "toolkit". Eg in the title, L30 "we..build a versatile computer vision toolkit", conclusion "framework can be leveraged to perform a wide range of computer vision tasks", etc. This claim is both technically false and unnecessary. First, to be precise, the method can handle and is demonstrated to do *image synthesis* and in a *class-conditioned* manner. This does not mean "computer vision". This is indeed more similar to computer graphics objective (synthesis), and leaves out most of computer vision which is about inferring low dimensional abstractions out of visual observations (the inverse). Does this method have anything to provide about object detection, camera pose estimation, surface normal prediction, 3D reconstruction, etc? These are examples of common vision tasks and have no synthesis aspect to them. Second, this is just an unnecessary claim and a distraction, in my opinion. Just concretely showing that, as this submission does, a robust classifier encodes strong visual priors about the world is a good enough finding. I would simply state that in a straightforward manner, without unsubstantiated extrapolations to the bigger umbrellas. This is a signifiant issue in the current submission, and I believe it is crucial to majorly revise the introduction, conclusion, and title. My preliminary rating is based on the assumption that this can be sufficiently addressed in the camera ready. Please comment on this in the rebuttal.  W2) The method in its current form is limited to tasks that are about image synthesis and are class-conditioned. This should be made clear upfront.  W3) Suggestion: I see a missed opportunity here. The paper focuses on image synthesis tasks. If the robust classifier indeed encodes better visual priors about the world, I would expect its embedding to be a better and more robust *feature* as well. For instance, if you compare the embedding of a network pre-trained on imagenet as an off-the-shelf vision feature (as it is commonly done), would the feature be better if the network was trained with the robust objective? This can be tested by measuring the transfer learning performance of the feature in a style similar to taskonomy CVPR18 analysis. While this is not a basis for rejection, I would recommend that the authors consider doing it for strengthening the submission and broadening the impact. Even if the results dont end up supportive, I would include it as negative results and an interesting point of discussion for the community, as I would find the result intriguing either way.   More comments:   C1: as L112 states, it should be possible to do the inpainting (and some of the other tasks) without knowledge of the target class. Have you attempted that, e.g. by adding an argmax for the target class to the synthesis objective? Similarly, how much does knowledge of the correct target class impact the synthesis results? Eg how would the inpainting results in fig 4 look like if other target classes were used for the same image? Could be an interesting visualization/analysis.   C2: for the sake of completeness, I would suggest in some of the visualizations you add the corresponding results achieved by a non robust network. We know from the adversarial literature that the results would look poor, but could be good to include an example in some of the figures as that's the main point of this submission.   C3: both super resolution and inpainting objectives (ie eq 3 & 4) have a component in them in which part of the input image is wished to be preserved. In eq 3 that is implemented using an additional term in the loss while in eq 4 that is implemented as a constraint over x' while it could be implemented as a loss term as in eq 3 too. Was this switch deliberate? If so, any insights why?   C4: in feature painting and eq 5: how do you know what semantics each component of R corresponds to? How exactly the R vector is formed here? Also, how do you get the masks 'm' for proper feature painting?   C5: have you considered doing the same, but using networks that are trained for tasks other than object classification, eg single image 3D tasks such as surface normal prediction? I suspect the visual primitives they encode would be substantially different from and complementary to imagenet based ones. Could be an interesting study.  C6: can you pose your good synthesis results and general concept wrt to the works that show the inductive bias in the neural network architectures, even in a random network, results in good synthesis output? (e.g. Deep Image Prior [38])