The authors introduce a new method to enhance the expressiveness of orthogonal RNN via the Schur decomposition, based on previous work by Ganguli et al. Orthogonal/ unitary RNN surpass gated RNN (LSTM, GRU) in tasks that involve long-term dependencies, but are limited in computational expressiveness due to the unitary constraint. Using the Schur decomposition for non-normal matrices, the authors partly overcome this limitation, with the off-diagonal elements of the Schur matrix enabling information propagation and interactions between the different modes.  Strengths: In my mind this is a strong paper that makes important contributions to the field, further advancing a promising concept for solving the vanishing/exploding grad problem in RNN. As the authors stated, their work is strongly rooted in previous work by Ganguli et al., but through further simulations and analyses they demonstrate how these ideas could boost unitary RNN.  Weaknesses: Given that the authors argue convincingly that the Schur decomposition should give much larger expressiveness, it is somewhat surprising that the performance gap between nnRNN and LSTM (0.12) is still twice as large as the one between nnRNN and expRNN (0.06). This seems to indicate that nnRNN are not really that much further ahead than the best-performing orthogonal RNN when compared to LSTM. How do the authors explain this, is this just due to the gating mechanism that nnRNN lack?  Minor: - Isnâ€™t the result in Fig. 1b completely expected, given that an orthogonal RNN lacks the off-diagonal elements (which could boost hidden states) in the Schur form?