This paper proposes nnRNN, which is the non-normal matrix relaxation to improve the expressivity of RNNs with unit eigenvalue constraints.  Previous studies use the unitary or orthogonal matrix constraints to prevent the exploding and vanishing gradient problems in RNNs,  but these constraints deteriorate the performance in empirical tasks due to the strong constraints. This paper relaxes the unitary matrix (which is a normal matrix that has eigenvalues on the unit circle) constraints as allowing the non-normal matrices that have the eigenvalues on a unit circle. In addition, this paper theoretically and empirically shows how the non-normal relaxation affects the dynamics of RNNs.    This paper is well written, and the motivation of non-normality is explained thoroughly. The main results are original and interesting since there are few studies that focus on whether the recurrent matrix is a normal matrix or non-normal matrix.  However, I tend to vote to reject due to the following issues.   1. There is no guarantee that nnRNN solves the exploding gradient problem. The URNN paper [ASB16] shows that URNN can solve the exploding gradient problem due to the spectral norm (the largest singular value) of one rather than due to eigenvalues on a unit circle. The singular values of non-normal matrices can be larger than one even when they have eigenvalues of one. Since the spectral norm is larger than or equal to spectral radius (the largest absolute value of eigenvalues),  the gradient vanishing might be prevented by nnRNN. However, nnRNN might not prevent gradient from exploding.  As a simple example, we consider the case of a linear RNN h_{t+1}=Vh_{t} where V=[1,2; 0,1]. In this case, V is a non-normal matrix and its eigenvalues are 1 while its spectral norm is larger than 1. The Jacobian of dh_{T}/dh_{0} can be \prod_{i=0}^{T-1} dh_{i+1}/dh_{i}=V^{T}= [1,2T;0,1], and the norm of Jacobian increases according to T.  Therefore, there is no guarantee that non-normal matrices can solve the exploding gradient problem even if they can alleviate exploding gradient.  In fact, Fig. 1 shows that multiplying the non-normal matrix increases the norm of the hidden vector.  This implies that the chain rule can make the gradient explode by using non-normal matrices.    2. The experimental condition might be unfair. In experiments, the eigenvalues of nnRNN are optimized as gamma while the eigenvalues of other methods are constrained to be one. Due to this difference, the contribution of non-normal matrix relaxation is not clear;  the improvements in nnRNN might be caused by the relaxation of eigenvalues.   -------------- I have read the author feedback. My questions are mostly solved, and interesting and important discussions will be provided. Therefore, I raise the overall score.  