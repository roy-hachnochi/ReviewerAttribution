The paper considers learning in the following model: the player repeatedly plays in the same game, that is not known a-priory. Gets feedback only about the  his value for the realized outcome, but also learns actions of the other players. Two important assumptions are made about the game: 1. a regularity assumption that the game has a low dimensional kernel, and 2. that the response function is noisy with Gaussian noise on top of the originally unknown value.   The paper offers an elegant algorithm combining Kernelized multiarm bandit algorithms (from ICML10 and ICML'17) and multiplicative weights to get a no-regret strategy. The main idea is the use the UCB-style upper bounds in a multiplicative weight algorithm, which moves the performance of the algorithm closer to full information feedback.   Section 4 of the paper offers empirical evidence on the better performance of the algorithm on three different classes of examples. Unfortunately a number of the important details of these examples are deferred to supplementary materials. But there is still a convincing case for the improved performance of the algorithm.  I read the authors response. The response provided for my question (of what application can the model proposed be a good model for) is not great. True, that maybe one can get information on the number of agents traveling a route, but the more natural sources of information is actually the delay there (say offered by Google maps). And true, one can observe properties of others bidding in the auction space, maybe true for the winning bidder, but not all participants, and not all their action.  I view the paper as a first step in an interesting direction, hence my relatively high rating of 7. 