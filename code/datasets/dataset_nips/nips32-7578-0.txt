Thus paper presents a model where a latent bag-of-words inform a paraphrase generation model.  For each source words, the authors compute a multinomial over "neighbor" vocabulary words; this then yields a bag-of-words by a mixture of softmaxes over these neighbors. In the generative process, a set of words is drawn from this distribution, then their word embeddings are averaged to form input to the decoder. During training, the authors use a continuous relaxation of this with Gumbel top-k sampling (a differentiable way to sample k of these words without replacement). The words are averaged and fed into the LSTM's initial state.  Results show decent BLEU and ROUGE scores compared to baselines as well as some nice examples. However, the authors don't compare against any baselines from prior work, instead comparing against their own implementations of basic Seq2seq, Seq2seq + Attn, and VAE models. As a result, it's a bit hard to situate the results with respect to prior efforts.  As for the model itself, I like the premise a lot but I am a bit disappointed by its actual implementation. Averaging the word embeddings to get an initial state for the decoder seems like "giving up" on the fact that you actually have a bag-of-words that generation should be conditioned on. It would be much more interesting to at least attend over this collection or ideally use some more complex generative process that respects it more heavily.  In light of this, I would also like to see a comparison to a model that simply treats the phi_ij as mixture weights, then computes the input to the decoder by summing word vectors together. As it stands, I'm not sure how much value the top-k sampling layer is giving, since it immediately gets smashed back down into a continuous representation again. I do see the value in principle, but this and other ablations would convince me more strongly of the model's benefit.  Based on Figure 2, I'm not really convinced the sample is guiding the generation strongly as opposed to providing a latent topic. The bag-of-words is definitely related to the output, but not very closely. The Figure 4 results are more compelling, but I'd have to see more than these two examples to be truly convinced.  Overall, I like the direction the authors are going with this, but I'm not quite convinced by the experimental evaluation and I think the model could be more than it is currently.  ========================  Thanks for the author response; I have left the review above unchanged, but I provide some extra comments here. I see now that this is in the supplementary material, but it is still unclear from the main paper. In light of this, I have raised my score to a 7. I think this model is quite nice for this task. The results are still a bit marginal, but stronger given what's shown in Table 4 in the response. Finally, the comparisons in Table 1, even if not totally favorable, at least situate the work with respect to prior efforts. So this has largely addressed my criticisms.