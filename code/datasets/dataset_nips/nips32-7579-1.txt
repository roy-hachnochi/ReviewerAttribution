There are two main models for differential privacy: the central model which assumes a trusted aggregator which can see and compute on the data in the clear but adds noise to the output in order to provide differential privacy, and the local model where each party adds noise locally to its inputs and the aggregator no longer needs to be trusted. While the local model provides much stronger privacy guarantees, unfortunately it achieves much lower utility which is not sufficient in many practical settings.   This paper presents a new model which assumes trusted processors that essentially execute the central DP mechanism but do not have sufficient memory to fit all data. In order to avoid the leakage from memory accesses that have to happen outside the trusted environment, the authors propose to use oblivious algorithms that have access patterns independent of their inputs. This conceptual composition of TTP and memory oblivious algorithms has been considered in many previous works that compose trusted execution with limited memory with oblivious memory techniques such as ORAM or other oblivious algorithms. Thus the contribution of this work has to be searched in the particular oblivious instantiations of DP algorithms.  The paper presents three oblivious DP algorithms in the TTP framework: algorithm for counting distinct items in a database, computing a histogram and computing heavy hitters. I found the proposed constructions quite direct following previous approaches. For the first problem of counting the number of distinct items, the authors use a streaming approach that relies on a sketching technique to maintain an approximate count in private memory and the authors propose to add Laplacian noise to these counts. The solution for histogram computation uses the observation that comes from the work of Mazloom and Gordon to add fake and dummy records to the data, obliviously shuffle and then just do the counts with a linear scan. In the description of this protocol I was confused by the fact that at one place the authors claimed that oblivious shuffle is expensive and then use oblivious shuffle in step 4 of their protocol. The third protocol for heavy hitters is not even described in the main body of the paper. This construction is also direct and at the same time costly with two oblivious sorting invocations: it first sorts all elements, then computes counts with a linear scan, then it sorts the counts and adds noise to them, and then finally extracts the top counts for the heavy hitters.  The constructions of this paper seem to follow mostly from previous work. I did not also see a convincing analysis for the efficiency of the proposed constructions in a TTP architecture especially given the oblivious sorts and shuffles used in the constructions.   Related work: there is a new paper that extends the results from the shuffle DP model to larger values of epsilon: https://arxiv.org/abs/1903.02837. There are ORAM constructions with better amortized overhead than the ones claimed in this paper: https://eprint.iacr.org/2018/373.pdf, https://eprint.iacr.org/2018/892.  