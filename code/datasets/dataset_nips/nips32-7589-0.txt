The proposed approach is compelling in its simplicity -- this is a good thing, especially given its effectiveness.  It is more of an engineering solution than a mathematically motivated method.  It is highly effective, and presented clearly.  The individual aspects of the approach (compaction, retention of critical connections, etc.) have been explored in individual methods, but this seems a compelling and novel combination of these methods.  I liked the ablative dissection of the approach into existing methods as special cases in Section 1: Method Overview.  As mentioned, this approach is more engineering than mathematically principled.  That's fine given its effectiveness, but it would be useful to develop or combine the mathematical foundations to better motivate the approach.  The use of the retention masks would seem to prevent new tasks from fine-tuning previously learned models, limiting the amount of reverse transfer.  Progressive nets is incapable of reverse transfer, which is another large limitation of that approach.  Is your CPG approach similarly limited in this way, and if not, how is it capable of fine-tuning previous models based on new tasks?  The experimental analysis is good, but lacks key details on the specific setups for all experiments.  These should have been added as supplemental material, and the final paper would need to be revised to include them.  Tables 1 and 2 would be much better presented as a line or bar graph.  Are these peak performances or performances after training all tasks (which might be the same)?  If the method did incur some sort of reverse transfer or fine-tuning from subsequent learning, it would be good to examine learning curves of all tasks over time, and also to measure any amount of forgetting.  POST-RESPONSE Thanks for your comments.  The biological motivation you mentioned isn't near as satisfactory as a solid mathematical motivation, although it would be quite difficult to come up with the math/theory behind such an engineered approach.