This paper addresses the problem of evaluating spoken dialogue systems, pointing out that current methods which focus on evaluating a single turn given the history bear little relation to the overall user experience.   The core idea is a hybrid metric M_h which is a linear combination of three metric classes, each being measured across a whole dialogue: sentiment using a classifier trained on Twitter, semantic similarity measured by Infersent plus a number of simple word based metrics, and Engagement based a question score and length of user inputs.  The dialogues themselves are generated automatically by self-play ie the systems output is simply fed back as input for a fixed number of turns.  The ideas underlying M_h are then used in two ways. Firstly its main components: sentiment and semantics are used to regularise training a standard HRED. VHRED and VHCR dialogue model.  This is shown to improve human evaluations of quality, fluency, diversity, contingency and empathy in most cases.  They then show that although single turn metrics show similar trends they are very noisy and correlation between with human evaluation is poor.  However, the proposed M_h metric correlates very well with human judgement.  Overall, this is an interesting paper.  The proposed metric appears to be very useful and the self-play idea is useful where there is no existing dialogue data to evaluate on (as would be the case during system development).  However, this is really two papers in one.  Although they are inspired by the same idea, the proposed regularisation for training chatbots is really a separate topic to evaluation and I would have preferred the focus to be on the latter.  In particular, it isnt clear why the proposed metric could not be applied to some of the existing published data sets eg Multi-Woz. 