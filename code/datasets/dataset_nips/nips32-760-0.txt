Some experimental details will need to be clarified in rebuttal:  - "Unless otherwise specified, we set the perturbation magnitude to be 76 for MNIST and 4 for the other three datasets" why choose those specific magnitudes?  - "We set PGD attack iteration numbers n to be 16 for MNIST and 7 for the other three datasets" could ATML stand robust to more iterations, e.g., >= 20?  - Have you used random starting to alleviate gradient masking?     - Would the authors consider releasing their codes for reproducibility?   - Two missing references, that both empirically studied the preservation of robustness under quantization:  "Robustness of Compressed Convolutional Neural Networks", IEEE BigData 2018 "To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression", SysML 2019. 