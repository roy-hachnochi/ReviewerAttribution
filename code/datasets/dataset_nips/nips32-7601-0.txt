Originality: Although VAEs using a stick-breaking construction with Kumaraswamy distributions has been considered before (Nalisnick, Smyth, STICK-BREAKING VARIATIONAL AUTOENCODERS, 2017), the idea to use such a construction and extend it by mixing over the orderings to obtain a density more similar to a Dirichlet is new and interesting. Related work is adequately cited.  Quality: The paper seems technically sound and claims are largely supported. Although Theorem 1 is a standard result, reiterating it is likely useful for the subsequent exposition. Experimental results show that the method outperforms some baselines, however, I feel that some additional experiments would be useful (see details below in Section 5. Improvements).  Clarity:  The paper is written relatively clearly, with some rather minor issues, see below.  Significance: The idea of mixing over the orderings in the stick-breaking process construction can be a quite useful idea - for practitioners it could simplify to computing the gradients using reparameterisation plus some Monte Carlo averaging - although it is not so clear how well it compares to some recent work. Also it could be used for applications different from the VAE considered such as in non-amortized variational inference in Bayesian models with a Dirichlet prior.   Some issues: -Can you explain why equation 181 holds and how you compute the KL? There is some dependence over the components when doing the stick-breaking, plus some additional mixing over the orderings, so this is not so obvious to me how this works. -How many Monte Carlo samples over the ordering does one need to get approximately symmetric distributions in practice? Like with a 50 dimensional latent space you are considering, does this not increase the variance of the gradients too much? -Apart from symmetry issues, can there anything be said about what does randomizing over the ordering imply for the moments of the distribution (compared to say y \sim Dirichlet(\alpha) having a negative covariance Cov(y_i,y_j)=-\alpha_i \alpha_j / (\sum_k\alpha_k)^2(\sum_k\alpha_k+1)?   Some minor issues: -it might be more clearer to make explicit the conditioning on x in q(\pi) and q(z) in equation 7 -There seems to be some confusing with the letter \pi and x in lines 173 and 181. -After lines 385 and 387, I find it more clearer if the integration over the variational distribution is done earlier than after the third line.  #POST AUTHOR RESPONSE: The response of the authors makes the paper stronger, as they include additional experiments comparing the proposed approach with recent alternatives (Figurnov et al., Implicit Reparameterization Gradients, 2018; and  Naesseth et al., Reparameterization Gradients through Acceptance-RejectionSampling Algorithms), that I had liked to see as an improvement, so I increase my score from 5 to 6. The proposed method performs well. A comparison with simple score/reinforce-gradient estimator would also be nice as it might show that low-variance gradients are necessary for this specific application. However, the calculation of the KL divergence in line 181 is still not clear to me, even after the response. Particularly, it is not obvious to me why the dependence of x_{o_i} on x_{o_1},â€¦,x_{o_{i-1}} seems to not matter in the KL-calculation, or is this meant be just an approximation?   