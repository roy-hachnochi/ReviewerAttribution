The paper studies the robustness of machine learning classifiers against adversarial examples. The authors use semi definite programming (SDP) in order to obtain provable polynomial-time adversarial attacks against hypothesis classes that are degree-1 and degree-2 polynomial threshold functions. With the same techniques the authors design robust classifiers for these two hypothesis classes. Furthermore, the authors show that learning polynomial threshold functions of degree 2 or higher in a robust sense (against adversarial examples) is computationally a hard problem. The authors also experiment with 2-layer neural networks and with an SDP method they either find adversarial examples or certify that none exist within a certain budget delta available to the adversary.  First of all, there are parts of the paper that are very well written and other parts that I assume were rushed for the NeurIPS submission deadline. I admit guilty of not having read the supplementary material, but it appears that in the current form of the paper, significant information is laid in the appendix; there are cases where important information, problems and equations are referred to from the main text and appear in the appendix. Having said that this is not the only issue with the paper. Regarding adversarial examples, the authors define as success the criterion f(x+delta) != f(x), where f is the learned classifier. On the other hand, the authors also want to discuss and have claims in the context of an adversarial loss, where success is now defined to be f(x+delta) != y, where y = f(x); i.e., y is the true label of the original instance x. It appears that the authors know about these distinctions due to the subtle sentence in line 36 of the paper, but somehow do not present clearly the context in which their results hold. In fact, they even cite the paper [11] which is explicitly about distinctions on such notions. Along these lines, the Bubeck et al papers mentioned in line 105 are using a yet, third definition, where now f(x+delta) is compared with the true label of the ground truth function at the instance x+delta. In this sense, comparing the work of this paper with the line of work along the Bubeck et al papers, is simply wrong.  I think a journal version of the paper is probably more appropriate as it appears that the authors have several results and lots of content, but somehow there is still work that needs to be done on the presentation. For this reason, I think the 8 pages allowed by NeurIPS, are somehow too little for good presentation of the material, or at least, this is the feeling that one gets after reading the paper.    ---------------------------------------------------  Comments after rebuttal:  I am happy with the response that the authors gave to my part.  However, I have one more remark, which is related to the response of the authors to comments of other reviewers. In particular I would like to see the authors add 2-3 sentences and clarify that the results that they have regarding the robustness of the learned classifiers hold under conditions -- if misclassification is the notion against which they measure the robustness. In particular, the two conditions are: (i) initially f(x) is equal to the true label c(x) of the ground truth c, (ii) the label of the ground truth does not change in the regime of the perturbation delta; i.e., c(x+delta) = c(x).  > I trust that the authors will provide such a clarification in the final version of the paper and for this reason I am raising the score from 5 to 6.  Why such a clarification is important? Some explanations below:  o The definition that the authors use for adversarial examples is in Lines 118-119 and completely ignores the ground truth that provides the true labels. This definition is referred to as "prediction change" in [11].  o A successful adversarial perturbation delta yields f(x+delta) != f(x). As such, it can be the case that an initially incorrectly classified instance x has to be pushed to an x+delta (i.e., delta is strictly nonzero) and in fact now for x+delta it may even hold that f is predicting the correct label at that point with respect to the ground truth! This is of course contrary to the aim that we have for adversarial perturbations where the adversary aims to make the learner make a mistake and of course no push was necessary.  o As another example, consider the case where the learner comes up with the hypothesis f that is precisely the ground truth c (this is common, e.g., for Boolean functions). In such a case, for non-constant functions c, one can always find an adversarial perturbation under the "prediction change" definition, when in fact f can never err on any instance in the instance space. In other words, again "prediction change" does not capture fully our aim for misclassification when dealing with adversarial examples.   Thank you for a very interesting paper!