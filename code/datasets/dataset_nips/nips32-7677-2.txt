The analyses performed in this study are concrete and sound good. It is an interesting direction to apply the theoretical framework of scaling limits of stochastic processes into the training of GANs. This work revealed that the training dynamics is highly nontrivial even in a simple setting with single-layer models (i.e., a linear model as the generator and simple perceptron as the discriminator). It enables us to theoretically quantify the effects of stochastic noise on the dynamics and, especially, to observe appropriately sized noises which lead to the success of feature recovery and prevent oscillation and model collapsing.     My major concern is how one can generalize this analysis into more general settings.  ・When d>1: Although the dynamics shown in Figure 1 is the case of d=2, the phase diagram shown in Supplementary Material corresponds to d=1. Since practical GANs usually suppose the multiple features (d>1), it seems to be better to show a detailed analysis of d=2. It would be helpful to enrich the description on d=2 which is briefly discussed in lines 45-50 in Supplementary Material and show its phase diagram.  ・Norm constraints on U (in line 77): Authors say that we can set columns of U to be orthonormal with each other without loss of generality. Is there any preconditioning or whitening method which can normalize arbitrary U to a matrix with orthonormal columns? Assuming this orthonormal U will be essential in the analysis of example 1 because it should be impossible to find the solution V=U when U and V have different norms.  It is better to describe how one can normalize U which determines an unknown data distribution.   ・Case of nonlinear generators: The linear generator (1) seems to an ideal setting and one natural question is how one can generalize this analysis into non-linear generators. It will be helpful to remark on the case of non-linear generators and explain in what point the analytical evaluation become intractable in such non-linear generators as a mixture of Gaussians and VAE.    Presentational issues:  - line 1, “shallow GAN”: This word seems to be hardly used. Precisely speaking, this study investigated a solvable GAN with single-layer generative and discriminative models.  - Supplementary Material, line 51, “heuristic derivations”: In what sense is it heuristic? The derivation shown here seems to be asymptotically exact when n is sufficiently large. Is there any approximation or do you mean that the derivation is not mathematically rigorous but intuitive?  Typos:  Caption in Fig 1: model collapsing -> mode collapsing? Line 8: this analysis provide”s” Line 297: our analysis provide”s”   --- After rebuttal ------------------------------------------------------- Authors' response solved most of my concerns. I am still not so convinced of how the analysis can be extended into more general settings, but Authors did some additional works for understanding the phase diagram of d>=2 and promised to add comments on more complicated generators such as ReLU and learnable P_\tilde{c}. So, I keep my score and am looking forward to seeing the revised paper. 