# Summary  The authors present a method that exploits graph embedding methodology to use graph structure as a proxy in causal estimation problems. They provide sufficient conditions under which adjusting for an embedding learned without reference to a specific generative model will yield asymptotically unbiased and asymptotically normal estimates of causal effects at the parametric rate. They then propose an embedding network architecture that is designed to estimate embeddings that could plausibly satisfy these conditions. They conduct a series of experiments using semisynthetic data to demonstrate how these methods could work in practice.  # Feedback  This paper was a pleasure to read. The exposition is very clear, assumptions are appropriately foregrounded, and the experiments are designed to test the claims that the authors set out to make. Full disclosure: I reviewed an earlier version of this paper, and am happy to say that it is much improved.  ## Elephant in the Room: Assumption 2  My main concern rests with the identifying Assumption 2. It is left unclear how one might reason about this assumption being met in practice. The authors rightly call this out as the crux of the method, and the most difficult assumption to evaluate. However, this discussion still leaves out how one might judge the “plausibility of the predictive embedding model” without falling back to judging a generative model of at least some aspect of the network. In my opinion, assumptions are only as weak as the heuristic by which they can be judged, so I think this question needs to be addressed.  One suggestion: the embedding loss defines a set of sufficient statistics of the network that the embedding is designed to reconstruct. I think a condition relating lambda(Z) to these sufficient statistics (for example, a completeness condition stating that unique values of the latent variable map to unique distributions of the sufficient statistics) could be a good starting point. Completeness conditions play a major role in other work on proxy methods. This condition could probably be weakened to focus on the task of predicting treatment or outcomes, but I think there needs to be some formalization of how much confounding information is carried by the observed network. Reasoning about these conditions could also help the user design the Sample() function, which is a major degree of freedom in this approach that could also use some discussion.  This being said, the exogenous confounding do a nice job of empirically probing part of this assumption to some extent. However, it doesn’t help the reader to try to reason about how confounding information might be represented in the aspects of the network that are being modeled by the embedding.  ## Empirical Experiments  A major strength of the paper is the empirical experiments, although, as with all experimental sections, there are more aspects to explore.  I have one major concern with the statement that adjusting for the network always helps, even when confounding is not fully explained by the network. This is not generally true for confounder adjustment: it is well-known that adjusting partway for confounders that are highly predictive of treatment but not of outcome can increase bias. This phenomenon is known as bias amplification or Z-bias (see, e.g., Middleton et al https://www.cambridge.org/core/journals/political-analysis/article/bias-amplification-and-bias-unmasking/B95DDA52BE93B761C067EEE60739DDBD, Ding et al https://academic.oup.com/biomet/article/104/2/291/3737784). The simple generating process in this experiment does not admit this behavior because the propensity score enters directly into the prognostic score, so it is not possible to explain variation in the treatment without also explaining a proportional amount of variation in the outcome. I would expect to see this problem arise in an experiment where the network is highly predictive of treatment assignment and much less so of outcome, but this possibility is excluded by design in the experiment.  This is not a damning concern, but I would encourage the authors to consider tempering their conclusions on this issue, and to raise the possibility in a discussion of results. Ultimately, one still needs to reason about what information is conveyed by the graph embedding. There is also some amount of control that the user has about what information the embedding will carry based on the weights they put on the two predictive portions of the network. Perhaps this could be suggested as future work.  Smaller questions: Why is the parametric model given 128 blocks? I am curious about what would happen if you gave the parametric model three blocks, as I imagine at least the district latent variable being expressed as a block structure in the network.  Is it possible to visualize how the user features manifest in the network? For example, does the adjacency matrix appear to have 3 communities determined by district?   ## Nitpicks  Miao et al does not need the marginal distribution to be identifiable. That is the primary contribution of that paper.  For the sufficiency of Q(1, Z), people often cite Hansen 2008 https://academic.oup.com/biomet/article-abstract/95/2/481/230183 as well.   In the network ERM objective, does the subgraph always include all of the original nodes, and are you only subsampling edges? If not, what do you do for units that are not included as vertices in the subgraph.  -------------------------  # Post-Rebuttal Feedback  I am still bullish on this paper because I think it presents an innovative approach to the problem and explains all of its moving parts clearly. I think somebody can read this paper and know exactly why they would dispute conclusions based on the methodology.  I am a bit disappointed in the response regarding Assumption 2. I was hoping for a little bit of deeper thought on this issue (especially given that this is the second round of feedback that's focused on this question). I don't find the analogy to image models to be particularly compelling, because we can directly assess the performance of those models against ground truth, but only have our assumptions to rely on in causal inference problems. In fact, there are active debates about the reliability of decision-making that relies on the internal representations of these models. So I think the authors need to reflect a bit more and modify their advice here. I think they need to at least highlight a number of the important factors discussed in the proxies/measurement error literature that determine how well causal effects can be recovered.  All of that said, I think this paper still meets the bar for publication. This would also make it easier to have discussions out in the open about the potential pitfalls of using embeddings as proxies for confounding variables, which I think would be useful for the community as a whole. 