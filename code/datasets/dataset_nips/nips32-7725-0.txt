This work considers a challenging and important setting of heavy-tailed rewards and combines a couple of existing techniques and ideas in a novel way: regret lower bound proof (Scarlett et al.), adaptive truncation in feature space (Shao et al.) and kernel approximation techniques (Mutny et al., Calandriello et al.). While the same problem has been considered in the k-armed bandit setting (and linear bandits) before, the extension to the kernelized bandits setting is new and introduces additional challenges that require a different set of ideas.    The paper is well-organized and clearly written. The exposition is clear and a great number of remarks help to better understand the obtained results in the context of previous works. The paper structure makes a lot of sense: a suboptimal algorithm that is easy to understand -> regret lower bound -> main algorithm and almost optimal regret upper bound result. The related work is also adequately cited in my opinion. Apart from the previously mentioned works, the related works on kernel ridge regression, GP-UCB, relevant stochastic bandit literature, and other BO works are well covered.   Unlike some previous works in BO that used kernel approximation schemes to reduce the computational cost of inverting kernel matrices (in, e.g., works on high-dimensional BO), this work uses kernel approximation schemes to obtain almost optimal regret bounds. This is done by adaptive truncation of features, while at the same time keeping the approximation error low (the latter was also done in, e.g., Calandriello et al.).  The connection between these two ideas is novel to my knowledge.  I did not check the proofs in the appendix, but the main presented results seem convincing and the overall analysis certainly looks non-trivial. The novelty in the lower bound proof is the construction of the reward distribution which is specific to this particular setting while the rest of the analysis closely follows the one from Scarlett et al. When it comes to TGP-UCB, I also agree that blowing up the confidence bounds by a multiplicative factor of b_t in data space provides valid confidence intervals, but I am wondering if switching to the feature space (+ kernel approximation) is indeed necessary to obtain the optimal regret (and confidence intervals) bounds, and if yes, why precisely would that be the case? In ATA-GP-UCB,  the ridge-leverage-scores-based weighting of different directions is used together with the adaptive truncation to compute approximate rewards. However, TGP-UCB performs truncation only, which leads to suboptimal regret bounds (e.g., in the case of SE kernel) even when reduced to the sub-Gaussian environment. This makes me wonder if some other raw observation truncation ideas can lead to at least a better (if not optimal) regret bound than the one obtained for TGP-UCB.  Minor comments: Unlike the exploration (confidence) parameter in TGP-UCB, ATA-GP-UCB requires the knowledge of the time horizon T.     Unlike the sub-Gaussian environment, here, one needs to estimate two parameters alpha and v. Perhaps, it is worth testing empirically (on synthetic data) for the robustness of the obtained confidence sets (e.g., from Lemma 1) with respect to the misestimation of these two parameters.   Finally, this work also complements the standard confidence bound results (from Srinivas et al., Abbasi et al., Chowdhury et al., Durand et al, etc.) with the new ones in the case of heavy-tailed rewards. Both the result and the technique used to obtain the tighter confidence sets that depend on the approximation error (similarly to Mutny et al. and Calandriello et al.) can be of interest to the BO/GP bandit community.  ---------------------------- Edit after author rebuttal: After reading the rebuttal and other reviews I remain confident that this is high-quality work and should be accepted.