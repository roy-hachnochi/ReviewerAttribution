This paper provides a new regularizer for robust training. The empirical results show the efficiency of the proposed method. But there are some places the authors should further clarify:  1. Previous work shows that gradient obfuscation is the mechanism of many failed defenses, but no work verifies that prevent gradient obfuscation can lead to better robustness.  2. In Eq (7), the authors give an upper bound of the loss gap, and minimize the upper bound in the training objective. I wonder why minimizing the upper bound will be better than directly minimizing the loss gap, as basic PGD-training does.  3. The authors should report results on more diverse attacks, like Deepfool, which is more adaptive to linear loss function.