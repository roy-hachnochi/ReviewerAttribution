+++ post rebuttal update:  I agree with the other reviewers that the clarity of some of the definitions and results of this paper should be improved but I do not think this is a significant argument in favor of rejection. My own concerns have been addressed successfully in the rebuttal and if the authors revise their work as promised I think it is in very good shape. I will thus keep my score as it is. +++   This work generalizes the seminal Fukumizu & Amari [4] paper from 2000 in the following ways:  - Section 3 studies embedding networks with size H_0 in a wider network of size arbitrary sizes H compared to just H_0+1 in [4]. Contrary to the case in [4], the networks can be of depth >3 and in this case embeddings (ii) and (iii) do no longer necessarily give rise to critical points.   - Theorem 5 studies the specific 1HL case and finds that embedding (i) gives rise to a saddle point  - Section 4.1 generalizes the original embeddings of [4,14] to the case of ReLU activations and shows that two of them yield more flat directions compared to smooth activation functions. - Section 4.2. shows that a minimum is always embedded as a minimum (saddle) for the inactive unit (replication) method. Again, I think this is only for 1HL networks but the Theorems do not specify this.   - Finally Section 5.2 makes an argument for better generalization properties of ReLU (compared to smooth activation) networks in case of embedding a zero-residual minimizer. I am not an expert in generalization bounds so I will not comment on this matter and thus enter a lower overall confidence score.  I consider these results to be interesting contributions. Yet, the generalization to deep nets is only made for the embeddings of critical points in general and not the embeddings of minima. The ReLU results are nice but at the same time not really surprising. In summary, I think the contribution is somewhat incremental but still valid and the presentation of the results is well done.    