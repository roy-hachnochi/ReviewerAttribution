This paper studies the landscape of training loss function for neural networks. Specifically, the authors consider three methods for embedding a network into a wider one and investigate how the minima of the narrow networks perform on the landscape of the loss function for training the corresponding wide networks. The theoretical results show that the network with ReLU activation gives flatter minima which suggests better generalization performance.   This paper has the following issue:  1. The contributions and results of this paper are not significant and important enough. The goal of this paper is not clear. In specific, the proposed three embedding methods can only cover a small subset of the representation functions of a wide neural network. In fact, most of the stationary points found by optimization algorithms for training wide neural networks do not correspond to the stationary points for training some small neural networks.  2. The representation of this paper is not clear. The authors propose three embedding methods including unit replication, inactive units, and inactive propagation, but do not clearly clarify them in the major theoretical results (Theorems 5 and 9). The authors should identify which embedding method is applied in these theorems and briefly discuss the corresponding theoretical results (comparison between different embedding methods). 3. For smooth activation and ReLU activation, the authors consider different embedding methods, the authors should clearly identify the difference and briefly discuss why such difference is necessary. 4. The comparison between generalization bounds for networks with ReLU activations and smooth activations is not fair because the results are derived using different choices of distributions P and Q. The authors should also discuss the generalization performance using the same choice of P and Q. 5. The experiment setting is not consistent with the theoretical results. In the experimental part, the authors set the output dimension to be 1, however, in the statement of Theorem 5, it requires that the output dimension is greater than 1.  After reading rebuttal: The authors have answered my concern regarding the expressive power of the proposed embedding methods. I would like to increase my score to 5.