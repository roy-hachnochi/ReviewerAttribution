The authors propose a Fair-Efficient Network to better to train decentralized multi-agent reinforcement learning systems in tasks that involve resource allocation. In particular they introduce a shaping reward and a hierarchical model which they train with PPO on three new reinforcement learning environments (the code of which is made available). Their model outperforms several baselines, and ablation studies demonstrate the usefulness of the hierarchical nature of the model.  The aims of the work are clear and well-stated. However, there are significant omissions in the review of related literature. Several papers have studied fairness in the context of common resources in multi-agent reinforcement learning prior to this work, namely:  https://arxiv.org/abs/1803.08884 https://arxiv.org/abs/1811.05931  and related works cited therein. Although this impacts on the originality of the paper, the methods used to generate fairness are different in this work, so with an improved literature review and drawing contrasts to the previous work, the paper could be greatly strengthened.  However, the choice of fair-efficient reward appears fairly arbitrary in this work. The equation in 3.1 could be replaced by many other options which also satisfied the criteria of Propositions 1 and 2. This is a weakness of the work, and the authors would do well to present a cogent argument for the functional form chosen.  The hierarchical model is the greatest strength of the paper. The authors derive an interesting information-theoretic optimization goal for the sub-policies. The results in Figure 6 are particularly striking. Indeed, it would be interesting to see whether merely using the hierarchical model in conjunction with some of the other baselines obviates the need for the fair-efficient reward structure. Comments / experiments in this direction would strengthen the paper.  In general there are some infelicities in wording which could be ameliorated on a proof-read. Moreover, the first two pages are fairly repetitive and could be condensed. On the other hand the descriptions of experiments are clear and concise. More details of hyperparameters and seeds chosen for the reinforcement learning training and models should be provided before publication for the purpose of reproducibility. Error bars and confidence intervals are provided in the results, but currently without sufficient explanation.  === Response to authors:  I was impressed by the response of the authors. They have clearly taken into account the feedback of the reviewers and make cogent arguments for the benefits of their method. They have also provided comparison against prior work and demonstrated the improvements that their work can bring. Moreover, it is now much clearer to me how both the hierarchy and the intrinsic motivation are beneficial and indeed complementary. Therefore I have increased my score by 2 points, and argue for the acceptance of this paper.