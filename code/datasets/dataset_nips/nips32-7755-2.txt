This paper assumes that the reward of each agent is independent from each other and the overall reward is addictive. This limits its applicability to more general multi-agent systems where multiple agents share the reward with common goals. This work uses the coefficient of variation (CV) of agentsâ€™ utilities to measure fairness. However, it is not clear how such fairness measurement is achieved with the proposed fair-efficient reward. There should be a formal proof that the CV is minimized given the decomposition of the reward. Propositions 1 and 2 are uninformative.  It is not clear why the switch of sub-policies is necessary. As stated in the beginning of Section 3.2, if other agents change their behaviors, an agent may need to perform different action at the same sate to maximize its fair-efficient reward. This seems incorrect because all the agents are trained by the same algorithm so they must be coordinated. It is true that this is a multi-objective optimization problem with fairness and efficiency. However, the agents should make their choice eventually to balance the fairness and efficiency. So these two objectives can be combined and the controller is unnecessary.  The decentralized training is only for one agent. It is not clear how the agents can coordinate with the policies trained in a decentralized fashion to achieve fairness. 