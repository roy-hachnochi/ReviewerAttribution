=== Update after reading rebuttal and reviews ===  We thank the authors for their detailed and convincing rebuttal. They have adequately addressed my concerns about both the correctness of the experiments, and the novelty of their theoretical analysis.  However, I agree with reviewer 4 that the presentation of the results could be improved. In particular a discussion about DGPDC and details about the experiments should be added to the final version. Further, I agree with R4 that the title is misleading---it is not really a FW method, and is applicable in very specific settings (l1 and trace norm balls) and urge the authors to reconsider the current title.  Given the above, I will increase my score from a 4 to a 6.  # Summary  This work deals with generalized least squares problems with a regularizer, and an additional constraint which can either be an l1 ball or a trace norm ball. The main result of the authors is a primal-dual algorithm where they show that if the optimal dual solution is always s-sparse, then all of the primal updates can be relaxed to also be s-sparse thereby resulting in significant savings.  # Significance/Originality   My main concern is that this work seems to nearly directly follow from [1]. The current results follow from combining the proof techniques from [1] with the observation from [2] for the trace norm constraint. Otherwise, the experimental results demonstrate a superior performance of the proposed method but I have some major concerns about their validity.  # Concerns  Major:  1. Comparison with [1]  2. It is never explicitly stated in the paper that the dual optimal is assumed to be sparse. This is quite a strong assumption and needs to be made more clear.  3. For the rcv1, news20, and mnist experiments (Fig 1), Acc projected gradient descent is faster than SVRG. This seems suspicious since typically the latter outperforms the former. I suspect this may be because multiple threads (i.e. parallelism) is being used which gives an unfair advantage to more expensive methods (including the proposed primal-dual FW). I would re-run the experiments with a single thread to have a fairer outcome and believe the current results are misleading.  Minor  4. In Algorithms 1 and 2, step 6 involves greedily minimizing a quadratic function. This is no more a Frank-Wolfe method and is probably misleading to call it so.         [1]: Lei, Q., Yen, I.E., Wu, C., Dhillon, I.S. & Ravikumar, P.. (2017). Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization. Proceedings of the 34th International Conference on Machine Learning, in PMLR 70:2034-2042  [2]: Allen-Zhu, Zeyuan, et al. "Linear convergence of a frank-wolfe type algorithm over trace-norm balls." Advances in Neural Information Processing Systems. 2017.