The authors propose to solve a non-convex optimization problem with non-linear constraints, which is a common template for a variety of problems in machine learning. The authors solve the primal problem inexactly, controlled by error \eps_k, which is gradually decreased, as penalty \beta_k is increased and the iterates approach stationarity.  The inexact problem is solved with the help of both first order and second order solvers and this paper analyzes overall computational complexity of iALM under both kinds of substitutions, which is a first.  The paper is well written, with good references and presentation. The experimental analysis gives useful insights into the performances of iALM with both Accelerated Proximal Gradient Method, a first order approach and 1BFGS.