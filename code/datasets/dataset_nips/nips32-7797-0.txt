Originality: Many recent attempts and benchmarks have been proposed that look into uncertainty for deep learning models. However, the authors provide a sufficiently diverse suite of tasks, and not just a single task, strengthening their argument, due to transferability to more than one domain. Moreover, to the best of my knowledge, it is the first time that I see a large-scale empirical evaluation of uncertainty calibration methods (e.g. temperature scaling) to Bayesian deep learning methods (e.g. SVI and MC Dropout).  Quality & Clarity: The ideas are clearly communicated and the paper is carefully written. The experimental setup is described adequately and the results seem to align with our intuition.  Significance: The empirical results and motivation of this paper can be very impactful, especially for practitioners but also for researchers! In case of exact Bayesian inference methods (e.g. HMC), we wouldnâ€™t expect to observe a degradation of uncertainty as to the data distribution shifts, but due to the approximations in the variational methods (e.g. SVI, Dropout) this side-effect emerges! As a result, this paper highlights this and goes against the intuition many may had before deep neural network based models.