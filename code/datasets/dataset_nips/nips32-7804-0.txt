Summary =======  The authors propose to adapt the common definition of a model's *predictive confidence* in the sense of the PAC framework.  They introduce a formal definition of the mentioned adaption of predictive confidence, which generalizes classical confidence (1) by a notion of (meta) uncertainty and (2) to arbitrary error functions beyond the 0-1-Error.  It is proposed that predictive uncertainty is generated by 3 main factors, i.e., distributional, data, and  model uncertainty. Under this assumption a uncertainty model is introduced by formulating one choice of estimation for each of those 3 factors.  In an empiric study the authors aim at showing that (1) the proposed estimator outperforms current state of the art and (2) their factorization (distributional, data, and model uncertainty) is indeed close to the true situation.    Pros ====  1. The authors tackle an indeed important topic which needs further attention.   2. The general idea of integrating measures of predictive uncertainty into a PAC way of thinking seem promising.   3. It seems reasonable to factorize predictive uncertainty and address/estimate the factors independently and the presented experiments (partly) support the proposed factorization.  Cons ====  1. Although interesting, it is hard to follow the authors particular choice of predictive uncertainty factorization; the wording is suggesting that this choice is obvious but little motivation is given.  For example, the distinction between distributional and data uncertainty seems to be rather fuzzy and view point dependent.   2. One claimed key contribution of this work is a rigorous generalization of confidence to the PAC frame work and to arbitrary error functions. With this in mind, its introduction (Sec. 2) lacks mathematical precision. Some concerns:   2.1 There are implicit assumptions on Y, e.g., countability. If the authors claim to introduce a rigorous definition there must not be implicit assumptions.    2.2 In Eq. 0 (the unnumbered before Eq. 1) there seems to be an algebra defined on Y as the term f(x) - \hat{f}(x) is assumed to be defined.   This puzzles me as from the wording Y seems to be some sort of countable label space.    2.3 Eq. 1 seems to capture a triviality. What is the sense of this transformation?  From my perspective the common root of most of those issues is that the authors formally do not distinguish between the label space and the distributional space and use them interchangeably.  To be precise, for a finite label space the corresponding predictions, yielded by the model, are points on the unit simplex (here an algebra is defined).  The corresponding label prediction is commonly the argmax of the label-wise probabilities.  3. I do not understand how the proposed score is evaluated.  In Sec. 4 this is introduced but the last paragraph is not understandable to me.  It seems that first the obvious way for evaluation is introduced (Paragraph 1+2) but then this is somehow changed to address the shortcomings of the competitors, but it is not clear to me how this is changed.  Maybe a formula would help here.  In this situation I cannot interpret the experimental results.   4. The selection of models in Table 1 puzzles me.  For each model there should be a U, O, and W version.  Why are some of those omitted?  5. The experimental setup addressing *distributional uncertainty* seems a little insidious.  Given that MNIST is gray-valued and CIFAR100 is RGB it seems not difficult to succeed here.  This is also reflected in the experimental outcome which is alarmingly good and indicating that the experimental design is probably not appropriate. Alternatively one could test CIFAR100 vs CIFAR10.     