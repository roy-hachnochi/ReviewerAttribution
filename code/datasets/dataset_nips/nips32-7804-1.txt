1. Originality The novel formulation of model competence is conceptually simple and straightforward, yet original. Although the proposed method depends largely on the state-of-the-art anomaly detector (Lee et al. 2018) for modeling distributional uncertainty and the simple transfer classifiers for data uncertainty, the novel formulation of delta competence enables simple yet effective modeling of model uncertainty for any classifier with any error function. As a combination, the proposed method is original as well as conceptually appealing.   2. Clarity This paper is very well-organized and clearly written. Introduction and related work are sufficient for non-experts to easily absorb the background, motivation, and significance of the problem of interest. Mathematical equations and their derivations are very well complemented by verbal descriptions, making the paper highly readable and understandable.   3. Quality The quality of the paper is acceptable. Although the predictive uncertainty experiments (Sec. 6.3) were somewhat rudimentary and limited to proof-of-concept (showing the robustness of the ALICE score in several manipulated/ablated scenarios), the authors explicitly acknowledged this limitation and were careful enough about their claims (L206-208 and L223-226).   However, the calibration experiments (Sec. 6.3) were unsatisfactory. Interpretability is illustrated as an important aspect of the proposed method as suggested by its name (AL*I*CE), but it is rather misleading to claim that the ALICE score is well-calibrated and interpretable. Although L114-119 properly add caveats, it was not adequately explained what “an interpretable probability score with inherent meaning” (L119) actually means. Both terms “calibration” and “interpretability” were overgenerously exploited. The calibration of the ALICE score against delta is not what most readers would expect from the term “calibration,” and the claimed interpretability is quite distant from what most readers would expect from the term “human interpretable.” In order for the ALICE score to be maximally practically effective, it has to be well-calibrated (or at least calibratable) against class conditional probabilities (e.g. Fig. 1 of Guo et al. 2017).  4. Significance This paper seems to be fairly significant. I would expect the present work could be a good reference point for other researchers to further develop the core ideas presented in the paper, such as considering all aspects of predictive uncertainty (distributional, model, and data) and developing practical methods with generality and scalability.  Arguably, simplicity of the proposed method and exceptional clarity of the paper add additional significance to this paper, increasing the chance that this paper is widely read and being motivational.  Minor comments - Why is the maximum value of the cross-entropy error function 0.2 (L217 and Fig. 3)?  - In Fig. 2, it is unclear what exactly “inverse true error” means.   - Was there *any* previous attempt to consider all aspects of predictive uncertainty (distributional, model, and data uncertainty)? L39-41 is unclear whether there exists *any* line of research.