I believe that the core algorithmic idea that is introduced in this paper makes sense, and the results indicate that the method could be useful to evaluate confidence of classification models. I have some reservation however with respect to the quality of presentation, and I am not convinced that the paper is ready for publication in this form.  I don't agree that mentioning PAC learning is necessary. The authors attach a name (i.e., model competence) to a particular, approximate confidence interval. If the authors want to refer to PAC learning, they should have some bounds that would include the sample size. This is missing in the paper, and for this reason I don't think that these methods should be put in the context of PAC learning.  Line 26 says "it is not clear whether performance on a held aside test set is indicative of real-world performance ...", but the authors don't address this problem in the paper. It seems that the introduction does not match the subsequent sections. Specifically, the paragraph that is above Eq. 6 says "we cannot know whether the model will be competent on out-of-distribution test points". The arguments and statements are not coherent.  There is something wrong with Eq. 1. I don't think that line 2 should start with "=".  The explanation of randomness in Definition 3 is not clear to me. Isn't it the case that randomness comes from the limited training data, and every time we sample a new training dataset, we can get a slightly different set of data points? Also, I don't think that we can say that f is a variable. f is a function.  Line 116 says "we simply seek to determine whether or not the classifier is competent on a point.". The authors should be careful with such statements because researchers who study interpretability would disagree with such views. See this paper:  Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and use Interpretable Models Instead, Nature Machine Intelligence. Cynthia Rudin  "pointwise binary competence classification" should be explained before it is used in line 138  "pointwise rankings" in line 150 should be explained.  Most of the section 5 looks good to me, and it presents and interesting heuristic approach that leads to promising results. However, as long as section 5.1 clearly describes the method, the method proposed by [16] evaluates the property of the dataset. So, this method could potentially be used to evaluate any model because it evaluates the data, i.e., it estimates the probability that a data point is in-distribution. This evaluation does not depend on any model, and all models can equally benefit from the potential of this method.  Section 5.2 is not clear to me. The authors say "we directly compute \mathcal{E}...". Does it mean that reclassification accuracy is used here? If so, on which data, training, validation or testing?  What are the "transfer classifiers" in line 204?  It is very suspicious that the ALICE scores are used on the penultimate layers. This design choice should be carefully explained using theoretical arguments or empirical evidence.  What is the role of softmax in line 246? What is it used for? It's use should be explained.