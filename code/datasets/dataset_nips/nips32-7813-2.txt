First I want to say that I like the problem setting and the general approach. Also I do not have a single main concern, but I have several non-negligible ones that, when piled up, determined my final score. In what follows, I’ll order my concerns/questions from more important ones to minor comments.  I felt that the main result, which is given in Theorem 2, was really hard to interpret. There are way too many terms, making their combination in the final result very hard to interpret. I found the discussion following the theorem very helpful, but it helped me understand how these terms occurred in the proof, rather than what their practical meaning is. I am happy seeing such a theorem if it is followed by multiple corollaries, given by different parameter instantiations, showing when the bound is tight and how the obtained complexity compares to prior work. Some comparison with the result of [4] is given, although in the fully general setting; it would be interesting to see settings when this difference is significant, and when it is not. For example, I liked the “One Dimensional Thresholds” example. Perhaps this is a matter of taste, but when I reached the last paragraph, I thought the paper ended too early; I was expecting more such examples to follow.   Another concern is that there have been multiple works on different bandit approaches to multiple testing (not all of which test for whether the mean of a distribution is zero, as stated in the paper). Some related papers that weren’t discussed include, for example, Optimal Testing in the Experiment-rich Regime by Schmit et al. (AISTATS 2019), A framework for Multi-A(rmed)/B(andit) testing with online FDR control by Yang et al. (NeurIPS 2017), etc. Moreover, these papers focus on sample complexity just like the current paper. Their setting is different, but the papers are similar enough that they deserve a brief discussion. Citing only 3 papers in the “Multiple Hypothesis Testing” paragraph makes me think that the connections to prior work in this area have not been fully explored.  If I understand correctly, the proposed algorithm is inspired by algorithms in related work (e.g. [4, 10]), but a fundamentally different idea is sampling in the symmetric difference, which has not been exploited so far?  Further, I found the writing very messy in some parts, and it took a lot of re-reading to go over some definitions and arguments. For example, A_k and T_k are introduced without stating what k is. At the end of page 4, there is a sentence that says “the number of samples that land in T_k”, even though T_k is not a set of samples. In the same paragraph, mu-hats are used without being previously defined. The paragraph assumes I_t is uniformly distributed on [n], even though that is not stated beforehand. In the definition of TPR, eta_[n] is used, even though it is never defined. I understand that submissions are thoroughly revised before publication, but these kinds of mistakes make reviewing harder than it needs to be.  I don’t know if this is necessary for the proof or not, but I didn’t see why the mu-hat difference is unbiased, as stated at the bottom of page 4. Especially if the formal result hinges on this fact, I would appreciate a formal argument explaining why this is the case. The rejection sampling strategy is essentially equivalent to the following: conditioned on “past information”, sample uniformly from the set T_k. This couples the old and new samples, making it not so obvious to me that the mu-hat difference is unbiased. Related to this point, I didn’t understand the part that says that the number of samples that land in T_k follow a geometric distribution. I agree that the wait time until you observe a selection in T_k is a geometric random variable.  Relatively minor comments regarding style: 1. It is incorrect to say that Y_{I_t,t} is iid, as written at the beginning of page 3; iid is an assumption that refers to a set of random variables. This sentence needs to be rephrased more formally. 2. I was confused by the sentence “Instead of considering all possible subsets of [n], we will restrict ourselves to a finite class … .” The set of all subsets of [n] is finite anyway. 3. There is a minus sign missing when introducing mu_i in the risk equation on page 4. Either way, I do not see the purpose of introducing the risk. The paper makes it clear that the goal is to maximize TPR given an FDR constraint, as opposed to minimizing risk. Much of Section 1.1. seems like an unnecessary distraction. 4. There are some typos that need revision, like at the beginning of Section 3.1 where it says “specific specific noise models”, or in the Remark on page 5 there should be R_{i,t}. 5. The Benjamini-Hochberg paper should probably be cited, given that the used error metric is FDR, which stems from that paper.  After rebuttal: The authors have clarified some of my concerns, and have promised to improve clarity of presentation. The other reviews have also convinced me about the originality of this submission, so I am increasing my score.