This paper proposes a distributional policy optimization (DPO) framework and its practical implementation, generative actor-critic (GAC) that belongs to off-policy actor-critic methods.  Policy gradient methods, which are currently dominant in continuous control problems, are prone to local optima, thus it is valuable to propose a method addressing that problem fundamentally.  Overall, the paper is well written and the proposed algorithm seems novel and sound. - In Algorithm 1, it is not clear what (s,a) and s is, for Q(s,a) and V(s). Does it stand for 'every' state-action pair and state, or the state-action pairs that are visited by the current policy \pi_k'? If it corresponds to the latter, it seems that DPO would possibly not converge to the global optima. For example, suppose that the the initial policy is given as the Dirac delta distribution \pi_0(a|s) = \delta_{0}(a), and the (deterministic) transition function is defined as f(s,a) = s + a. Then, with the \pi_0, only the initial state can be visited, thus the value functions and the policy will remain the same and not be updated. Assumptions about behavior policy are not mentioned in the paper. - L109: can can -> can - In L114, what does 'as \pi is updated on the fast timescale, it can be optimized using supervised learning techniques' mean specifically? Please elaborate on the relationship between supervised-learning and fast timescale. - In DPO the delayed policy \pi' is updated by the convex combination of two 'probability distributions', while in GAC the delayed actor is updated by a convex combination of two 'parameters' of each probability distribution. Therefore, DPO and GAC are not perfectly aligned. - For the actor, why did you choose to adopt implicit quantile networks, though GAC does not require 'quantile' estimation? It seems that conditional GAN or conditional VAE could also be possible. 