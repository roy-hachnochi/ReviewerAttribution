The idea proposed in the paper is very interesting. Policy gradient methods are very popular nowadays and this paper propose a method to approach one of their weakness. The paper is clearly written and figures are helpful for the reader. The DPO procedure would have benefited however to be a bit clearer: for example, why is the delayed actor present in this general procedure? It seems to me mainly helpful to help convergence and does not seem an requirement for the method. Besides, although valid theoretically, a three time scale algorithm seems hard to do in practice.  #Post-rebuttal update" I appreciate the efforts the authors put into their rebuttal. I will however keep my score as 7, as I vote for accepting this submission, but would not be upset if it was rejected (clarity of the paper could benefit from a restructuration and I am not a huge fan of the 3-time scale procedure, as it should be quite hard to find the right parameters to make it converge).