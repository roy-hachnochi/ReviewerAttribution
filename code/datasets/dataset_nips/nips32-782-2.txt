This paper presents the limitations of policy gradient-based methods which need the explicit p.d.f of action in continuous control, and gives the proof that the Gaussian distribution strategy cannot converge to the optimal under some conditions. Then it introduces the DPO framework that can converge to an optimal solution without the requirement of the underlying p.d.f and thus without the limitation of parametric distribution. Also, the paper presents a practical algorithm GAC that applies Quantile Regression and Autoregressive Implicit Quantile Networks which can represent arbitrarily distributions. GAC achieves good results in continuous control tasks and some are better compared to the policy gradient baselines, and it has the same efficiency but requires more computation.  Minor issues:  The description of ‘the sub-optimality of uni-modal policies’ (from line 73) is confusing. Which part of figure 1a corresponds to ‘the predefined set of policies’? In line 76, ‘this set is convex in the parameter µ’ seems to mean ‘this set is convex in the parameter space Θ’, and what does ‘it is not convex in the set Π’ mean? What is the definition of ‘(1−α)δ µ 1 +αδ µ 2’? The condition at the end of line 96 seems not written properly. Should the right side of the equation in line 99 (also 442) be 1-2ε and ε<1/3?