After reading all the reviews and the rebuttal, I still think that it is a good paper. The authors have also addressed my concern about the running time of the algorithm.  The paper considers reinforcement learning problems where the goal is to find a policy such that an expected measurement vector satisfies some fixed convex constraints. In the RL setting, this problem can be seen as an approachability problem. Therefore, after formulating it as a two-player game, the authors propose to solve it with a no-regret learning algorithm. Regrets bound are also derived from existing ones in online convex optimization.  In the reproducibility checklist, the authors state that a link to a downloadable source code is provided, however I have not found any in the paper or the supplemental material.  * Originality  Recently several works have considered constrained RL problems. This paper tackles a more general framework than those previous works by accepting any convex constraints. The reformulation of the overall problem as a two-player game is interesting and relatively novel, although it is not surprising for someone familiar with online convex optimization (see Chap. 8 of  "Introduction to Online Convex Optimization" by Hazan).  I think the authors should compare their approach with the following work:  @article{KalathilBorkarJain14,  Author = {D. Kalathil and V.S. Borkar and R. Jain},  Journal = {Arxiv},  Title = {A Learning Scheme for {Blackwell's} Approachability in {MDPs} and {Stackelberg} Stochastic Games},  Year = {2014}}  * Quality  The theoretical results seem to be correct. The overall game-theoretic formulation and the proof techniques are based on results from online convex optimization (regret bounds) and from Abernethy et al. (distance as max of linear function, cone from convex set).  It seems to me that some previous work (e.g., RCPO) on constrained reinforcement learning could have been easily extended to deal with convex constraints. It would have been nice to have some discussion about this.  The experiments are a bit limited (only one small domain and one baseline). I think it would have been interesting to compare with other concurrent methods for constrained reinforcement learning (e.g., CPO or the work by Kalathil et al.).   In Fig. 1, the plots for APPROPO end earlier than the other ones. I guess that the authors have not finished running them at the submission time. This is probably due to the fact that the proposed method is quite computationally heavy, I think some comments about the running times of APROPO and RCPO should be provided in Section 4.   * Clarity  The paper is well-written and clear.  * Significance  The advantages of the method is that it is a general scheme that can accept more general convex constraints and also it has some performance guarantees. The drawbacks of the approach is that the obtained solution is a mixed policy, which is not very convenient to apply in practice and also the computational costs of the method. In my opinion, these make APROPO more of a theoretical proposition, which may perhaps pave the way for more practical and efficient algorithms in the future.   Some typos: - l.149: Z(\mu) is used for the first time here. Should it be \bar z(\mu)? - l.152: "This means ..." is only true if the problem is feasible? - l.155: must to implement - l.181: apprimately - l.260: we previous  