This paper considers the problem of sampling from an unaggregated dataset in the form of key-value (x,v) pairs. Let nu_x be the frequency of the key 'x' in the dataset and let f(.) be any concave sublinear function. A typical problem considered in this paper is that of estimating \sum_{x \in H} f(nu_x), for a subset of keys H. If one had access to the aggregated data, then the optimal way is to sample key 'x' with probability proportional to f(nu_x). When f(.) is an identity function, PPSWOR is a randomized algorithm using exponential random variables that achieves this even on unaggregated datasets. PPSWOR can be considered as optimal sampling procedure in this sense for any f(.) if applied to aggregated data. This paper considers all concave sublinear f and unaggregated datasets.  The full algorithm requires two passes over the data. For the first pass, authors give a composable sketching algorithm to sample keys.  Once a sample of keys is generated, authors do a second pass over the data to compute the associated estimators. They show that the variance of the estimator (of f(\nu_x)) so produced is at most a constant factor worse than that of PPSWOR applied to aggregated dataset. In addition, the space required by the algorithm is of the same order as PPSWOR in expectation.   Any concave sublinear function can be written as an integral over 0 to oo, and following earlier papers, the authors write this as a sum of two integrals. The authors then give two separate sampling procedures to handle each part, and give a merging procedure to combine the two samples. The first summand is approximately equal to scaled frequency of the keys, and so PPSWOR can be used to produce a sample for this part. The second summand is handled by a procedure that authors call SumMax. SumMax (Section 5) is a procedure that samples (primary part of the) keys according to a sum of max values.   I find the result to be interesting and potentially useful. The techniques are interesting. One thing that was not clear to me was which part was technically the most interesting/novel contribution of this paper given [9] (and possibly other papers). One main complaint I have is that the conference version of the paper does a very poor job explaining the various parts of the algorithm. Section 3 has some explanation of where other sections fit into the overall scheme. But Section 4 and Section 5 do not fit in to the narrative and there is no/very little explanation of why these are needed for the problem. The supplementary portion is a complete paper in itself and is well written. But it was very unclear to me why stochastic version of PPSWOR (section 4) and SumMax sampling (section 5) were required before looking into the supplementary section.    --Post Author Feedback--- After looking at other reviews and authors' feedback, I will keep my score unchanged (or probably raise my overall score by 0.5 points). I appreciate the clarity of the feedback, and I am satisfied with their answers. 