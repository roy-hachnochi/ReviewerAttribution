Notwithstanding the significant contributions of the paper discussed above, there is perhaps too much focus on overall performance metrics and too little scientific investigation of how BERT and brain data relate to each other. How do representations in the pre-trained BERT model and the brain-fine-tuned BERT model encode linguistic information, and what is gained by fine-tuning? It would be interesting to compare the two models to try and understand what brain-relevant linguistic information is absent from the original BERT instantiation.   It is arguably a bit tautological to argue that “fine-tuning models to predict recordings of brain activity … will lead to representations that encode more brain-activity relevant language information” (Abstract lines 6-7; Also lines 143-144). Isn’t the more interesting question about what commonalities exist between brain representations of linguistic information and BERT representations of linguistic information? What kind of language information is “brain-activity relevant language information”? For example, emotional sentiment may be a particularly important aspect of the brain data, but not a very salient component of the representations in BERT, though sentiment may be decodable from BERTs embeddings (and thus can be magnified by the brain fine tuning process).    Lines 120-121: 20 words are used as the relevant context for each fMRI image, but these 20 words do not respect sentence boundaries. Does this mean that the input to BERT during fine-tuning does not respect sentence boundaries? If so, this seems undesirable - - it introduces a discrepancy between the format of the input in the initial BERT training and the format of the input in the current fine-tuning. Therefore, we cannot be sure whether changes to BERTs linguistic representations as a result of fine-tuning are a result of the neurocogntive signal relating to the input, or to the new input scheme (e.g. when it comes to the GLUE evaluations). 