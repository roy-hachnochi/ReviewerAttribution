I thank the author's for their response. I would like to reemphasize that I like this paper a lot, and applaud the authors for their work. I strongly suggest an oral accept for this paper. Additionally, I've raised my score 1 point.  However, I still felt that Section 3.1 was somewhat contrived, and stand by my initial criticism of this section. While it is an interesting example, and does demonstrate a particular problem that can emerge when the training metric differs from the performance metric, I still did not feel that it exactly demonstrated the point the authors were trying to make.  ______________ Overall, I really like this paper. The authors introduce an evaluate a novel hypothesis (related to action-gaps) and they propose a natural remedy. The paper is very well written, and each topic is clearly presented and explained in a logical order. Overall, the points the authors make are convincing.  Its worth noting that this work is particularly relevant in sparse-reward problems. In this setting, the advantage of store the value function in a logarthmic space is particularly clear, as it becomes feasible to learn the difference between the expected value of a single reward received in the future without becoming swamped by rewards received in the near-term. Sparse-reward problems are currently popular, so addressing a problem in this space seems valuable.  While the authors also provide the standard Atari evaluations, they perhaps more importantly provide direct evidence for their hypothesis, while refuting the "action gap" hypothesis. This was a nice inclusion, due to the well-known problems with direct evaluation of methods in the field.  There were some parts of the paper that were not totally convincing. I took some issue with the examples given in Figure 1, in particular with respect to the use of a finite horizon. Typically, the fixed horizon is applied during both training time and test time. In the example given by the author, the agent is allowed to run until it reaches a terminal state during training time, but during test time is cut off after h timesteps. The authors contrived an example wherein this results in an agent taking a path that is too long in some settings, meaning that the agent fails to reach the goal when larger discount factors are used. I think the purpose was to motivate the use of smaller discount factors in certain settings, but I did not find that this example really captures the reasons why they are useful. However, this wasn't a huge part of the paper, and thinking about the examples raised by the authors led to some useful insight that helped with understanding the rest of the paper even though I did not find the particular example convincing.  The authors chose to apply their technique to generate a particular variant of Q-learning. It seems to me that value methods have fallen out of favor relative to policy gradient methods, but it is perfectly clear how to generalize the author's approach. Perhaps it would be worth a mention by the authors? The approach of splitting the rewards into positive and negative rewards is a litle hack-ish, but its not unreasonable in most problems, in particular with respect to sparse rewards problem. Their approach was marginally better than DQN on most Atari games, but then again almost everything beats vanilla DQN. Nevertheless, the comparison was fair. That being said, it would be nice to see some demonstrations where the advantage of the author's method was clear and convincing, even in a contrived setting.  Overall, in spite of a few minor minor issues the paper was well-written, discussed a relevent problem, and introduced a novel fix. I would argue for the acceptance of this paper.