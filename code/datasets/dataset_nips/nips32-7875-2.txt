The authors provided thorough and detailed analysis of the discount factor's impact on the reinforcement learning process. Building upon their empirical observation, they contributed to a new metric, the standard deviation of the action-gap over sampled states, that better explained the impact of smaller discount factor on the optimization processes compared to previous metrics (smaller action gap or smaller relative action gap).  They also proposed a principled logarithmic Q-Learning variant that allows lower discount factor in reinforcement learning optimization process by mapping the values to a logarithmic space and performing updates in the logarithmic space. They empirically show (1) that the proposed algorithm is able to mitigate the issues caused by small discount factor in regular Q-Learning and (2) that the proposed algorithm could also outperform DQN on six Atari games (namely Asterix, Breakout, Zaxxon, DoubleDunk, FishingBeDerby and Tennis). The paper is well-written and easy to follow.     Some comments: -- (1) One question about the curves of A and B in Figure 1: shouldn't the performance change suddenly at a certain gamma value (for example around 0.725 for A)? But the plotted curves somehow look like ``smoothed''. Please let me know if I missed something.  ==> Addressed in the authors' rebuttal. It is an artifact of their plot procedure. Will be updated in the final version.   -- (2) Just out of curiosity, what would the learning curves look like for DQN with gamma=0.96? Also is logDQN more robust compared to the DQN baseline for different gamma values (in the investigated interval [0.84, 0.99])? It is of great interest to see the performance as a function of different gamma values for logDQN and DQN.  ==> The authors made a good point that the interplay of different hyper-parameters of DQN makes this unrealistic.   -- (3) Since the logDQN also changed the network architecture by adding more output units, the empirical result would be more significant if comparison against Dueling DQN [1] is provided. It is not necessary but it can make the results stronger.    -- (4) Some previous work also discussed the usefulness of a smaller discount factor in planning setting when model errors are presented [2]. The studied problem is not the same but I think it is relevant to the paper in a sense that both papers provide some insights into the problem of selecting a gamma value for best evaluation performance.  -- (5) minor typos: line265, movies  [1] Wang, Ziyu, et al. "Dueling network architectures for deep reinforcement learning." arXiv preprint arXiv:1511.06581(2015). [2] Jiang, N., Kulesza, A., Singh, S., & Lewis, R. (2015, May). The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems (pp. 1181-1189). International Foundation for Autonomous Agents and Multiagent Systems.