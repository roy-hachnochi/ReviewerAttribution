In this paper, the authors study the recovering bandit problem, where the expected reward of arms varies according to some unknown function of the elapsed time since it was played. This is a significant problem, which occurs in practice for recommendation or advertising, where to vary the proposed item or ad during time is relevant. This problem is very challenging from an algorithmic point of view, since in addition of handling a multi-armed bandit problem, one must learn the recovery functions. Moreover, as the rewards of arms change during time, maximizing the instantaneous regret is not optimal. Here, the authors propose to maximize the rewards of the best sequence of actions.  Three definition of the regret are proposed: the instantaneous regret, the full horizon regret, and the d-step Lookahead Regret. Instead choosing the full horizon regret, which compete against the best deterministic policy which knows the recovery functions, the authors focus on the d-step Lookhead Regret, which leads to tractable algorithms.  Proposition 1 states that any algorithm with a low d-step Lookhead Regret can also have a low full horizon regret when d is not too small. For estimating the recovery function the authors use Gaussian Process. The authors propose two algorithms: a fully Bayesian approach which uses Thompson Sampling to sample the leaf of the lookhead tree (a sequence of d arms equipped with d recovery functions), and a hybrid approach which uses a UCB like algorithm to select the best leaf. The main drawback of the d-step lookhead strategy is that this approach does not scale with d. To improve the computational efficiency, the authors propose to use optimistic planning in section 6. The algorithms are analyzed in the case of single play lookhead, where an arm can be played only once during d steps, and multiple play lookhead.  In the case of single play lookhead, the upper bound of regret are in O(sqrt (KT \gamma_T log (TK|Z|)), that is surprising since it does not depend on the depth of the lookhead d. In fact, \gamma_T has a strong dependence in d. For instance for squared exponential kernels \gamma_T = O((log T)^(d+1). The reviewer thinks that the dependence in d of \gamma should be explicit. Moreover, the upper bound of the instantaneous regret is the same than the upper bound of d-step lookhead regret (Theorem 2,3 vs Corollary 7). This suggests that the bounds in Corollary 7 are not tight, or that the dependence in d is not the same.  Could the authors clarify this point?  The experiments done on synthetic problems show the efficiency of the approach. Overall, this is an interesting paper on a significant and challenging problem.  There are some shortcomings or ways of improvement: - The setting is so general that it could be interesting to position it with respect to reinforcement learning with discounted rewards.  - The dependence in d of the regret bounds should be clarified.  ____________________________________________________________________ I read the rebuttal. Thanks for answering my concern. I raised my score. I recommend acceptation.