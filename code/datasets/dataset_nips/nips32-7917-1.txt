The paper promises a principled solution to account for correlation in discrete state-action domains based on Poly-Gamma augmentation. The framework is empirically evaluated in three different settings: imitation learning, system identification & Bayesian RL. In each of these settings, the proposed method achieves a significant performance improvement over a correlation-agnostic baseline.   Adding useful priors to RL tasks is highly non-trivial and can boost performance significantly. Hence, I was very interested in reading this paper. While the finite-state assumption is fairly restrictive, the generality of the framework, demonstrated by the different settings the method is tested on makes this potentially an interesting method to consider.  That being said, what reduces the value of the paper considerably in my view is that it is insufficiently clear how useful the (family of) bias(es) is that can be encoded by this approach. The experiments mainly revolve around a very simple grid-based task. What if the example contains walls and the task becomes more of a navigation task: would the prior still be useful?   It is possible that I have misunderstood something, so I will carefully read the author rebuttal and update my score accordingly.   ==================================== The author rebuttal was helpful, although it did not fully resolve my concerns. Specifically, this sentence "the proposed methodology can be beneficial in all scenarios that contain some form of structure" is a little troublesome to me. What does "can" mean in this context? It doesn't clarify in which situations it will be  beneficial and in which it won't. If the suggestion is that it will be beneficial in all situations that have structure, I'm not sure that I belief that.   I still consider this a borderline paper, but will upgrade my score to a 6.