The paper proposes a method to model environment correlations during learning in discrete Markov decision processes. The framework applies variational inference in a hierarchical Bayesian framework. The proposed method is demonstrated on some small experiments in imitation learning, model learning and bayesian reinforcement learning settings.  -The derivations in the paper seem to mostly be an application of the methods developed in reference [12] and standard methods from variational inference. The main novelty seems to be the application to MDP settings. -The paper gives a clear step-by-step derivation of method. -The paper provides a series of simple, but clear experiments to demonstrate the benefits of the method.  -2 of the example experiments are small toy grid worlds, the last experiment is a slightly more involved queueing example.These experiments show benefits of the method in different settings and demonstrate that the proposed method outperforms a naive baseline. While the queuing example might be more difficult, all experiments seem limited to small scale toy domains. -For the related work, it would be nice to also contrast the approach in this paper with the work by Poupart on including prior knowledge in discrete Bayesian RL using mixtures of Dirichlets.  *Poupart, Pascal, et al. An analytic solution to discrete Bayesian reinforcement learning. ICML, 2006. *Pavlov & Poupart. Towards global reinforcement learning. 2008 