Content: In this work, they design an efficient pure differentially private algorithm for covariance matrix estimation. Their empirical results show that their algorithm outperforms several other algorithms in the literature in practice. Their theoretical results imply that their algorithm outperforms simple noise addition in the high privacy/small dataset setting.   They explicitly consider the sampling procedure for implementing the exponential mechanism in their setting. I appreciated this part of the paper since this is often swept under the rug. The highlight the problems associated to relying on Gibbs sampling and instead use a rejection sampling scheme proposed by Kent et al.   Originality: The authors’ highlight several ways that their work differs from previous work. Previous work seems to focus on either approximate DP or sparse matrices. In contrast, this paper focuses on pure DP and attempts to estimate the entire covariance matrix with no sparsity assumption. The closest work seems to be Kapralov and Talwar, who (as far as I understand) also use an exponential mechanism and peeling technique. This work uses a different peeling technique to that work, when a new eigenvector is chosen, the authors’ project onto the orthogonal space rather than subtracting the new component of the eigendecomposition as in KT. The authors’ claim that this change allows them to implement a more efficient sampling mechanism. They also show in Corollary 2 that it can result in a more accurate estimate.  I don’t know enough about the DP covariance estimation literature to say if the algorithms they tested against represent a full spectrum of DP covariance estimations algorithms. Their algorithm seems to   The claim that this is the “first comparative empirical evaluation of different private regression methods” seems incorrect. Their are several others including, but not limited to, “Revisiting differentially private linear regression: optimal and adaptive prediction & estimation in unbounded domain” by Yu-Xiang Wang which also compares several linear regression methods on datasets from the UCI repository. The authors’ are not claiming that their algorithm should outperform all other algorithms in the linear regression setting since it is not tailored to this purpose (it seems inevitable that releasing a general purpose statistic will come at some cost), although it would be interesting to see how it compares to the more promising algorithms presented by Wang.  Quality: The proofs seem complete (although I didn’t read the appendix in detail).  I appreciate that they benchmarked against other algorithms on three different datasets. It would be nice to have some description of the properties of these datasets and why they were chosen. Are they datasets that differ significantly enough to capture a wide range of behaviors of the algorithm?  Clarity: The paper is well-written (bar a few typos) and easy to follow. The main algorithm is clearly presented. The experiments are clearly described.   A minor comment since it is in the appendix, but I would like to see them compare the linear regression algorithm to a few more benchmark algorithms. The algorithms they have don’t seem to capture the full range of DP linear regression algorithms that currently appear in the literature.   Significance: Given the popularity of linear regression and PCA, better algorithms for performing these tasks are an important research direction. Many of the DP algorithms for these simple data analysis tasks have focuses on the big data/ low privacy regimes. However, since there is a lot of research in the social science world that works with small datasets, it is important to develop careful algorithms that perform well for these datasets. The authors’ make good progress in this direction. In this vein, as more practitioners implement DP mechanisms, it is important to have efficient implementations of DP algorithms and explore the performance of these algorithms on real datasets.  The different mechanisms perform quite differently on the three different datasets. The AD algorithms is the best almost across the board but the shape of the performance curves varies significantly. This phenomenon, that the data structure plays a significant role in the comparison between algorithms, also occurs in DP linear regression. I’d be interested to see further work attempting to clarify the conditions of the data which are necessary for AD to be the optimal algorithm. For example, what do the authors’ conjecture is happening in the wine data set with eps=0.01?   Minor comments:  - Typo: introduction, eps, delta-> (eps, delta) - In line 14, “attack vectors” is an uncommon term - The quote “These algorithms typically train directly on the raw data… standard off-the-shelf algorithms for learning” in lines 20-23 doesn’t feel strictly true. I think the author is trying to make the point that sometimes it is useful to release summary statistics (I.e. synthetic data) that can be used for a wide variety of data analysis purposes. This is indeed a valuable approach, however it also has pitfalls which make me wary of calling it a “more general alternative approach”. - I’m unsure what the authors’ mean by optimal in line 58. They derive a better than trivial way to allocate the privacy budget to each of the eigenvectors, although it’s not clear to me that this is optimal. - The notation x(-t) is confusing since it is of a different type to x(t). In general, the presentation of the notation for linear regression seems more confusing than it needs to be. - If the data truly is sparse, do the authors’ conjecture that one should use a technique like Chaudhuri et al. or Wang and Xu, that is tailored to sparse data? - Overloaded notation: \tilde{X} is used in both line 83 and line 134 for different purposes. - Typo? In Algorithm 1, should it be \hat{theta_i) = P_i^T\hat{u_i}? - Typo: Algorithm 1: ortohonormal-> orthonormal - Overloaded notation: delta is used as a privacy parameter and also in Lemma 5 as a failure probability. Perhaps use beta instead for the failure probability. - I’m a little confused by the comments in lines 279-281 regarding the expected number of rejections. Why is it the case that the observed number of samples is much greater than the bound given in Kent et al., do the conditions not hold? - The graphs are quite small. This is understandable in the main paper since space is limited but it would be nice to have them bigger in the appendix.