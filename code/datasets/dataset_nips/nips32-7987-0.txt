This paper defines a weighting scheme that allows for the isolation of outlier points that might bias principal subspace detection, even in the context of using Robust PCA estimators. The main idea is that when columns of the data matrix fall completely outside of the main subspace spanned the the other columns, this can be detected to allow for the "sparse" part of robust PCA to contain complete columns instead of completely random locations anywhere in the data matrix. The model itself (and to some extent the idea of weighting) is similar in spirit to this un-cited paper:    - A.S. Charles, A. Ahmed, A. Joshi, S. Conover, C. Turnes, and M.A. Davenport, Cleaning up toxic waste: Removing nefarious contributions to recommendation systems. Proceedings of the ICASSP Vancouver, Canada, May 2013.  however the authors provide some conditions on correctness for their algorithm, which is different than the re-weighted l1 scheme used in the citation above. Overall I thought this paper was good. One piece that was unclear to me was how the innovations coefficients in Figure 1 were so well behaved. There was a large amount of variation in the single vector (A^Tc) calculations in the left two panels of Figure 1, however the variation was not as large in the right panel. I did not see where any averaging over coefficients calculated using different c vectors was done, which I assume would be needed.   A small comment in "Data Model 1": there is a comma missing in [B (A+N)]T --> should be [B, (A+N)]T.   I would recommend that the authors change the name of the algorithm. iSearch is used a lot for different search engines, and a more unique name would help. Perhaps Innovations for Nixing Outliers (In N' Out)? Maybe that one isn't good, but the idea remains.