Update: I have carefully read the authors' rebuttal.  I have raised by score to 6 from 5 to reflect their clarification about Figure 3 and Table 6.  It still seems that the speedups of the current formulation are often not of great practical significance, except for the language model which was able to give 2x wall clock speedup.  As another reviewer noted, it is disappointing that the overall training time is not reported in the main paper, instead of the average batch time, as that makes it unclear whether latency times and other overheads between batches might be a significant concern.  The author rebuttal notes that Appendix C shows time-to-accuracy, which would be good to mention in the main paper. But those results still appear mixed: for CIFAR10 SGD beats Rank1 and seems only competitive with Ranks 2,4, whereas for Language model all Ranks seem to convincingly beat SGD.  So it's not often this approach in current form will produce significant improvements for a given new task.  Overall this approach appears promising but lacks any significant theoretical support. Furthermore, the experiments offered mainly provide an existence proof that some wall-clock speed up is achieved using gradient compression, without putting the these results in a significant context (e.g. what are limitations, such as when the single step approximation of the power iteration fails and gives worse test accuracy?).  PowerSGD reduces gradient communication significantly in some experiments, but the net speedup is not proportional (e.g.for LSTM experiment, 10x reduction only yields 2x speedup).  However, even that claimed improvement seems misleading. Figure 3 shows that using modern NCCL, nearly 8x speedup on 16 GPUS across 8 machines is achieved by not only (Rank-2) PowerSGD, but also SGD and Signum.  Thus, tables 6 and 7 seem misleading, because although they seem to indicate significant practical speedups (e.g. 134 ms / epoch for PowerSGD versus 300ms for SGD for language model LSTM), that actually seems to be the case only when using the slow GLOO backend, where SGD and PowerSGD have significant differences as shown in Figure 3, and not when the modern, commonly available, NCCL is used for all-reduce.  If that is the case, then this approach is not clearly of much practical significant as is for modern practiioners (who typically do use NCCL).  So, the significance of the contributions of this work are unclear, especially given the lack of theoretical justifications and limited scope of the experimental work.  Algorithm 1 (rank-r PowerSGD compression) is presented without justification.  It is not clear from the experiments when this works well or fails, or what existing work this is based on (there are no citations offered). 