Updated review: After carefully reading the rebuttal, part of my concerns are addressed as the authors claim they will extend the experimental study to a larger scale. However, I don't wish to change my overall score.   I agree with Reviewer 1 and Reviewer 3 that providing convergence analysis will strengthen this work, but it may take time. Moreover, I insist on my suggestion that  i) the authors should consider studying PowerSGD under the setting that global batch size is fixed since this will lead to more promising speedups. ii) end-to-end speedups need to be clearly illustrated e.g. adding one table that indicates speedups of PowerSGD comparing to various baselines on converging to certain accuracies.  -------------------------------------------------------------------------------------------------  This paper proposed PowerSGD, a low-rank based gradient compression algorithm for speeding up distributed training. Compared to the previously proposed method ATOMO [1], PowerSGD avoids the computationally expensive SVD step in ATOMO to attain better scalability. PowerSGD is a biased gradient compression technique, which makes it hard to conduct solid theoretical analysis. Moreover, PowerSGD is the first gradient compression algorithm that is compatible with all-reduce based distributed applications compared to most of the previously proposed communication efficient distributed training algorithms. The paper is very well written and well motivated. Extensive experimental results indicate that PowerSGD has good scalability and end-to-end convergence performance.   [1] https://papers.nips.cc/paper/8191-atomo-communication-efficient-learning-via-atomic-sparsification