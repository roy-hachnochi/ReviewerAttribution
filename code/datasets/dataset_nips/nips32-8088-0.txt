In this paper, the authors focus on the problem preventing attribution attacks (small input modifications which don't change the prediction but change the attribution).  They do this by exploring two different framings of robust optimization that consider the Path Integrated Gradients, (P)IGs, for examples near the original example.  Most interesting to me (although not an expert in this subfield), is the degree to which the paper connects robust predictions and robust attribution; showing they are equivalent in simple NNs, explaining how robust prediction optimization as in Madry et al. allows for "cancelling out" of attribution shifts, and demonstrating empirically that optimizing for robust attribution improves robust predictions.  I'd encourage the authors to draw this out more.  The biggest weakness for me is does this approach change my interpretation of attributions?  Following the example from the intro about pathology, should I trust now the attributions from a model that is designed to not be sensitive to small perturbations?  Attacking attributions demonstrates their brittleness and shows that interpreting the "reasoning" of a NN from these attributions should be taken with a grain of salt, but it is unclear to me if this is a surface-level fix to a deeper rooted problem with attribution or if this is directly addressing a central problem.  So while the theoretical connection is interesting, I'd like to see a clearer argument for why designing models that have robust attribution is an important property.  Overall, this paper is interesting and in my opinion a valuable contribution.  Details: Sec 3.2 is gratuitous to me.  The objective is never used and at least as written I didn't find the connection as intriguing.   Is there any functional value to the use of an intermediate lay r for IG in (4)? Table 2 is redundant with Table 3.