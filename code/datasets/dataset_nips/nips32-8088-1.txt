I have scored the paper a 7 since I feel that this is a well-written paper on an interesting and relevant topic. Kudos to the authors for clear exposition, including code, and providing some guidance on how to set regularization parameters.  While I don't have much to critique, I do have two comments:  1. I do think that the paper could be *much* stronger if the authors could present some negative results regarding "robust" saliency maps. For example, it is possible to use their methods to build saliency maps that are robust to perturbations, yet are completely flawed in their attributions? This would make a nice use case for the method - giving us a sanity check to prevent rationalization. In line with this comment, it is worth adding a short note on the issues that are not fixed in with saliency maps (see e.g., https://arxiv.org/abs/1901.09749, https://arxiv.org/abs/1811.10154).  2. In some sense the previous comment highlights the following issue, which I would mention in the text. This method "appears to work" since the experiments report metrics that are bound to improve given the constraints (e.g., Kendall’s correlation between a saliency map and perturbed saliency maps is bound to improve given the set of constraints). While I don't think that this is a limitation of the work, I think that the paper should mention this explicitly for the sake of transparency. In short, it would help new readers to know that we are only fixing a form of brittleness that we can measure, and that it will not necessarily fix the more difficult attribution problem (though it can screen away clear cases of misattribution).  Other issues:  - l.183 One-Layer <- Single-Layer) - p.4 footnote: "We stress that this regularization term depends on model parameters θ through loss function l[y]" <- what does this mean?