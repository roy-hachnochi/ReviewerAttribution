The work studies an important problem, that of optimizing networks with respect to attribution robustness. The problem, in a way, is not only related to interpretability for its own sake, but also for improving the robustness of models in general, and that connection is very well phrased in the paper.   Comments: - The paper would benefit by getting another, maybe richer, dataset in the evaluation. MNIST is not a great example, especially when it comes to interpretations and robustness.  - In general, it is unclear (even from previous work in this space), how does attribution robustness correlate to the human perception of model interpretability. I am not aware of studies that have tried to measure this (empirically) but if there are, it is very useful including them in the paper. If such studies do not exist, it would be beneficial to at least have a paragraph that analytically explains how close this may or may not be to human interpretation. It is fair for the reader to know whether implementing such optimizations in practice, would even be visible to people at all.  On the same point, it is hard to understand how the improvements in IN and CC (Table 3) relate to practical improvements in robustness. Does an improvement of 0.02 really make the model outputs more interpretable to the input changes?  - On the ineffective optimization paragraph, point (2). This point deserves further and more precise discussion on why the authors think that the architecture is not suitable. Also, it needs a clarification on whether "architecture" here means the generic nature of NNs or the particular architecture of the networks studied in this paper.   - Minor: You could actually write the actual name of the metric in the header of Table 3 instead of the Accronyms.   