The paper investigates the use of parametric models and relates it to the use of non-parametric experience replay. Training an agent on the replay buffer is seen a planning in the sense that it allows the agent to improve using additional computation. Furthermore, the paper views replay-based algorithms and model-based algorithms under a unified algorithm and compares their performance in a fair setting to show that replay-based algorithms can perform as well or even better than model-based algorithms in certain conditions. In the popular Atari benchmark, It is shown that the replay-based algorithm Rainbow DQN is able to achieve better performance than a recent state-of-the-art model-based approach called SimPLe, using less data and computation.  Since the experimental results suggest that parametric models are not always necessary, the paper further investigates the necessity of parametric models. It is shown that planning in Dyna-style algorithms can easily lead to catastrophic learning updates. Alternatively, the paper suggests to instead use backward models which predict previous state and action from the current state and reward. This is beneficial because poor forward models lead to erroneous updates in an observed state while poor backward models only leads to erroneous updates in imaginary states. However, it might be more challenging to learn backward models due to their multi-modal nature even in simple deterministic environments.  The paper is very well written and easy to follow. The conceptual and experimental contributions of the paper are significant and future works are likely to build upon them. I recommend acceptance of the paper.   Post-rebuttal comments: Thanks for the clarifications and updates.