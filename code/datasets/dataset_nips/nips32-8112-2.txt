Classical image based CNNs are equivariant to translations (modulo pooling) and this is perhaps a major reason for their immense success. A image recognition task however also contains various other types of symmetries, that are usually incorporated by means of data augmentation. In order to incorporate more symmetries in a principled manner, such that they obviate data augmentation for those symmetries, group equivariant convolutional networks were proposed. Originally they incorporated simple symmetries such as 90 degree rotations, reflections in addition to translations. This was followed by work incorporating 360 degree rotations as in harmonic networks, gated harmonic networks and so on. In this paper using the theory of steerable CNNs, the authors work out a general strategy for equivariance to the euclidean group (that is the semi-direct product of translations with O(2)). In particular, for the Euclidean group, they work out kernel constraints that enforces equivariance, while generalizing a range of work on steerable CNNs for images.   Almost all of the paper is very cleanly and elegantly presented with almost no points that were unclear. The supplementary material also does a good job to fill in proofs. Thus except a nod of approval I don't have specific concerns or criticisms. In summary -- the paper is self contained and makes a strong contribution.   However, one concern that could be envisaged is that this work is simply working out details for a particular group, and using machinery that has now pretty much become standard. So why is it interesting? In addition to the points mentioned in the box above, it is perhaps worth emphasizing that the most significant advances in vision research and in industry mostly rely on CNNs working on the plane. Fancier models working on more exotic spaces still remain at quite a distance in terms of the sort of impact classical CNNs have had. It is thus nice to improve models in that space even further in a principled manner. Besides, a lot of the machinery described here is directly useful for manifold CNNs.   The experiments on MNIST (including those in the supplement) are really thorough. It is nice to see different combinations, while also working with restricted variants, while also checking the hypothesis that the equivariant models are able to exploit local symmetries. It is a bit surprising (although perhaps not a lot) though that for CIFAR even after autoaugment, there was a considerable improvement. I have believed that only a good sampling of the transformations is usually enough for good performance (which I think of as a valid criticism of GCNNs). Thus I might not have expected a very substantial gain. Although I do feel a bit concerned that such vetting of architectures might lead to some overfitting overall.  Some minor comments --  Please defined induced and restricted representations in the supplementary.  Line 81: Awkward sentence/typo: "could either be of.." Line 294: Awkward sentence.  Line 313: Typo "bad choices" LIne 332: Typo "loss of informations"