The authors present a new hierarchical reinforcement learning (HRL) algorithm that outperforms other state-of-the-art HRL algorithms. The algorithm starts with pre-trained low-level skills provided by existing methods from other hierarchical approaches. The HAAR approach presented in this paper then jointly optimizes a high-level selector policy and adjusts the pre-trained low-level skill policies. The key being that the low-level skill updates are performed using a proxy reward based on the advantage achieved in the high level policy from executing this skill. The authors give a theoretical analysis that such an approach leads to a converging policy when learned with TRPO. Empirically the authors compare to a number of existing HRL baselines, and non-hierarchical TRPO on multiple continuous control domains.  I think this is a good approach with good theoretical justification. Experiments support the method, showing it is faster. The empirical analysis is also well done.  Minor typo in algorithm 1 line 3 "shorest" -> shortest  I've amended my score after the author rebuttals and discussion with the other reviewers. The major concern being about the markovian assumptions with the experiments/representation.