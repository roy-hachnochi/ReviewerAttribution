The paper is well written and organized. The contribution, a scheme for hierarchical RL (HRL) in the online, on-policy setting, where the low-level policies are adapted, using the advantage function of the high level policy, seams to be novel, is elegant and might be a real improvement for HRL. The idea to improve the method by starting with large time steps and decrease them in an annealing like fashion, is probably a good idea in this kind of algorithms.  AFTER FEEDBACK The authors answer to my first question "How will the algorithm perform when starting with random low-level policies?" is very convincing. I found the answer to my second question on the Markov property not very informative, and the authors did not explain, what is meant by the "low-level skills" are "environment-agnostic" and "task-agnostic".  I did not change the "overall score".