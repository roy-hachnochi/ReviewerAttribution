There are, however, several deficiencies that should be addressed in order for the work to be publishable in a venue like NeurIPS.   -- The analysis of the achievability part is based on analyzing (9), which is the ML decoder searched over all pairs of graphs that are \eta-similar. The ML analysis is rather straightforward, involving merely large deviation upper bounds, straightforward counting of graphs, together with those for bounding the KL divergence already present in Santhanam and Wainwright (2012). Hence, it’s not clear what the novelties in the analysis here are.   -- The bounds for the converse part are useful but the techniques are based on specializing Fano and using techniques not dissimilar to those in Scarlett and Cevher (2018).   -- Even if there are technical novelties in the analyses above, the bounds as shown in Table 1 are not very tight; the sufficient condition(s) seems far from the necessary condition(s). Further, there’s no discussion of this gap. Next, they do not seem to depend on d.   -- One thing the reviewer fails to understand is how the authors obtained the plots in Figure 2. Usually such plots are obtained by applying tractable algorithms. However, ML is not tractable even for p = 100. Can ML be implemented in "real-life"? Do these plots show any phase transition in k, p, and \eta?  -- Some typesetting is very careless. “gammak” is present in Definition 2. Some compilation Latex errors in the supplementary material. Generally, this submission looks “rushed”. Also, in the supplementary, the reviewer wonders why the fonts for the proof of the achievability and converse parts are different.   -- One philosophical comment. Upon reading the title, the reviewer was hoping for recovery guarantees for a *single* graph given side information. However, while what the authors have done here is not incorrect, it is misleading given the title. They recover *both* graphs. Naturally, one would wonder whether one can do better (in terms of fundamental limits) if one seeks to only learn G_1 when samples from G_1 and G_2 are given (and G_1 and G_2 are similar in some sense). I would be thinking this is the true essence of graphical model selection with side information