The paper is overall well written, and the experimental design is fundamentally well thought out and rasonable - I cannot say if it is entirely novel or if similar looking graphs could have been achieved with different or similar techniques. The results look intuitively correct and confirm ones expectations. I find some parts of the experimental setup confusing: the CNN *model* has been trained on WSJ and Spoken Wikipedia, and is not performing an ASR task, but a closed-set word recognition task (in addition to the speaker ID task). Why was Spoken Wikipedia used in addition to WSJ? Would it not have been possible to use Librispeech or another well-known corpus? The entire paper would be much "cleaner" if both types of systems ("CNN"=word recognition + "DS2"=ASR) would have been trained and evaluated on the same type of data. If that is not possible - please explain.  What is the "CNN dataset"?  It is interesting that training a system towards phones will also increase the capacity for words. Would it be possible to perform the same experiments with characters (which is what the DS2 system has been trained with)? This should exhibit a similar pattern, but one could then also compare phone and character systems. In English, the grapheme to phoneme relationship (phones to characters) is pretty complicated ("tangled"), and it should be possible to show that this analysis can "measure" the degree to which certain phones have clear relationships with characters, and other phones have no unique relationship with characters.  