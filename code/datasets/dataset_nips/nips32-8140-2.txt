From the high-level of review, there are many works on understanding how neural network models do speech recognition internally, and this paper provides very solid analysis using a theoretical tool such as the mean-field theory. However, I'm not fully convinced how much the findings here can benefit the speech recognition research. In particular, the observations are from a particular model configuration (network structure, word-level label units) etc. It is unclear to me how much the observations would change in a different experimental setting, for example, with a system modeling the sub-phonemes as the traditional hybrid system, and evaluating on more challenging conversational speech corpus. The high-level conclusions such as normalization of speaker factors and untangling words in a speech recognition system are not surprising to me as a speech recognition researcher. It is not new, but this paper provides some more theoretical evidence to confirm what we have believed already.   A question: I'm a bit confused what is N the ambient dimension in section 2.1? And I'm fully understand why \alpha in section 2.1 is important. If P is the vocabulary size, we will use a softmax layer with size P for classification. What is this to do with \alpha and separating hyperplane?