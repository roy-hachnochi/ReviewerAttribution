RLSVI belongs to a family of algorithms that can be used with function approximation to encourage exploration. While the method enjoys some theoretical guarantees in the tabular setting, previous analyses provide Bayesian regret bound, and this paper addresses the frequentist (worst-case) regret. The writing is clear and the text provides interesting discussions and insights around the problem. In general I like the paper, and I have several clarification questions for the authors:  1. Regarding worst-case regret bounds for Thompson-sampling-like methods, it reminds me of the work of Agrawal and Jia [3]. If you look at their algorithm, it's actually two-phased: they use optimism when the sample size is small, and only switch to legit Thompson sampling in the large sample regime. What's interesting to me about the current paper is that there are no such two-phased structure, which looks a little bit too good and I will explain why:   RLSVI is similar and closely related to bootstrapping, and we know that bootstrapping only works when the data somewhat resemble the true distribution. This requirement can be difficult to satisfy in MDPs: unlike bandits where you can just sample each action a small number of times to achieve full coverage, in MDPs we often find ourselves not having any data at all from certain states in the middle of exploration. Now I guess this is circumvented by choosing \beta_k to be large enough in the beginning---in fact I was expecting \beta_k to start with O(H) value, the magnitude of the reward function, only to find that it starts with O(H^3)... but this does not fully answer my question: high noise would simply induce uniformly random exploration, which is for sure inefficient (e.g., in combination locks). Can you give some insights on this matter?  2. Algorithm 1 is very nice in the sense that one can almost directly use it in the function approximation setting (e.g., there is no mention of the visitation counters), except for Line 6 that samples the \tilde{Q} from a tabular prior. Some discussions about what's the counterpart of this step in the function approximation setting can be interesting.   ----------------Update after rebuttal---------------- Thanks for the response, and it makes a lot of sense. I keep my original recommendation.