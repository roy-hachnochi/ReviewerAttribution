The authors studied the implicit regularization effect of (accelerated) gradient algorithms for least squares. The paper is well-written and clear.  The main results are the population risk analysis of (accelerated) gradient methods for least squares, with the conclusion that accelerated methods achieve the same accuracy of vanilla gradient descent but with much fewer iterations. The assumptions are clean and standard. I like that the convergence bounds are proved in a very "unified" way based on spectral filtering technique. The authors also showed that their theoretical findings are empirically observed (and matched almost perfectly!) in the experiments, which is very nice.  One drawback of the paper is that there are only upper bounds without any corresponding lower bounds. In other words, it is possible that their analysis are not tight, although that seems unlikely to me according to the experiments. Also, the least square setting is a bit too simplified (is it possible to extend to general convex setting?), but I am OK with just least squares at the current state of research.  Overall, I think this is a good theory paper which connects stats and optimization. I suggest acceptance.