In this paper the authors introduce the Cormorant neural network, which is constructed to generate rotationally covariant feature representations. Therefore, it could be used to learn tensorial objects that obey the correct transformation rules. In general this paper fulfills all the requirements to be a NeurIPS paper, there is no doubt of its originality and quality. In terms of clarity, the authors give a clear presentation of the physical motivation in sections 2 and 3. Nevertheless, because of its robust and involved mathematical formulation and its implementation technical difficulties some parts of section 4 (sections 4.4 and 4.4.1)  are not clear. Still, the general structure of the paper should stay the same due the insightful presentation of the model in sections 2-3, but a clearer presentation should be given in section 4. I would suggest moving implementation/technical details (sections 4.4 and 4.4.1) to the supplementary material and take the necessary number of pages to provide an understandable explanation. The new space left in the main text could be used to elaborate in the description in sections 4.1, 4.2 and 4.3. This would greatly improve the presentation of the paper.   Additionally, the authors should address the next list of comments:  Introduction: The authors should add a fundamental pair of references by Alessandro De Vita’s group on covariant learning (Physical Review B 95 (21), 214302, 2017) and the recent work on covariant force learning with NNs by Mailoa et al. 2019 (arXiv:1905.02791). At the end of the introduction the authors state: “Cormorant is arguably one of the most general. ” There are no foundations to this statement. The authors have to provide a convincing argument otherwise they have to remove such statement.   Section 2: In line 65: What does “(spin)” stands for? If this is in the physics context (e.g. spin of an electron), it is a vectorial quantity, not an scalar as stated by them in line 78. Section 3: Line 103-104, The authors should explicitly mention what is d. Line 122: Why \hat(Q)_{l,i} instead of just Q_{l,i}? Explain. Line 126-127: Is \bar(T)^{(l)} a tensor or a vector according to Line 100? Just to be clear. Section 3.1: It would be very useful to provide a reference for equation 6 to get a better understanding of the content of this section. Section 4 Lines 166-168: The term “Physical laws” is an extremely general term, which makes the paragraph ambiguous. The authors should refer to conserved physical quantities such as linear momentum, angular momentum and energy conservation corresponding to translation, rotation and time invariance, respectively. Line 191: The authors should list what are the differences respect to known architectures in the field of potential energy surface learning. Since translation and rotational invariance are implemented in the same way as any other NN.  Section 4.2 Line 233: “of of”, Line 274: “be be” Section 5: QM9 has 12 learnable variables, why learning only 9? In section 5.QM9, The authors should discuss why in particular the Cormorant performs better or comparably good in those particular quantities and why it underperforms for R^2 and \mu? The MD17 dataset consists in energies, forces and molecular coordinates. The models DeepMD and DTNN were trained using only energy labels, SchNet was trained on energies and forces, and the GDML was trained only on forces. The authors should mention which labels did they use for training. In case that they only used energies, can they comment on the possibility of using also forces for training and what would they expect? In the particular case of the comparison with the MD17 dataset, the authors are contrasting they results to old results (Chmiela et al. 2016), even though there a more resent publication on this dataset (Chmiela et al. Nat Commun. 9, 3887, 2018). Such comparison makes more sense given that those are newer results and still Cormorant performs better. It is noteworthy that Cormorant excels a kernel method in this arena. 