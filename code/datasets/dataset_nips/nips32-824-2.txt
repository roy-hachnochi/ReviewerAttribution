Originality: Considering that multi-view and multi-modal RL papers tend to offer ad-hoc solutions to the problem, this paper's formalization is a nice contribution. Quality & Clarity: The paper is well presented, the math is mostly clear, although some parts aren't obviously translatable to an implementation. In terms of experiments, it seems that many details are lacking, and as far as I call tell, all figures represent a single run of each setting, which is worrisome. Significance: While the contributed framework does seem like a useful formalism, this paper fails to convince me that it actually is: - The proposed experiment in 4.1 creates artificial views which don't seem representative of multimodal settings, in that they all contain the same *information*. It would have been more convincing to feature an experiment where views are truly independent when conditioned on the current state (e.g. dialog and facial expression, partial views). - It's not clear that the advantage comes from your formulation rather than just more things being learned (i.e., what you propose reduces sample complexity because it is an auxiliary task). You should have experiments confirming this. - Again, each experiment setting seems to only have a single run. All your results could be plain luck.  Additional comments: - l49, "agents *to* reason" - section 2.1, iiuc, you force upon the agent to only receive information o_t^{i_t} about one view per timestep. What is the distribution of i_t? Is it dependent on state and action? Is it a choice of the agent? This should be clear in your framework. - l94 "existing *advancements* on" - l105 "thus being optimal than independent", what do you mean? "as optimal as"? "more optimal than"? - Figure 2, why are the X axes of different lengths? How did you choose when to stop training? This should be reported - Figure 4, I'm not sure I see the interest of having the X axis be in log-scale. - Table 1, Why the "\sim 360"? Seems like you should be reporting mean and variance, like "360 \pm 10" - l245, what view did you transfer? when? We need more details - l269 "primarily *encourages* higher"