Although this paper tackles an interesting problem and has some theoretical results, I do not fully understand what the assumptions really mean and whether they are valid assumptions. I am also not very much convinced with the experimental results.  I guess the main point of criticism for this paper is that, at least to me, the paper fails to justify the stability assumptions and to clearly articulate what these assumptions mean.   First of all, the authors claim that dropout stability and noise stability is likely satisfied by the solutions obtained from gradient-based optimization, but there is not enough justification for that. The authors use these stability concepts from generalization literature, which is different from optimization. So, I am not convinced if these stability conditions are valid assumptions to make in the first place.  I am also very confused with the assumptions themselves. - To start with dropout stability, it seems like only \theta_1 is used in the proof technique. Why does Definition 1 require all \theta_1, \theta_2, \dots, \theta_{d-1}?  - The quantifier in Definition 1 looks incorrect. “There exists a subset of at most h_i/2 hidden units in each of the layers from i through d-1” means that each of the layer i, i+1, \dots, d-1, the subset size is at most h_i/2. Note that even for layer d-1, the subset size depends on h_i. Is this what you really meant? According to the proof of Lemma 2, I think the definition must change because it looks like subset for layer j must be at most cardinality h_j/2. - For the quantities in Definition 2, there isn’t enough explanation on what these quantities are capturing. For example, is it better (more stable) to have large mu or small mu? - Definition 3 doesn’t make sense to me, because the smoothness \rho is defined to be the smallest number but if you make it smaller and smaller, then the RHS of the inequalities will get larger and larger, which means that the infimum of such \rho must be 0. - Definition 4 also is difficult to interpret; the authors present some scary quantity without any good explanation. Especially, the authors say that the network is more robust if \epsilon is small. Then this means that the network is more robust if c is small, which means that the activation $\phi(x^i)$ must scale up all $x^i$. So this encourages $\phi$ to amplify $x^i$. However, I don’t understand why this leads to stability because if $\phi$ amplifies its input, then any noise will also be amplified by a big factor, leading to instability. - Connection of noise stability to drop stability is a bit weird as well; at least intuitively, it looks like a circular argument. Interlayer smoothness is defined using dropout noise (Alg 1) and then noise stability is defined using interlayer smoothness, and it is shown that noise stability implies dropout stability.  I also have some comments about experimental results: - Why are experiments carried out on convolutional neural networks, when most of the theory developed is on fully connected neural networks? - The authors claim that Figure 2 shows that the solution is dropout stable. However, by Def 1 the dropout probability must be at least 0.5, and looks like the loss value 0.5 we get after dropout with p=0.5 doesn’t look like a very small number. So, I’m not fully convinced that the solutions that we get from training is dropout stable. - I think for the plots of loss function value through the constructed paths don’t give a good picture of whether the loss function value is indeed kept small along the path. Maybe comparing with direct linear interpolation will help illustrate the strength of the constructed path.  --------------------------------------------- (post-rebuttal) I read the authors’ feedback and the other reviews. I think my negative impression of this paper came in part from some critical typos in the definitions (Def 1 and 3) which made me very confused while reading the paper. Thanks to the author response and the other reviewers, most of my concerns were well-addressed.  However, I still believe the paper has some room for improvement, especially in its clarity of definition (and sketch of proofs, as the other reviewers pointed out). Besides correcting typos, I think it might be helpful to readers if the authors provided more explanation on the quantities defined in Definitions 2 and 4. At its current status, the paper relies heavily on readers’ prior knowledge of [Arora et al. 2018]; I believe the paper should be more self-contained.  Re: purpose of \theta_2 through \theta_{d-1}: Now I understand that step (2) in Figure 1 uses \theta_2, thanks for clarifying. However, it was unclear from the main text that these points are used in the construction, because Lemma 1 and the following discussion only mentions \theta_1. I hope that this will be clarified in the next revision.  Overall, I guess my initial score was overly harsh on this submission; I have updated my score.