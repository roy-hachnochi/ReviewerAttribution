Summary: This paper provides a theoretical analysis of the mode-connectivity phenomenon in deep learning [1,2]. First, under the assumption of stability to dropout (or more precisely existence of 1 dropout pattern under which the loss is stable, see Def. 1), the authors prove existence of a low-loss path consisting of a number of line segments that is linear in the number of layers. Under more restrictive noise stability assumptions [see 3] the authors prove the existence of paths with a constant number of linear segments. Next, the authors construct an example of architecture and dataset for which the mode connectivity doesn't hold despite overparameterization. Finally, the authors empirically evaluate the behavior of the noise stability metrics they introduced, and visualize train loss and accuracy along a path constructed by their method.  Originality.  To the best of my knowledge the present work provides first theoretical insights into mode connectivity for general deep neural networks. There exists related work on mode connectivity for restricted classes of neural nets [see e.g. 4], which is appropriately cited.  Quality.  The analysis is technically sound (I checked the details of proofs of Theorems 1,4, and skimmed the proofs for Theorems 2, 3). The constructions in the proofs are interesting and fairly intuitive.   One of small issue I can see, is that in the definition if interlayer smoothness the authors work with logits of the networks obtained along the linear path from a given network to its dropout version. It seems like requiring interlayer smoothness could potentially be implicitly requiring the network to be stable along this linear path. However, the results are interesting nonetheless.   Another question I have is about the construction of the dataset in appendix C. Do I understand correctly that for the case l < i ≤ m, it could happen that i ≠ j mod h for all l < i ≤ m, as you only require m - l > 2? In this case, the entries corresponding to this range of i's would all be zeros, and I believe this would break the proof of the fact that all a_{i,j} must be non-negative. Even if this is true, this is easy to fix by changing the condition i = j mod h to i = j mod 2.   In the empirical part of the work, I find it hard to analyze the distributions of the noise stability notions, as they only appear in the asymptotic bounds. In particular, it's unclear to me what exactly should I understand from their specific values. Could the authors please comment on this?  Clarity  The paper is generally clearly written and easy to follow. There are several things that could be fixed to improve clarity however: 1. The difference between \Omega and O complexities is not explained. 2. Definition 1 was not very clear to me initially, I think it's vague the way it's written. It says that there exists a subset of [h_i / 2] neurpons in layers i to d, such that a certain condition holds. I believe, it should instead say something like for each i there exists solution theta^i such that for each i ≤ j ≤ d there are at most [h_j / 2] nonzero neurons on layer j. Lemma 2 is ambiguous in a similar way, it should probably say ...[hi / 2] of the units in each hidden layer i for each i... rather than ...[hi / 2] of the units in each hidden layer...  3. While the proof of Lemma 1 in the appendix is clear, it would help if the main text included a definition of Ls and Rs.   Significance  I believe this work is significant in the sense that it sheds new light on the mode connectivity phenomenon, and structure of minima in neural networks. In my opinion this paper would be a good contribution to NeurIPS, so I recommend an accept.  [1] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs; Timur Garipov, Pavel Izmailov, Dmitry Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson [2] Essentially No Barriers in Neural Network Energy Landscape; Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred A. Hamprecht [3] Stronger generalization bounds for deep nets via a compression approach S Arora, R Ge, B Neyshabur, Y Zhang  [4] Topology and Geometry of Half-Rectified Network Optimization; C. Daniel Freeman, Joan Bruna  *Post-Rebuttal* I have read the other reviews, and rebuttal. I was satisfied with the rebuttal and maintain my assessment. 