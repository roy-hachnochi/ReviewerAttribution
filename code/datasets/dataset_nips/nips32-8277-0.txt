Summary: The paper proposes a new algorithm for learning linear preferences, which are objectives derived from a linear weighting of a vector reward function, in multi-objective reinforcement learning (MORL). The proposed algorithm achieves this by performing updates that use the convex envelope of the solution frontier to update the parameters of the action-value function, hence its name: envelop Q-learning. This is done by first defining a multi-objective version of the action-value function along with a pseudo-metric, the supremum over the states, actions, and preferences. Then, a Bellman operator is defined for the multi-objective action-value function along with an optimality filter, which together define a new optimality operator. Using all these definitions, the paper then shows three main theoretical results: 1) the optimality operator has a fixed point that maximizes the amount of reward under any given preference, 2) the optimality operator is a contraction, and 3) for any Q in the pseudo-metric space, iterative applications of the optimality operator will result in an action-value function which distance to the fixed point is equal to zero, i.e. is equivalent to the fixed point. Based on these results, the paper then proposes a learning algorithm to approximate the solution of the optimality operator by using a neural network as function approximation, a mixture of two losses to facilitate learning, and double Q-learning to learn the action-value function. Finally, the paper provides empirical evaluations of the envelope Q-learning algorithm in four different domains and demonstrate how it can be used to discover preferences when only scalar rewards are provided.  Review: The main idea of the paper is novel and provides interesting avenues for future research. The literature review was thorough and the results seem to include the appropriate baselines and seem to show some improvement over previously proposed algorithms. However, I have two main concerns about the results presented in the paper. First, the lack of clarity and details in the background and theoretical proofs, which makes it difficult for the reader to verify the results. In other words, I could not verify that the theoretical results were correct and, although I do not think they are wrong, I could not rule out the possibility that there was a mistake. I think this is mostly because some parts of the proofs skip over too many steps and not because the proofs are wrong. Second, the empirical results presented seem to be based on one single run which was tested over several examples. If this is not the case, the paper should include details about the number of runs used to compute the statistics; note that the number of runs is not equivalent to the number of episodes or training sessions. However, if this is the case, then it is problematic because the performance of deep reinforcement learning agents can vary significantly even when using the same initialization scheme. For all these reasons I consider the paper should be rejected but only marginally below the acceptance threshold. I provide more detailed comments in the following sections.  On the clarity of the introduction: Overall, the introduction was very clear and well motivated except for two exceptions:     1. In line 32, the statement: “The MORL framework provides two distinct advantages — (1) reduced dependence on reward design,” is not clear. Since in MORL the reward function is a vector and tasks are defined as a function of a preference, it seems that designers would have to define a more complicated reward function or a general reward function but a more complicated preference function. Hence, it is not clear from that statement alone that there’s reduced dependence on reward design. Perhaps I’m missing something, but to improve the clarity, it would be good if more evidence was provided to support this claim.      2. In lines 32 to 34, 46 to 48, and 49 to 50, there are three independent numbered lists with two items each. I personally found this unnecessary and confusing since it’s not easy to keep track of all these lists and it is not clear what is the purpose of each of them. This is mostly a minor complaint and a stylistic choice, but I think it would improve the clarity of the paper if you only had one list to emphasize the key insights of your approach (lines 49 to 50). These are mostly minor observations, but I think addressing them would improve the quality of the paper.  On the clarity of the background: My main concern about the background section is the lack of distinction between the Pareto Frontier and the Convex Coverage Set (CCS in the equation below line 72), which is only exacerbated by the lack of details provided for the plots in Figure 2. My first suggestion for addressing this issue is to have a formal definition for the Pareto Frontier (F* in the paper in line 71) under the MORL framework and emphasize how it is different from the CCS. Second, to improve the plots in Figure 2 on page 3, the paper should provide more context about what each element of the plot and each label means. Moreover, consider using different colors instead of a grayscale. Third, the plot should exclude  irrelevant information, for example the circles labeled L and K in Figure 2(a) and the dashed lines in Figure 2(b). Lastly, there was no context or details provided for Figure 2(c), so I would suggest either removing it or providing some explanation for it. There’s some allusion to the meaning of the plot in Figure 2(c) in line 115, but it is not enough information to understand the plot.   On the clarity of the theoretical results:     1. First, I would suggest defining the multi-objective optimality filter H in a separate line since it is an important definition.       2. Second, I could not make sense of the proof provided for Theorem 1 even after several attempts at it. Specifically, in the equations below line 601 in Appendix A.2.1, it is not clear how ⍵^T and the arg_Q operator cancel each other. Additionally, the Q in this same set of equations should be labeled Q* since the whole proof is based on the optimal multi-objective action-value function. Moreover, I believe that in the equation below line 604, there is an arg_Q operator missing before the supremum operator; otherwise, the equality would not hold. Finally, in the same equation, two different symbols are used to denote the transpose operator, which is confusing.       3. Third, just as in Theorem 1, the proof for Theorem 2 is also missing details. In the set of equations below line 614, it is not clear how ⍵^T cancels out from line 3 to line 4. Allegedly, this is because some cancellation between ⍵^T and the arg_Q operator, but, once again, it is not clear how this happens. Lastly, in the set of equations below line 616, it is not clear how line 3 follows from line 4.  This does not mean that I think the proofs are wrong, but I cannot verify that they are true. It would be great if you could provide a more detailed derivation for the less mathematically inclined readers.      4. Finally, in the learning algorithms part of Section 3 (Lines 169 to 202), it would be useful if the paper provided some intuition about why L^B is smoother than L^A. Moreover, there is a mismatch between the notation used for the losses in the main body of the paper and the notation used in the appendices. In the main paper, the losses are denoted L^A and L^B (except for one occurrence in line 190 where L^B is referred to as L^B_k), whereas in the appendices these loses are denoted L^A_k and L^B_k. To avoid confusion, the paper should keep the notation consistent, including in the appendices.  On the significance and clarity of the empirical evaluations:     1. First, the plots in Figure 3 shows similar issues as the plots in Figure 2: not everything is well defined and not enough context is provided about how the main body relates to the plots in the figure.       2. Second, no information is provided about the number of runs used for the results. There is information about the number of test episodes, but given that deep reinforcement agents can have drastically different performance based on the random initialization, several different independent runs should be used in order to demonstrate meaningful differences in performance. Moreover, there is no information about the meaning of the error bars. If computational resources are a limitation, consider providing statistically meaningful results for the smaller environments and just a few runs (e.g., 5) for the bigger environments.  ----  Update:  Given that the authors addressed all my main concerns, I have decided to increase my overall score. I still think that Figure 2 and 3 require a more detailed explanation and hope that the authors will address this in the final version of the paper.   