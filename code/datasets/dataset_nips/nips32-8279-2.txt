The authors develop a matrix-free version of the BBMM GP inference scheme of [11] to allow scaling to large datasets without running into memory issues. While the ability to scale exact GPs to over one-million training points is impressive, I found that this paper did little to extend the work of [11] since the inference procedure is basically identical except for an MVM approach that computes rows of the kernel matrix on the fly, and a more distributed implementation. That being said, I do appreciate that this implementation could be quite useful in various industrial applications.  It would be nice to see reference made to other matrix-free methods considering this is the main innovation of the proposed approach.  Nowhere in the paper did it actually describe the method of [11] that you used. The background would be an appropriate place for this.  The method used for predictive variance is simply cited in an external reference without any details about the technique in the paper (line 155). Please describe or at least summarize in some detail what the method is within the paper. Additionally, nowhere in the paper is it mentioned that this predictive variance computation is an approximation. This must be explicitly pointed out in the paper considering you are elsewhere claiming you are performing exact GP inference. In particular this must be mentioned wherever you make claims about the speed of the predictive variance computations since it's currently misleading. Lastly, does predictive variance appear in any of the studies? All you results appear to only consider the predictive posterior mean.  In the abstract it is claimed that the method uses “more powerful preconditioning” than [11], however, this appears to be incorrect: the preconditioning section clearly states that the proposed approach uses the same partial pivoted Cholesky factorization as was used in [11].  I like the idea of the ablation studies presented, however, I found you claims to be relatively unjustified as a result of the small number inducing points (m) used in the studies. Additionally, it is not clear why you had conducted the sparse GP comparison studies (fig 2) on two small datasets since (all things being equal) it is expected that there is greater redundancy in the larger datasets and so it is expected that greater compression would be possible through the use of sparse GPs on these large problems. I think it would be very interesting to “combine kernel partitioning with inducing-point methods to utilize even larger values of m” as you suggest. I think that your write-off of this idea as a result of figure 2 isn't justified until you consider an m that is much closer in magnitude to n for a large dataset. I think it is reasonable to expect that combining your matrix-free BBMM approach with a sparse GP approximation could be a promising extension to consider.  ------------------ (after rebuttal)  I thank the authors for their response and for providing the additional test negative log likelihood results. I again commend the authors on the engineering effort required to perform exact GPs on the scale considered. However, in light of very minimal theoretical contributions in the paper, all reviewers appear to agree that this paper's value to the NeurIPS community relies heavily on the quality of the experimental results. After careful consideration, I maintain my position that for this work to be deemed fit for NeurIPS acceptance I would still like to see the empirical evaluations extended to better support the authors' claims.  Additionally, I will note that a particular point I was unhappy about in regard to the rebuttal is that the authors never responded to concerns about their unsubstantiated preconditioning claim in the abstract. It appears to me that the authors have simply taken somebody's prior work (i.e. from [11]), modified a free parameter (the rank k), and prominently claimed it as a "more powerful" method in the abstract. This is unacceptably dishonest marketing and I'm surprised there was no commitment to change this in the authors' response.