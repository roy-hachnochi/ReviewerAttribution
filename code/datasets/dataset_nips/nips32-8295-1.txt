*author feedback*  I thank the authors for addressing my comments. I am willing to increase my score to 5 and I hope that the authors will do a thorough pass over the paper for the final version, if accepted. A better description and discussion of the experimental results must be added and I would also recommend to simplify presentation and add more intuition about the challenges faced in the analysis to the main part of the paper in order to put more focus its contributions.  *summary*   The authors propose and analyze a new algorithm, called Qsparse-local SGD. It combines three known concepts for achieving communication efficiency into a unified algorithm. That is sparsification, quantization and local computation. The proposed algorithm is analyzed theoretically and evaluated for training Resnet-50 on the ImageNet dataset.  *comments*  [Presentation] The paper is generally well written and easy to follow. I like the introduction with the related work where the differences to (closely) related work is explained in detail and effort is put into defining the contributions and limitations of the proposed method.  The presentation of the paper is a bit dense and a lot of equations and enumerations are put inline. This could be improved by adding a bit more structure. An additional 9th content page could help.  [Novelty] The novelty of the paper is the combination of existing techniques into one algorithm. This comes with some theoretical challenges as the authors say. Since this is the main contribution of the paper it would be nice to have more technical details on what these challenges are in particular and how they are solved.  While the combination of existing methods is a valid contribution I would like to see more thorough experimental results showing the merit of the individual components that you put together w.r.t the overall performance. For example, it would be interesting to see what the gain is of using quantization on top of sparsification? How do the quantization level s and the sparsification level k play together? How would one tune these values?   [Experiments] The experimental section is difficult to follow and important details are missing. This needs to be improved, in particular: - you use sign quantization, topk sparsification but what is the synchronization schedule you used to distributed the work? How many local steps are performed on each worker before synchronization? - The figures and the reference schemes need to be explained better. What do the individual curves correspond to? What do the numbers in the legend mean?  Which curve corresponds to local SGD without quantification or sparsification? This has to be stated in the main part. - Did you tune the parameters for the individual methods to have a fair comparison? - Please refer to the appendix if the reader can find additional experiments or supporting material there.  Minor comments: - The definition of b is hidden in the algorithm, should be somewhere in the text - In line 176 you refer to assumptions (i), (ii) whereas you use (i) for enumeration in multiple places. Use (A1) (A2) or something else for it to stand out.  *Summary* The paper is generally well written apart from the experimental section which needs to be improved. The contribution consists of the combination of existing techniques and their analyses. This is fine, but for the contribution to be strong enough I think the technical challenges should be emphasized and explained better and the merit of the individual components, as well as their tradeoffs should be evaluated more carefully.   