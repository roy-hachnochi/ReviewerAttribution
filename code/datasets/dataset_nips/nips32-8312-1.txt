Originality: This paper is the first demonstration of flow-based models to discrete data. As such, the work is fairly novel. The flow-based modeling community has been wondering how to model discrete data for some time, and this paper provides an answer to this question. That being said, the main technical contribution amounts to using a modulo operator (Eq. 5) and handling backpropagation through an argmax operator (Eq. 6) on top of the existing techniques of MAF and Real NVP. I view this simplicity as a benefit of the approach, but some may view this a simple extension of existing techniques.  Quality: The technical and experimental aspects of the paper are well-executed. The authors provide multiple experiments to demonstrate autoregressive and bipartite flows. Within these experiments, various hyper-parameter settings are reported to gain a better intuition for the performance of the models. Generation time is reported, helping to demonstrate the benefits of the models. For the most part, the technical ideas are fully developed and explored.   Clarity: The presentation of the approach is incredibly clear. Examples are given during the presentation, which help the reader gain intuition about when the approach is useful. The diagram in Figure 1 is helpful for unfamiliar readers. For the most part, the experiments section is also clear. Some details of the models and training set-up are unclear, particularly in the toy examples from sections 4.1 - 4.3. Additional details in the supplementary material would help to clear up confusion.  Significance: Although the introduction of discrete flows is a significant contribution, the paper currently feels like more of a proof-of-concept, rather than a competitive new approach. Demonstrations of new techniques are helpful, as other researchers will undoubtedly extend this technique to new settings. But additional experiments would help to complete this paper and broaden its impact. Many flow-based models have been applied to images, and it seems like discrete image datasets, e.g. binarized MNIST or Caltech-101 silhouettes, would be a natural testbed. In fact, RGB images are already naturally discrete. Likewise, with recent interest in discrete latent variable models, e.g. VQ-VAE, applying inverse autoregressive flows for variational inference would be another natural choice.  ---  Updates:  Assuming the authors provide additional details on experiments in the supplementary, then I will be happy with this aspect.  I'm perplexed as to why the authors seem resistant to running experiments on a simple binary image dataset, e.g. binarized MNIST or Caltech-101 Silhouettes. With binary data, there wouldn't be any issues with the ordinality of the pixels. And these datasets are small enough that getting results should take a matter of hours or less. This just seems like an obvious experiment to try to see how discrete flows compare with other families of generative models. It would also help to broaden the appeal of the paper to a wider audience.