The paper highlights that, despite not looking very obvious at first, normalising flows are in principle available for discrete data as well. The key is to design invertible transformation between discrete spaces. We can think of such a bijection, as a relabelling of the discrete sample space, and there's no need for computation of determinant Jacobians.  They also show that it's not difficult to design parametric invertible transformations. They show an example with XOR and a generalisation thereof based on mod K.   The real difficulty--and the paper could be more explicit here--is how to estimate the parameters of such discrete transformations. Their parameters are themselves discrete, thus if you have a NN predict them (for maximum flexibility), this network would require a nondifferentiable output activation.  The approach taken at this point is to ignore the problem, and employ the straight-through estimator (which the authors argue work well for problems where K is not too large).   The authors demonstrate the technique is effective in controlled artificial tasks as well as in char-level density estimation for text (PennTreebank and text8) showing both improved likelihoods and fast generation.   The paper is clearly original, and I imagine it will be of great significance (it brings two interests together, namely, flexible flow-based density estimation, and modelling discrete data).   The paper is mostly quite clear, I only have a few remarks in the next box. 