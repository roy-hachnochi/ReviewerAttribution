The idea is of value, however, I think there are few experimental protocols and writing and claims on novelty that can be improved.  First, I do not think "Wibergian Learning" is the best name for the approach or the title. Aside from the fact that the method is not immediately obvious from the name, it does not accurately describe what is proposed either. Wibergian approaches (as well describted in [4]) are essentially alternation algorithms, where the fixing of one of the unknowns give rise to a closed-from solution for the other. This work has nothing to do with giving rise to a closed-from solution, it only provides a method to update the parameters of the energy function in a stable manner. It also is not used as an alternation algorithm, since in all the experiments it seems like the inputs to the energy function (that is the output of another set of weights (2D pose detections) or algorithms (trackers)) are all fixed.  Furthermore, the technical novelty of the appraoch is limited, since [1] also has previously derived the same update. This paper provides a different derivation, although it follows the same decoupling assumptions, and it does not seem like there is particular value in this new derivation itself (a theory minded reviewer may disagree, however it seems like the techniques are standard). Really the key difference is in adding of the small positive identity matrix to the Hessisan to avoid numerical instability, however this is an age old trick in linear algebra (so much so that finding a reference is challenging). This is the first thing you would do if you run into a numerical problem.  This is not to say that that the method proposed in the paper is not valuable. This is to say that the contribution of the method does not lie in the novelty of the algorithm, since it has more or less has been developed before in [1]. Thus I do not think it is appropriate to call the proposed technique novel as this paper does in the introduction (line 41).  Given above, in my opinion, the novelty/contribution of the paper lies in how it brings attention to the fact that this type of weight updates energy functions play nicely with the current deep learning based approaches, and would have like to see more experiments and discussions on how limiting the assumptions are in practice, such as the assumption that the local neighborhood of x*(w) is strongly convex and how bad the drift is in practice. It would help to have a toy experiment.  Further, the writing may be improved. It's rather confusing that the paper calls the approach "networks" in the human pose estimation experiments, since they are more appropriate to be called as parameters of the energy function (eq 11), and it's not like these parameters are network outputs that take some input. It could make more sense if the stacked hourglass ddetection modules are also being updated at the same time, however this seems to be fixed.  The details are also missing, as to how long the training takes, how the weights of the energy function changes over time. Further in the 3D pose experiment, the energy function is made more complex, for example how the individual reprojection losses are now weighted. Although the approach is quite simple, on a first read it is difficult to figure out what exactly is going on.  Another contribution of this approach is that the method is quite general. I'm sure there are many dictionary learning papers that have dedicated ways to update the dictionary parameters, but the proposed approach provides a quite a general derivative update, that may be used for any kind of energy functions. While a positive of this approach is that it is quite general, it should still offer analysis as to how competitive it is when it is compared to a parameter update for a specific quadratic energy (for example, without accounting for the reprojection loss weights, if you only updated the basis columns (e_i) for the human pose estimation case, how does the proposed approach compare to doing usual PPCA update to the basis on the Human3.6M training data?)  In particular, the experimental protocol should have more ablations. Where is the improvement coming from? In [23] the basis are pretrained on a 3D mocap data. Simply re-tuning the basis on the Human3.6M training data may also give simila rise in the performance. As the paper says, the optimization of several sets of parameters are enforced, such as the model aprameters, the scale and the novel weighting of the camera and the reprojection loss. The improvements from updating these sets of parameters should be shown and be compared to a baseline approach (such as re-training the basis on Human3.6M data).  Further, the standard approach that it should compare to or provide some time analysis against is that of cross-validation or grid-search of these weights on the training data. Is the proposed approach faster than simply doing grid-search (for the weights of the reprojection loss for example)? This can be a baseline.  One thing that was not clear to me was how the dependance of the weights are being considered. For the human pose experiment, the three sets of parameters, where they updates all together or one by one? What is the difference of these approaches?