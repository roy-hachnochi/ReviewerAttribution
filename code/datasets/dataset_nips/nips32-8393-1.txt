Originality: The assumptions and the overall proof ideas is similar to a previous work [5], as mentioned several times by the authors. However, extending the analysis to residual networks is non-trivial and novel, as far as I know.   Quality: The claims and the theory seems sound, however, I have not checked all proofs in details. Clarity: I enjoyed reading the paper for the most part. Overall  presentation is clear; specifically, the related work section is well done. However, lines 13-14 of the abstract requires more clarifications. Is this statement precise? Is there a result showing that the super logarithmic dependence on depth is un-avoidable in deep fully connected neural networks? Significance: the results is interesting in particular because it shows a non-trivial improvement in the generalization error when skip-connections are used. However, it is not clear if this stems from a sub-optimal analysis for deep feed-forward networks.  ######## I have read the author feedback and considered it in my final evaluation. Overall, I think this paper is qualified to be published in NeurIPS.