First and maybe most importantly, how to reasonably assign these class prototypes. The paper makes it an optimization problem which resembles Tammes problem. It can be sometimes problematic, since Tammes problem in high dimensions is highly non-convex and the optimal solution can be be obtained or even evaluated. Using gradient-based optimization is okay but far away from satisfactory, and most importantly, you can not evaluate whether you obtain a good local minima or not. Besides this, the semantic separation and maximal separation are not necessarily the same. As you mentioned in your paper, car and tighter can be similar than can and bulldozer. But how to incorporate this prior remains a huge challenge, because it involves the granularity of the classes. Naively assign class prototype on hypersphere could violate the granularity of the classes. The paper considers a very simple way to alleviate this problem by introducing a difference loss between word2vec order and the prototype order, which makes senses to me. However, it is still far from satisfactory, especially when you have thousands of classes. The class prototype assignment in high dimensions could lead to a huge problem. From my own experience, I have manually assigned CNN classifiers which have maximal inter-class distance, and then train the CNN features with these classifiers being fixed the whole time. My results show that it can be really difficult for the network to converge. Maybe adding a privileged information like the authors did could potentially help, but I am not very sure about it.  Second, I like the idea of using hypersphere as the output space despite the possible technical difficulties. I have a minor concern for the classification loss. The classification loss takes the form of a least square minimization which can essentially viewed as a regression task. What if using the softmax cross-entropy loss instead? Will it be better or worse? I am quite curious about the performance