Post rebuttal: My suggestions/comments were not addressed in the rebuttal, so I keep my score as is. ---------------------------  The authors propose a model-free planning framework using goal-conditioned policies and a value function over a learned compact latent variable representation. Others have proposed this type of two step optimization where one first learns a compact representation with a VAE on randomly collected samples, then use various RL or planning methods on the representation. However, this doesn't work well for high dimensional spaces where random collection of data for learning the representation space does not give enough samples -- especially from the optimal policy. This work doesn't address this issue, by only evaluating on environments with very small state spaces, where random sampling to train the VAE is feasible. Originality: The idea of planning using TDMs over a latent representation is novel, and a promising direction for goal-directed planning in high-dimensional observation spaces. Related work in neighboring areas is adequately cited. Quality: The authors provide a theoretically sound justification for their method, and evaluate in both 2D navigation and a robotic manipulation simulation with higher dimensional latent space and show ability to perform better at long term reasoning compared to existing model-based and model-free methods.  Clarity: The paper is clearly written, with contributions and novelty clearly stated, and experiments, hypotheses being tested, and ablations clearly described and justified.  Significance: The results are somewhat important, in that they clearly show higher performance with reasoning in latent space using TDM compared to planning with MPC, model-free methods, and one method with imagined goals. However, it would be also useful to see how different training methods compare when using this method, as training on randomly collected samples from the environment will fail on complex tasks. Even an example of a more complex task in which this method fails because of lack of exploration, or trying an iterative training procedure would be useful to see and discussion of this failure mode. 