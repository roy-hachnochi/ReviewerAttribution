The paper proposes to combine model-free RL for short-horizon goal reaching with model-based planner over a latent variable representation of subgoals. The method is based on TDMs which decompose trajectories into several subgoals that are connected by the lower-policy. The higher-policy can utilize the lower policy as the model to plan a trajectory to the goal by optimizing intermediate subgoals. To overcome the high-dimension problem in the image domain, the paper reduces the states' dimensionality with VAE and solves the optimization problem on the latent domain.  The paper is clear and the experiments are sound. However, the idea of planning with model-free policy has already been discussed by TDM; using VAE to plan over images is also discussed in the literature. The paper simply combined these two, which is incremental to me.  The paper missed several recent references, e.g.:  Learning Plannable Representations with Causal InfoGAN Learning Actionable Representations with Goal-Conditioned Policies =========================================================== The authors addressed most of my concerns, so I increase the score to 6. Although the significance of the contribution is still limited, it will be a good step towards understanding the connection between modeling and planning in RL settings.