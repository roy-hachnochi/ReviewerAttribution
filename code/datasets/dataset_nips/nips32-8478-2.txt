Since the algorithm of estimating the propensity is proposed by Davenport et al. 2014, the originality of the paper mainly lies in the bounds derivation and experiments. For the bounds of the bias and overall completion error, there is no direct experiments bridging the proposed theory and practice. I would like more empirical evidences on the assumptions from real-world matrices, beyond the recommendation domain where COAT and MovieLens are from. The novelty of the paper is also less impressive when the motivation of investigating the adoption of nuclear norm is unclear.  From the experiments, it is only demonstrated that the proposed propensity estimator can achieve similar results as previous classic methods (and can be even slightly worse if data fits better for Naive Bayes or Logistic Regression). The performance gain of the newly proposed estimator on the MovieLens dataset (the largest experimented datasets) is not very significant compared with Naive Bayes, meaning that when m and n are large the bias and completion error is similar to Naive Bayes. Admitted that the new estimator does not require more features or MAR data, I would still say the established knowledge from this paper is not very significant in its current form. It can do better by considering whether we can use the user/item features and the MAR data when we have them in the 1BitMC algorithm. Can we then largely improve the SOTA?  The paper is in general well written and easy to follow. To make it self-contained, it would be better to introduce some background about nuclear norm.  The authors are also encouraged to spent slightly fewer spaces on the background of IPS related approaches, and introduce a bit more on the 1-bit matrix completion algorithms since it is a closely related work and the algorithm is the working horse for the proposed estimator.