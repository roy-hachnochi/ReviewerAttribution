UPDATE: having read the other reviews and the authors' rebuttal, I have decided not to change the review score.  ----------   This paper proposes an adversarial model for mel-spectrogram inversion. This is an essential step in many modern audio generation pipelines. Mel-spectrogram inversion is challenging because this representation typically contains little to no phase information, and this information is essential to produce realistic sounding audio. While this has traditionally been tackled using signal processing methods (e.g. the Griffin-Lim phase reconstruction algorithm), recently several approaches using generative models have been proposed (e.g. using autoregressive models or flow-based models).  The proposed use of an adversarial model in this setting is interesting: it is well known that adversarial models tend to forgo modelling all variations in the data in favour of focusing on a few modes, in order to be able to produce realistic examples given their limited capacity. This is actually a desirable property in the setting of e.g. text-to-speech, where we are simply after realistic conditional speech generation, and we don't care about capturing every possible variation in the speech signal corresponding to a given mel-spectrogram.  However, success with adversarial models in the audio domain has been limited so far, and most of the literature has relied on likelihood-based models, so I think this is a timely paper.  While the provided recordings of reconstructed and generated speech contain some audible artifacts (short hiccups / "doublespeak" which is characteristic of spectrogram inversion), the results are nevertheless impressive. My intuition is that they would need to be slightly better to be on par with production-level models such as parallel WaveNet, but they are remarkably close. The results in table 3 are impressive as well. The fidelity of the music samples is not so great, however.   Remarks:  - line 19: note that there is no intrinsic requirement for audio to have a sample rate of at least 16 kHz -- this rate just happens to be used quite commonly in literature for speech signals, because it is high enough for speech generation at a reasonable perceptual quality.  - line 36: a very recent paper by Vasquez and Lewis ("MelNet: A Generative Model for Audio in the Frequency Domain") addresses the issue of robotic artifacts by using very high-resolution mel-spectrograms. Although this work wasn't publicly available at the time of submission, and is largely orthogonal in terms of its goals, I think it warrants a mention in the camera ready version of this work.  - line 49: while it is true that WaveNet and other autoregressive models have trouble modelling information at time-scales larger than a few seconds, this does not seem particularly relevant in the context of mel-spectrogram inversion, and the criticism could arguably apply to almost any other model discussed in the paper.  - Related to the previous comment: in some places, the paper would benefit from a bit more clarity with regards to which task is being considered: mel-spectrogram inversion, or audio generation in a general sense. Some statements only make sense in one context or the other, but it isn't always clear which context is meant.  - lines 146-154: the motivation for the multiscale architecture refers to audio having structure at different levels. These "levels" are typically understood to be more than a factor of 2 or 4 apart though, so this motivation feels a bit out of place here. I think a link to wavelet-based processing of audio could perhaps be more appropriate here.  - For the comparison in Table 1, it isn't clear at all whether the same hardware was used -- could you clarify? If not, these numbers would be considerably less meaningful, so this needs to be stated clearly.  - Table 2 is excellent and clearly demonstrates the impact of the most important model design decisions. 