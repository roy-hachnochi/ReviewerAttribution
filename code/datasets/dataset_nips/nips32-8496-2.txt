The paper proposes a novel problem in the area of counterfactual learning. None of the previous work has explored this. The paper would be a good fit for the conference and a good contribution to the area of machine learning given the authors can fix a few minor issues with the explanations and text.  The paper is well written with only a few typos. Most of the results and experiment setups are clear. Proofs are present in the appendix but the major content is still included in the main text.  1) I really like the idea of certifying fairness through a held out safety dataset and suggesting that no solution exists and more data needs to be collected to find a feasible fair solution. 2) I am still not sure about the name RobinHood maybe this needs some explanation? 3) There is no mention of the support assumption that offline CB works usually make i.e. whether we can even evaluate a policy offline with the given data under the logging policy h_\theta. I am assuming it will be a part of the CandidateUtility function (or the argmax thereafter) but should it make a difference? if yes, in what way? 4)  There are simpler baselines that the paper could have compared to. For example, one could use a Lagrangian multiplier to impose the same fairness constraint g_theta while learning using gradient descent on an IPS/SN-IPS estimator (Swaminathan&Joachims). This would already do pretty well in my opinion. (This could be Naive FairBandit but algo 7 seems missing from the appendix). 5) I have not carefully gone through the proofs. But I would suggest that the paper contains a description of the inflateBounds argument in the ComputeUBs method. Why is it needed to compute the candidate utility but not when certifying fairness? 6) A uniform random policy would be a fair one (for example in case of the Tutoring system example Section 6.1). Wouldn't it be ideal to return that policy as the solution rather than an NSF? Or even better, if we can find a suboptimal utility policy that satisfies the fairness criterion, shouldn't it be the idea to return that. Also, is there a way to say if given an estimator for g_theta, there might not exist such a policy? For an equal opportunity kind of constraint, my opinion is that the uniform random policy always satisfies the fairness constraint. 7) Some minor clarifications and typos: - line 252: Is the expectation E on the policy \pi_theta - eqn in line 296: it should be P(A=1|caucasian)... rather than P(A|Caucasian)=1  