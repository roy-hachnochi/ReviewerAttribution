This paper extends the previous GCNs to handle the hypergraph by proposing a HyperGCN variants. Although there are many works have done the similar effort, this paper reduces the computational complexity to the linear scale via a Hypergraph Laplacian operator.  However, the main concern is that other advantages of the proposed method in this paper is not clear except the reduce of the computational complexity. This confuses the reviewer why the performance of HyperGCN outperforms previous Hypergraph neural networks since they use more complete information.  Besides, the extension of HyperGCNs with mediators and the Fast version are incremental and straightforward without any effort based on previous works. The authors have not given some useful insight based on the extensions. Finally, the experiments lack of thorough comparison more recent Hypergraph models.