In this paper, the convergence of the gradient descent-ascent algorithm is studied for solving min-max optimization problems, including nonconvex PL games and nonconvex-concave games. In general, the paper is well written. My comments are listed as below.   1) Table 1 should be better presented by elaborating on the differences between this work and the existing ones in the problem setup of (1), e.g., convexity, and unconstrained sets (namely, $\mathcal \Theta$  and $\mathcal A$).  2) I am not convinced about Remark 3.8: "one can easily extend the result of Theorem 3.4 to the stochastic setting". Will the stochastic gradient apply to both inner maximization and outer minimization steps? I assumed that the proposed analysis only applies to SGD, right? Will the analysis apply to the variant using Adam? If no, why was Adam used in Table 3?  If yes, please elaborate on it.  3) In (4) and (5), is the radius $1$ chosen for ease of analysis?  #################Post-feedback################## My questions have been answered in the rebuttal. Please be sure to make additional clarification and experiments in the final version. 