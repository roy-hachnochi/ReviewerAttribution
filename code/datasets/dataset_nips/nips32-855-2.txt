This paper proposes a simple algorithm for escaping saddle points in nonconvex optimization. Unlike previous algorithms, the proposed SSRGD algorithm does not need an extra negative-curvature search subroutine and it only needs to add a uniform perturbation sometimes. Thus it is more attractive and easy to use in practice. Moreover, the authors prove that the convergence results of SSRGD improve previous results and are near-optimal now. The authors also give a clear interpretation for the comparison of the proposed SSRGD and SVRG algorithm, which is very useful for better understanding these two algorithms. The paper is well-written and the proofs are easy to follow. 