In this work, the authors study the idea of surfing, the optimization of a non-smooth model by sequentially optimizing a smoothly varying sequence of models that interpolate between something very smooth and the desired non-smooth model.  This sequence of models can be obtained, for example, by snapshots of the model during the training process.  The authors provide empirical studies that show that some models which can not directly be optimized, can be successfully optimized by this process.  This has significant ramifications for solving inverse problems using generative models, as optimization over GANs has been shows to succeed with quite few measurements.    The authors present a clearly defined probability model for networks at initialization, for which they present analysis.  The authors then show that if a sequence of networks is given by a discretization of a continuous flow (in which the global optimizer moves in a Lipschitz way), then a projected gradient descent procedure successfully optimizes the models.    Limitations of the theoretical analysis are that the complexity of the algorithm depends on the number of pieces of the piecewise linear model G that are optimized within each step.  A heuristic is provided for this, but ideally a full proof would be established.  Despite this limitation, the paper does offer an insightful observation and analysis to the field.    The paper is clearly written, with well chosen figures and easy to follow text.  I think the paper could be improved by having a discussion about the tradeoffs involved.  For example, the sequential optimization procedure sounds like it could be quite expensive.  One would need to store a full sequence of models, and reconstruction time may be quite slow.  I would enjoy hearing informed thoughts on the ramifications for all this additional computation involved. 