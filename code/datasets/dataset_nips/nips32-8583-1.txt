This paper introduces a new meta-learning algorithm that combines population-based and point-based optimization. While population based approaches have been very popular in very rugged landscapes, current meta-learning methods are point-based and thus not suitable for optimizing such functions. This work presents two contributions, (1) a new architecture for population based meta-learning. This architecture, while more complicated, can be summarized as follows: each particle is composed of a set of 4 features (gradient, momentum, velocity, and attractions), an attention mechanism is applied to those features together with the hidden state. The outputs of the attention mechanism for all particles are fed into an inter-particle attention together with a similarity matrix. The output of this inter-attention mechanism constitutes the intput of the LSTM learner that outputs the variation on the optimized parameters. The second contribution is the addition of a differential entropy term on the meta-loss that balances exploration and exploitation of the optimization process.  This paper tackles the important problem of extending current meta-learning algorithms to take advantage of population-based training, which is necessary in extremely non-convex problems. I consider that the contributions of their work are novel, especially the proposed architecture. While the work is well motivated, the paper lacks clarity. More specifically, it leaves a lot of important components to the supplementary material to the point that it is impossible to fully understand the approach without reading the supplementary material. The paper could be restructured so all of it fits in the 8 pages limit without compromising readability. In this line, my main concerns in the paper are: How is the P(x*| D_t) defined? It seems a crucial part of your contribution, but the paper lacks its definition (besides citing Chao and She,  2019) The model architecture should be more clearly explained in the main section. A key component of your approach, is the attention mechanism it seems crucial to me that you explain in the main text how this works. Right now, your main contribution it’s explained in just a paragraph. Section 4.1, heavily discusses plots and results from the supp material. Those results are interesting and important, they should be included in the main body. Figure 3 ©, the definition of Q and M should be in the main text, otherwise it’s impossible to interpret what’s that plot means without looking at the supplementary material. Section 4.4 results are entirely in to supp material. Those results are interesting and important, they should be included in the main body.  Regarding the experimental evaluation, the paper would highly benefit of an ablation study. This work presents an architecture that consists of many parts, however it is not clear which parts have significant effects. Regarding baselines, an important baseline to run would be to run the DM_LSTM for k different initializations and pick the best. This would show if your method just benefits from having k independent runs or there’s actually a benefit in the attention mechanisms (given that the particle interdependence is not high).