# Originality  The authors propose to parameterize a monotonic network by parameterizing its derivative. The derivative of a monotonic network is much easier to parameterize because it's only required to be positive, and thus can be modeled by "free-form" neural networks, without special constraints on the weights and activation functions. The monotonic network can then be computed by numerical integration of the derivative network. Backpropagation through the monotonic network can be done by another numerical integration. For all numerical integration computations, this paper proposes to use the Clenshaw-Curtis quadrature approach, which is efficiently parallelizable. The monotonic networks proposed are used for constructing an expressive autoregressive flow model, which is proved to be competitive to NAFs and B-NAFs.  The approach is novel. The difference from previous work is clear. Closely related works such as NAF and B-NAFs are cited and compared.   I also like the explicit mentioning of numerical inversion of autoregressive flow models and the experiments to demonstrate the samples. Although the approach is straightforward, this is probably the first time that samples have been reported for these autoregressive flow models.   # Quality  The submission is technically sound. There are also some discussions on the limitations of this method in section 6. The experimental evaluation part is also largely satisfying, and it would benefit from clarifying the following:  1. In section 5.2, the authors hypothesized that NAF and B-NAF do not report results on MNIST due to memory explosion. I'm wondering why memory should be an issue for NAFs and B-NAFs to work well on MNIST? Why can using UMNN solve this problem? There doesn't seem to be any inherent difference of UMNN that makes it particularly good for memory limited cases.  2. Numerical integration is arguably slow and only provides an approximation, yet it is used for both forward and backward propagation of the network. How fast is the Clenshaw-Curtis quadrature algorithm? I noticed that the authors didn't check the "An analysis of the complexity" item on the reproducibility checklist. It would be better to explicitly discuss this somewhere in section 2 or 3. Also, accurate integration requires the derivative networks to have small Lipschitz constants. Do you have the results on performance vs different Lipschitz constant and different number of integration steps?  Both NAF and B-NAF showed some theoretical results on uniform density estimators. Is UMNN also a uniform density estimator? I would imagine so, but it would be nice to have some formal proof and discussion on this.  # Significance  As the results are reasonable and the approach is novel, I think this work provides an interesting and useful idea for the field.   # Clarity  The paper is very well written.