The paper presents a novel alternative to ERM that is based on an economic theory called CPT which is composed of an inverse S-shaped probability weighting function to transform the CDF so that the small probabilities are inflated and large probabilities are deflated. The only similar works considering a human loss/reward/risk have been studied in Bandits and RL. I am not aware of other literature that studies in the context of supervised learning for classification, although I might be missing something here.  The paper is well written and very clear in most arguments it makes. There are little to no typos except ones noted below.  Weaknesses: 0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used.  2. In the Introduction, the authors choose to over-sell their work by presenting their work as a "very natural if simple solution to addressing these varied desiderata" where the desiderata include "fairness, safety, and robustness". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al.  and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that.  4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: "A Reductions Approach to Fair Classification" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent.  