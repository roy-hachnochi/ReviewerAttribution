Update: I read authors' responce RE:sampling rate does not tell the whole story - i was suggesting to add information about on average how many instances were used for each of the splits (because it is not equal to sampling rate * total dataset size).  I am keeping my accept rating, hoping that authors do make the changes to improve the derivations/clarity in the final submission  Summary: this paper is concerned with a common trick that a lot of GBDT implementation apply - subsampling instances in order to speed up calculations for finding the best split. The authors formulate the problem of choosing the instances to sample as an optimization problem and derive a modified sampling scheme that is aimed at mimicking the gain that would be assigned to a split on all the of the data by using a gain calculated only on a subsampled instances. The experiments demonstrate good results.  The paper is well written and easy to follow, apart from a couple of places in derivations(see my questions). The method is more theoretically grounded than the common heuristics used and seems effective (it is compared with GOSS which was shown previously to do better than a commonly used trick to calculate the quantiles and use them as splits).  Detailed comments 1) The formulation you are dealing with fits a learner to mimmic a gradient. However most of the implementations use a second order taylor decomposition of the loss (xgboost, lightgbm, tfbt). I assume you can incorporate the hessian the same way - your gain would be sum of squared gradients / sum of hessians. Any reasons why you didn't do that? 2) Line 24 i find this statement somewhat inaccurate. Most implementations don't consider every possible value for each split. Instead, a fixed number of quantiles (say 100) are precomputed and used for split evaluation. you can consider quantile calculation another form of sampling of instances (at least for the part that determines the split). This is not uniform sampling (it is based on frequency). Further, XGBoost for example  when calculating quantiles, assign weights to instances based on their hessian, so it even more complicated 3) Derivations: Line 164 formula 4 - i am not sure how u go from 7 to 8. c_l can be either negative or positive, so when it is negative, the upper bound should be even larger than in 7 Line 167 replacing values on the leaves cl with a constant - i am completely missing an intuition why you think it is valuable. The leaf value depends on the gradients of the instances that fall into that leaf, and moreover, on the mix of gradients. You can replace the counts of instances that fall into a leaf with a constant that depends on the sampling rate, but not the actual values of the leaves 4) Experiments:  - in Table 3 or4 you should also report complexities of methods and how many actual instances were sampled (sampling rate does not tell the whole story) - I assume you don't include as a baseline the splits off the quantiles because GOSS was shown to perform better? I think it should be mentioned, because GOSS is actually not commonly used and quantiles are still the dominant way of choosing splits. Alternatively you can include the comparison with the quantiles calculated when instances where waited by their gradient, to make it more comparable to what you do) - You need to report SE in Table 3  Minor: line 131 which overcome ->which overcomes line 151 leave->leaf line 260 capable for -> capable of