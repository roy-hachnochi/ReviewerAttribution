originality:  Novel. The idea of Minimal Variance Sampling is interesting.   quality:  Good. Both theoretical analysis and explicit benchmark show the proposed MVS can work.  clarity: This paper is well written and organized overall.   significance: somehow significantly. The experiments show that MVS use few samples for the training of GBDT, and thus is faster. However, the overall speed improvement seems not very significant.  cons:  1.The idea of MVS is good, that is the sampled data should be able to approximate the expected squared deviation best. However, the derivation of MVS is not totally rigorous. In Theorem 1, it seems that the authors assume the tree structures are the same with subsampled and full training data. However, training a tree with full and subsampled data can result in different tree structures. In this sense, the \delta defined in the paper is not a true deviation. Assuming that the tree structure is not changed due to subsampling is also acceptable though, otherwise, the analysis could be complex. But it would be better to claim this assumption in the paper.     2. The datasets in the experiment part is relatively small. It would be more interesting to see how MVS affects the training time of large datasets.    3. The source code is not submitted, neither an anonymous github link is provided.    questions:  1. Line 175 to 186 is a little difficult for me to understand.  In line 175, it is said that the first term of equation (9) is responsible for “gradient distribution over the leaves of the decision tree”. However, in (9) there’s no “leaves” being involved, everything is a summation over the training data. Line 178 claims that \lambda is a “trade-off between the variance of single model and the variance of ensemble”. Everything in the derivation of (9) only concerns the information of current tree to be trained, and has nothing to do with the whole ensemble of boosting (which consist of all trees).    2. There’s a typo in formula (8) and Line 168, it should be Var(y_L) instead of Var(y_1). In Appendix, formula (2) should have no “\gamma \ge 0” constraint, since the constraint in (1) corresponding to \gamma is an equality constraint.   