Originality: The authors propose a novel relaxation (to the best of my knowledge) for networks with ReLU activations that tighten previously proposed relaxations that ignore the correlations between neurons in the network. The theoretical results are also novel (although unsurprising). However, it would be useful for the authors to better clarify the computational requirements and tightness of k-ReLU relative to DeepPoly and other similar  relaxations and bound propagation methods like [13] and https://arxiv.org/abs/1805.12514, especially the approximate version (equation (7)) in the paper.   Quality: The theoretical results are accurate (albeit unsurprising) in my opinion. The experimental section is missing several important details in my opinion: 1) The authors say that experiments are performed on both MNIST and CIFAR-10, but the tables 2/3 only report numbers on MNIST. What about the results on CIFAR10? It would be important to see these to understand how well the method scales to a larger dataset. 2) The sizes of neural networks studied in the paper are still tiny relative to networks that have been considered by previous work on neural network verification: for example, the works from https://arxiv.org/pdf/1805.12514.pdf (which has 107496 hidden units, almost 3x the size of the largest network studied in the paper). 3) The authors only report the time limit used for the LP and MILP solvers in their framework. However, these are just subroutines in the overall verification procedure and it would be important to see the total verification time and how it scales with network size. This is particularly important since the method requries solving an LP for each neuron to be tightened - a cost that can grow superlinarly with network size rendering the method ineffective. 4) The authors do not compare against many other classes of methods that have been shown to be effective at scalable neural network verification (in particular, I think comparisons against [13] and https://arxiv.org/abs/1805.12514 would be instructive). 5) It would be instructive to have precise timing comparisons between methods and also study how these comparisons scale with increasing network width/depth etc.  Clarity: I think the paper can be reorganized for better readability. Section 2 does not provide much insight in my mind - it would be preferable to explain the relaxation tightening geometrically rather than through a numerical example. Further, rather than making extensive the comparison to DeepPoly, it would be good if the authors addressed the broader literature in the field and qunatified the precise gains from kPoly over methods used in the literature.  Significance: I believe that the results are potentially significant, offering a way to interpolate between scalable but conservative verification methods and exact but computationally intractable verification methods. However, I do think that the experiments and connections/distinctions from prior work (particularly in terms of computational complexity) need to be fleshed out further to make the results in the paper stronger.