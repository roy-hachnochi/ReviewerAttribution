This work introduces an improved framework that allows abstraction based certifiers to compute the optimal convex relaxation over many ReLU operations jointly, as opposed to relaxing each ReLU individually (which had been the focus of much prior work). Overall, the work is clearly written and organized, presents a clear example illustrating the benefit of this improved approach, and has a solid empirical comparison to some prior work that shows reasonable improvement for some tasks and minor improvements for other tasks. While I tend to vote in favor of accepting this work, I do have some suggestions for improving the paper, and would like to see these comments addressed as well as possible.  Defining the word “precision” - the word “precision” is used many times throughout the paper, but it is not defined. It may be worth defining to avoid confusion with the definition of precision corresponding to “precision vs. recall.” The word is most confusing in Table 3. The column labeled “precision (%)” does not make sense. My best interpretation of the table is that the “precision (%)” column means the % of images certified to be robust. If so, that should be corrected.  Baselines - As far as I’m aware, the most common baselines for certification methods are the MNIST ConvSmall/ConvBig networks and the CIFAR10 ConvSmall network. While I see an improvement over related abstraction-based methods, the results would be more compelling if a comparison can be made to common MILP and LP-based approaches (e.g. [1] for MILP or FastLin/[2] for LP-based approaches. They all have open source code). This is especially important because the kPoly method uses a mixture of LP and MILP solvers, so it would be important to understand the tradeoff when you use a purely LP-based or a purely MILP-based approach. As it is, kPoly already seems relatively slow (and thus, maybe not extremely scalable), so it’s important to see how much benefit it provides over the faster LP approach, or to compare it to a slower exact method like MILP to see how close it is to the exactness of MILP. If DeepPoly is already implementing an LP-based approach, that’s worth clarifying too. - Additionally, robustness certification is most commonly performed with the goal of certifying a relatively robust network, and achieving high provably robust accuracy. Even though the results in Table 3 show an improvement over DeepPoly and RefineZono, the final numbers (45% for eps=0.3 on MNIST, and 23% for eps=0.03 on CIFAR10) are very subpar compared to related works like [1, 2, 3]. If that is the best result that is achievable by this method, I am worried about its usefulness relative to other approaches. Thus, I believe it’s worth trying to apply kPoly to networks where the best known provably robust accuracy is much higher (e.g. the open source models from https://github.com/locuslab/convex_adversarial/tree/master/models_scaled) - Finally, I’d be interested to know if this approach can scale to larger architectures like ResNet, which some other certification techniques do appear capable of handling.  Missing Related work - I believe that [2, 3] are important related papers that provide good benchmarks for comparison, and should be cited. Finally, the work of [4] may be quite relevant as well. It discusses exactly the idea of finding MIP formulations that are optimal when considering multiple ReLUs at a time, and it applies to the problem of verifying ReLU neural networks as well.  Additional Appendix Information - I would appreciate it if the authors could explain how the expression in section 3.3 leads to the simple linear constraints that 2-ReLU uses in Figure 2. I personally couldn’t follow how to do so.  Finally, some minor comments Line 88 - Should be Fig. 2, not Fig 3. Line 107 - Is 1-ReLU equivalent to the LP-based formulation of FastLin/[2]? It’s worth explaining how they are related. Figure 3 - Why are there two figures? What are (a) and (b)? Table 2/Table 3 - I think the final row is missing the dataset (CIFAR10)  [1] Tjeng, V., Xiao, K. Y., and Tedrake, R. Evaluating robustness of neural networks with mixed integer programming. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HyGIdiRqtm.  [2] Wong, E. and Kolter, Z. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning (ICML), pp. 5283–5292, 2018.  [3] Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin, C., Uesato, J., Mann, T., and Kohli, P. On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.  [4] Anderson, R., Huchette, J., Tjandraatmadja, C., and Vielma, J. P. Strong convex relaxations and mixedinteger programming formulations for trained neural networks. arXiv preprint arXiv:1811.01988, 2018.  *** After Author Response ***  After reading the author responses and other reviews, I have decided to maintain my score of 6. I appreciate that the authors tried to run their method on one more standard benchmark from Wong et. al. However, there is not too much I can conclude from 100 out of 10000 test images being verified. I hope that if the paper is accepted, that the authors use their method on the benchmark architecture for all 10000 test images (or at least 1000 of them - 100 is too small). Also, it would be good to report how long each certification procedure took on these larger architectures too.