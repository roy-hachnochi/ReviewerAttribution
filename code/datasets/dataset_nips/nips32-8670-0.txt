     Convergence of existing Stein variational methods is known to suffer in high dimensions due to the locality of the kernel. The authors address this problem by exploiting the structure of the posterior distribution. Concretely, they propose to perform Stein gradient steps in a low-dimensional projection subspace. The basis of the projection space is derived from the expected Hessian of the log-likelihood, where the expectation is adaptively approximated by an empirical estimate.          The introduced projection scheme and the corresponding Stein gradient steps are well motivated and presented. A theoretical analysis is presented to bound the bias introduced by the projection. Empirical experiments are performed to validate the effectiveness of the method for a linear and a non-linear inference problem.          Some remarks:  - The paper mentions a few important details without further discussing/studying them. For example, the authors assume that the update from the prior to the posterior in the subspace complementary to the projection subspace is negligible. However, there is no discussion or experiments whether/when this assumption holds, and what's the impact on the introduced bias (i.e., Theorem 1). What if the approximation of the avg. Hessian is poor, e.g., because a (too) uninformative prior? Similarly, Algorithm 2 (Adaptive pSVN) is a bootstrapping version of pSVN which is presented with a motivating idea only; no further analysis or experiments.  - The presentation and theoretical analysis is restricted to Gaussian likelihoods, whereas the proposed method should be applicable to a broader class of densities (essentially all inference tasks with differentiable log-densities). The method could be presented for this more general case (keeping the analysis for the Gaussian likelihood).  - The notation and text does not clearly distinguish mappings and arguments; e.g., the Fr√©chet derivate and the preconditioner in Eq. 7 are mappings from R^d into R^d and R^(dxd), respectively; gradient g (line 81) and projection x^r (Eq. 13) are mappings, too. Similarly, mappings and operators are mixed; cf. Eq 5 and 6. It should also be noted that H_{mn} in Eq. 11 denotes a dxd dimensional matrix rather than a single entry of matrix H. While for most parts of the text, the meaning is clear from the context, I would prefer a more stringent notation.  - The experiments focus on a simple linear Gaussian inference problem, and a non-linear inverse problem. It would be interesting to evaluate the method for other models such as hierarchical and mixture models. The method may also work for non-Gaussian likelihoods.       