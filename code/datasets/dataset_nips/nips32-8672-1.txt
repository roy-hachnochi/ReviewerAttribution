The authors studies the inconsistency between the training and evaluation of word embeddings. Specifically, the training phase consists of an optimization procedure that usually involves a low-dimension approximation of a representation matrix (e.g. the word co-occurrence matrix). In the evaluation phase, the embeddings are evaluated against an objective that is usually unitary-invariant (e.g. involving only inner-product of vectors).  The inconsistency between the training and evaluation objectives has to aspects: - Without any additional constrain or regularization of the training objective, the obtained embeddings are usually non-unique (the identifiability problem). - More importantly, since the training and evaluation phases are completely detached from each other, it is actually not clear why word embeddings should work for these evaluation tasks (the "meta" problem to which I am always craving for an answer!).  The authors primarily studied the identifiability problem, and noticed that the non-identifiability could potentially cause problems. I appreciate that a mathematical characterization is presented. Yes, if we look at the training objective alone, it has more "degrees of freedom" than the evaluation objective, which is only subject to invariance under unitary and constant multiplications. This is a discrepancy.  Whenever there is a discrepancy the natural question to ask is: which side should we fix? Evaluation is what it is, and it's our actual goal (we want embeddings to work well on them). In my opinion I do believe the non-identifiability issue is an artifact of the training objective function, which could be quickly fixed with some additional constrains or regularizations. Having an objective like ||X-UV|| is certainly not enough as V can be any full-rank matrix spanning the same column subspace, and this is clearly undesirable (for example, one can pick weird ones like the first column has very small norm and last column has very large norm). The previous methodologies (like skip-gram, LSA or word2vec) implicitly addresses this issue already, as they place implicit constrains (like requiring U and V to be -- either exactly or statistically -- symmetric). In the paper the authors made this point more explicit.  The paper made a niche contribution in analyzing this identifiability issue. But I feel overall the part that is lacking is the "so what?" question. As previous methodologies already implicitly constrained themselves to avoid this identifiability issue (or more precisely, the incompatibility issue as their embeddings are still non-unique up to only unitary-transformations, which the evaluation objective does as well), it is hard to come up with an established example that are severely screwed by this issue. So in general I feel it can be more productive in looking for examples where - we can benefit by looking at a carefully selected larger space (a good example is "Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation" which won the CoNLL '18 best paper, although they are more empirically focused), or - how to modify the training objective function to make the solutions compatible with the evaluation objective (i.e. we want solutions to be identifiable up to only the transformations we want, namely unitary transformations and constant multiplications), and - whether this can lead to new embedding methodologies that are theoretically more sound and performs better on these evaluation objectives?  There are a few examples in the paper discussing the above points (in Section 4) and summarized a few strategies (constraining and exploit symmetry, which people are using already), but I feel there can be more, especially in terms of new methodologies and experiments. This was the point I enjoyed about the Levy and Goldberg "Neural Word Embedding as Implicit Matrix Factorization" paper; they did not stop at the analysis part, but they actually proposed new methods based on their analysis and showed that they outperform the old ones on a few tasks. I appreciate the fact that the authors made the non-identifiability issue explicit. On top of this, it will be great if we can see what we learned and how the methods should evolve. Towards this end, I like Section 4.2 since it provides a new idea, but I feel it is still some distance away from being full-fledged. The authors did not propose a systematic approach and the experiments seems a bit ad-hoc.  ############################################################ I've read the rebuttal. The authors addressed my concerns reasonably well.  By "symmetric" I meant that they are statistically equivalent, meaning the objective will not change if we re-denote U as V and V as U, the entire procedure is statistically identical (hence they should have same singular values, for example). In the matrix factorization scenario, this effectively requires that if M=UV^T, then U and V must each share half of the spectrum. Or, think about the following scenario: the objective in word2vec only concerns u^Tv; as a result, a reasonable learning algorithm (like SGD) will treat u and v equally, acting effectively as an implicit regularization.