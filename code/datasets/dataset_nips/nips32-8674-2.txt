In general, I think the quality of this paper is good and it is written in a clear form. I think the content of this paper meet the quality of the conference.   However, I have a concern about the title of the paper. Is it appropriate to use the term of "generalization error". From my understanding, generalization error refers to the difference of error on testing set and training set, given the same parameters. The error bound in Theorem 5 exactly shows the generalization error. However, I'm not sure whether the other theorems refers to generalization error.  (1) In Theorem 2 and 3 , lower case (x,y) refers to test data. LHS is testing error with expectation over (x,y). The first term in RHS does not contain expectation over (x,y), so this is strange. I think it should be just L(h*), please check. If it were L(h^*), it is Bayer risk, not training error. So I think Theorem 2 and 3 can not be called "generalization" error. They should be called "compression error" and "quantization and compression error".  (2) In Theorem 6, similarly both terms in the LHS refers to errors on test data, so it should not be "generalization error". Also, L_Q(\hat{\beta}*_Q) is already expected over Y|R so there is no need to take expectation over Y and R.  I'm not blaming the significance of these bounds, but I think there should be a more principled way to make the connections over there three bounds, rather than using the term "generalization error".  Minor suggestion: I wish to see more discussions about previous work on quantized compressive learning. The authors provide some references in the introduction, but I wish to see more detailed comparisons of results of the previous work and the current work. This will make the main contribution of this work more clear.  ============================================================== I'm happy that the authors have answered my question well by providing a good reference. I also learned something new from their feedback. I would like to increase my score from 6 to 7.