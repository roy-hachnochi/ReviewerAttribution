SUMMARY: The authors study the problem of reinforcement learning (RL) when the learning agent has access to state observations from a demonstrator in additional to an environment reward signal. More specifically, the authors wish to leverage the demonstrator-provided state observations to improve the learning efficiency of the learning agent. To do so, the authors propose the use of "e-stops" in order to guide the exploration of the learning agent.  STRENGTHS:   * The authors propose an attractive and simple way to leverage state-only demonstrations in combination with RL.   * The authors provide extensive theoretical analysis of their algorithm.   * The proposed method produces convincing empirical results, albeit in a single, simple domain.  WEAKNESSES:   (A) The authors miss some related work that needs to be commented on for the reader to better understand the contribution:     (1) The concept of not straying too far from the demonstrator's behavior sounds similar in spirit to, say, DQfD [R1], where there is a large cost associated with deviating from what the expert would do. Is there a straightforward modification to that algorithm that would apply to the setting considered here?     (2) The concept of e-stopping sounds similar to the recently-proposed notion of "shielding" [R2], where one might provide a set of "safe states". How is the proposed method different?      [R1] Hester et al. "Deep Q-learning from demonstrations." AAAI. 2018.     [R2] Alshiekh et al. "Safe reinforcement learning via shielding." AAAI. 2018.   (B) Experiments in more domains would have been more convincing, as would providing any (even exceedingly-simple) baseline for Figure 3 that uses the state-only demonstrations in addition to the environment reward.  MINOR COMMENTS:   * The authors should define the term "e-stop" for the uninitiated reader.