The interesting part of the paper is a new analysis on the same quantity of the embedding error that has been repeatedly studied in the previous works. The previous approaches fail to generalize to either s > 1 or vectors with a general ell_infty/ell_2 bound. Previously the quantity of embedding error is usually treated as a quadratic form in Rademacher variables and the Hanson-Wright inequality can be applied to obtain moment bounds. However, this does not give tight bounds for v(m,eps,delta,s). Instead, this paper uses a more specific inequality on Rademacher quadratic form from Latala, and a bound on the weighted sum of symmetric random variables (originally proved by Latala, while the authors provided an alternate proof). The rest seems laborious case analysis and I wasn't able to verify everything.  The author clearly intended to connect this sparse-JL problem to machine learning and thus branded “feature vectors” in the main body of the paper, but the connection is not really not explained. I would recommend the author to drop this superficial phrase and make it a cleaner math paper.  Minor comments: - Do not need to write the subscript e for natural log. Use ln instead or just say that log stands for natural logarithm, as this does not affect the asymptotic orders. - Notation inconsistency: the distribution is mathcal{A} in the main body but \mathscr{A} in the supplementary - Last line of the statement of Lemma 3.2, change “...>=...” to “>= … >=” - First line below Section 4: missing a right bracket ) in ||Z_r(...)||_q. There are other places of missing closing brackets, please check the paper thoroughly Lemma 6.1: Change “x_i <= \nu” to “|x_i| <= \nu” - Page 12 of supplementary material: in the bottom equations, what is the supremum taking over? The condition is not clear. Is it T >= max{...,...,...}? This error occurs through the proof. - Lemma 8.1: Isn’t the first sentence redundant? 