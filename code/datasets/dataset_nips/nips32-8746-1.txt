#####   Thank you for your feedback. I agree with R3 that you did a poor job on relating your work to existing methods, in particular SARAH. Please also make sure that you carefully address the question of optimality.  I also realized that your method in fact has nothing to do with momentum. Consider for instance deterministic objective, f(x, \xi)=f(x). If one has a tight estimate, i.e. d_{t-1}=\nabla f(x_{t-1}), then from your update rules it follows that d_t=\nabla f(x_t), i.e. the method become gradient descent with no momentum! Your title, thus, is very confusing and I highly encourage you to change it. Otherwise it will pollute the literature on real momentum methods.  Please provide some basic extra experiments in the supplementary. I suggest you to 1) check the difference between uniform random sampling and random permutation 2) do some grid search to show how sensitive your method is to the parameter selection. I agree that the theory is not directly applicable here, but since you decided to do some experiments, it's better to do a proper study rather than a shallow observation that it works.  #####  First of all, let me note that in work [1] there has been developed a very similar method. However, I believe it is rather a coincidence since 1) works use significantly different stepsizes, 2) work [1] uses two data samples per iteration, while this work only uses one, 3) the analysis is different.  The fact that the method achieves optimal complexity for nonconvex optimization is the main reason I vote for acceptance. The method is simple, and it's clear that it brings variance reduction closer to being practical for training neural networks. The lack of experimental results disappoints me, but I still think the contribution is significant. Probably the most important part is that the authors manage to do here is working with expectations without large batches. However, based on the proof I feel that it's mostly due to assumption that all functions are almost surely Lipschitz in terms of their gradients (for instance it's used in Lemma 2, which is a variance bound, suggesting why Storm doesn't need large batches to control the variance). In contrast, SVRG and SARAH work without this assumption meaning that the results presented in this work are not strictly better.  Several experimental details are missing. It's interesting what minibatch sizes the authors used, and otherwise the plot with the number of steps does not tell us how many passes over the data you made.  How did you sample data during training? It's common to run SGD or Adam with random reshuffling, while your theory is for uniform sampling. Please range y-axis from 0.8 to 0.95 in Figure 1(c) to make the comparison clearer.  Clearly, I wouldn't have had this questions if the authors provided their code. Are the authors going to release their code online along with values of c that they used to get the plots?  Minor comment: in line 99, did you mean to write "e.g." instead of "i.e."?  [1] Hybrid Stochastic Gradient Descent Algorithms for Stochastic Nonconvex Optimization