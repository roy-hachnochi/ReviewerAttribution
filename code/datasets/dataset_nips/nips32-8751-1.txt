- The first step in the approach involves the offline learning of a goal-conditioned policy and value functions with distributional RL. The learned policy's inputs are a current state, goal state pair (s, s_g) and must choose an action which will reduce the distance to the goal state. The learned value function now also depends on a pair of states and can therefore be used as a prediction of distance between a (s, s_g) pair. The quality of the distance function is central to the subsequent approach.  - Next, the method uses the learned distance-predicting value function to induce a weighted directed graph G over the state space. The vertexes in G are taken from the replay buffer, capped to 1K states (e.g., images) although the appendix suggests the method works with smaller replay buffers as well. I'm not entirely sure how the replay buffer is initialized in an entirely new environment at "test" time and I was unable to find any details in the paper describing how the replay buffer is actually constructed during agent evaluation. Perhaps it's sampling the 1K states from the trajectories generated during the RL step? But the replay buffer may also be empty which suggests that it's being constructed from scratch at test time and updated as the agent explores. In which case, how does the agent decide which states to keep if it gets full? Recency or some other utility function over states? Perhaps Algorithm 1 ought to mention how the buffer is constructed and updated at test time.  - Next, the problem of navigating to a distant goal state can be decomposed into a sequence of simpler intermediate navigation tasks to waypoints, which are intermediate states in the graph constructed over the replay buffer. This combination of learned behavior (the goal-conditioned policy and value function predictions of distance) with graph-based planning allows the agent to do well on tasks like visual navigation, which have high-dimensional search spaces, sparse goal-based reward and very long-term temporal dependencies (e.g., moving away from the goal in order to find doors and hallways).  - Overall, the primary contribution here seems to be a novel method for learning the length of the shortest path (in [O, T]) between any pair of high-dimensional observations as well as a policy for navigating between them, using distributional RL. This makes the method particularly well suited for the task of visual navigation well. I'm not sure if / how the proposed approach might generalize to other types of benchmark tasks (e.g., Atari, MuJoCo, RTS games like StarCraft, etc.). I think the reader would benefit from a better discussion of the types of settings (besides navigation) where this method is applicable.  - Experiments on the effect of the replay buffer size in the appendix were informative and a bit surprising to me that performance didn't deteriorate more although the experiment with MAXDIST confirms that the replay buffer is very important to the approach. The question of what constitutes a "good" replay buffer seems rather important to the overall method. A better understanding of how to optimize the replay buffer in terms of computational tradeoffs, which states to include, etc. would be very interesting to read although it does seem out of the scope of this paper.  - The experimental section on SORB itself is nicely detailed and the results on the visual navigation task are strong. The baseline is a relatively recent state-of-the-art method so that seems like a strong baseline. Perhaps the sample complexity and computational complexity of SPTM versus SORB could be discussed in more detail.   - Given the clear importance of the learned distance to the overall performance shown in Fig 7, I think the empirical section could be strengthed with evaluations of other methods that learn compact / abstract state representations (e.g., [14]). Compact search spaces would confer computational benefits if nothing else. Overall, studying how compact representations of the state might might compare when used inside graph search seems like a nice way to evaluate just how much utility is added by the distributional RL component of the overall approach.   - Overall, the description could be improved with more empirical details about the replay buffer (besides the effect of size) as mentioned above. Besides that, the evaluation of SPTM itself is quite rigorous and the ablation studies reveal the key factors impacting performance. With additional experiments on other distance metric methods and a more detailed description of key components, this could be a nice addition to the literature on combining learning and planning.  UPDATE: I thank the authors for their detailed response. After reading the other reviews and the authors response, I reiterate my score. 