Post rebuttal: My suggestions/comments were not addressed in the rebuttal, so I keep my score as is. --------------------------- This paper focuses on the navigation problems in RL setting and proposes to represent the environment with a graph where the states in the reply buffer become nodes and the low-level RL policy connect the states as the edges. The high-level planner then finds the subgoal by solving the shortest path problem on the graph.  Strength: + Building graph with RL value functions is very novel and provides a new way to model the environment and do the planning. + The technique is solid and the paper is well-written.  + The paper discussed both useful practice and also contained fail experiments.  Weakness: - Although the method discussed by the paper can be applied in general MDP, the paper is limited in navigation problems.  Combining RL and planning has already been discussed in PRM-RL~[1]. It would be interesting whether we can apply such algorithms in more general tasks. - The paper has shown that pure RL algorithm (HER) failed to generalize to distance goals but the paper doesn't discuss why it failed and why planning can solve the problem that HER can't solve. Ideally, if the neural networks are large enough and are trained with enough time, Q-Learning should converge to not so bad policy. It will be better if the authors can discuss the advantages of planning over pure Q-learning. - The time complexity will be too high if the reply buffer is too large.  [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning 