The paper investigates whether it is possible to convert the pre-trained language model into the generator that can be conditioned on the vector representation of the sentence. This conditioning can be made by adding the vector z (in one form or another depending on the dimensionality) to the hidden states of the LSTM LM. The sentence representation can be found by employing nonlinear conjugate gradient descent for maximizing the probability of the given sentence where optimization parameters are represented by the components of the vector z. Basically, the encoder of a sentence is represented by the optimization process. For sentence decoding, a beam-search is used. For sufficiently big LM almost perfect sentence recoverability from the Z space is possible.  The paper explores an important direction considering recent development in unconditional text generation. However, it is not exactly clear how to apply this approach to attention-based language models (e.g. GPT), adding such discussion will clarify the limitations of the proposed method. The paper is well structured and contains all the necessary bits to understand the proposed method clearly.