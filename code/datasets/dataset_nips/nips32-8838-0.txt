1) Summary of the paper   The paper introduces a randomized version of the truncated kernel ridge regression algorithm. The algorithm essentially selects a random subset of training points and learns a (truncated) kernel ridge regression function on the selected subset. Under certain characteristic assumptions on the complexity of the function class in which the optimal function lies and on the complexity of the RKHS, the paper shows that the algorithm achieves optimal generalization guarantees. This is an improvement over the existing results in this setting in one of the regimes of the problem space. Additionally, the authors show that under a zero Bayes risk condition, the algorithm achieves a faster convergence rate to the Bayes risk. The main contribution of the paper lies in adapting the proof techniques used in the online kernel regression literature to the standard kernel regression setting.  2) Main comments  The structure and writing of the paper are fair and easy to follow, and the results are put into perspective with respect to the current literature. The paper also provides clear mathematical intuition behind the various assumptions made for the analysis. It is also fair to argue that the assumptions are standard in the kernel regression literature.   However, the usability of the results requires that the assumptions are relatable to some practical setting. This is not provided/discussed in the paper. For instance, it is not clear how the assumption on the eigenvalue decay of the integral operator translates to conditions on the underlying distribution and the kernel function thereby limiting the usability of the results. On a similar note, the regime under which the algorithm improves over existing results needs further discussion to emphasize the relevance of the obtained results.   One of the motivations of the paper are to address the discrepancy of the role of regularization in theory vs practice. The authors address this issue under the setting of zero / near zero Bayes risk condition. The authors briefly justify the usability of this setting through a visual classification example. However, unlike in the classification setting, this seems to be a very strong assumption in the regression setting (a distinction the authors clearly make in the discussion of their theoretical findings).  Furthermore, the main contribution of the paper stems from the adaptation of the techniques used in the online kernel regression literature to the kernel regression setting. A further discussion and emphasis on ways to adapt the techniques to more general settings in kernel regression could have further improved the usability of the paper.   3) Minor comments and typos  l. 36: missing ' after "easiness" Algorithm 1 is often referred to as Algorithm 3 in the paper. Algorithm 1: if I understand correctly, the sampling is done after solving n minimization problems. Why not first sample k, and only compute f_k?  l. 227: missing space after "Figure 5"  4) After rebuttal and discussion  The rebuttal adequately addressed my concerns, and I am increasing my score. I think the paper should be accepted.  