This paper studies the block convolutional orthogonal representation for convolutional neural networks in presence of Lipschitz constraints. During training of the network gradient norm preservation is utilized to combat gradient attenuation.  This simple combination of the two main ideas outlined above is shown to be useful in two settings, namely adversarial training and computing the Wasserstein distance using Kantorovic duality.  Admittedly, I might have missed important pieces of the paper, but I regret to say that the paper is limited in terms of its novelty. As is, the paper feels like a combination of two existing ideas from Anil et al. and Xiao et al.  Having a broader set of experiments could have made up for the limited novelty of the paper, I would argue. For example, the Wasserstein computation is a crucial step in performing Wasserstein GAN, so I was really curious to see if the benefits ultimately transfers to better generative modeling. On the same note, another application could have been learning stochastic models of the world in reinforcement learning, where it is is important to compute models with low Wasserstein errors.