EDIT:  I have read the author response and appreciate the effort made by the authors' to address review suggestions. Unfortunately, the empirical results given in the author response appear to detract from, rather than add to, the significance of the work. Deep networks produce less useful features than shallow networks. Since the similarity-matching-for-shallow-networks method was already present in the literature, it is unclear what benefit the method being presented authors. The authors provided no comparison between their structured network approach and an unstructured network, so it is not possible to tell if their other contribution provides any empirical benefit. Without such a contribution, I find it difficult to recommend this paper in its current form for publication, particularly in light of the point mentioned in my initial review that the paper's main conceptual insights are drawn from prior work.  Further experimentation or refinement of the method may address this issue, and I would love to see an updated paper that does so.  ORIGINAL REVIEW: This paper generalizes recent work that connects similarity matching to particular Hebbian / Anti-Hebbian dense and shallow network architectures, to the structured and deep network case.  The mathematical derivation is clear, particularly in the context of the papers it builds on.   However, I have concerns about the novelty and significance of the result.  1.  The feedback architecture is essentially the same as that used in contrastive Hebbian learning, a model not mentioned explicitly in the paper though cited implicity (citation 20, Xie & Seung).  Moreover, Xie & Seung proved that contrastive Hebbian learning can approximate (or in a certain regime be equivalent to) backpropagation.  Thus this paper appears to, in effect, stitch together the insight of the earlier literature on relating similarity matching objectives to Hebbian / anti-Hebbian learning rules and the insight of older literature on the ability of contrastive Hebbian learning to use feedback and local learning rules to assign credit in deep neural networks.  Moreover, Bahroun et al. arrived at a similar architecture excluding the feedback component. To me, this scale of theoretical contribution warrants publication only if it leads to meaningful empirical advances, which (see below) I don't find to be demonstrated in the paper in its current form.  2. More explanation is warranted for the value of similarity matching as an objective for deep networks.  After all, the goal of many deep learning problems is to radically warp the similarity metric between data points!  Similarity matching in the linear case has a nice interpretation in terms of PCA, but the authors do not give a similar interpretation to the present model (e.g. a relationship to autoencoders).  3. The empirical results are uncompelling.  First, the role and value of feedback, which is the main contribution of this paper beyond Bahroun et al., is not demonstrated empirically.  Second, the features learned by deep networks are not compared to those learned by a comparable shallow model.  Thirdly, no quantitative evidence of the quality of the learned features (e.g. their utility in a downstream supervised learning task) is given.    4. This paper does not claim to aim for state-of-the-art empirical results, but rather to introduce a biologically plausible approach to unsupervised learning.  As such, the biological plausibility of the model is very important.  The proposed model involves an obvious weight transport issue, which the authors acknowledge and propose a workaround for (namely, that Hebbian learning of both forward and backward weights will tend to cause them to become transposes of one another).  While an interesting argument, I am not entirely convinced that it is robust to biological noise, other sources of learning, etc.