——Rebuttal Response——  It looks like my score is a bit of an outlier here. I took some time to read one more time the paper, all the discussions, and the author’s rebuttal. While I hear the objections, my position remains unchanged. This is a very interesting paper and I think that it should be accepted. This paper is indeed a first look at the problem, rather than a fully conclusive and executed research program, but I think it contains an interesting idea.   Compared to the previous work on similarity matching, this is the first one that extends similarity matching cost function to networks with several layers and structured connectivity. In my view, this is a huge step. It is also nice that the authors provide a reasonably “biological” implementation of their optimization, yes, with a little bit of weight transport, but nothing is perfectly biological anyway.      Regarding table 1 of the rebuttal. I would encourage the authors to invest enough time to experiment with the hyperparameters and clearly state in the final version where they stand. If it turns out that deep architectures learn representations leading to a better accuracy, this is an interesting statement. If it turns out that shallow architectures lead to a better accuracy, this is an equally interesting and important statement. Since the proposed algorithm is different from the traditional backpropagation learning both results are possible.   I agree with reviewer 3 that the paper is hard to follow. I would encourage the authors to invest time in making the presentation more clear, as they promised in the rebuttal.    ————————  There is a large body of work devoted to approximating backpropagation algorithm by local learning rules with varying degree of biological plausibility. On the other hand, the amount of work dedicated to investigating various biologically-plausible learning rules and their computational significance for modern machine learning problems is rather small. This paper tackles the second problem and demonstrates that a network with structured connectivity and a good degree of biological plausibility is capable of learning latent representations. This is a significant statement. For this reason I argue in favor of accepting this paper.   Specifically the authors study similarity-matching objective function, and show how it can be optimized in deep networks with structured connectivity. To the best of my knowledge these results are new.   While the general idea and results are clear, I have several technical questions throughout the paper:   1. There seem to be a misprint in Eq (2). Is y_i the same variable as r_i?  2. I do not understand the “Illustrative example” in section 6.1 How were the inputs constructed? What does it mean that they were “clustered into two groups and drawn from a gaussian distribution”? There are many ways how one can do this. Is it possible to show examples of the inputs?  Without this, it’s hard to tell how non-trivial these results are. In general, this section would benefit from more detailed explanations, I think.   3. Figure 3 is impressive, but the description of how it is constructed is insufficient, in my opinion. I would appreciate a clear explanation of the sentence “Features are calculated by reverse correlation on the dataset, and masking these features to keep only the portions of the dataset which elicits a response in the neuron”. Maybe in supplementary materials?   4. What is the interpretation of the features in the first layer? Are they PCA components of the inputs or are they something else?   5. I understand that learning rules (2) are needed for the optimization of the similarity-matching (3). Out of curiosity, would something like figure 3 emerge if in equation (1) the weights L were fixed and all equal to a positive constant (global lateral inhibition), or is it crucial that the weights L are learned? In a recent paper https://www.pnas.org/content/116/16/7723, for example, a network with constant L was shown to lead to high quality latent representations. Are the representations reported in this submission in some sense “better” than the ones reported in the above paper (since the weights L are learned), or is learning the weights L simply necessary to make a connection with the similarity matching objective function?  