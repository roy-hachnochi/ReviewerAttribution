II think there might be good stuff in here, but it was pretty abstract for a neurips paper.  A more technical journal might be more appropriate.    I believe this is the setup.  You have a bunch of iid samples of the form - X_i ~ N(0,Sigma) - Y_i ~ N(theta^T X_i , sigma^2) where there are some j for which theta_j=0 and for all other j we have |theta_j|>1. The task is to estimate the set of indices j such that theta_j=0.    To solve this problem: 1) you design a way of making knockoffs (in this case I think that amounts to designing a conditional distribution Xtilde |X so that (X,Xtilde) has all the nice knockoff properties) 2) you solve min [(Y - theta1^T X) + lambda |theta1|_1]  3) you solve min [(Y - theta2^T Xtilde) + lambda |theta2|_1]  4) You sort the indices by |theta1_j| - |theta2_j|, and take the ones which are most impressive (with a carefully chosen threshold)  Some things that made me nervous: 1) in Definition 1 it says the amount of noise for each Y_i is actually proportional to the number of samples, n.  It is written N_i ~ N(0, nsigma^2), which I don't understand at all.  Why would that be? 2) it is never explicitly stated in definition 1 that X_i ~ N(0,Sigma), but I assume this is the case? 3) Any nontrivial example of how to design gaussian knockoffs would be nice.  For example, say X is a tree graphical model.  How might I make my knockoffs?  Apparently "conditional independence knockoff" gives me a way, but I didn't understand how you actually design the distribution. 4) you call your regression estimates "lasso coefficients."  That tells me that you must have fit the regression using lasso.  But you didn't actually say that, so I was nervous that I might be confused.   5) I can't actually tell whether maybe you fit both lasso's simulatenously, i.e. you fit min [(Y - theta1^T X + theta2^T Xtilde) + lambda |theta1|_1 + lambda |theta2|_1].  6) I conclude that the nonzero theta_j satisfy |theta_j|>1 because that assumption is made in proposition 4, but I'm not actually sure that it is made throughout the paper.  But assuming I got everything right, you then go on to show conditions under which your discovery rate was high.  Unfortunately, I couldn't make heads or tails of those conditions.  I can read them and understand their literal meaning, but I have no intuition at all.  The marginal variances Sigma_{jj} seems to have some very important role, I think I want them small.  It almost seems like I need sum_j Sigma_jj to be bounded in the limit as p->infinity with n/p>1 -- but that seems like an absurd assumption, since it would imply that most of the variables are essentially deterministic?  