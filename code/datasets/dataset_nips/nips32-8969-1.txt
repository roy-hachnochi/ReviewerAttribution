The phenomenon of ML having lower accuracy on minorities is well documented in the research literature and in mass media. The standard explanation that models have harder time training on underrepresented classes. The paper demonstrates that differential privacy amplifies this phenomenon by further suppressing learning on classes with smaller support. On the one hand, it is a natural consequence of the guarantee of differential privacy: output of a DP mechanism must be insensitive to the presence or absence of any record; if there are fewer records of a certain class, their collective impact is going to be smaller. On the other, it is not a foregone conclusion, and one can imagine techniques that push back against this effect. The paper is an interesting and timely contribution to this debate.