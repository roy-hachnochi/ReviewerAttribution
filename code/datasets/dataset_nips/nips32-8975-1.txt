Originality: this paper contains an original contribution, which is tightly based on [22]'s SPD networks, where the hidden representations are symmetric positive definite  (SPD) matrices. The originality lies in that the authors adapted batch normalization which is usually performed on real vectors to the SPD representations. Notice that in their batch normalization, the variance is not normalized. The authors may consider to use a different term (e.g. batch centering) because of this.  Quality: the proposed method is particularly useful for SPD networks, and could be useful in other networks with SPD hidden representations. The author should mention the complexity explicitly in computing the exponential map and its inverse. How much computational overhead is involved in applying your batch normalization?  Clarity: the writing is satisfactory and the algorithms are clearly presented. But the math formulae can be greatly improved. In particular in the beginning there should be a small section to explain the Riemannian geometry of SPD matrices.  Significance: this is a deep learning paper based on Riemannian geometry. It can be useful in the particular deep learning architecture SPD networks [22]. It may be also interesting from an application perspective (applying SPDNet to EEG/MRI datasets). From the mathematical standpoint the novelty is limited. Overall I feel that the potential significance is average because of these limitations.  minor comments:  The authors can cite related references is matrix Bregman divergence, Bures distance in deep learning, as well as log-det divergence. Notice that for Bregman divergence one can have the notion of variance.  eq.(1) explain what is "log" (use \log instead of log) Is that matrix logarithm?  L39 It is not clear what "each layer processes a point on the SPD manifold" means  The paper seems to be finished in a rush, table 1 & table 3 are formatted in a bad way.