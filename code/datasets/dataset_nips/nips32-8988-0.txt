Overall, the paper rests on a very clever idea "Instead of choosing a generic object-classification CNN as a source of visual features, we built encoding models with individual feature spaces obtained from different task-specific networks.", that deserves publication. It is indeed probably the most clever idea that I have seen on deep features encoding in brain imaging for years.  The paper has relatively nice figures.  I identified one technical issue: "To determine the significance of the predictions, we calculated the p-values of each correlation 109 coefficient and report false discovery rate (FDR) corrected p-values." -> these p-values are likely problematic, because correlations and structure in the data violate the hypothesis necessary to interpret correlations as p-values (iid etc.).  172-173 "Both Figure 2 and 3 show that prediction results are consistent across three participants or tasks despite anatomical differences in their brain structures." not sure that this statement is useful (it is hard to judge) or whether it is actually true.  It is bad that 6 of the 25 tasks were nor used (or are not presented) in the paper: even if they do not seem meaningful, they may carry some information. If they do not, it is also an interesting point.  the results are slightly disappointing in the sense that the process  * yields overall low accuracy, * does not seem to uncover new functional territories * as a result of low accuracy, only gives functional tendencies   ---not sharp conclusions.  Fig.6 I think that data-derived tree structures are too brittle, and thus doe not support sharp conclusions.  Fig.7 obviously the two graphs are not identical. There seems to be large modules in the fMRI graph not present in taskonomy graph.  Interesting point on covariate shift across images in the discussion.  typos "To better understand underlying the nature of the visual features" " et. cetra"  