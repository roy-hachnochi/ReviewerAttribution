## Quality  It seems a few pieces of crucial logic are missing from this study. First, it seems necessary to know the ceiling brain encoding performance in order to understand how adequate these models are as pictures of neural representation. How well could an optimal model of visual processing perform in brain encoding with this neuroimaging data? The authors could estimate a performance ceiling by attempting to learn an encoding model predicting one subject’s brain activations from another subject’s activations in response to the same stimulus, for example.  Second, I’m also not clear on the logic of the latter analyses and comparison with Taskonomy. The authors claim (line 193) that, because the qualitative analyses on the brain encoding error patterns match those of the Taskonomy transfer patterns, “neural representation of task information is similar to that found through transfer learning.” I don’t think this inference is valid — the only valid inference would be about the model representations, not the neural representations. Specifically, if model A predicts brain activations similarly to model B, then it is likely to transfer across vision tasks similar to the way that model B transfers.  The inference about neural representations is only licensed if we know that all of the variance in the neural activation space is explained by model activations, I think. We can’t know this without an estimate of ceiling performance for this dataset.  ## Clarity  Some (minor) methodological and presentational things are not clear to me after reading the paper:  1. How is cross-validation of the regression regularization hyperparameter fit across subjects? You should ideally use nested cross validation here to reduce the chance of overfitting.  2. BOLD5000 provides images at multiple time offsets after stimulus presentation — are you using the standard average of TR3–4 or something different? Why?  3. Figure 4: There is no interpretation of this figure in the text. (PPA seems to differ in correlation; otherwise I can’t see anything qualitatively important by squinting at these maps.) Also, I assume this is derived from a single subject — which? Do the qualitative patterns (whichever you want to pick out) replicate across subjects?  4. Figure 5: why is the diagonal not always equal to 1? (Are there multiple training runs of the same model input to this analysis? Or maybe this is averaging across subjects?)  5. I don’t understand what the utility of figure 7 is — it seems to be conveying the same message as figure 5, but in a less readable way.   ## Significance  Modulo some of the things I see as logical issues with the paper stated above, I think the claimed result (contribution #3 in my review) would be quite interesting if it were substantiated by the data.  ## Originality  This is an interesting combination of existing methods.