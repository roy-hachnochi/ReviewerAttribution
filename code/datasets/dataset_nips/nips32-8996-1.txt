Originality: The method builds on the previous work on capsule networks but is highly original in its own right. The architecture and learning algorithm of the Constellation Autoencoder are a clever innovation that avoids iterative routing while still learning part-whole relations. Related work is well-cited.  Quality: The submission is technically sound. The empirical claims are well-supported by a thorough ablation study. The authors are honest about the strengths and weaknesses of the work, though more information on the computational complexity of training relative to other methods would be useful and appreciated. They note challenges in SCA under-performs on CIFAR10 and suggest avenues for improvement. That said, this work feels complete as a foundational paper.  Clarity: The paper is well-organized and quite clear given the complexity of the architecture. Those aspects that leave room for interpretation would be remedied were the code made available.  Figures 1 and 5 are very well done.  Figure 2 would be much easier to parse with white and black / colored foreground.  Figures 3 and 4 are too small. Make them full width, moving a figure or other content to the appendix as necessary. Please clarify the difference between the left and right in 4c. L124: “We stop [the] gradient”  L136: Missing period.  Equation (12) has a typo in the middle.  L152: In addition to saying “We find it beneficial to”, it’d be helpful to give the intuition.  L244: Clarify what is meant by a “similarity” transformation.  L404: svhn in wrong font.  Figure 6 has a missing caption. Clarification is sorely needed.  Figure 7 should be full width.  Significance: The paper is a significant advance for unsupervised learning of object semantics, made especially exciting by SOTA performance on MNIST and SNHN without using labels, data augmentation, ImageNet pre-training, or mutual information. While the method is still far from practical real-world scale and applications, researchers are likely to learn from and build on the method and its component ideas in pursuit of learning with efficiency of generalization closer to that of the brain.