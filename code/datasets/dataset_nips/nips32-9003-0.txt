The authors propose a novel approach for solving POMDPs by simultaneously learning and solving reward machines. The method relies on building a finite state machine which properly predicts possible observations and rewards. The authors demonstrate that their method outperforms baselines in three different partially observable gridworlds.  Overall, I found the paper clear and well motivated. Learning to solve POMDPs is a very challenging problem and any progress or insight has the potential to have a big impact.  My main concern about this work concerns the scalability of the discrete optimization problem. The experimental results don’t provide much insight in that regards. How long does solving the LRM take in these gridworlds? How often are the LRM recomputed in practice? While better learning curves do confirm that the algorithm works, they don’t usually provide much insight about how algorithm behaves.  Additionally, without more information about the domain, it is difficult to appreciate the performance difference. For instance, without knowing how many steps are required to traverse the rooms, I can’t know if the 10 steps of history given to DDQN is a too much, enough or too little. I would expect that a sufficiently long history would enable DDQN to solve the task. Have the authors tried this?  While I am disappointed by the empirical evaluation, the conceptual and theoretical contributions has me leaning towards acceptance. There is a considerable amount of work that focuses on improving discrete and hybrid optimization, and being able to make use of that is very appealing.  I found the decision to replace the PSR’s optimization criterion (i.e., Markovian predictions) with the more limited constraints on possible observations quite interesting. Do the authors have any intuition about when this is not a good proxy? When would this lead to less desirable RMs? How does this work relate to [1] which also follows the PSR approach? Additionally, how does this scale when there are a large number of possible observations for some/all reward machine states?  Misc comments: - Line 81, typo “[...] only see what _it_ is in the room that [...]” - Experiments, how many steps does it take to go from one room to another? - Line 294, 295, “the maximum reward”, is it meant to say the maximum cumulative rewards/return? - Why not evaluate each method the same way? This seems unnecessary and a bit odd.  [1] Zhang, Amy, et al. "Learning Causal State Representations of Partially Observable Environments." arXiv preprint arXiv:1906.10437 (2019).  Post-rebuttal: ----------------------- My concerns have been addressed and I recommend acceptance. 