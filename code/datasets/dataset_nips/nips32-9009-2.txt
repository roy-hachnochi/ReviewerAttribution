In the local DIM method, the mutual information is maximized between a global summary feature vector, which depends on the full input, and a collection of local feature vectors pulled from an intermediate layer in the encoder.  This paper extends this method in three ways: 1. It applies data augmentation to generate different views of an image. Then, it maximizes the mutual info between the two views, instead of a single view. 2. It proposes to maximize the mutual info between the features of any layers. 3. It uses a different more powerful encoder.  Issues: - My main issue is the motivation behind each of the novelties of the paper. In subsections 3.4 and 3.5, the paper starts explaining the proposed approach without giving an explanation of why the proposed modifications to local DIM will be helpful. - Are these proposed modifications to local DIM really important? To get the answer, we need to look at the experimental results section. But then we found several issues in the experiments: 1) We do not see the results of local DIM (Hjelm et al. 2019) in the experiments. 2) There is no explanation about the results in table 1 in the text of the paper! This table and its results should be explained in detail such that we know which approach is better! 3) In the caption of table 1, it has been mentioned that "Data augmentation had the strongest effect by a large margin". What we see in table 1 is that multiscale has the largest effect, by a large margin.  --------------- After reading the response of authors: The authors presented a new set of results that shows the method works well. But, my main issue is still clarity and the intuition behind each step of the proposed method. This issue has also been mentioned by Rev#2. It is also crucial to see the result of local DIM in all the tables (properly cited), not just in fig(b) of the response. For these reasons, I keep my score "below the threshold".