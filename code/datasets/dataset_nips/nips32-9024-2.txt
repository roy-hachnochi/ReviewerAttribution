I read the other reviews and the author feedback addresses the raised questions / concerns, in particular about the dimensionality of u and comparison to phased LSTM. I think this paper addresses a relevant problem (limited memory horizon), presents a novel approach (LMU), and nicely analyses this approach theoretically and empirically. I raise my score to 8.   Originality: Even though the proposed approach is based on the work "Improving Spiking Dynamical Networks: Accurate Delays, Higher-Order Synapses, and Time Cells", its transfer to deep neural networks is new and of significance. In the related work, "Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences" could be discussed and also used as a baseline.   Quality: The proposed approach is well described and seems easy to implement from the given description in the text. The provided theoretical formulation for the continuous-time case yields a valuable interpretation / intuition and experiments are well tailored to show-case the strengths of LMU module, in particular the increased memory horizon. Also chapters 4 & 5 describing the characteristics of the LMU and a spiking variant are helpful and provide a summary of interesting features.  Clarity: The work is well written and structured. Figures and tables are of proper quality. It seems, that one could easily reproduce the results with the provided code and descriptions. 