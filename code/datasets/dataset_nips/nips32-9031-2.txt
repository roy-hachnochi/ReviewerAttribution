Originality: The proposed method is original, and to the best of my knowledge the related work regarding latent models is adequately cited.  Quality: The submission appears technically sound. I was able to check the main theoretical results, which to me are correct. The presented experimental results appear convinving, since they cover a wide range of problems. The authors discuss both strength and weaknesses of their approach, i.e., it assumes a jointly Gaussian distribution for correctness, and it suffers convergence issues with nearly-deterministic distributions.  Clarity: I found the paper hard to read. First, the paper suffers from a very dense content, in particular extensive mathematical derivations and experiments, which the authors have decided to place in the supplementary materials. Also, a lot of statements are directly referring to content in the supplementary materials for justification, which brings the actual total length of the paper to 21 pages rather than 8. Still, the main 8-pages body remains understandable on its own. A second point, which I believe the auhtors can fix, is that the paper is quite confusing and even a bit misleading in several aspects, such as the time complexity of the approach, the exact goal of the approach (structure learning ? regularized density estimation ?), or some divergences between the theoretical presentation of the method and its implementation, that is, the factorization p(z|x) = \prod_j p(z_j|x), which are not discussed at all. See the detailled list of things that have to be fixed below: l.8-9: The proposed method has linear time complexity w.r.t. the number of observed variables -> This is misleading. The proposed criterion to be minimized can be computed in linear time, however the whole learning procedure to minimize this criterion is most likely not. l.18-19: is a holy grail in many scientific domains, including neuroscience, computational biology, and finance -> any reference ? l.22-29: For instance, graphical LASSO, [...] have better time complexity but perform very poorly in undersampled regimes. -> What does it mean to perform poorly ? For which task ? Density estimation ? What does it mean quantitatively ? At this point the text is very vague. Please state from the start which tasks those methods or your proposed method intend to solve, and which criterion permits to compare them objectively. l.30-31: a novel latent factor modeling approach for uncovering the structure of a multivariate Gaussian random variable -> This is very confusing. Your approach not only learns the structure of a multivariate distribution, but also does density estimation. There exists a whole litterature in graphical model structure learning which learns the structure only (i.e., just the graph), see [Murphy 2012, Ch.26]. Please clarify this, e.g., "a novel latent factor modeling approach for estimating multivariate Gaussian distributions with modular structures". l.33-34: when each observed variable has a single latent variable as its only parent -> You are suddently talking about parents here, but at this point you didn't state yet that you consider directed graphical structures. There exists undirected structures as well, which can also be latent models. Please clarify. l.36: is block-diagonal with each block being a diagonal plus rank-one matrix -> Why would each block be rank-one ? It is not obvious to me, I first figured this was an assumption you were making. Is that statement important for the paper ? l.36-38: This modular inductive prior is appropriate for many real-world datasets, such as stock market, magnetic resonance imaging, and gene expression data -> Why would that be ? Any intuition behind that ? Evidence ? References ? Figure 1: (Thm. 2.2) / (for Gaussians) -> This is very misleading, since Theorem 2.2 assumes a Gaussian distribution. Or reformulate Theorem 2.2 as "a modular latent factor implies TC(Z|Xi), and the reverse is true for Gaussian distributions". l.40: gets easier -> in terms of what ? Time ? Log-likelihood ? Structural error ? l.44: unconstrained graphical models -> unconstrained latent factor models l.68: constrains -> constraints Theorem 2.1: This is trivial, and is more a definition than a theorem. Random variables X and Z define a latent factor model iff. p(x,z) = \prod_j p(z_j) \prod_i p(x_i|z) <=> TC(Z) + TC(X|Z) = 0. Definition 2.1: i=1,2,...,p is redundant with \prod_{i=1}^p l.89: sometimes called clustering of observed variables -> reference ? l.93: from [7] -> yhis is bad style. Please name the authors or the approach. l.98: jointly -> multivariate ? Please be consistent in the text. Theorem 2.2: Fig. 1b equivalence -> this is a very bad name for a Theorem. For the sake of readability, either do not give a name or give a proper name, e.g, Modular Latent Gaussian Characterization. l.108: jointly Gaussian -> This is not a general parameterization, but rather a constrained Gaussian distribution where TC(Z|X) = 0. How does that interplay with TC(Z) = 0, TC(X|Z) = 0 and TC(Z|Xi) = 0 ? Is this important for Linear CorEx to work ? Please mention at least that this additionnal constraint is compatible with a modular latent model. This was very confusing to me. Equation (1): minimize_{z=Wx+e} -> minimize_{W} l.145: by increasing m until the gain in modeling performance is insignificant -> How do you measure a gain ? Log-likelihood ? l.148: The computational complexity -> The stepwise computational complexity Figure 3: in both cases s=5 -> Either say what s is here (signal-to-noise ratio), or don't mention it. Knowing what s means requires digging the text. l.208: black -> block l.208: each black being a diagonal plus rank-one matrix -> Why is it rank-one ? Why is that important ? l.246-247: we chose the number of factors [...] -> using which criteria ? Log-likelihood ? l.252-253: We omitted empirical covariance estimation since all cases have n < p -> Why ? I would still be curious to have the numbers to compare. l.271: session 014 -> Why ? this sounds a lot like cherry-picking. l.272-273: We do spatial smoothing by applying a Gaussian filter with fwhm=8mm -> Why ? Your method doesn't work if you don't do that ?  Significance: The presented results are important, as they provide 1) an efficient way of learning modular latent Gaussian models, and 2) empirical evidence that this modular prior results in a better density estimation than other state-of-the-art approaches on a wide range of problems.  Some general comments which the authors may want to address: - Suppose the X distributions contains variables Xi which are independent of the rest, i.e., X_i \ind X\{X_i}. Then such variables may not be linked to any latent variable, and still your minimized criterion can reach 0. The learned structure is a modular latent factor model. How do you deal with that situation in variable clustering ? Your procedure picks the latent variable with highest correlation, yet all correlations with that particular variable are 0. An easy answer is to assign each such variable to its own cluster. You may want to discuss that in the text. - You can simplify the proof for Theorem 2.2, and at the same time make it more general than only Gaussian distributions. Indeed, Gaussian distributions, among others, support the Decomposition, the Composition and the Singleton-Transitivity properties (See, e.g., Sadeghi 2017, Faithfulness of Probability Distributions and Graphs, p.7). Then you have: Z_s \indep Z\Z_s for all Z_s \subseteq Z (<=> TC(Z) = 0) Z_s \indep Z\Z_s | X_i for all Z_s \subseteq Z and X_i \in X (<=> TC(Z|X_i) = 0) Then due to the Singleton-Transitivity: Z_s \indep X_i or Z\Z_s \indep X_i Either both sides are true, in which case X_i \indep Z due to the Composition, and Z_s \indep X_i for all Z_s \subseteq Z due to the Decomposition. Or one side is false, for all Z_s \subseteq Z, in which case the intersection of all such Z_s or Z\Z_s sets leaves out a single Z_j, the unique parent of X_i 