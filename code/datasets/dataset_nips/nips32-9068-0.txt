UPDATE after rebuttal: authors have addressed some of my concerns, so I'm updating my score to 8.  To summarize, this paper aims to shed light on the connections between artificial recurrent neural networks and biological networks, in order to gain insight into neural circuit functionality through studying RNNs. More specifically, the paper comments on the ability for RNNs to mimic the behavior SNNs and neural recordings despite a vast difference in their inherent architectures. Such a phenomenon may suggest neural invariants, which act universally (in the context of a task) across either all RNN and SNN architectures, or broader groups containing various architectures in each. The paper does not look at neural recordings or SNNs, but instead trains 96 RNNs of various combinations of architectures, activations, network sizes, and L2 regularizations on three separate tasks (discrete memory, pattern formation, and analog memory) common to computational neuroscience. For each task singular value canonical correlation analysis (SVCCA) and MDS are used to determine the representational geometry of the RNNs and a numerical approach to dynamical systems analysis (and again with MDS) is used to gain insight into the topological stability structure.  Three low-dimensional tasks are used: the 3-bit flip-flop is 3D, sine wave is 2D, and CDI is 2D. Given that these intrinsic structure of the problems, it is not surprising that the trained RNNs find low-dimensional solutions.  Details about the training accuracy is not provided (please do). I'm assuming all RNNs achieved near 100% performance?  The effectiveness of analysis (SVCCA & fixed point graph) are only presented visually using MDS. Although this is convincing in most cases, it is qualitative only. I suggest computing some sort of similarity metric that summarizes these results. (not suggesting any hypothesis test)  line 193: MDS is very weak support of this claim  Since Vanila is a special case of all the rest (which should be mentioned), any difference found indicates non-trivial difference in the solutions. Yet, the analysis suggests that there is little difference except in CDI. If Vanila can solve the task (I don't know since the performance is not reported in the paper), these differences are due to which solutions are easier to train from the initial conditions, right? Does the gated RNNs diverge from the tanh RNN solution if initialized close by?  For the sine wave task, was there always just 1 fixed point per fixed input? If that's not the case, how would one create a topology of fixed points? In theory, the RNN can find solutions without input dependent fixed point, right? Also, the key feature here is the limit cycle, not the fixed point. This seems to be a major limitation of this method.  Fig 2f does not mean much to me. Am I missing something? Why is this interesting?  line 212: what is the implication of Fig 2e?  For CDI analysis, a line attractor is presumed for the analysis. If there was always one fixed point per condition, it is very likely that every network would give identical "topology" due to the task structure, not due to what the network learned.  Fig 1c needs colorbar Fig 1d at first glance I couldn't tell that these are two different MDS visualizations Fig 1e fixed point graph is too small to see  line 86: Supplement line 156: Fig. 1b should be 1c line 238: "These results show that the all", line 220: "of architecture (right) and activation (left)" these should be reversed 