Paper summary: The paper suggests replacing the Bellman operator in Q-learning with a sequence of stochastic operators, which in the limit, can preserve optimal actions while increasing the Q-value difference between optimal and suboptimal actions (thus reducing the impact of inaccurate value estimate). This sequence of operators add an advantage term to the Bellman operator, with random coefficients whose means are between 0 and 1. The proposed method, named RSO, is tested on 4 simple control tasks on OpenAI Gym. Using tabular Q-learning, RSO outperforms normal Q-learning by a significant margin. Strengths:  1. Novel design of new Bellman-like operators with better properties.  2. Detailed explanation of the experimental design.  3. Easy to read and to understand. Weaknesses:  1. Some experimental results are not convincing enough. For example, Bellman and consistent Bellman completely fail in Fig 1. That usually is due to lack of exploration. It is possible that RSO works better simply because it has more randomness. In addition, section 4.5 claims that the uniform \beta distribution is better than constant, but no numerical results are provided. Also in Fig 2 (b), the difference between RSO and consistent Bellman seems very small compared to the scale of rewards.  2. It is perhaps better to explain intuitively why RSO is optimality-preserving and action-gap-increasing based on equation (4). Then the reader wouldn't have to read through the proofs in the appendix before getting a rough understanding. 