The authors present a way of self-supervised auxiliary learning in which the images in the training set are rotated with 4 different rotations, and the neural network has to predict the type of rotation. The authors show with various experiments that this type of SSL increases the robustness against all kinds of perturbations, ranging from adversarial attacks to motion blur and fog. In addition, the outputs indicating the rotation can be used for detecting outliers. The article makes a good case for both contributions.  One main remark is that the title of the article talks about uncertainty estimation, while the experiments focus on outlier detection. These two tasks are related but not identical. Outlier detection boils down to binary classification, while uncertainty estimation produces continuous real values. In principle, the scheme introduced by the authors could be used to produce floating point numbers (the sum of outputs is such a number), but additional experiments should then show that the magnitude of this "uncertainty" then makes sense.   One of the enticing properties of the method proposed by the authors is that it is so simple / elegant. When comparing with other approaches for OOD detection, the other approaches all have been explicitly made for this task. However, how well would it work to just take the max probability of the normal neural network (instead of the auxiliary outputs)? Should the probability of the max class also not be lower for outliers? And what about estimating the uncertainty via dropout at test time? How well does that work?  Concerning robustness, I would wonder why the proposed auxiliary learning works. When also rotating the images, does this lead to more "blurry" features, so that it can deal with blur better? Differently put: Would it work for any auxiliary self-supervised task? What is special about rotations?    Finally, when introducing this training, the "normal" performance almost always drops. For example in table 1, the performance goes from 94.8 to 83.5. What do the authors think of this matter? Can it be amended in the future, or is this just a trade-off between robustness and performance on clean samples?  Overall, the article is well-written and makes a clear contribution to the field.  