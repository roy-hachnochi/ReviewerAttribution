- This is an experimental paper with a quite clear message: The auxiliary rotation-based self-supervision may not improve accuracy, but it does something in robustness (both in adversarial, and corruption), and (especially) in OOD detection.  - Despite of its simplicity, the idea is generally well-presented, with good experimental results. - However, I feel the current manuscript does not provide enough motivation or insight on why the auxiliary rotation task could improve robustness and OOD detection. More detailed analysis on the results would much help the readers to understand the significance of this work: e.g. ablation study, comparing characteristics of the original and proposed networks in the viewpoint of robustness/OOD.  - My another concern is about a lack of novelty: Provided that "using pre-training can improve robust and uncertainty" [1], the results in this paper may not be that surprising for some readers, as self-supervision might be just another form of supervised bias in training a network. Again, I think this issue might be relaxed by providing more detailed analysis of the results: Is the claim generalizable to the other self-supervisions apart from the rotation? How about other auxiliary tasks, in the general framework of multi-task learning? Such questions should be justified.  - L117 (Section 3.1): In the adv. robustness part, It seems the proposed method makes two (different) modifications from the original adv. training: adding SS-loss (a) on the training objective, and (b) on the PGD objective. Among those two, which one would be more critical for the observed gain?  [1] Hendrycks et al., Using Pre-Training Can Improve Model Robustness and Uncertainty, ICML 2019.   ----------After rebuttal----------  The authors may discover an interesting finding. But, even after reading the rebuttal, I am still unclear why predicting rotations improves robustness and uncertainty.  They provide a part of reasoning/insight in the rebuttal letter, which is nice (hence, I increase my rate a bit, still on a negative side though), but not enough for me, e.g., then the authors should also show whether other non-rotational self-supervised learning works or not to support it.  I think the paper has a good potential in the future, but not ready to publish as the current form. 