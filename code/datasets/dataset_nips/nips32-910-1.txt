Originality: The main architectural pieces (VAE, message passing) have been proposed previously, but the specific focus on directed graphs using these components is new to my knowledge.  Quality: The proposal was technically sound, but the proposal has undesirable properties which were not addressed by the authors, and lacks sufficient empirical evaluation:  1. Does the proposal easily (from a practical implementation perspective) allow for batching?  2. Decoder sequence length. Instead of N steps with an RNN, the proposal's decoder uses N*(1+2+...+N-1) steps. This may limit the proposal to small graphs (the authors have only evaluated on a fixed, small graph size).  3. Lacks simple baselines, such as an RNN/LSTM (since this is claimed as a special case of the proposed framework, it would be nice to see that performance improves over the special case).  4. Lacks a strong graph-NN baseline. GCN has strong assumptions compared to a general message passing neural network. A concrete suggestion would be comparing to [Li et al 2018], which generates a graph using a sequence of add-node and add-edge decisions using a message passing network to represent the partial graph. Since Li et al deals with general graphs, showing an improvement over their model may indicate that the proposed restriction to DAGs is beneficial.   5. Ablation studies. Crucially, we do not know whether the proposed asynchronous message passing actually helps, versus using standard message passing with the proposed architecture.  6. Simplified experimental settings: fixed-size, small graphs.   Clarity:  The paper is generally written clearly, with a few exceptions: - The term "asynchronous message passing" is a bit confusing, since each node has to wait for its predecessors' message to be computed (hence it seems synchronous); the authors might consider defining asynchronous and synchronous precisely in the context of their proposal.  - (Minor): Line 29 - awkward phrasing ("the answer is yes").  Significance: Due to the issues above the significance is low-medium.   ----- Edit after author feedback, raising from 5 to 6 ---- The authors provided a comparison with [Li et al], ablations of the message passing, and evaluation on a larger NAS setting and variable-length setting - thanks for the efforts in implementing and adding these! The proposed method does show improvements over [Li et al] and the message passing ablation strengthens the case for the scheme proposed here (though performance drops significantly with variable length sequences). Due to the new evaluations I'll raise the review by one point. Minor: LSTMs could be used via consistently linearizing the graphs, but with the [Li et al] baseline I don't think this is needed. 