The paper presents a very interesting idea: uses combinatorial methods only for projecting neural (or hybrid neural-symbolic) functions to the programmatic space. The projection operator uses program synthesis via imitation learning such as Dagger.  The paper does not clearly explain why projection is much easier than searching in program space. A detailed explanation with an example will be needed.  The evaluation results are very weak. It uses very simple RL tasks.   The dropbox link https://www.dropbox.com/sh/qsgxk76xg9t5ow3/AACyJIkqwhrO9EUmVa6SIrJva?dl=0  does not have the code. The folder is empty.   ---after author feedback and reading other reviews--- The authors have addressed some of my concerns: projection via imitation learning is easier, and provided the code and a video comparing with NDPS. The authors did not address my question on evaluation well. Although TORCS tasks are important, a single environment is not enough for evaluation. In particular, when can IPPG learn effectively via imitation learning can not be established via TORCS itself. Online imitation learning may be hard in many application domains. Can you leverage offline demonstrations? As a result, the paper still needs lots of improvement. However, I am willing to increase my rating to 6 given some of my concerns are addressed.