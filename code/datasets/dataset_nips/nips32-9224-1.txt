The paper propose a system that takes advantage of combining an off-the-shelf RL model with supervised training to create different parameterizations of "data reward", which helps guide the weighting and augmentation of training data. The method is clearly described; however, given the idea of RL for data weighting/augmentation has been explored before, it is challenging to judge what the specific novelty is.= beyond the new data reward *function* itself.  The experiments are thorough and support the advantages of the algorithm. Table 1 and Table 2 show the performance of the method in the augmentation and the weighting conditions, but not when *both* are applied simultaneously. Next, the method is not tested for augmentation over the class imbalanced CIFAR dataset.   The main drawback of the method is that the training data cannot be weighted and augmented simultaneously. Given the algorithm is general enough and based on the same data reward function, combining these two popular data manipulation strategies would be interesting.