This paper addresses the problem of detecting adversarial attacks to image classifiers -- this is a very important problem, and its solution lies at the heart of what can be considered to be one of the main challenges to overcome in the near future for this kind of classifiers to be of use in real-world applications. The solution proposed is novel in that the existence of so-called adversarial perturbations -- usually considered to be the main problem -- are used as the main building blocks for a defense mechanism against attacks. The material is clearly presented, and experimental results show that the approach achieves good results on a challenging dataset.  The main question that comes up, on which the authors should comment in the rebuttal, is regarding one of the basic tenets on which the method is based. In particular, it's not clear whether the criterion regarding closeness to the decision boundary is universal -- is it valid for any kind of classifier? A discussion of limitations of the approach with respect to this issue seems necessary.  Another question is regarding the white-box adversary: what prevents the design of an attack method using the authors' method as a black box, basically iterating in the generation of adversarial images until it passes the detection method?  Finally, in the conclusion the authors mention in passing that running time is a limitation; could you please provide some details regarding this? As it is, it's not possible to determine if it's a major issue or just a minor drawback.