I think the idea of using attention or transformer inspired architectures for protein modelling is useful and the authors changes on the standard transformer are helpful since the full attention computations are typically costly. 