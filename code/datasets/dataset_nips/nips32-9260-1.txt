This paper addresses the problem of generation of protein sequences for a desired 3D structure, also known as the “inverse protein folding problem”. The authors introduce a model inspired by recent advances in language modeling (for the sequence decoder part of the model) and graph representation learning (for the encoder part of the model). Protein structures are represented as k-NN graphs, enriched with orientation/location-based features and features based on structural bindings. The encoder takes the form of an adapted Graph Attention Network, here termed “Structured Transformer”, which is enriched with edge features and relative positional encodings, and the decoder takes the form of an auto-regressive Transformer-based model. Results indicate improvements over a recent deep neural network baseline for this task.  The problem is of high significance and the authors make several non-trivial contributions that clearly improve the state of the art in this area. The paper is very well written and generally of high quality. It is well-positioned w.r.t. related work and all the contributions are well-motivated.   I am not an expert in the area of “inverse protein folding”, but the paper did a great job at introducing the problem and related work. It would be good, however, to provide a more detailed description of the SPIN2 baseline and discuss differences to this particular model.   The experiments are chosen well, but it would be nice to see error bars on results and further ablation studies, especially on the proposed attention mechanism and the relative positional encodings. Overall, I can recommend this paper for acceptance.  The authors seem to take it as a given that a Transformer-based model is naturally the best fit for this task, but I wonder whether a (potentially simpler) message passing neural network as in Gilmer et al. (ICML 2017) would perform similarly when used as an encoder for this task. To be more precise, this would correspond to performing a) an ablation study on the attention mechanism (i.e., leaving out a_{ij} in the update for h_i), and b) using a small MLP to transform v_{ij} instead of a linear map — this corresponds to the message in the message passing neural network framework.   ---  My questions have been addressed in the rebuttal and I am looking forward to seeing the comparison against Message Passing Neural Networks and a discussion of the SPIN baseline in the updated version of the paper. My recommendation remains accept, but I leave it to the other reviewers to judge the relevance of the new results comparing their method against non-deep learning baselines (for which I do not have any expertise).