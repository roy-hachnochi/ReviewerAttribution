Originality: Overall, the approach taken in this work differs significantly from past work. Unlike previous applications of deep learning to protein design, the authors present a model capable of modeling the joint distribution of protein sequences conditioned on their structure. The authors extend the Transformer to include sparse relations and handle data that is spatially structured. The authors provide a new representation of protein structure using a graph with relative spatial encodings. This representation is likely useful beyond the scope of protein design.  Quality: The submission is technically sound and the experimental results are clearly described and analyzed. I do have one concern: in order for this algorithm to be practically useful, there must be a way to decode a sequence with high probability under the model given an input structure. By only looking at log-likelihoods of individual sequences in the evaluation metrics, the authors avoid using any decoding strategies. One concern is that given the large length of protein sequences, greedy decoding strategies such as beam-search will produce low quality sequences (i.e. sequences with low log-likelihoods). A simple experiment would be comparing log-likelihoods of the top sequences from the authors choice of a decoding strategy to the log-likelihood of native sequences. I would ask the authors to include a discussion of decoding strategies and experiments in the paper.  Clarity: This paper is well written. The related works section is thorough and makes clear what the deficiencies of previous deep learning approaches are. The explanations of the graph representation and neural architecture are also clear. The results section provides a nice analysis of perplexities based on random protein sequences and first order profiles of protein sequences from the Pfam database.  There were a few aspects of the paper that could use clarification and/or expansion.    1) I do not think the authors explain what the input backbone coordinates are? Since there is one coordinate for each residue, I would assume the authors are using the C_alpha coordinate.    2) I believe there is a typo on line 198. "We find that the model is able to attain considerably improved statistical performance over other statistical performance."        Significance: The results provide a significant improvement over previous deep learning approaches but fail to make any comparison to traditional protein design algorithms. In my opinion, these comparisons would provide substantial value by illustrating how deep learning methods compare to conventional approaches. Nonetheless, the approach that the authors take is unique and seems to work reasonably well. The representations and architecture are likely useful outside of the scope of protein design. The authors do not provide any discussion of protein redesign, in which only a subset of the amino acid residues are designed and the rest are maintained at wild-type. The approach seems sub-optimal for redesign since it is tied to a particular choice of a decoding order. The only option I see to perform redesign with this method is to enforce the amino acids that are not being designed to their identities during the decoding. However, this might require more sophisticated decoding strategies to produce high-probability sequences. This limits the practical use of the method since full design is rarely the task at hand for practical protein design problems.  --- My primary concerns were addressed in the rebuttal stage. I am updating my score from a (6) to a (7).