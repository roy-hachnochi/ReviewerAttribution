The shows that layered feed forward neural networks that are comprised for affine transforms and convex nonlinearities (e.g. RELU) can be seen as implementing a  series of divisions of the input space whereby the division takes the form of a power diagram (a generalization of Voronoi tessellations) . This is proven by analysis of single units, analysis of single layers and analysis of layer-to-layer transitions.  The work hinges on the description of layers as implementing a max operations over affine transforms (max affine spline operators, MASO) which have apparently been studied in this context previously. The use of power diagrams to describe MASO output in this context seems new.  The paper goes on show some analysis results on the log-distances of training points in the CIFAR dataset from their power  diagram centroids  at different network layers. Finally, there is some analysis of decision boundary curvature.  Figure 3 seems to show that centroid ad deeper and deeper layers which match a specific input points become less descriptive of that point, apparently due to larger and larger radii (regions are described by centroids and radii). This results seems somewhat counter intuitive and seems to hint that this the centroids do not behave in a simple comprehensible manner. Perhaps the figure should be better explained in the paper.  There is no analysis of the neural network training processes or how the power diagram representation might help in understanding a network training session.  The man theoretical results provides a comprehensive description of what the most popular basing network architecture can do. It would be interesting to see if that result could somehow be used to achieve better performance or training efficiency of such basic networks.  There is no summary / concluding remarks and no code is provided (although there is evidence that code was used) 