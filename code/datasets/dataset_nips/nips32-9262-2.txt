 This paper uses a computational geometry approach to analyze the region decomposition of neural networks, which leads to power diagrams. Propositions and theorems in the paper however are not fundamentally new, following straightforwardly from the geometry approach.   The authors nicely observe that their centroids can be efficiently computed via backpropagation and apply the analysis to power diagrams to understand neural networks. Visualizing the region centroids and averaged distance to boundaries reals interesting information, which are however sort of expected, e.g. distance decreases with more layers to subdivide input regions. Results and their interpretation in the paper are still very preliminary.   Despite this, the paper advances in the promising direction of geometrically interpreting neural networks; it is to my surprise that contents in the paper has not been thoroughly studied in the past. I think conclusions in the paper will serve as the starting point that many future work in the direction will benefit from. Ideas in the paper such as considering curvature at decision boundary looks promising and may worth further study.    Exposition:  While the idea of the paper is clear from the high level, exact details of notations and equations in the paper are hard to follow in its current exposition.   Notations in the paper sometimes is too complicated. E.g. f^{(1->l)}_{k} in Eq. 18 is not explicitly explained and I have to guess the meaning of superscripts.   epsilon_{k,2}^(l-1)  in Eq. 21 is not explained, and should the superscript be (l) or (l-1)? It should also be explained where Eq. 21 comes from.   Eq. 20: why left hand side is squared distance? This equation is not explained.    Typo: Line 333: hve -> have   