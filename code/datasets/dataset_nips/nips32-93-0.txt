The paper provides a new splice detection approach that consists of two generators trained adversarially. The retoucher tries to tamper with the input image by adding a splice while the annotator tries to detect splices (as well as the object classes in the image).  Pros: + Nice and novel idea. It is also intuitively simple: train the splice detector against the various splice types learn by the retoucher. + Sound and effective architecture, cleverly using existing tricks in the literature. Clear exposition. + Extensive experimental analysis against several state of the art baselines on multiple datasets. Convincing results both in qualitative and quantitative terms.  Cons: - The paper could benefit from more analysis on why it is so effective. For example, the authors could show the tampered images by G_R, performance for each type of splice (rough/realistic), how G_A adapts over time, does it overfit when G_R becomes very effective (i.e. is G_A still able to detect less sophisticated splices)?  Also, the training of G_A seems broken when no G_R (it outputs the exact same mask) but the authors do not explicitly mention this. - Citation for [9] is wrong, I believe it should be Mejjati et al. "Unsupervised Attention-guided Image-to-Image Translation", NeurIPS 2018, also referred to as AGGAN later on. - Some details: losses for discriminators not explicitly stated, unconventional notation for L1 distance (should be '-', not ','), wrong GT mask for third column in Fig. 3.  The author's response successfully addresses most of the reviewers' concerns. I keep my acceptance score. 