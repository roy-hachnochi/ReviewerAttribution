Overall, I found this paper to be a nice read. It lays out the motivation for the problem and then illustrates how one can apply the idea for various different notions of a "close distribution," e.g., KL-divergence, Wasserstein metric, and distributions that match the first and second empirical moments.  One strange thing about this approach is that the optimistic probabilities found at the end may not integrate to 1 (for example, the kernel density estimator will integrate to 1). For this reason, it doesn't appear the optimistic likelihood is a likelihood in any traditional sense. Because of this property, I would like to understand better how this new sense of likelihood behaves. Does it relate to any other already studied notions of convergence? E.g., there are results that kernel density estimates will converge to the true density under certain regularity assumptions as the number of samples grows and the bandwidth decrease. Is some analog true here?  One area I found a bit confusing in the paper was how this could be actually used to solve the problem of Bayesian inference referenced at the beginning of the paper. For example, in section 6.1, how does one come up with the hat{v_i}? Does one have to know how to sample from p to produce these distributions?  While the ideas presented in the paper are interesting, I'm still unsure whether this approach is better than simpler methods like the kernel density estimation. I would be more convinced if this method were tried on more practical models, e.g., for those where ABC were necessary.   Detailed comments:  L74: Should say "denoising." L127: Should the v* be v*_{KL}? L133-134: What's e in e^T y = 1? Figure 1: Is it true that p(x) is unbounded near the poles -1 and 1? Is this desired? L224: Is it clear that this approximation really is an approximation? By the looks of Figure 1, it appears the density can be unbounded near x_i. L269-275: How does one construct hat{v}_i in this case?  Originality: The paper has a new approach for fitting posterior distributions in a non-parametric fashion. One limitation is that the distributions must be discrete, and they can be expensive to use as the dimension m and data size m increase.  Quality: The paper appears to be correct, although I have not checked the proofs from Section 5.  Clarity: The paper is quite clear and nice to read. There are a few parts where some details are omitted (see the comments above) but overall it was easy to follow.  Significance: This is the biggest area preventing a higher score on this work. Based on the limited empirical examples, fact that Theta must be finite, and fact that the optimistic likelihood isn't a true probability distribution, I am unsure whether this method is truly better than something simpler.