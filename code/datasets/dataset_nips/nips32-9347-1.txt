  originality: O1. The paper is based on G-invariant networks but uses an interesting idea to improve its power. O2. The use of measure theory is not new (e.g., Bloem-Reddy and Teh [2]) but the connections to function approximation were interesting.  quality: Q1. I like the paper very much. It is great to see the works on GNNs adding more formalism. Ring-GNN is a simple but interesting idea, add the powers of A to the G-invariant network representation.  Q1.1 I really liked the use of the CSL task, it provides a clean way to show progress in the representation power. Q2. [IMPORTANT] Adding SVD to Ring-GNN feels like cheating. What if k-GNN and RP-GIN methods were also given the eigenvalues [say, concatenated with their embeddings passing through an MLP]? The eigenvalues of the 10 CSL classes are likely different, even a simple MLP may be able to distinguish the classes without the GNN. Either show this is not the case, or remove the SVD approach. Q3. The sigma-algebra formalism has many parallels to that of  Bloem-Reddy and Teh [2], but I like the former better for the added probabilistic interpretation.  Noise outsourcing should allow this paper to connect the sigma-algebra formalism with representation learning and generative models. I think tightening the connection with Bloem-Reddy and Teh would strengthen this paper. Also, introducing the concept of orbit would be useful in the formalism. Q4. One of the main competitors on experiment 6.1 (RP-GIN) should be described in the introduction with GIN. Q5. When introducing CSL graphs (first mention of Figure 5), it would be useful to cite Murphy et al. [19] where they are described in more detail, otherwise it looks like something that the reader should just know.  clarity: C1- I really enjoyed reading the appendix, thanks for all the effort that went into it! (illustrations, examples, detailed descriptions, extensions are all great)  C2- The statement "Such a dependence on the graph size was been theoretically overcame by the very recent work [13]" right after "showed the universal approximation of G-invariant networks, constructed based on the linear invariant and equivariant layers studied in [16], if the order of the tensor involved in the networks can grow as the graph gets larger" is strangely worded , as it seem to imply [13] was abe to overcome the large order tensor that universality needs (which is contradicted after the comma in the next page).   C3- " [19] proposes relational pooling", ". [2] studies the", ." [16] studies the spaces" => it is odd to start a sentence with a number. Please add 1st author's names when rather than using numbers as nouns.  C4 - Section 6.1 does not comment on the Ring-GNN-SVD. Overall, I feel the SVD addition to Ring-GNN without adding the eigenvalues to other methods is cheating.  typo: sigmas-algebras => sigma-algebras  