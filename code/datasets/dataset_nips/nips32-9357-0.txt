The authors report increased accuracy and robustness by adding an extremely simple, linear filtering with a difference-of-Gaussian kernel. While this would be a quite remarkable finding if it turned out to be reliable, I have substantial doubts, which I will outline below:  - Instead of using a well-established baseline (e.g. a ResNet), the authors train their own architecture (similar to VGG) on what seems to be their own 100-class variant of ImageNet. Despite these simplifications, their performance is quite poor (a ResNet-18 would achieve ~70% top-1 accuracy on full ImageNet). As a consequence, we don't really know whether the improvement by their surround modulation module is an artefact of a poorly trained model or a real effect. Training a ResNet on ImageNet is trivial these days (scripts are provided in the official PyTorch or Tensorflow repositories), so it's really unclear to me why the authors try to establish their own baseline. In fact, training VGG-like architectures without batch norm is anything from trivial, so it's quite likely that the authors' model underperforms substantially.  - The relation between what the authors do and surround suppression in the brain is weak at best. In the authors' implementation, the only neurons contributing are the ones from the same feature map. Such specificity is unlikely to be the case in the brain. In addition, biological surround modulation is purely modulatory, i.e. has no effect if there is no stimulus in the classical receptive field. However, the authors' implementation as a linear filter will elicit a response if only the surround is modulated without any stimulus in the center.  - I am unsure about the value of the analysis about sparsity. What are these figures meant to tell us? I think they could as well be left out without the paper losing anything.  - Why is the surround modulation module added only after the first layer?  - Were the networks used for NORB and occluded MNIST trained from scratch on these datasets or pre-trained on ImageNet and only fine-tuned?