This paper incorporates surround modulation neural mechanism to convolution neural network. The authors add local lateral connections (defined in eqn 1 and 2) to the activation maps of convolutional neural networks that mimics surround modulation. To my knowledge, this work is of interest to both the machine learning and neuroscience communities. I find that the results that surround modulation improves CNN performance very interesting. However, I have the following comments: 1- The application of the difference of Gaussians kernel in the computer vision domain is not new. DoG filtering has been proposed before for feature extraction in computer vision (see for example Lowe et al., IEEE 1999 also a wikipedia article here https://en.wikipedia.org/wiki/Difference_of_Gaussians and https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). However, I still believe the results that using this kind of 'engineered' filters gives better performance than baselines is interesting finding.  2- The experiments presentation requires more clarity. In particular, I found it hard to understand the baselines. There are three things that one needs to control for:  1) the number of trainable parameters 2) the depth of the network 3) the structure of the SM kernel.  The authors tried to clarify their baselines in paragraph starting line 155. However, I find this description largely unclear.   3- The experiments lack hyperparam tuning. One simple explanation of the results is that the training hyper-parameters were not optimal for the baseline models.  4- For the generalization results, it is unclear whether one could get the same SM-CNN results or better by using standard regularization methods.  5- It would be very interesting to see if the same results would hold for larger networks.   