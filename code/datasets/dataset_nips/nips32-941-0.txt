This paper addresses an important problem in a unique way. Self-supervision is a promising avenue of research, but currently relies on significant domain knowledge. The authors propose to overcome this with model agnostic meta learning. The experiments are fairly comprehensive and the exposition is clear and well-cited.   Despite the originality of the work, the experiments do not currently make a very strong case for its significance. The comparisons in Table 1 show consistent but modest improvements in accuracy. While the improvements are greater than the run-to-run variation with different random seeds, it is unclear how such an improvement compares to variations in performance by modifying standard data augmentation and/or regularization. This makes it less clear how general the results would be.   Similarly, the results on Cifar100 are suggestive but not very convincing. MAXL clearly performs significantly better than single task training and the random baseline, but the relative advantage over the k-means baseline seems to be at most 1% relative. It is difficult to tell by the graph presentation, and it's not clear what additional information is provided by the time series of the graph.   Finally, it's nice that visualizations are presented, but the analysis of the visualizations is somewhat lacking. CNNs are notorious for modeling less salient features such as texture and global illuminance. Further quantitative or qualitative studies, such as salience / gradient visualization of the learned features could help illustrate the common characteristics. 