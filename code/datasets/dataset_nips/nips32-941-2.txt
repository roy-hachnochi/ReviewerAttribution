Though I think this paper proposed a very interesting approach to automating the design of auxiliary tasks. I am disappointed by its practical value on the image classification tasks evaluated. According to Table 1, the method outperformed the standard single-task learning baseline by a very small margin (less than 1%) on all seven datasets. Why didn’t we see larger performance gains using the proposed approach? I’d hope to hear the authors’ hypothesis. Also, with what kind of datasets/models, the proposed method for generating auxiliary tasks is the most effective?  I am also curious about the scalability of this approach with more advanced architectures and larger-datasets. In the experiments, larger images are rescaled to [32x32] low resolution. Could the authors comment on the reason of this design choice? Are there any technical constraints that limit the model in training with larger images? Would this model be effective in more realistic image classification setups, say training the state-of-the-art ResNet-101 for the ImageNet challenge?  Another baseline that I’d like to see is to use variational encoders (e.g. beta-VAE) to learn latent discrete representations from the data and use the learned discrete code as labels for auxiliary learning. This could possibly be a stronger baseline than the k-means baseline.  Fig. 4: what’s the intuition why the cosine similarity between the gradients of the primary task and the ones of the auxiliary task always possible? Would it be possible that the auxiliary tasks can generate gradients in the opposite direction to the gradient of the primary task so that it pulls it out of a local minimum?  Fig: 2: is there any parameter sharing between the multi-task network and the label generation network?  I appreciate the provided source code and the reported negative results in the supplementary materials.