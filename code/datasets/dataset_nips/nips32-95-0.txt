Summary:  This paper proposes a method for speeding up adversarial training by reducing the number of full forward- and  backward passes during the inner loop where the adversarial examples are computed. To achieve that, the inner loop that calculates the adversarial perturbation is split into an inner and outer loop. The gradients for all but the first layer are only infrequently calculated in the outer loop, while the gradients of the first layer w.r.t to the adversarial perturbation are iteratively calculated in the inner loop. This method is motivated by casting adversarial training as time-differential game and analyzing the game using Pontryagin's Maximum Principle (PMP). This analysis indicates that only the first layer is coupled with the adversary update.   The authors relate their work to "Free-m" [1], another recent work that aims to speed up adversarial training and mention that their method is a generalization with a minor change of the work in [1]. Analysis of deep learning training as control problem was previously been done in "Maximum Principle Based Algorithms for Deep Learning" [2]   Originality: - The main contribution of this paper is to speed up adversarial training where the method used is motivated by casting adversarial training in the light of control theory. There has been research on the control theory perspective on training neural networks, but not related to adversarial training.  Quality: - In the Algorithm following Line 138 and following Line 143 the gradients are not normalized and also not projected back on the epsilon-ball as typically done in PGD [3]. Is there any reason for that? -  A non-mathematical intuition about why the adversarial update is only coupled with the first layer is not given. This would be especially interesting, as this claim is counterintuitive compared to previous methods for generating adversarials that need to differentiate through the whole network. - The experimental section lacks evaluation of stronger versions of the PGD attack (restarts, higher number of iterations) and also evaluation with non-gradient based methods. - In Line 155-159 the authors write "we take full advantage of every forward and backward propagation, […], the intermediate perturbations are not wasted like PGD-r […] which potentially drives YOPO to converge faster in terms of the number of epochs". This claim has no support in the experimental section, e.g. a plot loss over epochs.  Clarity:  - Theorem 1 is very hard to follow and unclear to a reader not familiar with the Maximum Principle.  ○ Why is the Hamiltonian defined the way it is?  - Multiple variables are not described and unclear  ○ Is p in the Hamiltonian the same as the slack variable p?  ○ Notation of p_i_t (used in Theorem 1) is not introduced. - Algorithm listings after Line 138 and Line 143 are missing captions - Line 157: "allows us to perform multiple updates per iteration" What iteration is meant here? Formula 144 suggests that the weights are only updated once after the m-n loop has finished.  - If the coupling is only between the first layer and adversarial perturbation, why does p needs to be updated in the outer loop? - What epsilon is used during training?  Significance: - Given that adversarial robustness is only slowly moving towards bigger datasets like ImageNet, the significance of a method to speed up adversarial training is high, assuming that the robustness claims are true and hold in the setting of larger datasets. - Only one other method [1] has addressed the topic of speeding up adversarial training, thus this paper is a very welcome contribution to the field. According to Table 1, the method proposed in this work further improves the training time over [1].   [1] Shafahi et al. - Adversarial Training for Free!,  https://arxiv.org/abs/1904.12843 [2] Li et al. - Maximum Principle Based Algorithms for Deep Learning, https://arxiv.org/abs/1710.09513 [3] Madry et al., Towards Deep Learning Models Resistant to Adversarial Attacks, https://arxiv.org/abs/1706.06083 