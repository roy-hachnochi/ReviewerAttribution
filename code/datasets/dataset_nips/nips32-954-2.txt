Originality: While the variational formulation for the given image denoising is new, it is not clear why some of the choices for the prior distribution is valid. (e.g., eq.(4))   While the new formulation makes sense, there are several critical questions regarding clarity and quality of the presentation of the paper. I would like to see the answers for these questions in the rebuttal.   - The existence of the simulated clean x is most puzzling. It is not clear how we can always obtain those. Also, if those simulated clean x is available, what is the performance of the direct supervised model that maps y to x? Do we really need the complex variational formulation of the paper? Also, for real-world data, how can one obtain x? I think obtaining x in [1] is a special case since there are multiple noisy observations for the same underlying clean image. But, in more realistic case, it is nearly impossible  - Figure 2 is very puzzling. As far as I understand, both D-net and S-net are fixed once the training is done. How can S-net predict the totally new noise pattern that has not been seen during the training? Unless some additional process is described, e.g., fine-tuning with the test data, Figure 2 is not clear to me.  - The PSNR numbers for BSD68 (e.g., for sigma=25) in Table 2 is very high. Are all PSNR numbers reproduced by the authors?   ====  The rebuttals mostly clarified my questions. Still Fig.2 is a bit surprising result to me, but the generative nature of the Bayesian framework perhaps enables it. I increased my score to 6.   