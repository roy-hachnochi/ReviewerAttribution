In this work the author(s) focus on block coordinate descent as a method for optimizing the block-sparse version of LASSO. The main contribution is to reduce the computational cost of the iterative thresholding performed when optimizing over each block (dissected as the groups). This reduction is done via bounds on the thresholding decision boundary that are cheaper to compute, but indicate when a block is confidently not zero, or confidently all zero. I have a number of questions about the method as outlined below:  1) This method seems to only be applicable to disjoint groups of coefficients. Many applications require overlapping groups (e.g., Group sparse optimization by alternating direction method by Deng, Yin, and Zhang). Does this method trivially extend? It does not seem to, at least to me.   2) This method focuses only on increasing the efficiency of the block-coordinate descent. Other methods, such as iterative thresholding for group-sparse inference, which does not use a full internal block iteration and instead only performs one step of thresholding per block, can be used instead (e.g., Iterative Thresholding for Sparse Approximations by Blumensath and Davies and others since then). Do any of the benefits transfer? Depending on the number of iterations it takes to compute, these methods could be comparable, or be much slower. It would be useful for the authors to compare to these other methods   Minor comments: Line 60: Solution \hat{\beta} is obtained --> The solution \hat{\beta} is obtained Line 122: this norm is broken up over 2 lines: ||K^(g,l)[i,:]||_2 