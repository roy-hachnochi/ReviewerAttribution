 I think this is a very nice and original contribution trying to bridge properties of simple models of neural networks, the teacher-student soft committee machine in the present case, with empirical observations available in deep learning. Adjustment of the models to match the observed behaviour seems to be a very valuable way to proceed towards better understanding of the deep learning phenomena. The paper is well written.   Comments and questions:   ** Can the authors define the relation between number of epochs and number normalized steps? In  particular in Fig. 1, did the system see only 10*Epoch samples or was it the whole MNIST passed trough several times by the SGD.  ** To give a better idea of how good is the neural network the authors are using, can they state the accuracy on the test set corresponding to Fig. 1?  ** The authors summarize one of their contribution by saying: "By analyzing the macroscopic system we derived, we showed that the dynamics of learning depends only on the eigenvalue distribution of the covariance matrix of the input data, provided that the learning rate is sufficiently small." This should be stated more precisely. Surely the dynamics of learning depends also on the way the labels were generated, which is not considered in this sentence.   ** The plateau phenomena is intimately related to the specialization of hidden student units ot the teacher units, I think it would be valuable if the authors discuss this connection quantitatively and evaluate their theory in this respect. In particular the authors conclude "Considering this, the claim that the plateau phenomenon does not occur in learning of MNIST is controversy; this suggests the possibility that we are observing quite long and low plateaus." Shouldn't the specialization of hidden units or the lack of there-off be a good measure to resolve this "controversy"?   ------ post-feedback  I have read the other reviews and the author's feedback. I maintain my score. The problem this paper addresses is in my opinion important. At the same time I urge the authors to consider all comments from the reviews to make their paper clearer to the NeurIPS audience. 